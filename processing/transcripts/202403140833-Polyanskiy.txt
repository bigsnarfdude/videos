I'm excited to be here. I'm sorry I didn't put all the names of people here. I'll mention them in the process. Okay, so I'll talk about data-driven signal recovery. Do I need to wear a mic? Because you said it's going to be in YouTube, but it's always airport. All right, so here's my top plan. You know, I'll start with some kind of general thesis, and then I'll do some theory, and I'll show you some practice. Theory, and I'll show you some practice. I think this kind of outline is, you know, reflects the standing of our community, information theorists. For example, I found out a lot of machine learning people, and they think of us as being conceptual. So this is like the conceptual part, right? And we believe we are theorists, and we also influence practice. So I try to mix everything together. Okay, so what's my thesis? So the thesis is basically: I don't know about you, but when I teach information theory, I always feel a little bit uneasy because I keep saying, you know, this. Bit uneasy because we keep saying, you know, this stuff is general, but really, we can only do it for 90 gigabytes of noise, right? But we keep saying, you know, with work, you can extend it, but somehow it didn't happen. But anyway, so there is maybe I'll offer some plausible way out of this conundrum, right? So maybe we can analyze some other channels and statistical models. So for theory, I'll present some results on likely contrary hypothesis testing. That's again a topic which, as you will see, has a very, very deep root in information theory. In fact, it was started by Jacob Zeft. In fact, it was started by Jacob Zeith, and then I really hope to show you some fun stuff with the actual practical implementations. So I know, you know, I have lots of slides, I will go fast, but it's not going overwhelming you just because I kind of have one slide for each one of you, or so to speak, and I really want to show it, you know, so I apologize. So like, for example, the slide for Cindy is somewhere towards the end, and I really want to get to it. So yeah, anyways, but okay, so how do we teach classical detection and estimation, right? We say, alright, so if you You say, alright, so if you send a BPSK signal, it's constant one, observing Gaussian noise, or constant minus one. So, how do you detect it? You take you some wise and threshold it, right? So, more generally, if you have some kind of pulse shape S0, a pulse shape S1, right? We test hypothesis, we do the matched filter. Okay, so far so good, right? So, this is all optimal. Okay, even more generally, right, if you go further beyond Gaussian voice, we can say, we tell them, okay, we can also handle more general models. More general models. If you have y-arm sampled from P as one hypothesis, another hypothesis, maybe the channel is non-linear, so it's no longer argued noise, right? So some kind of, you know, then we also know what to tell, right? You just say, name a person. Okay, so what is more general? Okay, if you don't know the distribution, right? So then you could say, well, suppose my YM comes from a distribution class, right? I kind of don't know exactly which distribution generates it, but there is a class kind of graphic P, right? And then you want to test it. Graphic P, right? And then you want to test it again another class, so then do what? Okay, unfortunately, there is no answer to this, right? So then we have to be creative. So, I mean, one idea to usually try is generalize like ratio test. Sometimes it will work, most of the time it will not work, then you have to search, you know, another statistics and hope that somebody solved your problem. But in any case, the problem is that if these classes are realistic, then you will have a very small estimation of. Have a very small estimation or hypothesis testing sample complexity, right? Because if these classes are very difficult, then it's non-parametric, let's say, right? And you need really a lot of samples. So it becomes impractical. So then the question is, how do we fix this? Okay, so my thesis is that oftentimes we are not completely in this sort of general situation where we know absolutely nothing. In this case, we say that. In this case, we say that under null hypothesis, we really know nothing about distributions except that we come from this class. So, we usually have some kind of prior experience with this, let's say, I don't know, channel model. So, we may not have a good model written down in closed form like Gaussian Heidegger noise, right? But we might have some prior samples from this distribution, right? From this distribution of waveforms, right? So, and that's kind of the idea of likelihood-free conference, as you will see. Read confirms, as you will see. Okay, so now I was phrasing all of this story from the point of view of communication, right? But this actually, this exact scenario appears in sciences. And so these IAD samples, right, they could come in science from simulations typically. So you say that under one hypothesis of the world, let's say global warming exists or something like this, then you have one simulation model. Then you have one simulation model. And if it doesn't, then you have another simulation model. And these are slow simulations which occupy hundreds of GPU years, sometimes in hours, CPU and so on. And so you can get this IED samples from the distribution. Now in communication, that's my application part later on. We have knowledge about this to hypothesis in the form of RF captures. You could be sitting before receiving, before the demodulation. Before receiving, before demodulating your signal, right, you could sit and listen to the background, right, and figure out roughly what is happening in this world, right? Okay, so that's my motivation, right? So let's now move on and I'll show you how to analyze this kind of problems theoretically. And again, my goal here is not to overwhelm you, but I want to show you to you that I will analyze only the detection problem, right? But we can all, you know, I hope to excite you because we can do the same thing for, I don't know, whatever the The same thing for, I don't know, whatever the problem you like. Like channel coding, please. I mean, you have IED, you know, IED sort of measurements of the channel model, right? How do you design modulation scheme for that? Like, you know, all sorts of cool stuff. It's not just detection. Okay. Right. So let me tell you about likelihood frequencies now. Alright, so here is the rigorous model. So, I mean, this field has two names, LFI and SBI. So I'm just going to use LFI because that's something that. Likelihood. That's something that okay. So, why is it called likelihood free? That's because, you know, in classical statistics, right, we say, okay, if you have a parametric model, right, P sub theta is indexed by parameter, then you just know the distribution of p theta, right, for everything. It's given to you, some kind of gamma function or Poisson or something. So the difference in likelihood-free inference is that you only have access to the black box, right? Which is like a big box, right? You can input theta into it, right? And it's peak. Input theta into it, right? And it speeds out a sample from theta. That's your only access, right? Again, for sciences, this is very natural. Maybe if you're a machine learning guy, this is a diffusion model. Okay, so right, so then our task in the hypothesis, in the likelihood trigger inferences, given IAD samples from some true theta star, we need to figure out theta star. To basically play this game. Somebody input theta, secret theta star, God gave us M samples. Theta star gave us m samples and then gave us a black box. Right now we need to figure out what was the theta star key. Okay, right, so the reason it's called likelihood free is because you don't have likelihood function. You cannot do maximum likelihood. Okay, and as I said, so there are many examples in sciences. Let me give you my favorite example. In fact, that's exactly how I discovered about this problem. So I play hockey with a guy who discovered Higgs boson, and one day he explained to me how they did it, and that's how I started thinking about it. Okay, so. Thinking about that. Okay, so what is heat boson? It's some elementary particle, right, which was conjectured to exist 30 years ago. So, and physicists, you know, built this Large Hadron Collider, took them 30 years to build it, right? And so what were they testing? Then they turned it on, right, and it produced MIAD samples, right, from these collisions of particle rays, right? And they were testing two models, right? One model is physics without the Higgs boson, and the other model is. Without the Higgs bottom, and the other model is physics with the Higgs bottom. So, now while they were, you know, while some people were digging the ground with Switzerland, right, the other people were simulating. So, they were simulating samples from hypothesis with no Higgs and from the hypothesis with Higgs. So, it's actually staggering how much time it took them to simulate this. So, I think I used to remember this. This is like 140 CPU years, which is just to compare GPT-3. Which is just to compare, GPT-3 was 300 GPU years. So it's like comparable effort, right? Even though we never heard about it, right? But they were actually burning a lot of electricity. Okay, so what did they do after? So now you have, right, so you have this, let's say, 10 years of simulations, right? So what did they do? They trained a booster decision tree classifier, right? So it's a cat dog game, right? They have to give, it's a high-dimensional sample, right, which describes a particle collision, rate, right? Collision rate, right, emanating from the chamber that they simulated. And then they would train a booster decision tree, which would, you know, when you give it a sample from no Higgs, it says no Higgs with high probability. When you give it Higgs, it says Higgs with high probability. And then, you know, so this boosted decision should be some kind of classifier, right? Region like this. So when they finally turned on the LHC, right, they collected M samples, right? And then they check, is the density of samples falling? Is the density of samples falling into this, is the fraction of samples falling into this region? Does it match the no Higgs or not? And it turns out not, right? So they could assign the p-value and declare that the model with no Higgs does not resemble the reality. So that's how they did it. Okay, so let's try to do some theory. As I said, so when I heard that they did this, I said, okay, we need to figure out some theory about this. You're also in favor of this. Okay, again, depending on the time, right, I also wanted to show the physicists because this data, they gave me this data, right? And the LHC was run in 2011 and 2012. They only published this Nobel Prize winning paper in 2012, right, because they needed to collect M sufficiently large. And so when I started this, I said, oh my god, these guys know nothing about this. Why are they doing 0-1 classification? They should do approximate Neyman Pearson, right? With Neyman Pearson, right? Because they're essentially trying to aggregate statistics, right? Which is 0, 1. So that was my motivation. I said, okay, I'm going to show them that I can do this, I can find Higgs boson in just 2011 data. I could do it one year before them. It turned out that I was completely naive. They tried, name and tried to approximate local likelihood. It's just this thing works better. And actually, we proved later it's optimal. But, anyways, so let me tell you something. Let's do Feeling. Let's do minimax setting, right? Okay, so you have to start somewhere. If you put no assumptions about your distributions, then you cannot do anything. So we have to put some, what I call meta-assumption. Let's agree that my distribution under both hypotheses, they come from some general class. It's something, it's not arbitrary probability measure. Let's say we put some constraints, right? And that's going to be a giant non-parametric class. us, right? Okay, and then the second constraint we need to put, right? We need to select the statistics under new hypothesis and under alternative hypothesis, right? It should be at least in principle testable, right? By somebody who knows px and py, right? So then we should put some lower bound on total variation separation, right? So that at least in principle, you know, it's testable. Okay? And so now simulator produces for us, so now we play this game, right? So we fix nothing. So now we play this game, right? So we fix no parametric class. Somebody selects a pair of distributions separated by epsilon, total variation, right? And then we do IAD, we generate IAD samples. Okay? Right. And so then the nature decides which of the hypothesis is true, right? And gives us samples of either Px or Py. Right? Okay, now statistician observes this triplet, right? Statistician has sample from null hypothesis, sample from alternative hypothesis. Hypothesis, sample from alternative hypothesis, the sample to be classified, right? He knows or she knows a non-parametric class and the separation epsilon, right? And now we want to detect which of this is true. Okay, so let's pause for a second. Maybe just if anybody has a question, and it's not, and I will ask you a question. Why is it not a cat dog classification question? What's the difference? Again, if you if you heard me, give a talk, unless, but uh So, what's the difference here? Right, so far, right, you could say X is Mu hypothesis cats, right, Y hypothesis dogs, right? You have some training data, lots of cats, lots of dogs, right? And then you have a test sample, right? And then you want to classify it, right? So, what's the difference here? Why am I not talking about machine learning? Oh, nice. Anybody or not going to proceed until you tell me what's the difference here? Because otherwise you're not listening really, right? So I need to. Epsilon? Epsilon? Okay. Right. Okay. That's good. Right. And the cat dog, in principle, but we believe that they are separated, right? Because we see that. Right, right. Because we see that we can test them. Okay, that's good. But what else? Not a lot of data? Not a lot of data? Okay, so you say maybe and is different. Okay, let's uh I would say it's not the case, actually. We have more cat dog pictures, I would say. You also know cat. Okay, you know p, okay, that's good. That's that's another difference. That's good. That's another difference. Again, machine learning is not unpower, right? You say cat and dog. Do we know more about cats and dogs than those samples? So, like, we're comparing against other animals? So you say it's multi-class sometimes. Well, I don't know. I'm assuming that P is like animals or something. Right, or images more, like, right? It's like PO. Maybe some low-dimensional manifold distribution, right? Embedded in the pixel space. Right. Okay. Right. Okay. These are all good things, right? Let's, but the simulator producing data is different here. Whereas in the cat dog, you somehow have labeled data where somebody has looked at the images and said this is cat, this is dog. I see. So you say there is error as well there. Like human who classifies them has some small error. Okay, all that. Yeah, all excellent points. I actually haven't thought of half of them. I just wanted you to say the whole thing. I just wanted to tell you that you look at this M, right? Look at this M. So you see, when you do cat-dog classification, I give you one image and I ask you to do that, right? This is like trying to do BPSK demodulation from one sample, right? We in demodulation, we try to aggregate, right? We try to pull a lot data, right, accumulate stuff, right? Here, you're interested in how to aggregate, right? I'm giving you now M images, right? And you know all of them are either cats or dogs, right? So because you have a lot more, guys, you can test this with You can test this in a lot better, with a lot smaller probability of error, right? And also, another thing that you didn't say, that cat-dog, TV here is one, essentially. There, you can, right, the humans classify a cat versus dog from one image with almost no error. You're right, there's a little bit of error, right? In fact, I use in my class Bobox data set that he produced 1000 cats, 1000 dogs. And this is amazing when you run a classifier and it outputs conflicting images, those cats. Conflicting images, those cats look like bugs. It's very hard to tell. So it isn't. Anyways, so that's the point, right? So this is a game where we have m images all come from the same class, right? So you want to do that. All right, I'm going very slow, right? So what is the question that we will address? So we want to ask the following question. Can we actually do this? Can we do better? So one way to do this, to solve this problem, is to simulate so many samples that you learned at this point. To simulate so many samples that you learn the distributions, right? And then you just do Neyman Pearson for them. So the question is: can you do better? Can you avoid simulating the distributions? And the second interesting question, is there a trade-off between the number of samples we collect from the experiment and the number of simulations? And again, for us, M is like SNR. So basically, if you simulate more, can you detect lower SNR? That's the idea. The answer would be yes. Be yes, right. Okay, so let's formulate we are information theorists, let's do some kind of fundamental region stuff, right? So, right, so we are interested here in a set of pairs, m is the number of experimental samples, right, and n is the number of simulation samples, right? So, we are interested in this two-dimensional region such that there exists a test, you know, solving this problem, right? Okay, so we're trying to figure out a a region, right? Trying to figure out a region on this planet. Okay, so let me just mention a few words. As I said, I promised to tell you that this is, you know, this is a famous topic. It should be a famous topic in our community, but it's not. I mean, I like to put some citation numbers. Citation numbers to all of these papers are below 10. So let me tell you this, right? So this is something that completely was not observed. But Jacob Zeef actually invented this problem. You know, he was already back in, whatever, 88, right, worried about the fact. 88, right, worried about the fact that we assume completely null hypothesis, invented this term, you know, empirically observed statistics. So, yeah, we are not, so they looked at it from the old school way of error exponents, right? This is not the right way to look at high-dimensional problems, as we learned from Yuri Inkster in the 80s, right? So, we are doing it the right way. So, essentially, you're interested in scaling of sample complexity as a function of separation in T. That's the right point of view. I mean, again, I don't have, it's like we believe. Again, I don't have, it's like we believe that linear scaling of log m, the number of bits, right, is linear in the block length, is the only right way to look at the problem. Similarly, for statisticians, that's the only right way. So we have to just agree that it's. Okay, so now I'm in the following situation. So I guess, is that the right watch? Whenever we're off. Yes, well, that's less problematic. Right. Okay, so I'll okay, I guess I'll tell you. I'll uh okay, I guess I'll tell you the main result, and I'll tell you, but uh the thing is it's very hard to appreciate the main result without first showing you how this problem relates to other uh right, and then I'll I think I'll skip the rest and then we'll just go to the practical stuff. Okay, so let me tell you about some related problems with statistics, right? Okay, so first problem is goodness, so-called goodness of feed testing. Uh, this is the problem that Yuri Inkster solved in the 80s and Donovan actually called this fit. Donovan actually called this biggest breakthrough of 90 of like 20th century stats. One of the two. I forgot the other two. So the other is the other one. So what's the goodness of fit testing? So under the null hypothesis, you say, I know exactly what the distribution is. Say it's IAD Gaussian with zero, something like this, right? And under alternative, you say, I don't know what the, you know, the alternative hypothesis says it's some distribution in my, some smooth distribution, which is Tv epsilon away from the. Epsilon away from the IAT0 mu toups, right? So, and the sample complexity, the minimum number of samples to test between these two hypotheses, right? It's called N-Golf, in my problem, and goodness of. Okay, so, right, in this case, the Nuke hypothesis is completely specified. Now, the two-sample testing, that's a famous problem. If you've seen Jayadev Acari or Alona Relinski give talks, that's their stuff. So, in the two-sample testing, you have a nuke hypothesis, which says, you know, you have two samples, X and Y, right? You have two samples, X and Y, right? And your job is to compare whether they come from the same distribution or from distributions which are different. And again, in what sense are they different? After lots of false starts, the community agreed that the right way to measure is to fix some lower bound on T V and study the dependence of the samples, the number of samples you need to, you know, how big these X and Y samples are, to test between these two hypotheses. So you see that. So you see that, right? So the third important problem is that of estimation. So in this case, I'm not asking you to say anything about unknown distribution px or py, only that they're the same or different. And so estimation question is the canonical stuff, right, that we all know and love. Here you say, well, I give you NIED samples from Px, and you know Px belongs to some non-parametric class. How do you estimate how many samples you need? How many samples do you need to bring to the relation less than epsilon? So you can see that all these quantities are functions of epsilon. And epsilon always plays, is measured in Td, right? It always has the same meaning. Okay, so our question again, just to emphasize, is neither of this, right? Just like in two-sample testing, we have two distributions, px and py, right? And we say it's, you know, better and epsilon separated, right? But our two hypotheses are. Right, but our two hypotheses are that there is a third sample that is either equal to, so you know, x or y. And that's why our answer is a two-dimensional region, right, depending on how many samples we have from the nature and how many samples we have from the spotlights. But it's again a region which is a function of absorption. Okay, so again, if you've never seen this, I understand this is hard to part, but just to tell you that, you know, there's a But just to tell you that there's this hierarchy, and of course, the sample complexity increases here, right? All these problems become progressively harder. So that's why. Okay, so in our work, we considered many classes. Let me not talk about any of them except this pH. So it's a class of all smooth densities, which are beta times differentiable, right, on a d-dimensional key. So let's just focus on that. That's a nice little plus. Let's just focus on that. That's a nice little plot, right? Okay. So that's the only a priori assumption I'm putting, right? Okay, so what is known about this? So, first of all, the classical statisticians, right, they estimate, they studied this class to death, right? This is the famous, I don't remember, I think in the Russian literature this is done by Berdim Kankasminsky, and typically credit, well, in the American, in the Western literature, I forgot. I forgot this. But anyway, so they estimated this, right? So the exact estimation rate is given by n to the minus bet over 2 beta positive. You see, it becomes worse with dimension and better with the number of derivatives that you have. Okay, so the goodness of it, that's the famous result of Ingster, right? That somehow, if you're only interested in testing whether the distribution is the same as your conjecture, do not, right? Yeah, then you can do slightly better. This is the shocking point. This is the shocking point. Somehow there exists a whole range of sample sizes where you cannot estimate, but you know you are wrong. So that's the funny thing. Okay, then we will express everything. So that's the old school statisticians expressed everything in right. We are following computer scientists now like to talk about some complexity. So it's just solving this equals epsilon. Okay, so these are the numbers. Okay, so what's our result? This is with Patrick Gerber, who's a graduating student. With Patrick Gerber, who's a graduating student, he's not interested in the job, so you know, don't worry, Humbert. He will ask you about this. I mean, he went to, oh, this is reported. Can you cut this out? You're going to hate me for this. Okay, anyways, I mean, that's a fact. I didn't love it. Yes, it's okay. So, what did we prove? So, we proved that, right? So, first of all, remember, I asked, the biggest question is: can you avoid density estimation, right? Can you do better? Estimation rate. Can you do better than density estimation? This encodes that the answer is yes. I'll show you in the next slide now. But it also says that, right, so there's basically you need some number, minimum number of samples from the nature, right, of course, because TV is epsilon, you cannot test less than one over epsilon square samples, right? You have to get that, right? On the other hand, right, I mean, there is a lower bound on the number of simulations, right? So you have to simulate sufficiently, right? Whatever you do, right? And then there is some trade-off, right? And then there is some trade-off, right? If you simulate more, you can test with less, right? So that's the fun trade-off. Okay, right. So this is how this region looks if you put logarithmic scale, you know, on m and n. Again, just to remind you, I mean, n golf and ns are these numbers, right? There's functions of epsilon. Okay, so right, so the fascinating part in this, right, so the fascinating part in this is that if you So, the fascinating part in this is that if you plot this trade-off, then you get this point here, right? So, let's think about physicists, right? For physicists, collecting every sample meant they will probably be scooped by the other experiment. So, they really, both, there are two competing experiments, like CMS and Adamas, right? So, who were trying to find Higgs plastic. And so, they were really interested in minimizing M. So, if you really are interested in totally minimizing M, then it turns out from our thing, right, that because of this finite relationship. Right, that because of this finite relationship, which actually we discovered as a byproduct of this work, right? This was not known in classical stats, that the minimum number of simulation samples is exactly equal to density estimation rate. So it's a very negative result, right? It tells you, and this is why Kyle Kranmer, who was the lead on the design of this test for Higgs, he tweeted immediately after another paper: oh, these guys are wrong, we're going to show them. I think I convinced you. Yeah, this was maybe. I mean, I don't know. This tweet in Print. I don't know, at least we can continue this argument. So, right, so this shows you, right, if you really want to test with minimal number, you have to estimate the densities, right? And essentially run, you know, the Neyman Pearson test. It's not exactly Neyman Pearson, you need to run Cougar's Robust test, but right. But on the other hand, if you have more samples from LHC, right, then you can actually avoid estimating. You can get away with small. Okay, so okay guys, I'm sorry, I'm a little over time, so I don't have I'm a little over time, so I don't have time to explain to you. But basically, okay, if you're interested, I'm very happy to talk about this. I love this problem, right? But all these points have some natural, all the corner points have some natural meanings. And the region is only this nice for regular classes. If you go to something like discrete distributions, then there's more kinks here. Again, with lots of interesting phases and so on. Okay. If there are no questions, If there are no questions here, I think. Yes, Caleb, whatever. I was curious, there's an interpretation of this within the framework comparison experiments, like balance definitions. We can say in one regime, it's like getting a single sampler trying to estimate another product structure for some distribution plan. So you say, right, you say you could think of M sample as IAD sample, right? And then it becomes a classification question, essentially, right? I mean, I'm just thinking like I mean, I was just thinking, like, you're saying you have this special product structure, and it's as different in cats and dogs as you did with M. Maybe in some regimes that doesn't matter, and it just looks like a single sample from some. Yeah, that's what I'm saying. You could group the samples, right? But in another regime, this product structure dominates, and so now it looks like this other classical product. Yes. So this one is dominating which one here. I thought about this. Okay, so I think we couldn't ever extract anything out of this point of view, unfortunately. So somehow product structures matter, right? Because all of this is. Structures matter, right? Because all of this is asymptotic as m n go to infinity, right? This is all this characterization up to global factors, right? And I think like these global factors, right? I mean, it's like you have higher dimensional problem, right, with each n, right? So I think it's yeah, but that's, I mean, this point of view, I know, yeah, that's an interesting. Yeah. Quick question. Yes. So in many cases, you may have much fewer samples from one of the, for training, from one of the classes. Okay. Excellent. One of the classes. Okay, excellent. This is open. Yes. In fact, when I gave this talk at Fox, the computer scientists, I will remember the names, but they're very interested in three sample tests of causal influence. Sorry, for three samples, right? Because I have M and N are different, right? But you could say X and Y have like N1, N2. That's the question, right? So for some kind of, you can somehow interpret causal inference in this fashion. And then you will have like M is equal to N1. have like m is equal to m1. I forgot like the idea, but for them basically the more interesting case is to merge you know the other two numbers, right? So this is open actually. Yeah that's a that's a cool question. We didn't try to extend this. Yes? So I missed the part about the physics experiments, but I was wondering if in their case wouldn't have much more structure like in the distributions themselves, where is the stories for like general For, like, general, just some general smoothness assumptions. Excellent, yes, yes. So, in fact, I will tell you, right? So, maybe let me rephrase it a little bit. You could ask this question, where is machine learning? Right? In some sense, right? Because, I mean, right, so I mean, okay, my belief is that the right way to look, right, so I mean, this is for smooth densities, right? The right way to look at this is if there is a good representation of the beta, right? Like, for example, cats and dogs are not smooth densities, right? Smooth densities are no-dimensional manifolds. That's all density is on low-dimensional manifold, probably, right? So, how do you extract this? You could do PCA, for example, right, first, right? And then I think the model makes sense. If you first do PCA, and then it's right, but directly in the observation space, smooth densities is not the right model. And so then this is the machine learning part. How do you, it's actually what I'm going to talk in a little bit later. So I think you need to do some feature learning and then apply this. So, okay, so let me tell you, right. So, the coolest question I want to mention about this proof is that. Want to mention about this proof is that what is the test statistic? Okay, so the test statistic is very simple. You take the cube, you partition it into little discrete sub-cubes, right? Now the problem becomes discrete with the right number of beans, right? And then you do something hilarious. So you compute the empirical distribution, right? How many samples fall in each of these little beans? And then you just compute L2 difference. And then you just compute L two distance, difference of L two distance. If it's perhaps you say, Hey, you, the data from L H C, are you closer to empirical data from simulations with Higgs or without Higgs? Okay, so we thought of the statistic following the Inkster's L2 comparison idea. So Inkster basically said that this is the right statistical threshold for Google's of feed. And the surprise that Donka was quoting is because, anyways, that's a separate. That's a separate. I mean, it's known that this is a bad metric for learning densities. So you cannot do it like that. But somehow here, okay, there's a fascinating problem thing. We can prove that neither of these estimators, neither of these quantities estimates the true distance, L2 distance. So the variances are off, but the variance terms, which are the dominant, actually cancel when you subtract that. So that's the magical part right here. The magical part right here, and it still works. And I should say also, there was this work in 2011 by Ben Kelly and Aaron Wagner. They also used these statistics, as we discovered later. So they did it for discrete distributions only. So, you know, the fact that this test works is, in some sense, you know, it was anticipated by them. What's surprising is they didn't know about Instagram at all. So they just invented it from scratch. But in any case, so all points are achieved by this test. So, all points are achieved by this test. So, let's try to apply this in practice. So, how do we write? And where is machine learning? Okay, so now you see, if I ask you to do high-dimensional experiment, of course you don't want to discretize everything, right? You would get too many bits, right? Okay, so what we did is we said, let's do feature map, right? I mean, discretization is one feature map you could use, right? But let's have some arbitrary feature map, right? And then just compute L2 distance, right? L2 distance, right, between featurizes. This is called MMD, if you will, this term, but it doesn't matter. Just compute some features, right? Phi of x, and you just take empirical difference between L2, right, in a feature space. Okay, so, right, and then once you have this concept, right, we're going to apply exactly the same test, you know, which we know works for us, right? So that's the idea. So we proved that this actually has with a bunch of undergrad students. I usually put a plug for them, but they already found a good grad student. The plug file, but they already found good grad schools. I don't need to mention this because I do actually one of them was probably going to Stanford, so I first. Yeah, so, anyways, superstar, guys. Right, so then where is machine learning here, right? I mean, I kind of, you know, I think all of us believe that new stuff should be done by some help of machines, right? Where can machines help us? This is us, right? Now, machines enter when we try to optimize feature map, right? You just train the gradient descent in the kernel space, right? Ready in the sense in the kernel space, right? You just optimize the features to maximize basically the p-value. So that's how we did it. Okay, right. So we tried to run this experiment on a CIFAR 10, right? So you know the diffusion models, right? They generate nice images. So we applied this to check how well those images are, right? How good those images are. So that's, yes, let me skip. Yes, let me skip this. That's not so fun. Basically, it works very well. So, it can, you know, with like 300 images, it can tell, oh, this was done by diffusion map or tally, right? Not by, no, this is not the actual images, right? That's one example you could use this stuff for. Okay, then we went to physics part, and this is where the sort of the depressing thing happened, right? Okay, again, physicists are not interested in sample complexity, right? They're interested in achieving what they call five sigma, right, which corresponds. 5 sigma, which corresponds to just a very low p-value, then you can plainly discover it. So you want to have 5 sigma, and this is the training size, the number of simulations. And so we basically wanted to either reduce the number of simulations needed or the number of samples, right? And after trying every trick we could, we ended up using exactly the same number of simulations and the samples that they used. So this convinced me that, okay, the physicists, and if you've seen in the Nobel Prize citation, this blood, right, this is where. Citation, this plot, right? This is where kind of the spike, right, the Higgs is like has the right mass and so on. So we can also get the same stuff. So physicists were right. And later we proved theoretically in the last call, actually this 0, 1 classification accuracy test is actually minimum software as well. And in fact, it's better than the L2 test, because it also gives you high probability regions. Anyway, so this is like one star, but I wanted the reader to tell you about communication part. So and talk a little bit about. So, and talk a little bit about interference rejection. Okay, so right. Okay, so now here, right, so far we were saying the hypothesis were some images or something. Now, I want you to shift gears, right, and put your communication hats on, right? So, we are now going to play the game which is dear to all of our hearts, right? We have a received signal, it's equal to the signal of interest, maybe PTSK, right? Plus some noise and interference, right? And interference, right? So, B is going to be the background, right? For me, okay, and how does it look like? I mean, okay, if background does not overlap in frequency, you can just filter it out, right? So, the interesting situation is when you have signal of interest and background, right, overlap in frequency. So, then you get this, and then there is a slightly visible presence of the signal of interest. The question is, how to detect it? Because if this thing is Gaussian, we know from the first. Thing is Gaussian, we know from the first slide, right, what to do. We just do the matched filter. But what if this interference is some complicated stuff, like oil DM signal, right? The 5G or ARTS SpaceX, right, whatever, the satellite feed is interfering with the 5G. I mean, I think they overlap in some C-band somewhere. So the signals may overlap. So initially we thought, okay, well, FDM interference looks marginally Gaussian, right? It's the sum of a bunch of carriers. If you look at it, it looks very Gaussian. That you know, it's very Gaussian, so maybe you cannot beat much filter. So, let's right, so that's the goal. So, how do we exploit time frequency structure in the signals, which is very hard to represent in formulas? Okay, so right, so the idea was to train a machine learning-based signal or in the old school, by the old school terminology, source separation. So, let me explain this idea. Again, we're still doing hypothesis testing, but we're going to do it in a fun way, right? In a slightly different way. Way right in a slightly different way, that's why it's a practical. I have no theorems at this point, right? So, this is just to excite those of you who like to do stuff. Okay, so we're going to train the following box, which takes Y, and that's called signal separator or source separator, right? And audio, right? Multiple people speak, you want to split their voices, right, even though they overlap in time and frequency. So, right, so my signal separator is going to output S hat B hat, right? Once we have estimate of S hat, then we apply Once we have an estimate of S hat, then we apply just vanilla match filter detector. So this is going to be my likelihood three hypothesis tester. It's going to consist of some block which tries to map everything to Gaussian noise model and then apply mesh to technology. So that's the trick. Okay. Oh yeah, so here, okay, so this is a work as part of this collaboration with Air Force. And you know, we started this in 2019 with Greg Renel. 2019 with Greg Renell. And you know, we thought back then I was very naive, I didn't know much about machine learning, right? So I thought, okay, there's probably this pool of machine learning people who can't wait for me to come and tell them about the problem, right? And then just drop everything they're doing and train some DNNs. It didn't happen. It turned out that it took us five years to make this thing work. It works now perfectly, as we show you. And what are the problems? Why does it take five years to go from here to here? I'm sure most of you by now know. Here to here. I'm sure most of you by now know, but for me it was a revelation, right? You need data, people don't want to give you data, right? You had to use Lincoln Labs. You know, they measured some stuff on Gillette Stadium to get high-quality data. And, you know, there's lots of problems. People don't want to share their GPUs and so on. Okay, so I'm going to try to tell you. So, yeah, JF tells me I only have less than 10 minutes left. So I wanted to tell you about two architectures. So let me go quickly. So the one kind of architecture for the signal separate. The one kind of architecture for the signal separator is what we call supervised end-to-end, and the other one is Bayesian. So, I wanted to talk about the Bayesian for Cindy specifically because I know she loves alpha posteriors, and actually, we found that this is crucial in this project. Okay, so let's start with the supervised. So, what are we going to do? It's a very simple thing, conceptually, right? So, we try to extract from a sum of S plus B, right? We're trying to extract S. How are we going to do it? We're going to generate quadrillion of the synthetic. Quadrillion of the synthetic mixtures, right? S plus B, right? We're going to feed it to DNN and say, hey, dear DNN, please take this Y and give me S, right? And you keep punishing it, right, and we're giving it sugar until it tries to reconstruct this thing. So that's the idea, right? So this is currently the method which gives you the best performance, right? And there are some problems with it. Let's not go into it. Okay, so what the exact, okay, again, after five years of experience. Okay, again, after five years of experimentation, the one we arrived at is what we call WaveNet. We use some kind of resonance structure. I wanted to just, it's not important, I mean, I can tell you later, but just the funny numbers is that we use something like four million parameters, right? And it's trained in about a week on a GPU. We use eight GPUs. Okay, so how long does it do? So this is the funny part, right? Again, we're trying to detect QPSK, right, in the presence of OFDM interference. Again, OFFM. A OFDM interference. Again, OFDM margin looks very, very goblin, right? So the matched filter performance is the. Okay, sorry, this is MSE. Let me skip MSE. Let's just go to the beat error rate curve. So this is the matched filter. So the matched filter directly, which approximates OFDM as and this is signal to interference ratio, right? So minus ten, this means uh signal is ten times weaker than interference, right? Minus ten is one hundred times weaker and so on. Is 100 times weaker and so on. So here's the exciting part, right? So if you do mesh filter, you don't get any performance. If you add linear minimum mean square error, you get a little bit better. Then this is our stuff from last year, and this is like the wave net. So the wave net really crushes those 5G signals. Okay, so we got these results. And at this point in time, we decided that we need to ask others, can you beat us? So we had this, you know. Right, so we had this, uh, you know, we had this call for RF challenge on ICASP, you know, and many teams joined. I think something like sixty people were in this for the channel at some point. So, you know, when you run a challenge, that's something I can tell you guys. It's really nerve-wracking. Why? Because when you publish a challenge, there are two ways in which it can go wrong. The first is that nobody shows up. We worked this hard, we collected the data, we trained our baseline, we packaged the code, everything is ready, you can play with it. The code, everything is ready. You can play with them, right? And then nobody shows up. This happened to us three years ago. We were not ready yet, right? So, this time, many people showed up. Then, the second failure mode is that nobody can beat your baseline because you are MIT, you worked on this for five years, people become upset, depressed, and leave, right? So, for us, it turned out it was, I mean, it was like we hit the sweet spot, the sweet spot here. So, the black line is our performance of WaveNet, the result of five years of optimization. These are the top seven or five teams, right? Or five teams, right? So, you know, most of them in the Discord were very discouraged. So the challenge rant was four months, I think. So they were very, and most of them left because they couldn't beat our performance, even though we gave the call, right? You can just tweak the parameters, for example. But then there's one team by Miran Deba from TII. So they actually beat us quite a bit. And that's interesting. So I just wanted to say how they beat us. That's also important. They, first of all, quadrupled the number of parameters. And they also threw a lot more GPU power. Through a lot more GPU power, right? About double the size. So, why is it exciting? Not because I love these numbers, right? I don't care about this. What it tells us is that this is a problem. In machine learning, there are two types of problems. Problems where you throw compute and nothing happens, right? And problems where you throw compute and data, right? And things improve, right? So this is the latter kind, right? So that's why it's a safe. Okay, so so right, and so this is going to happen. Okay, I wanted to tell you about the last, what, two minutes I have? What, two minutes I have? Like whatever. Yeah, so okay, let's. So this is right, so this was a very kind of brutal way to do interference rejection, right? Why? Because if you change signal of interest, you have to completely retrain. So if you train this for QPS gate, right, and then you change the QAM, you have to completely start from scratch. You again have to spend eight GPU days to train this thing, right? So it's not practical. And again, for the target application with Air Force, it's infeasible to retrain so quickly. Infeasible to retrain so quickly. Okay, so then we thought about: okay, let's do Bayesian, right? Bayesian approach. So, how can you apply Bayesian? So, you can collect many samples of the background interference, right? OF DM, for example, right? Then you can train a diffusion model, right, which learns the distribution. So, you have a distribution, right? And then you can use the map decoder, right? You're trying to detect from s plus b, right? Just do. S plus B, right? Just do the Bayesian posterior computation, right? Okay, so let's look how it works. So, first of all, yeah, diffusion models generate beautiful images of stuff. We like to insert them in talks nowadays, right? So, it's great. The question is, can they generate RF waveforms? Okay, so we played with this. Yes, they can. Because RF waveforms are square care, right? They're very discrete in nature. So, somewhere deep in their guts, they're discrete. But it's not clear if these images are discrete in their guts. It's not clear if these images are discrete in their regards. So we trained some, you know, a nice diffusion model. And indeed, you know, if you look in constellation space, you know, we did it for OFDM, for QPSK, you know, the constellations are aligned. So this is, again, this is a neuron ad generating a QPSK signal for it, right? Which it thinks. And then if you apply some proper rotation and scaling, you can see that, yeah, it does look like the result of passing through the quasi-static fading channel. Okay, so, right, so we have this diffusion net, so how do we work with it? So, how do we work with that? So, right, so we want to do a map estimator, right? So, we just want to minimize overall signals, right, the posterior. Now, unfortunately, of course, even though we now have access to this guy, right, let's pretend we know the Bayesian prior, right? It's still very hard to optimize this, right? So, how did we do it? Well, we did gradient descent, right? Instead of doing mentorly hard minimization, we did gradient descent. We did gradient descent. Okay, so for gradient descent, you need to have grad of log density, right? Again, signal S is discrete, so this makes no sense. You have to regularize, right? So that's the trick here, first trick. You add noise, both the S and B, right? And then grand log of the density, something known as score. And funny enough, that's exactly this byproduct of training a diffusion model, that it gives you the score function. So that's the thing, right? So you can execute this gradient descent very well, and again. Right, very well, and again, so the thing which I want to mention: I mean, this idea was used many times in many different places, right, that you can do something like this. In fact, Song, the guy who many of us interviewed, right, when he was on the job market, the inventor of one of the famous diffusion models, he in the original paper already says this is the application. But I will show you that actually to make it work, you have to do lots of tricks, right? At work, you have to do lots of tricks, right? So, and one of the main tricks we thought was important to highlight is that you actually, instead of, you know, there's this trick that the CND knows very well, but others may not know that, you know, Bayesian inference works very well, right, on paper, but when you try to do it in practice, somehow this Bayesian rule doesn't work that well. So, and then the trick is to insert alpha here, right? So, instead of like, you know, minimizing, you know, the product of prior and posterior, sorry, and conditional, you know, p1. And conditional, you know, py given s, you raise py given s to the alpha spaw. And somehow this gives a lot of robustness, right? So CND has lots of results on this. So we actually found that this is a cool trick, and this is a crucial trick to make this thing work. Okay, anyways, and in pictures, I mean, why something like this works? I mean, why do you need to add noise? It's because if you don't add noise, then your landscape for optimization looks like this. So if you're starting somewhere here, you're very flat. But once you add noise, your landscape But once you add noise, your landscape starts to look like this, and the gradient descent is worked better. And the second trick we use is that we alternate with random noise, random amount of noise added. Okay, so anyway, so this thing kind of works. Again, I don't have to spend time on showing you, but this slide, this is from NURIPS, I guess. Did I show it somewhere? Yeah, this is this is from NURIX last year. So uh right, so so so basically we compared against other methods we did. Against other methods, we did similar things in typically in signal separation, sorry, in audio separation. And yeah, we get now, just like every machine learning talk, right? We are bit the state of the art at the very end. Right, so alpha posterior worked. Okay, so last slide. What did I tell you? So again, I tried to make you excited about this whole idea. You know, I started this talk from saying, I was always upset that somehow when we teach this class, it's a detection estimation. How, when we teach these classes, detection estimation, channel coding, right, we have to assume exactly non-channel model, right? And the reason is because we really don't know what to say first. What is an arbitrary channel model, right? What is it? Like, you can't say anything, right? But now we can play in this, you know, with this idea, right, that, okay, you want to assume some meta property of the channel model, say smooth density, right? Smooth conditional distribution. And then you say, then I have some extra stuff coming. So, how do I use that extra stuff to improve my, right, to To improve my, right, to generate my channel calls, I don't know, to improve channel detection and blah, blah, blah. Yeah, and so there's lots of open problems, and I hope, I don't know, I hope I make you excited about this. All right, thank you guys. That's it.