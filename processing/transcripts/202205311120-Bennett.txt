So my slides are going to show you mainly examples of the first one, which is complexity of the organization, two complexity of the care of the patient. We have large teams. There are financial drives that cause us to have odd data and also regulations. And not only regulations, but I should have put here, but the health of the patient. We can only intervene so much safely, and we need to create interventions that are tested along a pipeline. Tested along a pipeline, not a pipeline, a series of steps to make sure they're safe. The reputation drivers are driving us crazy, the need to be on the U.S. News and World Report and that kind of thing, which is a real effect. The organizations I belong to try to minimize that, and most do, but it still has an effect there. And then competition also comes up within the organization, even and then outside the organization. So when you want to implement something, this is a Want to implement something? This is a diagram by Sumitra Sengupta about our precision medicine pipeline. So now you have a new intervention you want to use. So this happens to be precision medicine, but it could be like the ICU or something. And I want to do something. Like I've got to figure out where I'm going to put this in here and everything else it's going to affect. So it's not a simple thing that I just want to drop in this little intervention and I'm just going to connect and I'm going to hit the database over here. I'm going to send the person a reminder. I'm going to feed some information from some monitors like you have to. From some monitors, like you have to fit into a very complex architecture. And often you have to fit into a complex governance. And this one is not shown so bad. This is a sample of EHR governance. But the point is, there's a lot of people all around. And then that's the governance. This is kind of the implementation and maintenance. So now you want to drop a system in, you have to figure out how is this thing going to get maintained? You know, the time to build a production system is six times the time to prototype. Is six times the time to prototype, and that's probably a vast underestimate of people who have actually put these things into production. And you need to interface with a lot of these teams here, so it's a big deal when you want to put intervention into practice. And the last slide I showed is Lena's slide, a challenge in the healthcare system. So this is a slide from many years ago now. So I guess this has probably shifted. But what I'll say is patient care, so interaction of the clinician. Actually, these were medical residents and fellows, right? Residents and fellows, right? Fellows also in internal medicine. And it just so happened that the warmer colors are seeing a patient like orange. And the cooler colors are doing things like using the computer, writing things on paper. The green is now more blue than it was 10 years ago when you did this probably. And then also other kinds of communicating. And then you look at the diagram and you're struck by it. It's like it's a very cool diagram and not many warm colors. And yellow is not good. That's moving or waiting. So take out the yellow and then you don't have much warm colour. And then you don't have much warm colors there, it's orange. And so the doctors who came in there to be doctors are actually, it's an information management task. And so much of your time is on the computer and communicating that you put in an intervention and it's driving them crazy because they already feel overwhelmed with all the stuff they're doing on that. And so that's why you have to. And so the managers, these people who are watching and are the gatekeepers for where your intervention goes in, you feel it's For whether your intervention goes in, you feel it's unfair because they're saying, No, we can't do this thing right now. But they're bombarded with 100 requests of what we can do to make care better, supposedly, or more effective, or a better reputation, or whatever. And they have to control how much that influences the caregivers' lives. And so it's a big, complex environment. It's hard to make changes in it. That's the challenge. But we do make changes. And, you know, Lena, you put your thing on the outpatient, Noemia on the output. On the outpatient, Noemi on the inpatient side, Kenrick on, you know, working on the concern stuff. So we do get there. Okay, tell. All right, hands. Is it yours or Tell's? Tell is going to stop. All right, there you go. There you go. Yeah, you by the way. We can't hear you, by the way. Alright. That is still the wrong game, hold on. That's but that is you. We do hear you. There we go. Look good to you all. Yep, very good. Alright, super. Yeah, thanks very much for the opportunity to speak again. I'm going to actually cover some areas that are a little different than George, so that's great. I think mostly this will fall under the umbrella of opportunity slash. Opportunities/slash challenges that developers or builders of interventions might run into. And also, you know, I'll start with a framework that is really relevant to most clinician scientists who are in the sort of clinical and translational science space. And it resonates with me, at least, in the informatics and data science tool development space. And I'll see if it resonates with you all as well. Basically, this is the training. Basically, this is the translational spectrum as defined by Zagudi and company years ago. And basically, the whole branch of the NIH NCATS is dedicated to accelerating interventions along this from left to right so that they get to people. And so, you know, the sort of one way to express tool development in this context would be that you're over here doing T0 basic assignments. Over here doing T0 basic science when you are developing a new model or perhaps bringing a model from another domain into a new one, you know, sort of innovation in an application. In Colorado, we have a great partnership with Colorado State, which has one of the best veterinary schools in the world. So we have T0.5 natural animal models. That's not always present on this translational spectra. Translation to humans for the first time, or T1, might. First time, or T1, might be that that tool gets deployed in a live electronic health record silo, meaning that it is not serviced to clinicians, but it runs in the background and you can see the output. Another step after that might be that you actually translate the patients T2 in the context of a single-center trial. And this could be a randomized trial or observational, depending on the type of intervention. And then ultimately, the classic T3 is a multi-center trial. And I do think it's And I do think it's, I do, as a clinical scientist, highly value the computational tools that have been brought to randomized trials, and often these are the sort of pragmatic or learning health system type trials that are run out of Pittsburgh and Vanderbilt and other places. Not everything needs to have a trial, but I think it is the highest quality of evidence for a reason. And harking back to the causality discussion just a little bit ago, and I think this may have. And I think this may have been what Dave was talking about with saying that there are people who really care about that. There are a lot of people in the clinician science and translational science world who really care about the quality of evidence that comes from trials. And so if there's any way to sort of conduct your research with that as an endpoint, I would encourage you to go a little bit. Anyway, switching gears. One of the other challenges in these next several slides we're going to other challenges in these next several slides are going to be challenges in sort of the data available to train or validate or otherwise develop into your model. And I think one difference between the real electronic health record data and the data that's typically available, especially publicly, is that they don't look like each other. And so most publicly available data in a common data model or CDM doesn't look like a real-time EHR data. Look like real-time EHR data, and you know, I and others at Colorado are as guilty of this as any others. This was a first-day COVID severity prediction model that we published last year using the NCAAS-like resource that Noemi mentioned earlier this morning. And this is a model because of the constraints of the data available in that resource that has to sum over that. That it has to sum over that first day. And honestly, at the end of the first day, the punishments usually know what's going on. And so the impact of this model, if it ever were implemented, would not be as high as if it was a LURAN on real HR data. Another related situation where BARD of the Biomedical Advanced Research Defense Group Association might not have that exactly right, but anyway, another brand. That's exactly right. Anyway, another branch of the U.S. government that funds things, and they're very sort of radiation, you know, infection emergency kind of oriented. They want to respond to these emergencies and deploy tools quickly. They had a pediatric COVID-19 data challenge built on that same N3C data, and I was part of the evaluation team. And the winning models, just like the Netflix prize, are really unlikely to ever be. Are really unlikely to ever be implemented. They were trained on CDM data. They were trained on data that isn't consistently available in real time, like diagnosis codes from the ends of encounters. They really look like they were identifying chronically ill kids in a way that clinicians already know who's chronically ill. And they have huge numbers of inputs. I mentioned yesterday, that's a data engineering barrier. So, sort of digging into So sort of digging into challenges related to data availability, oftentimes the good stuff in EHRs is less accessible right now. And I see this as a really open area for advancement for both model training and implementation. And so one way this manifests is that there are EHR instance-specific, meaning non-standardized data structures at each institution or health system where really useful information is buried. Very. In the EPIC vendor EHR, these are called flow sheets, and I show a really simple example of really important information, you know, vital signs and things like that, that are in flow sheets and need to ultimately get mapped at the local site out into a structure that can be shared. And that will only be done for a minority of information influences. So, one way that this became really clear to us is when we built the Became really clear to us is when we built the model that I mentioned yesterday. We built it on really important variables that are not reliably present in most CDM data, specifically cardiovascular medication, dose, and time. And so these are continuously infused medications that support your blood pressure. It's really important if you're on them, and it's really important at 11,15 that you are on a dose of 10 and not 2. And that information isn't in CDMs. Especially for COVID, it's really. Especially for COVID, it's really important to know: are you on two liters of oxygen or are you intubated on really high pressures? It's important to know if you are awake and alert and talking to your care providers or if you are really not there neurologically. And none of that information is represented well. And then with some variability, CDM, the CDM, or color transfusion can also be a problem. So, in a related challenge that sometimes kind of falls under the That sometimes kind of falls under the umbrella of data maturity. Melika works on a project to predict peripheral IV extravasation, which is a significant problem in kids, especially chronically ill kids, at Children's Hospital of Colorado, where I practice there's an image of an IV extravagation from one of my clinical partners. And they realized, you know, months into this project, that these variables that they needed, because they thought they might be highly predicted, were in fact. Be highly predicted, were in fact not in structured form. They were buried in those flow sheets. And so they had to kind of put on cost, the modeling effort, and go back and do some more data engineering so that they could surface the right data to Nellagra so that she could build the models they needed. Another example of this is that text note information represents something like three-quarters based on some estimates of the information in the EHR, but text notes. But text notes are really difficult to verify and be de-identified. I don't remember who said it earlier today, but health systems have liability if protected or private information leaks out. And so not just de-identifying it and then, yeah, it looks pretty good, but actually verifiably de-identifying it is a pretty high bar. And so the sort of downstream impact of that is that text notes are shared less often, and so they're less available for model development and testing. Model development and testing. Similarly, retrospective data have text notes and vital assignments are all there with time stamps corresponding to the physiology. And I'm slide that Kenrick is in the room. He can explain this better than I can. But I can tell you that in real life, nurses are busy and back charting happens, meaning that in our ICU, when realistic patients come in, the nurses take care of the patients. The nurses take care of the patient, and then two or three hours later, when things settle down and we achieve some equilibrium, they go back and chart all the stuff that happens. And you know, they help each other out, and they write things on paper towels, and they write things on our glass windows. So the information is there, but it isn't necessarily there at the moment in time. There is a score called a pediatric or rewarding score that works really well. If you assume that the timestamps of the data are accurate, but somebody went back and looked and It, but somebody went back and looked, and if you take into account the back chart, it doesn't work as well. So, asking the question: what timestamps are on your data? Is it the EHR log when the data are entered, or is it the timestamp of when the event corresponded to the patient's physiology? So similarly, really amazing data, imaging data, but raw images are typically on separate systems from the EHR, whether it's a PAC system or an ultrasound system. system or an ultrasound system or an endostophone system. They are also challenging to verify and de-identify and therefore they're shared through often less often and these files can get really large as most folks probably know them so computers start being used. That's Luna. So last data type So, last data type where this is relevant is physiological signals and ventilator waveforms. They are collected on devices separate from the DHR, which is a picture on the right of the bedside, the old bedside monitors in our ICU, and on the left of one of our newer types of ventilators. Many times these proprietary, sorry, these devices have proprietary data formats, and so it's required that the health systems purchase integrated. Systems, purchase integration systems, and so-called middleware. And Brad, who's in the room, can talk with you all about work that he has done with our Doll Health system to support generator waveform data integration with some of his grants. These are very large files, and so compute and storage become issues. They're collected at, depending on the system, say 100 to 500 hertz. They're down s file sounds, as an example, at my institution. As an example, at my institution, they are down sampled to every one minute in the EHR. Those data only persist for a week and then they're dumped and they are validated by our ICU nurses, and so this is more often than in other parts of the hospital, every one hour. And so you can see the kind of information loss based on the inability to store and retain all that data. And the health systems have really good reasons. The health systems have really good reasons to not retain all that data because if the production EHR slows down, then the patients are at risk and the health system is viable. If it slows down because they're trying to store all those huge values. So last example, incentive alignment. So EHR vendors have realized that data science tools are really of interest to patients and to health systems, and so they want to sell their models. And so they want to sell their models. And so they create incentives for their health system customers to do this. And there are features of lock-in to the way that these incentives work. Correspondingly, the models that they try to sell may not necessarily generalize to all the places that you might work to. An example of this, we a few years ago were asked by our health health system to evaluate a sepsis prediction model that was released by EPIC Corporation. Corporation, and we did this and presented it at INIA and also at one of the EPIC meetings. Interestingly, this was in partnership with a data scientist that at the time was on loan from the NSA. So it was a really fun collaborative project. And we found that it was okay, but it certainly was not as good as the rest of the days. Another institution did a similar study last summer, and it kind of caught the attention of a community, and I think it took a lot of time. Of a community, and I think it took a lot of heat for these close, you know, not publicly available algorithms and models that don't work as well as they say they do. So, despite all those things, it's really an exciting time, and I think there's a lot of potential for impact if we can figure out the messy system engineering. And there's one take-home point: it's that partnerships from the health systems, different types of experts, are really critical to get done. Thank you very much.  Alright, so we're gonna attack team again with the way we did yesterday, so hopefully it will work out. So we yesterday started talking about some of the challenges with implementation of these kinds of systems in the broader context outside of the hospital environments. We talked about the different platforms that enable That enabled both the delivery and the collection of the data, and how there was a need to collect new different data types and help individuals to reason over the data to arrive at conclusions, awareness that can lead to improved decisions in the future. And we also want to not just give them access to data, but also to make more concrete Concrete provide features to provide decision support, for example, with insights, recommendations, and predictions, as we've shown before. And we also want to, at the same time, collect more data as they're using those tools to learn new insights on larger populations. So, some of the barriers we wanted to talk about is for user-centered design that I talked about. User-centered design that I talked about at the very first day, which is kind of the founding principle in human-computer interaction, requires users and availability of users to design with. And I'll talk more about why it's challenging to design these kinds of systems with users later on. But here I will just talk about the access to users. Where do you get them? We'll talk about what it takes to build platforms. So George talked about how to build this kind of system. Tell how to build these kinds of systems in the hospital technological ecosystems. And there are some new challenges, somewhat different outside of the hospital environment. We'll talk about what it means to maintain this kind of platforms over time because we do want them to be used over some period of time. The modeling, the data, the modeling, and then engaging with users. So there are different, again, to deploy these systems with users, you need users to deploy them. Users, you need users to deploy them with. One of the advantages of working in the hospital is that you actually have them in a constrained environment. And there is an opportunity. You already have all the clinicians and users there. There are barriers to deployment, but at least they are all in the same place. When we deploy the systems in the real world, what does it mean? We are like, there are all the people in the world who could be using them, but the idea of if we build them. If we build them, they will come, it doesn't really work. Just putting something on the app store does not at all guarantee any kind of uptake. And I think we've both experienced that. So there are different approaches that are possible, but they all have their limitations. So my strategy has been to, I don't know, it's an expensive strategy, to do it through randomized control trials where I do have an opportunity to recruit a very select group of people who meet. Select group of people who meet eligibility criteria, but half of my, so those are done within the R01s, half of my budget goes into recruitment and retention of participants. So it takes just as much, if not more, just to maintain this group of people that I've recruited than it is to build the intervention and develop all the models that we want. And I think on Noemi's side, these are all the organizations that you open. Organizations that you can watch. So, COVID Watcher was another, we talked about it yesterday, where the idea was to collect data on symptoms and behaviors for a large population in New York City. And again, what does it mean? How do we get them to use it? Here's an application. So, we've worked with a large number of organizations just to increase awareness of this tool being available. Awareness of this tool being available. And with mixed success, we still ended up with a group of users that was in our local community more so than with individuals sort of outside of the Columbia area. So again, getting finding users and engaging them in the study is really difficult. Now, there is a question of how do we actually build the tool, right? So there is a model, there is some kind of computational system that runs, that generates predictions. Now we need an interactive system that deploys that or delivers that to users and individuals in the real world. There are different strategies here, and again, we've tried, I think both of us tried different ways. We can do in-house development. We can do in-house development. We also can outsource development to outside companies, software development companies. They both have their challenges. In-house development, so both of our backgrounds in computer science, I can tell you for a fact that PhD students have no idea how to build what software engineering actually means and how to build production quality software engineering systems. It's a completely different process. Coding is Coding is not the same as software development. So, every time we try to build systems in-house, we need professional software development who understand the best practices of software engineering and can develop software that has, like, doesn't break the next day after deployment and doesn't have a lot of duct tape. And things like automated unit tests, where when Unit tests where when there are bugs, you don't have to debug it manually. And so, there, I mean, software engineering is a huge industry with a lot of best practices developed there. And typically, we're not very aware of that. Then there's also the part that's development of the app or some technology with a very concentrated effort that you can build into, let's say, R1. And then there is the trial. And then there is the trial for me that lasts five years, and my technology needs to continue working for the next five years. Which means, but I don't need a developer full time anymore, so I just need a little bit of somebody's time. So, figuring out logistically how to make it work is a little challenging. Then, there's the integration of the mobile app with the system. So, for example, when I collaborate with Dave, we may have somebody like a Somebody, like a PhD student, coding up the model that creates this, generates some kind of inferences, and then we need a mobile app that continues to work with this model that is aligned in terms of inputs and outputs. Then, if we wanted to update the model, we can do that without breaking the app. And if we want to introduce the new features in the app, we can still communicate with the model. And then the big challenge for us is that students graduate. For us, is that students graduate? And then, if there was a student involved in the development of the model, Matt, who actually was involved in the development of the model in the group oracle, then what do you do new PhD students come in who want to work on their own models? And then again, if we want to maintain the model for years after that, it becomes an interesting challenge in itself. You want to talk about the commercial pressure? Commercial platform. So one other approach is to try to find a platform that's already built and that fits exactly what you're looking for. And the good news is that there are a lot of startups out there and in the mobile health community, a ton of people who are really eager to work with researchers and actually advance science. So that's nice. And there's a few issues, the biggest one being There's a few issues, the biggest one being that you still need to align your research with the commercial incentives and the needs of the startup. And so I just put these two as an example because they're interesting to me. Clue is very, very, it's the most popular master tracker out there. And so they're super research-oriented, it's wonderful, but they're so advanced in their platform that we can't really change anything. You know, we publish papers together, and that's it. We publish papers together, and that's about all that happens. Why DI is something that we're very excited to find. It's exactly doing what we want. It looks at people's menstrual cycle and gives recommendation about physical activity based on where they are on their cycle. And they're kind of starting. And so in this case, you know, it's kind of nice because you start a partnership and you can actually try to think about integrated research and applications. And applications. I'll take that one. So, support and maintenance, it's a big deal. The biggest part is that maybe in clinical realm, the bar is so low with electronic health record that there's no expectations about the quality of the interventions that you're putting out there. But in mobile health, it's not the case. And we're all users of app, and we have really high expectations. Really high expectations. And so when you put a research intervention on a mobile app, users are not going to be like, oh, it's okay, I can't use it well because it's research. They're just not going to use it at all. And so you have to really be able to have pretty much a full solution, even though your research might be extremely focused. There's a lot of requests and questions and bugs from users. And so in Fendo, for example, I have a project coordinator whose job is to reply. Whose job is to reply to emails within 24 hours. Maintenance, because we're dealing with a lot of mobile development, means sometimes recode everything because technology in mobile development keeps on changing. And there's a few constraints from the ecosystem, like Apple allows you to do a few things and not other things, etc. Yeah, that's like as I was writing these slides, like literally last week, we had to stop or Like, we had to stop or put a thing on Instagram saying, like, hi, we're stopping the app because we have a giant bug. And so it happens. Here, you should take that one. Yeah, I'll talk about this one. So there were many discussions earlier today that how models need to be true to data. Well, having true data is actually challenging in itself because, and George talked about quality of data in the hospital settings and in the outside of the hospital settings. In the outside of the hospital settings is just as difficult, if not more so. So, this is some experiences from Glucoracle, the app that was predicting blood glucose, changes in blood glucose levels. This is pretty much true to a lot of app usage, is that you have a very uneven usage trajectory. So, there's going to be a power law distribution. You have a lot of people who just collect a little bit of data and then stop using it and switch to a different app. And then you have a few. Up, and then you have a few users who use it a lot and create this very, very longitudinal record. And the challenge is to learn from both because it's hard, like in Glucoracle, for example, we have thousands of people with like a few days' worth of records. And then we have a handful of users with a few years worth of records. Putting those together is really challenging. And then the data quality. And then the data quality and reliability. I talked about it yesterday. So, again, in our case, we're trying to use blood glucose levels, nutrition, and physical activity. Physical activity is wonderful because Fitbeat takes care of that. It's passively trapped. The only question there for us is: what is it that we get out of it? Because there are many different ways to represent physical activity data. So, that in itself is a bit of a challenge, but at least it's there. But at least it's there. For blood glucose levels, if we use finger sticks, it means that we either need to integrate with glucose meters that have some kind of Bluetooth connection, which very few people have those. If we want to use any, whatever, basically use your own glucose meter that you have, they need to re-enter those numbers into our app, which becomes interesting what time that reading was actually captured and how accurate it is. Accurate, it is. CGM is great because it passively tracks, but it lacks all context. We had this discussion earlier today when I was using it for our research purposes, because we needed to train our models. I've discovered that my runs look exactly the same as meals. It's exactly the same trajectory of change in blood glucose levels when I run than when I eat. So if I'm the one tracking it and you see my glucose And you see my glucose curve, you don't know whether those were nils or rice. Which means that we may make some inferences about the general change in the trajectory, but those are guesses. And we don't really know what led to the patterns. And meals is a horrible situation. I was one of the organizers last year of an NSF-funded workshop on technology for diet tracking, and the conclusion of Diet tracking, and the conclusion of that is that there are no technologies for diet tracking. Basically, we've tried many different things. There are apps out there, many of them have people track from databases. Those are incredibly cumbersome, burdensome, and people prioritize convenience, which means they select the first option there, rather than accuracy, which means that those records are highly inaccurate. We tried other things where we build our own app, people can take a picture. We build our own app, people can take a picture and provide the textual descriptions. Pictures have poor quality, there are a lot of empty plates because, oh, I forgot to take a picture before the meal, now I'm taking it after the meal, which is a little less useful. Text has a lot of, and pictures also, meals don't come all in the same plate. There are additions, there are second helpings, there are leftovers. And if you try to Are leftovers, and if you try to capture all of it, it becomes incredibly challenging and complicated. And there are a lot of cold meals. Basically, by the time you track all of that, your meal does already cold. Text we tried also has a lot of jargon, very brief description. We have things like it was a yummy dinner, which helps a lot when you want to know what was in the meal. And then we still need to convert all of that to some numeric representation. Representation so that we can use it in the model. And again, we've tried many different things, and none of them work really well. So all our dietary information is basically subject to a lot of suspicion. And then you model it. So here I'm showing some of the questions that we ask ourselves in modeling data from self-tracking. So this is, again, analysis from this popular math. From this popular menstrual tracker. We have about 400,000 users, 3.9 million cycles. It's amazing, we love it. We're going to be able to predict when is the next period for an individual, except that humans are humans and they don't always track their cycles regularly. And so now there's this question of like you're modeling the cycle length, but you also don't know if a cycle is like forty-five days long. Did someone just forget to track? someone just forget to track at day 29 say or do they actually have something that's gonna be a longer cycle just this time and so that's kind of what you see here is like if this is doesn't really matter because there's these two big spikes here and then 30 days and 60 days and what that means that there's a bunch of people who forget to track one cycle and a bunch of people who forget to track two cycles in a row and so the way we try to fix this is when we want try to fix this is when we model any outcome we also try to model the behavior. So we have two things that we try to put together and it's kind of nice because now we have a probability of today it's very likely that you forgot to track something and we can actually leverage that to make some sort of like smart alert that doesn't come back every day. Like you don't want someone to say every single day do you have your period but we want to know at a certain time this is most likely for you. A certain time, this is most likely for you to have your period, and so if we haven't heard from you, we want to be able to reach out. And so, a little more on engaging users. We talked about how hard it is to find them. And even after we find them, there are still some challenges to recruitment. For example, privacy. People, generally speaking, can be hesitant to track their information and to share. Hesitant to track their information and to share their information. There is a difference again if we recruit them for clinical trials because there is a lot of disclosure of procedures and protocols. It's less so when we try to recruit people in the wild. I think both of us have used the consent form as part of the application where you look at the consent and you sign your consent form as you download the app. But as we all know, people don't really read consent forms, so they Don't really read consent forms, so they don't really know what it is that they consent to. And oftentimes, because commercial applications have very different procedures, when they're confronted with a really long consent form in a research app, that means they just close the app right away and switch to something else that doesn't have this kind of process. The trust in the intervention. In Global Oracle, it was very interesting because people could actually see how accurate the predictions were. How accurate the predictions were because we asked them to track blood glucose before and after. And the models tracked the, it was a long time ago, so I don't think that you guys are using those models anymore. We built the app five, six years ago. So it tracked some people really well and didn't track others at all. And it really had impact on their engagement. When people saw that the predictions were accurate, it really made them use the app more because it was more blind. Because it was more reliable. They could see that actually it was doing something, was learning something about them. When they saw the predictions were not accurate at the beginning, that really killed their engagement and their trust in the app. And I think one of the things, so it really, they're early, and with all mobile apps, we have a very short window of opportunity to get their attention. If it didn't work out for them and it didn't prove its usefulness within the first couple of Within the first couple of days, the first time they used it, it pretty much means that they've disconnected right there. Identifying champions is always a good strategy, and I think that works just as well in the hospital as it does outside. So Loimi has done a lot reaching out to public figures within the endometriosis space to have the endorsement. It is harder to do in the diabetes self-management space. Self-management space because also in economically disadvantaged communities. But I've worked a lot with community organizations trying to get them to sort of encourage their community members to engage in studies. And again, back to sustained engagement. It's not easy to get people to start using it, but it's much harder to get them to continue using it over time. Using it over time. So, yesterday, who was that who was talking about? It was Mary actually who was talking about paying for usage. So, that's one strategy, not necessarily very sustainable. And what we also found is that there is a bit of a tension between testing a research prototype and a research idea and creating sustainable. idea and creating sustained engagement. So in Blue Oracle specifically, the idea was to test the computational model and the prediction, but you can't expect somebody to use five different self-management apps. What they wanted is one diabetes self-management app that had everything. And we are a small part of their needs. So that makes it harder to create sustained engagement. Okay, that's our last, our last, but really should be. Or last, but really should be the first as academic people. The question is: who's going to fund this? We're in the business of doing research, not in the business of building apps. And so, you know, there's all these questions and it might be very different funding sources for each of them. The reason I put these screenshots of Fando is because we couldn't get funding to build the app and so we did it on the chip. And those are all pictures that we took with our iPhones in my office. That's me sleeping on a cushion. Me sleeping on a cushion in my office. That's the toilet next to George's office. And that's all of the students at DVMI jumping, sitting, drinking coffee. But that's what we had to do. So we had a lot of help, but it was hard. So one big question is when you build it, you want to, I mean, there's a cache 22 basically. And NIH is very interesting. And NIH is very interested in interventions with health consumers, but is not too much interested in building the applications. And we'll stop right there. Thank you. Awesome. Are you there, Jane? Excellent, I see you. You could just share your screen and talk, and it should work. Thank you, Jerry. Dave, did you stop it? Okay, you just switched the video? I did. Okay. Dane, give me one second to cycle the recorder.