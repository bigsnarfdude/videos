What I'm talking about. Uh so xy is a Ronald vector, so x is a spectral spacing R D and Y is real. And we want to compute the value at risk of Y given X and the expected shortfall of Y given X. So we have two functions, the bar function, so the quantile function, Q X and the expected shortfall given X. And we want to learn these functions using hypothesis spaces of functions for Q and S. We look for the function Q in the space of normal X T. Same for function S based on IED samples for XY. That's what we have to do. That's what we have to do. What is the motivation? Can you speak just a little more? So, what is the motivation? The motivation is to regress learn function by regression, if you want, so that you don't need nesting Monte Carlo to do this kind of computations, because nesting Monte Carlo takes too long. Another motivation, our method also works if we augment X with confidence level and. With the confidence level alpha for this value at risk and all controls, it means that we can learn the value at risk given x and given alpha. And of course, if we know the contributor for every alpha, it's as if we knew the law of y given x. So, what we do is a way to learn the law of y given x. So, the problem of counter-integration, so learning this function q. Aggregation, so learning this function q, is a well-established statistical problem. And typically, people address it by parameterizing the function q as a linear function. And then the problem of estimating the function q boils down to simplex linear programming, which is convenient, but at the same time it's not fully satisfactory because if you do that for two different alphas, for each alpha, you will have a linear function. Phi, you will have a linear function estimating the corresponding config function q. And obviously, if you have two linear functions, unless they are parallel, they cut, and it means for a certain value of x, the contile will be in the wrong order. So, there is this quantile crossing issue, okay, which is inherent to linear contile regression. So, we don't want to do linear contile regression with the non-parametric regression by means of non-inear functionals, and we'll do it. Functionals and we'll do it for conditional value atries and also for conditional equity chartfall because nowadays equity chartfall is the most important risk manifer practice. We provide a non-asymptotic convergence analysis for our extreme method. We'll space if I need to neural networks with limits and then we'll see how we can extend this to learning Q and S not only as a function of risk factor X. not only as a function of risk factor x but also at the same time as a function of constant level alpha for these var and so that's all based the background on these representations of var and os as are mean this is a very famous formula by Rockefeller and Uri IZF, okay, based on this pinball loss, which is a dysfunction Which is this function of y and q. So 1 over epsilon, small epsilon is 1 minus alpha, where alpha is a large confidence level for the var and less. Epsilon minus 1 times positive part of y minus q plus q. And then their realm, their 2000 paper, is that the var of y is, let's make it simple, argmin, so the minimizer of expected pinball loss of y and q. Pinable loss of y and q. So minimize all possible q's. Expected of fall is the value of the minimum, which means it is expectation of loss function, of pinball loss of y, the q set to the value of y. And because the yes is this expectation, an expectation as always is also the the R min or the corresponding these squares problems. So in this Squares problems. So, in this formulation, I assume the variable has already been computed, and then I know that the corresponding ES is the R-min over all possible S of this squared loss criterion. And the proof of this theorem is very simple. We are in the setup of convex optimization. We write fast order conditions and we get the results. It's very easy. Okay? And the merit of And the merit of uh putting uh var and OS computation uh in an optimization uh perspective is that you then have access to many optimization methods to compute bar and ES, especially stochastic approximation algorithms, or frequently stochastic weight and descent algorithms for computing bar and ES. And there is a whole literature about that. It's stochastic approximation in convex setup, so we have plenty of theoretical results like almost all. Results like almost certain elements, certain theorems, etc. Okay, now we want to do something similar, but in the conditional setup, we are not talking of var and yes as numbers, but as functions of the conditioning factor x, right? So first we need a functional extension of these formulas of Akafler and Poriza F. They are not hard to obtain. Actually, you can show that for any You can show that for any space of functions containing the true quantile function Q, then the true quantile function Q will be the argument over all the hypothesis F of expected loss function of Y and F of X. And similarly, the shortfall function S is an argument over all possible functions G of the square loss difference, where here I inserted Where here I inserted Q of X assumed already known, which is two-stage procedure. First, I characterize Q as a minimizer, and then knowing this Q, S is itself characterized as another minimizer. The way we actually, you do have some representations of the pair Q S as an Armin. That would be another way to approach the problem, but in our experience. But in our experience, it's much better to do it in two steps in this way. So better from a mathematical viewpoint. And the proof is just using the previous calculation. So an easy proof. Now, what are the algorithms that we want to study for learning these two functions? Well, simple, we have these representations of Q and S and R mean, okay. As R mean, okay, to turn this into an algorithm, we replace this set of depotty spaces, which here are, let's say, all direct functions. We replace them by large spaces of neural networks. But, instead of expectation, we do sample mean of the basis of the simulated pair XIYI. And finally, instead of doing a global meaning, Of doing global animization, we do, for instance, stochastic identity to get an approximate algorithm is very straightforward. We will use neural nets typically as hypothesis species. Neural nets means you take the input X and then composed by affin functions and ballard activation functions. And the outer layer is just Vinar. Or the year is just a Vinarino hat, which means hat and S hat would be my global minimizers for this problem where I replaced Bore functions by neural nets and expectations by sample means. Okay. I don't know what are the global minimizers, so I do test stochastic radon descent in the space of neural nets. And then I get these estimators. This is the output of the neural networks. Local minimizer, which approximates the total minimizer. And how do you construct your learning set? How do I sorry? You construct your learning set. To train your neural network. How do I train it, Junior? Yes, on which sample? Yes, on which sample? I simulate my sample. This talk is all in a world of simulated data. It's not on real data. But we might do it, but the idea is that it's useful in quantity finance. But the motivation initially is XV computations, where you need to compute, let's say, the economic capital of a bank, not only at time zero, but passwords in the future. And that means you need at every point in time. That means you need at every point in time in the future to know your economic capital depending on the possible relations for this factor at the time. And you are in a simulation environment. You are a clearinghouse, you would like to project your initial margin in the future. So, future point time on the base of simulations to compute some risk measures, some value at risk. So, you have a market simulator and Market simulator, and you don't know in close form what is the volume, the expected shortfall, and that is why you rely on a neural net to volume. In the application I mentioned, you do this compression at the level of the directory portfolio of the bank, so you have 3,000 clients, you have 1 million contracts, you have 3,000 risk factors, and you need to compute the capital based on that, so no no hope for any closed formula for any client. But you are in a simulation analytic. In a simulation environment. So once you learned your var, as a function of x, if you want to learn your ES, there can be a trick, a shortcut. Maybe instead of learning from scratch the ES function, the trick is to say, okay, let's freeze all the inner layer in our network for the ES to be what we got for the VAR, and let's only do linear. And let's only do linear regression for the linear readout of the PS. I'm mentioning this because in practice this code very well. And that would be the best way to do it. Now, we'd like to be able to say something theoretical. The direction is that, let's say you already learned your filter risk for a certain architecture. When it comes to learning, When it comes to learning the ES, I would recycle, I would just use all all the weights that I learned from the VAR to feed the inner layer of my ES neural net. And I would just retrain the linear reduction. So the outer layer. So you are frozen by the training rate. Exactly. I'm using the feature discovered by Russia Duran and accelerates anonymity a lot. Not exactly. No, I am not empty, but practice shows that not only it's of course faster, but it also works better. When you talk about making this approximation earlier, it's in what sense, like in what topology? Because it's the space of like boreal components. Because the specific boreal functions is. Are you talking about this one? Yeah. Not much to say about it, because I will not be able to control this approximation. The approximation I will control is the gap between Q hat and the true multiple function Q. S hat and the true multi function. So nowadays there are ways to dig a little bit into this. In fact, we are working on that. But it takes you to a completely different field, which is the study of. Field, which is the study of convergence of stochastic analysis in non-convex amounts. And you can proceed, but it's very technical. Well, it's a very simple question. So I will not quantify this thing here in the theory. Not today, at least. So by error control, I mean q hat minus q, s hat minus s, right? Okay, the theorem is to assume a tube of parallel width. The tube of bounded width delta around the true function q. Okay? And what does it mean? I mean, assume y is bounded. Well, then, of course, assumption is satisfied. Delta could be the bound on y, etc. Assume now y is not bounded, but x is compact and q is continuous. Then you have access to the control of delta. But there may be other cases depending on equations where neither x nor y nor y even x nor q are bounded and still the assumption is satisfied. And the important result is that we need no tail assumptions on why. We don't want to make tail assumptions on why because we are in finance. We want to be able to work with so we do it under minimal assumptions on what. And there are minimal assumptions on y and y even x. In fact, for the quantile part of the result, we don't even need impegability. And for the TS part of the result, we just assume the gravity of y. And under this assumption, basically, we're able to show that the differ error q minus q hat and s minus s hat, and this is the norm, okay, norm, okay, is control. Is controlled by the sample size n to the power minus one-fourth. And we have very explicit constants in terms of this tube width delta and the characteristics of the neural network. So our result for the error on the var is better, for instance, than this one for Y with blue. While with low purity order, our results are consistent with these ones, but in their case, they assume deterministic exile. We are more general from this viewpoint. And also, because we are for the yes, which they don't want to do. Excuse me again. How come your error doesn't depend on your neural net architecture? Like the number of hidden layers and the number of neurons and why it does or why it does not. How comes it does depend? But in this notation, I I was on the impression that the errors doesn't depend on the neural net architecture. The QAT is chosen, is picked in a set of neurons. So, unsurprisingly, the error between the true Rambler function and the best guy that you can find. And the best guide that you can find in this family of neural nets depends on the architecture of the neural net. So you have a convergence in terms of the size of your learning set, but you still have a gap because you have a neural debt. Another question regarding this convergence rate. In this non-parametric approximation, that the convergence rate depends on the dimension of the input and also the regularity of this function. No, we don't need this kind of function. As opposed to other patterns, holder queue or maybe components. Now, still what we don't control, as I said, is these guys, we don't know, which means at the end of the day we don't control the quality of these guys. Control the quality of this gap. So we'd like to propose something, maybe not a priori of control, just a human being, but something that we can say something about the quality of the final estimate. And here that's where we exploit the fact that we are in a simulation environment where we can simulate as much as we want. We can re-simulate if we want. So there is this validation, this aposteriori Monte Carlo validation, which is a Monte Carlo regulation, which is a companion Monte Carlo procedure, which will tell you apostoli whether your final estimate is good enough or not. How good or bad is it? And it's based on this result. So this is a part for the compile. You use parallel, actually. So that if y1 and y2 mean two. mean two copies of y conditionally is a pattern given x. Then you have this representation. What it means for quickly is that you are able to simulate x. Given x you can simulate y. Well I do it twice independently. So I simulate my x and then from this point independently I simulate y one and y two. And I have this representation And I have this shown, which is essentially Togolo, the same trick as in this uh branching particle literature. You know, basically the trick is here. This is the conditional uh probability square, because they have the same loop even x, is a product of probar of y1 greater than fx into the x, and probability greater than fx to the x. And because y1 and y2 are unable to do that, Y one and y two are unequivocal given X. This is the probar of the product, which is Y minimum as an X. And by the tomorrow, okay, you have collapse, and that will have here, which has no understanding probability of expectation of the square of the condition multiplied. Okay? And it's it's very useful because it it means. useful because it it means to estimate I'm given any function f okay any okay to estimate to which extent this conditional probability is far from one minus five if I come out with an estimator for the conditional var, this should be zero. Let's take my final estimate, plug it as f. Based on this Based on this identity, I don't know Monte Carlo simulations for y1 and y2, I'm able to measure this distance between quantity produced by my estimate. One minus a third should be if my estimate was perfect. And you can leverage on that uh product with a C-NR bound without the estimation the validation of The validation of your expenditure. So this works again because we are in the simulation environment, where I have no difficulty after my training to relaunch simulations and compute these guys. And it's aposteriary error control, but at least, okay, if the error is good enough, I'm happy. If it's not good enough, I can improve. I can use a rich architecture. Use the rich architecture, I can rerun my LGD, etc. So, heuristically, this is very useful. And also, a beauty of this approach is that you can extend it at no cost, basically, in the more general problem of learning your var of y given x and given alpha. Alpha meaning the confidence level of y var and less. The trick is simple to randomize, randomize alpha. Randomize alpha. In the same way as x and y are random. Randomize alpha to be uniform in a certain segment. And then basically everything I did before is extended to this more general situation. And then you get your variants as a function of x and alpha in only one minimization. So let me show you. So let me show you quickly what it means. Uh let's say I want to compute the contile as a function of contile of uh y given x and given alpha. But this is what I would solve in practice. I would there is this pinball loss function which is apparent here. So I have y minus cantile function of x and y positive part, okay, plus that's what's the way. Plus, uh, that's what the way I printed in the beginning, it was one over epsilon part, plus compile, here I multiplied everybody by epsilon to one, and that epsilon plus one minus alpha. Okay, now alpha is randomized, so I am something plus alpha x. Okay, and this I estimate the minimize the weight of the validity, I'm going to minimize this kind of. And the validity is to minimize this quantity over many samples. And then basically, the corresponding theta value is weight is my estimator for the value as a function of x and alpha joint t. And the theorem that I mentioned still works in this case. So at this n to the minus one fourth, we still have this possibility to validate a pressure by Monte Carlo equality. Data passed away by Monte Carlo, a quality first. We can even, in that case, at least penalize crossing compacts by having penalization term. So we penalize the negative values of the derivative in alpha of our compile function alpha, right? And it's easy to do in your net setup because we have access by To do in your net setup because we have access by AAD to this alpha derivatives. Okay, there are variants in this approach. That's another way to do it. Where internet with but one but k outputs, and then we combine the outputs in this way, plug them in the formula in each of the outputs. Each of the outputs of the neural net is an estimate for the accrement contained. So this is function numbers. So we run all these methods in the In the next slide, toy model, we have y is Gaussian, sorry, x is Gaussian, we divide it. And y given x is the sum between stupent given x plus lithium times the sum function of x. Why this specification? Because we wanted to avoid two easy cases, where things would be where the true answer would. Answer which is simply parametric expression with two parameters, and we have to have a detail as well. So, in this way, things are semi-analytic. We can compute the solution, more or less, at least by finding. Not too difficult to compute the true solution so that we can benchmark, but also the problem is not easy of the simulation. And then we get this n to minus. We get this n to minus force, done numerically. Okay, we get the minus force flow, which means that, okay, remember, n minus force was the arbitrary error for the q hat minus q. Here, what we are looking at is not q hat minus q, but it is a true estimate minus q, and still we get the minus force, both for the r curves because I'm showing you the curve for the I'll show you the colour for the average results over six different alphas. One orange or blue corner is when I do alpha by alpha. The other one is when I do one training for the six alpha at the same time, using the multi-alpha approach. Both exhibit the expected L minus one force concentrate. Here for the yes I even alpha I have four curves. Why? Because I can do it alpha by alpha. Or I can do six alpha at the same time. Or I can do six alpha at the same time, but I can, you know, to compute the yes, I need to plug a hypothesis for the quantile. And I do it by plugging the true quantile, which in this case I know, versus plugging the quantile I learned in the first place to show you that everything works well. And then, very, very briefly, what is very interesting is that, okay, if you are interested. Okay, if you are interested in several alphas or quite extreme alphas, again, you can do it alpha by alpha, or you can do it in one shot. So doing it in one shot is of course faster, but what is interesting is that it's also more accurate as far as the high competence people are concerned. As if doing when you do it in one shot, uh when you learn your high alpha content, Your high alpha compiled, you are like benefiting from the easier problems for low alpha compiles. So, again, doing watch up is faster, but it also gives you better results when it comes to high confidence levels. And this exact same table, it gives you the same case results. The previous table is plain error means error with respect to the true values which we know in this toy model. Here, error means this. Error means these twin Monte Carlo errors, apostolic valuation threshold. So it's crazy because this table opens the previous one shows you that this twin Monte Calo procedure is totally reliable to assess which method is better than another. So let's say in a quality environment where you don't know the two values, you can really use this apostle moticle to know whether your estimates is good or bad. Good or bad. And then, in terms of compile crossing, no surprise. If you do the joint approach, you are much better in terms of contained crossing than if you do it alpha and alpha separately. And I guess I can stop here. Thank you. Thank you. I have two questions. So, when you set up estimating your conditional. Estimating the conditional benefits can be expected shortfall. How do we guarantee that the expected shortfall is always larger than the valued risk? And the second one is: so you do this two-step estimation. So first you get an estimate of the value at risk and then the expected shortfall. So of course if the valued risk is wrong, that propagates through the expected shortfall and you get a larger error. So why don't you consider joint illicit ability of the valued risk and expected shortfall? So first question, for synthesis is fine. In fact, in the paper, Q is compile and air is a different short volume compiled, and this air is looked for in the space of positive. Second question, why don't, why, does two-step procedure dangerous? Yes, well, as you can see, for example, it here, you see, some of these You see, uh s in some of these curves I use a two contiler in the seven step. Other curves I use a learned contile in the semi-step and you cannot tell which is better than the other but the and then your last question is why don't you why not we choose a joint approach? In fact, we started the paper, that's what we did. We're going into the terms. And two problems. Theoretical problem is that we're not about to derive the theoretical diagrams. Much further from complexity. But this is this is not why we we We and the true reason is that numerically the joint doesn't work. The reason why the joint doesn't work is because you have two fails in your problem. You have a combined loss with terms, and it's very difficult to make it so that your joint procedure doesn't characterise either bar or yes. So Either var or yes. So you end up estimating the var well as a yes is very badly done or it's a latter. And yeah, this is much, much more robust. Well, maybe you chat a bit more because I've done the joint elicitability, so understand a bit more. Maybe not enough delay the programme too much, maybe delay the matchup or the match break. Okay, let's stand to the speaker, I think. 