So, representation learning is a little bit overloaded of a term, but here when I say representation learning, what I really mean is nonlinear factor analysis. So, the setting that we're looking at is we have some high-dimensional data. So, for example, we might have some single-cell gene expression data where we have thousands of genes and thousands of cells or different samples, and we're trying to learn some low-dimensional structure in this data. Learn some low-dimensional structure in this data. So, the model that we're going to consider is that we have some latents Z here, and oh, it's not really working. I guess I'll have to use the sticker. So, the model that we have is that we have some unobserved latent Z, and these Z are driving the expression of our observed data. So, when this Z turns on, for example, we're driving the expression in We're driving the expression in a subset of our genes. And what genes are connected to what factor is sort of informative of a biological process, for example. So this Z1 could potentially describe some biological process that then, when this is happening, it results in a number of genes being biased. This is sort of the cartoon of what's happening that we're trying to model here. So we want to learn some low-dimensional Z, as I said. So I'm going to Said. So I'm going to interchangeably say representation or factors just because it's kind of bridges the machine learning stats community, and that's what they often say in ML. But we have some desiderata for what we want our representations to come out as. So first of all, we want some flexibility in our model. So we want to be able to capture potentially non-linear associations in our data. And we also want some kind of interpretability. Interpretability. And interpretability in this context means we want to understand what of our observed data features are associated with which dimensions of our underlying factors or which dimensions of our representation. And finally, we want identifiability because if there are an infinite number of solutions that equally fit our data, then we don't really have any hope of interpreting our representations or understanding anything more about our data. Our data. Okay, so what are some of the challenges that we have here and trying to fit these flexible, interpretable, and identifiable models? So we're going to consider this deep gender model setup where we have our latent z here, and we're assuming that z is passed through some very flexible function. We're going to model this function with a neural network, but some flexible function f here to produce our observed data xi. Observed data xi, and we have some additive noise as well. So, as might be very clear from the bat, we can reparameterize this model as introducing some h inverse of z, and then our flexible function could be f composed with h and then this process will produce the exact same data. This has the exact same likelihood, but now we have these two competing representations. Have these two competing representations, z and h inverse of z, we can't decide which one to use. This is obviously then very problematic for doing any kind of interpretation downstream of our data. So what are some solutions or how can we even think about or hope to fix this identifiability problem? So, generally, what we could do is place constraints on either the distribution of our latent representation or our latent factors or Or our latent factors, or put some constraints on the functional form of f. And these should be inspired by our data and should be meaning constraints in terms of our problem at hand. So how does this help us? So we can see if we have the set of solutions f and z and then f composed of h and then h inverse of z, if we have some restriction on f, this restriction also needs to hold for f composed with h. So for instance, if f So, for instance, if f has some kind of sparse structure, you know, we can only, the only solutions that are like unidentified in terms of this H are ones that don't preserve this or that preserve this bus structure. And so that eliminates a lot of these possible H's that we can have. So, this kind of identifiable representation learning constraints has been studied a lot very recently in the machine learning literature and more broadly the machine learning literature and more broadly the or you know more specifically rather the causal representation learning literature which is you know trying to figure out what conditions do we need and you know how can we use auxiliary information to try and pin down our representations better so i won't go through all of these but um there's been a variety of approaches including using data augmentations or temporal dependencies or using some kind of interventional data or some kind of specific priors to get at identifiability and these all come Viability and these all come under the banner of you know putting constraints on our representations and then there's also work on putting constraints on the function class of f, including assuming some kind of sparse structure, using this kind of independent mechanisms assumption in terms of how our representations are producing the observed data, as well as some other constraints on the function class. So the work I'm going to Class. So the work I'm going to talk about today fits in this class of restricting the function f in order to get identifiability. And we're looking at a specific kind of sparsity that also gives us interpretability, unlike some of these other ones. So as an outline for what I'm going to talk about for the rest of my time, I'm first going to provide some intuitions from the linear case and how this Linear case and how this inspired our work. I'll then talk about the SPARS VAE and then I'll talk about the multi-study SPARS VAE, which is ongoing work. And so we just have some preliminary results there. So let's go back to the linear factor analysis case to get some intuition for the identifiability problem. So here we're assuming our observed data of n samples and g features can be decomposed into these much lower dimensional matrices. Dimensional matrices of factors and matrices of loadings. So it's very well known that we can introduce any rotation matrix into this setup and we're not changing the likelihood. So we have this lack of identifiability and learning our latent factors and loadings here. So this is problematic. If we were trying to interpret the dimensions of Z, obviously, you know, if ZP is also a valid solution, the interpretation is completely different in terms of what it means. Different in terms of what it means in terms of the original data. So, the key idea that we're going to leverage is the idea of anchor features, and this is going to help us get identifiability. So, the idea of anchor features has appeared in topic modeling back in the early 2010s, and more recently, for this linear factor analysis setup as well. So, it's a really neat idea in that it's the idea is that there are some columns. The idea is that there are some columns or features of our original data set that depend only on one dimension of our latent factors. And so this is sparse in that this assumption means your loadings matrix has only one non-zero value for that particular column. And to provide some intuition for how this helps, essentially we're observing noisy proxies of our latent factors in the observed data. And so this is helping us for a plan. That you know, this is helping us for a fundamentally latent thing. It's not well, it's actually observed, you know, plus some noise. So, generally, we need at least two anchor features per dimension. And that helps us because if you think about this kind of spa structure in the loadings matrix, if you introduce, oh, just go back a bit. So, if you have, it turns out if you have this specific kind of spa structure, then any rotation matrix is going to break this. Rotation matrix is going to break this specific kind of sparse structure, so it eliminates all these competing solutions, and so you get identifiability. So, in our work, we wanted to see, oh, can we extend these kind of anchor ideas to the nonlinear case? So, fitting neural networks to learn our representations, can we also leverage these kind of really fundamental ideas that have appeared all over the place in these factorizations? So, now I'll talk about our approach. And so, And so, what it really boils down to is we're kind of introducing the idea of a loadings matrix back into this kind of deep generative model setup. So, we can actually parameterize these edges between our Z's and our X's. So, we consider the same deep generative model from before, except now we have this WJ that we're introducing that is element-wise multiplying our Z's. So, essentially, before. Essentially, before our representation or our factors are passed through the neural network, we're masking them so that only a couple of dimensions are coming to reproduce our data. So each feature or dimension of our original data has its own WJ. So each feature, so you can think of this as gene J. So gene J will use some subset of the ZIs and gene L. And Gene L will use a different subset of the ZIs to produce this observation. Okay. So one way or another way to visualize what we're doing here is that in the deep gender model I showed earlier, you have all of these edges between your Z's and your X's, whereas now we're pruning these connections to get both interpretability and as I'll show. And as I'll show, identifiability. So we get interpretability in the same way that we consider sparse linear factor analysis interpretable, and that we can look at our edges and say, okay, well, you know, if gene one and gene two are both depending only on Z1 and we know something about gene one and gene two, then we'll call this Z1, you know, that biological process. Or we'll look at movie ratings data later. So for instance, if X1 is aliens and X2 is like Aliens and X2 is like predator, and they're both in the same Z1, then maybe you know that Z1 is a sci-fi factor of some kind and describes a sci-fi genre. Okay, so we get our first two desiderata in that way. We get flexibility because we're ultimately going to be using neural networks for our encoder and our decoder. And we get interpretability as well, exactly as I described by having these parameterized edges. Described by having these parametrized edges between our z's and our x's. So, what about identifiability? So, classically, as we all know, identifiability is if we have two parameters that have equal likelihood, then these must be equal. Because we're working in this more flexible setup, we need to relax this definition just somewhat. And so, what we're really concerned about in this steep generative model setup is getting the dimensions correct. The dimensions correct. So, in some sense, I don't want to worry about if I found z1 squared or z1 as long as I've separated z1 from z2, right? So, what we're referring to this as is, you know, we're identifying our dimensions up to coordinate-wise transform. So, we want to be able to say that, okay, if I've found some zk, then I know that it's just a coordinate-wise transform. It's just a coordinate race transform, and then up to permutation as well of an alternate equivalent solution. So we proved that we get identifiability up to this coordinate-wise transform notion under the following assumptions. So we need anchor features. So for each of our Z dimensions, so each of our factors, we have at least two features that depend on that factor in the same way. So we have the same function mapping for those two. Same function mapping for those two. So, this might seem like somewhat of a strong assumption, but you might expect that people will rate the same kind of sci-fi like movie the same. We think that this can be relaxed a fair bit, but currently in our theory, we need the same mapping here for our identifiability result. And we also don't need to know what the anchor features are in advance. We're going to be learning what these anchors are. So, we also need some So, we also need some assumption on, I guess, the scale of the neural network. So, basically, what this is saying is that if you're perturbing a factor, the anchor features are going to be perturbed more than the non-anchor features. But we show that this is satisfied by neural networks with ReLU activations and independent weights. Of course, weights are no longer independent after training, so it would be nice to relax this somewhat as well, but at least at initialization we have these kind of constraints or assumptions satisfied. Constraints or assumptions satisfied. And we also need a constraint on or assumption on the factor covariance. And essentially, what this is saying is that your individual factors, their variance needs to be greater than their covariance with any other factor. I mean, you can think about if one factor only ever turns on when another factor turns on, so they co-vary significantly, then you almost want to just put them as one factor because you never have any single identifying information from either of them. Find information from either of them. Okay, so now let's move on to estimation. So we, once you learn these anchor features, we're going to think about this in a Bayesian framework and apply a sparsity-inducing prior on these W masks that we're trying to jointly learn. And so I'm going to use my favorite sparsity-inducing prior introduced by my PhD advisors, the Spike and Slab Lasso, which I'm sure a lot of people are. Which I'm sure a lot of people are familiar with. But basically, we're just assuming that our masks come from either a Laplacian spike and so are forced to be small or a Laplacian slab and so are allowed to be large a priori. So we also have a normal prior on our representation, and we're assuming a Gaussian likelihood here. Assuming a Gaussian likelihood here for our observed data. So, we're going to fit this model with a variational autoencoder. So, I'll just briefly describe variational autoencoders in case anyone in the audience is unfamiliar. The idea of the variational autoencoder is that we're assuming a variational distribution for a latency, which is normal with some flexible function depending on the z and for the mean. And for the mean, and similarly, for the variance of our variational distribution. And so, pictorially, what's happening is that to get our varial distribution, we're forcing our X's through some bottleneck. But then also, we need to reconstruct our X's given this latent representation. And so these Z's are forced to contain only the important. You know, contain only the important information about this high-dimensional data here. So, we have some variational distribution. What do we do then for performing variational inference? We optimize our elbow, so our VAE objective. So, we have our likelihood component and then our KL component. And here, we can just, you know, these expectations are clearly very difficult. So we're just going to use Monte Carlo approximations of the expectations, and it works pretty well. Expectations and it works pretty well. That's pretty standard in the ML literature. And so we're going to have the same setup, but we just have these additional masking parameters that we're learning. And we can actually learn these with the variational EM algorithm, as well as using this variational auto-encoded machinery to learn our LZs here. So this is our setup. And any questions? And any questions? No? Great. So now I'll move on to some experiments. So I'll first talk about some experiments and how well we can recover the true factors. I'll then look at, okay, can we still do held out reconstruction of our data? Like, what are we losing by having these masking penalties or just masking our objective versus like a usual VAE? And then I'll talk about, oh, can we actually get interpretable results? Can we actually get interpretable results from this SPOS VAE setup? So, we compare our SPLAS VAE to a range of other methods. So to the standard VAE, to the beta VAE, which just adds some extra temps to the KL divergence, to this OIVE, which uses separate decoders for the different features, and then variational sparse coding, which tries to put sparsity directly on the factors Z. On the factors Z there. So the first experiment I'm going to show is a simulation study. So we have, it's a small data set. We have synthetic Gaussian data where we have a thousand samples, seven observed features, and two latent factors here. And so the setup is that the first three features depend on Z1, the next three features depend on Z2, and the last feature depends on both of these. And we have some nonlinear mappings between our latencies and our Between our latencies and our observed X's. And those latent mappings are shown here. We have some linear, some quadratic dependence between X3 and Z1, and then some sinusoidal for X6 and Z2, and then an interaction term at the end. So our true W matrix looks like this in terms of parametrizing. So here is the results for the SPAS VAE compared to a standard VAE. A standard VAE. So, what we can see here is that we're doing a good job of separating out Z1 from Z2. So, the sparse VE, when I plot the estimated Z's against the observed data, we're almost recovering these functional forms of the simulated data. Whereas the VAE sort of gets at the second factor, but really struggles because of the interaction term. And so, the first factor that's learned by the VAE. Factor that's learned by the VAE kind of is entangled with the second one there as well. So, we also run this experiment a number of times for different levels of correlation between our true factors. And at the top here, we have the held out mean squared error for our reconstruction of the data. So you can see it's fairly consistent across different correlation levels of our factors, which we may expect because we're still fitting our data. We may expect because you know, we're still fitting our data well in terms of you know being able to generate data, it looks like our observed data. And then down the bottom here, we have what's called the DCI disentanglement score, which sort of measures like how well you're separating out these factors. And we can see that we're doing very well for low levels of correlation between the factors. When factors get really correlated, it's much harder to solve this problem, but we're doing pretty well even when the factors are fairly correlated. Fairly correlated. So let's look at prediction. So we looked at this MovieLens data set of ratings of movies by different users. So we have 100,000 users and 300 movies. And we compare the methods in terms of the reconstruction error, holdout data, recall, and normalized discounted cumulative gain, which is sort of like a ranked recall. And in fact, we're doing better here than these other methods in terms of these held out predictive metrics. Of these held-out predictive metrics. We also do the same experiment for this peer read data set, which consists of word counts in paper abstracts. So 10,000 papers and 500 words here. And so this is nice in that we are not, you know, we're sort of constraining the VAE model in a particular way, but we're not losing out. And in fact, often we get better reconstruction, even though we're sort of constraining it more than the usual. Okay, so let's add. Okay, so let's actually look at: do we get meaningful W matrices? So, what we found was our W matrix, pretty much all of the dimensions were pretty interpretable in terms of the movies that were found in the clusters. I can't show all of them. So, some particularly fun ones where we found like a sci-fi factor, a Pixar factor, and an action adventure factor, or a Harrison-Ford factor, even. And we can also pick out which ones were found to be anchored. Pick out which ones were found to be anchored features as well. Okay, we also looked at a single-cell RNA sequencing data set with 3,000 cells and about 550 genes. And the cells are from different regions of the mouse cortex. And so the first thing we looked at was, so do our representations actually capture meaningful information? Can we use them in downstream tasks like predicting the cell label? And we found that the SPAS VE was. And we found that the SPAS VAE was the best at predicting the cell label, although that's within error ranges. So it's doing essentially the same as the VAE, I suppose, but it's doing better than linear methods. And moreover, even though it's doing the same as the VAE, we get this additional interpretability. We can do gene set ontology or enrichment analysis. And we want these clusters that are enriched for various biological processes, which is nice. Biological processes, which is okay. So, now I'll talk about ongoing work, which is the multi-study sparse VAE. So, as we learned from Roberta this morning, often we have data sets from multiple studies or multiple sites, and we're looking to understand what is some shared variation across these different data. So, before we considered a single data matrix, and now we have multiple data sets from different groups. So, our samples are going to be different. Our features. Are going to be different. Our features are going to be the same in terms of the genes that were measured or the movies that were measured, for example. So, the motivation for this is one of my collaborators at Rutgers has platelet gene expression data from patients with different conditions. And platelets are an important indicator of immune system function. And so, the idea is: can we learn markers of shared versus specific pathogy by looking at? Specific pathogy by looking at these kinds of multi-study setups. So, our goal is to disentangle the shared covariance and the group-specific covariance. So, the little cartoon that we have here is, so say we have group one. This is maybe like disease one and group two, disease two, for example. Suppose there's some shared process going on in both of these represented by this factor Z1. And when Z1 turns on, it drives variation in these. Depends on it drives variation in these three genes. But then we have some specific factors to the different diseases, say, so that are driving variation in the other genes in different ways. So a very natural way to model this is to introduce these W matrices parametrizing these edges for the shared edges and then the group specific edges as well. So, as you might anticipate, we're going to use the same strategy as before, but just with these W matrices that are shared and group specific. And then we're also going to fit this using our favorite Spiken's lab lasso prior to try and learn these masks. So, this setup, the goal is that by encouraging sparsity in these matrices, the model is encouraged to use the shared W matrix when appropriate and not put all information in. Appropriate and not put all information in the group-specific W. So ideally, so this is ongoing work. Ideally, we want to prove something showing that we can get at this shared covariance structure. But yeah, this is sort of the intuition is that, you know, maybe some failure mode of this model is that it could just, instead of learning, it could learn nothing in the shared component and just put everything in W1 and then everything in W2, but then this would result in a lot more dimensions being needed. A lot more dimensions being needed, as opposed to if you would have just one in that shared component versus having duplicates in these. So that's sort of the intuition for why we think this is a good setup. And in simulation studies, it seems to do something meaningful as well. So as I mentioned before, there is related work in the linear setting. So Roberta this morning talked about some of her multi-study factor analysis work. And then there's also been some recent work by Chandra et al. Some recent work by Chandra at all. So, I guess our difference here is that we're in this nonlinear setting and seeing can we generalize these ideas. Okay, so we have some very preliminary results on this platelet gene expression data that I told you earlier. And what we found is that for the shared components, we found they were enriched for multiple biological processes related to platelet function. Function. My collaborator is a platelet biologist. And so there's a lot more work to be done to sort of understand what's happening. And so watch the space in terms of filling out these results a little bit more. But some nice preliminary results. All right. So with that, thank you so much for your attention. I'd like to thank my collaborators on these projects. So for the first project on the SPARS VAE, I'd like to thank my co-authors, Dania, Shrida, My co-authors, Danya, Trita, Yi Shin Wang, and Dave Fly. And then I'd like to thank my collaborator Anandi, Krishnan at RECAS, who sort of inspired this project with this interesting platelet data. So with that, thank you. Any questions? Can you hear me? Or is somebody? No, but go ahead and I'll check if there's some online question. I'm online. Can you hear me? Yes, yes, we can. Can you talk to me? Oh, hey. So very bizarre. Yeah, but this is wonderful as always. I'm curious about this multi-study VAE. So you're putting are you putting like independent SSL priors on on each of those matrices, or are you somehow linking them in the hierarchy through like the proportion that comes from this matrix side? Proportion to come from this microphone. So, oh, yeah, so where we have the same setup in that we have these beta Bernoulli priors, and I guess we have the same like beta 1G prior on each of the sparsity levels in our columns. So they're linked in that sense that, like, the hyper prior has the same underlying sparsity penalty, but apart from that, the column. Philosophy penalty, but apart from that, the columns are, you know, so I'm sure people who know VMP Beta can tell me a lot about the exchangeability properties of that. But yeah, that's the setup that we're using. Cool. Yeah. Maybe I'll have to ask the three questions. Thanks for the super, super interesting talk. I'm wondering if you noticed any change, improvement or otherwise, in the ability to train these PAEs when you add Netflix parsity. When you add flux varsity, in terms of like, is there foster convergence or something like that? For instance, architecture search is that easier because it was sort of automatically detected. You added too much architecture, it just didn't work. Honestly, I felt like these kind of models are quite robust to like the number of just like dimensions of your hidden layers and stuff like that. Generally, I found that. Generally, I found that, yeah, just picking some, you know, if you look at your observed data and it's, you know, 600 dimensions, if you just arbitrarily like, I'll pick 500 dimensions, make it in layers, it actually does like quite well in many circumstances. So I didn't actually do a whole bunch of hyper-parameter tuning there. I kind of eyeballed reasonable things and it seemed to work pretty well. Yeah, so I guess I found, oh, one thing actually, it was a lot more robust to the learning rate. A lot more robust to the learning rate. Weirdly, I don't know. I found the VAEs tend to work way better with like one times 10 to the negative 4, whereas ours was quite robust to that. Whereas, yeah, I don't really have much intuition of why that is the case. Yeah, yeah, but it would be better to do like a whole extensive set of experiments and stuff. But yeah, generally, it seemed to work pretty well, which is maybe there is enough time to spend a package. 