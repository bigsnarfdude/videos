I will present one of my recent work, which is titled Functional Plant Transformation Model with Applications in Digital Health. Before that, I'll also try to give some of the research areas that I also work on. So, I got exposed to the idea of digital health while I was at Hopkins working with Vadin. And so, I also have past Hopkins accommodation. And my research actually training was in functional data, so that kind of nicely fit in when I started. 15 when I tried started working on it, uh, these kind of install technologies. And so, there's a comment: the room is muted. Oh, the room is muted, or yeah, no, no, it's not there, it should be on my audience here works for that. So, it's all good? Someone online, could you confirm that you can? Someone would like, could you confirm that you can hear us? Can you hear us? So, functional data analysis, I think most of the people in the room might be familiar with it, but just to give brief basics. So, in functional data, we deal with sample of smooth functions, and these vary over a so-called continuous index. Most often, it could be time, for example, your physical activity varying over the minutes of the time of the day, or could be other indexes such as spatial location. Indexes such as spatial locations or some wavelength also. And some examples can be particularly for digital health, this physical activity data varying over minutes, your heart rate data, your CGM data. So these are all examples in the domain of digital health. But it could be some other data also, like your long-term temperature or precipitation data varying over the years or months. So those could be also modeled using this functional data technique. So here I'm just briefly showing some example of this. Briefly, show you some example of this means that is collected in Enhanced Ins 2011 to 14. This is minute-level data where X, Y, and Z axis. And in functional data, underlying thing of interest is can we extract some smooth pattern from it, out of it, and if that smooth pattern is kind of useful for modeling our health outcome. So that's sometimes our major goal. So that may or may not be useful for your research questions. You have to understand where it actually functional data. Have to understand where actually functional data can be used. Okay, so in variables, nowadays in digital health, we use a lot of these physical activity trackers, heart track monitors, blood repours monitors, and these provide a very rich source of data, continuous user-specific data streams. And these signals are recorded often over 24-hour periods. So this fits nicely sometimes with the focus of functional data analysis. Another area that I started to work with while working with this type of Started to work with while working with this type of data modalities is called distributional data. And the idea is that when we have so much continuous stream of data, instead of like summarizing it over some aggregated metric for each person, why not use a person-specific distribution? So that's the kind of the idea. Because some of the digital signals you might not be interested in the circadian rhythm or the temporal pattern, but it's kind of the which frequencies, maybe it's the maximal quantiles or it's the contrast between your. Or it's the contrast between your maximal and lower quantiles in a person-specific way that might be useful for your health outcome. So, for example, for each person here, we have a non-decreasing quantile function that's what we use to encode this person-specific distribution. And this is some gate data we are using that we have episodes of walking, and from there we are extracting some gate feature and looking at like for multiple gate features for the same person. Gate features for the same person, encoding that in this person-specific distribution. And you see nice patterns in this quantile function that the major differences happen between, for example, in this example, between the AD and control group in the higher or the maximal quantiles compared to the medians. So, this could be useful in some applications using the person-specific distributions. And related to that, there have been a lot of related work in distributional data analysis and regression models in the last few years. In the last few years, and the main goal again is to use this person-specific distribution as data objects, as either as an outcome or as predicted if you are modeling some outcome, like health outcome. And this has shown a lot of applications in digital health, like in physical activity, gate, radio mix, CGM studies, and imaging also. And for example, in distributional regression, like in functional regression models, we can kind of divide into categories where we were modeling a scalar or health outcome based on the distributions. Outcome based on the distributions. So that can be called scalar or distribution recognition. And sometimes you can, the outcome can be itself a distribution, and you're trying to test some intervention effect or test some treatment effect and see which part of the distribution change. And that can be kind of formalized as distribution on scalar depression or distribution on distribution regression kind of framework. Okay, so that's kind of the background. But particularly for this work, we developed a survival model and survival model with functional data. Survival model with functional data. And in FDA, majorly the work in survival model and functional data has been done by GPAN and LGA here. And it's mostly like a few of the past work have focused on the COGS model and some work have focused on the escalator failure time model. So in this work, we tried to provide a kind of a slightly alternative and flexible framework to that because the proportional hazard assumption may not be always suitable to a real data and always the hazard ratio as a metric may not. The hazard ratio as a metric may not be always having a causal interpretation. So, the modeling the survival time directly might sometimes be a better alternative to modeling the hazard ratio. So, that's kind of the motivation behind this approach. So, this provides a kind of a more flexible and general class of survival models. And this encompasses the proportional hazard model, or the proportional odd model, or the AFT model, as special cases. So, slightly more general class of models using your functional experience. Using your functional exposure that you might have in your study. So, two motivating applications that we focused on in this. So, one is the enhanced, we used extradometer data from 2011 to 2014 as our exposure. So, we are looking at daily patterns of activities, so daily patterns of memes here as our exposure and modeling all-cause mortality of participants following through the 2019, so I don't know over 10 years of follow-up. And another data we have this CGM data. Another data, we have this CGM data where our outcome is an adverse outcome, multiple hypoglycemic events. And we are modeling the time to that adverse outcome in a population of type 1 diabetes patient using CGM data. And for that, what we do is use this person-specific distribution idea. So, instead of using A1C or the mean mean CGM, we use the person specific distribution of these groups profiles as exposure and try to model the uh this time to this adverse outcome. This time to this adverse outcome. So, those are the two applications. Good question. How did you pick 65? Yeah, based on literature, we saw a follow-up, like that amount of events amounts to a hypoglycemic, like severe hypoglycemia. So, based on the time of follow-up, we had picked it. But also, the time of follow-up? It's around 50 days or something like that. Something like that. Like 65 is a lot. That's where I was. Okay, okay. Yeah, I think you get some. Sorry, just one more question. Because by hypoglycemia, then do you mean an episode of hypoglycemia or just any time they go into that level? Anytime they go in the level. Single point below the count. Below that level. Okay, so the objective is. Okay, so the objective is pretty simple. We are to develop this flexible time transformation model. So, time transformation model already existed in 2000 literature and kind of extended to functional and distributional predictors. And we provide a flexible estimation method using CM maximum likelihood. And I will show this application in this two digital health case study. So, we have a kind of a standard survival framework. We are not going into too much detail on the notation, but we Much detail on the rotation, but we have a right-sensitive conditional independent scenario, and then we have a functional exposure that is our primary exposure in the effect we are interested in model. So, here is the functional time transform model. How does it look like? So, we model the monotone transformation of the survival time. So, h is unknown, you have to estimate it. And h of t, we model is just as a linear function of your covariates. So, if your x is like your confounders, we have a is like confounders we have a linear effect beta and for our functional exposure at xf you have a function of our dynamic effect beta s which we are trying to estimate so this is similar to a aft model suppose h is a log function so log of survival we have known that we can model this is aft model so this is slightly more flexible model that each function is an unknown monotone function that links the survival time linearly to the exposure of interest and you have a class of error functions And you have a class of error functions which are again like kind of known. We assume that they come from a common class of known error functions, and we try to maximize our or obtain our parameters kind of a maximum likelihood estimation kind of way. So the main effect that we are trying to get here or estimate here is this beta s and because there is a negative sign there and h is more than increasing. So if beta s is positive, so that kind of indicates a negative sign. So, that kind of indicates a negative association with the survival type of exposure. Okay, so then we can do like get the expressions of your survival function and your density function from this given functional time transformation model. We assume that the error distribution comes from a known class of logarithmic error distribution. And this is a very general class of error distribution. And for example, for particular choices of our parameters like r equal to 0. For parameters like r equal to 0, this reduces to the extreme value distribution, which is the Cox model. And for R equal to 1, the distribution reduces to a logistic distribution, which is the proportional odds model. So we allow flexibility. Yeah. Oh, sorry, I was just having a clarifying question. So HFT is just going to be like a log, so it would be like an AFT model. Yeah, so HFT is known, it's a log, it's exactly the AFT model. Yeah. Right. So we are keeping it kind of a monotonous increasing function. Increasing function. So, based on this parameter, R, we can actually get a different like spectrum of models ranging from the Ox model to proportional Lord's model or different kind of behavior of your several outcome. And so, there are two parameters which are unknown here: the H function, which is the monotonous transformation, and this beta, along with this scalar coefficients beta, also. And we model them using Bunstrand polynomial analysis functions. So, these Bunstrain polynomials looks like. This Bunstead polynomials look like these beta densities or Bernoulli density values. And these basis coefficients, then what we are trying to estimate from our model, which once we estimate that, we can just plug them in and that would give us kind of the estimate of this functional coefficient and the transformation function. Okay, so without going into again too much theoretical math details, we plug these expressions of the survival function and the density function. And the density function, and obtain our likelihood function, which is for right sensor data, a combination of your log density and your survival function. And we maximize this over the set of parameters of interest, which are essentially the basis coefficients. And by maximizing this over these parameters, we can get those estimates of our basis coefficients and the corresponding functions that we are interested in. And there are some tuning parameters here. This N0, N1 is the. This n0 is the degree of the Bernstein-Besis polynomials, and this kind of controls the smoothness behavior of these functions. We choose them using a AIC kind of criteria, which accounts for the like overly using a number of bases, which can result in a weekly function kind of. Okay, so this is the estimation approach and the inference approach. And we can also establish some theoretical results in our paper under some standard inverse. In our paper, under some standard linearity conditions and doing inference, so we can get confidence band in particular of those estimates. So, I will come to the application results. We did some simulations and have pretty good simulation results as usual, but come to the application. So, here the outcome we are modeling is all-cause mortality in enhanced 2011 to 14, and we are looking at older adults age 50 or older, and we had like around 3,000 older adults at 3000 older adults at this should be 2011. So 2011 to 14, we are focusing on that. And among all the samples considered, at the end of the study, we had around 592 events. And we apply this proposed model and adjust for age, BMI, and gender, with the main exposure being this daily pattern of physical activity. And this could help us in understanding which time of the day could be most useful for intervention and directly seeing the And directly seeing the association of survival time with this thermal physical activity pattern instead of modeling the hazard ratio. Okay, so here's the result. First, this is exploratory plot. So this is the daily pattern plot for the people alive and the people diseased by the end of the study. And this kind of plot is very common. We have seen this in past in enhancement. We see this trimodal kind of diagonal pattern in the older adults. So when we fit our model, this is the solid line. Tower model: this is the solid line here, is the estimated functional effect beta s, and we do get significance but in a very small portion here around daytime between 10 to 2 p.m. And because beta is negative here, so that means, and based on our formulation of FREDM, that means it leads to an increased survival time directly, not the hazard, but the survival time directly increases based on the increased amount of physical activity here. Activity here and the parameter for the error distribution was like 0.35 chosen by cross-validation. And that kind of indicated that you can do better than slightly than the Cox model. Because for Cox model, the parameter would be 0. And so we do get back this kind of insights from here. And the result that we get back is in terms of survival generate, not the hazard ratio. Yeah, quick question. So, here your input is the entire phase introductory. Higher physical activity trajectory. Yeah, the physical activity daily pattern, yeah. So hybrid rights only impose the cyclic spines, because here I see the impact difference. Yeah, that's a very good suggestion. For this, we didn't impose the cyclic spines in some of our recent work we did, but we didn't impose the cyclic spine, particularly in estimating this. But yeah, very good suggestion. And actually, we can estimate this h function, and if you look at h, it looks like a lot. And if you look at H, it looks like a log here. So might be this indicates some integration of A3 model. Yeah, so H is something you're actually estimating. Estimated. Are you constraining it to be? It's a monotone increasing function. That's the identifiability criteria. It has to be monotone increasing. But do you have to constrain it in estimation? Yeah, we have to constrain estimation. So we have, I didn't talk about this in detail, but we have to restrict these parameters to be increasing. So we use some deparameterization or can you sublinear and equalize Parameterization, or you can use sublinear inequality constraint to ensure that it's a monotonous function. That's one constraint we have on the age function. And the second application, we looked at this CGM study, modeling time to this multiple hypoclastemia events. Hypoclecemia arises when the blood sugar is lower than the standard range. And for this, we use this distributional idea where we are using this person-specific distribution, UIT. And as a functional predictor in this model that we propose. And we had around 296 subjects at baseline, and we estimated the QIP from a baseline of 10 days from CGM data. And thereafter, in the entire follow-up, we are trying to model time to these adverse outcomes. And we had around 260 hypoglycemic events around this follow-up by the end of the study. And this is the model we are using that we are just adjusting for age and gender. Just adjusted for age and gender. And again, the primary exposure here is the person-specific distributional representation of CGM that we are trying to use to predict, trying to model event outcome. Okay, so again, if I look at the QIP for the two groups, the event group and no event group, so this kind of corresponds to the Washer Strain very center of these two groups, the average quantile function. And we see there is some difference at this point in the exploitatory that in This is clearly explanatory that throughout the different quantiles, there is some difference between these two groups. And this is the estimated beta p, the coefficient which carries over these quantile levels. And you do see some significance here, particularly putting negative weights in this region 0.8 to 9, 0.2 to 3, and positive weights here. So this means beta is now a constraint, the contrast between these regions because it weights Q by P negatively in these regions and positively in the And positively in the earlier quantiles. So, a high negative value of this contrast would mean it is putting kind of larger weights in this 0.223 and 0.829 region, and that would lead to a longer survival or longer time to event outcome in this case. So, we get kind of a contrast effect, or capture kind of a contrast effect compared to using the mean CGM or just using the AI A1C. AI is A1C. And we also compare the predictive performance of this model using painful process validation. And here is what we see, which we see some gains compared to some standard models. For example, if we just use HBA1C, this Cox model, the C index perspectivity rate was 0.56. So from that, we see kind of a large gain. And if you just use a Cox model even, which is distributional idea, P by P, the C index was like 0.626. The C index was like 0.626, and if we use the mean CGM value, so that's also like around 0.56 range. Compared to all this, we do see a predictive gain using this FTDM model and this combination of this distributional idea, the person-specific distribution of CGM instead of using the mean. I'm not sure if you have a combination already or not. Um but if not and you want um it for the clinical audience, I would strongly suggest you retime the low range. With time-to-low range. The style and range is what is more commonly used in time-to-lows range. And because you're looking at the hymoglysting event, I think from a clinical perspective, it would be very interesting if you can be time-to-low range with this model rather than the CD. Yeah, yeah. So, this is we looked at time below than the standard range. I mean, we defined hypoglycemia as time below that standard range. Yeah, yeah, but I mean, for the baseline data, you can compute that metric. Baseline data, you can compute that metric. Right. So rather than comparing with the BNP. Oh, okay, okay, I understand what you're saying. Just use that metric and compare. Because that's what they use on clinical practice. Right, right, right. I think that would be clinicians, that just would make it more better competitive than you have standard comparison. Yeah, definitely a good idea. Yeah, no, I mean, you did a lot of fingers, this is more just like I think for that audience, that would be more right, right, definitely. I think probably I like we should include that. Yeah. Should include that, yeah. Here also, we get age estimate, which is and this is more pumped than a long log function. The parameter R that the error distribution is again kind of non-zero, so that indicates a departure from the Cox model. So, so coming to the end of the talk, so we kind of proposed a kind of a flexible survival model for functional data with right sensored outcomes, and we provide some inferential and theoretical results here in this paper. Results here in this paper. And this is a flexible alternative to Cox model, but you should obviously try the Cox model as I have tried just to see if you can get prediction benefits or if the proportional assumption is not there, you can check in the schedules. Then this model might work out better for you. The application was shown on digital health signals. We have our code available, public available in GitHub for this project. And yeah, this was recently published, but we are definitely thinking of some extensions. Definitely thinking of some extensions. So, definitely some of the comments that you'll give, definitely try those going forward. And some of the distributional methods that I have mentioned that we have also freely available code. So, if you have data that you want to try these ideas, please feel free to use those and I'll be happy to talk with you about those connections. Thank you. Thanks. I was just going to say, I just want to say, can you throw a link in Slack to your paper? Yeah, sure. Questions? Yeah. So, great work. I have a question about your performance metric. So, BMC is a wonder have you considered the calendaration, right? BMC is kind of about discrete. Can you say that? Can you say how long it was about short of survivals? I don't know what's the exact measure you're going to use. I'm just curious. Right, yeah, that's a very good question. Actually, we didn't use AOC, we used the C index, but we could use something like a time-varying AOC and the B-R score. So, those are some of the other indexes or metrics we could possibly look at. Matrix we could possibly look at for better feeling and better sense. And here, like, for example, even with this distributional QIP, and it has been shown, although in this work I didn't try timing range, but QYP, this report density ideal metric has been shown to be more sensitive than using some of the other metrics like timing range. So I suspect it works actually quite well compared to just HVORC or those kind of metrics. Because we did see some gain when we tried this with the distributional representation. Is with the distributional representation and the cost model. So it seems like, so you have smoothing on the functional covariates, right? And then you also have like a constraint smoothing on the h function that you're asking for. Right, right. So actually the smoothing is not explicit here because we don't impose any smoothing penalty, we just control the number of basis functions. In that way, there is like an implicit smoothing penalty. Okay, so you're not penalizing it. Penalizing the roughness, yeah. And for the base, for the coefficient functions or for both? For both, actually. We just put them as in and kind of in a data-driven way, we use the truncated business approach to use those. Okay. All right. Thank you very much. Thank you. 