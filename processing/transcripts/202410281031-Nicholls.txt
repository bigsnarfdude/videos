Here for inviting me. It's a real pleasure. And it's kind of nice to give one of the first talks. It looks like they want to get out of the way, and then I can just sit back and pony. Unfortunately, I follow one of the best talk givers, so it's going to be a step down, but I'll try my best. So yes, I've got a long, long title, but I think all the words need to be there. My daughter was looking at my slide from the title before I left, and she's like, Dad, there's just way, way too many words in there. So, collaborators on this project are John and Jinyu. And at last, we've submitted the paper on this work after quite some time. And this is built on some of the ideas that they were thinking about in these papers in 2021 and connected to some of the work I did with Fernando Reisick back a long, long time ago after I had a mark. So. So, I thought I should put some pictures into my slides. And so, you do a Google search on solitary waves, traveling waves, and you start to find all the pictures that everyone else puts in their talk. So, this is from the Wikipedia page for a solitary wave. And what I just want to illustrate here is briefly what can water waves look like. So, this is in a wave tank. This is a solitary wave. This is also from Wikipedia. From Wikipedia, interestingly, for a page on the canoidal waves. These are waves out in the open ocean, and you know, there's lots of little ripples, but the overarching effect that you're seeing is it looks like a periodic wavetrain where it's periodic in one direction and laterally invariably. So these look like what I would call two-dimensional traveling water wave patterns. Other people like to call these one-dimensional. And then here's another picture from that, say, Wikipedia page from a point in France next to this lighthouse, and you see the waves coming in from the ocean, and now you're seeing not periodic wavetrains invariant in one direction, but interactions of these wavetrains. And now they're forming these square, rectangular, hexagonal patterns, maybe. If you start counting things like this, as maybe. Is maybe having a little bit of a wave crest-like character. Then you talk to Bernard, wait a bit, and he shows you a picture of something that's very different than any of this. So this is DOM 1D, this is.2D. So there's some structure there, obviously. But the structure is really rather complicated. And so this goes back to John's point. If you're flying over the ocean and you see waves, they're not periodic. They're not periodic, and they don't decay to infinity. So, what do they do? Those boundary conditions, I don't know, they're fine to work with, but they're not really what you're seeing. And here's another example of such waves. And so, I want to talk about the sorts of wave patterns which are allowed, and I want to think about them in the context of my old friends, the Dirich-Legend Neumann operators. So, the Dirichlet-Neumann operators, or Dirichlet-Neumann maps, Operators, or Diracleton Neumann maps, or Steklov-Poin-Curry operators, come up in many, many problems of applied interest. So I think about them a lot in the context of boundary value problems, where you're scattering linear waves by irregular structures, so acoustic waves, electromagnetic waves, elastic waves. And here the difficulty is that oftentimes the structures are very, very rough. It's corners. And so it's a challenge for theory and for computation. But they also come up in. But they also come up in free boundary problems like obstacle problems or the sorts of problems that have brought us all here to this beautiful setting at the lovely mountains. Moving boundary problems, so the water wave problem, free surface evolution of an ideal fluid on the effects of gravity and capillarity. And so why are they so useful? Why did they come up? Well, for a number of reasons. I use them to, first of all, reduce the dimension. First of all, reduce the dimension of the problem that I'm thinking about. If you've got a water wave and the fluid is pretty homogeneous, it seems a little silly to solve in the entire depth of the fluid. You really should be able to solve just at the interface, and then you can save yourself a lot of time and trouble by doing that. Now, you don't get any free vectories. The Europlatron operator is a complicated thing to work with and to compute, so. About that as well. I also use them for domain truncation. And so the water wave problem, well, again, there's a bottom to the ocean. And so it's not such a big concern. But somehow, if the waves are at the interface and the water is very, very deep, again, it seems a bit silly to consider going all the way down to the bottom of the ocean. Maybe there could be a subdomain of the full domain, which is much, much thinner. Of the full domain, which is much, much thinner. In these boundary value problems, genuinely, many times, what you're doing is you are eliminating a structure, and the waves come from infinity and scatter off to infinity. So if you're going to do any computation, you have to truncate. So everyone here, I think, knows these governing equations, but I'm only the second speaker, so no one's quite bored of this slide just yet. Governing equations for, say, deep waterways are that Water weights are that you're looking for a potential flow. So the velocity end up with the velocity potential is equal to zero. And then you've got two boundary conditions at the moving interface. Knomatic condition, which is the particles don't fly off the interface, stay on the interface. And then the second equation really contains all the fluid mechanics. That's the Berducci condition. The velocity of a fluid particle. Velocity of a fluid particle inside the fluid domain, if you want, it can be recovered from the gradient of the velocity potential. And then you supplement this with initial conditions. And in fact, it suffices to specify the initial shape and then just the velocity potential at the interface, at initial shape. But you're still not going to get a unique solution unless you specify some more boundary conditions. So for a water of infinite depth, Order of infinite depth. You just want to say that something goes to zero. And it's not necessarily the velocity potential, but it's the y derivative of the velocity potential needs to go to zero. And then, the content of this talk, which oftentimes is kind of brushed aside, what lateral boundary conditions do we need to satisfy? So, typically, what people do is they say that things decay as x goes to plus or minus finity. The phase eta goes to zero, the velocity potential goes to a constant, that's for the gradient of the potential that matters. Or you say that things are periodic with respect to sunlight. But I've tried to argue with my sequence of pictures at the start that perhaps this isn't the right choice. Perhaps we need something different. And perhaps a contender for this is some sort of notion of quasi-periodicity. And so John's been thinking about this for a long time. About this for a long time. And I read some of his papers, and I thought, this is a great idea. Sign me up. So I wanted to make a contribution regarding Dirac Legion Neumann operators and see what we could say. So DNOs are useful. So one way you can use them to make your life better is to reduce the dimension of the problem that you're considering. So in this Considering. So, in this incredibly important paper from the late 60s, Sakharov pointed out, demonstrated that the water wave problem was many things, but a Hamiltonian system, and that the proper conjugate variables were the shape of the interface and the velocity potential at the interface. And so, while he didn't say it explicitly, he was hinting at how you could rewrite the water wave equations in terms of. One-wave equations in terms of a Dirichlet-Neumann operator. So, all you need are these interfacial quantities. And so, Craig and Soulin, in their 93 paper, made this all very explicit and wrote down the Dirac Latin-Neumann operator and gave it a letter, like a little G. And so if you set yourself the following problem, solve Laplace's equation beneath this interface eta, and why don't we just fix time for a moment? Specify Diracle data at this interface, call xc. Specify a boundary conditions, you go to minus infinity. And then provided you have good lateral boundary conditions with this, you're going to get a unique solution to this problem. And from that, you can compute the Neumann data. You choose the gradient of phi dotted with a normal, which is not normalized, because that fits in best with the governing equations. With the governing equations, and this operation appears to be you can rewrite the governing equations for the water wave problem as dA to dt is equal to the variation of the Hamiltonian capital H and d c dt is equal to minus the eta variation of the Hamiltonian where the Hamiltonian is written down here as done by Craig and Soulet. As done by Craig and Sulem, one-half integral CG, which depends on eta, applied to xi plus g eta squared. And now, while it's not too terrifying to compute the c variation of this, computing the eta variation involves the eta variation of the Dirich-Lage-Leumann operator, and that's a bit of a complication, but you can show that you get interfacial governing equations which are equivalent to the wide wave problem. To the wide wave problem. Yeah. Has that done an integral over the dx? It's the integral over of x of the y. It depends on the boundary conditions, it should be. Oh, yeah, so in the case of quasi-pereonic, right, I think it would be infinity. I'm sorry. You might have to lift it to a higher dimensional tourist. Probably. Yes. All right. Dairy-like genomic operators can also be used for domain truncations. Truncation. So if you are in a problem where y going to minus infinity is genuinely a place you need to think about, what you can do is you can take your problem, so the Dirac-Leg-Denum operator problem, and say, all right, I'm going to put in an artificial boundary. I'm going to put it down low enough so that it is never going to interact with the free surface. And I'm going to break the problem up into two bits. Into two bits. So I'm going to solve Laplace's equation below this artificial boundary at y is equal to minus a, subject to Dirac Lay conditions being specified there, and then whatever the appropriate conditions at minus infinity are and lateral boundary conditions. And then if you provide me with this psi, then I can compute a second Nira-Pejon operator, which will map psi to the normal derivative of w. Of w, I don't know, say pointing on, or pointing at, whatever you'd like. At z is equal to, or y is equal to minus a. And so this problem can be solved exactly because you're on such a simple domain. It's been solved using separation of variables. And then what you can say is that the Dirac Legal Meuman operator governing problem is equivalent to solving Laplace's equation from minus A up to eta, subject to the Dirac Leg condition up at the top. Subject to the Diracling condition up at the top. And then, I mean, it looks like a tautology, but it's the typical thing that you see in a domain decomposition method. That should be a y derivative, on v at z y is equal to minus a minus t applied to v at y is equal to minus a is equal to zero. This just says that the y derivative, sorry, is equal to the y derivative. And so, in this way, now, regardless of whether the fluid domain is of infinite extent or just really, really deep, and you really don't think you need to discretize that whole domain, you can now chop it down to one that's on a much, much smaller domain from eta down to minus a. All right, returning to periodicity and quasi-periodicity. So, while there's just the one notion. While there's just the one notion of periodicity, it seems that there are several ways that you can attempt to extend that to be quasi-periodic. And the method advocated by Moser, this Ice Review paper in 1966, is exactly what John was talking about. So if you have a function which maps Rn to R, then you say it is quasi-periodic, if you can find an auxiliary function, some sort of envelope type function, f tilde, which maps Rd to R, where D is going to be greater than. R, where D is going to be greater than or equal to M, which, the F tilde, is periodic with respect to the lattice of integers scaled by 2 pi in dimension D. And you have a Fulbright matrix K, which is in R d by N, so it's going to be a pole matrix, so that you can express the function f of x. Express the function f of x is equal to f tilde of alpha, the same alpha that John had, which is equal to kx. So this is going to allow you to do exactly what John was talking about. Express functions which have this enhanced notion of periodicity to quasi-periodicity. It doesn't take to convince yourself that you can compute the x derivatives of f in terms of the alpha derivatives of f. In terms of the alpha derivatives of f tilde, with a simple formula, it's given by k transpose grade alpha of f tilde. And then the Laplacian we're also going to need. So divergence gradient. That's divergence, and you get a k transpose, gradient of alpha in the middle. So k transpose is a square matrix, but it is not invertible. Which has consequences when you try and prove a theorem. Consequences when you try and prove a theorem? So if d is equal to n, then in fact you're in the periodic case. If k is full rank, k is square, it's invertible, and the lattice of period design is characterized by the k. Now, if you don't choose wisely, even if d is bigger than n, you could still have a periodic function. So for instance, if you choose your k in the n equals 1, d equals 2 case, In the n equals 1, d equals 2 case, to be 1 q over r, where q and r are integers. You can do a little calculation to demonstrate that the function f, in fact, is going to be, well, not 2 pi periodic, but 2 pi r periodic. So you don't want to choose rational entries in your matrix if you're going to have something that's genuinely quasi-periodic. So what we're going to do is we pick matrices which have entries featuring irrational numbers. Featuring irrational numbers. And so this is going to be something genuinely quality periodic. And so here's a picture from one of John and Chinu's papers, where they display the outcome of one of their simulations, which is quasi-periodic. And then they have this lovely picture, which I think describes everything. So here, they plotted the level sets of the function. sets of the function eta tilde, which is bi-periodic. That's not really the physical quantity. The physical quantity is you want to travel sort of along this function, starting at the origin and going along with a slope of, I think it's 1 over root 2i, and as you travel along, you come on the other side, and you just keep going. Over here, and you sort of sample the entire landscape of this function. And because you're at an angle that is an irrational number, you never actually come back to where you started. So that's what we're going to be after, computing these eta tildes. And that's the genius of the approach and the method. So we want to do numerical simulations of Dirac Latinoim. Of Dirichlet-Neumann operators with these sorts of geometries, so that we can solve water wave problems, maybe electromagnetics problems, or any number of other things. And so the problem with these quasi-periodic problems is that cooking up numerical methods for a quasi-periodic boundary condition, it's not obvious what to do until you adopt this framework, and now it becomes really simple. Really simple, what you should do. So, what you should do is you should not solve for, well, you should not directly estimate the eta and the xi and the nu, the shape and the Diraclet data and the Neumann data. What you should numerically estimate are these envelopes. But the envelopes satisfy really rather straightforward boundary conditions. They're just too high periodic in every direction. The cost of this, of course, is that now you are in a larger. Now you are in a larger dimensional setting. So if you want to do something that's quasi-periodic for the 2D problem, well now you're doing a 3D problem. And I believe Jimmy is going to show some numerics for the 3D problem. And the envelope function depended on four variables. So this is a good calculation. Alright, so the thing to do is to abandon the untilded variables and now focus on solving the problem with the tilded variables. And now it becomes important that you be able to discern what happens with the x derivative and the Laplacian. So these are what the governing equations now look like. Laplace's equation for this new envelope. Derek Lay conditions, this transparent boundary condition. This transparent boundary condition, which enforces the boundary condition for the behavior, as y goes to minus infinity. And now the v tildes are periodic with respect to I times Z to the D. This, you can compute what the Neumann data would be. And this Dirichlet Neumann operator maps the C tilde to the Nu tilde. Now, you can imagine all sorts of numerical methods. Of numerical methods based on any of your favorite approaches: finite differences, finite elements, spectral methods. As I argued earlier, perhaps for a problem where all the action is at the interface, these are going to be a bit wasteful. So perhaps we should do something a little bit more designed to the problem at hand. Boundary integral, boundary element methods are certainly used, and John talked about. Are certainly used, and John talked about one of those. Conformal mapping methods also get used, and John talked about one of those. I sort of specialize in these high-order perturbation of surfaces methods. And the idea is that you build up a perturbation sequence based on the observation that for many problems of interest, this eta tilde is not going to be too big. And you sort of circle back and demonstrate that, in fact, these methods are going to work even when eta tilde is rounded. Even when eta tilde is rather large. And the method and its analysis is based on rigorous results, which we've produced in our paper, on analyticity properties of this diary denominator operator. So under what conditions on the eta xc do these perturbation expansions actually converge? So this is what we proved. Great effort. That was great effort. Then an integer s greater than d over 2, those became alternate properties of these Sokolev spaces. If F is in the space Hs plus 3 halves, so the normal L2-based Sobolev space, and if C tilde is in Hs plus 1, then this series, the Dirich-Latin-Neumann operator, actually converges strongly, is an operator from Hs plus 1 to Hs. Plus 1 to Hs. And you get an estimate on the size of the nth term in the series in the Hs norm. And it's bounded by the Hs plus the norm of C tilde, A1, which contains a number of factors from our estimates, and then B to the N. And the B is bigger than the Hs plus 3 has norm of F tilde, and that tells you how large the domain of band. Tells you how large the domain of analyticity is, or how large the disk of analyticity is, based at the origin. And so our high-order perturbation of surfaces Hopps method for approximating the Dirichlet-Neumann operator is you just take this series and you truncate it after so many terms and then call that your approximation. So it was a number of methods for accomplishing. For accomplishing this ops scheme. And let me just show you this one briefly that Fernando and I worked on now many years ago. So the problem with these irregular boundary problems, with these moving boundary problems, is what do you do with this irregular or moving boundary? And so one approach that we've taken, which works well in either two or Which works well in either two or three or any number of dimensions is to do a simple mapping, which goes by the name of the C method in electromagnetics or sigma coordinates in atmospheric sciences, where you just find a map which flattens out the top and leaves the bottom flat as well. And so what you have after you do this mapping is you have a problem now posed on a very simple domain that lies from minus a. The lines from minus a to zero. We theoretically and Neumann conditions basically preserved. Laplace's equation essentially preserved, except you get additional terms from this change of variables. And so those all get lumped into the f tilde and the j tilde. And they're a little bit complicated, but they're not that bad. And the important thing is they depend on eta tilde and u tilde. On eta tilde and u tilde, but they depend on eta tilde at least to the first power, so they are higher order corrections to what's on the left. So, this is how the scheme is going to work. You expand everything inside in a Taylor series in the magnitude, the size of eta tilde, so right, eta tilde is epsilon f tilde, and then write down what do the u n's need to satisfy. And it's a sequence of boundary value problems, which are inhomogeneous, which is a little bit. Problems, which are inhomogeneous, which is a little bit inconvenient. But the point is, on the right-hand sides, are corrections that you already have. So if this is F, this is Fn, you will have Un minus 1 because you've got one power of epsilon of that eta tilde. So we implement this with a high-order spectral method. You do, well, it's periodic in the alpha variables. You might as well do. The alpha variables. You might as well do some sort of Fourier method. And then in that little slice where you go from minus a to zero, you do a Chevyshev collocation approach. The Dirac Latino operator. And the way we do it is using what we call the method of manufactured solutions. You've got a problem, you can code it up. You can coat it up and probably just as easily out a version which will handle an arbitrary right-hand side. It's probably just as easy to compute the inhomogeneous solver as it is the homogeneous solver. And so what you do is you think up some solution to this problem, and you just apply the differential operator p to it, and just call that f to make that a solution to this problem. So this was the solution that we thought up. It kind of looks like Solution that we thought up. It kind of looks like what a solution to this problem would be. And then for a collection of epsilons, heights of the interface, we do numerical simulations with two methods I did not talk about, and this new method, which we've worked on with Fernando, which resulted in the proof of the analyticity. And you see that whether you sum things up just as a regular Taylor series or whether you use Paday enhancement. Or whether you use PADE enhancement, after a small number of boundary perturbation terms, you get really good answers. But then if you want to do larger perturbations, it gets to be a bit more challenging. And what you find, if you just sum up the Taylor series as is, you will eventually have divergence. So there are some issues here that I've not even talked about, where you have instabilities due to. Where you have instabilities due to some numerical ill-conditioning. And the point is that this new transform field expansion approach overcomes these, and you still get steady convergence, even though the size of this perturbation is quite large. Let me just do one last calculation with a rather large perturbation, where it took not only our new approach, this transform field expansion approach, but also PADE summation, PADE approximation. The approximation. We approximate truncated Taylor series by rational functions in order to get good convergence. So, yeah, at the end, we were very pleased. We had a theorem where we showed that this TFE approach would converge. We were able to implement it, do these numerical simulations, and either for rather large deformations, get reasonable numerical results. Okay, so I should probably stop there. 