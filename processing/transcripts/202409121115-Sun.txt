And I'm Sunya from Peking University. I'm a PhD under Professor Mohanchung, and my topic is machine learning-assisted kinetic energy density functionals implemented in ABAX. And first, I will introduce something about ABAX. ABAX is an open source initial software initiated by Software initiated by Professor He Li Singh of USTC in 2006. And the first work was released in 2015. And in that year, Professor Yunxing Guo was involved in the development. And in 2022, 2021, ABAX joined the GitHub and cooperated with Professor Erwina in the different modeling community. And this year, AI for Science. And this year, the AI for Science Institute was established with Dr. John Linfeng as president. And the electronic structure team focused on the development of ABAX. And although there are so many softwares are still already available, in our consideration, EFT software is still worth building for several reasons. First, the rise of AI offers new opportunities. DI offers new opportunities for the development of algorithms. And second, we want to enable DFT software to migrate more easily across different platforms. Third, the field of DFT algorithms remains in continuous evolution. And as a result, the goal of ABAX is to develop an open source, feature-rich, user-friendly, and developer-friendly software. Developer-friendly software. What's more, from 2021, Abex began embracing the open source philosophy under the different modeling community. And for now, many functions have been implemented into ABAX. For example, ABAX supports both plane wave basis and the numerical atomic orbital basis. And to the numerical atomic orbital basis, To the numerical atomic orbital, we can deal with the larger systems, for example, containing more than 1,000 atoms. And besides Kung Sham DFT, ABAX also supports stochastic DFT, orbital-free DFT, and real-time TD. And for exchange correlation functionals, we support LDA, GGA, Meta GGA, and hybrid. And there are some extra functions. And of course, we can do structure automation. And of course, we can do structure optimization and molecular dynamics in various ensembles. And besides functions implemented in ABAX, we also support several interfaces to other programs, for example, to ASE to set up the simulations and to Phonopy to deal with phonal and related properties and to Vanya Netty to solve the maximally. To solve the maximally localized vanilla functions and to have ATP to do an initial test boundary simulation, and also to TP2G to solve the magnetic interactions. And Alex also supports several AI-assisted algorithms, including the DIF-MD and DIP-KS, which is a machine learning-based exchange correlation functional, and the DIF-H, which is And the deep H, which is machine learning Hamiltonian matrix, and also the deep tight bonding approach. And others can be downloaded from this official website. And the version in development can be found in GitHub. And the normal conserving pseudo-potential and the corresponding numerical atomic orbitals can be found in this library. Okay, and also an online document is available on this website. And the auto-free algorithm has been implemented in ABAX to do charge density optimization, molecular dynamics, and cell relaxation with the help of VLPS and high-quality local cell potential proposed recently. For now, ABEX supports. Now, ABEX supports five different kinetic energy density function laws, including the Thomas Fermi, which is exact for the three electron gas, and the Mohma cycle, exact for the one electron and the two electron systems. And also the TfÎ» Vw, which is a linear combination of Thomas Fermi and Mohma-Lucker. And the fourth one is LQT, which is proposed by Professor. Proposed by Professor Rokai and Professor Kawasif, Professor Tricky in 2018. And it takes a brief formula. And the last one is one TTAKDF, which is a non-local functional and works well for simple mentors. Okay, it's time to go to the machine learning assisted functional. And as we know, the As we know, the kinetic functionals can be classified into two categories. The first one is local and the semi-local functional, whose kinetic energy density is a function of charge density and the gradient, the Laplacian of charge densities. For example, the Thomas Fermi and the Mohmann Darker. And the second category is the non-local function whose kinetic energy density is a function of charge density. For example, Charge density. For example, the WTKDF, which introduced a kernel function to introduce the non-local information. Sorry, in general, the non-local functionals are more accurate than local and semi-local ones, but are less efficient. And another way to construct the functional is through the machine learning technique. And before developing the machine learning functionality, we have listed. function loss we have listed three criteria criteria first the non-interaction kinetic energy is highly non-local so we wish to introduce the non-local information into the machine learning model second although we do not know the exact formula of the non-interaction kinetic energy we do know some physical constraints which should be exactly obtained and several works in the uh Several works in the field of machine learning exchange correlation functionals have proven that introducing the physical constraints is helpful to construct an accurate functional. And the third, the computational stability guarantees the practical value of a machine learning model. And in recent years, several machine learning function laws have been proposed, and they can also be roughly classified. Can also be roughly classified into the semi-local and the non-local. And the several semi-local functionals surpass the traditional semi-local functionals. However, due to the lack of the non-local information, they cannot match the accuracy of non-local functionals. And in order to introduce the non-local information, some models utilize the convolutional neural network and some models. network and some models project the charge density onto a set of bases and use the projection coefficient as the descriptors however the physical constraints are commonly negligent by most of them so our purpose is to propose a machine learning model which meets all the three criterias and so let me introduce what So let me introduce what is physical constraints. And we know in the field of OFDFT, we can define the poly energy and the difference between the non-interaction kinetic energy and the moment cycle K DF. And the corresponding potential is so-called polypotential. And here we list three properties of polyenergy and polypotential, including the scaling law, the non-negativity of polyenergy density, and the in. And in the free electron gas limit, the polyenergy and polypotential will return to the Thomas Fermi model. And in our work, we define the enhancement factor of the polyenergy as the output of neural network and to facilitate the incorporation of physical constraints listed here. And in the framework of KSDFT, we can calculate. Of KSDFT, we can calculate the poly energy and the polypotential exactly with the help of the well functions and occupation numbers and so on. And so the training target is easy to get. And next, oh, sorry. Next, the non-local information is introduced by our descriptors. Here we list four descriptors. The first one is semi-local. The first one is semi-local, which is the reduced gradient of charge density, and it is commonly used in the DFT, for example, the exchange part of PBE functional, and in the gradient expansion of kinetic energy. And we define a non-local version of P by introducing a kernel function, which is similar to that used by WTKEDF and shown here. Here. And what's more, we define other two non-local descriptors to capture the fluctuation of touch density. And it takes a little complex, but we can divide it into two parts. The numerator can be regarded as the average Fermi vector. And the dominator is introduced to make the distributor dimensionless. And also we discrete a non-local. Describe a non-local version of CC by introducing the same kernel function. And we note that these four descriptors are invariant under the scaling transformation. So we guarantee the scaling law automatically, as we will show below. And we constructed a training set containing eight metallic systems. What's more, we have tried several other Had several other descriptors and missed here. For example, the reduced Laplace of track density and its non-local version, and a simple version of C, which is this one, and we define gamma and its non-local version. But unfortunately, these descriptors are computational unstable. And here are two examples. This is our first version of machine learning model. motion linear model and it takes the gamma gamma non-local p and the q as distributors and we calculate the energy volume curve of the magnesium and here the the yellow one are yellow curves are obtained by PSDFT and the blue ones are obtained by machine learning model you can see that in many structures the machine learning model cannot get convergent and this is the second example it takes the C non-local and Takes the Xi, C non-local, and the PQ and the distributors, it is much better than that one, but it still cannot get convergent in several structures. So in our following works, we do not introduce these descriptors. And this is the workflow of our model, which is called the machine learning-based physical constraint non-local KEDF and MPN in short. And it attacks the X. It takes the C, C non-local, P and the P non-local as descriptors. And as we mentioned above, the Z guarantees the scaling law. And the non-activity of the polyenergy density is guaranteed by this function. This soft plus function is an activity function used in machine learning. And it itself is non-negativity. So the f is non-negativity. And the free electron. And the free electron gas limit is introduced by a translation of output of a neural network. And so all these constraints are sent to file. And we also note that the potential term in the loss function is crucial to help the model to find the energy minimum correctly. Okay, and here is an illustration. And here is an illustration of the scaling law. We first get a one-state charge density of FCC aluminum and scaled it to the Ru lambda. And then we calculated the kinetic energy with our MPM model. And here, the red star. And the gray line represents the kinetic energy predicted by the scaling law. And all the red stars fall under the gray line. Starts fall under the gray line. So the scaling law is exactly obedient by our machine learning model. And it is because the descriptors are invariant under the scaling transformation. And here is a detailed derivation. Okay. And furthermore, we have explained how to introduce the free electron gas limit of the poly energy. Here we will introduce. Energy. Here we will introduce how to introduce the free electron gas limit or polypotential. First, this is the polypotential of the MPNK EDF, and it is complex, but the free electron gas limit is guaranteed by two facts. First, the kernel functions integrate to zero in the real space. Second, in the free electron gas limit, all the descriptors become zero. And if we substitute, And if we substitute these conditions into this formula, you can see all the complex terms vanish. Only the first term remains. It is the homozo Fermi potential times the enhancement factor. And we know that the enhancement factor returns to one in the free electron gas limit. So the poly and the free electron gas limit or polypotential is automatically sent. Is automatically satisfied. Okay, and so in order to examine the performance of our machine learning model, we have constructed two test sets. The first one consists of four structures of lithium, four of magnesium, and four of aluminum. And we calculate the bulk properties, including bulk modulus, equilibrium boom volume, and equilibrium boom energies with different functional. Energies with different functionals. And we calculated the mean absolute relative error by comparing the results obtained by OFDFT to those obtained by KSDFT as displayed here. Here, the TF lambda, VW, and LKT are two semi-local functionals, as displayed in blue here. And the yellow one, WT and WGC, are two non-local functionals. And the red one is our machine learning model. We can see in all the tests. We can see in all the tests, the MPM model exhibits the accuracy of semi-local functionals and approaches the accuracy of non-local ones. And the second test set consists of 59 alloys taken from materials project. And it has no superposition to the training set. And we calculate the total energy and the formation energies of this alloys. And here is a comparison. And here is a comparison with the KSDFT. The horizontal axis represents the results obtained by KSDFT, and the vertical axis is the OFDFT. You can see that the energies obtained by MPM model is close to those obtained by KSDFT and it exhibits satisfactory transferability in this alloys. And it is very stable in the computational. Computational. And here we display two charge mix or charge densities of allowance. Here, the red line is obtained by the MPM model, and it is smooth, and it is similar to that obtained by KSDFT. Okay, so we conclude that the MPM PDF is accurate, transferable, and stable in the simple methods and their alloys. However, in the same However, in the semiconductors, it is unsatisfactory. For example, we have trained an MP model with 10 semiconductors and calculated the energy volume curve of silicon and zinc blend gallium arsenide. And here, the yellow lines are obtained by KSDFT and the green lines by machine learning model. It is totally wrong. So next, the question is. So next question is how to improve the MTMK EDF. Our first idea is to find a more suitable kernel function to define the descriptors. And we have tried defined a Yukava form. It didn't work. And since the electrons in the coordinate bond are localized, so we think maybe we compress the kernel to form a more local kernel and to capture the And to capture the information from the covalent bound, but it didn't work so either. And so, how about to string to the kernel? And it still didn't work. And according to those results, we found that the machine learning model is sensitive to the form of kernel function, but none of kernels appear to be effective. And our next idea is what if Our next idea is: what if we employ more than one kind of function? And it is the motivation of our multi-channel MPM KDF, which is CPM KDF in short. In this model, we compress and astring the kernel function used by NPM model. And here, a kernel function with a typical lambda defines a channel. A channel gathers the information among a specific Information among a specific scale and transforms the charge density into three non-local descriptors. And the descriptors output by several channels are gathered and feed into the MPM model, which guarantees the physical constraints and we explained before. And finally, we get the enhancement factor. And so the CPM-KEDF integrates. KEDF, it integrates the information gathered from several scales. And we will show that this architecture helps the machine learning model to deal with semiconductors. And in order to evaluate the validity of our multi-channel architecture, we have defined three different models, each utilizing the one, three, and five channels respectively. Channels respectively, and we note that the CPU1 takes the same descriptors to the MPM model. And okay, and we calculated the energy volume curve of cubic diamond silicon and show here. You can see that the CPO1 PDF cannot produce a smooth curve. And with the number of channels increasing, the accuracy improves significantly. Improves significantly. For example, the CPN3 KEDF exceeds the accuracy of WGC, which is green here. And the CPN5 KEDF approach the accuracy of KSDFT and Honkata KDF. And we noted that Hwonkata KDF is designed for semiconductor specifically, and it is a non-local functional. And here we displace the five kernel functions. The five kernel functions utilized by CPU5 KDF, and they head to zero and different radio so they can catch the information in specific scales. And here we display two charge densities obtained by CPM KGFs in the pubic diamond, silicon, and zinc blend gallium arsenide. These two structures are taken from the training set. From the training set. And first, all the CPM PDFs can produce a smooth charge density. And with the number of channel increases, the accuracy of densities increases improves. And specifically, the CPM5 KDF can reproduce the covalent bond efficiently. And even the Honkata KDF on the rest. On the rest image, the charge density in the covalent bond. And in the test set, we calculate the charge density of hexagonal diamond silicon and white gallium arsenide. We can see that the CPM5 KDF can also reproduce the covalent bond efficiently. And this table lists the mean absolute relative error of charge densities for the training set and testing the set. For the training set and test set, we can see that in both training set and test set, the CPO5 KDF yields the smallest average MARE, and it exceeds the accuracy of the Hwonkata KDF in this test. And so in conclusion, we have proposed a new machine learning model named MPN KEDF. It incorporated the non-local information and in And three exact physical constraints. And it approaches the accuracy of non-local functional in simple mentals and their alloys. And what's more, we introduce the multi-channel architecture and extend the MPM KDF to semiconductors and construct a CPM KDF. CPM5 KDF, which utilizes four or five channels approaches. Five channels approaches the accuracy of non-local functionals in semiconductor systems. And the detail, the detailed information can be found in these papers. And in the future, we will try to construct the machine learning model to handle the mental and the semiconductor systems simultaneously. And we also will try to extend the machine learning model to finite temperature systems. And thanks for the help from. And thanks for the help from the Professor Chumo Han and Professor Genghawi. And thanks for the support from the modeling community and ABAX. And thanks for listening. Thank you.