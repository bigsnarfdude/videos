Okay, so let's okay. So, good morning. Let us start. So, we have the pleasure to have with us Masu Khalif from Western Ontario. He will talk about bootstrapping Dirac ensembles. Do you mind if we record your talk? Your talk? No, that's fine. Okay, excellent. So we can see your slides, so you can start. Okay, well, thank you very much. Thanks. I should thank the organizers for inviting me to give this talk. And sorry that I cannot be there. As I was telling some of you, we have a PhD defense tomorrow and it cannot be done at any other place except at the university. So that's the way it is. So that's the way it is. So let's get started with the talk. So it's bootstrapping the rock ensemble, but I will take the occasion to also talk about the overall program a little bit. And I should say I will mention the relevant people who are engaged in the research on this and related areas, but I should acknowledge my Acknowledge my dedicated students. I will name them as the occasion comes who played an important role in all this. So what's bootstrapping? First of all, I mean, as a word, of course, as you know, it means pulling yourself up by pulling your bootstraps or shoestrings. It looks like an impossible task. So that bit. So that be it, but in physics, it has a concrete meaning. Actually, it started as part of the S-matrix program by Jeffrey Chu in early 1960s. So the idea was to use any sort of consistency conditions available to compute various correlation functions, especially to formulate a strong theory, a theory of a strong interaction. Theory, a theory of strong interactions, and the mantra was: particles pull themselves up by their own bootstraps. Because of this consistency, you can reject a lot of other possibilities and you'll be down to certain choices. So it had some success at first. They even predicted, I think, one or two particles. But the idea eventually, I mean, by Eventually, I mean by mid-1960s, died out and gave way to much more successful non-abelian gauge theory, which up to that time, it was thought that gauge theory is not going to, was people actually thinking of abandoning gauge theory completely. But then things completely changed over a few years and gauge theory became the as you know, became the primary tool of standard model. Of a standard model. So then again, very recently, like in 2008, there was big progress by Ryzchkov and his group of collaborators. Well, they were very successful in not solving, but getting very precise information as part of the conformal bootstrap program for conformal field theory. Conformal field theory in 3D. In 2D, two dimensions, it was already used and pioneered by Pelyakov, Polyakov, Zomolochikov in the celebrated work on conformal field theory in 1983, around that time. So the difference between dimension two and dimension above is that, of course, in dimension two, you have an infinite dimensional Lie algebra of symmetries. Lie algebra of symmetry is the Virasora algebra, so that gives you many, many constraints, and that's easier to solve compared to three dimensions that has a finite dimensional group of conformal symmetries, and that's harder to solve. Okay, so that's that background. But then in random matrix theory and lattice gauge theory, the idea was revived recently in two works: first by In two works, first by Anderson and Khrushchevsky, Lattice Gauge Theory. And in random matrix theory, as far as I can tell, it was Henry Lin in Princeton who started this. So he applied Bootstrap to do some reasonably simple, but beyond the known cases, get information on those random matrix models. When I saw the paper, the paper. The paper, the paper, and what he was doing, it occurred to me that for problems that I was thinking about, we were thinking about, might be a very useful idea. And indeed, it turned out to be the case. So with Hamed Hassam and Nathan Pagliaroli, so we managed to do some work that I will describe. And almost simultaneously, Kazakhova and Zheng also used the idea of bootstrap. Zheng also used the idea of bootstrap in some random matrix models that they were interested in. So recently, the most recent work, there is a cubic Dirac consultant that I will briefly mention. This was pursued by Hassan, Pagliaroli, Verhoeven, Luke Verhoeven, and myself. So I will also describe this a little bit. Now, for your Now, for your information, I should say that the idea of bootstrap does not end with matrix models or lattice case theory or standard model, conformal field theory. In fact, it has been found, and that's really amazing, it's very useful to get very precise information on Laplace spectrum of hyperbolic manner. Spectrum of hyperbolic manifolds, two-dimensional hyperbolic manifolds, Einstein manifolds. They show that the eigenvalues and eigenfunctions of Laplace performing operator satisfy some consistency conditions and they can even get precise estimates. For example, the not Sarnock, but something which Sarnak worked on, estimates on, I think it's called On estimates on, I think it's called one over four conjecture for first non-zero eigenvalues of Laplacian on hyperbolic manifolds. They can verify that numerically at least to large, large precision. Okay, so now that my talk, it's about, yeah, so pass integrals in non-committing. Pass integrals in non-commutative geometry or quantum gravity, if you want. As we know, the object of interest in quantum field theory of any kind is expectation values of some observables. So if you have some classical observable or its quantum expectation values, can be computed, proposed to be computed by a pass integral. By a pass integral, when d is the measure that doesn't exist mathematically, we know that. And S is the action, and that's the recipe, Feynman's recipe for computing observed expectation values. So this functional integral, I should say, functional integral. Integral, I should say, functional integral rather than pass integral, has been found to be renormalizable, as you know, and gives finite values for Yang-News' action of the standard model. And also in conformal field theory and statistical mechanics, it's a very successful gadget and it gives results. However, for the Einstein-Hilbert action of classical gravity, where the action is your The action is you're integrating scalar curvature R against the volume form. This action is actually the pass integral corresponding pass integral is highly non-renormalizable. I mean the most daring physicist even couldn't make progress on that. So there is a book by Feynman. Well, he records his struggle with this. I think he His struggle with this. I think he gave 25 lectures in the early 60s, and it's quite a fascinating read how he thought he could perhaps solve this problem, but at the end, nothing came out, more or less. Okay, so but then people, of course, as you know, try different ideas, and one idea is to use non-commutative space-time, so non-commutative spaces, and there. And there, there is a formula due to Alan Kahn, Kahn's distance formula that recasts the metric, the geodesic distance in any Riemannian spin manifolds, for example, in terms of the Dirac operator by this formula, is the supremum of that quantity. That quantity. So it's a formula that's like a dual to the classical formula, which you take infimum of the lengths of all possible curves connecting P to Q. So instead of probing the manifold using curves that you put into the manifold, you probe the manifold by studying waves on the manifold, which are functions that you define on the manifold. That's a totally dual picture. Under market. That's a totally dual picture that exists. But the point of the dual picture is that it can be extended to non-commutative settings. And the extension is something we know, is Kahn's notion of non-commutative Riemannian manifolds or spectral triples. There are some technical conditions that have to be satisfied. It doesn't concern me right now because I will go to finite dimensional spectral triples right away. So this suggests. Way. So, this suggests that Dirac operators are like metrics. So, we can take them as dynamical variables of classical gravity, like g mu nu. We can replace it by d and try to formulate quantum gravity in terms of d. So, then the idea is that, first of all, we don't want to do the actual gravity, you please. Actual gravity is a Euclidean gravity because this whole machinery of NCG is really patterned around positive definiteness of the Euclidean or Riemannian metric. And the corresponding Dirac operator is only in that case, it's elliptic and has nice properties, discrete spectrum, stuff like that. That's one thing. And the second of all is that you have to start from somewhere. You want you have to start from somewhere, and you start from finite-dimensional non-commuted manifolds. So, just to bear a very simple example in mind, you can take just your algebra to be n by n matrices and your Dirac operator to be anticommutator with the Hermitian N by N matrix. That's pretty good to start with. So, Hn is my notation for Hermitian N by N matrices. Matrices. Okay, so then using this, John Barrett, and he pursued it with L. Glaser later on, but I think the model was proposed by John, is to define some sort of toy model or whatever you want to call it for Euclidean quantum gravity. So again, as I was saying, you replace integration over matrix. Integration over metrics to integration over a space of Tirac operators. Oh, there's a typo here, I'm sorry, of a finite real simple spectral triple. It means things like NN of C and such Tirac operators. There is a classification of these things depending on signature of the Lorentz metric and the action of the reality operator J and epsilon, these sort of things, that does not concern me right now. Concern me right now, but basically, this is the recipe: you replace integration over metrics to integration over Diracs, and for finite-dimensional spectral triples, the set of all acceptable Dirac operators is a finite dimensional space. For example, it could be Hermitian and Bayern matrices. And so that's why we call it Dirac ensembles, because these are ensemble. Dirac ensembles because these are ensembles of Dirac operators. You treat them now statistically. Okay, so then you need an action. The actions is like in Landauginsburg theory and in all statistical mechanics, there's a very natural action. Mind you, that we put G, the coupling constant, in front of kinetic term, trace of D squared, not in front of trace of D4. Of trace of default, there is a good reason for that, but that be it. So, this is our action. Um, so when you see this Dirac operator has some relation to your permission matrix H. For example, in simplest cases, when you write down your Dirac operator in terms of H, and then when you write down your action in terms of H. Action in terms of age, you get some kind of crazy-looking terms that usually in random matrix theory people don't study because unless you have a good reason. So basically, what happens is that you have multi-traces, so products of traces. That usually does not happen in random matrix theory, at least for a standard cases. Mod standard cases. Another thing is, which is even more complicated, is that immediately after you pass the first two simplest cases, absolutely simplest cases, you run into models which have two or three or more matrices. So they're called multi-matrix models. So immediately you run into multi-trace and multi-matrix models, and that's a real problem. Problem. So, nevertheless, Paratank Laser studied these things using a Monte Carlo Markov chain and they witnessed phase transition in some cases and some other interesting features. So, my idea was to treat these things analytically. So, this was the beginning of the project. So, as I was saying, typically, was saying uh typically models in uh random matrix theory of the form there are one trace there's trace of some polynomial in age single matrix single trace and there is a quite well understood theory for these things but beyond this again uh doesn't it doesn't exist so now let me tell you uh what are the objects of interest in this in this so one of So, one of the objects of interest is mean value, mean eigenvalue, density function. So, you have these matrices, each deterministic matrix has some n eigenvalues. When you take ensembles of matrices, they have a distribution. We call this mean distribution by rho sub n of x. That depends on the size of your matrices. It's a curve of finite support. It's a density function, a distribution function. So, computing this is the first step, but usually you don't see anything here. You want to let n go to infinity to see some universality, to see some pattern in the limit that repeats itself. Okay. And usually, the way you do these things is by looking at limits of traces of power. Of traces of powers of your Hermitian matrix, expectation values of such things. So these large end limits, properly re-escaled, exist. And if you have a good understanding of these moments, then you can have a good understanding of the limit distributions. So one big thing now, it's a bit of detour here, but I want to say it one big major One big major progress that happened in the subject was the work of First Beit Hooft, which was totally interested concerned with the strong force in mid-70s. And the work continuation by Berzin, Itsexun, Parisi and Zuber, they found a very nice relationship between these random matrix models and the geometry of Riemann surfaces. Uh, Riemann surfaces, the topology of Riemann surfaces. So they discovered, actually, Tuft was the first one, discovered the expansion, the Feynman rules for computing these things perturbatively. So Zn is an integral, which is in general divergent. To compute it perturbatively, you have Feynman rules, and they discover the Feynman rules basically what they are. Instead of summing over graphs, you sum over surfaces of various genus. Of various genus. And the weight, the Feynman weight of each surface is defined in terms of those coefficients Tk of your polynomial, your potential function. And that basically tells you how many polygons you have to use. So basically, it becomes a combinatorial problem computing if G. Problem computing means you have to compute certain decompositions of surface of G and S G in terms of given polygons, the polygonalization count. So this was our first thing. Of course, as I said, the models that you immediately see, they are multi-matrix and multi-trace. So there is a theory, modern, more modern theory of those things. At first, we tried to do that in my student shop. To do that in my student shop was afar. So that's basically what's it called topological recursion. I mean, in that case, you even have to go beyond plane, beyond maps, because so you have to allow decompositions in terms of like cylinders, something which is more complicated than simply connected domains, which are polygons. Which are polygons. But this theory exists, and we try to work with it. But then we realize that actually, this is first of all very difficult to work. And second of all, it's a perturbative expansion. We want, because the integrals that we have are convergent, actually, and it's very hard to relate these perturbative things to convergent original matrix integrals. So that was a So that was a bit of a disappointment, but anyhow, it was a good start. And so we tried to, I mean, in that case, actually, we were able to compute the first two terms of the topological recursion, which are the most important thing. Then there is a recursion you have to apply. It's a lot of work that hasn't been done yet. So that's still an open thing to do, but the original part has been done. The original part has been done, and so maybe this is something for the future. But then, with my student Nathan, Nathan Pagliaroli, who is here actually, we try to do non-perturbative methods to tackle these random matrix models coming from non-quantum geometry. And the object of interest over there, the main player is the so-called green function. So, when you have the green function, So, when you have the green function, which is defined as the expectation value of your resolvent functions, the trace of 1 over z minus h, you can expand it for large values of z away from the spectrum and away from finite parts of the plane. And you go towards infinity, the series is convergent typically, and the coefficients give you the moments of your random matrix model at finite n. So, this is a very nice function, the G N, because it's holomorphic and immediately relates your problem to complex analysis, which is very powerful. There are many, many powerful tools in complex analysis that you don't have in real analysis. So, that's one point of applying this. Another thing is that there's actual complete correspondence between green function and the original density. You can recover, it's like one-to-one correspondence. You can recover. Correspondence. We can recover the density, original mean density function, as the limit of that i is imaginary part of the green function as you approach the boundary. So you consider it as a function in the upper half plane, for example. So there's a one-to-one correspondence. Now, another nice thing about this approach is that this green function actually satisfies the differential equation. It's a non-linear differential equation. Some cases you can solve it. This P is difficult to compute, but essentially computable. Otherwise, it's just a non-linear differential equation of Riccotti type. But solving it for finite n is very difficult, but when you let n go to infinity, that one over n term you can show that disappears. Disappears. It's not obvious, by the way, but this disappears, and then you get a kind of curve relating g to parameters of the problem, like potential and p of x. So this is called the spectral curve, and it's given by this. You just solve the equation, it's a quadratic equation, you solve, and you get this. In the simplest case, for example, you can immediately use this to drive. I mean, first of all, using this, you can show that the support of your distribution function is as a finite support and they are in general disjoint. And that's it. And you can, in the simplest cases where you have potential, just quadratic potential, you can solve it and you can. Potential, you can solve it, and you get this is the GOE Gaussian unitary ensemble, and you get the Wigner semicircle law by this method. So, in our problem, of course, in all these models, the integrands are unitary invariants. So, this means that we can write everything in terms of eigenvalues. There is an expansion, there's an integral instead of space of Hermitian n by n matrices, which has dimension. which has dimension n squared, you can reduce everything to dimension n, which is our n, the space of all n eigenvalues. And you get integrals like this. And then you want to analyze these integrals using saddle point techniques, but done in a very rigorous way. And saddle point techniques is not enough. You really have to go to do more and derive some integral equations. And derive some integral equations for political points eventually to solve such things. So, in the simplest model, the two functions v and u are given by these two formulas, which you immediately drive from the form of the potential. That lambda i minus lambda j squared term over there, that's the van dermann determinant that always appears in all these things for perturbations of Gaussian models. Perturbations of Gaussian models. And the term u lambda i lambda j is the difficult term which usually doesn't exist in standard random matrix models and is a feature of these sort of things. Anyhow, I should not spend too much time now on this. So there is an Euler-Lagrange equation. I mean, you can turn the question into a variational equation. Variational equation and minimizers of this action. This is called energy functional IU. Its minimizers are the limit distributions. And the minimizers of this action are actually an integral equation that you have to solve. So the principal value of this integral, which is divergent, Integral, which is divergent at critical point, but you take its principal value, is given by some cubic term on the right-hand side, and the coefficients m1, m3, m4, you have to, you have to, the moments, the first three moments you have to obtain, m1 is one, but the other two moments you have to obtain by other means, you can. And this is very rigorous now, is because Percy Dave worked on these things as part of his non-linear Riemann-Hilbert correspondence. So this has been Hilbert correspondence. So, this has been, well, he has a book on that. So, that's been totally done. So, once we did that with Nathan, then something very interesting happened because you do indeed get the value of the critical point that was found by Barrett Glazer using their Monte Carlo Markov chains. Exactly, I mean, exactly, I should not say exactly, I mean, but very, very close to what they have because what they have. Close to what they have because what they have anyhow is approximate, but what we have here is a number which is very, very precise. So this was it. The criticality shows itself in when you change the value of g, you pass from negative minus half to negative 3.5. At one point, At one point, this one support, connected support, decomposes into two support functions. That's the first point that touches the zero, and then the thing starts evolving into double support things. So, this has been obtained by a kind of difficult method in a way. I think it would be nice actually to do the calculation. Would be nice actually to do the calculation via mean field theory, which is I'm just learning mean field theory now. I think that should be something because some, I mean, okay, we know that it's not precise and we know that it fails eventually, but it would be good to try it in this sort of models. And because, I mean, this model that we did is really the first case. So, anyhow, so this is a kind of job. Now, the bootstrap, this is the project with. Project with Hamed Hassam and Nathan Pagliaroli. And so the bootstrap, I will explain. There are four steps in the bootstrap. So maybe it just doesn't make sense to spend any time on that. So let me just go to the problem right away. So what you do in the bootstrap, so you have a matrix model, you want to understand its large, for example, For example, eigenvalue distributions, you want to understand phase transitions, you want to compute some correlation functions. You have all kinds of problems about these things you want to solve. Schuger-Dyson equations is a very powerful method to find constraints between various moments of such. Of such matrix integrals. However, one drawback of Schwinger-Dyson equations is that they are kind of very complicated and there is no immediate recursive relation apparent. And that's why this topological recursion was very successful because it turned Schringer-Dyson equations into something which was very recursive, although very complex. Recursive, although very complicated, but still recursive and computable in principle. So you have such a multi-matrix multi-trace model in general. Schrinkled Eisen, as I said, gives you constraints or relations between various moments of your problem. So you define moments like that and you want to see the relations between them. And so, for example, in the simplest case, for a single For example, in the simplest case for a single matrix model where you take your word to be single word H1 to the L, these single moments, products of traces are related to single traces by such equations. You can drive these equations by various methods, for example, integration by part. You just have to do the right kind of moves. It's just not immediate integration by part. You have to immediate integration by part, you have to do it in a particular way and then some particular terms. So that's a big thing. Now, one of the things that actually helps you with these models is that with these Schoenger-Dyson equations is that if you go too large and limit, something fascinating happens. So you can actually write your products of, I mean, like this kind of so did your random. Kind of so your random variables become almost independent in the sense that product of these traces becomes trace of the product. For finite n, you can prove such estimates and then as n goes to infinity, you get this thing. So these equations in the limit, as n goes to infinity, has some nice properties, much nicer structure compared to finite n, which is much more complicated. Finite end, which is much more complicated, as always, of course. So, the first thing we tried our hands with, so the first goal was to see that if we can actually predict that phase transition using these bootstrap techniques. So, Schunge-Dyson equations gives you these equations at large and limit. They are constraints between various moments. Constraints between various moments. The first thing you prove is that, first of all, all these things just depend on M2. You can actually solve all these things in terms of M2. So there is only one unknown parameter, which is M2. But finding M2 is very difficult. That's impossible. But then what comes to your rescue, and that's the crux of the matter, really, is a classic. Is is a classical observations from observation from probability theory actually. So this is the Hamburger's observation from early 20th century even that says if you have a sequence of first of all imagine you have a probability distribution mu and you look at the moments m0 m1 m2 yeah you just define these things then there is a Then there is a matrix you can attach to these moments. It's called Hankel matrix. You just define it like that. And what happens is that you can show that actually easily indeed, show that this Hankel matrix is positive semi-definite in the sense that any corner, top left corner you take for 2 by 2, 3 by 3, 4 by 4, et cetera, they are always positive semi-definite matrices. Semi-definite matrices. So these moments give you an infinite number of inequalities if you think of them as Emerson. Well, Hamburger proved something much deeper, namely he proved that if you have actually such moments that satisfy this condition, then there is a probability distribution that whose moments are the given numbers mk, provided that mk don't grow in. That MK don't grow in a too crazy way, I mean, like exponential, exponential, something if they grow in a controlled way, in a reasonable way, which is kind of generic, well, I shouldn't say generic, but is in applications usually grow like that, then there is a probability distribution. That's what Hamburger proved. That's what we use. Now, the thing is, is that actually there are now this is that the Dirac moments are actually. Dirac moments are actually, you get an extra set of constraints. So it's not just the constraints in terms of H, there's also constraints in terms of Dirac moments. So you do get even two sets of constraints. So in a way, this is the added benefit of working with Dirac consoles rather than working just with a matrix. So yes, so then, yeah, we pursued this and actually. Pursued this, and actually, what we observed is that indeed you have to go to. I mean, here there is a lot of work, computer work involved, really, because you have these constraints. So you choose a cutoff, for example, lambda, I think maybe eight. So maybe you go up to eight by eight matrices. So you have all these equations, eight of them, maybe seven. Eight of them, maybe seven. And you find the domains of validity of the inequalities. And actually, what happens is that they converge, it gets narrower and narrower. Eventually, you have this blue line. And the blue line is what gives you the relation between second moment and G. And as I told you, second moment is the only thing you want to know because everything else is computed in terms of. Computer in terms of and using that, you notice that the nature of blue lines changes as you cross the phase transition point, and that's how you find, it's from non-linear to linear, actually. That's what happens. That's how you discover phase transition in this case. So let's see if I can move this now. Yes, but this was the warm-up case. The case that The case that actually is totally impossible to reach by usual techniques, random matrix techniques, is this so-called type 20 random geometries or positive geometries or Dirac ensembles. In this case, the Dirac operator depends on two matrices A and B, two Hermitian matrices, gamma one, gamma two are. Are poly spin matrices. And when you set things up, again, quartic potential or action is pretty good in this case. You get really now some very complicated terms. So trace of D2 is like that, trace of D4 is going to be, has a lot of terms. And these two matrix models are totally beyond research. Are totally beyond reach of again. I'm saying it many times, sorry, of computations. So, sorry to interrupt you. I think that you have less than five minutes. Less than five minutes. Well, it's 45, including questions. So, you're sure. Yeah, I think I'm doing okay. Yeah, thank you. Thank you very much. Thank you. Thanks. Yeah. So, yes, this is the partition function. And then, in this case, you really have to. And then, in this case, you really have to deal with multi-moments. So, any non-commutative word in A and B is part of the game and has to be, because they enter into Schroinger-Dyson equations. So, you have this system of Schroinger-Dyson equations. And what happens in this case, again, you show that actually all these equations can be analyzed in terms of M2, the first two moments. Two moments. So you get these are the relations that you find between mixed moments and higher moments and just M2. These are, you get these rational numbers and just they exist for all of them. So it's been, we computed them and actually it's very nice to conjecture what these expressions mean, but let's not think about it. Let's not think about it. You have similar things for Dirac moments because this is a Dirac ensemble, after all. This is just a single moment. Now this is easy because you just take trace of D to the power M and that's the M. And they are also expressible in terms of the second moment of the problem. In this case, you again, you use a Humbuger moment positive. Positivity constraint. So, again, this matrix that you cook up from these non-commutative polynomials has to be positive, semi-definite. You have to cut it off somewhere. You cut it off, and then you just have to analyze where are the domains of positivity of these matrices, I mean sub-matrices in the corner. Okay, so this again you do. So, this is also in this paper I mentioned with Essom and Pagliaroli. Again, they really do converge on some kind of line as you reach the boundary and gives results in this case as well. So, this is the So, this is the before-phase transition points. So, this is what you get. So, the current project now is to look at actually a cubic divergent, I mean only perturbatively convergent matrix model, which is we call it Airy Dirac and Somme. I mean, the interest of Airy Dirac and Sons is that these cubic matrix models have been. Matrix models have been very, very successful in the work of Kunsevich and then followers. They are related to topology of modularized spaces of Riemann surfaces, they are integrable hierarchies, they are related to a lot of different things. And I'm just guessing that actually this, you see, it's a function of the parameter, complex parameter lambda, but it's not complex. Complex parameter lambda, but it's not a complex parameter, it's really a matrix. The coefficient of D, the source term, if you want, is a matrix. So it's a matrix, it's a function of a matrix. And understanding nature of this function, it's very interesting. So I'm hoping this can be also analyzed using ADLACONSOMES. Now, the cubic ADLA consoles, A radirac console without I, not A rider, by the way. Well, there is a nice factor because this integral in the previous page, this is divergent. This is only conditional conversion because there's an I term over there. But the integral that we considered in this paper with Nathan, Hamed, and Luke, we did it in this paper, is really convergent, I mean, perturbatively convergent. Perturbatively convergent, and because of that term trace of d squared. And you can analyze that using this bootstrap techniques. So this gives me hope that AD delugs also are within reach, but this has to be seen. And this is the curve that domains that we got for that. And these are the papers that That we have done so far. All of it are published except the first one, which is well, okay, so that's another story. Well, thank you very much. Thank you very much. Are there questions from people in the room? Yeah, but I ask whether there is any question here. Yes. Here? Yes. Sorry. In the results that you showed at the end, you showed only even moments of the Dirac. Was this put in by hand or are there also some asymmetric solutions with odd moments of the Dirac non-zero? Typically, odd moments should vanish for these models, but yeah. Yeah, they vanish quite symmetry, quite symmetry, yeah. Okay, John, please go ahead. Hi, Masood. I was looking at your graphs with the blue line on it, the thin blue line. And you said you can see the phase transition on them, but I couldn't really see it. I mean, is it what? How do I you mean uh like this one, yeah. One, yeah, or the previous one. So let's look at this, for example. Yeah, so where's the phase transition? Well, you see, before phase transition, this is linear. After phase transition, this becomes non-linear. It becomes curvy. So here, you see, here there's a curve. Right. But how do you see where the phase transition is? Well, the computer tells you where it is, more or less. Say more or less, yeah, it really changes. Yeah, I mean, it doesn't give you the value. This you have to do a little bit of eyeballing, also, but computer, I think, tells you basically. Okay, yeah, that's uh, yeah. Thanks. There is another question from the room. Yeah, in just in your final cubic model, I didn't really understand how it could how it could make sense. Yes, Yes, what one expects in large N is that if there's a well there, then the eigenvalues can be contained in the well. But I don't see any well in your case. Well, yeah, there is no well. I mean, this is only conditionally convergent, disintegral. I mean, it's very much like. So this one is okay. This is the imaginary, this is the airy one, but the other one, the final one. Scary one, but the other one, the final one. Oh, you mean? Oh, I see. This one, you mean? Yes. Oh, yeah. This one is, yeah, so. Yeah, this is per se is divergent, but we treat it perturbatively. Yeah. But the good thing is that each term is convergent. When you write the perturbative expansion, You write the perturbative expansion, as you know. I mean, then terms because exactly because of this Gaussian term, there minus trace of d squared, you can bring down trace of d cube down, and then it becomes a sums of conversion integrals. Then I think what you're dealing with is just an truncation of the exponential of the cubic term. Yes, yes, yes, that's what we do. Yeah. Okay, thank you very much. Okay, thank you very much. And then let us move to the next talk by Leiden Boyle. So I stop sharing. Yes, please. So, Leyden, are you ready? Yes, can you hear me? We can. A new theory of the early universe. Yes, you can share your screen. Okay, can you see my screen? Yeah, we can. I might stop that. Okay. Okay. Um okay.