Our work and organizing this wonderful event for me. It's my great pleasure to come here to report some of my recent work. If there's some familiar faces and some younger people do not know, but get to know them. Okay, so I'm going to talk about some recent results on label modeling. And this is in collaboration with Tracy Cole, who is one of the co-organizers, and two of his postdocs, one of my former students. Students. These are in five and five grade planning. So the issue I'm going to discuss today is statistic neck models. Sometimes I feel maybe in my personal opinion, maybe we propose too much, too many models for the same safety. We have done with the same kind of safety, same kind of labor, but we have buttons with different models. Um do we really leave that mind in the minimum model? Leave them in your model. These models overlap with each other. And many of them motivate a long list of research. But this research a lot list is transferable. So therefore, for the whole community, for the good of the community, this is not very good because it a lot of waste of time, talents, and energy. And there are several, to deal with this, there's a several approaches we can take. One is mathematics. We can take. Why is mathematical approach? So, from a linear algebra or mathematical way, we try to identify how significantly these models are overlapping with each other, or in some time they could reach each other. And the second of all, you can do a statistical GOF for the fit. So, out of the dozens of models for the very simple setting, pick ones which are promising and also identify those are not very promising. Not very promising. And eventually, of course, we want to find a sweet spot. We want to find one or very few models that strike the ideal balance between the theory, adequacy, and the mathematical credibility and adequacy. I'm going to start with the RAM-K model. The RAM-K model, we are going to focus on undirect synagogue or synagogue-like work. But this can be straightforwardly generized to the nan late works, and it also can be generalized to the direct late work and so on, so forth. We have undirected late work, and we simplify the binary. So the JSS matrix is the binary, so if it is 1, if 8, otherwise it's 0. JSS is very simple and standard. Only thing you need to know is that. Only thing you need to note is that conventionally do not count self-axe. So the diagonal entries, both entries on diagonal is zero by default. We're assuming the other triangle entries are independent Berlin, but the Berlin parameter can vary from one entry significantly to the other. From one could be 0.1, but the other would be 1 over 1000. We assume there's a K-perceivable communities, and for example. And for example, in the web blog data, there's two communities: Democrat and Republican. For the co-authorship later, Jen Ting mentioned this morning. You may have a Beijing community, we have a nuclear community, and also as community. So community is just a group, okay, subset of the late work. They are not component. Component are not collective, but community is still collecting. We say a rank K model if this is the data matrix with the Berloudi matrix. We take the Berloudi parameter part, so this is massive here, and this is the centralized Berloudi noise here. So this is a signal, this is the noise. The luck sense of this one, it comes from all the diagonal entries are zero. So therefore, we have, it's better to subtract, you pick up omega and subtract the diagonal entries. So this part, the diagonal is always zero. This way, you can impose. This way, you can impose, for example, no rank structure on omega, otherwise more difficult. So, therefore, this is omega, which is the main signal matrix for me, this load matrix. This is a nuisance, but it's a secondary effect. So, could you clarify that? So, I mean, usually when I think about these types of data, the diagonal is just undefined. It doesn't make sense to call it zero. Really, you said instead of a symmetric, you're going to have a symmetric pictures. Yeah, that's just modeling. Pictures. Yeah, that's just modeling the upper triangle without the diagonal. So we do the cop. Yeah, we do the cop the diagonal here. We do not. Okay. Yeah, we do not. Okay, so we assume that it is maybe the diagonal is zero, but of course you can model it. Are they zero or are they not? The diagonal entries are all zero. And that's part of the model. Yeah. Okay. Um you may we can generalize a little bit. You may, we can generalize, we can extend it, but this is only a very small part. This is not essential. This is not the essential part. Okay, this is mainly Ramkay model. Okay, I'm going to the first one is Blonde Model Family. Bloom Metal Family here has four members. So this member has been introduced in Holland and collaborators now in more than forty years. And they're just more recent the degree creative block model and it's just all sort of mixed membership. And it is also the mixed membership stochastic model, and also the degree correcting mixed membership model. So, this is the GIT has been talking about this morning. All these are ranked pay models. You will see very soon. But they are more interpretable compared to the original rank pay model. Rank model only says omega is ranked, but if entry, what does that mean? It's verlootic proven, but aside from that, it does not say anything. Say anything. So, these models capture three very noteworthy features frequently found. So, this is a severe degree of hologenity means. For example, Jen Qing's load of degree is much higher. So, he's a fresh PhD student hundreds of times. He has much more many goals or scores or citations. Well, Elon Musk, his degree is much, much higher compared to other people, social networking. So, the second order mixed membership. Mixed members are also prevalence because. Mixed members are also prevalence because you know, Ilomas, it's 100% Republican, no, it's 100% Democrat, no, probably somewhere halfway. And the third thing is very rarely, not very much mentioned, but we do observe. So the leading AMIs of GSS nature A, some of them may be negative. The first one is always positive, but the second one, the third one, okay, they remain negative. I'm going to talk about the degree correct development a little bit more. So, in this model, this is what I have mentioned before. All these models, they are the same rank paid model, but they have a slightly different way to model this omega matrix. In the degree creative mixed membership model, they have three legs. First of all, for every ocer, we use one degree per protein factor to model the To model the prevalence of influence of that, for example, you know, Maz was settled very large, okay, and some other people may be much smaller settlement. And also the mixed membership. So suppose there are, for example, take the three. So you have three communities with the Bayesian, there's a non-chromatic, this is the biostatistics. So for each author, there's a three-dimensional weight vector. Each entry measures weight influences citation. Influence the citation or interest of that author in different communities. Okay, so if K equals 3, so this is 3-dimensional. And this is a 3x3, very small matrix, a tiny matrix. But this is also important because it's a baseline collecting probability between nodes in different communities. For example, Beijing and the Namatrix was a baseline collecting probability. So if you put all this together, so it depends on the degree of SETI SEDGA, depends on their Set GA depends on their mixed membership and also depends on the baseline modulum together. This becomes rank matrix. And here, set i, pi i, r t are all interpretable. So this is something statistics alike. Earlier than this, maybe the same time, about the same time, there's uh other classes of Ramke model. And among them, I think this the random dot product model is very well known. It's very well known. So, this model says, okay, omega can be decomposed as the product of M by K matrix and its transpose. So, this seems much broader because you know every DCMA model is a special case of this one, but okay, not quite. Why? First, omega must be positively finite. And then, the John Hopkins group was proposed. The Jeff Hopkins group was proposed that, okay, if that's an issue, I can insert matrix back to the matrix here. Some of the diagnosis one, some of the diagram negative one. So this is solved common. But the bigger problem of this model is it's hard to choose this pi so that this matrix is entry-wise non-native. Because this is the probability matrix. Every entry needs to be between 0 and 1. How you choose this big matrix so that different row, the angle between them is always smaller than 0. Between them, it's always smaller than 90 degrees. That's very hard. It's like air for a row and all other rows, they have to leave in a cone, a small cone, so that they can be, the angle between them is actually. So that, okay, so the first question here, okay, here, so therefore this model looks like broader, okay, but actually they have, if you want to simulate certain metrics and whatever, to make sure only gets To make sure only because it's a entrywise non-negative, okay, it's quite okay, it's not that easy. You have many special choices, but that makes it much narrower. This is the Venn diagram. This is the Venn diagram. So here is the four members in the block model family. I'm going to focus on this later. So this is the broadest MM. In the DCMM, if you In the DCMM, if you force the snow mixed membership, so every node is pure in the sense that your pi i is either one zero zero or zero one one zero one zero or zero zero one. Your pi i is only, okay, if only allowed that, then you reduce to this model. Okay, if you do not model the degree of heterogeneity, your setup are all the same, then you reduce this model. If you do not model none of them, know the model nine none of them then you become this model okay so this is the passive log model okay now here the DCMM okay here is R D B G and the generalized R D B G by Sir by Chargy okay it's Rob Chargy he says the contra Grafan model to some sense is also approximately a Ramk model okay actually a Ramp Ak model so Grafan is not a subclass you can Profile is not a subclass, you can imagine this curve. Okay, this region is outside, part of it outside this, but you know, okay, part of them is significant overlapped with the Ramp model. So therefore, we have introduced the. Can you expand actually on your last statement, the overlap between the RAM K and the Graph model? Ram K model is by a paper by Chatter G. It says, okay, for some setting, you can approximate write a graph model as a RAM K model. Broadhawk model other than Khaw. Okay, refer to just yeah, okay. So, in some sense, okay, so yeah, this part is approximate so, but just for the video effects, okay. So, then the first result I'm going to record is about the following question. Okay, so ECMN is a spatial RAM K model, but how much broader is the RAM K model? Or the question is, when will a RAM model? The question is: When will a RENT-K model also a DCMM? So, RENKK model is broader, obviously, but less interpretable. This one is narrower, but a lot of people use it, and it's more interpretable. So, the first result is if k equals 2, they are the same. So, if k equals 2, there's only two community, then without any extra condition, as long as x omega is a cognitive metric. Omega is a cognitive metric which is non-native. Then you can always write this way. So you don't need any extra conclusion. So I'm going to showcase 11 data sets later. So on the left, you see six of them. It's always tables two. So therefore, for these six different network real data sets, if you believe RAMP model is adequate, then this MN is also adequate. You don't have to worry anything. On the right-hand side, the K is bigger. Essentially, 111, that's pretty big. But, okay, so the case for general k is as follows. Late lambda k is cosi k is case eigen pair of omega. So for example, lambda k is a case large eigenvalue of omega in magnitude, okay in magnitude. So cosai k is corresponding eigenvector. M is a number of negative eigenvalues of omega. So omega only have k non-zero eigenvalues because of ranked k. So out of this k, m of them are negative. So if half of them are non-negative, no more than half of them is non-negative, and this condition holds, then you can always rewrite the Ramke model as a design model. How strong is this condition? Of course, this is the condition. It's not a very random model to subside this, but if you most difficult in terms of analysis, most difficult case actually is this matrix is literally this matrix P is literally a constant matrix. Is literally a constant matrix. Why? If the baseline collective probability is very different, so the community are very, very different, it's not very hard to tell the difference. The most difficult part is they're similar. Then it makes it very difficult. But when this is similar, it makes lambda k much smaller than lambda 1. So therefore, this is a very small number. This is a very small number. This is a very small number, and if this is a regular, so that these are bounded and be constant, okay, then this pose. On the right-hand side, this is the next one. So therefore, it is a condition, but it may be strong and may not be that strong. But this is only a sufficient condition. It's not the necessary condition. Okay. If this is satisfied, then it it's okay. Okay. If this is not as fine, it may be okay. So this is the first part I examine. So, this is the first part of my example. So, any questions? So, this condition needs to hold for every R? Oh, yeah, yeah, yeah, yeah. Yeah, yes. Thank you. Yes, for every R. Okay, thank you. So, the collection to the latent space models, the latent space model is also very popular and a long list of motivated long lists of research. So, one of the criticism of the D. So, one of the criticisms of the DCBN or the DCMM is as follows. For example, if I take a log on both sides, then it becomes a very familiar model, right? Very familiar model. But some statistics may argue, A binary, right? Mays binary so more natural link function. Okay, if you do, especially if you do logistical regression, what kind of link function would you choose? You would choose logic instead of log. Logic and log are very close. Login and log are very close here because for many late work they are sparse, so omega is very small. So the difference is not very large. So for sparse late work, they are very similar, but these are linear and more natural. So you may want to consider this model instead. Now, when Q is positive semi-definite, so this reduces the very well-known latent state model. For example, Peter Peter has done a lot of work on that. More generally, it's Okay, more generally, it's equivalent to this way. So, for sparse late work, they are very similar. For late works, some rows are sparse, some rows are not. So, this is probably more natural. So, what's kind of problem about this model? Analysis. Analysis, why? Because the previous model is ranked K model. So, if you want to use spectral algorithm, that's very natural. This one is not. That's very natural. This one is not a ranking model, because of long linear terms. But fortunately, recently we have found a trick. We have found the trick, we can know how to deal with these non-linear terms. We can run step to approach first, use that trick to estimate this non-linear term cost by logic. Then we refit and then we run. We find the very good, very encouraging results. So that may shed light on how to deal with this data-space model. Okay, so that's my first part of the result. Any questions? One question. So can you clarify the meaning of 1k model is equivalent to approximate weighting of this n? Because there is a every entry of value distribution, right? Do you mean it's equivalence in some kind of I mean, in exact this sense, okay, so if you, in this sense, I can, okay, for example, k equals 2, or k equals 3 with this condition holds, these two condition holds, this and this condition holds, then I can rewrite this as a design model. All these are lump permanent, a lump negative. So expectation. You can factor them. I can factor them. So only in the expectation, not in the distribution. What's the expectation statement? What's the expectation? Oh, it's all matters. It's all matters. Okay, can you ask patient? I mean, this psi, they define up to orthogonal rotation. So are you trying to find this orthogonal rotation which makes everything a negative? Or you are trying to prove that it is possible to find it? Are you just trying to find that false expectation that's alparthy to sell? Everything the cell is constructive. It's constructive in the rotation, which makes P non-negative if some conditions exist. So if this condition holds, I know how to find that also node K by K matrix. And I know how to construct this set up, pi, and P. I know that. Yeah, so but that because it's not the main interest of today, so it's in the paper. Okay, I'm just trying to. Yeah, so I do not cite the paper. Yeah, so I do not cite the paper, sorry. I think it's a MIPS paper. It's MIPS, so I say no. I don't remember one of the paper in 2021. Okay, but June 2021. If you're interested, look into that. Just to clarify the question you had previously, so you're saying that just because something, the, like, you have a link function with like a generalized set of model, so you have data that might be. Have data that might be binary, there might be counts, and you want to use some sort of factor model to describe the variation. But you're saying if it's like it's a GLM or the logic case, just because it's transformed, that's not considered a rank K model? Yeah, it's not a rank K model because this logic, okay, it's a logic term, it looks like this. Okay, that's a very good question. Okay, so this model you can write as omega equals k. Equals k factor omega. So this is my omega. This is another matrix. This is the entry-wise product. But I understand that you're saying that you're restricting this definition to me describing the mean of the matrix and not some transformation. Not the particular mean. But in a lot of cases, it would be natural to think about looking at the transform scale because you have a mean-variance relationship. Relationship. Right. So in binary data, you have a mean variance relationship or a curve as well. And so in additive model, additive mean competition. So the CMC is you can, of course you can. Okay, of course that you can. But the thing is, once at the bank of PCM, it's a random camera, so for the spectral accuracy, very easy to use, and for the very easy to feed. But if you have interest line in the term, it's an ongoing resource. Is ongoing research. I have not completely solved that problem yet. So now the second part I'm going to talk about putting to fit. We are going to focus on the four models in the block model family, but the idea is extendable. I'm going to propose a general recipe for doing GOF general models. We're going to focus on LED and neural network data sets. You're going to see that. Work data sets. You already see that. So, part of the data from the recent published papers are most frequently used data sets in the statistical limiting. And there are several new ones from the MADSTAD data on my group. So, here's about the general recipe. I'm going to introduce the general recipe for doing GOF with later model. So, here are the, okay, so we introduce two things. Two things. This is called a cycle count. Why is it called a cycle count? So this is adjacent matrix. So if this product equal to only have two positive values, 0 or 1, when they are 0, it means from I1, I2 to In, you make a cycle in the night or HS matrix. Otherwise, 0. So if m equals 3, you are counting how many triangles the adjacent matrix and the quadrilateral and pendulum and so on and so forth. So this is. So on and so forth. So, this is the number of m gons. You can also use it this way. Suppose I have an estimation of this omega, I subtract it by A, then apply it to A hat. I'm going to divide these two terms by this normalization. Take a square root and normalize by 2n. Okay, this is not very hard to calculate. The computation cost is modest. So, what's the surprise? The surprise is if I know about me. The surprise is: if I know omega, of course, this is the oracle situation. If I know omega, then this converts to standard normal as long as the late order is sparse. Means every entry, every node, the degree is smaller than n by order, by small order. And there are some mild conditions I do not specify, but it's not very strong. But in very general case, this converts to standard model. Why is this interesting? Why is this interesting? Because how many parameters Omega has? Thousands of them. They have thousands of parameters, but if you construct this way, none of them show up in the limit. So this is called distribution-free or parameter-free limiting law. This works for m equal to 3, 4, 5, and so on and so forth. But we fix m equal to 3 for two reasons. For two reasons. Analysis is larger than M, it's more difficult. The second about humanity already can be achieved. It depends on the situation. Sometimes chemical 3, sometimes chemical 4. So there's a little reason to go to higher pain. So this motivated a general recipe for doing good fit. You construct an estimate for omega for omega using the assumed model. For example, you like the stochastic loss model. Like the stochastic block model, you have only had to. You count m equals 3, so this reduces to omega. So this is a t stands for triangle. So when the assumed model is adequate, we hope omega hat is sufficient accurate for omega. So this still converts to a standard normal in the non-case. But if the assumed model is inadequate, we hope this tends to infinity. Of course, the remaining. Of course, the remaining question is how to construct this formula and how to control estimated error. Our plan is going to, our paper discussed all these four cases in the model family. Today I'm going to quickly go through this one and spend more time on this one. This is a special case of that one, so to speak. This is a special case of this one, so to speak to speech. For the degree correct develop model, the deal is you can have severe degree hologram G. So you can include minimum mask. But we assume every node is pure in the sense that every pi i is either 100 or 010 or 1001. So it's very restrictive on this pi. So the idea. So, the idea is we are going to first estimate pi. But this is why start with this one, because this is a latent variable. We know for the latent variable model, okay, the better start with this one. Now, once, okay, so this is a score to detail it later, but let's focus here for now. So, we are going to estimate this pi. So, this is actually the clustering algorithm. So, every load is clustering to a community or cluster. Community or cluster. So based on the cluster label, you can decompose the vector of ones into this sum of all this k vector. Now, this k vector different entry cost one if i belongs to the estimated community or zero other ones. So this decompose. Then you have a formula for set i and p and you plug in them. That's estimation for omin hat. Similarly, these are pretty elementary. The tricky part is here. Here. The tricky part is: okay, we're going to use the score. So the score is as follows. I think Jian Chi mentioned this morning, but this is the algorithm in detail. Late cosine hat K is a case egg vector of A. So the corresponding egg value is a case large magnitude. You construct a matrix R when we take the entry-wise ratio of different electrons. For example, Of different egg vectors. For example, the second egg vector divided by the first egg vector entry by entry. So this becomes a matrix with the n rows, but only k max1 columns. Then we just apply km to plus 3. So the deal doing entry-wise ratio here is to remove the effect of a severe legal opportunity. The reason is, you know, Elon Musk's political viewpoint and one of his friends may be very similar, but their degree can be very, very different. Different. So, therefore, by removing this, you can remove the degree effect you can expose their political new point. So, this is the rationale behind this step. Now, the first result is net all we have is as above, and suppose this goes to infinity, then this metric we develop is indeed covered to standard overall. To than the lower mode. Now, when this converges to zero, what's going to happen? When this converges to zero, then it's impossible to SMAK. Okay, it's impossible to SMAK. So therefore, this is actually nothing but a signal-to-motor ratio. If this number is big, built for things, you can do. If this tends to zero, you can do nothing, almost nothing. Even K is not estimable. You can say, I can model this. Can say, I can model this with two bits of DCBM with two different K, but the most powerful test in the world cannot tell the difference. So basically, this model, the L1 distance tended to zero, so it is indistinguishable. So therefore, the key to the proof is, okay, we can show this pi hat and this pi up to a permutation in the columns, they're always the same, except for the small probability. For the small community. This is possible because pi is simple. Every row of them is either 100, 0101. If I allow Europi i to take any numbers, this is impossible, right? Because you know, you still have concentration. I say pi hat and pi can be closed, that's fine, okay? But your pi hat can never concentrate exactly concentrate on finite main mainly non spoken matrix. non-spoken matrix. So that's the deal. So this property makes that pi hat can concentrate at one non-stochastic matrix with high quality. For this MM model where pi i can be more complicated, this is impossible. So we need new tricks. That's like is there a condition on like some lower bound on how dense the graph is? On how dense the graph is? Like, if the graph is. You need a load down for this one? Yeah, if the graph is very, very sparse, then you don't expect the spectral. So you need that the one goes infinity. So therefore, okay, so this is about sparsity. So you also, this is smaller than n. So this would be, I I might have n squared there, but okay. I might have n squared, but uh the range is lambda. Range is lambda needs to be n. So that is sparse, but not that sparse. And when you say much greater, you mean polynomial scale, or is log scale also allowed? Here is maybe like a log n sum. Log n sums, yeah. I don't need a log n sums. It's a very good question. So therefore this covers whole range of stars of major interest. Of course you can go very, very sparse cases or very dense cases. Okay, but I need a log n factor on both ends. Fracture of both ends. So for the DCMM, it's harder. And it's harder, it's similarly using the same recipe, but the real, okay, we can estimate pie first, then we refit. Well, the same recipe. The mixed score detail, thanks for Jianqing. He already had a warm-up pretty much this morning. Okay, so these two 70s is the same as the score, and this is the same place structure. And this is a simple structure with Tracy discovered in our 2017 paper saying if you view every row of this R hat as R hat as a point in k-dimensional space, then there's a simplex. It's two dimensions, there's a triangle. Okay, it's a triangle. Then, up to alloy's corruption, To alloy corruption, every row falls exactly within the simplex. And if a pure row, your pi is 100, this kind of type, then your fall exactly on one of the vertices. So based on this simplex structure, you can actually retrieve this pi i easily. For example, you need two small dimensional matrix. For example, you need a diagonal matrix D hat, you need a V hat, which corresponds to the vertices that are simple. Which corresponds to the vertices of that simplex. But once you have this k by k matrix, you can just use this r hat to recover this hat star. So what remains is just very mild regularization. So in this morning, Jian Tin said this is very complicated. I do not agree. You see, it's very simple. One card, the S and it is two low dimensional quantities. But this low dimensional quantity is, we have blazing. The blazing is Blazing. The blazing is their rate is much faster than S mini. So therefore, you treat them as known. You have the same rate, right? You treat them as null, so the remaining step action is not that high. So that's, I think, okay, I would encourage everybody to, there's no really big hurdle for the hypothesis. Okay, but what the really hurdle means is downstream analysis. It's downstream analysis. If you're coupling these averages with many things like Jianqing's though, it's ranking. That's good. Okay, analytically, there's no hurdle. But you would come with this cycle constant, you do have a challenge. It's not the challenge, it's too hard, you cannot overcome. It's so tedious. It's so tedious. For example, many proofs may need 100 pages. It's just because this statistics, so-called use statistics, what we do with use statistics, we decompose them in many, many terms. We cannot say, okay, all these terms are similar and I analyze one of them. You cannot do that because they are always different in some way, but high-level is similar. So you have just the, in a regular paper, you need to analyze one term. In this paper, you need to analyze 10, 20, some 100 terms. Okay, it's tedious. It's tedious, but it's not hard. But is there a friendly version? Make this analysis much much easier? We do. So for this version, it's hard because the spectral part, the spectral part, how to characterize the leading egg vectors. So for Jianqin, we introduce V1 out rule, so on and so forth, okay. But here I'm going to introduce a new version, okay, which is kind of regression-based. So regression-based was much faster to be doing. Regression base was much faster to be doing this. The regression base is as follows. We are going to use mix score as an initial step. Then I'm going to put a net, very sparse net, a very sparse net, for example. So then for this pi-hat estimated for mixdoor, you can just kind of replace each row of pi-hat. To kind of replace each row of pi hat by the nearest negative point. So it basically says if I'm going to replace this, I'm going to just close this to this one, I just let it equal to this one. Closest to this one, equal to this one. So eventually, I can show after this manipulation, this edge hat is not per hat, okay? Edge hat concentrate on Concentrate on several alarms to cast matrix H0 with high probability. So therefore, we again have the concentration inequality we want. So in this ECBM is given, in this one, you have to go some extra miles to get there. But the cost is just run mixed over one more time. Then do this time networking, then you get there. Networking, then you get this. Now, once you have this H0, which is close to the 2π, in some sense, what is long-stochastic? It's long stochastic, then you can just run regression. Regress is not hard, okay? You run the regression with all this detail, then you have this, then you no longer have the analysis kernel. The analysis curve, okay, so you have very similar theorem says the signal-to-load ratio needs. The signal-to-load ratio needs to be slightly larger than root log n. Then, okay, this converts to standard normal. If the signal-to-load ratio, these two terms, 10 equals 0, cannot do much. This is the, okay, this is the theoretical theorem for my second part. So, I'm going to move to the numerical study. Any questions? Okay, so first one, it's a Okay, so first one, the simulation, I think, is quite interesting. Okay, so for GOF, the deal is if the model, the true model, if the true model is either correct or broader, then the GOF metric always converts to the standard or not. So we start with this one. The true model is SDN. So every member in this family. Member in this family certified. So you see everybody converted standalone. On this screen, the true model actually is the DCBM, DCMM. So DCMM is adequate. All other three in the family are not. So you can see DCMM is still converted to standard normal. But if DCBM you see is away from standard normal. SCPM is further away from Is further away from the standard normal. And this one is MMSBM, is modeled the mixed membership, but not the degree heterogeneity is somewhere in between. So this is a simulated example you can see. This is a true model, and if this model is not true, you can see the deviated from standard model. And there are two other different cases. Now I'm going to discuss 11 data sets. Devin data sets, okay. Most of them are from the literature. So, this is many people familiar with them, so this does not need an introduction. But a couple of them is from the MATSTAC data set. The MATS data is in collaboration with Tracy, Chi Pong Chung Chi, and some of my students we have collected. This is a large-scale multi-attribute data set. So, we have the citation, we have the bib text for. We have the big text, for example, for every paper. We have downloaded the title, abstract, affiliation, MSC, subsequent constitution, abstract, references, and everything. Not the main content, main content too long. But we have this many, okay? 41 years, so it's a very good idea for studying dynamic networks. And you can construct many social networks and also as text data, you can use it for text learning. And use it for text learning and so on and so forth. It's free to everybody. Where did the data come from? Was it MathScient or? Web scraping along. We go to dozens of resources, including MathSignet. But MathSignet has some issue, but you have to go somewhere else. Okay, so therefore you see, we have two later, and we have plenty artificial, but we have plenty of authors, but later you can construct them. The network you can construct them. So, this coercive network I'm going to discuss more carefully. This is very small, and this is a sub-network of it called the fan network. Okay, there's a good reason for that, so we will discuss them. And then many other, this one is also constructed from it. This is three accounts from it. So, these are tiny networks, very small networks, okay, so it's early literature of network analysis, but these are more recent ones, okay, more difficult ones. The web block is trying. ones. The web block is 20 years old, but it's actually a lot of nodes, more than 1,000 nodes, and it's also slightly challenging. Now here's the results. What would you say? When are you going to accept it under 3? Now, 3 is for simulated data is okay. For real data, it's a little bit challenging. So I'm going to make it a little bit relaxed to find. So, therefore, it's smaller than 5. I'm going to claim adequate. It's between 5 and 7.5. It's a slight lack of fit, but it's not terrible. It's above 7.5. It's mumbling. So you can see here, you see huge numbers here. You can see huge numbers here. So for the CIT label, you try SPN, you get a GOF. If the mod is true, you expect somewhere under 5. Okay, but you have 759. So for the small network, actually, if you say karate 5 SPN, it may not be wrong, but it only has 34 nodes. What we can say about the sympotic analysis. Okay? So just like Oberhamer said, okay, it will be nice the probability to electrify the atmosphere to zero. Over time we can offer it near the zero. Over time, we can offer it near the zero. We can never make it zero. So, because along have 34 nodes, and for others, you know, for the others, you can see a reasonably small, okay? Except this one and this one. I'm going to talk a little more about these two. We are very disappointed that the web block data does not fit that well. Because we expect it to fit perfectly, but it got a score. But it got a score over 7.7. And the DCBM, not that bad. Actually, DCBM, not that bad. So this mode has been used as a DCBM in many, many papers. They are not that bad. It's 9-something. But if you use SPM and MSPM, that's terrible. The number is much larger. But then, what's the reason? The deep-down reason, we do not know yet, but one possible reason we really But one possible reason we realize is the original data is not annoyed. The original data actually needs a direct network. It's a direct lit work. The age means one block referring to the other block. So the original data is one way, it's not two ways. But the popular way block data we use for communication is after you brute forcely make that add direct data. Even just one direct edge, you make the One direct edge, you make the double edge. That's why you get this data set. So, therefore, by nature, it's not really an elected work. So, that's the main reason. The other reason it has a long story, so I'm going to spend more time on it in this model, is this network. So, the co-authorship network had 236 nodes, and this has a sub-network called a fan-j2. This has a sub-network called the Fan Changing Fans Network, as I said in my network. Okay, before I get to there, let me summarize the result. SBM, DCBM, and MSB are inadequate, especially when the network is large. The main reason they do not model either severe protein or distinction or both. The DCM, however, is more encouraging, it's generally adequate, but a slight neck of feed for the two of them. But just a reason. Okay, so this is the network. Okay, this is the network. Is the network? This is the network. This is constructed using our data set. And I tried this network in somewhere around 2014. So at the very beginning, I have tried four different methods. I assume it's a degree-created block model. Because by then, I'm not aware mixed membership is that important. So I'm using the more popular degree-created block model. More popular degree-grade block models, so no mixed membership. And I considered the community problem. We have tried four methods for this model by Zen. So some of them by Peter Pico and some of them made by Newman, Carl Newman. But all these four methods, they agree with each other nicely with block data, on other data sets, but they cannot agree on this data set at all. Their results are drastically different. Drastically different. For example, this is what a fine bounce score. So there's a UNC cluster and there's a Ray Carroll, Cairo Hole cluster, and this is a Jenging fan. Later, we find out the three possible reasons. Weak signal. This network action, the signal is pretty weak. The number of average degrees, only 2.5. Only 2.5. And maybe the other. And maybe the other is the wrong model. We should assume DCMN instead of DCBM, and maybe our average is just that. It turns out we tend to believe the second reason. It's tend to the second reason after we take a look about all these authors. Because we find in this late work, about this 79 is the co-authorship, all its co-authors. And JNE has a mixed membership. Why? Because. Why? Because he is a big guy in non-primaturing. So he had big ways with Ray Carroll and Peter Ho, who are non-pronatural. But he had been on the faculty of UNC for many years. So he had a lot of postdoc students like Ren Zhou Li and people come from there. So therefore, Jianqin overviews has mixed memberships that cannot be relegated. So therefore, my experience or find is the network has two interpretable Network has two interpretable communities: the Carol Hall, which stands for Lamprometry, and the Sister North Carolina, including the Duke. We have Duke folks here, UNC, NC State, and other universities there. But a lot of them are faculty on these three universities and also their collaborators, former students, postdocs. But for his leadership in Lamp and past affiliation with UNC, Jianqing. Affiliation with UNC Jianqing and her student students, many of his students, like Chen Minjiang, Renzo Li, and many others, and also collaborators like Permi Klan. They are actually in there. Therefore, DCBM is not appropriate. We prefer to use this APM. And instead of the community detection problem, we should consider the problem as membership estimation. So the community detection is not. Estimation. So, the community detection is not reasonable here because these people have heavily mixed membership. So, this motivated us to develop the mixed code algorithm specified later. There's another thing I would like to say about mixed membership. So, the first time I come up with the score idea, that was 2012. That's the first paper, a project I work on on social networks. So, I go to a So I go to a small journal club kind of meeting hosted by my former colleagues, Steven Feinberg. I showcase the real data results on Karate and our web block. Then you see? This method is very simple. You see, very simple. Only have three lines coding, but it produces an error rate. It's low by far. It's only like 3.4% by Z. I expect. Unexpectedly, Steve rolled his eye on me. I'm not impressed. Asked why. He said, You did not model the mixed memberships. So from then, I recognize mixed membership is something big. I have to model it. And then until I get in to this network, I really come to fully realize the importance of Linux membership. You do not model it. You do not model it, then you become DCBA model. The method do not agree, which means the communication result is not trustworthy. You model it, let's say the results. If you model the DCMM and you estimate the membership, here are the results. Here are the most mixing nodes. Here are the most pure nodes in both communities. Here's the Jianqing Fan. How many students and friends in Jianqing Fan? So, okay. So, okay, then this is former postdoc dating fan and collaborator dating fan, former student dating fan, former student, former student, former postdoc, former roommate, roommate dating fan. And okay, many of fan's group are here with high degree notes. And here, these are more pure notes. For example, the Peterfall, Reikiro, Tony Kai. So these folks, they do not have that many collections with. That many collections with the UNC. But here, okay, so here are most pure nodes in North Carolina. You can see Joseph Airp and UNC, David Dancing's Duke. So this UNC, this UNC, this Duke. Okay, you see the point. Okay. Okay, maybe several more there. Okay, so therefore, okay, so by. So therefore, okay, so by okay, so in summary, okay, GOF is part of the tool to tell which models are adequate and which models are not, but it's not the only tool. For example, for this co-authorship network, the GOF mission fails. Why it fails? It only have 200-ish nodes, so it's very small. And also, the average degree is very small, 2.4, 2.5. That's very smart. It means the signal to both ratio is very small. Smart means the signal to motivation is very weak. You know, these kind of cases, okay, we take a terminal approach, we feed with different models, we use the method we like most, and this one turns out to produce much reasonable results compared to complete detection. I'm going to use just two minutes to talk about power. For the GOF, traditionally people do not talk of power much because power is just a different project. Is just a different project, okay? It's a relatable different project, and a lot of extra effort. But we do, okay, we do, we can analyze the power, and it's also very interesting work. We convinced, we have not solved this much, but in the future we probably do more work on this. But the main idea is: if you can write omic hat as a mapping of A, then similarly, you can map your true omic to omic 2 hat. Omega to omega tutor. So this omega tutor can determine the signal to load ratio for the power. For example, so therefore you can learn from your construction of your procedure, you can figure out this n. You can neglect so minor things like regularization, but then you have this omega. So you have this omega tuita, then this is determined by power. We consider a toy model where is a true model with this mm with two. Model is DCMM with two communities, some parameters. Now, the assumed model is DCPM, so this is not adequate. Then you see we have power. So the signal-to-noise ratio is lower-bounded, so as long as the right-hand side goes infinity, the power is going to be one. I'm going to wrap up here. So severe degree hologenetic and the mixed temperature are prevalent in large networks. So this is something we have to model. This is something we have to model. If you do not model them, then we still very far away from the real world. And this is quite extendable. I come to understand, okay, and I'm aware that many of you are doing tensor. I don't see too many hurdles. There are some hurdles generalized to like tensor, for example, PCA. But the cycle count has no variable. The cycle count is just almost the same. It's just almost the same. You can generize many of these ideas straightforward into consumption. And the Google needs to fit in a powerful way. And we propose a general recipe for doing GOF. And for the 11 data set, we use the DCM provides adequate fit, the other three models do not. And we also have a lot of nine forks to study when the Remarket model is an ECA model. So big down here is. So, big down here is not a problem, it's called a leap. I think Mariana points out what the technical deal. The technique deal is non-negative inverse a mining problem. So it asks, I give you k numbers. When you can, when, under what condition I can find the non-negative matrix with these k numbers as a minus. So that is a problem proposed in 19. Problem proposed in 1938 by Komorov and then by one of his students, Suleimona. Nova says, okay, Suleimona. So it stays there for seven years. It's not solved yet. So if you're interested, you can take a bite on that. Okay, so for the cycle count, you may look at this paper and for score, mix score, and the action trace, it has a red background stat. Survey pages that on the survey pages that so yeah, that's probably the basic basis to give you interest with this and for the magazine. Okay, thank you very much. This is all helpful. Questions? So, as we pointed out, and as we showed in the examples, you considered Examples, if the bottom is not right, you get very different answers, but you get very different groupings of nodes. So I guess I'm trying to think about why not fit a model, fit a model and estimate parameters that is most general. So for example, in a latent factor model, you just let this, so the way I would do it, I would have to look at. Way I would do it, I would have a logic, but I know you don't. But hold on. So then you have a matrix that represents the probability of these sort of tied with each node. And then, so you estimate that, and that's sort of pretty much unstructured. It's not making that many assumptions other than a rank. But then you can say, now what sort of groups am I looking for? Then you can apply, say, if I'm looking for groups that have a strong within group. Have a strong within-group connections that I can then take that fitted matrix and then find those groups. Or if I'm looking for a different type of group, like a mixed membership model. So I guess I'm thinking more like people will use the stochastic block models, well, because they're easier to analyze mathematically, but also because they give you like an interpretable parameter estimate, even though it's not really representing what's going on. So I guess I'm just trying to think about distinguishing between. I'm just trying to think about distinguishing between the modeling part and having a flexible model that can capture the different types of network patterns you might see. Then, once you do that, then say, well, how do I take the result that I got and a separate step sort of make me take a side piece of these? Yeah, that's actually a great comment. I think that's probably precisely next step we are going to take. Okay, so I think your point is, okay. So I think your point is, okay, you always want something analysis, right? You want doable, okay? So if the model has like a non-linear part in them, for example, if you take a latent space model, I think Zongbi Ma has some work on how to fit it for the frequency viewpoint. Here, okay, no, here. Okay, so here, okay, so this logic introduced some kind of language. So, first of all, it breaks the low rank structure. Well, it's still low rank on that scale. No, if you write on, okay, you cannot make this omic on low rank. Okay, so this omic is low number. So, this omega is no longer no rank. But something else is low-ranking. Yeah, yeah, yeah. Okay, you can write it this way: this is no rank, this is not, this language part is not. Through the logit, you can make it, you say the logit probability of a pi is counted out equal. Yeah, but the link function is different, right? If you rewrite this form in my model, your omega becomes omega star. Here has a non-linear matrix times this omega. Okay, okay. That's not really the mean. That's not really the name. Okay, yeah, no, no, no. But you can have degrees different. Just the OJ would be like you have some row effects, some column effects, and then some of the things. Yeah, I think the short answer to that might be some technical difficulty. I guess what I'm more wondering about is the using models that you know are wrong or that are not very flexible just because they have the nice interpretation of the parameters. So why not fit? So why not fit the most flexible model, or at least a model that's flexible, that's a captured model? Yeah, if you maybe the broader just analysis. And then maybe you can test to see. So you don't have this kind of theorem. That's what I worry. So you can always do good fit, but whether this convert is the normal or not, I do not know. That's just an analysis issue. So if I do not have the theory, I pretty hard for. It's pretty hard for me to make judgment here. I think that's why I think DCMM is kind of a sweet spot. It's still approachable mathematically, but if you make it broader, I need an extra device kind of that. Never seen it here, so that crawl fits. Okay, anyway, so what the good news is, I think I have a recent paper which I use a recursive algorithm deal with line terms. How does that deal with other returns? Thank you. Any other questions? All right, I was thankful, Speaker Kat. Now we have a coffee break for the 30 minutes. What are we talking about? Yeah, 20 minutes. 20 touches. 