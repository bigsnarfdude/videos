I send it by email to all beer participants. So your email should have a link to the Shiny app, which looks like this. Sorry, actually, if you go to my website and you find CGM, it actually has all of that there, which is why I'm displaying it. But there is a Shiny app. The Shiny app is if you don't want to have R and you just want to play with some of the metrics. Fun to play with some of the metrics that I'm going to talk about. You can do that here, and there is a pre-loaded example data that you can use for that. Alternatively, if you go to research CGM on my website, you will have a link to slide tutorial. Those will be the slides I'll be using today. There's R script only if you want to just call the R. And then there's a positive cloud project which has the environment with R argued. Which has the environment with R markdown, the slides, and the R script if you want to follow along. Alright, I'm going to make sure that whoever wants to pick their own adventure has access to that before I start. Should I share my slides through the Zoom? Or can I just go like that? Like that. You've got the slides available for people to follow along online. Yeah, I do have them online. Yeah, and it should be on email. Okay. Any questions? Everyone has access to materials who wanted to? Great. Well, welcome everyone. I think you all know Piamarina Gai Nanoa. I appreciate you being here at 9 a.m. early in the morning. So my goal today is to My goal today is to introduce you a little bit to CGM. I'm actually not going to talk a lot about CGM. I'm going to go straight into the package. And I'm going to talk a lot more about common metrics, tissue-del biomarkers, outcome measures that are used with CGM data. I then will describe a little bit of more non-standard metrics, some visuals, and throughout I'm happy to answer any questions you may have. So, this is for those of you who want to follow along and be hardcore. You can install igloo and diplo from Sirat. It's more as the same as I have on GitHub. You can also install it from GitHub if you want to. And the slides also have a link to the Shiny app. So you can follow that way as well. Alright. So the objectives for today is: I want you to get a little bit more familiar. I know many of you are already familiar. Familiar, I know many of you are already familiar with CGM data. I'm going to talk a little bit about the context of its use. I'll show you how to visualize it in my glue, consensus metrics, apply SMI control, some additional metrics and time permitting I will end with some broader perspectives. Alright, so CGM has small wearable devices. They look like this. They typically go in the arm. They have a little tiny needle in them, and that needle measures in. Them and that needle measures interstitial glucose, so it's not technically blood glucose, so there's a little bit of a difference between the two. The typical measurement frequency is five minutes. One meter lasts 10 days for Dexcom, 14 days for Libre. Those are the two most common on the US market. And so, what happens is that you put it in, you shower with it, you sleep with it. When it's time, it beeps at you, you take it out, you throw it away, you put the new meter in. It's very easy to put in. It's very easy to put in. When you buy them, they come with this like stapler thing, and just like I think it's important to remember: don't look inside because the needle doesn't look that small. Men seem to be more issue with needles, actually. I thought it was. Yeah, so I think before we go into it, this is the slide that I like to say. If you know nothing about the glucose, it's important to kind of keep in mind what the normal glucose looks like. So normal meaning you don't have diabetes. So the glucose range is consistent across individuals. The normal glucose range is somewhere between 70 to 120 milligrams per deciliter. And the main challenge is it's a very non-linear trend. It's highly dependent on the environment. It's highly dependent on the environment. So, depending on what we eat, our glucose rises. When we exercise, we spend the energy. Our body transports glucose into energy, so the glucose goes down. If you have stress, it's affected. If you're sick, it's affected. Whether you eat like very carby pizza versus you eat something that is, you know, more like whole wheat, like all of that is going to affect how the group goes looks. But generally, you will see the peaks from. But generally, you will see the peaks from immune intakes. And within the diabetes, and in general, when you go to your doctor among your blood tests, you will usually have hemoglobin A1C test, which is a standard classic, very well validated biomarker glycemic control. And basically, it represents average glucose for the preceding two to three months. So, this is what the CGM data looks like, and this is a person with type 2 diabetes. A person with type 2 diabetes. This is five-minute frequency data. You can plot it very easily using iGlue as I'm illustrating here. And the horizontal lines on this plot, they correspond to what is called in range. The below is 70 and the upper one is 180. So recall, I told you that normal people usually are 70 to 120, but for people with diabetes, the typical target range is 70 to 180. To 180, and they already have a really hard time staying within that range. So you can see based on this that there is a lot of picks, there's some missing data. Overall, I would say it's some more smooth data compared to octigraphy, but it still has this like expressions, peaks, and ideas. Any questions so far? So when do we use it? So, when do we use it? So, most commonly, it's used within the context of diabetes management. So, people with type 1 diabetes, they have to take insulin and they have to decide how much insulin to take, not just based on what they're going to eat, but also based on where their glucose is now. So, American Diabetes Association recognizes GM use for all people with type 1 diabetes. I think the actual use is somewhere around 50 to 80 percent among the people with type 1. On the people with type 1, and so they have an app on the phone that connects to the meter. And on that app, they can look at their glucose values real time and then they can make decisions when they need to take insulin. This can also be paired with insulin pump. And if you try to fully automate this process, this is what's called artificial pancreas. So you have a CGM that measures glucose, and then you have an insulin pump that pumps the insulin, but I'm not going to talk about insulin pumps more about that. Talking about insulin comes from what we have. With people who have type 2 diabetes, it's a little bit more complicated. Some of them are on insulin therapy, and for those who are, American Diabetes Association also recommends CGM. Most people with type 2 are not on insulin therapy, and insurance will not cover CGM for those people. But if they were to use it, then there is a lot of more behavior modification you can do because then you can see how your food affects how your rise. Affects how you rise, and that can be very powerful. I had a gestational diabetes myself with my youngest child, and it was really helpful for me to see, instead of a donut, I would eat half a donut, I'll have this, and then I'll wait, and then I'll have another half a donut. And this way, I sort of stayed within. And then I see it's going off, I was like, oh, I should go for a walk. So it was very motivational to actually adjust your behavior as you. Adjust your behavior as you see what's happening. So, diabetes management is sort of the first broad category. In clinical practice, endocrinologists use this data to make treatment recommendations. So, as they see patients with diabetes come in, they usually pull in the metrics from CGM. And based on this metrics, they would say, Oh, you know, I want you to adjust your insulin, I want you to increase it, or I want you to decrease it, or I want you to try this or that. Within clinical trials, Within clinical trials, CGM data used for outcomes to evaluate the efficacy of drugs. So you will have a controller treatment group, you will put CGM data, and then some of those metrics and the change in those metrics, you will see whether the drugs are effective. And then the research at large, of course, we can look how baseline characteristics affect glucose, how comorbidities affect glucose, what are general associations. So I think that's a pretty comprehensive book. I think that's pretty comprehensible nutritionists in other work. I have some collaborators that are looking at different diet effects within non-diabetes populations just to see how we react to protein or fat relative to carbs. And CGM data is very helpful for that too. Okay. So now, what do we use from CGM data? There are a lot of things one can do, and the community actually exploded. Community actually exploded, in my view, with the availability of metrics. This is an older paper, but it describes over 40 different CGF metrics that we can compute from this data. And many more continue to be proposed and derived in the literature. So in our package, basically, and it's by far not the only package, there are many packages out there. This is the one I obviously know the best. And we're just like, let's compute them all. I don't know which one is important. Which one is important? Let's just compute them all. And we have some idea of what they should look like for non-diabetic vulnerabiles because there is a recent study that looked at sort of what the range is alpha-holked. But there's a lot. So the good news, and this is the paper that Madim highlighted yesterday, is that there has been consensus on what are sort of the standard metrics to use. And different metrics obviously capture different. Different metrics obviously capture different aspects of lecemic control. Software generally agrees on how these metrics are computed. I say generally because there's a caveat to that that I can elaborate later. And there is a consensus, so that's really nice. Of course, a lot of these metrics were designed primarily for type 1, because that's still the biggest user base. So if you start looking at different populations, you may have to sort of revisit which metrics. Revisit which metrics may be most important for that. Some of the metrics are selected based on interpretability. Clinicians want to understand what the metric is. The patients want to understand what the metric is. But of course, we know that there may be more complex ones that may be useful, even maybe not as interpretable. And if you have more complex metrics, the more complex the metric, the more variance there is in translation, so there's some differences in algorithms. So, there's some differences in algorithms. And that's all I'm going to say, but I do have a discussion point. I think this one you can find online. It's a little hard to find. So, if you want to look at it, I'm happy to send it to you. All right, so we're going to go to example data sets now. And there is two example data sets in iGlue. I'm going to use primarily the first one. It's a very non-imaginative name, example data five subject. It's five subjects with type 2 diabetes, dexcom, G4. To diabetes, Dexcom, J4, CGM, five-minute frequency. There is another one, example data hull, that is both pre-diabetes and diabetes, similar frequency. In general, if you're interested in public CGM data, we have a GitHub repository, Awesome CGM. It has DOI on Zonata. We have a wide collection of public CGM data sets with processing. The caveat is that some of them are public, but we do not have hosting rights. But we do not have hosting rights. So, basically, what this means is that we will direct you to where it's hosted, and then you can run our scripts and get it exactly in the format that you need. So, that's kind of how we did it, to not deal with a hosting client. Yes? Quick question. So, the frequencies, can that be changed or is it? They could change. Five minutes is most common. Some Odyssey GMs have 15-minute frequency, and Libre technically has one-minute frequency, but they always show. One minute frequency, but they only show you five minutes. But minute, you will not find a frequency less than a minute. One minute would be the most frequent, and five minutes is the most common. Okay. So, and again, there are additional resources. So, if you want to follow Amo, we have two papers that's basically extended vignettes. The website has vignettes. The website has vignettes. There is additional vignettes on Mage, episodes, lasagna plots. So you're more than welcome to go through those. If you already know CGM and you want to do that while I'm talking, fine with you. All right, so let's start first with how much data do you need? And there is consensus guidelines on this, and the consensus guidelines state that as long as you have an outpatient GM data, Patients' GM data, you should aim to have two weeks of data with at least 70% cognizance. Now, I have some thoughts on whether this is right or not, but this is the current consensus guidelines. So if you're sort of just starting, I would say this is the easiest way to kind of have your inclusion criteria. For inpatient settings, it's much less because we don't have patients usually inpatient for that long of a period. But that is the standard. But that is the standard. And the first thing you can use iGlue for is to check what does that look like for your data. So we have this function called active% for our five subjects. And it will tell you what is the number of days. So what is the start date, the end date, and out of those days, how much, what was the percent of non-missing measurements in them. And generally, the missing can happen for various reasons. Sometimes the monitor disconnects, so you just have very So, you just have very short periods of missingness. Sometimes the device falls off, or you change the device, and then you have a longer period of missingness. And visually, you can sort of see that that aligns. You can plot all those five subjects simultaneously. In particular, for subject two, you see that there is a pretty large gap here. And if you go to Active Percent, it will tell you I have almost 17 days. Almost 17 days, but I only have 59% of the colleagues in the data line. Questions? So other ways you can visualize the data. So John alluded to lasagna plots, but hasn't shown them. I'm showing you the lasagna plots. It's really plot lasagna. In this case, I'm plotting one subject. You can plot all subjects, but I find one subject is more informative. And basically, each day here is a row. Basically each day here is a row, and I use colors to represent the glucose. Those are non-traditional colors in the diabetes community, but I really don't like traditional colors. You can do traditional colors. The reviewer asked us to do traditional colors, so there's a toggle for the color scheme. But in this plot, the red corresponds to higher glucose, the blue to the lower glucose, gray are the periods of missing data, so it just kind of gives you the visual illustration of what are the highs and what are the lows. What are the highs and what are the lows? What are the traditional colors? Traditional colors are red and orange. No, sorry. Yellow and orange for high, red for low, and green for the middle. And you will see them. Yes, I agree. You will see them when I show time and ranges, especially given that red and green. There's a reason for that though. I think it's it's like the alar it's like an alarm sort of because like green in the middle means you're safe, you're in a safe zone, but red for the low is like you could go into a hypochondriac. Well yeah, but red and green most oh yeah, red and green for the buttons. Yeah, definitely. But I think that is like because it sounds insane, but like that's the least that you need. But yeah, anyway, so you can do those colors for lasagna plots, please. For lasagna plots, specifically, you can do this colors. What is probably my favorite feature here is that you can pick your lasagna type. In particular, your lasagna type could be time sorted, which basically sorts within each time period. And that gives you the visual representation of what is the average profile without the 24-hour period. And so basically, what you see here is that there are sort of more peaks during the day than during the night. So you see that the person is. So, you see that the person is generally lower during the night, and you can sort of see that they probably have the highest meals usually in the afternoon. So, it's what it shows. Yes. What's going on with the... Is it missing data? It's because it's missing, yeah. Why would it have that sort of pattern? Because the y-axis is dates, right? Yeah, because it's sorted. So this one is the original. And if I want to look at the average patterns over. Look at the average patterns over 24 hours, I can sort each column. And that effectively, what it does, it sort of tells me where the average peaks are. Also, where the average missingness is? Yes. Yes. That as well. You see the periods where the most missingness is mighty. That's more missingness than I would have expected. Is that is that usually what you see? I see missingness all over the place. Sometimes it's less, sometimes it's more. Times is more. I think it depends on how well it's curated. This one is Dexcom G4, so that also could impartially be wide. I also found that people who are not typical CGM users are not good users. So you have disconnects and things like that more often, sometimes. But yeah, it varies. Dex from G7 in particular, for some people, has a lot at this point. Particular for some people has a lot of disconnects, and then you have tons of this in the side. What are the other lasagna things? I think I had days sorted too, and I think I had subject sorted. Documentation will be the best. And if you're interested in lasagna plots, I think I just went crazy with lasagna. There's a vignette specifically for lasagna plots on the website. We haven't done much with lasagna. We haven't done much with them except created them and said, hey, this is what you can do. But I think it can be useful to try to do more with it. Because I do like this way of displaying the data. In the IBTs community, as I'll show later, they usually have fixed times and range. And it's basically like five. This is like a continuous scale of the same idea, but with a color. So I kind of like that. Yeah. All right. Alright, so what are the common summaries now that we know what the data looks like? The common summaries, and those are the consensus metrics, I mean and GMI, time and range measures, coefficient of variation, GRI and glycemic episodes, and I'm going to go through them in that order. I can calculate the mean. Yay! We can all calculate the mean. So that's exactly what you think it does. It just takes that whole profile and gives you the mean. It gives you the mean. And now intuition is matched. You can see that subject 2 has the highest mean. Look back to subject 2, they do appear to be the highest. What is maybe less obvious is that GMI is the mean. It's a linear transformation of the mean. GMI stands for the glycemic management indicator. It's replaced EA1C, which was the previous A1C indicator. Was the previous A1C indicator. So basically, all it does is puts the mean on the A1C scale to make it easier for clinicians to think of this measure as if they were thinking of A1C measure. Specifically in the context of A1C, you are considered to have pre-diabetes if you are between 5.7 and 6.4, diabetes if you are above 6.5, and typical treatment goal for diabetes is to keep your A1C within 6.7. Was to keep your A1C within 7%. So when I translate this means you can see not surprisingly that subject 2 is the highest because again it's like deterministic linear transformation of the mean and the others are a bit lower. I guess the conclusion for you, if you do want to play with those, don't use both of them because the correlation is one. But one of them was just translated, so clinicians. Was just translated so clinicians have an easier way of interpreting it. Now, time and range is currently the most common and accepted standard. I would say if you do any kind of CGM metrics and you don't do time and range, that will be the first question the reviewer probably would ask. Why have you not done time and range? And what it is, it is the percent of time you spend within those bars. So the time and range specifically is defined. The timing range specifically is defined for 7180, although you can adjust it within these functions. It is a percentage. So here it says that subject one, 92% of the time they are within the range, they are in range. And subject two is the least percent of the time in range. And typical goal for diabetes is to try to stay in range at least 70% of the time. So this metric, you are just for missing data somehow, or it is just a great question. Or it is just that. Great question. This is complete case analysis metrics. I have some simulations to show that that's sufficient. In some cases, it may not be, but the iGlue by the Cologist does complete case analysis. And the idea is that a good analysis will look at the active percent first to see what the amount of missingness actually is, do the inclusion-exclusion based on that, and then calculate the metrics. There are some caveats. You could have informative missingness, but it's usually not affecting time and range. So the most meters, they range as 40 to 400 milligrams per deciliter. So they will not measure above 400. For type 1, it's more rare that you will go above 400 because if you do, you will be in the hospital. Type 2 can sort of walk and breathe. Can sort of walk and breathe and be above 400. But the meter will just say 400 or high. But because 400 is way above 180, that usually does not create issues for us. Big question. All right, so you can check our intuition. So subject one, most in range, you can see it from that plot. Subject two, least in range, you can also see it from that plot. And the plot, you can also adjust those thresholds. Can also adjust those thresholds if you wanted to. I'm just showing you the defaults. That's very accepted defaults: 70 to 180. All right. So they like time and ranges, so you can do this exercise and basically divide your whole time into five categories. So you have hyperglycemia, which is high glucose, and you have hypoglycemia, which is low glucose. And then for both high and low, you have level. For both high and low, you have level one and level two, with the idea that level two is more dramatic than level one. So below 54 is level two, hyper, between 54 and 70 is level 1, above 180, but below 250 is level 1, hyper, above 250 is level 2, hyper. And the sum across all is 100%, and that gives rise to the bar plot, which is a very common display, and those are the colors that I was trying to explain. That I was trying to explain. This is the standard colors accepted in the diabetes community. If you don't use those colors, they complain to use those colors. Hyper, level two, orange, yellow, in range, green, and then red for hyper. And just for your information, if you don't have diabetes, you're probably in a range about 95% of the times. Yeah. So how do you organize those like level two measurements? are those like level two measurements you say you have like two diabetes so let's think a little bit of I don't know if I have it on the slides but why why are they differentiated the consequences are different so hyper which is high glucose is nerve damage that is basically slow death complications neuropathy all of those things is because high glucose basically fries your nerves so the higher you go the worse it is from The higher you go, the worse it is from the long-term perspiration. Low glucose, very rare for type 2, very common for type 1 because it's a result of overtake of insulin. Low glucose, you can die on the spot. I guess I'm more interested in the hyper, but you mentioned that the over-the-counter devices that you can buy without insurance don't go yet. They don't go above 250. I mean, for type 2, not going above 250 is pretty low. To go in above to 50 is pretty limited because I've seen a lot of type 2 that go. But if you're well-controlled type 2, hopefully, you don't go there as much. But yeah, you will just not have as accurate understanding, I guess, of how high you went. But for type 2, maybe it's like if you can control it, it's not as dramatic. I think it's all about how often are you actually there. And I think the reason is like people without diabetes never go there. People without diabetes never go there. So, if you do this over-the-counter device that is sort of consumer-grade, you will never need that range. But if you do have diabetes, you do need those numbers potentially. But yeah, very different consequences in terms of being too high or too low. Everyone gets so quiet when I talked about the consequences. All right. All right, I think we're still doing great. I'm going to get a little plaster. So you can do this beautiful plot in iGlue. There's this function called plot ranges that does this plot for you. So you can see how much everyone is in different range. You can visually compare. You can calculate those percentages individually if you wanted to. You can adjust which thresholds you use for below and for their above. Now, the fixed thresholds are very good. Now, the fixed thresholds are very common. If you want a data-driven threshold, this is like a shameful plug. We have a recent work that can give you data-driven thresholds in an unsupervised manner, which may give you more discriminative power. So, if you want to try it out, it's not in iGlue, it's in Python. A wonderful POSDOC did this work at June and Clark, but you can try it out. All right, so the next measure is coefficient of variation. Again, pretty standard. Again, pretty static. This is just D over the mean. What you perhaps don't know is that within diabetes, the typical treatment target is to have this number below 36%. So they look at it usually as a, is it below 36% or is it one? I think in medical field people just log thresholds, so everything has a threshold. Common way to summarize all of this metrics is umbulatory glucose profile or A. Glucose profile or HAP. So it will tell you, it's a little overlaid. I apologize. You can do it with iGlue. It gives you this bar plot. It gives you the duration you had, percent it was active, that is not missing, what was the average glucose, glucose management indicator, coefficient of variation. And then also it gives you like a visual of the average 24-hour plot for the quartiles. More complex. More complex, glycemia risk index, or glucose risk index. This is actually should be very interesting for statisticians because what they did is they collected a lot of CGM profiles and then they asked a lot of endocrinologists to rate those profiles in terms of what they thought was most severe and less severe. And then they did principal component analysis on those ratings, and based on those results, they figured out how to combine the metrics. The metrics. So basically, what they wanted to do is rather than have this five time and ranges, they wanted to summarize up and low into one number. And this is how they summarize it based on the PCA. So you basically take those percentages you spend at level one and level two. You take a linear combination of those percentages where those coefficients were determined from the species analysis, and you arrive at one number that is still unprecedented. Number that is still in percentage and is capped at 100. So if you're 100% in range, by definition this will be 0. But as soon as you start going outside of the range, the relative weights are different. And if this is above 100, it just returns 100. Again, sort of the lesson here is if you have all of those individual five percentages, this is x. Percentages, this is extra because it's a direct linear combination of the four groups. Just said that the hyper, that the time and hyperglycamic has different consequences than time and hyperglycamic, right? So what is the value, I guess, of this index if you are mixing this together? They wanted one number, and they wanted one number that will be representative of. Number that will be representative of what the clinicians think is more severe versus less severe. So that would be like an excuse to look at these other indices before other characteristics, right? You look at this, okay, it's 20%. I better look at the timing range. Is that so? I don't know that they look further than that. I think they really just wanted one number. I think there is like this pursuit of one number and glucose. Pursuit of one number, and glucose is not one number. But they want one number so that they can say this one is better, this one is low, you are improved, and you are not. That is my understanding on this. Yeah. Does it make a distinction between no, it does not. You're correct, because one number they do weigh hyper more. In general, within type one, they are more worried about hypermore. Type one, they are more worried about hyper because that's immediately. Yeah, yeah. You mentioned that the amount of time we need is if you have at least 40 days or more to these personally, I think the more the better. So the more data you have, the better. So, I think 14 days is what there was one study done that basically showed that if you have 14 days and you care about three months, that it's sort of well representative. We are trying to be better at defining what well representative means. But a lot of the studies just do 14 days, and part of that has been mentioned, the meter. Has been mentioned, the meter will stay for 14 days. So, from the study budget perspective, you only need one meter if you do 14 days. You need two meters if you're going to go beyond that. So, I have sort of some thoughts whether this is enough or not. And I think it really depends on what it is that you're trying to get at. You mentioned that they calculated this based on principal component analysis. Did they take into account? Did they take into account compositional nature? I doubt. I haven't read that paper in a lot of detail, but I doubt very much that they took into account competitional nature. But I also don't think you will be able to get access to those individual ratings. Although I haven't tried. Wow. Alright, so the next one are glycemic episodes. So this is pretty similar, but it's actually... Similar, but it's actually algorithmically very complicated. So, the idea here is rather than looking at one number, they wanted to look at the excursion, and they wanted to count an excursion. And it's considered an episode if it's above the threshold. So let's look at the high glucose, for example. If it's above the threshold, it's at least 15 minutes. So, you need to look at the consecutive representation, and then when it goes down, you count it as one, and you keep going. And you can do the same. And you can do the same. So, for DAXCOM, that is five-measurement frequency that basically translates into having three consecutive readings that are above or below to be counted as items. We have a function epicult profile. This is just illustrating on one example. It counts episodes of different levels. It has extended episodes. It gives you total episode, average episode per day, average duration of the episode. It gives you an illustration. It gives you an illustration here where those episodes happen. So you see yellow at the top, there's two. So you see hyperglycemia level one total was two. You see red at the bottom, that is hyper. And you can also see the total was 10. It's a little bit complex how level one and level two episodes are computed. So here I'm just illustrating a visual picture. So because you have to go through level one. You have to go through level one to get to level two. You basically have a nested structure. So, in this example, you will have one level one episode, but you will have three level two episodes. And we actually had to adjust the definition to do this. And this is the correct interpretation, in my view. And for more episodes, we just have the paper out where I argue that that's how you should interpret. How you should interpret the episodes. And then you can also get those numbers directly. Episodes are generally considered a secondary measure. So if you're just starting, use time and range. Episodes is something that other people use. You can play with that. They are more complex. And there's more disagreement across software and how they're managed. And the missing data here to the earlier question you have to interpolate because otherwise you will miss. Because otherwise, you will miss the APISOLs. Yes, so that is more with the rate with which the glucose change, and that is the metric I will also describe a little bit. All right. If you want all consensus metrics at once, we have all metrics with a toggle consensus only. It will give you all the episodes, time and research. It will give you all the episodes, time, and range. That was like a popular request, so we did it. And that gives you all of the consensus ones. Among episodes, only extended episodes, extended are like 120 minutes in duration, will be included. If you want all the episodes, then you have the episode function. Do you have a question? So we have similar, we're running into similar issues with like emodynamics and like time and range for certain things, but our clinical collaborators are saying very much like it's the actual time. Very much like it's the actual time, not percentages. And the fact that different people have different observed times because it's like surgery. So, do you have different observation times per participant, or is it much more similar to protocol? I think you can have different observations time per participant. I'm just counting percentages, and I think my answer to that will be similar to the answer I gave. I will look at the active percent first. I'll do my inclusion-exclusion based on the. My inclusion exclusion based on the availability of data, then I will compare the percentages directly rather than the minutes. You could get the minutes out, you just get percentages, times. So like yeah, it just seems like percentages are generally accepted. Percentages is generally what they look at. Yeah. From the clinical side, and I could tell from my collaborator, they look at the percentages. Functional value to modify all measure. All metrics, no, but the individual wants to. You can modify the thresholds, and yeah, you can get all kinds of thresholds you want. Alright, so what we discussed, mean glucose, time and range, C V G rhyoglycemic episodes. If you want only one, time and range is the one to pick. Beyond that, depends on your collaborator, I guess, on what the question you have. More metrics. More metrics, there are 60 plus. If you really want to look at all of them, we have this metrics heat map function in the package. This is using example data from Hall because it has more subjects. It's 19 subjects. So basically, it does hierarchical clustering of both the metrics and the subjects. And that gives you at least some visual idea of which metrics tend to be more correlated in your data, which metrics tend to be less correlated in your data. So if you really want to do So, if you really want to do like exploratory metric selection, but you didn't want to do 60 multiple comparison adjustments, one way you can do it is do this hierarchical clustering, look at the groups that are formed, pick sort of one representative from each group, and then go that way. I think by default we use correlation distance measure. I think it can be adjusted. And some of them I will describe it. And some of them I will describe. I think Vajim asked me to describe those, so I'm going to describe them. And those are variability measures. So those are non-consensus measures, but it's among the list that was proposed. So one is called SD measures, and it's actually based on the original code by YATSAC. It has been modified many times since then. And basically, the idea is it's not just looking at the overall standard deviation, but looking at standard deviation. But looking at standard deviation across days, within time, between days, you know, all variations and combinations in between. They're super highly correlated. So I wouldn't just like put all of them again in the model, but maybe you have reasons to care about specific times of day or between days, and that could be useful from that perspective. Mage is what's called a mean amplitude. What's called a mean amplitude of glycemic excursion. It's the most complex metric we have. It's also the one that you will find the highest level of disagreement across software. And the reason is that it's based on automatic peak identification. So depending on the algorithm you use for automatic peak identification, you're going to get different results. But basically, what it does is trying to identify all the peaks and the deers you have. The peaks and the deers you have in the data. It looks at the amplitudes of all of those. It filters the ones with the lower magnitudes, and then it takes the average of the ones that are left. This is sort of a visual illustration. It's a very busy plot. I fully acknowledge it's not the easiest plot to read. But all the red tells you the peaks, all the blue tells you the nadirs, and then there's the arrow that's trying to get at the amplitude of all of them. Get at the amplitude of all of those, and it's the average that it returns them. So, the idea is that if you're looking in the context of meals, this is sort of not a global variability measure, but it's not fully local. It's like excursion-based variability measure, so it does give you an idea of what the average excursion size is. And if you don't want to plot it and you just want to kind of look at it, those are the values for. Those are the values for the five example subjects. This is a milligrams per deciliter, so that's the average peak height. And based on this, you can see that subject 4 has the lowest peaks, subject 5 has the highest peaks. And if we look at the data, you can kind of convince yourself that that is true visually. Subject 5 has really high extrusions, subject 4 has really small extrusions. Because then you run into the so the normal profile I show is like you have this three wonderful peaks and it's very clear. This is obviously not like that. So depending on peak you use like do you combine those two excursions? Do you not? Like they're all of those things. Yeah, I mean I think like in the plot you showed there was sort of like a big drop You showed there was sort of like a big drop-off, but halfway down there was like a little uptick, and so like the excursions, like it's not measuring the whole thing, it's going like half and half. Some of them it does. I guess, is it to the right or to the left? Yeah, I think it's to the right. And maybe I'm misreading this plot. Maybe I want to buy too quickly. But like, honestly, my question was: do you, regardless of which of the algorithms, do you find basically the same ordering of subjects? Not necessarily. Okay. Necessarily. So overall, the correlation is better than the absolute agreement, but not necessarily. Which is some people say you should discard this metric, and one of the reasons it's not in consensus is basically they say we can't agree. We did some comparison versus manual calculation when we did this algorithm, so I have a reasonable confidence that we do what sort of visually the person would do, but Person would do, but none of the algorithms are perfect, so you would always have some difference. So I think if you use Mage, it's very important to specify the software that you're using because you would have very different results. All right, this one is the standard deviation of the rate of change. So, this is kind of more like how quickly they change. Again, this should be dear to our heart because basically we are looking at the derivatives. So, at each time point, So at each time point, I'm going to try to go fast, we have 15 minutes that we are looking at, and we are looking at the derivative within that 15 minutes. And then we can do this across all the time points. And this histogram shows you what those derivatives look like for one subject. And basically, because you go up and you go down, what this creates is a symmetric distribution where the positive derivatives. Where the positive derivatives correspond to going up, the negative derivatives correspond to going down. One of the rationales when this metric was developed is that glucose data itself tends to be skewed. So if you think of standard deviation of skewed measurements, that's not the best measure. This is always symmetric. And if you want to characterize the spread, that is how dramatically you tend to change, standard deviation of this makes a lot of sense. Of this makes a lot of sense. So, the sort of the larger is the spread, the higher is the derivative, the faster your glucose change locally. And fast glucose change is sort of unstable, so that's bad. You want it to be more narrow. It looks like the the right tail is a little bit bigger. Yes, in this case it looks not fully symmetric, you're correct. Uh is that because you go up faster than you come down? Up faster than you come down? I'm not sure for this particular subject why that is the case. That is possible. Most of them are more symmetric than the original Boolean data, I would say that. But there could be some asymmetry. In the literature, this asymmetry is not considered. They just take the standard deviation of that. And those are the values. So you can see again that subject 5 has the most dramatic rate of change. And from the plots, we're sort of And from the plots, we sort of remember they had the highest peaks on the amplitude. So you would still have high correlation, I believe, between these measures and the others, but they all sort of tell you somewhat different thing about the profile. Right. We have more metrics. I'm not going to bother you with all of them. We have reference on the left side and again heat map can give you an idea what are the ones that on your data appear to be more different. Appear to be more different. And just a note that I want to say: all CGMs have measurement error. So we're sort of worrying about how we compute the metrics, but they're not perfect. So kind of keep that in mind. Now, most of the CGM I worked with until recently was from curated studies. And so compared to Actigraphy, there's very little processing that needs to be done. So you just use the data as is. Just use the data as is. I'm now involved in a collaboration where I look at CGM from Research Data Warehouse, where basically the CGM data is automatically uploaded from the devices, and it's a mess. And it's a mess because you have repeated uploads from the same device, you have separate accounts for the same patient, you have glucose measurements uploaded from your CGM and from the pump. The pump reads from CGM, but it's not always set at the same time. Time, and so you have what's called repeated profiles and duplication that you have to adjust for. For most of you, it may not be relevant. If this comes up, we have an algorithm to deal with that. It's not perfect, but it's just something to keep in mind that you may not be able to just use it as is. You may need to check that there is no duplication going on. Yeah, this is the one from Ino. See if you can spot why this is my personal one. You can spot why this is my personal take. But since we're almost at the end of time, I'm just going to say that I think integrating CGM with other devices, that was a common theme here. I think that's really promising for artificial pumpkins in particular. I think trying to pair it with an insulin pump is a very hot area. There's a lot of AI, ML algorithms for trying to do glucose chloric casting using those measurements. The reproducibility and validation is something that I spent a lot of time on, and we're still working on it. Like, we don't really know which of the CGM measures we should use because we don't have long-term prospective studies on like risk of cardiovascular disease or long-term complications. So, it's a little bit of a wild quest. And glucose is not constant even for people who don't have diabetes. Even for people who don't have diabetes. So, how do we characterize those deviations is a little bit tricky. I think we need to have multiple stakeholders. I learned a lot from working with clinicians. I always push my, like, why, this is what I use, why this is relevant for me. And then I also had a lot of experience with patients, and that was also very good perspective to have sort of what is relevant for them. Novel methods. Novel methods, great. There's a lot of work on distributional approaches. I think functional approaches are more challenging for this data. You either have to do some registration or specific time periods. And then this is something that is again my personal opinion, but I believe we have enough metrics. So please don't do new metrics for new metrics' sake. I keep adding them to I glue in. I keep adding them to iGlue and it keeps growing. There's a lot of correlation among them. I think we should be a little bit more science-driven and application-driven. If there is really a good argument for this new metric, that is great, but I feel like we can better explore the ones we already have, have methods of development that are informed by data problems. And thank you very much. I'm going to stop here. Oh, yes. Outstanding. Very good work. Thank you so much.