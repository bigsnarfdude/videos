Beautiful voice and all the possibilities to listen to so many interesting speakers and talks here. And it's really quite a pleasure to meet you. Yeah, my talk is some kind of survey talk on the development on risk bounds and in particular the aspect of dependence uncertainty. And the main motivation for this subject comes from the regulation documents like Solvency or Basel. And the aim is to establish risk bounds under information, reliable information on the underlying model. And the importance of this question The importance of this question comes from the fact that these risk balance correspond to the risk capital which is necessary for the companies to obtain. And I will talk in the first part also a lot on the connection of this problem to mass transportation problems. And then I will go through some. will go through some of the models which we have developed and investigated. And in the final part there are some newer results based on strastic ordering metrics for determining worst case distributions. So in storastic In thoracic dependence, two main subjects there are the dependence modeling. We assume we know the marginals of the components of a risk vector and then we want to construct a model and we have some idea, maybe a parametric family, and then we have to adapt it somehow to the marginals, which happens through For example, through the iterative proportional fitting procedure and the construction space a lot on copular models. And the second main subject are the Hofting-Flichier bonds, classical Flushie bonds, which connect two forest ordering questions, extreme independence bounds, and which are one of the really early. which are one of the really earliest sources of these subjects. Going back a lot goes back to the Italian school of probability already in the 30s or 40s. And so in the case we have a risk vector with real components. Components, you know the margins xi, and then the question is: what is the maximum tail risk of the joint portfolio? Or equivalently, by putting the inverse quantile, what is the maximum or the minimum value of the value at risk of this portfolio? The value possible range is a closed interval, and somewhere in between. And somewhere in between is, for example, the independent case or the co-monotonic case. And the problem is to determine these upper and lower bounds. And to this purpose, my starting point on this was in the late 70s, and I introduced these classes. And I introduced this class of functionals. So I call Erfting Fouché functionals generalized, which describe the maximum influence of dependence on the integral of a function. And so this was motivated by these results on determining risk balance. And my personal And my personal motivation had also direct mathematical component. I was interested to see how can one improve the classical inequalities like Haufting, like Jensen inequality or Cauchy-Schwarz inequality. For example, Jensen inequality gives a sharp boundaries. Gives sharp bounds when one knows only the marginals of the components. And what would be the improvement if we know the distribution of the marginals? And similar for many of the classical inequalities. So that was also one mathematical motivation for starting this. And the main result was then the duality. was then a duality theorem. Oh, it's next question for this Hufling-Frouchet functional, which looks like this. And I established this mainly in the context of upper or lower semi-continuous function or functions which can be approximated uniformly by Uniformly by from the ring of the indicator function of product sets. And this result was generalized by calorie to general measurable function. And so that was somehow this duality side. I did not know at that side time about the Kantorovich development, so in his famous paper of 42, where he stated this mass transportation. This mass transportation problem, and he also gave a duality result for that mass transportation problem. And so the other problem is to transport a mass to another mass. And his duality result, however, I don't know whether you have looked at his papers, is only correct. Is only correct if the function is a metric. So, because the dual form is only fitting for a metric case. And it was later, sometime later, I found it in functional analysis book, maybe 87 or something. There was a correct version of that, but the original version was only in the metric case. In the Russian school, which worked a lot on these kind of problems, then On these kinds of problems, then mainly there was a lot concentration on the metric case or on the so-called master and shipment problems, which reduced metric case problem. And the main result here, the cantor which I'm theorem concerns. And so while this problem of generalized as a very natural direct formulation, it was formulated from the beginning for. Formulated from the beginning. For a multi-marginal case, the transport problem at first is transport between two cases, two distributions. And so one gets quickly for the value at risk and tail value at risk so-called unconstrained lower and upper bounds by the tail value at risk of. By the tail-value of the risk of the common atlantic sum. This only uses the convex ordering result. And this was the starting point of many more involved, more specific bounds. In particular, so there's a lot of further results. In particular, based on the duality theory, there was somehow one variable. Somehow one way was to find good dual functionals leading to various forms of dual bounds. And one interesting point is the description of the set of all distribution functions here with specified marginals as by the rearrangements of the inverse distribution function. Of the inverse distribution function. And this leads to a reformulation of the problem of maximal tail risk as a rearrangement problem. And so the basis of this result is a measure theoretic result on isomorphism of measure spaces by from by Rothni, from fifty two or something. Or something. And this led to the introduction of a rearrangement algorithm which allows precise determination of these bar bounds. And here is an example of Pareto, two distributed variables where one knows the exact values and one sees that the algorithm. Algorithm is very precise and is very close. And here, for an example, that are the Moscadelli data of operational risk. We have the lower bound and the upper unconstrained bound. And the range we see is very wide. And so that means this information is not really good enough to be applicable. And in this example, Be applicable. And in this example, it's a nice point. The co-monotonic risk has a value of risk curve which is close to the best possible. And so it's not the worst case situation, but it is closely to the best situation. Every data have a lot of habitat, right? Yeah. Yeah, yeah. Even the mean. Excuse me, I have so much material. Excuse me, I have so much material I would prefer just to discuss at the end, but otherwise only of course I don't I like to discuss, but I have put a review and so many stuff. And so the aim is to introduce conditions on the distribution, structural or Structural or dependence kind, so that and to solve these problems with this additional under this additional conditions in order to get closer and better usable results. And one could add here also martingale constraints, and this leads to improved price bounce, and this is the topic which actually has done quite a lot of papers. And of course, in this way, it becomes important to have good ordering results within the subclass which one considers and in order to determine. And the second point is these worst case risk with respect to risk measures correspond to non-linear mass transportation problem and also the case of higher dimensional. So in the case of higher dimensional risk is of interest. And I start with this point, with this higher dimensional point, so I have a risk vector, the components are d-dimensional. And I want to find a joint portfolio, which is the worst case. So I consider the risk of the sum in d equal one. In D equal one, this is given by the co-onatonic sum, which is long time no. But unlike in D equal one, it became soon clear that there is no general notion of homonotenicity. For example, there is no generalization of this common autonomy. No generalization of this common atomic improvement theorem. If one has a decomposition of an allocation of a risk into n parts, then one can find a co-monotonic allocation where each component has less risk than y, so it is a better location. But a result of this type is not possible. If one considers, for example, the case n equal to two, just two risks. To two, just two risks, d-dimensional and the L2 risk. Then the problem of determining the worst case portfolio is equivalent is the problem expectation x1 minus x2 norms pairs minimum. So that means just with the optimal coupling or mass transportation problem between these two. And one does not have, if one has a problem with n marginal. Has a problem with n marginals, then one cannot restrict it to the case of two marginals. There do typically not exist random vectors such that all pairs are optimal cover mix. In the normal case, this leads to a very strong restriction, common tragedy of the covariance matrices. And also, there do not There do not exist dependent structures, which are worst case for all convex risk measures. And here it is just taking the LP, minimal LP distances, and one sees how the domains changes. And so the main question is: if one has a convex risk measure, then we would. Then we would d like determine the worst case dependence situation, and that we could call that as a solution we could call rho common. So it depends on the risk measure. So that is the first main problem. And a second result connected in the case of coherent risk measures, one has this inequality and the d d diversification effect, and one can consider the worst case diversification. Worst case diversification and Mark and co-authors have introduced this notion of strong coherence and there is some economic motivation also for that. And it is interesting to determine which risk measures are have this property. And there are classical results by Kozovoka in the case D equal one. In the case d equal one for coherent risk measures, saying at first they give a representation of a law-invariant coherent risk measure, namely at the supremum over so-called spectral risk measures. Spectral risk measures are mixtures over the tail value errors. That is the first result. The second, strong coherence of rho is equivalent with rho being just a spectral measure. And now to extend this to the higher dimensional case, at first based on Moreau or convex duality theorem, one can get this kind of representation for convex lower than continuous risk measures, like here, where where where the in the dual uh space is uh the space of uh density uh of measures with with densities on LQ in the dual index and it is a bounded additive function as in the case infinity. And one has the result for a finite complex matter. This is this supremum is attained. Supremum is attained by a representation set which is little close in LQ. And there is also a version of the Namioka theorem which gives as a result continuity. And now we are interested in law invariant to convex risk measures. So indeed So in d equal one, this result of Kuzuka has been extended, and there one gets an additional penalization term, but otherwise the same. And then the question is, can we get this? What is the analog for portfolio risk measures? So for risk measures with vector conformance. And the idea for the solution comes from this little proposition. Little proposition. If one has a convex risk measure and one considers the worst case risk over all equivalent random vectors, then this is a convex law invariant risk measure and in fact it's some kind of parameterization of all of them. And this led to the introduction of maximal correlation risk measure. We take the set of all scenario densities where the components are in LQ. Consider the correlation coefficient of x with some direction or some direction y up to normalization. And then this worst case construction. And this only depends on the distribution of the direction. So it's a measure depending only on the distribution mu. And this is the law invariant convex measure. And And uh for D equal one, this uh uh max correlation risk measure is just a weighted average value at risk. For D greater than one or in general, this max correlation this max correlation risk measure is given by the supremum of the integral xy over all joints. XY over all joints, so it is just identical to the optimal mass transportation problem. And so this shows a very close connection to mass transportation. Oh yeah, no, the result tells us that the result then is if one has a convex wrist measure, then it's law invariant if and only if Law invariant if and only if it has a representation as a supremum over some scenario, a set of scenario measures of these max correlation risk measures. And that means the maximal correlation risk measures are the building blocks of all law invariant risk measures and there is also a Welsh and without standardization term for the coherent case. And now there's a There is a strong connection with the L2 mass transportation problem. And the main result here is the existence result. The main point is B. The characterization appear X is optimal, L2 coupling if and only if X2 is in the subgradient of some convex lower ZB continuous function of X1 almost sure. And this, since the convex. And this, since convex functions are almost really Lebesgue are almost really differentiable, if one measures Lebesgue continuous, then the subgradient is just the gradient, and then therefore one has a solution in functional form of this problem, and then, of course, also of the Manch problem. And there is a uniqueness result of creating And this result was first stated in a paper with Radchev in 1990. Renier had the result in 1991 in the case of Lebesgue continuous case with bounded support. And the sufficiency part was already earlier, like not in Smith, in fact already in the paper 84. And in the literature. And in the literature this result is often or usually called Praignier's theorian, but I think by this history it is clear that it is not Braignier's theor but and so just wanted to make this point since I guess also yeah and an extension to An extension to general cost function was given 91 based on known convex optimization theory and there the corresponding characterization is that x2 is in the C subgradient of C convex function of X1 almost surely. And this result from 91 was reformulated by Smith in terms of Cyclivi monotone. monotone support. And then later on there was a lot of uh uh work on on this uh result extension to manifolds and so on. Yeah. And these C-convex functions one can use in class of examples directly to solve these kinds of problems. So for example, a uniform distribution on square with discrete distributions, then one Then one gets the points x where xj is the maximum on the sides, and that are just the points where xj lies in this c subgradient of this function. These are Voronoi cells, and so one just has to find aj so that the probabilities are the correct ones. And in this way, one can solve smaller discrete problems. And here in this example, And uh here in this example it looks like this here the Boronoid cells which are mapped to the corresponding points. Yeah and now the question of worst case portfolio one can also formulate for convex risk measures in the average sense. And if one looks for the max correlation risk measure For the max correlation risk measure, then one can determine quite easily the worst case side. Namely, it looks like the following. A vector x with components xi is the worst case dependent structure if and only if there is some y with a distribution mu such that all the xi are optimally coupled to the same. Are optimally coupled to the same bike. And this was I was Mark and co-authors. And there was a first version, then here a second version, and a third version, or something of this kind. And how does it look in the case of general Lorentz baryon convex risk measures? So I take this representation from previously established. The established representation as a maximum of max correlation risk measures. And then consider the average risk functional, which is the average risk with this penalization term. And this depends only on the marginals. So it is just a function of the marginals. And μ0 is the worst case scenario. If it maximizes this average risk function, so in order to determine it's a variational problem. And then the characterization is the following. If psi is a finite convex law invariant risk measures, then the worst case risk is the same as the supremum of the average risk functional. And if u0 is the worst case scenario, and xi star and Scenario and XI star a mu node common aton, then this is the worst case joint portfolio, and up to some existence, this is also a necessary condition. But I have only formulated this version. And so this determines the worst case portfolio in the general case. And there's also a version for the total risk. Version for the total risk with the total risk function. And concerning the question with the diversification effect, so we have the worst case diversification effect in the convex case. And there the second Couzo cartilagin is this one. Psi continuous convex this has no worst case diversification effect strongly. Strongly coherent if and only if it is a translated Max correlation of this form. And this was by Mark and so on, I think in the coherent case and I gave it for the convex case also some kind of different proof also. So in order to determine worst case dependence structures, so one has to determine worst case scenario measure, variational problem, and solve n optimal coupling problems. And there were a sequence of algorithms allowing to approximate solutions like radiant descents or Like radiant descents or combinatorial boronoid type partitioning or also linear programming approaches. And yeah, in location scale families where the scales are non-negative and the basic measure is just the measure of x distribution of x. Distribution of X, there one can have easily the result that these things are new commonoton and therefore are worst case distributions. If one has an orthogonal invariant basic measure, then one can extend it to affine affine transformations just by using polar factorization, and then one gets result of this form. Result of this one. There are also other classes like Achimedian copulas or spherical equivalent and so on. But an interesting case is coupling to the sum. If you take the L2 risk measure and burst determine the worst case, joint portfolio, then one can see that this problem is equivalent with this problem sum x. is this problem sum expectation norm x i minus s n square is the minimum. And equivalently also one can see that the solution of this worst case thing that the law of the solution of the average solution is the Barycenter. And this uh in a paper with uh Stephen and Giovanni Stephen and Giovanni Puccietti published in 21 or something how to use this in order to determine and developed an algorithm for how to approximate the solution of Barycendal problem. Knott and Smith, they used this as formulated the optimal coupling to the sum principle and they considered And they considered it in the case of the normal case, assuming that the sum is also normal, this covariance 10. Then here we have the optimal couplings of the sum to the components. And one needs this condition, the sum identity of the TIC identity, and this needs non-linear matrix equation. And if this has a solution, then they showed by Young inequality. Young inequality that this is the worst case quadfully. But the question remained quite some time open whether a solution of this exists. And in a paper with Uckelmann, by means of this duality transportation result, we got there is a solution of this equation and the optimal coupling principle is working. And in general, there exists a reverse. There exists a worst case portfolio and optimal coupling is necessary, but in general not sufficient. But under the condition on the support of the solution, then it is also sufficient. And okay. And now that was the part with the concern. That was uh the part with the connection with optimal trans direct transportation problem. And now come to the question how to reduce the risk bounds. And so I want to go and the intuition for there to show a little bit go through some of the examples, classes of examples. And the intuition is positive dependence. Positive dependence information allows to increase the lower risk bounds. The finance negative dependence information allows to decrease upper risk bounds, but not the lower risk bounds. And this is the topic of a book which recently came out with Caroll and Stephen after a long, long time on working on it. On it, really, and what I now is some of the chapters, examples dealt with in this book. And the first more classical case is one has more information by one knows higher dimensional marginals, not only one dimensional marginals. And one considers CRZ tail risk problem more of a youth risk problem. Value address problem. And there's also a duality result for this problem looking similar, but now the dual is more complicated. And this dual result leads to various improvements, for example, of these Frichet bounds, improvement upper bound if one most general. Bound, if one was general, or if one has a two-dimensional margin, here is a very nice improvement of the lower bound in terms of the two-dimensional margins which are given. And uh in some cases, in some decomposable cases, one can get exact uh bounds and it was mentioned already in the talk yesterday also. Already in the talk yesterday also uh this in some uh some application. And if one knows the higher dimensional margins with these sets ji, then one knows also the distribution of the sum in these subsets, say hr, the distribution function. And then since the sum of the y r are the same as the sum of the xi, one gets then direct. Xi, one gets then directly upper and lower bounds for these improved shape bounds. And one can take more general weighting schemes. But the important point is that here the upper bounds and the lower bounds are now one-dimension margins and they can be solved by the real. And they can be solved by the rearrangement algorithm. But the degree of improvement depends a lot on the information one gets. Here's an example, the 600 dimensional parent two. And on the left-hand side, and given are the pairwise marginals. And yeah. And here, the pairwise marginals are just co-monatonic, and one sees. Monatonic, and one sees the improvement over the unconditional bound, unconstrained bound, is very small. On the right-hand side, the pairwise distributions are independent, and one sees instead of say 20,000, one gets something like 14,000. So, a considerable improvement of the risk balance by this information. A very nice class of examples comes if one has the additional information about on the variance of the sum. And this information is often available and then one can determine Cantelli bonds, simple Cantelli bonds depending on this parameter. Depending on this parameter. And the bar bonds in the upper have the property that they have a relation to convex order minima in the upper and the lower part of the distribution. And that is the content of this proposition. I agree. It is just if one has in the upper part convex smaller things, and in the lower, like here, the mixing. Or like here the mixing, then one can concentrate to the smallest and convex order. And in this paper, we also extended the rearrangement algorithm to solve this kind of problem with variance constraints. And this is the step one chooses at first at the two steps, the domain starting from largest, then does a rearrangement in the upper part. Does a rearrangement in the upper part and in the lower part, then checks the variance constraint fulfilled. If not, then shift the domain and iterate and so on. And here's an example with a rate of three distributions. And here one sees the correlations there. Correlations there determine the variance bound. And if the correlation is smallest, then one gets the best bounds, large is the worst bound. And here is the result of the ERA algorithm. And here are the results for the upper and lower bounds. And if one compares, the values are very close to each other, which shows that the bounds are good and also the hydrogen works. Also, the ideals in Boxby. Application to a risk portfolio with 10,000 loans. One has knowledge of the default probability and the default correlation. This leads to the formula for this variance, which we take as a variance bound. And then we get as a result, for example, for 90% the For 90%, the tail risk lies between 4% and 13%. And here are three results which come from standard methods used in industry. And they are all relatively close to each other. They fit well in this interval. But all one can say if one has. All one can say if one has only this information is that these bounds are true. And these bounds are the reason for similarity is that they use essentially the same factor model for this. So yeah. And there's also an extension in this paper. In this paper, with Carroll, Steven, and one has knowledge of the first K moments, and one sees here, for example, in this example, that the unconstrained bounds are 6970, and it goes down from 900, and it can be reduced further by now in the next And the main point here to notice is that the variance restriction is a global negative dependence assumption and it implies therefore reduction of the upper bounds, but not so of the lower bound. And the second type of examples comes from positive-negative dependence information. so for example in the weakest form upper and lower order and dependence. Upper and lower order dependent assumption. And for one-sided dependence information, one assumes, for example, that G, one has a G which is smaller than lower order ordinance, F would be in the upper tail. And if you take, and for example, G could be the product distribution function, that would then mean x is. functions that would then mean x is positive or independent or something. And with this information, one can get improvements of the standard bounds. The standard bounds for the distribution function of the sum is given by this upper and lower term. I think here this is Freeman missing in terms of the upper and lower Freichel bounds. But if one knows, for example, that the tail distribution function is bounded above by h, then one gets improved upper bound. And in these bounds this improvement can be quite considerable. The information can come from different sources From different sources, for example, higher dimensional margins, parameter uncertainty, or one has known domains where one knows the distribution function, or one has inequalities on these domains. And there, such inequalities were developed for deeply two already long time and for greater than two more recently. And for example, one has this situation in financial context where one has access to the digital options. Access to the digital options, I mean default times for ones or something, and as a result gets improved light rounds for options. Yeah, my time is climbing a little bit. There are modifications of this dependence assumption. For example, one For example, one important class is the one that is split into subgroups and one has a comparison vector which has independent components and within the subgroups it is co-monotonic. And one assumes that one has a comparison vector which is smaller than in some of these orderings. And if k is equal to d, then this is just a very weak ordering. If k is Coordinate if k is equal to one, then it's very strong homogenicity. And what one gets here in an exit, so if we assume that y is smaller than upper ordering, then one gets improvements of the lower bounds. So instead of nine unconstrained, one gets seventy two. And when that's 72, that in the common atomic case it goes down. And if k is equal to 8, which means only positive alternate, then nothing remains. And one can use a stronger form of the positive dependence, like cumulative dependence, and there one has a conclusion that the sum is greater than the independent sum, and one. Some and one can avoid in this way this effect and gets improvements up to the end with K. And a nice second example is the following one again with independent subgroups and we allow any dependence within the subgroups and then one has natural way this this This sum, SCK, the sum of independent components, and here is just the common atomic sum inside the subgroups. And then one gets these bounds which are induced by independence information. And here one sees in a Pareto example, 50-dimensional. The more independence information one has, the better are getting the bounds. And this one can extend to partial independent substructures where only the black dotted things are independent, the others are not. And then one has two terms in the upper bound, one coming from the independent stuff and one of the other. Both are easy to calculate. And this extends further to independent graphs. Further to independent graph structures. So, if one has graph structural models, one can do the same and combine it with variance bounds. And here, for example, the situation then is typically like this. You have the variance bounds. If the variance bound is small enough, then this is the best one. But if it's not small enough, then the independence bar is dominating for the reduction. Here is a very nice example from one of a big European insurance company, and by information of the company, it has a portfolio, 11 dimensional and car and four independent subgroups. And here for this example, one gets a reduction of the upper bound 209 to about 150. And this is given by the information of the This is given by the information of the company. And this was interesting enough for the company, and they added this as an additional tool for their back testing methodology. And one can also do a two-sided balance. Just briefly, another very interesting model is one has a factor model, but with One has a factor model, but which is not completely specified. One assumes only the components of the individual risk factors with the systemic or systematic risk factor are known in the distribution, but not the joint distribution. And then one can give improved bounds over the marginal models, but these are not easy to calculate. And by a mixture representation, one can get a very easy. One can get a very easy formula for the value at risk in these mixture models. And this then leads to worst case upper bounds, which are simplified by the conditional tail value at risk vector. And just to give an example, here we have a factor model in one group. In one group, the main factor is Pareto 3 distributed. The individual risk factors are Pareto 4. And in the second group, with probability P, the first, the risk factor is comonotronic, with 1 minus P it's anti-monotonic. So with 1 minus P, it generates negative dependence of this P positive. And so we expect if And so we expect if P is small, then we have negative, lot of negative dependence, and so we expect there will be an improvement of the upper bounds and converse for P big. And this is exactly what one observes. Here is the upper bound dependence on the P, and for P small, we have a strong improvement of the upper bound, for P big of the lower bound. One can also do this for more general mixture models and for example then in this case and the new mixture models in insurance or in finance valence mixture models like hyperbolic distributions or something like that. Yeah. I'm trying to see how to continue. continue we have uh now in in more uh we have uh I consider some ordering results and risk models which are determined by these assumptions on the structure and dependence and and for example we have seen already For example, we have seen already if we have a subgroup model, and then in a paper with Julian Bitting, we have considered in a very systematic way all possible combinations. So one has subgroups, one has the condition ordering in the subgroups, plus an ordering of the group loss between the subgroups. And so this allows also much generalization of the Much generalization of the first two examples, so one does not need any independence or something like that. And this leads to a wide range of ordering results. One can combine, for example, in the subgroups with partially specified factor model assumptions, and so on. And in order that the comparison works, one needs positive dependence notion like conditional increasing in sequence. And then one gets very strong ordering of results. Very strong ordering results like directionally convex ordering, if one has this positive dependence on components. And so, for example, there are in elliptical models, which is an interesting class, there are classical results concerning the normal case, there is a conditional increases equivalent with the inverse covalence with an M matrix, and we have seen. And we have seen that this extends in one direction also to the elliptical models. And also, here is an ordering with the components of the correlation matrix. This was also known long time for normal case and was independent in these papers in the general case. And then And then one can use this example at first if both are the same, just the poor case. And then, for example, one two extreme cases, this unknown dependence in the subgroups, and then the other one is this ordering, or partially specified factor models, the other one has the conditional co-monatonic vector. And then one sees that. sees that how well it works. The bigger the subgroups are, then one gets more reduction. And also between the subgroups. And for example here we assume one copula between the subgroups is in this positive dependence ordering smaller than the independent copula, so we have a negative dependence. We have a negative dependence assumption between the subgroups, and this negative dependence assumption allows improvements of the upper bound. And so here, for example, and we checked it through in many examples, also extreme value distributions and now I wanted to do the more I wanted to do the more recent ordering, as I have many of them, or I think nearly all of them, with Jonathan, who is also here. And what we do in this part is we have for various models we determine worst-case situation by ordering results and And so, for example, here one has given bounds for the partial correlations corresponding to a C-wine structure. And then one can determine recursion by a recursion certain matrix, and one gets that the elliptical distribution with this covalence is the worst case. And there are a lot of examples of similar kind here for this partially specified. These partially specified factor models, which I have discussed already a little bit there, one gets a very clean ordering result and also ordering of the worst case situation, or one has lower bounds for the correlations on some part and upper bounds on another part, and allows also for variable distribution functions and And so is able to determine here the worst and best case. And very interesting, and this is not my final recent result is for general factor models. One we introduce the product of copulas. So these are something like the specifications, the two-dimensional specifications. Dimensional specifications, and these are conditional copula, and then the star product. This, then there is a scar theorem. This gives the copula of the general factor model. And now we can determine various, give various ordering nozzles based on this star product. So, this is a copola, bordering with respect to conditional copulas. Or here is a very nice. Or here is a very nice one. One has the conditional copula is component-wise convex, and one has very weak ordering of the specifications. So the di are in lower ordering smaller than the EI. And then one gets that the star product of the di is smaller than the star product of the EI. And this applies then directly by applying transformations to other marginals ordering. And marginaliz ordering results in use of factor models. And here there are many variations of this with different kinds of ordering. And the method of proof is based typically on some classical rearrangement theorem by Kiefan and Lorentz theory and detailed construction of mass transfer, which is a combinatorial task, which is not always easy. Yeah. Okay, so sorry for extending the time. Thank you very much for thank you, Luke. Are there any questions? That was only during the talk because I noticed yesterday or before that often the speakers only came to half of their transparencies. But now even without questions. With all questions. I have a question if no one else does. I wanted, so you talked about the optimal coupling to the sum. Right. And you had a result somewhere where it said, okay, the optimal coupling to the sum is necessary but not sufficient in general. And then you had some conditions that made it sufficient. But without these conditions, is there a simple counter example where you can see that? Yeah, there was a similar but I could not. There was a sign, but I could not. Okay, maybe we can. Awesome, awesome. Are there other questions? Got we're just about at coffee break time, so let's thank Lutheran meeting at 10:30. That's a pretty nice talk. 