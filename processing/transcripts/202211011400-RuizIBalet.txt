Thank you for the introduction. And yeah, thank you to the organizers for putting this together. It's a lovely place. And I guess I apologize to Bertrand and Jose, who have seen the talk before. But yeah, you know, you always learn something new, I guess. Hopefully. But anyway, so the problem. So, the problem I will be looking at, or more around the system, is that we have interacting SDs. So, xi dot is going to be one divided by 2n of the sum from i equals to 1 up to n of the gradients of some w of xi xj plus some inverse temperature. That's Brownian motion. And we are going to think that these particles are all sampled at initial time from some distribution row initial. Okay, so okay, so the idea is that here we have n particles which are Which are indistinguishable? Okay, so yeah, so I'm here when I'm talking about Brownian motion. This is a family of Brownian motions that for each I they're independent. So, I mean, that's more or less the idea. So, the idea of that m particles are indistinguishable is if I change the labels of one particle one. Of one particle one with particle three, or the other way around, it doesn't actually affect what the distribution of this is going to be. Okay, and so that is what is going to like actually come up to give us a mean field limit. So, in general, I will look at this from the PDE perspective, and this is probably the last time I write the SDE. And the problem I'm interested in is when we take this particle system, when we take n to infinity. When we take n to infinity and t to infinity? And can we actually change the limits? And can we actually get the same thing from one or the other? Of course, you can put some extra potentials. Here, I will mostly work in like periodic boundary conditions, but you can work in whole space. You can work in like slightly more general things. Okay, so this indistinguishability, what I'm going to be looking at is at raw n, which is and which is let's call it of t which is going to be the law of the full particles x1 of t all the way up to xn of t okay and kind of what i'm saying is if i have xi here and xj if i change the labels of i and j i still get the same log okay and this is what i'm going to be denoting as the symmetric probability measure Symmetric probability measures on whatever my state space is here. Let's just for simplicity say t to the n. And the point is that this state space is becoming larger and larger as n goes to infinity. Okay, but I kind of try to want to find something that makes sense even as n goes to infinity. Okay, so the classical result is by Schnitzman. Actually, yeah, maybe before I go to the classical result of Snitzman, the idea is that I'm going to be using PDEs in this talk. So the point is like, what is the PDE associated with rho n? And the idea is that because I'm looking at a really high-dimensional state space, then I actually have linearity for rho n. And we actually get that the equation that rho n satisfies is that That rho n satisfies is that dt of rho n minus the divergence of the gradient of some potential h n times rho n is equal to this inverse temperature parameter Laplacian rho n and the initial condition is just initial condition tensorized n types okay and this is usually called the forward common or And this is usually called the forward-Komogorov equation or the Will's equation, depending on which country you're coming from. Okay, so what is the result is I want to take n to infinity, and I still need to get something that makes sense in the limit. So restating the theorem of Snixman, maybe operate it here. And okay, I guess I haven't put the main assumption. So, the only assumption that I'm going to be taking is that the Hessian of W is bigger or equal some kappa w times the identity, where kw is any number in R. Okay, so I only have an inequality for the. I only have an inequality for this, and this is usually what's referred to as lambda convexity, but I'll use lambda for the log-So-Lip inequality. Okay, and this implies that W doesn't actually need to be convex, it actually can be strictly concave, but at least you have a bound from below for the Hessian. Okay, so under that hypothesis, this is the whole space for the Hessian. Yes, this is for the whole space, yeah. Okay, so um. So, I mean, I'm sure you can reduce it if you have some confinement or something, but usually you can trade some convexity. I'm just wondering, it's not just a condition of the minimizer of that or you're supposed to like that. No, no, no, no. This is everywhere. Yeah, this is everywhere. So the theorem of Snitzmann's basically says that the Wasserstein two distance between rho and Rho n of T and the mean field tensorized n, but t okay is less or equal than some constant divided by square root of n. And I guess before I keep going, so here I will always use some, let me like kind of what it is that. I kind of put it is I will always, oh, that's a terrible color. I will always scale the Basserstein distance to take into consideration that I'm in a higher, like bigger and bigger state space. So this Basserstein 2 distance is 1 over square root of n, the standard Basserstein distance. Okay, so usually when I put something on top of the two or the next quantities is because I want to be able to take the limit when n goes to infinity and obtain something. When n goes to infinity, and obtain something that is finite. Okay, so that's more or less the idea of this. And this is the right scaling, so I get something meaningful in the limit. Okay, so if I don't scale, of course, this would blow up as n goes to infinity. So this is the theorem of Snitzmann, but of course, I haven't told you what the mean field limit is. And now the mean field limit is just so where raw mf satisfies. Satisfies the equation dt rho mf plus the divergence of something like a convolution rho equals to beta inverse Laplacian rho mf and I'm missing a rho mf here okay and the initial Okay, and the initial condition is rho mf at zero is just rho initial. Okay. So is there any questions? Perhaps notation or somehow. So this is kind of the setup. And here I put C over square root of n, but what I'm meaning to say is actually e to some column. E to some constant times t minus one. Okay, so as the time goes further and further, this theorem doesn't actually tell me I can approximate this well, and I'm kind of losing any sort of approximation. So the question that I'm hopefully going to address is when is this necessary? When is the exponential is this? Is this necessary? Okay, do I actually need to have that the distance between these two problems actually grows as t goes to infinity, or can I actually keep it bounded and finite? I mean, of order square root of n when I take t to infinity. Okay, and so that's kind of what I'm kind of asking the question. Can you flip the limit between t to infinity and n to infinity? Infinity. So the perspective that I'm going to be looking at is by, I mean, I already used the buses and distance, so what we're going to be looking at is gradient flows. Okay, and so the point is that both of my problems are actually gradient flows of the Baselstein two distance. So rho n is a gradient flow. Is a gradient flow in, let's put the scale d2 distance of the sum n free energy, which would be en is equal to the integral over h n d rho n of x plus the entropy rho n log rho n and let's put Rho n, and let's put the parameter beta here on the left. So rho n is the gradient flow of this equation, and then rho mf is the gradient flow in also these two, but now of the mean field energy, which is equal to a half of W convolution rho. convolution rho rho plus and then again i have a beta here plus rho log rho okay and so i have two gradient flow um i have two gradient flow representations so i guess the the natural question it was when i was working with jose a few years ago was are these two representations the limit of each other or not The limit of each other, or not. So the question is: can you actually take n to infinity and say that this gradient flow framework that I'm talking about actually converges to the gradient flow framework that it converges here? Okay, and this is actually true. And this is some work we did with Jose and Greg Pavliotis. And you can pass to a limit in this thing, okay? And the point is: you know, I'm And the point is, you know, I'm scaling the Wasserstein distance. And the idea is that I'm scaling in such a way that I actually have a limit, and the limit coincides in some sense with this T2 distance over just the densities. Okay, and in fact, this, I mean, not the convergence of gradient flows, but the convergence of the energies was known classically for statistical physics. And in fact, yeah, so the point is that you have gamma convergence of these energies. Of these energies. And this was a classical result by Messer and Sponge. Okay, and then more recently, and in fact, like this convergence of the energies of the distance was also done by Jor√© and Micheler. Okay, so this is the perspective that I will have in the background, and in a sense, Background. And in a sense, kind of what I want to try to understand is with this exponential factor has to do with the properties of this mean field energy. Okay, so if I can actually understand how does the mean field energy work, I can actually say if I can actually commute the limits or not. So just to give you an idea of the flavor of stuff that kind of we were doing, and I guess this is joint work with also with Greg Pogliotis, Risha Cablani, and Scott Smith. Kavlani and Scott Smith. So, if I want to prove this statement, so the classical Snitzman results using some coupling methods, but what we can do here is just take the derivative of the Basine distance. Okay, and this is something I haven't seen much done, but in fact, you can just take straight up the time derivative between of the fastest time. Of the Basestein 2 distance squared between rho n of T and the mean field of T tensorize them. Okay, and so in fact, you can see this as, I mean, if you do the interpretation of the Barcelona metric as Riemannian manifold, you should see that this should be. You should see that this should be just like what is the inner product between the tangent vectors that are pushing me forward. And so you can actually, when you take the derivative of this, you can actually check that that's literally what's going on here. And you end up getting the associate tangent vectors to these PDEs, which are just the vector fields that are induced by the equations here. And so you get a term that is of the form gradient of the log. Of the log of rho n evaluated at x minus the gradient of the log of raw mean field tensorized and evaluated at y multiplied by x minus y against the optimal pairing between raw and Between raw n and raw min field tensorized term. Okay, all right, so this thing here is what comes out of the Laplacian part. And in fact, what this is, has to do with the part of the entropy. And the idea is that the entropy in the Basterstein distance is actually convex. And basically, in any mostly recent. Any mostly reasonable sense, the entropy is convex. So, what you actually obtain is that this term that looks really, really, really awful the first time you see it has a sign, and it's always negative. So, you actually have a contraction with respect to the heat equation, which is something that people know. And then you have to take care of the other terms. But the idea is that this is type of the this is kind of the idea of like, okay, if you actually understand this at the level of the bus strain distance, even though this term looks like this. Vaste and distance, even though this term looks awful, you always know that this is negative. So, I'm going to spare you the rest of the calculations, but I mean, morally, what you end up getting here is just some Gromwald type estimate in which you just discard this term, and then you get a Kw, the distance squared of the bus system distance between these two guys, plus the same thing with the square root of n. Okay, so if you follow this computation through, which is similar to what Snitzman did in a different flavor, is that you actually get a Gromwald type estimate with now this Kw, which was the convexity constant of the potential. And this will give you the result that we are looking there just by doing a Gromwald inequality. So the point or the question is. The point or the question is, I kind of have discarded the term that is coming from the heat equation. So, what is happening with this Laplacian here? Can it actually help me to actually get some contraction or not? And the point is that it should, and it should actually give me something that is slightly better than the estimate that I'm getting here. Okay, and so the question is, can we leverage the convexity of the entropy to get something slightly better in the estimate that I'm getting? And what is the Estimate that I'm getting, and what is the natural limit? Of course, now, just from looking at this inequality, if Kw has the right sign, you expect this to be contractive. If Kw has the wrong sign, then you expect this to become expansive from this estimate. Okay, so yeah, so I guess I would say what is this necessary if you look at the work of Malria, if you actually have convexity for the potential W, then you actually don't need. W, then you actually don't need the exponential factor. Okay, so I would say, Mauria, if Kw yeah, and I guess here I'm missing a minus, if Kw is positive, we don't need the exponential growth. Okay, but now, I mean, that's quite an easy answer because at the end of the day, you're using like the same method for something that is completely concave. But the question is, can we use this to actually improve this a little bit? So, from the point of view of Baselstein, I still haven't seen any way of doing this. Any way of doing this. But the point is, I don't know where I'll write this. I guess I'll write it here. The point is that there is some new coupling methods, and this is a theorem by This is a theorem by I'm going to forget somebody probably Dermos Adarle. And Simmer, in which they actually show that if the interaction beta is small enough. So basically, if I have enough noise, so if I have enough of this term, then what they actually can show is that something like the one Baserton distance between rho and Between rho n t and rho mf tensorized n t is less or equal than now the other way around e to e to some e to the minus lambda t divided by square root of n. Okay, or basically we can just forget about this. And this is true for every t positive. Okay, so my Positive. Okay, so morally, if the interaction is small enough or the noise is strong enough, then I actually convexify what's going on. And I actually can have that, in a sense, I can exchange the limit between n infinity and t2 infinity. Okay, they're actually the same thing. So, okay, what we were kind of focusing on is: can we quantify in some sense where does it break down? Okay, and so what is the natural Okay, and so what is the natural obstruction to be able to satisfy something like this? And so the natural obstruction is the mean field equation having more than a singular than a single steady state. Okay, so the point is that, okay, and I've erased it, the equation for rho n of t is linear, and the equation for rho tensorized n mean field is non-linear. Field is non-linear. Okay, so all right. So the point is that when n goes to infinity, when t goes to infinity, rho n of t always converges to a single state, which is the Gibbs measure, which is mn, which can just be given at e to the minus beta. To the minus beta times hn divided by a normalizing content, which is Cn. Okay, and this happens always for rho n of t, but now rho mf is now a non-linear equation. So rho, this equation can actually take more than a single steady state. Okay, so let's let me try to perhaps give you an example of this. Actually, something concrete and really easy to see is that let's say that we are taking omega, our state space, to be tau, and w of xy to just be the cosine of x minus y. So, this is our super smooth interactions. Everything is nice. There is no singularities, there is nothing like that, but I still can show that the limit between n and t to infinity can. Between n and t to infinity cannot actually commute. So, okay, so if you look at the equation associated to this, just to write something down, this is dt of rho plus the derivative in x of the integral of the sine of x minus y d row y times rho x, I guess, is equal to It's equal to the Laplacian or dxx of rho, and here in front of this, I have a beta. Okay, so this is my drift term, which is just non-local, but it's as smooth as you can actually think that it would be. Okay, so if you notice, something is that if you take rho star, which is equal to Which is equal to just the Lebesgue measure d of x. This drift here is equal to zero, and the Laplacian here is equal to zero. And so this guy is always, the x is always a steady state. Okay, but what I'm going to be able to show is that this guy is not always the minimizer. Not always the minimizer, so it's not actually always stable between the dynamics. And in fact, what you can do is look back at the mean field energy that I was writing before and remember that this rho mf has to be a gradient flow in this energy. So if I can actually show that this is not a minimum, then I'm actually showing that this is unstable. Okay, so the way that you can look at this is if you take a sort of linearization around rho star and you take Around raw star, and you take the mean field energy of this raw star plus some perturbation. Then you take the limits when epsilon goes to zero. You properly scale this by epsilon squared. What you end up getting, okay, is this beta divided by two plus one by hat of one squared. Of one squared plus the other modes of phi i at squared. Okay, so this is just the Fourier representation. So depending on what is the size of beta, this term can be positive or negative. Okay, so morally, if beta is equal to minus two, I have lost the convexity of this problem. Okay, so I've taken the limit of the perturbations, and now this guy, if And now, this guy, if beta, I can check that beta exactly equals to minus two, I have a degeneracy that's going on. Okay, so if I need to draw this, what's going on is that here at beta equals to minus two, I have a phase transition. Okay, and the phase transitions are the limit of these uniforming time theories. Okay, so maybe try to draw like So, okay, so let's kind of if beta is bigger than minus two, the mean field energy is nice and convex and looks something like this. If beta is exactly equal Is exactly equal to minus two. What I actually obtain is that the mean field energy projected onto the right mode for phi will be looking like something that is really, really, really flat. Okay, so basically, what I'm saying here is this f second of zero is equal to zero. Okay, and Okay, and this is where the phase transition happens because then if I take beta, which is smaller than minus two, what I end up getting is that this energy is no longer convex. And what we obtain is that the energy looks like this. Okay, and this is the picture of the standard kind of bifurcation phase transition, in a sense, in which I have. Transition, in a sense, in which I have a unique steady state, but then when I cross this threshold, I don't actually have uniqueness of the steady states anymore, and the minimizer is not even unique anymore. So, if you draw this, like this is what the usual like pitch fork bifurcation would be, but now you're in infinite dimensions, so this is actually infinite dimension. Okay, and so and the point is that naturally the limit of these type of things is exactly here. It's exactly here. Okay, so let me see how much time I have to actually keep saying. How much time does it? 10 minutes. Okay, I don't really know how much time. But that's okay. So that's kind of what I wanted to at least kind of show you where is the limit of this. Well, the limit is when the mean field energy loses this type of structure, and I actually get more critical points than what I had at the beginning. And okay, so the point. And okay, so the point is that if I start my n-particle system, no matter where I start my n-particle system, I will always be converging to the same measure mn. So I do not have a commutative diagram. So if I start my initial condition, my row initial to be dx, what I have is that rho n of t converges when t goes to infinity, always to the same mn. Always to the same mn. But when I take n to infinity first, here I'm assuming that the initial condition is just raw star tensorized. Sorry, this is if I take n to infinity first, this just converges to raw start, which is independent of time. And so when I take time and so when i take n to infinity for mn is not the same it's not the same limit as when i take t to infinity here okay so these guys do not commute okay and so the only way that you can actually have a uniform in time propagation of chaos result is in this diagram i can actually Diagram, I can actually commute, okay. And that's kind of what we were trying to do with the paper: is that if you can commute this diagram, then you should be actually able to show this. So, we didn't exactly do this, but something fairly close, which is, okay, so the only way that you should be able to commute this diagram is if I have a uniform speed of convergence of my n-particle system to my limiting Gibbs measure. Okay, and this is the Log's Hovelab inequality in a sense. The logs overlap inequality, in a sense. Okay, and yeah, so let's try to perhaps go to Logs Overlaps inequality. So, and yeah, so the point is that at least in my head, I only see this. And depending on which kind of where are you fitting on your threshold, then you should be having one result or the other. That we can prove it is a different story. So, okay. So, how do we actually look at the uniform in time convergence between rho n and mn? This energy that I wrote here, in fact, is just equal to the relative entropy, the relative entropy between rho n of t and the limiting Gibbs measure. And then, And then, if I take the time derivative of this, this is exactly equal to minus the Fisher information between rho n of t and m n. Okay, so what is this guy? This is the relative entropy. This is the log of rho n divided by mn integrated against rho n. And this thing here. And this thing here is the gradient of the logarithm of rho n squared integrated against rho. Okay, so if I want exponential speed of convergence with respect to relative entropy, the inequality that I want is that this is less or equal than minus lambda. And like now I'm going to call it. And like now I'm going to call it n lsi times the relative entropy between rho n of t and m n. Okay, so if I have this inequality, I have uniform in time convergence, like I have exponential convergence of this lambda n L Si and exponential with this lambda n L Si. And okay, so how do I define this guy? This guy Find this guy. This guy is just the infimum over rho n of the relative Fisher information with respect to Mn divided by the relative entropy. Okay, so in general, it's at least in sort of easy assumptions. This being positive is not too hard to prove. Being positive is not too hard to prove, but the thing that is hard to actually understand is what is the limit when n goes to infinity of this guy, or at least more complicated. And so, can I actually have a uniform bound for this? So, yeah, just jumping into the results that we are talking is if we know that beta is small enough, then we can actually show that this does not degenerate when n goes to infinity. Okay, so I mean, this inequality at the first time you see it looks like really. It looks like really actually complicated, but it's actually not that terrible if you start thinking of this in 1D pictures. So, if you look at it in 1D pictures, the inequality, that inequality there is just telling me that I can relate the derivative of the function with the function itself. Okay, so this guy here is equivalent to saying F To saying f prime of x squared divided by f is bigger or equal than lambda. Okay, and this is just a 1D function. So let's say that we have f that is going from r to r, this is the only thing I'm asking for. It's like the derivative of at x. derivative of at x squared divided by f of x is bigger than lambda. This is the same as that if you interpret this in terms of the Basest time distance. And the point is that, I mean, okay, now I could grab a really motivated undergrad and tell you, okay, if I have this inequality, why can I prove with this? Okay, so one of the things that you can prove is exponential convergence of the gradient flows. And the other thing that you can prove is that f of x. you can prove is that f of x has to be bigger or equal than okay so let's say that f of zero is equal to zero is lambda x squared lambda over two x squared okay and if i translate this in optimal transport this is the tallagrand inequality so what this is telling me is that the relative entropy between rho n of t uh rho n Rho n and mn actually is controlling the Basserstein two distance squared between rho n and mn times lambda divided by two. Okay, so this is the same inequality. And in fact, in the paper, we gave a new proof of the Talagrand inequality coming from the Log Sovolep. But the idea is that once you interpret it like this, it's actually not. Interpreted like this, it's actually not too hard to realize that this is what's going on. Okay. So originally there was a proof by Vilani and Otto, and I think there was another proof earlier, but their proof is a bit more complicated. They say this in their paper, but they actually do something slightly more complicated when they are trying to actually show it rigorously. And we are using the gradient flow structure, so it's a bit easier to do. And I don't think I have much time left. A couple of minutes. So, okay, just to. Minutes. So, okay, just to close the ideas is okay. So, what happens? The idea is that now if this quantity does not degenerate, then I can show everything that I want. And if that quantity degenerates, then I don't know. So, maybe I'll erase. Okay, so the theorem is if the limit of lambda and LSI is strictly positive, then the mean field has a unique. A unique steady state. And what we actually can show is that the distance between mn and this row infinity tensorized n is less or equal than some constant divided by square root of n. Fun and the next thing that we can say is that if we have this non-degeneracy of the log-sovelib inequality, then we have uniform in time propagation of chaos. Okay, basically, we don't need the exponential factor, and then the last thing that we can say is that fluctuations at equilibrium. Okay, so this a priority seems completely disconnected with everything else, but the nice thing about this kind of trying to understand the fluctuations at equilibrium is that the fluctuations at equilibrium should be giving you the idea if you're about to understand. The idea if you're about to undergo a phase transition or not. So, morally, I guess when we're talking about like global warming or climate change, we are saying, well, yeah, the indicator is that we have crazier and crazier weather. The idea is that the fluctuations are topping to become Gaussian and they become unbounded. And that is actually been seen by Dawson, particularly, is that when you have an energy that is having this transformation, the fluctuations will be. Transformation, the fluctuations will become unbounded and will no longer be Gaussian. And so you actually expect fluctuation to explode. And so hopefully we will not be seeing that in our climate. But that's kind of one of the explanations why people usually when something starts fluctuating, you should be careful because you're about to change the system completely. Okay, that's kind of the idea. All right. And then I'm going to finish this talk by saying, okay, well, this is an assumption, of course, but now. Assumption, of course, but now, of course, we can actually show that if beta is small enough, then we have this assumption. Okay, so if the noise is really strong, then I actually have that the lambda and LSI does not degenerate. And then the other thing that we've been working on is to try to actually identify the limit of this quantity. And it seems that the limit of the lambda n LSI is the dissipation inequality for the mean field. So you can actually. For the mean field, so you can actually say that when n goes to infinity, okay, so this is a conjecture, we haven't actually shown it, not even in an easy case, because for some reason we haven't been able to do it, but that this lambda n LSI is actually converging to the infimum of the dissipation of the non-linear equation divided by the mean field energy. Okay, and this should be a lambda infinity. So we can show an inequality which is a trivial. Can show an inequality, which is the trivial inequality in the sense that the limit of this will always be less or equal than lambda infinity, but we cannot show the other way around. That's kind of what we've been working on. Yeah, sure. So if you put the equation in n in the in the equation, it's for similar dynamics. Can you come again? I didn't understand. I have n particles. Yeah. And I plugged this in the can I plug this in the mid-tel equation, like sensorizing it or something? So the problem is that when you start with like things that is caught. That when you start with like things that are chaotic, so they're like separated at the beginning, then you like they become instantly correlated. And so, in fact, what you are showing in a sense is that when n goes to infinity, the correlations disappear. But I mean, I don't know how to put like something some random rho n of t and evolve it through the through the mean field equation. I was trying to think, I mean, maybe it's very far, but Maybe it's better. Could you ask the thing on getting some Lyapunov exponent and you? Well, you start with some initial data and thanks to the. This would be kind of the Lyapunov exponent. I mean, I'm not sure if that's what you mean by the Lyapunov exponent. I mean, I don't see it exploded. Yeah, I was thinking like maybe this could be. So in a sense, it's again this inequality. So if you like these two guys are just, this is at the end particle level. Are just this is at the end particle level, and this is at the kind of mean field limit, but the inequality is the same. In fact, I think in machine learning now they're calling it, they're using it, I think classically it's called the PL inequality. So for not exact, this is, it's a bit more general. If you put a theta here, it's called the PL inequality, which if I try to write the names, I'll butcher them. But I think now in machine learning, they're trying to use this inequality to say that you have convergence in parameters and this kind of stuff. Convergence in parameters and this kind of stuff. So, yeah, this is the new kind of thing. But at the end of the day, it's just the Logs Hovel inequality for us if you understand them as like the bus and distance gradients. Yeah. So, what do you know about the fluctuations in terms of the uh can you characterize the equation for the fluctuations and do something for you gonna say that here's airline is and the blood with them? This would be the first order. They love large numbers, yeah. Yeah, I mean, I guess people would say that kind of the next order for expansion more likely is the not the not these fluctuations, but then the what is called the father version of this. So, but yeah, you can literally find. So, but yeah, you can literally find what it is, and it's the linearized equation. So, it's just, yeah, so you can actually characterize it, yeah. Yeah, so these are Gaussian, and you can characterize. We haven't done that, no. I don't know if you can do that. I wouldn't. So, in a sense, the result that we're looking at more that we are interested in. Looking at more that we are interested is saying, okay, if I am at the phase transition, fluctuations are not Gaussian. Yeah, no, I mean before, but for the phase transmission, it's like I think you probably have large deviations here. Large deviations, that's what I want to say. Yes. And then the large deviation estimates will be that this is the next then responsibility. Yes. Yes, I don't remember right now. So and this is before the would be like um get a another term expansion. Even after you mean the this fluctuation. Yeah, yeah, you can I mean the point is that you have something that is like one over square root of n times some Gaussian process. Yeah. Yeah, you can write it down. I just I we haven't seen like kind of the need of it and we were focusing more on what happens exactly there. Moral, what happens exactly there. I think we can touch up again. I'm going to