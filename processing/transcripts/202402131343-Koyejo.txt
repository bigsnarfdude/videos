I think this was gender and self like self-stated categories in this data set. But, anyways, they looked at some label for co-occurrence between sports and male-female. You could see, I think, a strong bias there. Maybe on the other way, if you look at the kitchen one, there's also a strong bias. I think part of our job as algorithm scientists has decided this is. Algorithm scientists decided this is what we want in our algorithmic systems in terms of the structure that they're capturing and what they might end up having when they end up. Like, if we build systems with this kind of data, we don't, and we're not aware or try to mitigate issues like this, what it might mean for downstream performance. There's also other work from, in this same paper, looking at visual analyses of, say, images that have flowers in them. Say images that have flowers in them, you see an over-representation of female presentations. If you look at flower and male, it's usually actually the difference here is the kinds of scenes where you'll see, for instance, flower. Sorry, that was the wrong button. There's the button. Nope, that's the wrong button. There we go. Found the right button. Okay, so moment of comedy, because my incompetence. Anyways, flower. Anyways, flower with female, again, I think self-identified in this case. And again, showing some of the stereotypes that occur in the world and what they might mean for the data that we're using to build many of the large-scale systems. Okay, so maybe a bit of discussion along these lines. So I think one part of the argument that hopefully That hopefully I've partially convinced you of is that there is most data that we use to build systems have explicit or sometimes implicit biases built into them. These biases will have some effect in the models that we build. And I'd argue that it's up to us to recognize when we can what these biases are and then when we can figure out what they might mean for downstream algorithmic implementation. For downstream algorithmic implementation and try to mitigate fastenes that we decide we don't want. And that's, I think, a meaningful decision that we might have to make. There's, I think, great work in trying to be much more explicit about data that's used to build machine learning and AI systems, things like data's useful data sets, share about documentation, and hopefully make it easier for people that sort of take data sets and build systems on top of them to have some better understanding of what might be built into. What might be built into the data sets that we're using for building access to this? I think, you know, maybe to, I guess, reduce the fatalistic discussion that we've had so far, I'd argue that there is a lot we can do and there are algorithmic tools and work that many of us have been doing to think about ways that we might be able to reduce bias in data sets. So, all the way from basic things like resentment. All the way from basic things like resampling of various kinds to methods that use generative models, types of some of what we heard in the morning, might be able to reduce some of the bias in data. Personally, I think the bias, we'll touch on this a little bit more in a few slides, but the bias maybe is what it is. I think often we should care more about the impact. And so I think this opens avenues for mitigation, but also in terms of thinking about. But also, in terms of thinking about issues that come up, it's often meaningful to think about potential harm that can come about from algorithmic systems and where it might come from and what we can do. And there's emerging interest from policy folks. And so more and more, I think, they're having an opinion, and perhaps this can be good in certain cases, on what is acceptable to include data sets and what effects that might have. Have. Okay, as mentioned, and maybe not surprising to everyone, all of large AI systems are internet data. As mentioned at the beginning, but just explicitly stated, the bias in the data that we can easily get access to, I think notably bias in search engines, ties to bias in, I noted here, images, the same exact thing is true of text and ChatGPT and sort of all the other kinds of models. Ease of So ease of data collection often is a big driver for a lot of technology well in AI. And this has an uh downstream effect on, again, the technology that we end up building on top of these things. Okay, so that's narrative one. I'll switch gears now to narrative two. So there's a lot of effort in the field thinking about maybe feature selection as a procedure for mitigating algorithmic fairness issues. And the argument Algorithmic fairness issues. And the argument comes from the observation that often we think some of the issues of fairness are tied to just specific features that might be sort of sufficient to explain what we might be seeing as a substantial bias issue. So the feature selection often lives in the algorithmic development part of the sort of deployments, algorithmic deployment lifecycle. Life cycle. So I think there was a well, there used to be an argument, and there still is an argument outside of technical circles, hopefully much more rare in technical circles, that you could tackle fairness questions by removing explicit information about demographics, socio-economic factors, and stuff like that. And the argument was: if we don't, for instance, be interested in fairnesses with respect to race, the argument is: well, if I simply remove any information about race in my data sets, perhaps this fixes my problem. In fact, the political, if you hear political arguments, often this case is made that simply removing demographic information should be enough to fix fairness issues. By now, there's been quite a bit of technical work. By now, there's been quite a bit of technical work suggesting, arguing, I think, quite strongly, both in theory and practice, that this doesn't work. And does anyone know why? Simply removing this information. Confounding that. Yes, so compounds everywhere. It's all variables that are correlated with everything else. And the effect is, thank you. And the effect is that simply removing what seem like obvious demographic information in general doesn't actually fix observed issues with fairness and data. Observed issues with fairness in data sets. So there's an alternative, maybe growing approach that suggests perhaps we can just build better representations. So the idea is, I'm going to take my data, I'm going to do some data transformation in such a way that it's, if you like, deconfound respect to sensitive attributes of various kinds that I'm interested in, making sure my algorithm or my data set doesn't contain these kinds of social values. It doesn't contain these kinds of social biases. And so I won't do a good job of representation learning. If you're already familiar, I guess this is a bad explanation. If you're not familiar, given the audience, I'm sure everyone is familiar with PCA. And so you can think of, at least in the machine learning world, representation learning is sort of fancier PCA. Generally, with, again, probably most of you know what I'm talking about, in case you don't. You know what I'm talking about in case you don't. Often people use autoencoders, but these are like low-dimensional embedding models, or low-dimensional representation models that use neural networks in the middle as opposed to say linear neural transformations. And so they become popular as a way to re-represent data in such a way that, and the key here is that you will often add something to your algorithmic system when you're building this representation, such that you might be able to remove potential. That you might be able to remove potentially, say, social biasing. So, you know, get me a clean representation if that's the goal that you're trying to solve. Okay, so I'm showing here work from some colleagues in the visual, in a visual predictive modeling system. What they did was they looked at, so they built a representation learning model. If you look at the, if you build a visualization. If you build a visualization of the representation that's learned, what I'm showing here is that there seem to be quite strong patterns. The colors here represent, I'm almost certain, again, self-expressed male or female gender choices. And what I'm showing here is a lot of structure. So there seems to be a lot of clustering of one color in certain regions. I mean, the whole thing is non-linear, but still there's a lot of structure in this data set. But still, there's a lot of structure in this data set, arguably. Meaning, roughly, the reds seem to cluster around the reds, the blue seem to cluster around the blue. And if you're interested in finding a representation that has low social biases, what you might try to do is add something to your learning paradigm such that this clear clustering structure goes away. And this is exactly what they did. So they learned what they call the fur representation. I'll go through some methods we've done this. I'll go through some methods for this in a second. But what you hopefully, this is a visual representation of something you can quantify, but what you should be able to see here is that there's much less clear structure. So the colors are more mixed. And so the argument here is if I wanted to tell, just looking at, say, a point in its neighborhood, whether it was necessarily going to be red or blue, it would be hard for me to tell because the structure is much less clear. So in this setting, if I want to build a predictor for red or blue, if I had a If I want to build a predictor for red or blue, if I had a sort of reasonably nonlinear classifier, I could do this quite well. If I want to do this in the right-hand side, this would be hard. The argument made here is that I've now successfully decorrelated social attributes from the representation of my data. And so if I do this, then I can build a learning model, say, a classifier on top of these features. And I should have much less effect of whatever social factors are sort of part of the original data. Sort of part of the original data, original embedding. Okay, so you know, this part of the algorithm worked. And then they looked at the task that they wanted to solve, which is a classification task. And here, what I'm trying to highlight here is also lots of structure and the classification labels. So here, again, colors represent different classes. Classes look like they have general cluster structure. If you're building a classifier on top of this, the argument is. Classifier on top of this document is sort of the classifier will have a good job of chopping apart different classes. So you should get pretty high classification accuracy in this setting. So, again, in the baseline setting, good classification accuracy for the tasks we actually care about. And also, you know, maybe something we don't like as much, good classification accuracy for social variables of various kinds. In the modified representation learning, we have bad accuracy for social variables. So, this is good in terms of So, this is good in terms of destroying a particular structure that we don't want to keep in our data. But they also found, and this is not nearly as clear as I wanted it to be, but hopefully what you can see, and I'll sort of highlight a bit more in a second, is that the classification accuracy actually gets quite a bit worse in the second second. Well, visually, and we can quantify or just point to the paper for some classification. But the effect was, and again, The effect was, and again, this is meant to be provocative and sort of push towards some nuance here, but you successfully reduced the predictability of social factors, but you made the target prediction task worse. And this suggests perhaps a trade-off and has been tied to lower the discussion in the field about whether asking for trustworthiness things in our algorithmic systems forces a Algorithmic systems forces a trade-off with performance or capability. I think it's part of a large discussion that's happening in the field. And here is maybe an example of something that looks like it, where we made the representation a bit better by some metric, but made the prediction a bit worse. The effect of these kinds of things is a slightly different example, but hopefully highlighting a similar point. So here's an example where it's kind of an interesting. It's kind of an interesting, weird thing. So, they took visual data sets in this gender artifacts and visual data sets. They started with fairly high-resolution images, and they asked the classifier to predict, again, in this data set, they had a gender label. Again, I'm pretty sure self-identified male or female. And they looked at actually the same COCA data set. At actually, same COCO data set as before, and what I want to highlight here is on the x-axis, they're reducing the resolution of the image. So, you're going from an image where you could probably tell kind of, actually this is a bad resolution top of bad resolution, but in full 2024 by 2024, you can roughly tell who the person is, as a person looking at this point. And by the time you get to 7 by 7, what you find, okay, so if I look at colour. Okay, so if I look at color images, you can actually do much better than chance still at predicting gender from the image. And importantly, this is at the point where the image looks like this. So there's no signal that a human, I think, can look at at this level and tell what gender is. But a predictor, again, correlation data set from maybe certain colors are more likely for images that have male or female. To have male or female identifying folks in them, and the effect is again quite high accuracy, at least much better than chance in this case. Similarly, if I remove the color, so it's just grayscale images, again, you have to go pretty low in resolution before you start doing worse than random guessing. Yeah, so again, highlighting how How good some of these can be. And here's a quite naive model that just takes a whole image and takes three numbers from the image. So it takes the average red, green, and blue color channel from the image, so three numbers from the image, and it tries to predict male and female, and it does almost 60% in prediction performance. So what hopefully I'm trying to highlight from this is, again, this idea of quite strong correlations in data. Quite strong correlations and data tied to various kinds of social, social, well, demographics of various kinds and other attributes that we may be sensitive to on building models. Much more than human, even intelligent human investigation might suggest. Similar things have been found. There's some work from the medical imaging world, where they looked at predicting race from Predicting race from medical images. They did similar things. So here they played with filtering of various kinds. So they, it's quite striking if you get a chance to look at the images in their paper. But they took images, they had race labels, so red images were tied to a black patient or not. I think they might have multiple races in the evaluation. They did a bunch of filtering, for instance. They did a bunch of filtering, for instance, a significant high-pass filtering, where by the time you're done, the image looks pretty much like noise, like white noise. So, you know, any human looking at it will not be able to tell, or a clinician looking at it won't tell any signal of any kind from the image. And this work for a clinician, it's additionally worrying because even in sort of full resolution, full everything, there's not obvious cues that tell you anything about the race of the patient that has this x-ray. The race of the patient that has this x-ray or that sort of slice of CT or brain. There's sort of no meaningful signal here, but there's enough correlations in the data and maybe cues that models pick up on this and actually get it sort of predicting, again, socioeconomic variables of various kinds, but demographic growths of various kinds. Okay, so discussing this, I think there's a positive argument that representation learning can help. Argument that representation learning can help mitigate some of the bias. This seems to suggest that there's a trade-off, and I think a lot of the work in the field have been trying to, some of my work, have been trying to understand if this is a real thing. So is there actually a trade-off in meaningful use cases? And if there is, what the Pareto frontier of trade-offs might be, and what then might we want to implement developing operations. Particularly in vision, but I'd argue very similar things and many other. Very similar things and many other kinds of tabular data, textual data, cues end up playing quite significant role in ways that are quite hard to remove, won't show up for smart people looking at data to try to see if there are any cues. But machines generally, algorithms will pick up on them. And they're enough often to bias potentially the model and potentially downstream. Potentially downstream. And maybe a caution: I think the argument about sort of feature transformations being a solution for fairness issues should at least be taken with a grain of salt and much more contextualized. Because I think there are lots of settings where it can lead to overconfidence or carelessness because, again, correlations in the data can be pervasive in ways that are hard to tell just by looking even at some quantitative contracts. Contracts. Okay, and maybe the general discussion. I think it's meaningful to maybe comment a bit that particularly in the vision example, the medical imaging example I gave two seconds ago, it's unclear. So it's clear that the algorithms pick up on demographic variables. It's not that clear. It's not that clear, like the fact that the algorithms pick up on this are by themselves harmful. And if the way, one of the, I think, meaningful ways to think about potential issues that come from algorithmic unfairness are the potential harm of the algorithms. And maybe a counter-argument to this whole discussion is, you know, perhaps these correlations remain in the data, but if we're aware and we're careful in how we build the algorithms that we want to build on top of it. So if you're on the classifier, I'll discuss a bit on classifier. I'll discuss a bit on classification settings and frameworks that you might use to mitigate issues after your classification decision. So, the argument here is perhaps even if the features that come in have correlations with social, economic, and demographic variables of various kinds, this doesn't necessarily mean that the algorithms we build on top of them will end up being harmful. And we can sometimes mitigate the issues sort of later in the development stream. Sort of later in the development stream. And we'll discuss this in a little bit more detail. This is vague in a couple of minutes. Okay, so that's that for the second narrative. The third, we'll talk about frameworks. So I think the fairness field has been quite productive in coming up with different ways that we can turn conceptual definitions of what fairness might mean into algorithmic, I guess, definitions that we can then. I guess definitions that we can then build optimizers and try to solve. And so some of the challenge in really nailing this down comes from the fact that different stakeholders in algorithmic systems often differ in what they might want from, say, fairness in an algorithmic, in a, say, AI system. So, some tensions that come up. Some tensions that come up. Individuals generally worry about individuals, and naturally so. So I care about my case. And if I'm interested in, or if I'm worried about fairness issues, I will often frame a fairness question as some hypothetical or known example of someone who I think is equivalent to me. How did their outcome differ from mine? So that's a sort of individual definition of fairness that completely makes sense for an individual when interested in was, say, a Say, some decision-making algorithmic system. The common example given is, say, a loan system that uses algorithms. Was it fair? Well, I compare myself to someone else who I think is like me and ask, did they get the loan and did I not? And I might use this to decide whether or not this algorithm was fair. Policymakers generally like population kinds of things because the decisions that they have to make are generally about sort of averages over large populations and are often worried about. Populations and are often worried about notions of fairness that tied to, say, differences in impact across different stratifications of the overall population. And so this leads to some kinds of definitions that will often differ meaningfully from individual definitions. Lawyers, advocates often focus much more specifically on harms, which may differ from the point of decision making. The point of decision making. Often will depend on a specific use case and often will play this mix between the sort of population case and individual cases, which makes things complicated. And again, or I guess in addition, media often will focus on individual cases. There have been arguments sometimes about whether the individual cases are representative, if you think about things in terms of populations, whether they're outliers. Or whether they're outliers. But the main point here is that all this significantly complicates how we might think about what fairness is. And so the open questions here are what are the statistical algorithms that we can bring to bear on these kinds of issues and what guarantees can they give us? And maybe a harder one, and one that I don't know that there's a solution for. So this is an open question. Are there approaches, models, algorithms that can satisfy? That can satisfy these different stakeholders simultaneously. So, in a few minutes, I'll go through some examples of frameworks, but mostly they target one of these definitions or one of these stakeholders and have something that are designed to hopefully meet their definition of what fairness might mean. But I think a meaningful, open question is: if one is interested in making all these stakeholders happy, it's unclear that there are frameworks or definitions that meaningfully sort of have good coverage. Meaningfully, so have good coverage over all of these potential definitions. Okay, so this often shows up in algorithm developments. Again, often people think about this as ways to mitigate unfairness in operating systems. So I'll do a very high level. I think hopefully some of the, and also often an outcome definition. So some of the speakers coming up might go in more detail, and one of the threads that I'll In more detail, and one of the threads that I'll start, but I'll just mention some of the frameworks that exist. So, roughly under the umbrella of what I might call observational fairness definitions, there's sort of two broad buckets. So, one broad bucket thinks about overall populations across groups. So, you define sensitive attributes or sensitive groups that you want to sort of mitigate harm. To sort of mitigate harm towards or against. And then you look at how the algorithm performs on these subgroups compared to the overall population or across different subgroups. And so these kinds of methods are generally called group fairness methods. So for instance, you might ask that if I'm building a classification algorithm, say let's go back to our loan decision-making system. What I want is that the algorithm makes, has Algorithm makes has equal accuracy for say two different groups that I might define. There are other notions, I think, most popular in this setting is a notion known as individual fairness that tries to design fairness guarantees that work not at the group level but at an individual level. They often mirror what I mentioned as this idea of take an individual, take another individual that looks similar, see what the algorithm does across different. See what the algorithm does across individuals that you think should be roughly equivalent to the perspective method. And so these are generally called individual fairness methods. Again, have some sort of broad literature on work, quite popular in sort of CS theory circles for various reasons. All these have pros and cons. I'll talk about some of them as we go along. There's also a line of work that recognizes that Recognizes that observational fairness doesn't take into context some of the issues that come from causal impacts and sort of, I guess, compounds that might show up in how we might measure fairness. And so there's a full literature as well that broadly categorizes causal fairness. Some of the settings within there include interventional fairness kinds of notions. So often they're asking about. So often they're asking about some kind of intervention, either some policy or downstream intervention. Sometimes they do this with demographic intervention. So it's actually sometimes quite related to individual group fairness, and ask what the methods, what models might do in those two settings, and then define some fairness notion type to this. And then also kind of factual fairness, which generally, I guess, kind of factuals are a bit messy, and I will not do it generally. Kind of factors are a bit messy, and I will not do just this in five minutes. So, definitely one of those. If you already know about it, great. I think you might already sort of follow the rest of this quite easily. If you don't, I just mention there's such a thing as kind of factual fairness. So I will mess it up if I try to do this too quickly. But it's roughly, it's closely related to individual fairness notions because often reasons about changes to individual samples. Okay, maybe a little bit in group fairness, just one of the areas that are most studied. So, group fairness generally defines some notion of outcome or statistic and then asks if this statistic is similar across different groups. So, in classification settings, this is often using various kinds of statistics that are tied to classification. Kinds of statistics that are tied to classifier performance. So for instance, I mentioned briefly equal accuracy. So this is asking, I set up my set, my, well, I evaluate my model over sets of groups, and I ask, again, how different the accuracy is across the different groups or subgroups. Equalize odds, so I look at true positive rate and false positive rate across different groups. Demographic priority, probably of predicting positive, and I look at how that differs across groups. So there's, again, a lot of literature. Is again a lot of literature by now, I think now good sets of algorithms that tend to allow you to plug in the notion that makes sense for your applied setting and then have so I guess not in early work is very specific to a particular definition. I think now we have general purpose methods where if you come up with the notion of the statistic that you want to be similar across the groups, it plugs into an algorithmic. It plugs into an algorithm or a loss function that you can sort of optimize and get the kind of behavior that you want, at least within the training sample. So there are pros and cons to group fairness. It generally is, compared to the other approaches, relatively efficient because it sort of plugs in quite easily into the standard machine learning or estimation framework. Because it's at the population level, it often is Level, it often is easy to explain to policymakers because you can discuss things in terms of differences across different populations. So that's a language they often like to think about. And there's a bunch of work in this area, ways of doing this before training algorithms, within the training loops, adding something to your loss function. There's also a bunch of work on like, I take an algorithm that, say, predicts probabilities, and what can I do post-training? So, in black box settings, for instance, to change the predictions that I get, some notion that I. That I get some notion that I play. Some of the challenges: you have to define what you want. So there's this additional choice of statistic, which ties in this outcome definition question that ideally is done in context of the problem you're actually solving, but can be challenging to do in general. Generally, you need to know the groups in order to define how the performance of an algorithm differs across groups. So you need a group label. A group label. Group labels are considered or can be challenging in settings, in many settings. Often, this is correlated with issues of privacy. And often, this leads to additional trade-offs that can be challenging of fairness and privacy, meaning being able to calculate the differences across groups needs you to know something more about the individual that they might be willing to share or want to share in various settings. And this leads to. To share in various settings. And this leads to this dichotomy about how easy or hard it is to implement good parents if I don't know sensible attributes. Okay. And in some settings, we care about differences across groups, whether or not there's a causal chain that sort of explains what this difference is. In those settings, on the pro side, this is good. On the con side, again, this doesn't grapple with The sort of causal causes of potential differences in today that we're observing. Okay, individual fairness. So this is individuals should be treated similarly. Often this is done in a more diffused way. So you define a small neighborhood and you ask that examples within small neighborhoods have similar predictions of various kinds. Some of the good things, it tends to be easy to explain. It tends to be easy to explain to individuals because, again, it has this like my example, some other example, but I have across examples. It has some nice connections to statistical machine learning theory, so things like generalization are closely tied to the guarantees and individual fairness. It also gives you some group fairness notions for free, which is considered a good thing. There's some challenges in defining what it means for two individuals to be considered equivalents. To be considered equivalent? So, what does it mean for two people to be considered similar? So, this defines, requires an additional choice of metric for similarity across individuals, which is, I think, an even harder problem than often deciding what metric to violate in terms of outcome. So, yeah, you need to, like, deciding this ends up being, I think, quite a taunting problem for many applications. It generally is a bit more expensive computationally. Again, it doesn't account for causality. Again, it doesn't account for causality. So, causal fairness, just straight to advantages and disadvantages. So, it gives strong causal guarantees, which is great. It's great in applications where there's a bunch of domain knowledge that can help you specify a causal graph and some of the mechanisms. It often will be a great match for applications that are close to science. So, genetics often will find good application of tools like this. Application of tools like this. And there are also connections, which I just mentioned a paper that talks about this. There are connections between some definitions of causal fairness and some of the group fairness definitions. As many of you know, causal definitions in the causal graph, causal graphs are notoriously hard to specify, so that's hard. Generally, a bit more complicated to implement and enforce. And I generally find it quite intuitive to explain if you don't already know a lot about causality. So, if you're trying to, again, interact with stakeholders, I think some of these definitions make this challenge. Okay, moving on. I think I'm slightly behind time, so I'll speed up a little bit. So, you know, frameworks capture important aspects of the problem, but as hopefully I've highlighted, they all have blind spots and failure cases that they don't fully capture. They don't fully capture. Again, fairness issues are complex. I'd argue the complexity here is, at least in my opinion, it's a benefit, not a bug, because I think to solve this problem well requires grappling with often with impacts and with stakeholders. But I think this is a good thing for building better systems. And I think there's a lot of scope for folks who are interested in building frameworks or algorithmic tools. So, the better we can make these tools to capture more of the complexity, I think the better impact some of these tools can have. And I mentioned at the beginning, I think some of the biggest open questions are ways to bridge across what different stakeholders might want from a fairness algorithm. Okay, metric choice. So, metrics, challenging. Choice. So metrics, challenging. I'll speak a bit about some of my work on metric definition, but metrics are often hard to pick. Here's an example that I think really highlights difficulty of metric selection. So if you've seen a fairness talk in the past, I don't know, 10 years you've seen this. This is every fairness talk ever. But I think it's a good example. It's a good example because it highlights, again, oh, for many reasons. There's some bad things about this example as well. As well. So I'll ignore, for instance, whether or not it's a good idea to use algorithms to decide whether individuals should go to prison or not. This is the task that this algorithm was designed to solve. But I'll mention that this algorithm was designed for this task. And when it was designed, actually, interestingly, the folks who designed the algorithm were where fairness concerns and had a fairness definition that was roughly tied to calibration across. That was roughly tied to calibration across groups that they wanted to be similar. So that was the definition. They built it, they implemented it. Once deployed, ProPublica did a post-evaluation and noticed that false positive rates and false negative rates were hugely different across race groups. And so for me, I think the takeaway that I find most meaningful from this is the fact that different definitions can actually not correlate in terms of. not correlates in terms of outcomes. And actually sometimes the anti-correlate with each other. So give you two different fairness definitions, I build an algorithm that satisfies one of them, and I actually do really badly respected the other one, vice versa. And so this, I think, really highlights the difficulty of metric choice. And in fact, this led to, I think, some quite interesting work in the field with a variety of people who coined the term impossibility results in fairness, which tried to highlight the fact that multiple metrics are not achieved. So multiple metrics are not achievable in general simultaneously. So generally you have to pick. Something happened to this one, by the way. Or at least over there. Okay, so some work in my lab trying to tackle some of this has been thinking about elicitation approaches as a way. So one, I think we argue that the fact that it's sort of context and stakeholder specific might be a good thing, yet there's difficulty for particular stakeholders to specify. This difficulty for particular stakeholders to specify exactly what they want in terms of a metric definition. So, we have some algorithmic tools that we have been building to try to help stakeholders define metrics via pairwise preferences. I'll talk about something sort of related to this on Thursday, I think Thursday, about some of the work we've been doing on preferences, pairwise preferences as a way to define sort of learning systems. But, anyways, we have a line of work thinking about ways to simplify the task. Ways to simplify the task of picking metrics via pairwise preference elicitation. Okay, so on the pro side, metrics make it easy to set up machine learning problems. This is the thing that we know how to do in the field. If I give you a well-specified function, tell you this is the thing I want to measure, I know how to build learner optimizers and sort of optimize the hell out of it. But it's hard to pick the metrics, and I think. And I think importantly, again, the metrics generally are context stakeholder specific, and you have to be aware of this. Some emerging work, thinking about listening as a way to select metrics, it does have a con in that it depends on stakeholders to help you in that task, and they themselves have specific biases that one has to sort of handle correctly. Metrics are hard to select. All metrics have blind spots and failures. We're reducing a big, complex problem into a number or a set of numbers. This will lose something in general. And I'd argue, you know, often we should be careful about making sure we do a good, good post-deployment evaluation of our tools. I think that's often meaningful because there can be gaps in just looking at the training data and not thinking about sort of post-deployment. About sort of post-deployment, unintended consequences. I'll spend two minutes on this, assuming there's time at the end. Okay, so I'll move on to the last narrative on fair data, meaning fair models. So I'll discuss this in the context of post-deployment considerations, because I'll make an argument that sort of while we can fix a lot of the algorithmic issues, and at least in sort of thinking about training a machine learning model. Thinking about training a machine learning model, there are additional issues that come up that can make the problem additionally complicated. I'll touch on some of them in a couple of minutes. Okay, so as many of you know, is a favorite cartoon for SKCD. It's intentionally a cartoon, sometimes. What it says is, well, how do I build your machine learning system? Well, you pour data into a big pile of Need Algebra, you mix it till it sort of looks right, and you solve your machine learning problem. And the reason. And you know, the reason this happens, or you know, at least what this is trying to highlight, is some of the weird sensitivity and sort of human choice and actually tied to some of the robustness discussion today, and a lot of how we build machine learning systems. So, in fairness, some of the issues that come up include generalization. So, there's a lot of effort paid to generalization of the models that we're building. It turns out, and this is not a surprise once you spend sort of two seconds on it, but you know. Spend sort of two seconds on it, but you know, maybe not obvious a priori that fairness issues also have generalization that may or may not always work. What I mean specifically is: I might build an algorithm, I might specify some fairness notion within my algorithm instruction. So on training data, I sort of correctly sort of satisfy my fairness notion. So I'm showing that all the tops clear circles are definitions of within group. Of within group behavior, where the model actually does what I want it to do. So, in training data, or here in the source, the gaps between groups are small, there's quite some notion that I think is meaningful for this problem. Yet, when I deploy this on tests that has some distribution shift, I see big differences across groups. So this is highlighting the fact that all the generalization issues that we come up, we focus on generally purely in terms of performance, also show up for other things that we care about. Also, show up for other things that we care about, like fairness and many other issues that we might talk about over the next few days. There are also adversarial attacks, so if you like adversarial stuff, there's a bunch of adversarial things that you can do, and fairness as well. In particular, one that has been pointed out, and I think can be meaningful in some settings, is something known as fairness gerrymandering. And the idea here is that I can build I can build actually quite easily. I can build models where at subpopulation levels, sorry, at subpopulation levels, I satisfy some fairness notion. But if I dig a little bit deeper, the algorithm is sort of cheating in a very particular way. So the example I'm given here, the example used in the Kern's paper, they called, I think they coined the term Kern S gerrymandering, is the idea that I could build an algorithm where if I look separately at, say, self-identifying, At say self-identified male-female gender, the algorithm has similar behavior across those groups. It looks separately at black and white, it has similar behavior across those groups. Yet, if I look at intersections of those groups, so particularly the example they said was a black woman and a white man can have extremely different predictions or outcomes because what the algorithm has done is it's hidden sort of variance in the averages in a particular way such that in aggregate things look Ways sex at in aggregate things look fine, but in particular, these look quite bad. And so there's a whole history of thinking about intersections of different demographics and what this might mean for fairness. But I'd argue that this issue shows up in all of the ways that we think about fairness. I'm arguing this as the adversarial gap here. If someone wanted to build an algorithm that satisfied some external evaluators' checks, but was intentionally doing something bad, this is one way that they could do it. What I would define as bad. I would define as that. And we're close to time, so I'll just mention a couple more things. There's also a bunch of work on delayed impact. So algorithms deployed, often the impact of the algorithm is sort of not right at deployment, but sort of after some time. You often feedback loops in algorithm deployment. This ends up making all these issues quite a bit more challenging. Okay. Okay, so training data is a big part of the problem, but often it's not everything. So there's lots of other issues. Oftentimes, other things that we care about in building good machine learning systems. But many of these issues show up again when we think about fairness, evaluation, and mitigation. Generally, composition breaks things. So if I sort of guess a fairness property, it's some outcome of some algorithm, say representation learning, but whatever you like, I build something on top of it. Generally, the next thing will not side. Generally, the next thing will not satisfy the fairness constraints that I put in the first thing. And this is true again for many things, but definitely true for, or often true for fairness. Okay, so beyond trained data sets, there are lots of other issues that can come up. So I will end. Fairness is socio-technical. Algorithmic contributions can provide useful tools. For those of you who find this compelling, I think there's sort of work within domain with stakeholders. Within domain, with stakeholders, thinking about specific contexts and trying to build tools that better address specific problems. But I also think there's lots of opportunity for algorithmic work that helps sort of provide frameworks that can be then hammers or handles that people who are trying to solve specific problems can bring to bear. We should try to avoid oversimplifying. I think this is something that we, particularly algorithmic cults, fall into very quickly. And sometimes it's not a simple solution. Maybe that's fine. Okay, I will end here. Thank you very much. Let's take the speaker. Any questions from the audience here at first? Can I riff off your final point and sort of ask a perspective about saying that you've given us lots of things that work sometimes or sort of work, and really, where is your perception about is this? Perception about is this different things work in different cases and that's fine, or lots of things work a little bit and every case is hard? So I think every case can have particular nuances. I think the opportunity for us is maybe, just repeating it a little bit, bring in tools that can then be applied to hopefully, so the generality opportunity. Generality opportunity, because we like to build tools that can be applied in many places, is the tool building that can then people can use as part of their arsenal for solving the complicated sort of issues that come up in specific use cases. I would say most meaningful cases have nuances. It's generally not one shot either. So, you know, you build something, you get some feedback, hopefully, you improve the thing you built. I'd argue that this is not that different. I'd argue that this is not that different from the rest of our practice, but somehow feels more complicated in the setting. So, everything I've said, in fact, I'd argue, if you're doing a, hopefully, doing a good job of any of building an algorithmic system that has some impact on some real application in the world, it will be complicated. There will be lots of corner cases. You won't cover everything. To really, really solve a problem, you have to talk to stakeholders and sort of dig deep into the problem. You usually can. You usually can make meaningful progress and build tools that are general, but be part of an arsenal that a particular sort of domain person will use for, I don't know, say a clinical or knowledgeable decision-making problem. I don't think it's meaningfully different here, at least in my opinion. Like all those same properties of all of data analysis, I have machine learning tools show up here. It feels more complicated. And hopefully, a part of my goal here was to maybe suggest that it's closer to what you're accomplishing. It's closer to what you're comfortable with than it seems. Like it feels much more complicated, but it is the same. It's very similar, I would argue. I mean, the impacts, I think, would be maybe more concerning in many cases. But at least the data practice part of it, I think, is often similar to many of the other parts of building good tools. We'll leave one more question and then move to the next speaker. One more question and then move to the next few. Okay, so you mentioned towards the beginning that some of these fairness problems may not hold under distribution shift. So is this specific to this? So what kind of notion of fairness are you talking about here? So I can see that this is obviously would be true for observational fairness, but for causal fairness I assume this will not be the case. Um so the difficulty with causal fairness again is the stability of the causal graph. The stability of the causal graph. I think if it is true that your causal graph is sort of correct and sufficiently stable, and I guess, you know, in fact, one of the reasons we like causal type methods is the distribution shift question specifically, as we're noting. So, with the caveat that you have the right causal graph, sure. Yeah, so only distribution shift. Yeah, and if distribution shift is consistent with the causal graph. And if the distribution shift is consistent with the calls of rep, then in principle, you should be fine. Good question. Thank you. All right, let's take the speaker away. Okay.