So, hopefully, the audio and the video worked on that. But just let me know if you didn't hear any audio or if you didn't see anything on your screen. It worked. Awesome. Perfect. Great. All right. So, today I'm going to be talking about how we can build data sets that can enable us to build. Data sets that can enable us to build immersive neural fields like the ones that I just showed. All right, so before I begin, I want to take the opportunity to thank all of my students. Pretty much all of the work that I'm going to be talking about today was work that was done by most of them. So a huge thank you to all of them. As humans, we lead extraordinarily interactive lives. So we might go to a market, pick up vegetables with our hands, inspect vegetables before we buy them. Before we buy them, we use multiple modalities to help us make these decisions, including touch, vision, sound, and so on and so forth, language with other humans, interaction with other humans. We use machinery to build large infrastructure like buildings and bridges. We use our hands to do very fine-scale activities. For example, here you see an example of a person peeling a garlic, and you can see the subtle sounds that are involved in doing something like this. And as humans, we are really, you know, And as humans, we are really attuned to listening to many of these fine-scale multimodal inputs to perform our daily tasks. And we might do other skillful activities like woodworking and cooking and so on and so forth. Now, one of the grand challenges in robotics and AI in general is to build machines that have some similar capabilities. So, we want machines that can help humans perform these daily activities like cooking or woodworking. Perhaps we want robots that can automate. Robots that can automate some of our daily mundane tasks. But in order for robots to be able to perform some of these tasks, it's really important that they have the capability to understand the world, the dynamic 3D world, as well as the multimodal sensory inputs that these 3D worlds actually generate. So our world is composed of vision, but also of sound. We speak as humans, we speak with each other through language, and the whole world is dynamic. So it's not a static setting, but everything's. So, it's not a static setting, but everything changes dynamically. So, if we want robots to operate in such environments, they need to have multimodal dynamic 3D understanding. Now, obviously, multimodal dynamic 3D understanding is something that has been investigated both in the computer vision and broader AI communities for a very long time. And what I'm trying to do in this slide is just to provide a tidbit of what the current progress in multimodal dynamic 3D scene understanding is. And I'm going to do that from three perspectives. And I'm going to do that from three perspectives, right? So, from a computer vision perspective, one of the things that we really care about are representations of the world, especially if we are talking about modalities other than vision and shape, things like audio and things like maybe other kinds of wavelengths like infrared and so on. So, there's been a lot of work that's been done to build representations, rich representations of the 3D world. So, there's obviously been a lot of work. And more recently, and I'm sure this is something that most of you. sure this is something that most of you are familiar with is is is these neural field representations or neural radiance fields that have been that have been increasingly popular and being increasingly used for many everyday tasks involving dynamic 3D understanding. There's also been a significant amount of work trying to understand the dynamics of the world, including things like human motion, motion of vehicles on the road, and other kinds of dynamic activities. And again, here, Dynamic activities, right? And again, here, more recently, we've seen a lot of work that uses neural fields to capture rich radiance fields of the world, dynamic 3D world, and being able to capture this very quickly and very efficiently. And finally, we've also seen significant advances in multimodality, specifically with language and vision. So we've seen methods like CLIP, methods that incorporate CLIP into neural fields, and so on and so forth. So obviously, you know, there's been phenomenal progress. Obviously, there's been phenomenal progress that's been made in 3D representations, capturing dynamics, as well as building or connecting different modalities. In this talk, I'm going to be specifically focusing on multimodality in 3D with a specific emphasis on neural fields. So, again, neural fields, by neural fields, I mean a broad category of methods that use neural networks to capture fields, but more specifically, sometimes I might interchangeably use neural fields. Sometimes I might interchangeably use neural fields in NERVS. So, there's been a lot of work on trying to build representations that are captured by NERF models, dynamics, as well as multimodality within NERF models. And again, I'm kind of preaching to the coir here, but most of you are probably familiar that neural fields are becoming more and more popular. This slide is almost a year old now. And just at CBPR last month, there were over 120 odd papers that deal with neural fields in one form or the other. So it's really, you know, there's been significant. Really, you know, there's been significant interest and advances made in this area. Of course, there's been some limitations with neural fields. I mean, neural fields are not really not yet the solution to many of the things that we want to do in 3D computer vision. So I want to talk about some of the limitations that we have with using neural fields to capture multimodal dynamic 3D scenes. So let's talk about what those limitations are. So, first of all, in computer graphics, representation of the computer is a very important thing Graphics, representations such as meshes and point clouds have been around for decades, right? And there's been excellent techniques that have been developed to edit these representations. So, here you see an example of 3D meshes being edited in real time with a user interface. So, these are tasks that we can readily do on most of our machines, on our smartphones, and so on. But these are tasks that are still very hard to do if we represent the underlying shape or the appearance with neural fields. So, if you were to use a nerve model. With neural fields. So, if you were to use a nerf model of this person's head, then editing this neural fields is still a challenging task. Of course, there's been some progress in that area, but we are not yet at a level where we can do some of these editing tasks using the same kind of hardware that we use for meshes and other popular representations. Similarly, for dynamic capturing dynamic scenes, there's been again a lot of exciting progress on dynamic neural fields. So, here you see an example of work from Work from last year that tries to capture a dynamic neural field of a particular scene, of a dynamic scene. But these methods are still limited to short durations. So typically, these methods are anywhere between one to 30 seconds. Whereas you can go to YouTube, you can go to Netflix, and you can watch a two-hour movie using that is just represented as pixels, essentially. And this is something that we can't yet watch a two-hour nerve with synchronized audio and so on. So we don't yet have. Synchronized audio and so on. So, we don't yet have Nerf movies, right? So, that's another limitation that we are currently experiencing. And then, finally, multi-modality in 3D neural fields is still just getting started. So, we've seen some really great work, for example, Object Folder from Jiarjan Wu's group at Stanford, which tries to capture very rich representations of objects. So, in this case, we have objects like Coffee Marks, which contain rich information about the appearance, the shape, the impulse. The shape, the impulse responses that the audio that you get when you touch that particular object, as well as tactile information about what happens when we touch the object. So there's been some exciting progress in this space as well. However, this progress has been limited to static objects and sounds that are just impulse responses. So we don't yet have rich neural fields that can capture dynamic objects that make dynamic sounds, essentially. So what we wanted to do with in our group is to try to address some of these limitations. So essentially, we wanted to enable us to build very immersive neural fields of dynamic 3D scenes with rich information, multimodal information. So not just appearance, not just shape, but also things like audio, as well as text description that describe the activity that is being performed in this dynamic scene. So that was our goal. And the rest of the talk, I'll essentially be giving you an overview. I'll essentially be giving you an overview of this data set and data set that we've built and how we've used this data set to demonstrate that we can capture immersive neural fields. Okay, and again, I'm not going to go into details here. There's been a lot of exciting progress on the data set front as well. Many, many data sets exist that help us accelerate progress in building such immersive neurofields. However, the primary limitation has been around the dynamics, so scene dynamics. It's very hard to capture. It's very hard to capture NERSH models of highly dynamic scenes because we need a lot of views. There are also not many data sets that have rich text descriptions as well as audio corresponding to the activity that is being performed. So these are essentially the two limitations that we specifically wanted to address. So the result of this discussion that we had in our group is this new data set that I'm introducing today. So we're calling this the Dynamic Visual Audio Data set, or DVA360 for short. Or DVA360 for short. So DWA360 is essentially a data set that is captured using a multi-camera capture system. So this capture system is roughly, it has the shape of a cube with six sides. There are in total 53 high-resolution cameras that capture at 720p and at a very high frame rate of 120 frames per second. We also have six microphones that are located all around this capture volume to give us spatial audio. Give us spatial audio. And then we capture audio-visual scenes in this. So, not just static objects, but also highly dynamic scenes that generate a lot of audio signal as well. So, using such a capture system, we built two data sets. The first is what we call the DBA360 dynamic data set, and the second is a static data set of static objects. And all of this data set contains 360-degree views of the object from these 53 cameras. We have detailed text descriptions of all of these objects and scenes. Of all of these objects and scenes. For the static data set, these objects are canonicalized, meaning they're oriented in a consistent way, similar to what you would have in ShapeNet, for instance. And then finally, we also have rich spatial audio and together with the text descriptions of the dynamic activities that's happening in this data set. And this was work that was led by several students, including Cheng Yu Paysin and Angelo. So, how does this data set look like? So, here's some Set look like. So, here's some statistics about this data set. So, we have 46 prolonged sequences that vary from five seconds to up to three minutes long. So, these are 53 views, 46 sequences. We have about 1300 seconds of synchronized audio and video, 8.6 million image frames in total. For the static data set, we have about 95 objects across 11 categories. And in total, we have over 8,500 words of text descriptions that describe. Of text descriptions that describe both the dynamic scenes as well as the static objects. So, this is, you know, this was really a large effort by many students from my group to collect such data. All right, so what are the challenges if we want to collect data like what I just shown you? So, there's a few challenges. First of all, we need a multi-camera capture system that can provide us a way to capture such diverse data. Towards data. When we collect multimodal data, sensors, calibration, and synchronization is always an issue. So we tackle some of that. And then finally, we need to do something useful with the data. And I'm going to show you some examples of some of the immersive neural fields that we can build when we have such high-density multimodal data. So I'm going to be talking about each of these in turn. So, first, briefly about the capture system. So, we designed our own multi-camera capture system. So, that's the box that you see here on the left. So that's the box that you see here on the left. It's a time lapse of my students assembling this capture system. So this is essentially a one-meter cube. The volume is inside this box. And there are cameras on all sides, all six sides of this cube, including in the bottom. We have a transparent glass shelf that allows cameras in the bottom to image stuff that is placed on top of this shelf. And on the right, you see the interior. So this also doubles up as a light stage. We can actually control the color of the LED walls so that we can. So, that we can simulate different kinds of environments. So, this is, and I'm kind of glossing over some of the details here, but we essentially built, designed this hardware ourselves, built software for synchronizing all of that, as well as developed the annotation pipeline for taking this raw data and annotating useful information from it. So, that's the capture hardware and software part of it. So, how does the data look like? So, hopefully, this plays okay on your end. Okay, on your end. So I'm going to play this. So this is this shows 46 images, 46 video streams from the 53 video stream that we actually capture. So this is a person using their hand to place vegetables or fruits on a ball. And you can see that there's synchronized audio as well as detailed text descriptions. So we have toys, such as what you see here. I'm told that this is called Bluey. This toy is called Bluey. So this is Bluey making a lot of noise. Of noise moving its hands around, so we have like a very omnidirectional view of what this toy is performing. In addition to that, we also have very subtle interaction. So in this case, this is a person using a screwdriver on a toy. And the reason why we are also interested in subtle interaction is there's been a lot of research in cognitive science that shows that when we do hand manipulations, the same part of the brain that is responsible. You know, the same part of the brain that is responsible for performing that manipulation also lights up when we hear the audio corresponding to that manipulation being performed. So, there's been extensive work in the cognitive science community that associates audio as well as being able to look at something and replicate the same kind of skills, right? So, that's why we are really interested in synchronized capture of both video as well as audio signals. All right, so there's also All right, so there's also static data. I'm not going to talk too much about this, except to say that we have about 100 objects from 11 categories. These are all real-world objects, everyday objects that we found in the lab and home. Okay, so in addition to all of this multi-view data, as well as audio data, we also provide foreground-background segmentation masks. And we also provide the six-doff pores of all of the different objects that are available. The different objects that are available in the that we capture in the system. And you know, I was told by Andrea that there was a lot of discussion in the last couple of days, which I unfortunately missed because I'm in a very far away time zone. But I was told that there was a lot of discussion about canonical spaces. So I wanted to just say a couple of things about how we get the annotations for the orientation of the different objects. And we don't actually do this manually. We use a self-supervised method in order to automatically obtain these annotations. And the method that we Annotations. And the method that we use is actually work. I'm going to take a slight detour here to talk about that, which is work that appeared at CVPR just last month. And the idea here is that given a bunch of input nerves belonging to a certain category, like cars or chairs, these are object-centric nerves, and they can be in any orientation, essentially. And one of the problems that we have, and many of you here have worked on that, is aligning nerve models with each other. And essentially, this is the problem that we are trying to do here. That we are trying to do here. So, given on the left, you know, different NERF models of cars in different orientations, we would like to get a NERF model of the same cars in a canonical orientation. So, that's the task that we're trying to solve here. So, it turns out that in order to solve this, you actually need to make sure that the neural network that you're using to solve the problem is rotation equivariant. And rotation equivariance in fields is a little bit more complicated than in scalar fields. So, we care about rotation equivalence. Fields. So we care about rotation equivalents and vector fields. So, in addition to just when you rotate something, just moving a scalar from one point to another is insufficient. You also need to change the orientation of that particular point. So, you know, again, I'm kind of glossing over things here, but essentially we take this into account in our neural network architecture. And essentially, the architecture that we built for this pipeline looks as follows. On the left, we have input nerve models from a certain category that are in arbitrary orientation. Are in arbitrary orientation. So we use these networks called tensor field networks, which are equivariant to rotation, translation, as well as permutation, to extract features that are also equivariant to rotation. And that gives us an estimate of the rotation that would rotate the input shapes to the canonical shape. To make it more robust to different instances from the same category, we also compute the spherical harmonics on top of the input nerve models. On top of the input nerve models that we have. And then we have another set of equivariant features. And it turns out that when you take the dot product of two equivariant features, you get something that is invariant. And that invariant thing is the canonical pose or the canonical shape of the input nerve that we are looking for. And again, the paper has more details, and I'm happy to talk about this offline, but this is the high-level idea. And what we can do with something like this is to build models that can take as an Build models that can take as input what you see on the left, the left columns, so nerf models of objects in arbitrary orientation, and make them all consistently oriented. So, that's the task that we're trying to solve. So, this is what we use to canonicalize all the different objects that we get from the DIBA360 data set. So, the static data set, we actually use a self-supervised canonicalization. So, we don't have any manual annotation for that. Okay, so now back to the DWA360 data set, which The DWA360 data set, which by the way is now publicly available. So you can go to dwa360.github.io. You'll see more information, the paper, as well as links to download the entire data set, which is very big. So I wouldn't want you to download it on your laptop. It's about 3.5 terabytes. But there's more information on this website if you're interested in using this for some of your own research. Okay, so the data set is only as useful as what we can. Only as useful as what we can actually do with it. So, in this particular work, we try to demonstrate that this is useful for certain benchmarking tasks. Specifically for dynamic data, we're interested in evaluating how some of the more popular methods, current methods, perform on reconstruction quality. So, we compute standard metrics in the NERF literature, such as PSMR, SSIM, LPIPS, and so on and so forth. And the same thing for static data, where we also evaluate. Static data, where we also evaluate pose estimation accuracy in addition to the standard appearance metrics. For the dynamic data, we compare two methods: MixVoxels, which is a dynamic NURF method designed to capture dynamic scenes, as well as instant NGP, which is we compute instant NGP on a per-frame basis. So we take our entire data and then we apply, we estimate an instant NGP nerve for every single frame. So let's look at some of the results of some of the neural Of some of the neural immersive neural fields that we get. This is the put-fruit sequence that I showed earlier. And the result that you're seeing here is a per-frame instant NGP fit. So we're fitting an instant NGP Nerf model for every single frame in our data set. And this has audio as well. So I'm going to play both of them now. So you can see that there's both appearances for the So, you can see that there's both appearance as well as geometry as seen in the vertex map. And here you can see some of the subtle interactions, and here we capture. Some of the subtle interactions, and here we capture both small objects such as the batteries, but also the small fine interactions of the hand screwing the screw on the top. Perhaps the most interesting insight from some of these experiments is that we compared the dynamic NURF methods as well as per-frame instant NGP, and we noticed that per-frame instant NGP actually is better in most cases. So, the question that I think maybe we should ask ourselves is: should we just do ourselves is should we just do it just like NPD on a per-frame basis right so instant NGP is better both in terms of quality of reconstruction but also in terms of training time it's much shorter than some of these dynamic nerve methods all right so we also have a few examples of static objects so here you can see static reconstruction of an instance from the cars category this is from the chairs category these are all miniature models these are not real Category. These are all miniature models. These are not real cards, but these are miniature models that fit inside the capture volume that we have. And here's a banana everyday object that we capture. All right. So again, you know, I'm not going to go into the details here, but we compute all the metrics that people might be interested in. And our goal here was not to saturate these metrics already. So we envision this as a data set that people can compare on. And therefore, some of these metrics are not the best that they can be. These metrics are not the best that they can be, and that's fine. So, we want people to kind of use some of these data sets and improve methods and hopefully use this to enable capabilities in neural fields that weren't previously possible. All right, so yeah, so that's to summarize. This is the dynamic visual audio data set that we've recently captured: 8.6 million frames, both dynamic as well as static objects and dynamic scenes. Of course, you know, Dynamic scenes. Of course, this method this data set isn't perfect. There are some notable limitations. Firstly, the focus here is not on scaling it to lots of different objects and lots of different categories, but rather to limit ourselves to a small set of high-quality objects and high-quality dynamic scenes. So, by construction, we focus on quantity of data for a limited number of objects rather than scale in terms of number of objects and categories. And secondly, the benchmarking metric. And secondly, the benchmarking metrics that we use are currently limited to measuring only image or visual performance, not necessarily for audio or other multi-modalities. So that's something that we are really looking forward to doing in the future. And then finally, this is not yet open world. So this is in a very constrained lab setting where we need to put miniature objects and so on and so forth. Ideally, we'd like to expand this to real-world, open-world problems. And how we do that is still an open question. So, yeah, to summarize what I presented today, I think we've been seeing really nice progress in using neural field representations to solve dynamic 3D understanding and multimodal 3D understanding tasks. And hopefully, the data set that we captured is going to be useful for the community to kind of continue to make such nice progress. And yeah, this data set is available publicly, should be available soon. Should be available soon. And I talked about some of the examples with the capture system that we built in order to capture this rich data. So, in terms of future work, I mentioned about open world problems and being able to capture such rich data in everyday settings. And I think this is the holy grail in some ways, and this is what we're trying to do. And one of the things that we are exploring currently is if we can build wearable capture systems that can capture this rich multimodal. That can capture this rich multimodal data. And just as an example, here is one video of a variable, wrist-mounted variable capture system that we're currently building that captures hand data in everyday settings. So in this case, this is me shopping in a supermarket. Yeah, so you can see that the kind of data that this kind of capture system has is very multimodal, but it's also very hard to operate. How do we build rich representations using such eco-centric data?