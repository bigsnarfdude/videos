This morning, but hope to be around for most of the other days. And I'm glad we're still having this. Of course, it's nothing like being everywhere in person, but we do what we can. So today I'd like to talk about some work that's related to inference and stochastic modeling. And I hope it's reasonably on topic. And there'll be some links to machine learning that perhaps are interesting. So I apologize if this is not exactly on topic for optimization under uncertainty. On topic for optimization under uncertainty, but I think you could imagine this as being relevant for understanding the conditions of uncertainty under which you solve an optimization problem. And this is joint work with my PhD student, Ricardo Baptista, my former student, Alessio Stantini, and my former postdoc and ongoing collaborator Olivia Zon, and a couple other people that we'll mention along the way. So in this talk, I want to talk about a couple problems of interest, and I hope to convince you by the end of the talk that these problems are actually all very much. These problems are actually all very much the same, or they have a lot of very common elements. I want to talk about Bayesian inference and stochastic models. So, imagine inverse problems where the forward operator itself is stochastic. I also want to talk about data assimilation problems. So, essentially, you know, imagine state estimation problems or estimating the state or parameters in the state-face model. And also I want to talk about likelihood-free inference, or people are now calling simulation-based inference, which is very much the same problem, but in the particular set. Very much the same problem, but in the particular setting where the density functions that you might normally enjoy having at your disposal in Bayesian inference are not available or cannot be directly evaluated. And I'll describe exactly what that means. Now, in this setting, the particular technique that I want to use involves transport, just a little cartoon to get everyone sort of oriented on what do I mean by transport. So in general, we can think of our goal as being one of characterizing, for instance, sampling from a complicated probability decision. From a complicated probability distribution whose density I'm denoting pi or measure sort of interchangeably. And so essentially, these are contours of the density of the complicated distribution, which here is a banana for our cartoon purposes, but in general, something high-dimensional and complex that I don't know a priori how to draw samples from. And then on the left, I have a distribution that is sort of nice and familiar. It's a standard Gaussian. And this, of course, I do know how to draw samples from in arbitrary dimensions. Arbitrary dimensions. And this is sort of an easy construct quadrature with respect to this distribution. And the goal is now to construct some deterministic coupling of these two probability distributions. And I want to do that using a transport map t. And if these are distributions on Rd, then essentially t is a function from Rd to itself. And this is the notation that says T pushes forward the distribution eta to pi. So you could think of this as if I could draw samples from eta and I evaluate t on the samples, then I'll get samples that are. T on the samples, then I'll get samples that are distributed according to pi. Okay, and alternatively, actually, we'll talk a lot about this sort of reverse problem where I have samples from pi, and maybe I would like to transform them back to samples from a standard Gaussian. And I'll just use that using a map that's the inverse of t, which I'm going to denote s. And we're going to put in conditions that make sure that s and t are invertible. And so this case, s would push forward pi to eta, but these are all equivalent. And of course, we are realistic people. And of course, we are realistic people. If we could do this exactly, we would have perfect independent unweighted sampling. But we will be happy if we can satisfy these conditions only approximately. And we'll talk about all of what that means as we come along. Okay, so that's kind of the idea. Now, of course, given two distributions like this, you can see there's an infinity of mappings, of transport maps that push forward eta to pi and that push pi back to eta, as long as, for instance, eta. As long as, for instance, eta and pi don't have atoms. If they have atoms, then you get into some trouble. But assuming these are two absolutely continuous distributions, then there's an infinity of mappings you could choose from. Now, which one you choose actually becomes very important for the kinds of problems that I'm going to discuss. Now, one could certainly imagine using optimal transport maps or various other kinds of transport maps. But the particular kind of transport I'm going to use here is not the optimal transport, but in particular a Optimal transport, but in particular a triangular transport. And the triangular transport is basically has the following structure. So if we're considering functions S from Rd to itself, the mapping S has D component functions, S1 through S D. And the first component function is a function only of one variable. The second is a function of two variables, so on to the D component function that's a function of D variables. Now, ordering here is important, and this is the blessing and a curse, but let's say we have chosen an ordering. But let's say we have chosen an ordering, then you can show that there is a unique triangular mapping that pushes forward pi to eta, essentially under mild conditions on pi and eta, essentially that they don't have atoms. Now, why do we pick this triangularity? There's both some sort of practical numerical reasons. So on the one hand, by constructing the map in this way, well, essentially, if I hold constant the earlier arguments of any given function, I could think of these as being functions that are invertible in the last variable, and then the whole map. In the last variable, and then the whole map becomes invertible. Also, the Jacobian determinant, which is something I'm going to need to compute a lot, is essentially the determinant of a triangular matrix, which is simple to evaluate. So the Jacobian of S is going to be something simple to evaluate. And then Montenest needs one-dimensional, and this actually leads to simple parameterizations and things like that. But the deeper reason, and the one that kind of relates back to the goal of the talk, which is to talk about likelihood for inference, is Talk about likelihood for inference is that constructing the triangular mapping in this way exposes marginal conditionals. So, what do I mean by marginal conditional? Essentially, what I mean is to have in mind this kind of telescoping factorization of the joint density pi. So, this is a density in d variables, but essentially I can take any such density and factorize it this way, where I have the marginal on x1. I have the marginal conditional of x2 on x1. So, I've marginalized out x3 through xd. I'm only focusing. out x3 through xd i'm only focusing on this one-dimensional marginal and then i have x3 conditioned on x1 and x2 all the way to xd conditioned on x1 through d minus one each component sk characterizes this the kth marginal conditional in a very specific way and that's going to be useful for us later on okay and i just do want to emphasize um please feel free to interrupt me with questions um this can be as conversational as you like okay so what Okay. So what let me just elaborate on the previous point. When I say each component SK links marginal conditionals, what I really mean is the following. So let's consider the kth component of this mapping. So somewhere here in the middle here. And let's consider the one-dimensional function that comes from fixing x1 through xk minus 1 and letting the only letting the input argument effectively be xi. And let me consider the one-dimensional function that maps xc to this quantity here. Now you can show Now, you can show pretty straightforwardly that this function pushes forward this marginal conditional, the kth marginal conditional, to a standard Gaussian, if I've chosen eta to be a standard Gaussian. So I'm assuming here that eta is going to be n0i, okay, for the most part throughout the talk. If whatever other, you know, as long as eta is a product of marginals, then this would be true for whatever the kth marginal of eta is, but then assume it's going to be standard Gaussian. Going to be standard Gaussian. So, this basically gives me a way of characterizing the marginal conditional. For instance, if I wanted to draw samples from that marginal conditional, all I need to do is invert this one-dimensional function at samples that I draw from the standard Gaussian. So if I draw samples of this, then essentially invert this function, and I'm going to get a sample of xk condition on x1 through k minus 1. Okay, so that's quite good and quite useful. Now, I want to give kind of the more Now I want to give kind of the more general version of this and I'm going to bring it back to likelihood free inference. So this slide is essentially kind of going to be the case idea that we're going to use throughout the talk. So now suppose I have parameters n-dimensional and observations n-dimensional and a joint prior model on y and x. So consider, for instance, I maybe have a prior on x, maybe I have a conditional distribution of y given x, whatever it is, I just suppose I have some. Whatever it is, I just suppose I have some kind of joint prior model. It can be hierarchy in here, I could break the parameters into different pieces, whatever it is. Let me now say we construct a Rosenblatt map, a triangular map that pushes forward this joint distribution on the data y and the parameters x to a Gaussian, a standard Gaussian of the equivalent size. So these are all full-dimensional transformations. And let me choose a particular ordering so that the y variables are on top. Y variables are on top and the x variables are on bottom, and the whole thing is triangular. And if it's triangular, then it's certainly block triangular. And looking at the map in this block-structured way, you can, in focusing on this last component, there's actually two properties that I want to highlight. First is this, is that Sx, the bottom block of the map, pushes forward samples from the joint prior on y and x to an n-dimensional standard Gaussian. And this is just a consequence of the fact that this S pushes forward. This pushes forward the full joint prior to a full n plus n dimensional standard adjustment. Okay, so this is just a sort of a marginal event, that's fine. But then the same kind of conditioning argument applies, which is that if I take this component of the map and I fix y to some y star and consider now it to be a function only of the last argument, the last block argument, then this block map pushes forward the posterior x conditioned on y taking some value y star, again to a standard. Star again to standard Gaussian. Okay, so this is just a block version of the conditioning that I mentioned earlier. And this, though, immediately gives me a way sort of several interesting things that are useful for characterizing the conditional. The first is that I can approximate the conditional density by taking the standard Gaussian of n dimensions and pulling it back by this map. The superscript chart means a pullback. So essentially means applying S inverse X to this distribution. This distribution. Okay, so that gives me an approximation of the density. And what I'm going to show you in a second is the idea that maybe I build this map without knowing this density. But in building this map, I will have estimated the joint density and these particular conditions. So that's the sort of the first property. If I approximate the density, then also I can sample from the conditional distribution just by block version of what I mentioned before, which is to say, here's this map Sx. If I set it equal to X, If I set it equal to xi, where xi is drawn from a standard Gaussian, then I simply need to invert this map, and it's a triangular map, so I can invert it for xi. So having fixed y star, draw xi and invert this mapping for xi. So that's just all kind of using this bottom map here alone. And then the third version of this that I want to point out is to say, okay, this map here pushes the The conditional x given y star to standard Gaussian, that's well and good. But what I could do is rather than starting with samples from a standard Gaussian, what if I just started with samples from the joint distribution of y and x? And suppose I want a way of transforming those samples to samples from a particular conditional of that joint. Particular conditional again means x condition on a particular value r star. Well, what I can do is imagine, okay, well, let's take the first, let's take Sx, which in its entirety. Take Sx, which in its entirety pushes forward this joint distribution to an n-dimensional standard Gaussian. So that takes me, so essentially, by applying this map at this stage, I have an n-dimensional standard Gaussian. And then let me invert that Sx for a given Y star and take samples on the n-dimensional standard Gaussian, and I should again get samples from this posterior. So this is a different route. It's kind of a more circuitous route, if you will. So it's not clear why you'd want to do this. I haven't told you that yet. But in general, though, you could see. Told you that yet, but in general, though, you could say that if the map is exact, all of these are ways of generating samples from the conditional. I could essentially take samples from pi xy and transform them to samples from the conditional, or I could, as I just did here, take samples from an n-dimensional Gaussian and again transform them to standard, samples from a standard conditional, or from the desired conditional. Okay, so these are all just sort of some building block ideas, and now we want to use them and understand them better, and so on and so forth. Okay, but And so on and so forth. But the takeaways are by building the map in this particular triangular structure, we can extract a conditional density estimate, and we can sample from the conditional in two different ways. The sort of single map way that I've described here and the composed map way that I've described here. Okay. Any questions on this so far? So, okay. So, okay, so this is all just kind of saying if we had such a triangular map, what you could do with it. Now, I want to start to get into how you actually construct such a map. One natural, there's different ways of doing it. The formulation I want to focus on today is the idea of maps from samples. So suppose I have a sample from some target distribution pi. Let pi be the joint distribution of y and x, just any arbitrary target. So the point is, if I want to find maps from such a target, and I maps from such a target and I enforce that s is a triangular is a triangular mapping. I can actually find each component function of s k separately and also if I pick the reference distribution to be something that is log concave for instance a standard Gaussian as I'm writing here then this problem is convex in S. So what I want I want to do is for instance let me take the KL divergence between my target distribution pi and the pullback of the Gaussian. And the pullback of the Gaussian, and I want like these things to be as close as possible. So I want to drive that KL divergence to zero. I want to minimize over some class of maps S. Essentially, that becomes this problem here. When I plug in the standard Gaussian density for eta, I get this quadratic term here. I get this log of the Jacobian determinant. And this problem has separated across map components. So for each k, I have a different sk that I want to find, but these problems completely separate. And this is convex and SK. And this is convex in SK. And of course, this expectation under pi, I don't have this in close form, but I'm going to approximate it using IID samples from pi. And essentially, then you can think about this as maximum likelihood estimation. I have a bunch of samples from pi, and I'm going to manipulate my model, S pullback eta, by changing the map so that I maximize the log density, the sums of the log densities of all the samples from pi under this model, S pullback eta. Okay, and that's what this becomes here. So this is just maximum likelihood estimation. This becomes here. So, this is just maximum likelihood estimation and the well-known equivalence for KL minimization in this particular direction. But my point is that this problem is actually eminently tractable. This is actually a very nice problem to solve. It's convex, it's separable, and a lot of nice things work out like that. Okay. So that's the setup here. What I haven't yet told you, and what I will talk about, but I want to defer talking about a little bit is, okay, how do I parametrize maps SK that are, as I've really That are, as I've written here, increasing functions of their last component, which is what I need to get monotonicity and invertibility and all those things. I'll come to that in a little bit. But for now, we can just think of the problem at least in more abstract sense. It's convex in SK. Once you start to parameterize, you might start to break some of the structure, and we'll talk about that. But notionally, given a sample, given some way of describing triangular maps, I could go off and find these things. Okay, so with that kind of setup, this is essentially. With that kind of setup, this is essentially, you know, if you take away one thing from the talk, it's just that you can use triangular maps for these things. I want to give you two examples. And then I want to come back to some of these underlying issues of how do you solve the optimization problem? How do you parameterize the maps? Why would we compose maps as opposed to using single maps? I want to get into some of the kind of questions under the hood. But first, I want to start with some examples. So the first example I'll give you, this is essentially parameter inference. Essentially, parameter inference with a um with a in a stochastic PDE. And this is joint work with a bunch of people. This is joint work with Lang Hao Cao, Josh Chen, UT Austin, Tang Chen, who's on the line, UT Austin, Omar Gatas, and Tims Leoden, and also Ricardo Bapista, my student on this talk as well. And the motivation here is kind of a physical motivation. Essentially, it's phase field modeling. We have a PDE model that models the separation patterns in an. Patterns in an added manufacturing process, essentially something sort of die block called polymamel. So it just relates to added manufacturing. And you could think of this as basically describing the evolution of some, a loose way to think about it is the evolution of some film on a substrate. And the property, the structural property of the substrate essentially affect how the film evolves. And this is described by a phase field model, where essentially there's kind of a double well potential in the phase field that tries to induce separation. Separation. Now, the point about this model, I don't want to get into the details of the model too much, but the point about this model is that if I have different initial states, then the PDE runs to it evolves towards some equilibrium state, but the equilibrium state depends on the initial state. Initial state is essentially uncontrollable. It's essentially random. So it's actually a high-dimensional random field. And for different realizations of the random field, I'm going to get different equilibrium states. So these are essentially you as time goes. Essentially, u as time goes to infinity in this equation, beginning with different initial conditions that I'm describing by Z, all for the same set of model parameters. So this is now stochastic model, because essentially, if I fix these model parameters, which appear here in this PDE, I can then hold distribution over final states, which is induced by this distribution over initial states. Now, under that setting, I would like to learn these model parameters so that I can describe the system better. But this is an example of a But this is an example of a stochastic forward model. I'll just comment briefly on, like, what, you know, another whole interesting question is: well, how do you extract information from these kinds of image data? And for that, we've been looking at different energy functionals, different kinds of observables that essentially are integrals over the image and maybe the energy, maybe something that like sort of normed gradients. So, how much interface is there, how much non-local interaction is there, you know, things like that. A T V term is kind of similar. In any case, that's kind of a different distance, but the idea is that I have. Kind of a different distance, but the idea is that I have a couple different observables that I can extract from any one of these images. But the distribution of these observables, given a particular value of the model parameters, is not degenerate because the model is effectively stochastic because of this uncertain initial condition. So, you know, of course, one way to think about this is that, well, I just need to do inference over a larger space and marginalize out this uncertain initial condition. But that becomes quite intractable. Let me sort of give you the example. That becomes quite intractable. Let me sort of give you the example. So, you know, one way to think about this: what I'm ultimately interested in is these parameters x conditioned on the observations y star, which are particular values of these observables. And it's proportional to a likelihood and a prior on this side. But what is this likelihood? This likelihood essentially takes the likelihood given a particular initial condition and marginalizes over the distribution over those initial conditions, this auxiliary variable z. So I could go to the trouble of calculating this for every single mu x. But that actually quickly becomes intractable. So, what we can do, what's sort of readily at hand in this kind of problem, is the ability to simulate from the Ford model. Let me draw a sample of the parameters from the prior. Let me draw a sample initial condition. Let me run the system to equilibrium and let me extract the observables, which is essentially sampling from this model. So I can generate samples of parameters and observables arbitrarily. You know, not a zillion of them, but I can generate a bunch of them just by solving this PDE with stochastic. This PDE with stochastic initial conditions. Now, what does the distributions look like? Essentially, well, if I, you know, if I have samples, marginalization is easy. Marginalization just means ignoring the components of the samples that you don't care about. So what we want to do is then essentially learn this posterior using only marginal samples of x and y. And x and y have a complicated distribution, and it's essentially a marginal of this joint distribution that I do know how to draw samples from. So the idea is. Draw samples from. So the idea is then: let's draw samples from the joint. Let's cover up our I and not look at Z at all. And then we have only samples of X and Y, train a map, learn a map from those samples, and now extract conditionals. And this is just under different conditions of running the forward model with certain kinds of blurring. The Q's are the same as the Y's. These are distributions of the observables. These are the parameters. And you can see they have a rather complicated joint distribution under different conditions. And the point of all this. And the point of all this is: okay, now, having done this, I just learn a triangular map in the appropriate structure. And what you can see is that the posteriors that you get are, you know, in general, quite non-Gaussian. These are prior samples. These are the posteriors given different observations. And here I'm conditioning the posterior on two of the parameters, epsilon and sigma. The posterior on two parameters conditioned on four of these energy-based observables. And we get something that's very concentrated in this case. But having built the map, it's essentially. But having built the map, it's essentially trivial to extract these posteriors and posterior samples and conditional density estimates. And as a side note, I'll mention that because you can get conditional density estimates, you can also do things like compute expected information gain from prior to posterior. And you can play games like, you know, what's expected information gain in parameter M given different combinations of observables. And you can figure out, you know, which observables are most informative for certain parameters and so on and so forth, all in the context. On and so forth, all in the context of this stochastic model. I won't get into that too much. That's one application. The other application I want to highlight, and this is actually where a lot of this work started, is data assimilation and filtering. So this might seem entirely different in character than the stochastic PD model, but under the hood, the inference challenge is actually very much the same. So this is now a sequential inference problem where certain density functions of interest cannot be evaluated readily or in close form. Readily or in close form. So, what it does mean: so, this isn't a picture from NCAR, this is an ensemble weather forecasting exercise. It's a more glorious application than what I'll show you. But the idea, if I can take that kind of, you know, sequential problem and abstract it, essentially it's a Bayesian filtering problem, where now the goal is to say, I have a state of my system, of my dynamical system Z. I've discretized time, so the state evolves in time. The subscript here indicates time. I have observations, perhaps incomplete, indirect observations. Perhaps incomplete, indirect observations at any given time. And I would like to condition the state at any given time on all the previous observations. So, time k, I'd like to get the distribution of zk conditioned on y0 through k. When a new piece of data comes along and I've advanced the system to new time, time k plus 1, then I'd like to learn the distribution of the state z k plus 1 condition on y0 through k plus 1. So these are marginals of the full Bayesian solution, which would essentially be looking at the joint. Bayesian solution, which would essentially be looking at the joint distribution of all the states over all times, condition, all the data. But let's just focus on the filtering problem here. Okay, so this is a classical problem, and this is a classical problem in particular in geophysical data simulation. And in geophysical data simulation, very often one has to deal with certain constraints. The states are typically high-dimensional. The dynamics that get me from ZK to ZK plus one are nonlinear and in particular often encoded. In particular, often encoded by some kind of black box model. So, given samples or realizations of the state of my weather model at time k, all I can do is drive that state realization forward for some amount of time to get a realization, a corresponding realization at time k plus one. I couldn't tell you the density. In many cases, the density could be a degenerate or maybe it's a chaotic model, so it's not very tractable. All I can do is essentially get forecast samples, draws from ZK. Draws from Zk plus one given Zk. Also, due to computational constraints, I might have relatively limited ensemble sizes, and the observations of mass state may be sparse and local in time. Okay, so those are kind of the typical constraints. But these are the constraints that actually lead us into the likelihood-free inference setting. Now, what's the workhorse algorithm in this setting? It's basically the ensemble common filter and its many, many variants, ensemble transform filters, things like that. Transform filters, things like that. And these are actually fascinating algorithms in their own right. They're remarkably effective, sort of vexingly effective. But in a nutshell, what do they do? They have two steps, a forecast step and an analysis step. The forecast step says, suppose I have samples of my state at time k minus one, conditioned on the data up until that time. So these are, say, samples from my filtering distribution at time k minus one. And by this, I'm And by this, I'm just using these little circles to denote samples from some distribution. I'll run my dynamics forward as the first step to give me samples from the forecast. Sorry, this is the filtering. I run the dynamics to get samples from the forecast. So this is essentially samples of ZK conditioned on Y, Z, 0 through K minus 1. So I have not yet conditioned on the new data. And then the new observation comes along, and somehow I want to condition on it. And so in the dashed blocks here, this is the Bayesian inference step. This is the Bayesian inference step. This is the likelihood-free inference step of interest. And what the ensemble-common filter does is said, okay, this problem in general is difficult to solve. If everything were Gaussian, then I would know exactly how to go from the Gaussian prior to the Gaussian posterior, from the Gaussian forecast to the Gaussian analysis distribution. And I could encode that update using an affine transformation. So, what the ENKF does is essentially estimate sort of covariances and learn an affine transformation that is actually not consistent. Transformation that is actually not consistent as when this distribution is not Gaussian, but works remarkably well for certain purposes, like estimating the mean and things like that. But it moves the samples with an affine transformation and doesn't use weights, doesn't use resampling, and thus it's very robust. So, okay, looking at this problem and thinking about what happens in this dashbox from a certain perspective, from the transport perspective, sort of led us to this whole idea of, you know, can we generalize this affine transformation to something? This affine transformation to something that is actually ultimately consistent or that could be made consistent with enough samples as the number of samples goes to infinity, and as the transformation becomes rich enough, can we actually come up with something that truly does converge to the real Bayesian solution while preserving a lot of the scalability and robustness of things like the ENKF? Now, this analysis step, if I abstract it, is exactly the kind of problem, the assimilation step or analysis step, exactly the kind of problem we looked at before. I have sampled. Looked at before. I have samples from a prior, which is the forecast distribution. I have the ability to generate, say, observation, generate realizations of the data given an x. Maybe I do have this density in close form in some settings. But what I'd like to characterize is the posterior, which is essentially the distribution of the state at time k conditioned on all the observations, including y star, which arrived at time k. So this is an inference. So, this is an inference problem where I want to sample the posterior given only typically a few prior samples, x1 through xn, and the ability to simulate data, yi condition on each xi. That's exactly the likelihood for inference problem we were looking at before. So essentially, I have samples from the prior, I can simulate this, and I'd like to get samples that go here. Okay, so that's the setting. So, what you can think of is now just taking the legs. One you can think of is now just taking the likelihood for inference idea that I described using transport and slotting that into the analysis step of the EMKF. And essentially, it looks like this. I compute the forecast ensemble from a prior step applying the dynamics. I can generate samples from the joint distribution of y and x, essentially by for every forecast ensemble member sampling a yi. And now I'm going to build an estimator of this map, t that I described earlier, this composed map, and essentially now take samples from the joint distribution. Samples from the joint distribution and apply the map t that I've estimated, t hat, and get samples from the analysis distribution from the posterior. So this corresponds in particular to the composed map approach that I described, the third approach that I described on that key slide a little while back. Okay, let me describe in more detail what that is. So what is this composed map? So essentially, given samples from the joint, we're going to build this S. I'm going to essentially use the maximum likelihood estimate of S. I'm going to essentially use the maximum likelihood estimate of S. I call it S hat. And T hat is what I get when I compose those two maps together. So Sx, this should take samples from the joint, transform them to standard Gaussian if it were perfect. Otherwise, you get something else. And then I take the standard Gaussian samples and I invert this map for a fixed y star, and I get samples from the posterior. Okay, so this is just the earlier maps from samples approach. And the third path that I described, where we're using this composed map estimator. Now, why do we do that? Now, why do we do this? Well, this is actually exactly what the ENKF does if I had restricted the map S to be linear. So if I had restricted this map here to have linear SX, linear in Y and X both, then the really interesting thing is that you recover the perturbed observation form of the ENKF, the stochastic ENKF. And there's variants of this that can give you a square root version, but this is really where things coincide. So the ENKF is doing exactly this, and then this is sort of. NCAF is doing exactly this, and then this is sort of a fortuitous thing, but this actually, as I'll show you later, is one of the reasons that the composed map approach, the composed map approach is embedded in the ANCAP, and that actually turns out to be very effective, more effective than the other approach that I described. But now, by thinking about these maps as sort of a broader choice, we actually have much more freedom than what we had at our disposal if we were only restricted to athline maps in the ENKF. Restricted to athline maps in the ENKF. And there are various ways of doing this. I'll talk more about map parameterizations in a second, but I do want to point out that for very certain, you know, a simple class of map parameterizations that are essentially some linear basis expansion of G and then affine in XK or linear in XK, I can actually solve this problem without even using optimization. I can write the solution in close form. Now, how I choose this approximation space, which is essentially here, this is a space of triangular maps. Space of triangular maps, H just denotes some sort of discretization or some finite approximation. There's now a very interesting kind of bias-variance trade-off. I could use richer parameterizations for the triangular maps and thus get closer to the true Bayesian solution in general non-Gaussian settings, but of course I might have higher variance. So now we have something that we need to navigate and trade off, and that's something I'll talk about in a little bit. But any questions on just the setup here before I give you some examples of how this works in filtering? I give you some examples of how this works in filtering. I have one, Joseph. Yeah. Probably I should have asked earlier, but so we are always guaranteed to so there the transport plan always exists, I believe? Yes, yeah. Indeed, the map always exists. Yeah, the triangular map always exists. But you assume sort of that the intrinsic dimensionality of the data is equal to the prior distribution, right? Distribution, right? Yeah, so here I'm not reducing dimension at all, but the map that I'm looking for is on the joint space of data and observation. So it's essentially on the sum of those dimensions. So the parameters can be of whatever dimension. The data can be of a different dimension. And I'm looking for triangular maps on the space that's the sum of those two dimensions. Sure, but if the data lived on a manifold, Data lived on a manifold, this would still work? Yeah, so it would. So, in some sense, that joint distribution would be degenerate. And yeah, and so that's something we're not, that's, I think that's both a blessing or that's something we're not taking advantage of yet. Because in principle, it'd say, well, yeah, then a full-dimensional kind of approximation is wasting some degrees of freedom. And one would like to detect that manifold and proceed from there. But yeah, it's a good question. So that is actually related to some things. So, that is actually related to some things we're thinking about doing and have some ideas on, but that's for now. We're not using that structure. If you have an answer for this problem, how to find out, even if the data is on a manifold, I'd be very interested. Yeah, if it's linear, I could help you. If it's non-linear, you know, it's hard. Anyway, thanks. Yeah. So, yeah. Okay. So, yeah, so let me give you some examples. Okay. So. You some examples. Okay, so let's start off with the problem everyone likes to start off with, Lorenz 63. And okay, here we're just using a very simple map parameterization that's actually separable. So it's basically a sum of nonlinear functions, linear plus maybe so we have some RBFs and some sigmoids in every single component. So there's no xi times xj terms here. Everything is just separable. But that's a very easy parameterization. You can solve it. You can find the coefficients very easily in close form. Efficiency very easily in close form. And here, just a bit of performance. So, essentially, here I'm looking at the root mean square error, which is essentially a measure of the tracking performance of the filter. It's not a measure of the full fidelity of the True Bayesian solution, but it's kind of a good practical measurement of at least the mean estimate, the quality of the mean estimate. And what you see here is a pattern of computational cost that shows up in many other places. As you increase the ensemble size, and as I play with the map parameterization, richer map parameters. Richer map parametrizations become more useful as the ensemble size goes up. So if I restrict myself to linear maps, essentially I recover the performance of the ENKF and you get a plateau as the ensemble size goes up, because now no matter how big the ensemble gets, you're limited in performance by the approximation class of maps that you chose. Okay. Now, if you use a richer class of maps, then you can the plateau moves down. So this is linear just plus one little bump, radio-based. Linear just plus one little bump radial basis function in each component, linear plus two. So these are not that many more coefficients, but you essentially are moving this plateau down. And this dash-dotted line here is the performance of a particle filter with bazillions of particles, as bazillions being, I think, millions. So this is about as good as you can get. As good as you can get. And you can see we sort of, we approach that, we don't reach it perfectly. We're limited. In this particular simple parameterization, we don't even include cross terms, but you can achieve it. However, if your ensemble size is very low, then Ensemble size is very low, then you're actually better off using simple map parameterizations. And as the ensemble size goes up, you can use richer and richer parametrizations. So there's a trade-off here, and this is really the bias variance trade-off. And I'll come in a little bit, hopefully there's time to discuss how we can navigate this trade-off in a more automated way. You can also look at fidelity to the true Bayesian solution in this problem, because with millions of particles, you can actually compute the true Bayesian solution. So this is the average, one glimpse of that, this is the average error in the That this is the average error in the posterior covariance measured in Frobenius norm as a function of ensemble size for different map parameterizations, and you can see you can do better in terms of fidelity to the true Bayesian solution as you enrich the space of maps in which you look. So you can beat the EMKF by looking in broader map spaces, but you need a sufficient amount of particles to be able to detect that non-Gaussian structure that the map is taking advantage of. You can do this in higher dimensions. I won't go Do this in higher dimensions. I won't dwell on this too much. This is essentially the fact that in higher dimensions, the analog to localization, which is essential for EM chaos to work in high dimensions, is to sparsify this map. And there's a good rationale for sparsifying these maps in that there's actually a precise link between the sparsity of these triangular mappings and conditional independence in the target in pi. So, by sparsifying this map, you're essentially equivalently imposing some kind of conditional independence assumption. And in a little bit, I'll talk about how you do that. Assumption. And in a little bit, I'll talk about how you detect that automatically. But with this sparsification, we can go to things like a Lorenz 96. And this is kind of a hard test case configuration of Lorenz 96, where there's a relatively large time between observations. So things become relatively non-Gaussian. And again, you see the same kind of pattern of computational cost. This is RMSE. And you can actually beat the ENKF, you can reduce the RMSE by about 25%, which in this hard test case is not trivial. This is actually something we're very pleased about. Trivial. This is actually something we're very pleased by. And you can also look at, it's hard, you can't compute the full Bayesian solution in this setting, but you can look at other metrics of quality. For instance, some scoring metrics, this is a continuous rank probability score, and in some sense, a measure of the quality of the posterior, under overestimating uncertainty. And you can see we again do better for sufficiently large ensemble size as the maps as you start to look over richer maps. Okay, how do we do this? We'll be imposed some sparsity with a five-way interaction model. Some sparsity with a five-way interaction model, and here we're still using a very simple, separable, non-linear parameterization of each map component. Okay, so and here's a pretty picture. You can see Lorenz 96 has these kind of left-running chaotic waves, and you can track it pretty well. So that's just, you know, some eye candy. So now a couple remarks. Using this idea of likelihood-free inference and recognizing that the analysis or the assimilation step of the EMKF is just an example of that. Of the ENKF is just an example of that. We can generalize, we've generalized the ENKF to become something non-linear. And this has the nice features of the ENKF in that you are moving ensemble members using a transport mapping. You don't introduce weights and you don't have the problems of degeneracy of particle filters and things like that. Now, the idea behind it is that we're going to essentially learn non-Gaussian features of the forecast or of the joint forecast data distribution using nonlinear mappings. And they can be found in a simple way when you parameterize. Found in a simple way when you parameterize them simply. And essentially, picking that basis and also imposing sparsity in that mapping provides certain kinds of regularization. Now, within this kind of setting, there's a lot of questions. So, in principle, this filter is consistent as I enrich the approximation space for the maps and as the ensemble size goes to infinity. But how should those things increase together? And for any fixed ensemble size, how do I pick a good approximation space for the maps? Good approximation space for the maps. Also, we're just using this kind of very simple maximum likelihood estimator. You know, are there better things to do, especially if you think there's sparsity? Can we say something about finite sample properties? Can we say something about properties of the optimization problems in general? And then also, you know, how do you relate the map structure to the underlying dynamics and observations? And like Lars was asking, like, what, you know, do the data level events unmanifold? I mean, so there's a lot of interesting questions underneath this method. When we announce Toronto, start to take each piece of it and To want to start to take each piece of it and dig into how to make some of these choices in a more automated way. So, in my remaining time, which I realize is somewhat limited, I want to just focus on two of these underlying questions. One is how do I parameterize maps? In particular, how do I choose various ways of parameterizing maps and perhaps tune them to the ensemble size or the size of my sample from the distribution in question? And second, you know, what's going on with this composed map that seems to be at the heart of ENKF, and yet it seems a more KF and yet it seems a more circuitous route. Why not just use single maps and what are the pros and cons of that? Okay, so hopefully I'll get into this, have enough time to get into this. So how do I parameterize maps? One thing I want to point out is, you know, the machine learning way of parameterizing maps is essentially to use neural networks and to build what's called normalizing flows or autoregressive flows. So normalizing flows are an industry unto themselves in machine learning and it turns out they are To themselves in machine learning, and it turns out they're just transport maps. And many normalizing or auto-aggressive flows are actually just built from very special cases of triangular maps and their compositions. So here I've listed a couple popular architectures for normalizing flows and what do they look like. Essentially, you could think about these as writing the k-th component of that triangular map in a very particular form. So NICE is one of the earliest ones. Essentially, it is a xk on its own, and then some nonlinear function parameter. Some nonlinear function, parametrized by neural network of x. Yeah, that should be, well, index for x minus i minus k. Yeah, this index makes no sense. Inverse autogressive flow takes the same thing and puts in an affine scaling. This ensures that it's positive. Summit squares polynomial flow is a way of ensuring that things are positive. They have positive roots for the next k. But these are all just particular choices of triangular mappings. And many of them are. Of triangular mappings, and many of them are not sort of they don't cover all possible triangular mappings. But the way that then people get expressivity is by taking these maps and composing them with essentially permutations or masks and then more maps and permutations and more and masks and so on and so forth. And these give you very expressive flows. But of course, the optimization problems that you come up with are optimization problems for these coefficients, which are generally neural networks. So you learn a bunch of weights of a neural network. And these optimization problems are just. Network and these optimization problems are just brute force. Sort of there's ad hoc structural choices, and there's a whole bunch of more structural choices I haven't even touched upon here. And the optimization problems or the training problems are challenging, and they have all the problems of kind of neural network learning. So, you know, we're here what we've been thinking about this problem, we're taking a sort of decidedly maybe non-ML approach, even though this is, I think, a structure that's very often used in ML. But the motivation is actually we would really like to do things with a relatively small ensemble size. With relatively small ensemble size, motivated by these data simulation problems, where it's a hard, you know, hard to press practitioners to compute 60 ensemble numbers rather than 10, much less 100,000. So we really want to think about doing things in a sample efficient way and also making this optimization problem very tractable and very easy and very reliable to solve. So the challenge here is how do you satisfy the montanistic constraint for this map? And essentially, I want this mapping SK to be monotone in the case. Monotone in the kth variable so that I get triangular maps that are invertible and so on and so forth. Now, you could do this by just enforcing this at samples, you could use the specialized constructions that I described earlier. But what we've been looking at here is kind of a more general construction that says, given any kind of unconstrained function f, if I put it through this transformation, which I'm calling curly or a rectifying transformation, which all rests on the choice of any sort of bijective and smooth G, essentially. Objective and smooth g. Essentially, what I do, I take f, I take its kth derivative, I apply g to that derivative to ensure I get something positive in the integrand here, and then I integrate that with respect to xk. So I recover monotonicity in xk, essentially guarantee that the result of the rectification of f is a strictly monotone function in xk. Then I get a nice way of parameterizing maps just by parametrizing some class of f. Now, what Okay, now what does this do? Here's kind of a picture I think is pretty cool that Ricardo put together. Essentially, I can feed in an f that's shown in blue here, it's not monotone, and if I rectify it, I get this monotone function here on the right. Essentially, here's f and red is the derivative of f. I apply g to that, so it positivizes it, and then I integrate it, and then I get the rectified f over here. Okay, and this is for a particular choice of g, this kind of soft plus function here. Okay, so that's well and good. Um, what you can do. Well and good. What you can do, so now suppose I've essentially converted this optimization problem where first I'm looking for maps that are monitored in indicator variable to maps that are essentially over some unconstrained set of functions f, where I've taken this objective j and I've composed it with that rectification operator. Now, what happens to this? By picking this rectification operator in this way, essentially, okay, what do you do? I mean, maybe it's easiest to think about this in finite dimensions. I have a convex function j. j is convex in J is convex in maps S, convex in its arguments. But if I kind of have a function and I compose it with, say, a smooth invertible function, I might lose convexity. But I will preserve all the minimum. I still have, I essentially have, if I have a strictly convex function, I'll still end up with a unique global minimizer to the composed function. So the fixed points get mapped and everything turns out to be nice. And essentially, you can show that the same thing happens here under appropriate assumptions on the target. Here, under appropriate assumptions on the target, and these assumptions might be a bit restrictive. Um, you're essentially saying it's bounded by standard Gaussian. I think they can be generalized, but essentially, under these assumptions, at least in this special case for this kind of choice of G, you can get an optimization problem that has a unique global minimizer, even though in general it's not convex, but it's much easier to solve. And to give you a sense of what this looks like, here is basically the optimization objective. What we did is basically begin with random initial maps, run a gradient-based optimizer. Run a gradient-based optimizer, get to a critical point, and then look at segments in the function space that connect the starting point and the critical point, and look at the objective along the segments. And with an appropriate choice of rectifier, you get, first of all, the objective, the critical points are all, you end up at the same critical point, and everything in between is smooth. For other choices of rectifier, this actually corresponds to what's been proposed in the ML literature under sum of squares polynomial flow. You get this very ugly objective. You get this very ugly objective that has a lot of local minima, and you don't end up that, you know, you end up somewhere. You get stuck in different places. Okay, so the point of this is you get now an optimization problem that's much easier to solve. I realize I'm running late on time. Is it okay if I take just a couple more minutes or end at like 2.50? Yep, sure. Okay, sorry about that. So, okay, the idea is now I have this optimization problem to solve and now I can embed this in an adaptive enrichment procedure. An adaptive enrichment procedure in which I now want to look for f in some space, but I'm going to enlarge the search space for f based on some kind of error indicator. And essentially, the error indicator that I look at is this is really analogous to what people do with adaptive sparse grids or adaptive sparse grade polynomial approximation. We're searching in some, here I'm parameterizing f using some a multi-index. We're using particular Hermit functions because that essentially decays away in the tail, but parameterizing these things with a multi-index and looking in the These things to a multi-index and looking in the sort of set of active multi-indices and looking on the reduced margin and deciding to enrich the multi-indices according to whether the most recently added indexes sort of pushes the objective a lot in some way. So essentially, it's a greedy procedure. We're going to look for new features, if you will, new basis functions in this representation of f in the reduced margin of this multi-index set. And with that, you can actually represent some pretty interesting and complicated density. Um, interesting and complicated densities. So, here's some complicated densities on top. And what we call here the adaptive transform map algorithm basically looks for functions in the search space for f and lets you complicate, you know, represent some pretty complicated looking things. Okay, just using Hermite functions. No neural networks here, but it still works pretty well. And essentially, yeah, we essentially kind of look for some error indicator that's kind of based on the gradient of the objective. Of the objective, and the practical thing one needs to choose is when to stop, you know, how many features, how many basis functions, and you can do that via cross-validation. But what this ends up with is basically a balance of bias and variance so that for any given sample size, you have a certain stopping, a certain number of features. And as the number of training samples goes up, you can enrich the approximation space, and you can essentially follow along this bottom margin here. So the adaptive algorithm ATM is represented by this. ATM is represented by this blue-shaded area here. Okay, so you can do that, and you can apply this now back to the Lorentz 96 data. Just to kind of give you a sense of the, yeah, I guess the other point I want to make is that the maps you find, the searching for F, actually has some interpretability because essentially you're searching for S's, and you can actually learn the sparsity of S via this adaptive ingredient algorithm. And in the Lorentz 96 data, the Lorentz 96 state is distributed on a ring. So we actually expect. Ring. So we actually expect S to be relatively banded, except it's periodic. So you expect some more sort of entries down here. And indeed, you recover that just from data. Okay, with 316 data points here. So implicitly, you can start to discover some of the conditional independent structure of the target. And what we have is a semi-parametric method that gradually increases the complexity of the maps that you search for with the size of the ensemble. Okay, so that's one thing. I guess the other thing I'll point out is that. I guess the other thing I'll point out, I'm a little bit low on time, is just so I'll make this point very quickly. All the calculations I described earlier, I think, as I said from the beginning, are just instances of likelihood for inference, whether it was the stochastic PDE problem, whether it was the ensemble filter. And the central idea is that we only need to draw samples from the joint distribution of state and observations to construct S hat or T hat and thus to draw samples from the conditional. But there are two ways of, two approaches for posterior sampling that are embedded in this problem. That are embedded in this problem. And these are the two things that I mentioned on that slide when I first described conditional sampling. I could, for instance, just draw samples from a standard Gaussian and apply this map, this is the inverse of that bottom part of the triangular map, to the standard Gaussian samples and get samples from the target. And if the target were exact, this would be a perfectly fine thing to do. This would be exact. If the map were exact, this would be exact. I can also do the more circuitous route that I described, the compose map thing, which is what That described the composed map thing, which is what our ENKF and its generalizations do. They correspond to computing the map S, but then not drawing fresh Gaussian samples down here. Forget about this. They involve drawing, taking the same samples or an independent set of samples from the joint distribution of state and observations and applying this composed map, which we call T, to those samples and transforming those to the posterior. But these are two different things one might do. And if everything is exact, Things one might do. And if everything is exact, there's no difference. If things are not exact, then they're actually quite, quite different. And here's just an example. The details of this aren't too important, but this is essentially the single map, and this is the composed map. Single map on the left, composed map on the right. This is different, you know, playing with different regularization methods, so the details aren't too important here. But what's important is that the KL divergence in the conditional on the left is order 10 to the 15th at the top of this thing, and here it's order 10 to the 1. So there's a dramatic. Or 10 to the one. So there's a dramatic difference in error. Now, this is a very undersampled regime where this difference in error becomes most apparent. If you have enough samples, then the difference in error is much, much more modest. So what exactly is going on? Why are composed map effectives? So here's maybe the last thing I want to leave you with. Essentially, by using this composed map construction, you introduce degrees of freedom. And here is essentially kind of a proposition that explains, I think, partial. A proposition that explains, I think, partially, I think there's a lot left to explain, but essentially it's this. Suppose I have some map that takes the posterior of interest, x condition on y, for any y and pushes it forward to some reference eta. But eta is not left alone. It's not a standard Gaussian. Any marginal transformation of eta is okay because this marginal transformation of eta L cancels itself out in this composition. So for any L, no matter what, So, for any L, no matter what L is, this composed map will push forward pi yx exactly to the posterior. You can think if the map were exact, this would be the identity. Insofar as the map is not exact, I have this L sitting here. But the point is that essentially, what the map has to do is remove the y dependence from the joint. It can then leave the marginal in whatever form it likes, but it has to just remove the y dependence from the joint. So essentially, you could think of this random variable. You can think of this random variable z does not need to be standard Gaussian. So, with outside of this requirement, there's a lot of latitude. And what this then motivates, you know, so you can think about this idea of removing the y dependence from the joint, removing the data dependence from the joint, but leaving the marginal, whatever it is, is actually something that's been used in kind of, you know, linear, sort of linear bays and things like that in a relatively ad hoc way. You know, one thing is just, well, let's just take remove. Way, you know, one thing is this: well, let's just take remove the dependence from the mean on y. Um, and if I let the mean be a linear function of y, if I restricted that to be my mean, my onsets for the mean, I recover the E and K F. I could remove the dependence from the joint on y of the mean and the variance. And I could go much further. These are just very limited assumptions. So, what this actually motivates is now some ongoing work where the goal is not to take a joint distribution and map it to standard Gaussian, but the goal Or Gaussian. But the dual is to take a joint distribution and remove dependence from that joint distribution. And so this motivates different objectives, for instance, mutual information objectives as opposed to KL and so on and so forth. Okay, so I think I'm way out of time and I'll just conclude. The idea I want to convey is that likelihood-free inference, just using samples, has a lot of utility, whether it's in filtering, whether it's in inference and stochastic models, whether it's just inference in generative models generically. Generative models generically. The underlying tools to do this, I think, fundamentally use things like triangular mappings. But underneath this, there's a lot to do in terms of understanding how to do adaptive approximations, how to make the optimization problem tractable, and how to find Taylor objective. And there's a lot of theory that I think we're trying to develop and others are trying to develop that essentially understand how good of an approximation you can create as you enrich, function approximation results for these. You know, function approximation results for these triangular maps, analysis of the statistical properties of these estimators, and there's a lot of extensions. I'll just point out that these kinds of structures can be used to learn structure in continuous non-Gaussian graphical models. So, to learn that conditional infinite structure more formally, we have a recent paper on that. Understanding low-dimensional structure, and that goes as to large question.