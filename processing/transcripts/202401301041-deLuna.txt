A valid inference in high-dimensional settings. This is joint work with Nielo Formousavi, Jen Heckstrom, and Mohamed Gassenpur. Mohamed is finishing his PhD this year. Most of it is with the two first authors. uh authors but it will show out I guess so the that's this is our outline of this talk is based uh mainly on on a paper uh recently published in Statistical Science so the problem of interest or the issue of interest I'm going to talk about here is how to perform inference if you have been doing things with your data like model selection and fitting users models if Different machine learning algorithms, for instance. The context of interest is going to be missing outcome data. I will review the solutions that exist using semicrimetric theory, but I will talk also about their cost. And we have a proposal which is a kind of compromise between the solution that exists and the estimators that people often use in. estimators that people often use in practice which are not valid, and I will show some monte-colour experiments to illustrate these issues. Okay, the big context is that we have lots of data, as you know, in society. Typically, you have lots of background variables on the individual in an observational study. So, for instance, in Sweden. So for instance in Sweden we have databases including the whole Swedish population, about 10 million individuals, that we can follow through their life. And we have both health and socioeconomics variables. So it's of course very sensitive data, but we are allowed to have that kind of databases as we have in Umio for research. Enumeral for research purposes and give you need to have like ethical setting of this research, of course. With so many characteristics, typically we have like hundreds of thousands of characteristics for each individual. So you need to do some data-driven modeling choices. Even if you have some theory that tells you which variables you should have in your models, typically you don't have very precise theories. Don't have very precise theories, and you have lots of variables, and you will need to do some data-driven analysis. To be a bit more precise, I can just briefly tell you about one study we have done where we look at health outcome if you retire early. So, the population we used in this case was all those born. Was all those born in Sweden in 46 and 47 that had not retired at age 61. So they had to survive and be in the labor force at age 61. The outcome of, since we're interested in health, we are taking here as outcome the annual number of days that people spend in hospital at age 65, that is some years after. That is some years after an early retirement age. And the early retirement age in these studies is 62. The typical retirement age in Sweden is 65 years of age at this time, for these cohorts. So since not everyone is going to return at age sixty two, obviously we have a an outcome missing problem because we will problem because we will some of them will retire at sixty two and then we will be able to observe their health at age sixty five and some of them will retire later. Yes, please. So why do you choose a 62? Yes, it's just we could you could use 61, 63 is and we have done that and just is this is just to fix ideas. So it's it's before sixty five, because sixty five most people will retire at sixty five. So we're interested what happen if you retire earlier? Happen if you retire earlier than the typical age? So the normal retirement date is 65. Yes, so for these cohorts it's 65. If you have more than 60% of people retired yesterday, and it's both like a cultural thing, because people somehow think that that's the age issue return. But they're allowed to retire from 61 actually and get retirement pension back. Retirement pension, but of course, you would get less pension if you retire earlier. So, what we're going to assume here is that the typical assumptions that the decision to retire is random, conditional on a very large coverage notes. So hopefully makes this assumption realistic. And in particular, the control coverage will include hospitalization and labour market. And labor market history for the individuals. So the parameter of interest here, you can describe it in words as it's written here. So if you take a random sweep born in 46 and 47, what will be her average number of days in hospital at age 65 if she retires at age 62? That's the This is an expectation of an outcome. This is a bit loose notation so far. And to estimate that kind of parameter with observational data, you will typically need to fit nuisance models, typically the propensity score, or the probability to retire the CC2, even X, and Or expect an outcome model, expectation of the outcome given the coverage. And we do that with, since we have very many coveries and also time series, because we observe hospitalization through time, we observe labor markets variables through time. So we use a convolutional neural networks to estimate the nuisance models. And the question is. And the question is: Is it obvious how to perform inference if you do that? And the answer is there are solutions, but you need to do it carefully, yes? I'm curious, uh, what proportions of people retire at uh before age sixty two? Yeah, that's a very good question. Now I don't have it Yeah, that's a very good question. Now I don't have it in the top of my mind. But it's it's probably uh uh like around ten percent, ten percent plus minus five. I hope you didn't get too excited about this uh example because I'm not going back to it, it's just to fix ideas. If you're interested If you are interested, we have a paper. You can look at it. And I can tell you that the results of this study is that it doesn't matter for health if you retire at 62 or age. So it doesn't help you. You will get the same. The same health at 65. So, the main issue of interest is what you do if you want to do inference after data-driven model selection or equivalently after machine learning fits. And this is, I'm going to somehow review the literature on that. How can you do that without having valid inference? And I will talk about the cost of doing that correctly and Correctly, and that we might be want to do it in another way that we propose. So just to make you understand the problem and how it can go wrong, I take a very simple example. So, if I have a data generating mechanism, which is just a simple linear model, now I'm not interested in it. So, here I don't have any. interesting so here I don't have missing data but it's just to illustrate the problematic problem so if you have a linear regression you have two covariates called t and x and I'm interested in alpha that's a parameter of interest so so so this this parameter is a nuisance parameter I might want to estimate it or not I could think about the two-stage data analysis where I first Analysis where I first decide whether I'm including x in the model or not, and then I estimate my regression model with x or without with OLS. And if I take the first decision of using this rule here, that is I estimate the I include x if the estimator, the OLS estimator, beta hat, is somehow big enough. Big enough. And so I have a decision rule here which is taken so that if beta hat is close to zero, then I will discard x. And if beta hat is larger than, is large, is large, I will include x. And it needs because of this root log n here, it beta hat needs to be smaller and smaller for it to be excluded if with a larger samples. Larger samples. So then I given this decision, then I make, I run OLS and I take a usual confidence interval for alpha. So the properties of this two-step procedure is that the first step is a consistent model selection. You are doing it correctly, asymptotically. And the second step gives you an estimate. gives you an estimate an estimated uh uh uh alpha which is as efficient as maximum likelihood that that is if I just use this without this decision and it can even be better so if beta equals zero if the true beta is zero then doing this gives you a super efficient estimator in the sense that you get a better a lower bar a synthetic variance for alpha. Variance for alpha alpha. Of course, this sounds very good because you get my procedure is super efficient, but it's actually too good to be true. And this was illustrated in a paper by Lee Van Butcher. So, although for each fixed beta, the distribution is approximately normal. The distribution is approximately normal for big enough sample size. This big enough is going to depend on n. And that's going to create a problem. So the finished sample behavior is not going to be reflected by the point-wise asymptotic result. So you can show pointwise, given any beta, you can show that alpha hat is asymptotically normal, but this is not going to be very useful. And you can illustrate. And you can illustrate this with this figure. So here, this is the final sample density of root n alpha hat minus alpha. And if the true beta equals zero, let's take first, if the true beta is 0.5, a very large beta, then I get a finite sample density, which is very well approximated by the symptotic normal distribution. Normal distribution of the MLE, of the one-step estimator, MLE. If the true beta equals zero, then the finite sample distribution is going to be super efficient, so I have a better efficiency than the typically asymptotically normal MAD estimator. But the problem is that if beta is something in between here, for instance 0.21 or 0.25, then I would get something. Then I would get something which is here by mode of very and not at all well approximated by normal distribution. So this is this is so you have clearly a bias for if the true beta is 0.21 so the rules So, the root and bias of a naive two-step estimator goes to infinity or stay bounded if you have a if you choose a sequence of worst-case scenarios TGPs. So you can choose a sequence of worst-case scenarios, TGPs, and get. Now, this is uh one one one variable, one covariate, x. But it's not covariant. Yes. Yes. So the problem is already there in very low dimension. So the problem is that this two-step procedure that I have chosen to look at is not regular asymptotically linear. You can show that. And therefore, you won't have a uniform linear. You won't have a uniformly asymptotic distribution for this two-step estimator. So, what you want is to have really a regular asymptotic linear estimator if you want to avoid this problem of super efficiency. Of course, and thank you for the question. So, of course, X is just So of course, x is just one variable. So what, then I can just, to avoid this problem, I just include it in the model. I don't do any, I don't need to do this step. I just put it in the model. Why take it away? And use MLE, and then I have no problem. The problem comes from the fact that I wanted to first decide whether I have to include X or not. And of course, that works very well if I have a low-dimensional. If I have a low-dimensional, if I have few cover bits. I think this probably is not ideally countable. Yeah, but we come back. We go back to that. I will do it in another way if x is high-dimensional. What you do, for instance, is to use a lasso estimator time. But the lasso is not. Of them. But the lasso is not regular either. That's the problem with the lasso. Anyway, so I come back to the problems I was interested about. I have lots of covariates, and there I cannot just put all of them, for instance. Even if I don't have lots of coverts, I might want to I may want not to assume linearity in XL, so I may want to have f of X here, an infinite nuisance parameter of M. So if I go back to my problem of interest, that is missing data in the outcome. Data in the outcome. If I denote y1, it's my outcome of interest, but is observed only when t equals one. So my example was that I got retired at age 62. So I have a low dimension parameter of interest, which is in this case the expectation of y. And typically, to estimate this expectation of y, I will need to use Newson's functions. Functions, the mutants models. And as I said, they will typically be high-dimensional. So I will need to estimate either the expectation of y even x, which I call m of x, or the expectation of t even x on y1. And as I said, I assume the ignorability of of uh the missingness, so missing at random, so y here disappear in the conditioning. In the conditioning, and you will typically assume that this is a positive probability for x. So typically, to estimate this parameter, you are going to need to estimate one of these functions or two of them. And one estimator, which you can think about, is a plug-in estimator. So, what you can do. So, what you can do is to say, okay, I can estimate this function here, either by doing some models, coverage selection, or by using a depression tree or any other method of your choice. And then the plug-in estimator is to take the average of my imputed values for my. So this is also called, this plug-in estimator is also called the regression imputation. You do a regression non-parametrically, for instance, and you impute the values of the response and you take the average over the whole sample. Because you don't observe the health of those that don't retire at sixty-two, for instance. This is going to be a super efficient estimator. Super efficient estimator if you start choosing covariates here, for instance. So it's not going to be regular, asymptotically linear. So a better estimator, at least in some respects. So to avoid these super efficiency problems, you take the augmented IPV estimator. IPV estimator, which we heard about yesterday, also. So instead of just taking the imputed average, you take a correction here. This estimator is called double robust estimator also, or augmented IPW. So it's not if you are saying. It's not if you are say that for M you use you want to use a linear regression for all the covariates, but you do some two steps. So you first choose covariates, which covariates you have to be in M, and then estimate M. If you do that, then this is... So the problem is in M itself. The problem is in in uh in that is in the estimation of M itself, so that you have a two sta and you have a two state two stage estimation. So how you can go in showing that is that so in order for this is going to be you are going to be able to show that this is a regular symptomatic linear estimator under some conditions. And the typically typical conditions are the following ones. Are the following ones. You need to have some weak consistency in estimating both E of X and M of X and you need the product rate condition. So the error in estimating E times the error in estimating M needs to be of order root 10. Under these conditions, you can show that then you have a regular asymptotic linear estimator, and in particular the root and bias is going to be all one. So you won't have this problem of the final sample problem I showed earlier. But this condition you require both nuisance to be Yeah. Very weakly. I know what you're thinking about. But so you can also get similar results if one of them is not weakly consistent. But then you're thinking about in a different way, and we can discuss that later on. But if you ask the interesting with this condition, The interesting thing with these conditions is that you are say that you are doing some non-parametric estimation for E and for M, for both, then this is very weak consistency. I mean, they need to be on very slow rate of convergence, but that's going to be okay. This is a bit stronger. Equivalent thing to first two is just consistency. And the third one, is it equivalent to require each one to be equal to n minus one fourth? Yeah, so that's so if both of n quarter, one quarter, then this is going to be yes. Because then if for instance if one For instance, if one is going slower than that, then it's going to be okay if the other is going quicker. So this is called in the literature double rate double robustness. So if both of this order that's the typical order of non-parametric estimators under smoothness assumptions, so this this is not strong condition in this sense. not strong condition in this sense. But what is even better is that uh even if you have uh a bad estimate of E of X, then uh if if you can retrieve your results if you use a quick a better estimate. So it's a kind of non-parametric double robustness in this. So in the same paper Farad showed that you can Paper Farad showed that you can retrieve this convergence order by using LASSO to choose coverage. So, here now I give an example of how you can get this convergence rate by using LASSO to select cover rates. So, if you remember, I need to fit this model and this model. I have lots of cover rates. What I do, because I have got many cover rates, I use Lasso. Say that Say that M of X is linear in all these covariates, but it could have also polynomial curves. But anyway, say that it's linear. Then I use LASSO. Many of these parameters will be hopefully shrink to zero, so I get a subset of all my covariates, hopefully much lower dimensional. The same with this probability of t over 1 given x. I will look at which variable by using less. I will look at which vari by using Lasseau, I will select a subset of cover X, which I call Xt. Now for this AIBW to be regular synthetic linear, as you want to be, then what you can do to get that result is that you use XY and refit M by OLS, for instance. You take X T. You take XT, refit this, say that it's a logistic regression, you use MLE and refit it. And then you plot this into this. And then you can show that you have under sparsity assumptions. Of course, that so works that typically if you have some sparsity assumptions, then you will get an asymptotic linear estimator and the correct inference eventually. Okay. That's yes. Sorry to interrupt you a little bit. I understand the way like using the augmented inverse probability weighting scheme can be treated as a debale procedure very similar to the debalced scheme. Yes, yes. Sometimes you say. So, now what I'm going to show you before I show you the estimator that we propose is that this gives you good properties, but it's going to have a cost. And to illustrate that, we have some simulations here. So, we simulate y1 as a linear function of my x's, and I have as many x's as I have said. And I have as many x's as I have sampled in my observation, as I have individuals in my samples. And the same for the probability of being missing. I will use a logistic regression and I have as many cover rates here as I have individuals in the sample. So the parameters here will be so that some of them will be will so some of these cover rates are going to Discoveries are going to be related to the outcome and others not. So there are some zeros here. And the same for the gamma. So some of the variables will be related to y1 and t. Some of them will be related only to y1 and some of them will be related only to t. And I'm sorry because in the simulation study we also have the y0 that you can ignore it for the sake of illustration here. Here and the X's are multivariate normal. So, with this example, I'm estimating actually the difference between these two expectations, but the results will be exactly the same if I will estimate only this object, which I was talking about. So, what you can do is to use the plug-in estimator, as I said, to model selection with Asul and then To model selection with LASU, and then use the plug-in estimate, the imputation estimator. And that's the blue one. And you get some bias, as you can see, which is of the same size as the standard error. And that makes you an empirical coverage of 80%. The nominal is 95, so you want to have 95. If you instead use, you see I call it double we call it sometimes called double selection here. Sometimes called double selection here, this AIPW where I select XY with LASU and XT with LASU and then plug it into this AIPW. Then I manage to decrease the bias, but at the cost of variance inflation. So I get almost double standard error here. But that gives me the right inference. That's what I want. That's what I want. So this is this is a symmetric linear it does the correct so you can you can trust the asymptotic distribution of this estimator and you see it in and you can see that it's similar results for smaller samples. Both methods are valid, it's just the inference, one is not valid. No, well this is not this is not this is a This is a super efficient one. This is not asymptotically linear. But for this fixed beta, the bias and bias should be still zero, almost zero, right? You have no guarantee for that. Oh, you have the changing data. Because the number of covariates is increasing with n. So we are. So we have achieved what we wanted, something that works, but at the cost of very large variance inflation. So we told us, well, is this really necessary? And so we stopped thinking about this. Maybe we can use this plug-in estimator after double selection. So what I'm going to do now is that I'm selecting XY with Lasu, as I said. X1 with Las, as I said, Xt with Las, as I said, but now I take the union when I'm fitting M. I'm not using now the probability of being missing. So I'm just using the plug-in, but I'm using the probability of missingness only when in order to select text. So it's there implicitly, but I'm not using the dot augmented inverse problem in this image. So we have been able to show that under some product rate conditions of the same type that I just showed, then the bias is uniformly of root 10 model. I don't think this is uh uh asynthetically linear. At least we are not we don't know too sure that. Probably it's not. But its bias is uniformly of the right order. So how does this show in this setting? So this is the orange, that's our proposal here. And you can see that what happens is that the bias is decreased. Is decreased also very much, as the asymptotic result predicts, but without cost invariance inflation. So the variance doesn't increase as it did here. And we have the correct. This is not guaranteed by the theory we have so far, because we only guarantee that the bias is uniformly, but it seems Bias is uniformly, but it seems like it's the bias which is the problem. Yeah. Why and that would fix? It's because intuitively it's because the short answer is that we know, and there is a lot of papers, simulate simulation studies out. Simulation studies out there. We know what the problem is, why the variance becomes so high, is because of the variables. It's because the estimation of this is very variable. If there are instruments, so if there are variables here in XT, if there are variables that affect only T but not Y, then the this is quite variable. Is quite variable. Using this increases, the IPW gets very much variable if there are variables that affect only t instead of y. I'm not sure about the intuition, but that's the mechanism. If you don't have that kind of variables, then you won't see these problems. So the idea was here, okay, how can we need to have Xt because if you don't have it, we know that that's If you don't have it, we know that that's going to cause this problem in bias. We can also that's when we would get this double, this bimodal problems in finite sample. So we thought, well, we need to have Xt, but maybe we don't need to use the propensity score, the probability of missingness into the estimation. So that was how we thought, but I'm not sure if that's a good answer to why things. Answered from why things happen. May I ask what is K there? What is K? Yeah, what is K? K is just that we value in the simulations. We have different settings, the simulations. We have used several values. So we have more simulations than the one I showed. So you can put here 0.1 or 0.2 or 0.3.1, 0.4. Yes, yeah. So we have these chosen cell phones. In your uh DGP, do you require all your X to be continuous? Well, in some ways. I mean the DGP, the X's are normally distributed and independent. So it's a very simple situation. But it's just to illustrate the problem. I mean, it doesn't, the problems that I show doesn't disappear if you get other x's. You can. I mean, the estimators don't. You can. I mean the estimators don't can be uh used with uh discrete axes or or dependent axes. Yes? Compel the performance for the old proposal is cross feed half of them to f meter, m height, and then use the second half for the s meter of top. Yeah, but this uh this is not the same. This is not going to solve this issue. This is in general, this is CrossFit is used to ensure that this is asymptotic linear in specifics without in wider generality. So this is asymptotically linear if the functions that we're estimating belong to the downsquare class. And to avoid To the downscript class, and to avoid that kind of assumptions, you can use cross-fit. Actually, we can use the cross-fit first half to stack variables. You can use the second half to add. I know that can be used to address the technical issue of council class, but I think in terms of performance, it may perform the end or regional proposal that can use the footy data to select the X in the function and then use that LDM. But what will that solve? You mean that that will avoid the inflation variation? I don't know. Just ask. Yeah, I haven't tried. I don't think so. I don't think so, but I haven't tried. Yes? So this does it, does it have to do whether your selection procedure itself is consistent or not? Good question. So, so the the you can you let me take for half for alpha hat to be uh super efficient. To be super efficient, then this has to be a consistent or conservative. Because of course you could have a model selection here which is taking away too many covariates, but that's going to be, then you will have bias. So you you want if you are using a model selection, you want it to be consistent or conservative, but it is not you don't take away That you don't take away important coverage. Yes, more questions, oops. So I've been talking uh uh a lot about A lot about model selection and then fitting these nuisance functions, but this applies also to if you're estimating these nuisance functions with non-parametric methods or machine learning algorithms. So, you need to be, if you're doing that kind of things, if you are interested in one parameter, you need to know that if you're doing data-driven stuff before, you should be careful how you do your inference. And that somehow illustrates these problems. And we know that many of us, and all of us, probably have done that. You have looked at the data, done things, and then eventually you choose a model and you do your inference based on that model. But that can. Model, but that can give you wrong inference. So that's maybe one of the takeaways. There exists solutions when the target parameter is one of the parameters in a linear regression, or when you are in a missing data outcome context, and you are missing a random. That's the thing I showed to you. And proposal inference is really Inference is really the same context. So you also use these double robust test images for which you have the correct asymptotic distribution. I've tried to Google yesterday a bit about sensored covariates. I think I found one paper at least doing Cox regression with sensored covariates and uh using AIPW. Maybe some of you know more about all the papers. Know more about other papers. So, probably in that situation, in these situations, you might get in the situation of this paper, you might have the correct inference if you use model selection in the user functions. So, yes, we have tried to mitigate the variance inflation problem. We have a very recent paper now. So, of course, missing out random is. Because missing at random is not always realistic and you cannot test it temporarily, so you might want to do sensitivity analysis to this. And then the concept of sensitivity analysis, we have looked at this validity at high-dimensional settings or so. That's what I wanted to tell you today. Any questions? I see. Yes, okay. Where is it time?