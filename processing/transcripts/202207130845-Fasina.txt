My first year in applied math at Yale. I'm in Smith Krishna Swami's lab. Today I'm going to be talking about estimating using diffusion curvature to estimate local curvature in high-dimensional data. I'd like to acknowledge the co-authors, Donna Jay, Kincaid, Dawson, Bastion, Ian, and Smith. So, an overview of what I'm going to talk about: why do we care about local curvature and high-dimensional data? I'm going to I'm going to introduce the diffusion geometry framework. I'm going to formulate diffusion-based curvature and then talk about some applications. So one of the things about manifold learning is curvature is an intrinsic property of manifolds. And so we want to gain novel insights about high-dimensional data by looking at this notion of curvature. So manifold learning and diffusion geometry have been useful frameworks to analyze. In useful frameworks to analyze manifolds. And so, if we can leverage diffusion-based curvature, we can define some notion of local curvature that can give us insights about point cloud data. So, here are some examples we had in mind. So, lost landscapes of neural networks, and then also point cloud data in general at the bottom. So, for lost landscapes of neural networks, looking at Of neural networks, looking at second-order information could be useful. So, for example, you could try and discern between an optimum and a saddle, whereas first-order information wouldn't necessarily tell you that. Secondly, point cloud data. I know this is an example of a torus, but if you had some sort of biological or biomedical data that had branching patterns, looking at positive and negative curvature could tell you something about the cellular lineages and whether a cell is going to go under like apoptosis or something like that. Something like that. So, this is sort of like more motivation for looking at local curvature. So, I want to talk about diffusion geometry and eventually go all the way through diffusion maps and make this connection between diffusion maps and curvature and why using diffusion-based curvature is a useful framework for defining local curvature. So, sort of again to motivate using diffusion curvature, Olivier Ricci have defined this notion of curvature based on edges. But point cloud data doesn't naturally come with edges. So I guess that description is not as useful in the point cloud setting. So this necessitates the need for a diffusion framework to define a diffusion-based curvature. Because if you have point cloud data, such as data. Because if you have point cloud data, such as data and a neural network or the biomedical data that I mentioned, sort of this edge-based notion doesn't make much sense. So, on the right, just going through this picture, which many of you know because we've been talking about graphs and graph neural networks, you have some data, you want to compute some distance metrics and then distance metric, and then you take it to an affinity matrix using a kernel of your choice, and then you can sort of start to learn the manifold in this. Manifold in this fashion. So, a lot of manifold learning hinges on the manifold assumption, which is that there's some low-dimensional intrinsic manifold that you're trying to learn. So, given a set of points in Rn, there's some lower RD manifold that we want to make use of. And so, diffusion maps sort of hinges on this manifold assumption and a lot of manifold learning methods in general. Manifold learning methods in general. So, this is sort of more defining the framework that we're going to be using. So again, diffusion maps is a method introduced by Coiffman and others. So just sort of repeating, you define some distance matrix and then some sort of affinity matrix with a kernel of your choice. In the paper, we use an anisotropic kernel where the alpha parameter can be used to separate. The alpha parameter can be used to separate between the density and the geometry of your data. And in this case, you know, you could have like cells as your observations and genes as your features, and you create some n by n distance matrix and then the affinity matrix and so on and so forth. And so this gives you some notion of similarity between your points. So continuing with the diffusion maps construction, once you have your kernel, you can create some sort of row number. Create some sort of row normalized stochastic matrix. And then you can eigen decompose this matrix to get your diffusion axes. And so your first K diffusion axes basically are going to be the most prominent patterns that traverse your manifold. So I guess like a more useful reason for using this framework and why I'm building all this up is because in the diffusion map space, the Euclidean distance is equal to the diffusion distance. So later when I define this local diffusion Define this local diffusion-based curvature, this is going to be sort of a key result. And if you want to see a formal, Koichman defines this theorem in his paper. So now I'm switching up a bit and talking a little bit about curvature, and then I'm going to try and connect diffusion curvature to this. So this is just curvature in the continuous setting. So essentially, the intuition behind this is, you know, looking at the tangent plane. Looking at the tangent plane to a 1D line of curve. So if you look at like an oscillating circle over on the left, it's sort of like this one over R relationship where R is how much the oscillating circle is touching the curve. So the smaller the radius in the denominator, you know, the higher the curvature you'll have. So yeah, larger magnitude and curvature. And then the inverse holds for this case over on the right. So, again, going to, or I guess like sticking towards that sort of intuition where you're looking at two surfaces, one of the things we tried to use to intuit this idea in our paper was this idea of a sticker phenomenon. So essentially, if you imagine you have like a Euclidean disk, which is like a sticker, and you have some sort of ball, a sphere, that sticker, if you adhere those two surfaces together, is going to bunch up and you'll have like positive curvature. And you'll have like positive curvature. If they're flat, if you have like a just a cylinder, then the sticker and the cylinder will adhere perfectly and you'll have zero curvature. And if you have the saddle, the curvature will be negative, the sticker will rip essentially. So essentially what I'm trying to get at is it's sort of this comparison of areas between the geodesic disks on these surfaces and then the Euclidean discs, which is the sticker. So, going towards Olivier-Ricci curvature, again, I know I mentioned it, but essentially they look at this sort of edge-based notion of curvature where they're looking at the transport transportation distance, Walgreen Steam distance between two points. So, what I'm trying to show in this image is basically, if you look at the right, if you have two balls sort of like on a plane, well, they sort of have like one notion of distance, but if you put them on a sphere, you can see the distance is increased. The distance is increased, and therefore the curvature is higher or lower. So, this is sort of like the notion of curvature that they went with. But again, it's like high computation costs. And if you have point cloud data that doesn't naturally come with edges, then this framework isn't as amenable. So, now I'm going to talk about Bishop Gromoff, which sort of ties in the sticker phenomenon I just mentioned. I want to talk about diffusion. I want to talk about diffusion curvature, and then I want to sort of connect this with our main theorem and then proof. So again, looking at the sticker phenomenon, this sort of Gaussian curvature area comparison definition encapsulates this. So again, in the case of a sphere where the area of the sticker is larger when you put it on the sphere, then you'll have a higher curvature value, whereas in the saddle, the curvature Whereas in the saddle, the curvature will be negative because the sticker is smaller, the area of the sticker is smaller than the area of the corresponding saddle. So Bishop Gromoff essentially extends this to higher dimensions. So essentially, I guess towards the end of this theorem, they're saying if you have like something with a constant cross-sectional curvature, which in our case is the Euclidean disk, versus like some sort of other manifold and Manifold and in this theorem it's bounded below. So it's basically the positive case. Then the volume of the volume of the ball of the Euclidean disc will be higher than the volume of the ball of the disk on the manifold. So this is essentially relating back to the sticker phenomenon and that intuitive definition of how you relate two areas or volumes to define a notion of curvature. So, then, continuing with this framework, like I said, diffusion maps are nice because in this framework, the Euclidean distance is equal to the diffusion distance and the diffusion map space. So we define a ball where the transportation distance is the diffusion distance under some radius, right? And then we say we define a probability measure of going from one point on the ball to another point and basically define. Point and basically define our notion of curvature using this. So these two pictures are sort of highlighting some empirical results we ran using our definition of curvature. So you can see if you're powering your diffusion operator, you can see sort of the spread on the ball. And then we also have a comparison with ground shoot curvature and then diffusion curvature. And so, sort of building up to our main theorem, which hinges on Bishop Gromov, essentially, if you have a ball that's positive and has larger curvature, and then you take this to the 2D case and you invert this, then you can get some sort of notion of a definition that will scale with positive and negative curvature. So, this is again sort of refining our definition of curvature and showing that it holds for positive and negative. And showing that it holds for positive and negative values of curvature. So, I want to talk about some results that we gathered using this definition of curvature. So, this is for test data, single-cell data, and Hessian estimation of neural networks, where in the last case, we actually try to define this notion of curvature with a quadratic form in the form of a Hessian. So, again, we compare to Gaussian curvature, and we use comparisons of cross-correlation and mutual information. So, you can see the values are pretty close just looking at these biaxial plots. And then, sort of the same result here, you can see the values are close, although on the bottom where I guess this bowl is like accentuated, you can see that our method actually calculates. You can see that our method actually captures the high curvature at the basin of the bowl, which is nice, whereas I don't think Gaussian curvature is capturing that as well or as much. And then for single-cell data, from days zero to three, where things are more stable, you have like positive curvature, whereas from days 18 to 27, where things are branching and sort of differentiating, you have negative curvature. Have negative curvature. And so, this is just a fate plot of embryonic stem cell data set. So, these are sort of nice validations that our definition of a curvature makes some sort of sense. So, now something, like I said, we wanted to do is define sort of this quadratic form, scale this up to quadratic form notion of curvature. So, we basically generate like 5,000 quadrics. Of like 5,000 quadrics. And it's simply we train a neural network to predict the Hessian. So we input the domain and range of these quadrics, and then we're trying to learn the Hessian that defines this quadratic form. And so when we go to our neural network, we're essentially trying to learn the Hessian of this neural network by training the neural network to learn Hessians of the domain of the parameter space and then also the loss value or inputting those things so that we can learn the Hessian. The Hessian. So, again, the way we do this is we sample endpoints that are k-dimensional, and then we take their loss values. We concatenate that loss value to construct a diffusion map. And then these diffusion axes and axes and their corresponding loss values go into the neural network so that we can predict the hash shape. So, these are some results looking at the eigen spectrum of the Hessian. So, the top result, basically, we train a feedforward neural network on MNIST, and we looked at the last epoch, 200 epochs, and then the 25th epoch. And so, you can see that at the 25th epoch, the eigenvalues shift. So, basically, you have more negative eigenvalues with the shift to the left at the 25th epoch, which indicates Which indicates, I guess, a path of more descent. And then the same thing on the left, where you have less negative eigenvalues at the lower epochs, which means that there's still more room to descent. So yeah, those are the main results we use to test curvature with. So we use priors from diffusion geometry to create some new notion of diffusion curvature. And then Diffusion curvature. And then we basically, you know, are saying that we can extract meaningful information using this notion of diffusion curvature. And so, some limitations we encountered were the sampling and also that our notion of diffusion curvature doesn't really have a sign, it's just like a scalar value. So, these are some things that we're going to try and improve upon. And so, future work, we want to try and apply this, I guess, Hessian estimation or second order information. Estimation or second-order information for just more neural networks to look at generalization. There's a lot of theory looking at like if you have like a flat basin, that's more likely to generalize or not theory, empirical studies. If you have like a flat basin, it's more likely to generalize, whereas if it's curved, it's less likely. And then we want to look at more theory to see if we can extract more meaningful information from point cloud data. And then also, there's sort of this notion of how much information are we losing by capturing a low-dimensional. Are we losing by capturing a low-dimensional estimate of this high-dimensional system? So, we want to do maybe some more looking into that a little bit more, I guess. So, yeah, I'd like to acknowledge, again, the co-authors of this paper that we've submitted. And there's the paper on archive if you want to take, you know, more look at it in more depth.