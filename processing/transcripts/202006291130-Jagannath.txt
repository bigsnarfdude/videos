So please Okash with the second part of the lecture. So All right, so just to sort of recap, where were we before the break? Oh, sorry, one second. There seems to be some sort of echo. Does anyone else hear that? Yeah, I did hear that too. And hopefully it's over now. And hopefully it's over now. Hopefully it's gone. All right, so yeah, so just to recap, you know, you know, we studied the random energy model a bit. And at least we started, you know, to get a hint or at least see sort of a simple version of. Of this notion of replicosymmetry breaking. And so what I'd like to do in this part of the talk is to sort of explain sort of more broadly the replica symmetry breaking system and sort of introduce the canonical models to which. To which the story for today and sort of tomorrow's lectures will be about. And so, okay, so what we'll do is we'll start, let me just start by defining sort of the canonical models in this setting. So, okay, so what we'll be thinking about for this part of today's lecture will be a mean field spin system on n particles. So, again, you know, as before, the configuration. You know, as before, the configuration space for the system will be the discrete hypercube. And on the discrete hypercube, we'll put a few different kinds of Hamiltonians. The first one we want to talk about is, we'd mentioned this already, was the Sherrington Kirkpatrick model. Or SK model. And here, what we'll do is we'll have it's an infinite range two-body interaction. Where here we normalize things so that you know JIJ are going to be standard Gaussian random variables, IID N01. And this normalization And this normalization is, you know, one thing you should check is that the variance here will be of order n. You know, the idea being that, you know, each particle will sort of contribute in order one quantity, and the energy itself sort of maximally will be of order the number of particles in. The number of particles in the system. That's sort of why we chose this normalization. So the next class is what's called Derrida's P-Spin models. So here, instead of having an infinite range, two-body interaction. An infinite range two-body interaction will have an infinite range p-body interaction. And then, you know, you could say, okay, well. And then you know, you could say, okay, well, why just stop at sort of two-body or three-body or five-body? Why not have you know mixtures of these kinds of interactions? And so you can think about mixed P-spin glasses, which are just linear combinations. So the story I'll talk about today is supposed to apply to sort of this broad class of models. And we at least know it for sort of a large class of mixed-piece bin models now. Class of mixed p-spin models now. And sort of fortunately, you know, it's a large enough class that when it comes to sort of doing useful calculations, when we go to sort of applications in other courses to other fields such as, you know, combinatorics, et cetera, you can sort of approximate any model you want by the class we can consider, you know, in a way that sort of doesn't affect any of the main calculations you're going to try to sort of shoot for. You're going to try to sort of shoot for. And if what I said seems extremely vague, it was sort of deliberately so. But feel free to ask more about that during the break. So, you know, the questions we want to think about again is: so, can we try to understand the structure? Of the Gibbs measure, you know, whatever that means in a mean field setting. And specifically, you know, we want to understand it in this sort of low temperature replica symmetry breaking phase. And then after this, we'll talk a bit about the free energy for these systems. And we'll sort of end by discussing the phase diagram and sort of what's known about what systems are replica symmetry breaking and when they are, as you vary the temperature. And for that, And for that last part of the discussion, it'll also be helpful to add an external field, by which I mean we'll consider the sort of modified Hamiltonian, oops, which is say the p-spin model plus n times an external field times. Times the magnetization, sorry, which I guess is better written than this. All right. So, okay, so first step is to try to sort of understand the structure of the Gibbs measure in the replicosymmetry breaking phase. And so, what I'd like to do now is start to sort of explain what the predicted and The predicted and now known in a broad class of models structure of the Gibbs measure. And sort of the idea behind this was introduced in a bunch of papers by Mazard, Parisi, and Vira Soro and various colleagues, and is now sometimes called the Mazard-Perezi Virasoro Picture. Prizi versus oral picture or onsets for the Gibbs measure. And so their idea is that in the replica symmetry breaking phase, the Gibbs measure should decompose into Into what are called peer states. So, okay, so that, you know, for those of you who are familiar with sort of other statistical mechanics models, sort of non-disordered systems, you know, it's sort of similar in spirit to this idea of pure states you see there, but sort of very different in terms of how it plays out. And in particular, And in particular, you know, one of the really so first you have to sort of understand what you mean by pure states in this setting. You know, the standard sort of DLR approach to Gibbs measures won't work because you're studying a mean field setup. But sort of the really incredible thing is not just that it decomposes into pure states, but that these pure states, if you look at the sort of space of pure states, they say it should have this really It should have this really incredible decomposition. And what they say happens is: well, okay, so if I draw my space of pure states, which is going to be a ball in infinite dimensions, so this is my sort of rough attempt at the space of pure states this space, what will happen. What will happen is you'll have your pure states. There'll be tons of pure states at low temperature. But these pure states are themselves going to organize into groups. Into groups. In the following sense, you know, every point, so every distinct point in these groups are going to be equidistant from each other. And these groups themselves are going to decompose into groups. Where again, every point in the green groups is going to be sort of equidistant from the points in the other green groups. And in particular, you know, there's sort of hiding behind all of this is this sort of incredible branching structure. What I'm saying is, you know, at the highest level, the green groups, all the states in the green. All the states in the green groups are going to be orthogonal to each other, that means are going to sort of be equidistant to each other. Within these groups, they're sort of the barycenters, which are going to be themselves, you know, equally separated. And then within And then within these groups, you have a separation as well. So we expect is this sort of really incredible sort of clustering of states. You know, I draw this. You know, I draw this picture where the idea is: you know, every point, each of these leaves that are connected by a single parent will be equidistant, et cetera, et cetera. So what we expect is that, so this example is what's called three RSB, so there's three levels of replicosymmetry breaking here. And you see, what's happening is that you have clusters of states, which are Of states, which are within clusters of states, which are themselves within clusters of states, the number of levels of clustering is the corresponds to the number of levels. Corresponds to the number of levels of replica symmetry breaking. You know, the idea being, you know, if I okay, let me just continue. And furthermore, you know, there's, you know, the picture is even more precise. You know, if I fix to a certain depth in this tree and I marginalize over all the states, Marginalize over all the states at that depth, and I look at the fractional mass each step takes, what you find is that at fixed depth, the rank weights follow a Poisson der clay structure, and more precisely, they follow what's sometimes called the Derrida. Called the Derrida Royal Probability Cascade. And the idea of an RPC is just as in the REM case where you just had this point process of the largest weights, and then the true Gibbs measure was just the math was. Gibbs measure was just the math, was you just sort of normally re-normalized by the weights. For the Royal Cascade, sort of very vaguely, the idea is you're going to assign independent Poisson point processes to each of these vertices in such a way that the masses at any given level will be a Poisson-dere-Clay process. And sort of, you know, as if this weren't crazy enough, one of the central predictions is that you can have Can have a continuum of levels of clustering. And this is sometimes called full replica symmetry breaking. And then finally, they predict. And then finally, they predict that sort of at the finite level, finest level, you know, under a certain assumption, which I'll just call the EA property, the states at the finest level are pure in the sense. Pure in the sense that, sort of, if you just look at that state, it's a replica symmetric state. Okay. So that's sort of a lot to digest. And so, what I want to do now for the next chunk of time is to sort of start to unravel this picture and what we know about. About so the starting point is: you know, I want to try to understand what does this sort of crazy branching structure have to do with replica symmetry breaking. At least, you know, as we've As we've started to understand it in the last lecture. But remember, last time around we said, okay, replicosymmetry breaking should relate to sort of the behavior of independent copies of the Gibbs measure. And so what we're going to want to do now is let's look, you know, not just at two copies from the Gibbs measure, but let's draw infinitely many IID copies. IID carpet. So Marius is asking in the chat if you could define what state means in this context of the original models that you defined. So, okay, we're going to get to that. That by itself is a very non-trivial answer, I'd say. So hopefully I will get to this in like a few minutes. And Gurag was asking, Jurav was asking for which models this picture is predicted to apply. All models is predicted to apply. But I mean, actually getting exact results, I think, is part of the difficult story here for mathematicians. It's supposed to hold for all of these sort of canonical models that we introduce, these mixed P-Spin models, the P-Spin models. PSPIN models, the P SPIN models, and the SK model. Okay, so okay, so how do we go from this sort of crazy clustering picture to something about replica? Right, so we had this picture where we had states. Picture where we had states, whatever you know, following the question earlier, whatever that means, and then within these states, you have states. So, what does this have to do with replica symmetry breaking? How can I turn this into a question about the replica? Well, what I'll do is one thing we could think about is I'll form the following matrix. Matrix Rn of normalized inner products between the different replica. So it's a doubly infinite array. And what you expect in the setting is after applying a permutation, A permutation, the draws you get should be organized in such a way that when you look at the Gram matrix, the Gram matrix has the following very structured form. We're sort of on the largest off-diagonal, you have one allowed value of overlap, which you can think of as zero. Within the sort of red regime, you'll have another allowed value of overlap. Allowed a value of overlap. And this allowed value will happen sort of in all of these regions here marked in red. And then at the finest level, you know, except for the diagonal, of course, you'll have yet another allowed value. Right, so you know, a replica symmetric picture is supposed to correspond to there just only being a Q0. So this infinite by infinite matrix you get is just on the off diagonals, it's just one number. And so you just have this huge matrix where, you know, in some sense, every entry is you can. Entries you can swap with any other entry. But when you have one level of replica symmetry breaking, as we had in the random energy model, now you'll have two allowed values. You'll have, say, zero in the picture we have here, but it could be non-zero if you have an external field. And then you'll have some other value of inner product. And then you could have, you know, in this picture here, we have two steps of replica symmetry. Have two steps of replica symmetry breaking where again you sort of have three different allowed values and products. And then again, the point being is that whenever you're within any one of these boxes, you have sort of the matrix should sort of look locally constant. All right, so okay, so what's going on here? So, you know, behind all of this is the phenomenon of altruetricity. So, what do I mean by ultrametricity? If you have some metric space, It's called ultrametric if for any for any sort of three points x, y, and z, the distance between x and y satisfies the ultrametric triangle inequality. It's the maximum of the distance from x to z and the distance from z to y. This is to be contrasted with the usual triangle inequality where the distance is at most the sum. Here we're saying the distance is at most the maximum. So the simplest example of an ultrametric space is a tree. Whenever you think ultrametric, you should, you know, in the back of your mind, think about, think there's a tree somewhere. Tree somewhere. And more precisely, you know, if you let x denote the leaves of this tree, and on it you impose the metric, which is just, so this is actually, sorry, a rooted tree, say. And the distance between x and y, what we'll do is we'll say it's the depth of the least common ancestor of x and y. Of X and Y. And this will be my metric. Maybe subtract from this by the actual size of the tree. And so the exercise. So, in the homework, you'll sort of go over an exact, a precise example of how you find an ultrametric space in a tree. But the sort of key points I want to bring up are the following properties of ultrametric spaces. So, one thing to keep in mind is that in an ultrametric space, all triangles are acute and isosceles. So what do I mean by this? You know, all the triangles, you have two sides which are equal, but the two sides that are equal are also the longest. That are equal are also the longest sides. The next thing, sort of useful to note, is that if I have two balls Of say radii R1 and R2, where R2 is bigger than R1, then the intersection there's a dichotomy. It's either the empty set, or it's the entire sort of smaller ball. Furthermore, you know, if I have two points of x1 and the ball around x, oops, and x2 in a ball around And x2 in a ball around y. And if we know a priori that these are disjoint, then the distance between these two points is actually equal to the distance between the centers. And then finally, you know, this is sort of in the sort of last thing is the reason why I say when you think ultrametric space, you should think tree. Well, I think what you get is this. Well, if you fix a radius, you know, by property two, if I fix a radius, the balls of that radius form a partition of the space, right? They're disjoint and they'll exhaust the full space. But, you know, it also tells you as you make small. Also, it tells you as you make smaller and smaller radii, these new balls aren't going to sort of lie willy-nilly, they're going to be nested within the bigger balls. So, in particular, if you have, say, a finite sequence of radii, then there's a family of partitioned partitions indexed by a depth k tree, a depth, you know, if it's length n, sorry, a depth n tree. It's length n, sorry, a depth n tree, where this, you know, the family of balls form a partition, which is a filtration. You know, in the sense that what you can do is, you know, if I, if I have, you know, the picture is going to be, if I first let me take my largest balls, and then we'll say, okay, And then we'll say, okay, within those largest balls, there'll be some, maybe, some smaller balls. And then what I'm saying is that now these two balls and this ball, if you can group them in the following way, you index to this level the red balls. And then you label the vertices at this level with the green balls. And the point is that this labeling is natural in the sense that if alpha is a child of beta, then the ball corresponding to alpha is contained in the ball corresponding to beta. So, okay, so where are we going to see ultramatricity in our spin glass model? What we'll do is the following. Let's, you know, as before, we'll look at this doubly infinite matrix. Infinite matrix of pairwise inner products. Well, if you think about it for a second, you'll realize that this sequence lies in a compact product space. And so, in particular, it's tight, so it'll have weak limits. And looking at this weak limit, what we find is the following sort of important result of Penchenko, who showed that for a broad class of spin glass models, which I'll call generic, the probability. The probability, I'll put this in quotes, that the support of the limiting Gibbs measure, the probability that that support is an ultrametric set, when endowed with the sort of inherited topology from little L2, inherited metric, this probability is one. Or more precisely, you know, one thing to observe is. You know, one thing to observe is that this matrix is weakly exchangeable in the sense that if I relabel all of my copies, all of my IID copies, it won't change the law of this matrix. And so then one way of writing this is just saying the probability that the inner product between the first two guys is bigger than the minimum of the inner product between the first guy and a third draw, and the second guy. And the second guy in this third draw, the probability that this inequality holds is one. And to translate from the ultrametric inequality to this, just realize that I've switched from distances to inner products. And so mins become maxes and inequalities flip. So, just a word, since there were some questions about this, what do I mean by generic models? So, generic models are a class of mixed-piece spin models where so it's a mixed p-spin model, so it's a linear combination of these p-spin models. And the assumption is that if I sum over those p such that the coefficient corresponding to that p is non-profit. The coefficient corresponding to that p is non-zero, the inverse of p, this should be infinite. And the interest in generic models, as we'll sort of talk about in the next slide, is that, you know, while it is in principle a subset of the spin glass models, so in particular, this doesn't apply immediately to, say, the SK model or the P-SPIN models, what you can say, for example, is by Say, for example, is by a continuity argument, the free energy of any mixed P-SPIN model. So, for example, the P-SPIN models and the SK model can be approximated. Can be approximated arbitrarily well by generic models. Right, so if I want to compute the free energy model of, say, the SK model, what I'm saying is that there's the sequence of generic models whose free Of generic models whose free energies converge to the true free energy, sort of after the limit when n goes to infinity. So the reason why this is helpful is, you know, at the end of the day, the main sort of reason for introducing the replica symmetry breaking onsats in the physics literature was that it allowed people to start making computations about free energies. About free energies, large deviation rates functions, but sort of hiding behind all those calculations was some form of ultrametricity. And so, what this theorem is telling us is that, you know, we get this ultrametricity for at least a dense class of models and the space of models. So, okay, so just getting sort of very briefly. Sort of very briefly to the other question that was sort of brought up. What do we mean? So, okay, so now we see altruetricity. What do we mean now by states and pure states? Right, so the idea, the question we want to ask is: you know, how do we understand what these states are from the Gibbs measure at finite n and the idea. And the idea is as follows. So, the starting point is to show that for generic models, if you look at the sequence of Gibbs measures, the sequence is what we'll call a problem. Is what we'll call approximately ultrametric. So a precise definition of approximately ultrametric will take, you know, I think, quite a bit of time away from our discussion. So I'm happy to speak about it in the breakout sessions, but I just want to explain what it means heuristically, because that's sort of the key point. The idea is that instead of looking at the gift The idea is that instead of looking at the Gibbs measure, instead of trying to pick out sets, you want to look at sort of the effective geometry of the hypercube as sort of viewed from the Gibbs measure. And the idea is that with probability in the law of the disorder, which I'll denote by bold p with p probability tending to one, there's a collection. There's a collection of points, of sets, of clusters, indexed by this AR, by which I mean it's the infinite tree of depth r such that each non-trivial Such that each non-root vertex has countably many children. Now these sets are almost balls. They're almost balls with a certain collection of. With a certain collection of radii. And these balls, for these balls, you have these sort of the properties one through four of ultrametric spaces with high probability in G. You know, by which I mean, so for example, if I think about the pairwise distance property, what I'm saying is, you know, if I take two independent copies, That lie in the same cluster, the chance that they're much further than you expect is very small. But if I pick two points and two different In two different clusters, the chance that they're oops, sorry, got that the other way around. Seems that they're further from each other, yeah. The chance that they're much closer to each other than you expect is also very small. So the point is that these sets sort of from a So, the point is that these sets, sort of from a geometric measure theoretic sense, are sort of acting like balls in ultrametric spaces. And then the states are then just the Gibbs measure conditioned on these false. All right, so I think maybe perhaps now is a good time to take a five-minute break or a short break of something. And sort of up next is just we'll talk about sort of answering one of the other questions about, you know, when do you see what kinds of replica symmetry breaking in what models? Breaking in what models, yeah. So, if anyone has any question in the chat, please type in their own.  It seems like Rosa answered a lot of your questions. Okay. So in that case, maybe we sort of continue along a little bit. I think that's a good idea. We'll end maybe early and sort of leave more time for questions. Sort of leave more time for questions. All right, so the last thing I want to do is I want to talk a bit about the free energy and then go from there to talk about what we know about the phase diagram for spin glass models. And so the starting point is what's And so the starting point is what's called the Parisi formula for the free energy, which is predicted originally by Parisi in the late 70s and proved in sort of a sequence of works by Guerra and Telegrande for even models, for models with only even components, and then finally by Pinchenko. By Pinchenko, sort of building off of work of Eisenman, Sims, and Starr. Which I want to say is around 2007. And what it says is that. And what it says is that the free energy of any mixed piece bin model converges almost surely to a number which is given by minimizing a certain variational problem over the space of probably. Over the space of probability measures on the unit interval. And the functional that you're trying to minimize is now called the Parisi functional in the math literature. And so what it looks like is as follows. You take your measure, and what you do is you're going to solve two PDEs. Let me describe the picture with an external field. External field, just since we're going to want to talk a bit about phase diagrams in a bit. So you solve two PDEs and you evaluate them both at a certain point in space and time. Now, these PDEs, so So you knew they solve a certain family of Hamilton-Jacob E. Bellman equations. So they solve equations of the form dt phi is at most some time-dependent coefficient times Times the Laplacian in space om phi plus another coefficient, which is the C D F of that measure mu times the gradient squared. And the final time data you choose here is different. Is different for the two problems, and same with so here, you know, this choice of coefficient will depend on the model you choose. And so, for example, in the case of P-Spin models, it's like It's like p times p minus 1 times t to the p minus 2. And the choice of f depends on the geometry of the problem. So for us, since we're looking at the spin glasses on the hypercube, for u, f will be the log of the hyperbolic cosine of x, and for v it'll just It'll just be e to the x. So how does this formula relate to replica symmetry breaking? Well, the idea is as follows. What we do is we look at the unique minimizer of P. We'll call it mu star. We'll call it μ star, which is called, and we'll call it the Parisi measure. Proving the uniqueness is, of course, yet another challenge, and it was an important result of Ofinger and Chen from 2000, I want to say 14. And the idea is that the Preezy measure is, you know, this minimizer is essentially going to be the limit. Going to be the limit of the overlap distribution. Where this is actually inequality in the case of generic models. And so then the point is: we'll say that a model is represented. The point is, we'll say that a model is replica symmetric if the minimizer is just the Dirac mass at a certain point. And replica symmetry breaking means that the support contains more than one point. So, what ends up happening in the various models are as follows. So, let's describe the phase diagram. So, first, let's talk a bit about the Sherrington-Kirkpatrick model. So, here what happens is as follows. So, on the x-axis, I'll be drawing the inverse temperature. On the y-axis, Temperature on the y-axis, I'm doing the external field, and there's a certain curve which is called the Dielmeida Tau List line and sort of what was originally conjectured. Sort of what was originally conjectured is that the model should be replica symmetric here and replica symmetry breaking here, and in fact, it should be full replica symmetry breaking here. So you should have exponentially many, I mean sorry, you should have a continuum of levels of replica symmetry breaking. So the state of the art as we know it right now is sort of, there's an important result of Fabio Toninelli. Who showed that below the AT line, you are replica symmetry breaking. And more recently, there's some work of mine with Ian Tabasco, where we showed that outside of an exceptional set that's An exceptional set that's compact and away from the critical fields, you have replica symmetry breaking above the AT line. Now, the story for full replica symmetry breaking is still a really exciting open question. And very recently, there's been important progress by Ofinger, Chen. Ofinger, Chen, and Zheng, who showed that you have full replica symmetry breaking at zero temperature in the sense that you don't have finite replica symmetry breaking there. You have at least infinite replica symmetry breaking. All right, so but what happens for the p-spin models? So, here the story is a bit more complicated. And it will sort of serve as a bridge toward our lecture tomorrow, where we start talking about dynamics and algorithms. What's expected to happen is the following picture. So So one thing you could do is you could plot the analog of the DL meta taulus line. But what we can show now is the analog of the DL-made detalis line looks like this. And so, in particular, it's not the correct phase boundary at sufficiently low temperature. Instead, what's supposed to happen Is that there's some other phase boundary above which you're replica symmetric. There's yet another phase boundary where you're one step of replica symmetry breaking. And then finally, when you cross this phase boundary, you're full replica symmetry breaking. And so what's known at the moment is that again below the AT line, you have replica symmetry. You have replica symmetry breaking, and it's sufficiently high external fields. We know that the DLM-Datalis line is eventually the correct phase boundary. But what's going on, sort of, in this entire region here, is still, you know, from a mathematical perspective, a real mystery. Of a real mystery. And we'll talk a little bit about it tomorrow when we start getting to questions related to dynamics. But I think this is maybe a good point to stop for today. All right. Sorry, it's a main issue over here. There are some questions for you in the chat. So the pink line in the Derry dot P Spin model was the analog of the Delme data taulus line for the P Spin models. So, okay, the questions are getting okay, so there are a lot of questions. Let me try to answer them in order. So, okay, or maybe reverse order. So, is the conjectured picture on the right for any p greater than or equal to 3? Yeah. So, this picture is supposed to hold for all p bigger than or equal to 3. And at least the results that are known are also known for all p greater than or equal to 3. And sort of in general, there's sort of a long discussion of AT lines and sort of their generalization for p-spin models. And when you know they're sort of the correct phase boundaries in this work of myself, with Ian Tabasco. So from 2016, it's in PTRF. Hopefully that one person asked at least for a pointer. To briefly describe the AT line beyond that, you know, I'll just say this because it'll come up in tomorrow's lecture. So there is an explicit formula for the AT line, but its meaning is supposed to be the following. There is a certain, in the replica. There is a certain in the replica symmetry breaking sort of picture, sort of replica trick story, there's a certain quantity that shows up that depends on the points of the support of the pre-C measure. And this quantity is called the replicon eigenvalue. And the A T line is the curve where this eigenvalue. is the curve where this eigenvalue is zero. And sort of a dynamical interpretation of it I'll sort of give a bit in tomorrow's lecture, hopefully. Before we move on to another question, let's just thank Okosh for his two talks. You can all unmute yourselves. You can all unmute yourselves and we'll have a kinklap. So yeah, after this, what we'll do is