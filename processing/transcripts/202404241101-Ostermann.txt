Wonderful workshop. Enjoyed a lot. So the title of my talk is a filtered lease splitting for the good Pusines equation. You might ask why I'm speaking about the good Pusines equation here and not about Schrodinger equation, but I will show you in a minute that this equation is very much related. So the difference is very little in the analysis I'm doing. And this is a very recent work I did together with HG student of Wandafolk. PhD student from the fourth PhD student, Long Li, and the visiting PhD student from China, Hang Li. Actually, Hang is a student of Chumei Su. So we did this work together. So what is the Good Bosinesk equation? I've written down the equation over there. So it's a second-order problem and it's a dispersive equation which It's a dispersive equation which was introduced by Business to explain shallow waterways, and it's still a very interesting model. So how did we solve this equation? Well, we first transformed it to a first-order problem. The transformation is here. And well, this doesn't really look. I see it now. The transformation is here. And it's about one-page calculation. And it's about one-page calculation. You see, what you get is an equation of that. Well, how can this be that from a second-order problem, from one equation in second-order, we could get one equation in first-order? Well, this is possible because now this new equation is a complex value. So u is a complex value. Well, if you look to that equation, you immediately see that this equation is very close to the non-linear. Very close to the non-linear Schrodinger equation. Because here, the operator F, this Japanese bracket, we saw it already in the previous talk, and this operator is almost the Laglacian. Difference is very little. Very kind, thank you. I am now fully equipped. Fully a curved. So, this is more or less a second derivative. And here, the nonlinearity, the dominating term, here is a quadratic term. So, it's very close to a quadratic nonlinear Schrodinger. Don't be afraid of this second derivative. This is compensated here by the end of the so that this is and therefore all what I will tell you now also applies to the Schrodinger equation. And actually, we have done this first to the Schrodinger equation. Done this first for the Schroedinger papers, so we have already papers for that. And this is now a new work which is a little bit different than the Schrodinger case. Of course, we are not the first that consider Business equation. And I mentioned here a paper not because Jews is in the audience, because this is really a very, very nice problem. So even now, it's now more than 30 years old. More than 30 years old, this paper, it's still very nice to read, and it gives a perfect introduction to the pseudo-spectral method for discretizing this equation. They require somehow smooth data, meaning that whenever they want to show second-order convergence and the proof and the convergence, say in L2, they need initial data, say in H4. So they need four additional. H4. So they need four additional derivatives in order to prove second-order convergence. And this is something we want to overcome. We are here interested in non-smooth initial data. So initial data with less smoothness, of course, you also then get not second-order convergence, you get a reduction. But nevertheless, how can this be done? And I have done together with Jumay in twenty nineteen the first paper where we proved first and second order conversions First and second-order convergence for initial data in HS plus one and S plus three, respectively, for say showing convergence in HS. This means that for first order, we needed one additional derivative, and for second order, we needed two additional derivatives. This was then improved by Hang and Chumei in 23, where they showed that two additional derivatives for second order are still. To second order are still enough by a modified scheme. But all these computations here, they are quite, I would say, standard analysis, meaning that when you have a differential equation, you dot AU plus F of U, and you apply to such a differential equation, say a standard integrator, you can be an X-dimensional integrator, a splitting integrator, E. Integrator, splitting integrator, E, M Roman Kota, doesn't matter very much. What you need for stability, you need Lipschitz. We met that F is Lipschitz. Otherwise, this classic stability rules all break down. Now, there is a problem if you really want to go below S equal to a half. And this is what we aim to do. Namely, when you uh well speak here about the stability, we saw that in this equation we have a we have a quadratic term. With a quadratic term, then you need the so-called bilinear estimate in several spaces, and you have this bilinear estimate, but only for s larger a half in one dimension and s larger d over two in d dimensions. So this is a real restriction. And the question is, can this restriction be overcome? Because in the analysis of this equation, the restriction can be overcome. And there are also solutions with initial data in there too. So the question is. Initial data in there too. So the question is: can we produce something similar, also numerical? Can we really go with the data? And same question for Schrodinger equation. So in one space dimension, it's clear or it's known that you can go with the initial data up to h minus a half. And the question is, what can you do in Lumack? Well, I show you immediately the answer to this question. We have to. Question: We have to change the framework. We have to work in a more abstract framework in order to be able to show here convergence for very low regularity data. And well, the best I think is to study to study theory, to study what people in theory data. And this brings us to Jean-Bougain. Jean-Paughin, a famous analyst. A famous analyst, he became famous for proving the existence of the solution of dispersive equation with low regularity initial data, and we orientated ourselves in this framework. And I'd like to explain a little bit this framework, not too much, because I don't have that much time. Framework is complicated, but I'd like to give you some ideas how this can work. And what we do is we define We uh define a space uh by defining the space uh in in in on the Fourier side. So we are taking a time space Fourier transform of our function. And then we say that this function u belongs to a certain bogel space if and only if a certain norm is finite, where the norm is defined in terms of the Fourier coefficients, as it is. But forgetfulness. But forget for a moment this red term. So you set just p equal to zero. You see immediately this is nothing else but the HS norm. So this is a well-known space. But now comes this red term. So this is in addition. You see this Boogan space has two indices, S and B, and actually this is the clue that we can do more. And well this second weight is a bit more complicated because we see this is a mixture of time. This is a mixture of time and space. So the first variable is the time variable Fourier, this is space variable in Fourier, and they are mixed. So there's a mixture in time and space. And a very complicated mixture. So how does this mixture come? And well, this actually is motivated by the linear part of the equation. So the linear part of the equation, I copied. I copied it here once more, and when you are doing a Fourier transform, you see you actually get exactly this. So the sigma comes from the derivative in time, and the spatial derivative translates into that. So this is intimately connected to the equation, to the linear part of the equation, which means that when we are using this framework for dispersive equation for each type of equation, Equation for each type of equation, you have different weights. So the weights they are very sharp in the sense that they give some of the best you can get for your equation. And it relates time and space. Well, for the non-linear Schrodinger equation, the weights look a little bit different. You get that. But I think it's obvious that these are equivalent norms. So for Schrodinger, Norms. So for Schrodinger and for pussy nests, we get equivalent norms. And the study you're doing here is modest. For the same B or do you need? No, no, you have to then play also with the B a little bit. Well, if you know this interaction representation of the flow, then you can motivate this norm I had here very nicely just by taking that norm here. taking that long you know you you get whatever whatever well now you can spend days in studying properties of these spaces are not spaces maybe you've seen before so you have to study properties I don't want to discuss a lot here I just want to discuss two properties because with those I will illustrate why these uh spaces are uh the right way. Are the right way to solve these problems? And well, I said we have now these two indices, so we can play with the indices. And what we can do, well, here we can say go from a lower index to a higher index, and then we gain here something whenever this function here is pre-multiplied by By a test function with compact support. So, this is one property. And the other property, so I think all of you recognize that this is the non-linear part of the variation constants former. So, this is actually the essential part when you say study stability. And what we can do here, we can estimate the null. Well, estimate the norm of this object by the norm of the nonlinearity, and here we lose in the second index. We lose one in the second index. So this can be proved. Now, what else do we need? Well, we need some replacement for this Palini estimate, which is not true. And this is the replacement. You see? Is the replacement. You see, now we have an almost bilinear estimate with an index that is arbitrarily close to zero, which we hadn't before. But the price we pay, we have to raise the second index. So this is the price we have to pay. This is the freedom we have, and there's the price. And now, if you like mathematical analysis, you can play a little bit around. And you can prove this property. Proof this property, it's a consequence of Holder and some embedding properties. So, again, this takes you maybe half a day or even longer, depending on how much you put these objects to verify this difference. And then you can show that for s greater than equals zero and p in a certain range, well, this uh putsinetsky equation as a unique solution in that space, and uh well this space is continuously embedded. Space is continuously embedded in a continuous functions with valid HS. So, this is also, say, a more classical solution. Maybe you are now a little bit afraid of this space, but at the end, you don't need the space. The space you only need for the construction. At the end, you throw it away, and you have again very nice properties. Of course, the solution doesn't need to exist for all time, so you have a maximum existence and the proof is by fixed-point theorem, and actually, what Fixed-point theorem, and actually, what I plan now to do, I still have a little bit of time, a planner to do to oops, this was the wrong direction, a planner to show you part of the proof, of course not the whole proof, but the most important part. The most important part is when you start with the variation of constants formula and you're doing fixed point iteration to construct the solution, because there you need the sediment. Because there you need the civility. And well, what we do is the following. First, we fix a small delta and then we construct the solution on a small interval. So the time is now bounded also by delta. In that situation, we can pre-multiply our variation of constants formula with smooth cutoff functions. So these are cutoff functions that are one on the interval minus one, one. On the interval minus 1, 1, and they vanish outside minus 2, 2. Know what these functions look like. And I can do this without changing here anything because t is so small. And now you see I'm in the situation that I just had before. I had this integral here, the nonlinearity, which is multiplied by a cutoff function here. Then I can bound this by just bounding. This by just bounding the nonlinearity, and I'm losing here in the second unit. This possible. And well, the F is here more complicated. The F consisted of two terms, but I just take the two reproductive terms. So this is the first step, the first property. Then I still have an eta here, a cutoff function in front. And what I can next do, I can gain here a little bit of the small delta by changing it. By changing again the index. So, this was the second property I've shown you before. Now I'm using this modified bilinear estimate. I'm losing a little bit. Instead of 0, I have now 308. But if I restrict my b to be between a half and 1, I can estimate it here easily by x. And now what you see here, at the end, what you get is something which looks like Something which looks like Lipschitz in XSP. So if you would have, say, any space, the HS space, and you have a Lipschitz condition here on that, then you would exactly get such an S. So at the end, you get what you also get classical in this very complicated framework, and you can carry out your standard fixed-point iteration and construct the symbol. So this is the main thing. So this is the main main thing. Well, but that's not the end of the story, because now you need something which Bougain did not develop. You need discrete Bougain spaces because we want to do numerical analysis. So you need discrete Bougain spaces and instead of having now functions in two variables, you have functions, you have sequences of functions in one variable. So u n of x is the numerical approximation at time tn, and x is the approximation. T n and x is here still continuous. So you get these sequences and you do the same business again. So maybe I'm very short here with this slide. I know the slide is complicated, but you introduce spaces by, say, using a very similar norm. Actually, this norm is motivated by the discrete injection representation. And then you verify again the properties we had before. The properties we had before. So the properties that are here, namely this embedding and this bilinear estimate. What you see now here is that the formulas changed. So in contrast to before, we now have projections in front of these operators. And we need these projections in order to filter out the high frequencies. So when you have low frequencies, So, when you have low regularity initial data, you are doing a Fourier decomposition of this initial datum. You have contributions with very high frequencies. Now, when you have a non-linear problem, the high frequencies start to interact with each other, energy is transferred forth and back, and this creates all the problems. So, there are two possibilities here. So, one was nicely demonstrated by Boo Yang on the first day. He has shown for a particular He has shown for a particular equation that you can do differently without filtering this away. But the standard way when you have not many good ideas is just to filter them away. You cut them away, then they are gone. So we filter them out. And we do this by, well, maybe you look to the Fourier representation of the filter because then you see it's just a cutoff of the Fourier functions. And this cutoff means that the highest Fourier mode squared times da. Highest Fourier mode squared times TOR must be of size 1. Well, you say this is a very strong stability restriction now. It reminds me to explicit method. If I were using explicit method, I would have just this stability condition. This is really useful. Well, I have to tell you, you also need this for the non-linear Schwilling equation. Edward Hugh has shown that if you really want to have a good long-term preservation, then you need Preservation, then you need such a condition. Otherwise, long-term preservation is also lost for the cubic long-bit-schroding equation. So we have to live with that. Then we have results. Then we can construct a numerical integrator. Now the numerical integrator is easy to construct because we just take our equation and well, we multiply the equation with the filter. We multiply the equation with the filter. So you see, we put the filter everywhere where it's necessary. So this is the filtered equation. And then we apply a standard least splitting scheme to this filtered equation. So we consider here two parts, the linear part and the non-linear part. And this least splitting means that, well, I take E of the linear part, which is this, and then E of the non-linear. And again, if you have, say, if you don't want to do a hike this afternoon, Do a hike this afternoon, but you want to do mathematics, you can take two sheets of paper and you compute a little bit, tack, tack, tuck, and you find you will get this formula. I don't present the way how to construct this formula, but we find here an explicit formula. And then the strategy of the analysis is that in order to estimate the difference between the numeric and the exact solution, and you see, at the end, I measure everything in L2 also. I'm working in this very complicated space. Working in this very complicated space, at the end it's L2. I'm using the Tranquil inequality, so I take the difference of the numerical solution with my filtered equation, the solution of the filtered, because the numerical solution actually is the numerical solution of the filtered, and the difference of the filtered and the original. This is then the strategy of the proof. And this was again the wrong so I'm a specialist, you'll always take in the wrong direction. Or is taken the wrong direction. Now, now I can give the convergence result. The convergence result works like this. So, as I said before, we have a maximum existence interval. And if I'm taking now time which is smaller, and I denote, say, the exact solution we've set, and the nominal solution we've set n, then we can show that for steps. Then we can show that for step sizes which are small enough, but of course independent of the stiffness, etc., we can show that we have convergence of order s over 2. So you might now think this is very disappointing. Small s, s over 2, is almost nothing. But I don't think that we can expect m more if you are really close to zero. Only for very particular equations maybe you can do something, but in general You can do something, but in general, this is w what you what you can get. And this is, of course, a uniform error estimate on this interval. Let me show in numerical experiments that you also get this. And well, here you see non-smooth initial data. So, this is initial data meant H0.2, this is in H a half. So, very rough and constructed in the Constructed in a straightforward way. So we are taking random vectors, we work in Fourier, and then we smooth them a little bit with random vectors to get these solutions. I think this is quite standard. And well, then we do experiments with this rough issue data. And we work in different spaces. You see the space always here, so H0.8, H1. This is the slope. This is the slope, the theoretical order of convergence we prove. And the red line is on the vector results. Everything fits very well. And we can go even down. You see here h one-third, here h0.2. What you also see is that the improvement here is very little. If you very, very non-smooth initial data, of course, the accuracy of the solution is not very good. This also has to do This also has to do, of course, with the fact that we are using this filter. So, for the step sizes we are using here, we are not using a lot of Fourier confusions. We can buck them out. Well, to conclude, so I spoke about the dumb integration today for good Business, but as you have seen for non-Linear Schroeding, For non-Linear Schrodinger equation, everything looks more or less the same, and I will show you some references on the next slide. Standard methods fail for non-smooth initial data. Well, I didn't give you a picture for that, but you can do numeric experiments. And when you're doing numerical experiments with non-smooth initial data, say for non-linear Schrodinger equation, where strength splitting looks like this, the worst sonances. Resonances. So here is the logarithm of the time step, as usual, and here is the logarithm of the error. Well, you can say there is something like an order of convergence, but what happens is when you're taking a smaller time step, it might happen that the error increases by a factor of 10. So this is a very inconvenient situation. And in our case, such a Such a behavior of the arrow is replaced by a very nice keen line, as we have seen on previous slides. Well, as a first step, Fourier integrators were considered, but these Fourier integrators, well, they are also filtered, they require less regularity, but they have the problem with this SOLAF embedding limit. And what I've shown today to you is this. Today, to you is this approach with Boogean spaces, and they are useful for the periodic case, so for all dispersive equations on a torus with periodic boundary conditions. And for all these equations, you can work with this framework. And I'm pretty sure that you get similar results. So, we worked a lot together with Katharine Schwarz, Frederick Wussy, on the cubic linear Schrodinger equations. We got Schroeding equations, we got similar results also for full discretizations. And well, here are the references. So, these are the papers together with Frederic and Katharina, where we worked on the cubic million-Schroding equation. And this is the reference of this new paper concerning Business equation. But with that, I think I stopped. Thank you very much. Thank you very much. What's the width of the filter? That's an optimal filter? No, the filter has to satisfy the condition that I've shown before. Maybe I can go back. So this is the condition the filter has to satisfy. So this means here that it is. This means here that it is of order one, so it can also be ten, doesn't matter that much. But if you use it even smaller, then you even cut off more frequencies and you get a larger error. So this actually, this filter here is the reason why we get to over two. Because we get an error from the space discretization, we get an error from the dungeon discretization, and this sum. Danny's goodization, and this somehow makes the error of both them in the same size and bigger. Is there any information how large Tx is a function of S? For this equation, yes. This depends a lot on the initial data. So if it's small initial data, maybe f in some situations it's even infinity, but for other initial data it's smaller. Initial data, it's smaller. So the situation is less studied than for the Schrodinger equation. The Schrodinger is no less. At least I know more. What is good about the good Pusyinesque equation and is there also a bad Pushinesque equation? Well, this depends on the choice of the signs. So when you have this equation. Yes. Yes. Maybe I don't have to show the first slide, but on the first slide I had several terms, so you can choose either minuses or pluses. What is that if you choose it in the wrong way? It was nice to exchange one actually, which is one. Yeah, but for the good you can also have a power. For the for the good boost, I think there's also not guaranteed that the integrated quite complicated. I know that there are buggies uh That there are various versions of equations, and I know that Julia Meisu, in one of her papers, in the introductions, she and the co-author, Muslim Gutski, Turkey, they had a very nice, extensive introduction, and it is really complicated. It is not only good and bad, but everything in between. And this is about the blow-ups, and quite a few of you, quite a few of you. This equation has singular solutions, so that they are not finite, but if you analysis, those can interact among themselves. So you can use the name one. They have poles, but uh you can have solution poles both for one another has. Of one another, but elastic conditions are different. Other questions, comments?