Studies and I'll just briefly review some of the basics behind genome-wide association studies. So, GWAS are basically the main approach to identify genetic associations with phenotypes of interest. And they're based on, you know, And they're based on a very simple approach. So you basically have, you measure millions of genetic variants in the genome, and you have an outcome of interest why, and you want to identify which of these variants is associated with this outcome. And by and large, the GWAS are based on this simple linear and additive model. So So you test for association, you do marginal association. So you test each of these variants individually for association with y. And then you have some strict control of the type 1 error and the kind of the typical approach in GOS is to control the family-wise error rate using the bomb-ferning method. Okay, so in today's talk, I'm going to So, in today's talk, I'm going to talk about some alternative ways to identify these kinds of important variables in GWAS. And, you know, this is like the kind of the title of my talk on the Knockoff-based inference. But before I get to that, I just want to mention very quickly a related approach to linear regression, which is a quanta regression approach. So, right, so we all know about linear. So, right, so we all know about linear regression. You try to identify, you know, to test whether a genetic variant influences the conditional mean of a phenotype. But, you know, you can ask the question, you know, how about effects on other aspects of the trait of the phenotype distribution? So, how about effects on variance or other moments of the distribution? And so, quanta region. And so, quantile regression is an approach that allows you basically to look at associations beyond the mean. So, you can test whether a genetic variant is associated with the conditional quantiles of your phenotype for a given quantile level tau. So, you basically get this, you can use. Get this, you can use some methods from the quantile regression literature. So, you basically minimize a quantile loss function, which is usually this, you know, sorry, this pinball loss function. And you can basically estimate these quantal specific coefficients. So, we can estimate beta j for a variant j for a given quantile level tau. Then quanta level tau. Okay, so you know, quanta regression is not very commonly used, definitely not commonly used in GWAS, but it's not commonly used even in more general in epidemiology studies. And so I'm going to, so we wanted to kind of look to see what are the advantages of the quanta regression in the context of the JIWA studies. Of the JIWA studies. And just to give you a little bit of intuition, so what when quanta regression can do well and can do better than linear regression. So we did some simulations. So this is a simple homogeneous model where you basically have homogeneous effects across different quantiles. So this is kind of, you know, the ideal scenario. So, the ideal scenario for the linear regression, so linear regression shown in blue, will tend to have higher power than the quanta regression. Quanta regression will do better when you have more heterogeneous, more complex models. So, for example, you can have effects only localized at the, say, the upper quantals of the phenotype distribution. Then you get Then you get, you know, quantile regression will tend to have higher power compared with linear regression in those settings. Where, you know, another setting that's somewhat relevant to genetics is kind of this departure from additivity. So when you have dominance effects, so dominance effect means that the two alleles at a local, they interact. Then you again, you You again, you can, the quanta regression can have higher power in those scenarios. So, I mean, we can design these models, right? Where we can see the quanta regression can identify associations the linear regression can miss. But I guess the question then is: you know, how often do we see these models in practice, right? Yes. Hypothesis, this is for a specific quantile when you're measuring powers. Yeah, but we can, yeah, so you do. Yeah, but we can yeah, so you do the test at the quantal level, but then we so we get a p-value based on a given quantal level, but then we combine p-values using this Cauchy combination method to combine, kind of correlate it on the overall wire. Those are the three genotypes groups. So we have zero, one, and two, and then we have 0, 1, and 2, and then we have each curve corresponds to a quantile level. Oh, so yeah, so sorry, on the axis, it's a con is a quantile, the different quantile phenotype for the phenotype. And so we applied quanta regression to the UK biobank. So, you know, about so we looked at about 350k individuals. About 350K individuals, 10 million SNPs for 39 quantitative traits. And so we can just look at one of these plots. So it's kind of the number of, so what we show is for each trait, we show the number of loss i that we identify using. So the gray bar is loss i that are identified by both linear regression and quanta regression, okay? Linear regression and quanta regression. Okay, so most of the loci are picked up by both methods. And then you have the blue bar, which are variants that are picked only by linear regression, are missed by quanta regression. And then you have the red, which are only quanta regressional loci. Okay. So at this point, you may ask yourself that, you know, why bother doing this linear, this quanta regression? Because we identify so few loci using quantal regression. Quanta regression. But then, if you look more carefully at those loss sites that you identify using quanta regression, we do see some interesting results. So we can look at how heterogeneous those loci are that we identify by quanta regression. So we can look at how heterogeneous the individual, the quantum specific betas are. And so obviously, because you know. And so obviously, because we are well powered in the scenario, so the quantal regression, so the red distribution here corresponds to quantal regression only loss size, so they tend to be more heterogeneous. So I'm plotting here a measure of heterogeneity of the betas across different quantile levels. The other thing that we've noticed is that this quanta regression, if we compare this quantile regression-only loss site versus the linear regression-only loss, side versus the linear regression only loss side. The quanta regression only loss size, they tend to have more functional effects. So if we use this kind of functional prediction methods to functional scores, the quanta regression only losses, they tend to have higher functional effects. And then let me see. Yeah, so the other thing that you know you can ask where the Thing that you know, you can ask where this heterogeneity is coming from, and so one possible mechanism for this heterogeneity is from gene-by-environment interactions, right? So, you have gene-by-environment interaction that you don't include in your model, and that can lead to basically this kind of localized quantile effects. And so you can So, you can take those loss sites that we identify using quanta regression and look whether there are significant gene-by-environment interaction effects using some environmental factors that are measured in UK Biobank. And what we find is that indeed, those SNPs that are identified by quanta regression methods, they tend to be enriched in. Methods they tend to be enriched in gene-by-environment interaction. Sorry, so this is one this is a little bit out of order, but so for example, one particular SNP, I mean, you can, you know, we have all those results for 39 traits, right? So you can go and look individually at those. But I'm just going to highlight one result here that was also identified in a recent paper. Identified in a recent paper that looked at dominance effects. So, this SNP in this particular gene, we found it to have a very significant interaction effect with age for this platelet distribution with trait. And the same SNP was detected to have a strong non-additive association with the distribution with platelets. So, I guess what I want to say is that. I guess what I want to say is that there are different ways you can identify kind of these non-additive or non-linear effects. And there are different methods in the literature, like people look at, you know, they use methods to identify, say, effects on the variance, right? Or they use methods to identify dominance effects. But quanta regression allows you kind of in one framework. In one framework to identify those effects. And then, of course, you have to investigate what is leading to those kind of quantile specific effects. So, I won't spend much time on this because it's not the actual topic of my talk, but I just wanted to mention it. Talk, but I just wanted to mention it as kind of an easy way that you can complement existing GWAS studies by doing quanta regression. So quanta regression can be applied efficiently genome-wide. So we applied it at large scale in UK Biobank using standard statistical packages similar to what you do with linear regression. It does help you identify variants. You identify variants with heterogeneous effects across different quantiles, including non-additive effects. The other advantage is that it's invariant to monotone transformation of the phenotype. So you don't need to normalize your trait or do any pre-transformation of the phenotype. And that's an advantage because it's kind of the transformation of the Because you know, it's kind of the transformation of the phenotype that you have to do for linear regressions that will affect the results and the interpretation. And just simply, as the other advantage is that it does provide richer information on an association because you can look at different quantile-specific facts and it can pinpoint variants that can be interesting, you know, they can have gene-by-environment interactions. You know, they can have gene-by-environment interaction, so you can, you know, it provides more richer information than linear regression. Okay, so that's all I'm going to say about quantum regression, but I feel like it's an underutilized technique. So, the main topic I want to discuss is this knockoff-based inference. So, this is a technique in high-dimensional statistics to basically select important variables with control of FDR in finite samples. Okay, and so what the reason I kind of became interested in this technique is because of some issues we have with the GWAS. So, one of the major So, one of the major confounding factors in GWAS is LD, so this correlation among sites in a given region of the genome. And so, what happens is like, also, you know, remember when I first introduced GWAS, I mentioned that we basically do this very simple marginal test. So, we test for association. So, we test for association with each variant one at a time. And because of these high correlations that you have in a specific genetic region, when you do this marginal testing, you'll have a bunch of SNPs that show significant associations in a GIWA study. And then the point is that among those SNPs, only a small number are going to be. Going to be relevant, so you know, causal for the trait of interest. And so, there is a lot of work post-GWAS in trying to identify which of these highly correlated SNPs that you identify in GWAS is, in fact, the causal SNP. So, the way the typical approach that we do it in GWAS, you first do your GWAS, you identify the region with significant snakes. With you know, significant SNPs, and then you use a fine mapping method, usually a Bayesian approach, to compute you know, posterior inclusion probabilities, PIPs for the SNPs at this locus and try to identify a set. So, this is a credible set, so highly correlated SNPs that with high probability contain at least one causal SNP. And so, you have kind of have this two-step approach. You first do the GWAS and then you do the fine mapping. And then you do the fine mapping, and in some sense, it's suboptimal because the GWAS model and the Bayesian model that using fine mapping are different models. And also, there is no overall control of the type 1 error at the genome-wide level, because you do it locus by locus. And so, this is kind of where the knockoff framework will come in and kind of help to select. Kind of help to select important variables and kind of more bring us to bring us closer to causality, but providing some guaranteed FDR control genome-wide. The other confounding factor in GWAS is population structure. So, and we are here in Mexico, so we worry about population structure in European-based studies, but Based studies, but you know, if we talk about that mixed population, that is even more of an issue. So, yeah, so the problem is that you want to basically establish an association between G and Y, but if you have this confounder population structures that you don't adjust for, you can get spurious association between G and Y that are basically due to that structure. And so, again, you know. Again, you know, this is a well-worked area. So, in GIWA, so people use principal component analysis or linear mixed effect models to account for such structure. But yeah, so the NOCO framework is actually helpful in this setting as well, as we'll see. So, I'm going to talk. So, I'm going to talk in more detail about the Knockoff-based inference. So, the Knockoff framework, as I mentioned, is a framework that was introduced by Emmanuel Candes and Rina Barber. It's a framework to perform variable selection in high-dimensional settings that provides control of FDR in finite samples and also in the presence of arbitrary correlation structures. And the main idea is basically to construct a syntax. Is basically to construct a synthetic version of your data of your genotypes that's independent of the trait, if conditional on the original genotype. And the idea of this synthetic version of your data is that it can serve as a benchmark and so it can help identify the real causal variants or genes by basically contrasting this original data. This original data versus the synthetic data. So, before I give more detail about the knockoff framework, I just want to mention some of the strengths of this framework. So, as I mentioned, it helps control the FDR. And this is different from GWAS, where we control the family-wise error rate. And so, FDR being, you know, less. And so, FDR being a less stringent FDR type on error control, it helps improve power. Unlike commonly used procedures for FDR controls, such as Benjamin E. Hochberg procedure, the knockoff controls FDR under arbitrary correlation, and it's also not a p-value-based method. So you don't need to have valid p-values to perform the knockoff-based inference. The commonly used linear models can be misspecified. Linear models can be misspecified in GWAS, as we alluded to before, and knockoff-based inference can help in those settings. And I'll show you in a minute how that helps, but you know, the bottom line is that with a knockoff-based inference, you don't need to have valid p-values. So you can incorporate results from more complicated non-linear machine learning models. And then the And then, an additional strength of the advantage of the knockoff-based inference is that you perform conditional inference. So, conditional on the other variants in the genome as opposed to marginal inference, which is what basically you do in GWAS. Right, so in the knockoff-based inference, so in the in the um so in the in the in the g was you basically test um this null hypothesis that um y is independent of g for each for each variant one at a time in the knockoff based inference you you test a conditional hypothesis so you test the null hypothesis that y is independent of g j conditional on the other variants in the genome okay And so it is basically this conditional testing that allows you to reduce the confounding due to L D and also confounding due to population structure, right? Because population structure is encoded in kind of the genotypes in the genome, right? G minus G, J. So this is how the knockoff-based inference works. So you have a first step which So, you have a first step, which is a knockoff generation. This is kind of the tricky step where you have to take your original genotype data and then you construct a synthetic version of your genotype data that has some exchangeability properties with your original data. So, I'll talk about that part. The second step: so, once you have the knockoff genotype data, Genotype data, you can calculate an important score on, you know, let's say for a genetic variant on the original data and on the synthetic data. And then you contrast them. Okay, so you calculate the feature statistic in a certain way. But the choice of the important square is very flexible, right? Square is very flexible, right? So you're not restricted to p-values from GUA. So you can use whatever important score you want for your variable. And then the last step is the knockoff filter, which is basically telling you, like, you compute a threshold and you reject basically all the variables that are have. Variables that are have feature statistics above this threshold, and you compute the threshold so that you have FDR control at a certain target level. Okay. So just more formally, you have the first step is the knockoff generation. And so you basically construct this G fieldum, the knockoff genotypes, so that you have this kind of what's called the Kind of what's called the swap exchangeability property. So if you take a set of variants and you swap between the original and the knockoff, the distribution remains the same. So in this example, if I swap G2 with the knockoff version G2 tilde, the distribution remains unchanged. So that's the swap exchangeability property that's kind of critical. Property that's kind of critical for the whole procedure to work. The second step I mentioned is this calculation of feature statistics for each variable gene. So I'm going to calculate the feature statistics that is basically this anti-symmetric function. So I'm going to, for example, I can take the difference between the important score of g minus the important score of g tilde. Okay. And you And you have this coin-flipping property, which is conditional on the absolute value of the feature statistic WG. Wg is equally likely to be positive or negative. So the sign is equally likely to be positive or negative if g is null. And so these properties is essential to have FDR control. Okay, so the third step. So, the third step is the knockoff filter. So, you calculate this threshold tau so that you have FDR control at the target level Q. And tau is defined using this formula. And so, what we have here, so this ratio is an estimate of the FDP, of the force discovery proportion. So, what you have in the denominator here, you have all the Here you have all the discoveries you'd make at threshold t, so all the number of discoveries for which you know wg is greater than or equal to t. And what I have in the numerator is an estimate of the false discovery, right? Because of this coin flipping property, I can estimate the number of false discoveries that I have among my discoveries by counting the number of G with W G. Of g with wg less than or equal to minus t. Okay, and so yeah, so and then once I have this threshold tau, I'm going to clear significance those g with wg greater than or equal to tau. Okay, so that's kind of the knockoff procedure. And you know, you know, you know, you have a certain problem, right? So you have to adapt. Certain problem, right? So you have to adapt it, right? So this is where we've been doing some work in the context of GWAS. And so the first step is, and the most important step is basically to construct a valid, to have a valid knockoff construction. And there are various ways you can do it. So one way is the model X. X knockoff construction. So in the model X, you basically assume that the distribution of genotypes PG is known, but you make no assumption on the conditional distribution of Y given G. And so the question now is how to sample from this distribution PG so that you have those exchangeability properties. So you can, so one trivial choice is to just take G tilde equal to G. Okay, so this is a valid nothing. Valid knockoff, but obviously has no power. And so there have been different constructions in the literature. For example, you know, for multivariate Gaussian or hidden Markov models. So there is one paper that built a knockoff sampler based on hidden Markov models. And this HMM-based knockoff sampler is very natural for genotype sequence data. However, this HMM knockoff generation is very slow. So that's the issue. And as I'll mention later, this knockoff construction is not trivial. Is not trivial in, you know, if you, so if we want to make it practical to be used in a genetic study in the context of GOS, we have some work to do. And so kind of a general way to generate knockoffs is this sequential conditional independent pairs algorithm, where you basically iterate. You basically iteratively for each variant, you sample GJ tilde from this conditional distribution. So the distribution of GJ conditional on all the other variants and the previously generated knockoffs, right? So you do it kind of iteratively for each variant, right? So for example, if you have three variants, you sample G1 tilde first from the conditional distribution of G1, given G2, G3, then you. G3, then you know, then you sample G2 tilde from condition distribution of G2 given G13 and you know, previously generated G1 tilde, and so on. Okay. But you can see that, you know, sampling from this conditional distribution is not trivial, right? So we'll see how we can make the knockoff generation more computable. Generation more computationally feasible. So, just some general remarks about the knockoff generation. So, you know, it can be computationally slow and memory intensive. Each knockoff copy of the original data, so you know, it is a little bit of a headache. You have to generate for, you know, you have this, for example, you know, in the UK biobank, right? So you have half a million. You have half a million individuals and 10 million markers, right? So you have to basically generate these synthetic or knockoff versions of your data. And so that's, you know, that's not easy, you know, and storing those. It does, the other issue is that it requires availability of individual level data, right? Individual level data, right? So, if you work with GWAS, you know that oftentimes we only have summary statistics available. So, that's another potential issue. Now, you know, if you just do the knockoff sampling one time, then results can be unstable, right? So, you have to, and also there are some issues with the power if you don't have enough signals. Don't have enough signals. So you have to actually do this. You have to generate multiple knockoff versions of your data. And so, you know, because of these issues, the knockoff-based inference can be challenging. So one approach that we've been working on is based on this autoregressive model for knockoff generation. Model for knockoff generation. So we model the genotype G at the variant J as a linear function of the other genotypes. And you may be asking, obviously, we don't condition on all the variants in the genome. So we use kind of this banded structure of the covariance matrix to only look at variance. Look at variants in a neighborhood of the Jace variant. Right, so we have this part, and then we have the previously generated knockoff genotypes. And based on this model, we can basically construct the knockoff genotypes. Genotypes, and I'm skipping some details here, but you can show that this autoregressive model is an approximation to sampling from that conditional distribution if you make the assumption that the genotypes are multivariate normal. Now you can, you know, you can further improve that approach in terms of computational efficiency, so we can basically use sub-sampling to select a subset of informative samples to estimate the parameters during the knockoff generation. And so that can, so we use a Can so we use this shrinkage leverage-based sub-sampling approach to greatly speed up the computation. So, how much time do I have? Okay. Yeah. Anyway, so that, yeah, so the point is that, you know, this, I mean, I don't even have the HMM-based. I mean, I don't even have the HMM-based knockoff generation generator here because in this comparison of computation time, because it's very, very slow. But basically, this sub-sampling method is much faster than even like, you know, the original sequential conditional independent pairs algorithm without the, you know, kind of these computational tricks to improve the speed. So, yeah. So, yeah, then so let me skip this part because I talked about this. So, this is like once you have the knockoff genotype, you can build your feature statistic and calculate the threshold for FDR control. I'll just want to, I mentioned that one of the disadvantages of the NOCO framework is that it requires individual level data and And in fact, we found out that if you have for this particular z-squares that we get from GWAS summary statistics, so G transpose Y, we can use a certain knockoff construction for normal data, for Gaussian data. It's called semi-definite program construction, which basically allows you to generate. allows you to generate directly the z-square, the knockoff version of the z-square without generating the individual level knockoff data. So now we can, if we restrict ourselves to this summary statistic as our kind of important score, we can in fact very easily generate knockoff version of the original z-scores and Of the original this course and kind of perform the knockoff-based inference using the you know without generating the individual level knockoff data. Yeah. Yeah, yeah. So you need to have that from yeah, that's a little bit of an issue because you usually have to get that from an external panel, and so that can introduce other issues, but Can introduce other issues, but yeah, we do need that. Yeah, of course. So we need to get it from a reference panel. So, like, but you know, you can have kind of issues because these scores come from a certain study, and then your reference panel might not have the same covariance structure as you. Covariant structure, as you know, as you have from in your original. So that can create some issues. So you can lose kind of these guarantees of FDR control in those settings. But anyway, so that's, I think that's any method that uses reference panel, covariance metrics from reference panel will have those problems. But it is a cool method, actually. So just to kind of summarize. So, just to kind of summarize, so when you have the GUS, so when you have the GUS, you kind of, you know, you do your, you have your original data, you get your z-scores and p-values. When you have the knockoff-based inference, you have the original cohort, and then you have to generate this knockoff-based cohort, and then you, you know, you have to run the whole knockoff-based inference to select the important variables with this kind of method we call Gauss knockoff. method we call gust knockoff. You only need the z-squares. You build the, you know, you construct your knockoff version of the z-square and then you select the causal variants. And so you can show that basically this summary-based method actually is equivalent to this method if you are restricted to those summary statistics. But you remember one of the advantages of the knockoff-based inference is that you're not restricted, you can, you have. Not restrict, you can you have a lot of flexibility in how you choose those important scores, so you know it is in some sense a limitation that you know we can only do it for those you know the square statistics we get from GWAS. So since I'm trying to wrap up with you sorry, there's another top online. You can solve one with Project. Yeah, so I guess, you know, so I mentioned this ability of the NOCO framework to remove confounding due to LD. And, you know, because we do this conditional testing, it is sort of intuitive. But basically, so this is like what you get from a GWAS, right? So you have all these variants that are above the threshold that will be significant. If you do the knockoff, you If you do the knockoff, you reduce the number of significant associations at the locals, right? So you remove some of the confounding due to LD. And then the other thing that I want to mention that I think it's also, I mentioned this confounding due to population stratification. So this is a data set where we have, you know, individuals from different ancestries, including African American and non-Hispanic white. So now, So we did this as a kind of as an just as a proof of principle. So if we do the knockoff-based inference without basically without taking into account population structure, so being totally agnostic to population structure, we actually control the FDR at the target level, right? Because our, so I'm not sure, I should have a plot here where. I should have a plot here where what the knockoff data looks like, but you basically recapitulate the same structure that you have in the original data in the knockoff data. And so that kind of goes away when you contrast the original and the knockoff. Whereas, you know, if you do kind of a naive association test without adjusting for, then your FDR is highly inflated. Okay. So it does have this kind of robustness properties. Properties that I think kind of a typical GWAS doesn't have. So maybe I'll just show one result of real data. So we did work on various applications and various designs. I only talked about GOS, but we looked at family-based design as well, where you have correlated individuals. So you generate knockoff, taking into account those. Of taking into account those correlations. So it's a very flexible framework that you can apply in different ways. So, this is an example where we applied it to the UK Biobank. It's a gene-based test. So, we select genes, not variants. And so, I'm showing here one particular locus that, you know, where the Knockoff inference did quite well. So, what I'm showing here on the left, so on the On the x-axis is the negative log 10p value from a conventional gene-based test. So, if you do, like, for example, just test like SCAT or Borden. So, you get all these kind of significant associations here. If you do the knockoff-based inference, which is basically, you know, if you look above the red line here, horizontal red line, you get these only two genes. And, you know, this is cholesterol. And you know, this is cholesterol, so it turns out that you know, this one gene is you know, it's well studied in the literature for cholesterol, and it, you know, it's highly expressed in liver. So, you know, it's a good candidate for a causal gene. Now, obviously, not every locus will look like this, but it does, the knockoff framework does have this ability to To through this conditional testing to get you closer to causality. And yeah, so I'm out of time. So I just want to acknowledge people from my group who worked on this. So Zihuai used to be a postdoc in my group. He's now in Stanford. And then, you know, Ling Shi, she's in Pittsburgh. And the other people who are in my group that contributed to this work. Thank you. Thank you. Okay, thank you. Okay, so I guess when we transfer it to the talk online, we can do some questions for you, Rena.