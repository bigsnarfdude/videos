Well that's kinda equivalent to a fully based thing because we always model the joint idea because we don't have a prior anyways. And so uh we do the f joint thing, but we have the same kind of doubts about you know second guessing the calibrators and so on. But then the onus is on the analyzer to basically choose a summary system. So basically to amputate their data so that it like if you do the full base thing, but it it actually like practically it doesn't update the image drivers. Explicitly choosing a view of the data of your second stage data so that it's not able to inform the user drivers. So is this something you study this with? It sounds a little dodgy. I mean, whenever I mean this is statisticians, right, toying with the data to get your results. Well, it's like a statistics. I know we do it, but it's one of the things we're not supposed to do. No, no. What he's saying is that you can, for instance, make broad. That you can, for instance, make broader bins so that you can reduce the sensitivity, or you know, you remove the information. Right, so it's a choice you had to make anyway. It's not like you're taking this data and tossing it. So it's like a coarse draining of the data so that it becomes less sensitive to, or it's not as powerful anymore. Yeah, I know I haven't thought of it. Interesting. So when you look at this cluster and its parts, you will describe it. Part you were describing. This is essentially a fuzzy cluster. And then basically, the question is: how do you choose the membership function? Because the image of the galaxy from, let's say, in the plane of the Milky Way would be very different if you would look from the perpendicular point of view, right? So do you have an uncertainty of that, or do people provide you with those membership functions? No, so we just. No, we have the point spread function in the instrument. No, but you're clustering it, you're clustering, at least the image, you have a cluster of galaxies, and you identify 14 galaxies in the cluster to which you assign the 14 galaxy. So you're thinking about if it were, if it were, because it is three dimensions, there could be. Right, exactly. So each galaxy could look different depending on where you are looking. Different depending on where you are looking from. So, and yeah, so just because these are not galaxies, they're stars, so they are point sources from what we're doing. So, you have footing stars in your K or Foutin plot? Because if I'm looking at this illustration here, this is the Orion Nebula. Okay. So, it's something in our galaxy, the stellar nursery at the core. Okay. So, maybe I don't understand what those 15 so. Maybe I don't understand what those 14 sources are. What are they associated with? They're stars. They're just only 14 stars in this city. Yeah, well, sorry. We're looking at this little teeny corner. Oh, no, center. Sorry, yeah, that's good. But at point source, it's not a problem. So it's not really clustering in the way I saw the path. No, if you have to start thinking, clustering, if you were thinking about something that looks like this, that's a lot harder. Something that looks like this, that's a lot harder problem. Is it the structure, right? Like when is it a new source, and when is it just a change in the source itself? Yeah. So you talked about this fragmented Bayesian approach. And I was wondering, like, could you do a frequent dispersion of that in the sense that do the same thing you're doing or sample from the prior, but replace the posture with some kind of frequent? Replace the posture with some kind of frequency confidence set or a parameter estimator or something like that. And basically, what you would have as a result is kind of a sample of frequencies estimators that would kind of reflect sort of a Bayesian answering on the missions parameter, but you still have for each sample in that ensemble, you'd have sort of a frequency simplification for that. That would be some kind of a combination of hybrid of frequency and Bayesian approaches, but have you the first thing I would think about is. The first thing I would think about is this OPAT has, because OPAT is frequent just in principle, right? And there is this version of it, but also this other version where it involves sampling. Yeah, so what I'm basically saying is that you would replace the second part there with some kind of a base and say, I'm sort of some kind of a frequency, let's say, like a point in a center. Right. So you would use a different notation for this, right? So this looks like an 77 post 2. Is this what you mean? Is this what you mean? Yeah, so that would be fine. So you still sample like the noises from a prior or like a poster from a different analytic nuisance. And whatever frequency analysis you do will depend on that noises parameter. Like let's say you construct a confidence interval for a fixed value of A, and then you just have an ensemble of those confidence intervals depending on A, and you just basically sample A's from some confident distribution, then you have a distribution of confidence. That's like, would that be? Right, it's kind of. Right, it's kind of like what Bob was describing, right? Because you could just say you're integrating over the nuisance parameters, but you're integrating over the prior. Yeah, exactly. You wouldn't integrate. The result of your analyze would be that whole ensemble of frequent solutions, depending on the mission. So you would get a further picture. Yeah, but there was that picture. I think that exactly. So, imagine each of those would be a different. Like you know, imagine each of those would be like a frequent risk confidence set. That's what it is. And uh um and and and then you look at this and use that as like a way of communicating the result of the analysis. Would that make sense? I mean, yeah, you want to think about what it means, I suppose. But like if you try to take this thing as this, you know, I just there's a thousand here, and I don't have 25 here just because like, you know, a bunch of red. But so there's probably other So, there's probably other ones here, for example, so it would fill in better. I guess you could, it's just not clear to me how to decide what was the 95% line or something like that. Right, right. There's like a question of how you visualize and connect it. But I think what it means, I guess it basically means that you know that one of those would be like your correct confidence site, but you don't know which one. So, you basically present like a a base and posture of of confidence that's in a in a sense you I want to connect back to the particle physics and to the good, bad, and ugly uncertainties. As many over simplification could be that you could use the full basic approach but better for the good system I'll just give you an example where I I'll just give you an example where that's problematic. I think like in any measurements you do at C, this modeling uncertainty from theory. So like for instance there's a so-called normalization scale and funk. I mean this is a deep problem, right? I mean I don't remember we tried for stamera data to strong upping constant and also computing at the same time the scale, the normalization scale as a nuisance parameter. Parameter. Then it was like, no, we are not allowed to do this. Because it's just important. The idea of a model training like that is just forbidden. Because this is one of these ugly uncertainties, we call it ugly, right? So it's just, because we don't know what's the correlation structure, these are kinematic ranges for the hymnization scale, you know, and so, right? So there are cases where it's just, we don't trust the model. I'm adding just one loosence parameter here when we need several of them. Is this correct? That's just correct, what I say? Yes, I mean I probably got this, yes. You have time for who should drive, right? Maybe. So this is uncertainty in the theoretical model. Is that what that is? Yes. It's the it's the fact that the thing that you're comparing with I mean I calculate some theory prediction, but if that prediction itself is incorrect, I yeah. Incorrect. I never know the exact theory. I make approximations to calculate things. Sorry, does I want to say it enters the measurements when you use the template for the theory? Yes. To use the theory as a model to define the template. Yeah, so I have another problem, actually the one that I didn't want to tell you what it was about. That is atomic physicists, and they tell us if I have a plasma at a certain temperature. Plasma at a certain temperature and density, where do the ions, or the electrons live in what shells, and with what probability? And I need that if I want to look at the plasma, see where the lines are, and learn about the density and temperature of the source or the star. And so we went back to them because they have uncertainty. I don't really understand what they're doing, right? But I know they're doing theoretical calculations combined with some computations, and they did come back. And they did come back to us with these sorts of uncertainties, that is to say, replicates of what it could look like, which we folded in the same way we did the points, the effective area curve. Two hands raised up there, Henry? So I I'm not sure I can express myself the way Roger expressed himself very eloquently. Expressed himself very eloquently. I'll just ask: do you have a quantitative measure for overstate? Overstate uncertainty, do you mean? No. What? I'm going to have to remember the context. Oh, wait, it tends to overstate. No, I mean, you can, like, now point. You can, well, what's an example? Was it the sound of. Yeah, yeah, yeah, but this is what it's showing you, or this one is what it's showing you, right? You know, so what would client? So, what would quantitative mean, like to say in general how much bigger this is than this? It sounds like a coverage. Yeah. Do you mean that it's some sort of frequency property of these integrals? Overstate the uncertainty, no. I mean, because these coverage here, you can see that these are not great free confidence intervals, right? They just stretch out and catch the truth, right? They're not. The truth, right? They're not, this is what confidence interval ought to look like. But it's clearly overstating the uncertainty. It's saying the uncertainty is this big, not this big. Yeah, it's covering all the time, so it may be the orange ones. Yeah. I was the one who made that comment to somebody else, didn't I? They are, yeah. Yeah, they are in this case, yeah. So that, yeah, I mean, yeah, so they're overcovering, but that's not exactly what I meant. I just meant the sigma. The sigmas are too big. Are they correlated with each other? These are not. These are independent. This is 30 replicate data sets. So it's a frequency. It's a computational description of the frequency coverage, so independent data sets. So in one plot, they know. 30 independent data sets, 30 independent data analysis. I mean, this one will be correlated with that one because it's the same data set. I I wanted to add something on my maybe make an argument which maybe was not explained very enough, but the Cisterian uncertainties. There's one other case where this is a special one. You know, we often measure an LHG some specific processes, but with so-called jetucial volume, that means we are applying cuts, you know. Like we require a lepton to have the momentum. A lepton dress, a momenta, minimum momenta of 100 GeV, for instance. And then we measure this in this phase space, and then we extrapolate finally. We measure this to the full phase space, but then we use a theory model. Again, there's a kind of general understanding, I think, that for this extrapolation, you use this pragmatic thing. You use the prior uncertainty for your theory, and you're not using this uncertainty which you can have the constraint. You can have the constraint in the division. I mean, you can constrain the uncertainty in this measurement region itself, but you're not trusting this for extra relation. This is also an example for not expressing theory, yeah. Other questions? Can I if we have a minute, I can show you why I was what my what why I don't always want to use a profile. Don't always want to use a profile. We have another five minutes and then we give five minutes just to change speaker. Yeah, I won't even go into it. I just want to say the profile versus it wasn't my reading, right? Because I saw this all the stuff on the agenda. So when your posterior distribution, I mean, you have to get to the asymptotics before you kind of believe the asymptotics. Before you kind of believe the asymptotics. Your sample sizes have to be big enough. And you kind of expect when the sample size is big enough that the posterior distributions will start looking like Gaussian distributions. And I sometimes do see posterior distributions that look like Gaussian distributions. But it's very easy for me to give lots of examples where they just don't. You know, I was, this is a plot that my student showed me last week when I was making slides. You know, just they just come to me like this. You know, if you, it's, you know, Like this. This is not a great example, but it's kind of the example. This one has at least as much mass as this one, but this one is higher. Do I really want to use the profile, the post just plug in? It's not exactly clear. I just want to marginalize either, don't get me wrong. But it is, you know, it's not quite so simple. I won't go into anything. This one's the only one that doesn't come from a physical example, astronomical example. So these little tails. But these little tails, I'd cut them off, but they would go all the way into the next room. These little tails. And they don't have very much probability, but they do make you wonder what's going on, right? Like, why are these little tails in this construction? That's all. Should I be glad that this discussion session is a discussion only?