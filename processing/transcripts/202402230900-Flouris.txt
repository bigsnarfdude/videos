You know, manifold flows and generate the models. So I thought that's a better fit for this workshop. Also, it's like a recently published work, so I think we're much more enjoyable at the CPO than I thought. It's a bit short, I think. So yeah, it's a relaxing Friday morning. Okay, cool. So I start with the motivation. So of course you have this um you know the idea of generation, of how to generate yeah so generate high dimensional How called generic high-dimensional data, and you know, that's an overlapping theme in AI nowadays. More specifically, of course, we have, again, like systems or dynamics like this, but it depends. I mean, this work here is more like it solves this problem, but it's always by just treating the data. There's not really learning the dynamics. And more specifically, we have in my group who work with medical images, so we have some. Images. So we have some generation problems. For example, here we have the lower pack spine image and then we segment it. And then afterwards, we get some sort of low resolution image because it's taken from an MRI, which is like thick slice. And we want to upscale it such as it has a good resolution, which is we can use like high-resolution CT images to train a generate network to actually do that. So as you can see here, for example, in this where For example, in this well, in the recent work we've done, we slice this lumbar spine subvertebra, and then by using a generative model, we can increase the resolution, which is a huge thing in medical imaging, because it means they have shorter scans, less ionizing radiation on the patient, and things like this. It's not a solved problem yet, because this needs to be exactly precise, it's not. But yeah, it's work in progress around the world. And the other motivation. Yeah, and the other motivation is hematodynamics, which was what I was going to talk about in the beginning. Basically, we want to take a marae of deserts in the brain. I want to simulate flow, for example, to predict aneurysms and stuff. In this specific problem, it's mostly aneurysm, but it's hard to fix to predict. But as you can understand, when we get a flow MRI and then we want to feed a classical simulation, this is very hard, like a clinician. Very hard, like a clinician has to go transfer the information to an expert that can use open phone or something and then get the simulation, it's almost impossible. So, you can see the appeal of a machine learning method, where you can just sort of have a pre-trained model that directly goes on the data, looks at the flow video, and generates the prediction what's going to happen in the future, and also can increase the resolution again because again resolution is an issue. So, yeah, so this is where. An issue. So, yeah, so this is where generative models come into play and then also relate to flow and consistence. Just for fun, I show like this. I use a GNN here to solve the Washier stress problem here. I just showed it for fun. I think it's nothing. I just like to show that it's an open form simulation of the yeah, of the of the vessel that goes through a through an ionizing bulge. And then basically we we yeah, it's just the preliminary result from before. The primary results from before. So I just saw the potential of generative methods like this that they can do. But it's still like there's lots of things to be done because the really big problem, and then you have to resolve inside the bulk as well, bulk of the flow, which is a much bigger problem because it's a big simulation. Okay, cool. So now I got to the main part of what I'm trying to explain. So it's not so large. So basically, we were proposing this new method, which is called kinetic and micro value flows. And money for learning flows, which is the derivative of neural nets, and then money for learning flows, and we're proven by Karamicard, which I will explain later what it means. Yeah, let's call it introduction. Okay, so it has already been discussed, and we mentioned it a few times, which I was happy to hear that. But yeah, there's this idea of manual hypothesis so that high-dimensional data can be resembled in low-dimensional space. Dimensional space. Finally, there is lots of work going on about this. We've seen the more bigger simulations of fluid dynamics where people will use a decoder, for example, an autocode system to reduce the dimension. In our case, it's a bit different. It's similar but different because we'll talk about manifold limit flows, which are injective flows, and that we said the probability testing uses. That we said the probability testing gives on a lower dimensional manifold, which is embedded in the high-dimensional space. And the difference here is that our method is dependent on this embedding into the lower dimension, so in this manifold in the lower dimension, for generation. It's not just, it cannot be just a marketary auto-encoder, which is in the dimension, do something with it, and then increase the dimension. So the whole method is based on this. Okay, so that's a bit of a small difference, but yeah. But then in general, it's similar. In general, it's similar. So, then the idea we have to learn whatever the manifold data leaving the lower dimension. Fine, there has been some work that they, as I think it has been shown before also here, that the manifold can be known before. For example, if it's like the, you know, what's called the plasma simulations on a torus on a sort of a torus, or the weather prediction data outlook of the Earth, then you know that the data somehow can live on a torus, on a sphere. On a torus, on a sphere, and then you can model that as the electron manifold, for example. If you have that, then calculate on these probability densities. Actually, sorry, maybe I have to say that calculating probability density is kind of a big thing in this field because that's what allows you to give to generate from the samples from the prior distribution. It's what allows you to generate. So, calculating is a big thing. So, if you know the So, if you know the manifold, then you can know how to go towards it and away from it all the time. Then you have your metric, you construct an emetric, and you can do it quite straightforwardly. In our case, okay, this is where it gets interesting in terms of like novelty somehow, is that we solve this company and then we started realizing that there's some lots of learning pathologies going on and the methods that exist are not, you know, they have, they're not so. You know, they have, they're not so clean as they can be, and there's lots of repetition of information, and the latent space is not sparse at all, and it's somehow degenerate. And then we realize by looking into virtual machines and PCAs, ICAs, we realize that maybe we want to encourage sparsity and local functionality. This has already been addressed before, like the, I forgot his name, that he was discussing about money for lending. He was saying, I want to. Manifold learning, you're saying I want a PCA and then I mix that with my auto-encoder as well, then I get something in between. This is a bit similar, just we make the distinction that we don't want fully a PCA, because PCA is a linear transformation, and that's not expressive, it tends to be not expressive in terms of generation. Actually, it's not expressive at all. We have like a linear transformation for generated network, it's not gonna do very well. So we have to solve we have to make it So we have to solve, we have to make it better, but at the same time be careful on keeping the interpretability and occupy, okay, this is already somehow prerequisite. But we want to make sure it's a non-linear transformation more or less, or it is allowed to have a non-linear transformation, and so it can be a good generator of images. Okay, so both of the commonality, but also we have to keep it very expressive. And that's the, you know, kind of the game we're playing here. Maybe I. Maybe I'm okay. That injective flows are more expressive than normalized flows, but I think I will explain later. Okay, good point. Now I go to the more specific motivation. I saw how I tried to explain just now while moving slowly. Okay, so basically we have a data here in two dimensions, and then this is just learning this data space. This data space, which is the black points and just a fuzzy learning point. So basically, we observed, if we try to generate the network of this class, these money for learning generative networks, we observed that the latent space, when we sample from it, here we sample from it, it's actually two-dimensional latent space, the same as the manifolds, the data space. So if we sample from one dimension only, for example, Z1 and the other one we set to zero. For example, Z1 and the other one we set to zero, we get this five. And then we do the same, get this one-dimensional option strain, so it's, of course, a line. And we do the opposite, and then we see again it's kind of similar, you know, it's a sort of normal token, just degenerate more or less, you know, covers the whole space. Ideally, we want it to be one on this axis and the other one on this axis. Because in this case, it's very clear that it has two dimensions to work with. It should split them, you know, perfectly into sort of orthogonal bases, because, yeah. No, because yeah, so we've seen that most methods don't do that. No, they do something like this, which is not ideal. Okay, so not degenerate, we want some sort of PCA. And then also a tracing dimensional estimation, some other motivator. You know, how many dimensions do we need to model this object? I mean, this object is two, I guess. But we're going to see other options later, they need less, and then sometimes the network takes more. So we do have a way of estimating natural. Of estimating naturally the tracing dimensions. And of course, we want some simplicity, general ability, probability, expressivity, efficiency, you know, for high-dimensional data. This is where the engineering part comes in, that it cannot be just pure nice mathematics. It has to have some non-linearities and stuff. Let me show you as well. Okay, so basically, it's also how to make neurologies as generative models work for higher-dimensional data. Okay, thank you. So, I've So, maybe I can go fast through this, or okay, I have some time. So, basically, this is how you use a neural ODE as a generative model, which is called normalizing flow, basically. I think people have seen this, or people will talk about it anyway, maybe. But I just present here what's specific to mine, which is basically how to generate from it. So, the transformation and everything is the same as neural DEs. So, basically, we're saying in normalizing flows, because we have. We're saying in normalizing flows because we have this smooth transformation, we have a diffomorphism, and then that means that it's invertible because of the first logic divided in time. So it's invertible, it's an invertible transformation. Fine. So starting from X, you go into the Latin space and then you come back to your generation. So what's going on here, the name of the game basically, is that once you have trained in one dimension, Once you have trained in one direction, you want to make sure you can transform whatever latent distribution you use to model your data. I mean, most of the time it's just a calculator, just a normal distribution. You want to make sure you can sample from it. And in this case of neural load ES, it's very beautiful mathematically at least, because all you have to do is just invert the transformation, find the, yeah, you have the inverse transformation, you just transform from the latest space into the image space directly, like sort of one-to-one. Of one-to-one. So there's no like Bayesian inference stuff like this. So, you know, mathematically, it's quite beautiful. Fine. So, what's going on here, basically, this sort of methods, is that we're discussing these problems in this workshop. I think most of the problems here were kind of low-dimensional. When you're talking about high-dimensional images, which are like every point in the image is one-dimensional, so it's high-dimensional, it's quite computationally expensive. When you do this, like for a big Celeba image, it's really very, very slow. Is really very, very slow. Actually, most of this method don't really work well. And that's where our method comes in, I think. Because yeah, basically calculating this is hard. That's all. That's what's difficult. Yeah. Yeah, we transform the samples, product line, we approximate the distribution by the network, basically, which is this normalizing flow. Okay. So, yeah. Now I move to this class of options called multiple learning flows, which again are generative models. But the difference now is that. But the difference now is that the latent dimension is lower than the image dimension. Okay, now it's a bit tricky because now, as we sort of can understand, it's basically an ejection, so it's not python morphic anymore. Nevertheless, we can still write things down, so that's what we're doing here. We're writing, we know what we're gonna do here. I mean, we want to go from a next space, the image space, which is in R D, large D, to D, large D, to latent space, which is D is more D. So, how can we do this? People have proposed, actually, we're working on this project as well, that we can just project. Okay, so it's basically, yeah, we just project into the lower dimension, like a simple projection. And fine, and then when you come up, you add a padding, and the padding can be just some trivial embedding, adding zeros into the whatever you go into higher dimensional, you add zeros. So basically, you go down by protecting. You are zero. So basically, you go down by projection and up by some padding. And what's going on here, you have some latent space here. Okay, a project here is my Z, now it's U, but anyway, it's just the Z here. And then you have some transformation in the latent space, that's a diffeomorphism. And then you part, that's the engineering part, and then you transform again. And that's again a diffeomorphism by itself. So these are all like these coupling layers, these lots of engineering going on there. Lots of engineering going going on there, so you can have efficient, you know, normalizing flow in whatever dimension you have. So, there's lots of stuff going on here, but you know, in the engineering sense, but we'll ignore it for now. We'll just say it solves, you know, the neural OD. Okay, fine. And in this case, now you can imagine this Jacobian transformation. Now, it's like the rectangular object. So you can't just take the square root. So it looks a bit different. And this is now hard to count. And this is now hard to calculate again. I mean, not only is it rectangular, it's not a bit tricky, but it's a much bigger object. Or let's say the same size as a normalized flow would have been, which is a full-dimensional flow. And that's okay if it's like just some two-dimensional data, it's fine. But it's highly dimensional, it's hard to calculate. So, what people have proposed in the beginning, sorry, maybe, okay, I'll talk about it later, but for now, let's keep that. You know, we have this sort of You know, we have this sort of flow set up, and it's hard to calculate this Japopian-Japobian product. Okay, fine. Something else that is missing here is that people have, and we and others have realized that actually if you do this with an injective flow, because your latent distribution is a lower dimension than your actual data manifold, when you are trying to do a what's it called, technically, what we call. Uh, what's it called? Take this, what we call here, maximum likelihood maximization. So, here, when you're trying to do this, maximize the likelihood. What happens is it's possible that your your manifold, okay, I don't know, which, yeah, so this is the data, this is the data, and this is whatever manifold you are modeling. So, because of the difference in dimensions, as you can imagine, you know, because you are bedding to something, it's possible they are misaligned. So, if the probability. If the probability distribution, for example, in the extreme case, 90 degrees to the data, we could be very happy to put everything here, put the whole distribution here, because it's going to overlap there. So just purely doing likelihood maximization is not going to work out well in most of the cases. So basically, you need something, so you align the lower dimensional distribution to the higher dimensional manifold data. Data. Yeah, sorry. Align the lower dimensional latent dimensional manifold, let's call it, to the data, to the high-dimensional data. Okay, align, yeah, align the two objects. So basically, a simple way to do it is just to use two laws with a reconstruction error, basically. You have your input data, and then you have the whatever is reconstructed by the whole. Whatever isn't constructed by the whole network, parameterized by fly. And then you have this construction loss. And if you minimize this, it means that they can align with the data. And that's also shown here. That, yeah, you finally align. Naive likelihood, you know, gives this, which is not good. And then using this, it helps align the manifold. Okay, so we arrive to the sort of form of these manifold any flows. So I give you a summary here what's going on. And then you have this total. What's going on, and then you have this total optimization objective, which is basically the likelihood maximum log likelihood maximization. And as I said before, this is a hard object to calculate the whole thing. So the first people that did this, they said, let's ignore all this and just skip this one. Basically, the lower dimensional flow and just ignore it. It actually worked. That's the funny thing with these methods. It worked really well. Even just ignoring this, this train is very. Well, even just ignoring this, this train is very fast, it doesn't have to be it's only lower dimension, it's h in the lower dimension. So you can just generate from using this. But more clever people came afterwards, or whatever, more diligent. And they said, okay, but we can also calculate this. It's a bit more tedious, but we can have some approximation for it. They said we are approximately by stochastic and bias estimator. And then basically, they showed that you can have this sort of what you call rectangular manifold any flows where you have the whole. Manifold any flows where you have the whole, you know, the whole mathematics there. It's a bit dubious in terms of like, because you're still, this thing is still like, it has a bit, you know, some parts are missing, some parts are just zero. But yeah, it works better though. So that's the manifold learning flows. Fine. I don't know if somebody has questions. I don't know. Okay. So, how does this compare with a globalizing global without? Without diminishing the dimension. You have an intuition of what the flow is doing. If I have full dimension here, if my H is full dimensional. If it's full dimensional, I have an intuition of what's going on. It's yeah, it's the same as a normalizing flow. But this one is like an approximation to the normalizing flow? Yes, this is uh this is a norm yeah, it's This is a normal, yeah. It's no, this one is more like a sub-manifold of the data. We model a sub-manifold of the data. So if you think of just a sub-manifold of the data, the one that is in dimension of H, or the dimension of the latent space, that manifold is complete, like it's correct. It's just, but the thing is, the rest of the d the rest of the the dimensions of the data are not modelled by a normalizer flow. They are modelled by an on autoencoder kind of thing. Auto-to-encoder kind of thing. So it's a combination of the two. So part of the data is modeled by a normalizing flow, and the rest of the data is just free to do whatever it wants. Okay, so that's why I say how you can think about it. Yeah, and these extra degrees of freedom that they can do anything they want, they're not diffeomorphisms anymore, they make the network much more expressive. And actually, these methods are much more expressive than normalizing flows for these reasons. So, but still, it's still the But still, there is no inference. We don't do any inference like variational auto-encoders doing stuff like that. So, our inference comes from the sub-manifold of the data. Okay, thanks. That's a good question, actually. Okay. Okay, now we go to our method, which we published last year in Europe. Yeah, so, okay, cool. So, basically, we take We take okay, what we're doing here. Ah, yeah, sorry. Yeah, we define what we call canonical manifold. In our case, this is not just orthogonal, as maybe the name can mean, but there's no clear definition of canonical manifold, I think, in literature. So we'll be free with it. And then we said, okay, fine, what if that object is just orthogonal and those parspaces? Such that. Okay, so basically, it's a manifold that has. Is a manifold that is described by some of the ground bases, let's say, and that can also be sparse. Okay, and that's the object that we're looking for. Okay, and then I will talk about some technicality later. Anyway, how do we achieve this? So basically, we observed that as soon as we have made the hard work of calculating the Jacobian, transpose Jacobian product, basically in this setting, this is just the same as you call a metric. Is just the same as you call a metric tensor, okay? Moving from the image space into the latent space. Okay, the transformation of the chart. Okay, so now we have a metric for this whole transformation, all for free, which is amazing. Because yeah, normally it would be very hard to calculate. But because people went through the effort to calculate this, we realized it's the same as the metric. Fine. So as soon as we have this, we've seen. Have this. We've seen this an interesting thing here because most the first intuition we had was what we've what we what we have a diagonal metric, what we force the metric to be diagonal. And then we realized other people have tried to do similar things. For example, this conformal embedding paper, where it's basically they say, yeah, that this is that you want the ometric tensor to be diagonal one, the identity, and it defines as the And they defined as a conformal map. And they showed, okay, that's good, you know, like it's orthogonal and everything works well. And then, similar to what they call PCA flows, they did something in between. Fine, and that means like, ah, yeah, my basic will be a total. Sorry, my basic would definitely be sparse because only diagonal terms would be relevant. But we realized all these methods are not expressive at all. Are not expressive at all. Basically, you have gained expressivity by having non-diagonal terms in your metric tensor, and then you lose it by enforcing it again to be a diagonal metric. So we realize, what if we just set the off-diagonal elements of the metric tensor to zero? Sorry, allow the off-diagonal elements of the metric tensor to go to zero if they want to. Okay, and the method to do that, basically, we say, fine. We say, fine, let's use an L1 loss, which basically I think people must know this, but because of the form of the L1, it means that some parts will go to zero and then that satisfies the laws. If one of the dimensions goes to zero, it satisfies the laws, but not all of them have to go to zero. Okay, so such laws on the orthogonal elements will enforce sparsity and also some orthogonality. Okay, because we go to zero and blah blah blah, but it's not strictly enforced. It's not like. Strictly enforced. It's not like every single transformation of, you know, from, as I showed before, not like every single one of these, you know, elements has to be zero, but some of them can go to zero, and then the L1 loss will be satisfied. Okay, and I show later what does this mean also. But this is the important crux of this work, I think. This is what we're doing here. And other works, also, the reviewers were discussing this, but many people would do the L2 norm of the diagonal elements, and that's fine, but it's again. That's fine, but it's again in the whole class of constraints or flow that it's not very expressive. So, are you doing this globally or? Locally, at every single point. At one single point, okay? Yeah, I go to all points. May I ask you again, little G data? What is that? You had it in the previous. Yeah, sorry. So basically, yeah, so transforming from the image space to the latent space. Image space to the latent space. So this is that, I don't know, I think you can think of this as a chart, and then the transformation of the chart. So I already have Jacobian of the, of, no, yeah. Of my this network G5. G is the whole thing. I don't know. I'm sure. Yeah, G is the whole thing. I know G here, okay. No, for me, G is the whole thing. This is the older method. Okay. Okay. Yeah, good. Fine. So we're proposing this. Diagram minimize the off-diagonal elements. Okay. Yeah. Yeah. So it minimizes this, as we just said, the transformation, and then it will either reduce this, make a sparse basis, or reduce the magnitude of the dot product, make it more orphana. When this is zero, this is orthogonal. The basis of Fogona. We're going to see the results now actually, maybe become more clear. So basically, we have the previous option. So basically, we have the previous option. I know this is a bit high, it's not, it's like again this optimization objective, you know, because it's, yeah, we could enforce in the network, but I don't know how yet. I don't know if it's worth it. But we do this on the laws. So basically, yeah, we have the same as before plus our terror. You know, the one load that we just discussed. So this is our. Fine. So now I show the results of the previous methods. So previous methods will do this, which I just shown this in the beginning. And then, for example, for the sphere. And then, for example, for the sphere, this is the interesting one. This is a mobus band. I had to make it a bit larger so the deformation is less, so it doesn't crash. So that's why it looks a bit like that. But if it's very close, there is too much. Fine. Yeah, this is sphere. And as you can see, even for the sphere, if I model the sphere with three latent dimensions, they are all like sort of degenerate. They're sort of random and it learns, but it's not very good. Anyway, here comes our method, which is like. Here comes our method, which is like, that was like the moment of happiness when we saw this happening. This type of method is rare to see something like this, and especially in the method that is not telling it to specifically always do that. So basically what it does, it chooses the dimensions that it should use. And in this case, it's two dimensions, this and this one, and it's beautiful, fine. Even more nice, I think, is these examples, where, of course, it's a two-sphere fitted space. Of course, it's a two-sphere and a 3D space. So basically, it decides to take two, like theta and theta and phi, and the third one, it just collapses to zero. So it says, I don't need the third dimension, so I'm not going to use it. Which is, I think this is brilliant. This is like, yeah, we're just, okay, there's a long way to go, but this is a latent dimension, intrinsic dimension estimation kind of thing done naturally. Okay, and the same with the mobus plan. In the previous method, I really solved this because I think it's because of the topology of the. Because of the topology of the space. Yeah, they find this problem hard. But our method, because I think it allocates resources more efficiently, I guess, or I don't know, it does, yeah, it solves this problem. So, yeah, we did the same thing, you know, two dimensions so perpendicular to each other, and the third one collapses. Okay, this is all the beautiful stuff, which is two-dimensional simulated data. We'll go to images now, it's much uglier. Okay, keep this in mind. This is always, but when you This in mind, this is always, but when you go to the real world, it's much up there. Nevertheless, our method works really well. And what I'm trying to show here, this is we have 10 for MNIST, an omni plot. Our method is this, and then the previous method is this one. Now I'm just showing this object, this metric that you asked before, for example. It's this thing here. But okay, I mean here it's like the cost similarity, which is more or less the same thing. And this is just the diagonal elements of it. Just the diagonal elements of it. Basically, what we can see, you know, for fashion enemies, for example, as soon as it's plain, okay, this is an average now, of course. It's okay, it's a bit tricky. I mean, we can discuss how to prove this, but I think it does show the general trend, though. It shows that at least you go, like, you go, now you have much of these dimensions I can use, so you obtain sparsity. And this is not 100% clear from the image, but if you do an average of the process similarity, this is lower than this. Also, you can see the bit, this is more white somehow. Can see the bit is more white somehow than this one, yeah. And that shows that it's more orthogonal. So we achieve somehow more sparse and you know more orthogonal basis or more degenerate representation of the image data and detected space. And this is from and this is constant across different data sets. So our method does this, but does it so slightly as is necessary. And the way to, I think, to visualize this is by this. Like in the two-dimensional By this, like in the two dimensions or three dimensions, whatever, we can visualize like this. In the image dimensions, it's a big discussion, so we don't know what's going on, but at least we get more functionality, which we estimate the more efficient representation of the data. Okay, fine. Another way to see, okay, this is just a, yeah, I mean, it's just an FID score. I'm using, I train something with 40 dimensions in the latent space. This is for MNIST. And then I say, what if I keep only the important dimensions? If I keep only the important dimensions, and important is a big discussion. Here we keep just the biggest one, that's just the ones with the largest average value. We should have done something else, I think. So we can do this in the next work. But we show that, yeah, in the beginning, the most important dimensions, the FID once yeah, FID drops a lot, no, you can say that up to using these dimensions, the important ones, you know, it generates well and you don't need so much the other ones, basically. Don't need so much the other methods basically. Okay, and also here we show a comparison. This is the previous method, this is ours. And I think we choose, yeah, we choose like the most important dimensions again in the bottom, and then we keep samples. Yeah, okay, then we sample from different maintenance dimensions as we've seen in the 2D case. But now it's for the image, and how do you visualize it? No, it's so clear, but we do this. So here, just using the most important dimensions, you know, you have. Dimensions, you know, you have a good generation of images, and all other dimensions are not so important, which is not so clear in the previous methods. Where, you know, yes, of course, the most important dimensions also here, they seem to generate well, but also the other ones, so like the information is stored doubly maybe, and our generators are a bit better than those. So basically, you can think of all the information that is sort of collected here, it collapses to only this direction. It collapses to only this dimension. So it's kind of beautiful. Now, everything, all of these goes here. So much like, you know, just two dimensions needed. Okay. Fine. And then I'm almost done. So I just showed them, as people said, like some people have shown two good results and some people have shown two negative results. We strike the right balance of numbers. So we show that our method slightly occurs in the previous ones, which basically is a Which basically is a verification of what we've seen up to now. Like it looks better, but also it comes in the numbers. We show that the FID scores are improved from our method and at no extra computational cost. And we also managed to work on celebration actually, which this method was tracking a bit. So we managed to even work on higher dimensional data on the previous methods. So that was good results. Okay, and here I saw tabular data, and one of them was a bit mixed because of the training. The training. So this worked well. And this one, we kept running it every time it gave different results. And we didn't want to just say, I use the results that we like. So yeah, we have different runs, exactly. So we sort of investigate why does this one train badly. And all methods here train kind of badly. They go down and then they oscillate. Where the other option, for example, Hermas, it goes nicely. So that's why one of the tabular data which are hard to look at. These are these tabular data, which are hard to work with. There's lots of work with people working on this field specifically. But it's like a benchmark for methods like this. I think people have seen it before. Okay, and again, we showed some improved methods to use inputs. Okay, so that's it. So I just have some conclusions. Yeah, we're still computationally expensive when it comes to higher dimensions. We cannot work with ImageNet, for example, not yet. Because of this neural, these are beautiful, but they're expensive. These are beautiful, but they're expensive. You have to calculate that the Jacobian is expensive. There's again a discussion of, you know, as you mentioned, like the topology determination. So what if your data is one topology and then because you have a sort of homeomorphic transformation, this topology is kept in the written space, but maybe you don't want this, you want to be more expressive. There's all this discussion about topology, the topology of the data, which is a hard one. Which is a hard one. And also, as we saw, like this sort of functionality basis doesn't mean it's always better. I mean, it's our intention to say that it's better, but it doesn't mean, I think, like, for example, coupon representation maybe is better. Like, it would be, you know, it's what describes the system better, for example. So that's another limitation. And then, yeah, there's other methods that work better. These diffusion motors are much better. They are slow, but diffusion motors. Low, but the diffusion models are the best. We cannot compete with diffusion models. But okay, fine, we don't have to necessarily compute everybody, but it's an acknowledgement that there's better generation models out there. So yeah, just to conclude, we proposed this L1 loss of the off-diagonal elements, and I think we estimated that other people can use this when they need their methods, because normally it was just the diagonal term. That gives good results. We can use Yeah, we can use these in other optimizational schemes. This land representations we have in the latent space, as we was already discussing in this workshop, can be useful for feature for other things. For example, this is the auto distribution detection. We show that it has some potential for auto distribution detection. And then, okay, yeah, learn this compact and generate intrinsic manifold basis, improve generation. And yeah, and also, yeah, we'll propose this, it's a way of doing intricate dimensionality estimation. Increasing time estimation with our method. Okay, that's it. This is a picture of the creation. I have kind of a, I forgot who presented for what metric he used in the previous paper that also looked at basically the sharp decrease at a specific dimensionality. Yeah. I forgot what metric he used. What metric you used. Yeah, I was still cool. Okay. Is the FID strictly decreasing? Or is there any way to judge whether that would work for all manifolds? Or is there some case where the FID would increase? Yeah. Yeah, so that's a good point. Yeah, that's a good point. So it's very data dependent. Okay. And also, the plot I show, actually, it's a good question, what he shows. I show, actually, it's a good question. What he showed, because I think I should have done something else, not just get the variance of the latent space, how much it varies, not the average value, but how much it varies. So that would have been a better metric. But we're actually working on how to make a plot like that much clearer. So we can really know where the dimension goes, you know, where the FID goes down, and how you choose these increasing dimensions is the multi-dimensional data. Is the multi-dimensional data. Yeah, but I mean, of course, if you keep it, well, if you keep increasing the latent dimensions, this will go up, but in our method, it should be fine because it should ignore those dimensions, I guess. But I haven't really tried. You know, like the other methods will go up again, and our method should stay low, I estimate. But yeah, good question. Yes. So the it comes to my mind. And it comes to my mind the work that we have done in our department. There are neuroscientists in our university and they have been able to describe the topological format of some data from neuroscience, so-called grid cells, that are useful to map the space that we have in the That we have in the grain. So they published this wonderful paper in Nature, and where they discovered that the grid cells have this topoidal format. So I wonder if with your method you could discover this manifold. Because it's also something that the neuroscientists know, they would believe in a torus, but if you go for higher dimensions or things. For higher dimensions or things which are more weird, then they would not go for it because they have some other way of understanding that these torso should be. And it's extremely interesting, actually. I would like to see that. I'm very curious, yeah. Because there is a big discussion exactly about this. What happened in this big dimensional data? What topology does it have in the lower dimension? And yeah, okay, I'm very sounds really, really interesting actually. Thanks. And then I always wanted. And then I always always wonder with these sort of methods if there is a connection to the dynamic Law Rank approximation techniques. Okay. Christian Lubric has worked that. Okay, I'm not familiar, but I will tell you. Yeah, there is, I think, I estimate like things like this, they have also all the results that I'm curious if people know about because, yeah, we're not really inventing something here, we just sort of apply it. We just sort of apply it. So, yeah, there's lots of things to work on. It's just new, all these things are like two, three years old. So it's lots of trying to still understand them. I had a question about the. So you have this Jacobian term in the last question. Yeah. So you said you have some way to maximize it. Can you describe it? Because it's how you can have large dimensions, how you can see it. Yeah, I'll go remember. I have the details on the paper. I'll send it to you. Yes, I think, I mean, there is. I forgot. I did, I wrote that some time ago. But yeah, I think it's I think it's sort of like what do you call? Like important something? Within it, you're sampling to construct this loss function. Yeah, construct this loss function. Yeah, and then you, yeah, I think some how important something fibers. But I have to go back and show you. Oh, links, curious. Yeah, it's very interesting work. Yeah, that's, I can see it's a problem. Yeah, of course, of course, yeah, it's a big problem. There's many different ways, you know, right, that people have tried to tackle it. Yeah. Okay, thanks. No questions? Thanks, Speaker. Thank you.  Do you not use the cooker? Oh look, what are the laser clicker?