Let's continue with the same topic of model-independent searches. We have Purvasha to talk to us about semi-supervised methods in classifies. Take it over. Hi, good afternoon, everyone. As you can see, I'm a statistician, so I'm trying my best to answer the physics questions as best as I can. So the talk is going to be very similar to what Gaia was talking about. So I'll talk very much. Was talking about along very much the same lines. And the work I'll be presenting today is joint work with Mikhail Ching and Larry from Scalvio. So similar to Gaia, we are basically trying to look for any evidence of beyond standard model in the experimental data. And we are doing this search in the high-dimensional space. So that's one of the important things. And the other important one is obviously we are trying to do this in a model independent. Are trying to do this in a model independent way where the modeling is again in the physics sense. And when I say model independent, we mean mainly independent to a signal model. Okay, so the standard model is still there. So when I talk about experimental data for this talk, I'll just put out some mathematical notation that I'll be using. So you can think of experimental data as coming from one of two processes where background is. Processes where background is going to refer to the standard model and signal is going to refer to anything that is beyond the standard model. And so you can think of this experimental data as coming from a mixture of the background and the signal, where lambda here gives you the signal strength. And again, I'm going to assume lambda is between 0 and 1. And if there are any arguments about this, we can take it to the coffee brick. So we are still going to assume that. So we are still going to assume that. And no signal here implies lambda equals 0 because then the signal component vanishes and so the experimental just becomes the background. And so equivalently you can think of it as the experimental distribution being from the same distribution as the background distribution. So to test for the signal, it's basically a statistical problem where the test can be written as Where the test can be written as lambda equals 0, which is no signal, versus lambda greater than 0, which is there is some signal. And this is technically a two-sample testing problem as well, where you're basically saying that the experimental data and the background data have the same distribution versus they do not have the same distribution. So, let me quickly give the objectives of this talk as well as our work. So, as I said, one of the main things is that we So, as I said, one of the main things is that we're trying to do this in a model-independent fashion. So, we will try and make no assumptions about the signal at all. And the second thing we are going to do, which is the main part of this talk, which is how can we use semi-supervised classifiers to perform this test? And we will also look at interpretability because high-dimensional classifiers are often notorious for like we don't really know what it's doing. Like, we don't really know what it's doing. So, we will try and see if we can get some information about it in the next section. Where so we will do signal strength estimation, which is trying to understand how much signal did we find. And the second thing is we look at this thing called accurate subspace method, which basically gives you an idea of what is the subspace and the data that is influencing this classifier. Okay, so I talk about all of that. So I'll talk about all of that. So just to understand model independent methods, I'll talk a little bit about model dependent methods. So in model dependent methods, the data that you generally have is this background MC simulations and you have signal MC simulations from the background model and the signal model and these are labeled observations. So you know which ones are background and which ones are signal. And then you have the experimental data which is think of it as unlabeled. Which is think of it as unlabeled data. So we don't know which ones are background and which ones are signal events. And you can think of this experimental as coming from this mixture. And as I said, we are trying to test for a lambda equals 0 versus lambda grid equals 0. And even in model dependent methods, they often train a supervised classifier to separate the signal from the background to understand where the signal is. And then you can test it in different ways, like you can perform cuts. In different ways, like you can perform cuts on this classifier and so on. But I'll not go there because that's not part of this talk. But the idea is: in the model-dependent methods, they do train a classifier to separate the signal from the background. And I think Gaia gave an amazing intuition as to why we need model-independent methods. I have a much simpler toy example in a two-dimensional space, but the main idea is that. But the main idea is that if we think the signal is here and we draw a classifier between the signal versus the background, and if your original signal is here, then this classifier is going to be unable to find the signal. So that's the basic simple example, but I think Gaia gave a much better motivation of it. But so that brings us to model independent methods. So, like I said, we won't assume anything about the signal. So, we basically throw away. So, we basically throw away the signal samples and the signal model, and we basically just have the background MC simulations. And then we have the experimental data, which is again from this mixture. And now the trick we use is instead of training the classifier to separate signal from background, we ask it to separate experimental data from the background. So, we are basically trying to find differences between these two samples. And just to make a note here, that remember here in our case, this is a simulator for background events, and PS is basically an unspecified signal distribution. And so basically, we only have access to the background data and the experimental data. We don't actually have access to any of these, like lambda, P B, P S or even the Q, like we don't have access to any of that. We only have access to these two samples. only have access to these two samples. So what we basically have is we have the experimental samples, we have the background simulated samples, and then we have the semi-supervised classifier to separate them and we want to perform this test, right? And so a recent approach, so this two sample testing problem is a very common testing problem in statistics and there are many unidimensional approaches. Approaches, but recently it's been shown that using classifiers to perform this test in high-dimensional space is actually pretty effective. And the idea here is that if a high-dimensional classifier is able to find any difference between these two samples, that means there is a difference, right? So that is the main idea that we are going to use, which is that if a classifier can find a difference between the samples, then there is a difference. Difference between the samples, then there is a difference in the distribution. And if it can't, then we really like we are unable to find the difference. And so again, similar to Gaia's work, we are going to consider first a likelihood ratio test where we consider the ratio of the two likelihoods. And we noticed that you can write the likelihood ratio as basically a ratio of the densities of the experimental data versus the background data. Experimental data versus the background data. And we realized that what we can do now is, even though the lambda is sort of inside this Q here, instead of estimating Q and P B separately, technically what we need is just the ratio, right? So what if we estimate the ratio directly instead of trying to estimate these two high dimensional densities individually, which is a much harder problem. So what if we just instead estimate this ratio directly Instead, estimate this ratio directly using a classifier. And that's basically what we're going to do. And the reason we can do this is because of a simple Bayes rule. And the idea is that you can write a probabilistic classifier output as a function of this density ratio. And so if you just flip this equation, now you can estimate the density ratio using the probabilistic classifier output. And you can get the simple likelihood ratio test statistic just. Likelihood ratio test statistic just as a function of this classifier. Okay, so now we are going to propose. So, as Cam mentioned again, that if we had a simple versus a simple hypothesis, then we know that the likelihood ratio test is the most optimal, it's the most powerful test. But in our case, we don't have a simple versus simple, right? So, we should technically be looking at other test statistics as well to see if something has more power to detect this difference compared to the To detect this difference compared to the likelihood ratio test. And so, in that direction, we now use this idea of if the classifier has a good performance, if it's a good classifier, that means there is some difference between the two samples. And so, if our classifier behaves like a random classifier, which is how it should behave if there is no difference, then if you look at this ROC curve, so which gives the false positive grade. So, which gives the false positive grade versus the true positive grade. And if you look at the area under this ROC curve, then for a random classifier, the area under the curve should just be 0.5. And if it is a good classifier, which is somewhere over here, then it should have an area under the curve greater than 0.5. And so that is what we are going to test out. So we are going to use the area under the curve statistic and say that under the null, when the two distributions Under the null, when the two distributions are the same, then technically the area under the curve should be 0.5. So, under the null, it should be 0.5. And under the alternative, when they're not equal, we should get a good classifier and so the AUC should be greater than 0.5, right. Similarly, you could also use the misclassification error of a classifier, and again, under the null, it should be 0.5, which should be just like a random classifier, and the error should be less than 0.5 if it is a good classifier. If it is a good classifier, now comes to the question of calibration, which I know has been a very important part of today's talks as well as some of the previous days talks. So how can we actually calibrate these tests to control the type 1 error? So we could obviously do the asymptotic test, but along with the asymptotic test, we also look at the non-parametric bootstrap test and the permutation test. Test and the permutation test because we don't know how good the asymptotic test is. So, for the AUC, so let me just show you an example. So, just for the AUC, I'll show you what it looks like. And so, if you look at this paper, it basically gives a very good expression of the variance, which is very easy to calculate. And it shows that this basically the AUC statistic follows: like, if you create a walled type of statistic out of it. Walled type of statistic out of it, it is asymptotically normal. So it's a very simple asymptotic test. For the likelihood ratio test, again similar to Gaia, we do end up having a little bit of a problem. And so in the likelihood ratio test, especially, we recommend using these other tests. And so let me quickly now show: I finally added a few slides to explain what the non-parametric bootstrap and the permutation test do. Test to. So before I go into that, let me emphasize that under the NUN, the background data and the experimental data is supposed to have the same distribution, right? So technically we should just be able to replace them, right? Because they have exactly the same distribution. So we're going to use that idea to perform the test under the LUM. So this is the usual way we find the test statistic, which is we split the data into training and is we split the data into training and test data. The training data is used to train the classifier and then we use the test data to find the three different test statistics. What Bootstrap does now is we keep the trained classifier as it is but we resample with replacement just the test data. So in this case similar to there you could be using like other data as well to calculate this but in our case we just decide to resample our existing data itself. Our existing data itself. So you resample this with replacement and then you randomly give it labels of background and experimental data. Because if they're from the same distribution, it shouldn't matter. So you randomly assign, okay, you're a background and you're an experimental. And that's how you get a series of bootstrap test statistics. You create its distribution and that will give you an estimate of the null distribution. In the permutation case, what we do, and this is something that statisticians use a lot. Something that statisticians use a lot, but I don't think I've seen permutation being used in the particle physics world so far. I'm not sure if it is popular at all or not. But what permutation does is it resamples without replacement. So it's basically the same data, but you're randomly assigning it experimental and background. So the idea is if the two classes have the same distribution, you just shuffle them around between the two classes, compute the test statistic again. Is compute the test statistic again, and that should give you the null distribution. Because under the null, they are the same. So you can just shuffle them around. We also do an in-sample version of the permutation, which is slower because what we do in this case is we also train the classifier repeatedly for each of the resample data sets. So for each of we resample the entire data set itself and then with the resample. itself and then with the resample data set we again train the classifier at each permutation step and we compute the permuted statistic. And the idea here is that two things. So one is we are using the whole of the data to train the statistic, to train the classifier. So we should have more power to detect the signal. And second is that we are also training the classifier for each of these steps. So you are sort of incorporating the variability in the training process. The variability in the training process as well into the error that you're incorporating into the test. The only disadvantage of it is that it's extremely slow because you're training the classifier for each of these iterations. Okay, so now I'll quickly show a couple of results. So these are results on this Kaggles-Higgs Poisson machine learning challenge. And basically, I will show you information. Will show you in 50 experiments for each of these signal strengths, whether the methods could detect the presence of the signal or not. And so, this is in percentages. So, what percent of the experiments found a signal versus which ones didn't. The first three rows, so this first section are the model dependent ones. So, they have access to the true signal model. And the next few rows are the ones that do not have access to the true signal model. That do not have access to the true signal model. And we see that basically, and this is for like different signal strengths, and yeah, so these are percentages. So you see that the slow permutation method with the AUC test statistic has the best performance among the model independent methods. And you see that the performance is not that bad compared to the model dependent ones either. So we are not losing that much power by not having access to the signal model. Access to the signal model. Of course, the main motivation is: does it have power to find misspecified signal? And as we expect to see, the model dependent ones do not have any power to detect it, but the model independent ones still have power to detect it. So, this is exactly the reason we even did this exercise. And this is to show what happens if we had more samples. So, in this case, So, in this case, we were basically constrained by computational power. So, with n here as 2 into 10 part 4, if you take 100n, we didn't have the computational power to do the bootstrap and the permutation tests. So, these are only the results for the asymptotic tests, because for those, you don't need to do all of those iterations again. And so, you can see that as we expect to see, the power increases as you have more samples. We also see Samples. We also see that we have power to detect like 0.005 signal strength in this case, which is a good thing. And that was the main point of doing this exercise. And okay, so finally coming to the interpretability part. So remember again that the semi-supervised classifier doesn't have signal labels, right? It cannot access signal labels. So it cannot classify something as a signal. It can only classify something. As a signal, it can only classify it as whether it's background or experiment, but it doesn't know where, like, which ones are signaled precisely. So, it's not so simple to basically understand what is affecting this classifier or what is it even finding, is it finding something? And also, like, what the signal strength is. So, how much signal did we actually, like, how much signal is there in the sample? And so, for that, we did this signal strength estimation technique. Signal strength estimation technique using this idea of Neyman-Pearson quantile transformation. So I have backup slides if people are more interested in this, but for now I just wanted to say we have a way to estimate the signal strength. And here what you can see is these are bootstrap standard error bars for these estimates. And what the active subspace method does is we use this idea of active subspaces. We use this idea of active subspaces to identify combinations of input variables that most affected the classifier and so which indicate signal versus background. And so let me just interpret one of these graphics for you. So these vectors basically capture the variable dependencies that influence the classifier. So if we look at just this last plot over here, I don't know if you can see the labels here, but basically these four variables. Basically, these four variables that it picks are the five variables, and what it's saying is that these four five variables, if they are different together, like if they're increasing together or decreasing together, it indicates the presence of a signal. Like that is what differentiates a background sample from a signal sample. So, what are these five variables defined for the output? So, to take care of the rotational invariance, we sort of subtracted all, like four of them. All like four of them from one fixed one. So basically, we fixed one to zero. So technically, yes, it's like the difference between the tau variables in some sense is what is between the five variables is what is affecting this. With respect to leading jets, so it's basically. Yeah, so with respect to leading jets, so we fixed the leading jet's angle to zero and took everything else as a difference from that. Of course, it's a normal speed correct. I was wondering. Was wondering. Yeah, yeah, it makes sense. I mean, it's just four instead of five. That's a good question. But basically, like what I was trying to say is, it does seem to pick variables that have physical interpretation. It's not just picking random variables. It is giving some information that makes sense. And so, just to wrap it all up, I spoke about a lot of different things. So, to just put them in one sort of a figure. Just put them in one sort of a figure. So we have this probabilistic classifier that we're fitting on training data. We're computing these three different test statistics. We also compute the score statistic for the model dependent one. But for the model independent one, it's not easy to estimate. So I won't go into that. Again, happy to answer questions if you have any questions about that. And then we, on the other hand, we are also doing this estimating the null distribution, which is for the calibration of the test. Distribution, which is for the calibration of the test. And then we perform the test, compute the p-value, and then this is the signal detection process. And then we have another branch, which is the intercredibility branch of the classifier, which is, does this classifier make sense? How much signal does it find? And what are the variables that okay? So far, I haven't spoken about systematics at all. And so I want to point out here that. So, I want to point out here that all of these methods proposed here assume that the background simulations that we have come from the true distribution, but obviously they may not be coming from the true distribution. And so they are likely to be systematically misspecified. And so, are the signals found true signals or differences between the true background and misspecified background? And the answer is right now we don't know. I think Gaia's work is the only one so far in. Work is the only one so far in the model independent framework who have done like some form of background systematics. We haven't done it so far. And just to make a note here, we can still use these methods to sort of identify and characterize regions of high-dimensional space where the background may be mismodeled. And we can also perform pilot analysis with these models to guide future model-independent searches. The important point here is that moving forward, we want to incorporate systematics in these methods. So, ideally, you want a null hypothesis where your experimental distribution is like the background with respect to some nuisance parameter should be your null. And the alternative should be that there is no nuisance parameter such that this is going to work, like it is in the set of background possible distributions. And this is an open problem. And this is an open problem when it comes with respect to our methodology, and it needs new methodology. So, as I mentioned, Gaia's group makes a very significant contribution in this direction. And I just want to make, just want to add here that it's not that simple to understand how we would incorporate the systematics into our AUC statistic or the MC. AUC statistic or the MCE statistic because they are more like classifier-based statistics. So we have to think about how to incorporate it into our classification process itself. So it will require more thought and more statistical new methodology, which is super exciting. And so to conclude, we do this model independent detection, and they may have more power to find unexpected or misspecified segments. Specified segments, and we use semi-supervised classifier tests in order to perform this test. And we explored, like, not just the likelihood ratio test, but we also looked at these two other test statistics to see if they have more power than the LRT. And in case of the Higgs Poisson challenge, the AUC and MCE had more power than the LRT. So it just shows that in some cases maybe likelihood ratio test statistic is not the most powerful. Statistic is not the most powerful. So, it is important to try other kinds of test statistics. And I would definitely put the score test statistic into there. It's a very good tool. And finally, we also looked at different kinds of calibration methods. So, not just the asymptotic one, but non-parametric bootstrap and permutation. And we actually saw that the slow permutation actually had the most power, right? So, it shows that there is. That there is reason to try these other calibration methods. And finally, in the interpretability domain, we looked at signal strength estimation and active subspace methods. And the open question is how to incorporate background systematics in this work. Thank you. So, this is the paper that has all of this. If you want to go look into it, it radio. Okay, great. Okay, great. Thanks a lot. We have maybe one or two questions on the talk. You go ahead. Okay, could we please go on to slide eighteen? Slide eighteen. Yeah, so in hydrogen physics, the typical what we call the discovery criteria is called sigma. That means that your Coulomb in the zero Coulomb should become by a factor of a hundred thousand smaller. Of a hundred thousand smaller. Is this possible? So we are doing this in the style. I mean, no, we would have to do way more permutation cycles, way more bootstrap cycles, and need more data. So, like I said, we were sort of constrained by our computational power. So, the samples we had were of the order of like 2 into 10 pi 4. So, five sigma discovery is impossible with this thing, correct? No, it would be possible. We just didn't do it. Like, you would have to do more, you would have to do more permutation. You would have to do more permutation and bootstrap cycles. Like, you would have to do like 10 par 4, 10 par 5 number of cycles. We just did 100. Also, if you point for the watchers, I don't think anybody else who does money and the kind of stuff does five sitna. Maybe Gaia actually. Maybe you don't know what level of significance. It depends on some of the desks we use 100 toys as well, but the thing is that you can always. Well, but the thing is that you can always add more toys after these weeks to increase your day, right? Yeah. So what we usually do is that we do a reasonable amount of toys that also constrain by computational power that we have disposed of. So technically in this case you would just have to increase the number of, like, you would just have to increase the number of bootstrap cycles you do. And in this case, we don't need toy data, so you just need to resample it that many more times, if that makes sense. If that makes sense. Are you going to hit some problems when you resample a finite sample of data that you're not going to be representing the true fluctuations that you have in real life at the level that you want, like 3 times minus 7? Because when you start with the final example, even if you go stop until the end of time, we never hit that is true, but I think in this case, we. True, but I think in this case we might have big enough samples to do that. Like the bigger the samples you have, like the bigger the size of the samples you have, the lesser of a problem that is. But yes, that is true. That is definitely true. Can I follow up on the question? I'm going to start.