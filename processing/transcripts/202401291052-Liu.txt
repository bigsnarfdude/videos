Hi everyone, well thank you Tanya for inviting me to this workshop. I really enjoyed it. I appreciate the opportunity. So today I'm going to talk about a project I'm working on with my colleague in power engineering. So this work is motivated by a real world problem by the large black cow in twenty twenty one. So it's a power outage. So it's a town outage impacting 7 million people in California and Arizona. So it's down here. So there's a large area of power outage impacting these many people. So how do we get to this problem? So I just learned that a series of cascade failure can lead to these loss of power generation. And in the power engineering expert, they said adequate flexibility could have prevented this kind of outage. So, here is something I learned in the last year. So, it's about power grids. So, this is how. So this is how we get our power supply in our house, in our office, and the place we spend our time. The power supplies are operated by this large power grid network, so it's a very complex infrastructure hardware by the government. There's a lot of uncertainty in these power supplies, so we kind of group them into static and Book them into static and dynamic features. So, those static features will include infrastructure facility, hardware, and renting resources. And those dynamic features will include wind, solar, energy resources and exceed renewable generation, like uh and also demand response and direct um load control and more more than that. More more than that. And there is a new definition and a new concept of flexibility in power energy. And in this new definition, there is a rich literature defining what is flexibility in power supplies. And among this literature, we find that these are two, the definition by these two papers are most important. These are two papers are most inspiring. So, how do we define that? We adopt the definition that the ability of a smart group to deploy its resources to respond to load or generation variation. So that's how we would like to define the flexibility. But the novelty in this project is we kind of formulate flexibility as a binary action. So we define it as a zero. So, we define it as a 0, 1 binary decision problem. So, that by doing this, we will be able to provide those power operators with a measure to decide whether flexibility is needed or needed to ensure smart grid reliability. And in terms of the more macro level, so we will try to provide those policy makers to come up with some evidence and practice to in some evidence and practice to improve those infrastructures so that we can better so that they can better serve the general public. So that's our goal of formulating this problem. So because of this way of modeling the problem, we are kind of trying to ask the question, does the higher power system reliability will be solely due to operating the smart grid? Operating the smart grid without flexibility. So, this is essentially asking a causal question: like, what reliability would have been if flexibility had been alternate? So, in general, this is similar to the question, say, a change of policy or treatment cause a change of outcome response. So we know that this is a general setting of the causal inference problem. The causal inference problem, but unfortunately, it's our good large inference network. We are not able to do experiments on our household or our office. So, this brings in a practical challenge in analyzing this problem. So, and other problem of analyzing this data set is that we have large-scale data coming from multiple data sources. The first one is we have data coming from the We have data coming from the renewable generation power. So, this data is generated from the solar forecast arbiter from the Department of Engineering. So, I'm still trying to understand how the data generating process goes in this problem. And the second source of the data is the low data. The low data will corresponding to the regenerable. To the renewable generation data for specific weather patterns. And the third source of data is the system reliability data generated from large scale of those grids. So when we drive in the highway, we see those are like a large thing, like a bird. So that's the power grid that generates the data. And what I learned from my colleague is that. learned from my colleague is that the power grid power grid reliability depends on large large number of factors. So in statistics we define that that's a high dimensional covariant problem in the data analysis problem. And also when they try to make the flexibility strategy, some of the data, they make the decision based on the observed data and the data could be inaccessible. And the data could be inaccessible or error-contaminated. So, in this talk, in this project, we try to address these two problems. So, one is that we try to model the reliability, which depends on high-dimensional covariance, and the flexibility and the flexibility policy based on the observational data which might suffer from measurement error. From measurement error problem. So let me define the notation first. We observe the data Y, A, G, and X star. So Y here is the reliability, which takes value between 0 and 1. So it's a double-bounded observational output. And the larger value, it means a higher reliability. And flexibility is a binary action. Stability is a binary action as we see here. It takes value from fuel and one. And when A is one, that means the power grid is operated with flexibility. And when it is operated with flexibility, we will observe reliability, y1. And when the system is operated without flexibility, we will observe y0. And we did not get to observe most of these. And we did not get to observe both of this y1 and y0. So we have observed this y, which is a combination of this case. So here we define Z as the static feature which is error-free. So that means we are good. Yes. Sorry, my answer to this for you. Can you give me an example of what flexibility looks like in practice? Yeah, so that's a good question. That's a question we are trying to answer. So the definition we defined in, the definition we adopt in the power engineering is that it is the ability of a smart group to deploy its resources to respond to those low or generation variation. But it's kind of difficult to understand, to model what is the ability. So we try to define it as a binary action. So either we're making the system flexible. with making the system flexible so it can be react to sudden react to actual demand or power or maybe an uncertain wind or power or solar system those are things so it's a final reaction and then we let X to denote those are dynamic feature Dynamic feature, but the true dynamic feature is X, but we do not get to observe its true value. So we observe its surrogate version, which is written in this X star. And our goal is to estimate these causal effects. So that's the, we denote it as power grid. So we kind of model a power grid problem into a causal inference problem. And in order to make this a causal In order to make this causal question identifiable, we adopt these three standard assumptions, which is commonly assumed in causal inference. So that's consistency assumption, positive treatment probability, and the no unmeasured confounding assumption. The first estimator we propose to we yes. So the potential outcome is the one we define as that's. The potential outcome just mentioned is the same as vision in the first error. Oh, yeah, when the system is operated with flexibility, then we will observe y1. And when the system is operated without flexibility, we will observe y0. But we did not. That's the same as the two. Yes, exactly. And we try to estimate this causal inference using the inverse probability weight estimator. So I think this is quite familiar to the general audience here. So I'm not going to waste your time. And then in here, this pi, pi i, that's the proportion. This pi, pi i, that's the propensity score. And what we try to do is here we have observed this A, which is the flexibility, and Y, that's our system reliability. We observe both of these two things correctly. And pi i here, it depends on the true covariant xi and zi. But the problem is we do not know, we do not get to observe the true xi. So it comes with So, this will lead us to the measurement error problem. So, measurement error can be anywhere in our data collection procedure. And in the power system problem, the data comes from multiple sources. So, measurement errors are kind of a big issue in the data. So, if we ignore the measurement error, it might lead to misleading results. Lead to misleading results. And most of the existing methods we find in the literature, they try to address the measurement error problem in the association analysis, but not that many literatures study measurement error in the causal inference study. So you have the case in what's the XI will be? In my application, the XI represents those dynamic features. So it could be wind power generation. Wind power generation and those are solar panels like those in our roof. So, how much energy they accumulated during summer days and during those windy days, things like that. And there could be more. Oh, DI, that represents the infrastructure facility, like those power grid, how many bus they can have. Bust they can have and how many nodes they have. Well, I'm still trying to learn the power good thing, so many of the terminology is still new to me. So, may I have a question? Could you go back to your MR confounding assumption? Sure. So, why not you assume this is a condition of X star and Z? Yes. Uh-oh, here, why not X star and Z? Okay, because Because we think that the policy, the flexibility policy, should be made based on the true observed covariate. But the data we have is surrogate version X star. So here we still make the assumption that the conditional, the unmeasured conforming, the conditional independent assumption is conditional on the true covariant, not the observed version. Not the observed version, and we try to use the observed version x star to estimate the density score, the flexibility strategy. Yes. The u will be assumed to be independent of x of z of this u here. What would be the assumption on u Here we only make the assumption that u follows a normal distribution here. X independent of Z. I don't think we make that assumption in our setting here. So we did some literature review on measurement error in console inference. Error in console inference. So, I would like to bring to your attention that these are some useful monographs on measurement error literature. So, the first one is studied from the linear case and the non-linear case focused on application. This one is also application, and there's a lot more literature on that. And in terms of causal inference, we find these are three papers that are mostly most clear. Paper that are most related to the problem we are trying to address, but we also find some limitations there. So, in summary, we found that most of these existing literature, they try to address the measurement error problem by using multiple imputation or symax type or regression calibration. And most of them focus on numerical study, but they do not provide a synthetic. They do not provide a syntactic result. So, here is what we propose to do. We propose to model the propensity score using this model. So, where this H is a logistic distribution function and theta is some unknown function of this gamma transpose Z. So, this Z is error-free, and this X is that might contain some measurement error. That might contain some measurement error in the data. And then, because of the way we model this propensity score, we can directly use the result from this Stefansky and Kerro's paper. So this delta is a complete and sufficient statistic for the true covariate x, so that after some sort of calculation, we come to this model. this model. And then we propose to estimate, we propose to estimate the unknown parameter beta and its and gamma and the theta using the estimating equation approach. Well here our major focus, our major interest is this beta and gamma and theta. That's we call them a Newson, we call them, they are not our major interest, but we will require this two But we will require these two quantities when we estimate this beta. So we propose to use the geometry approach proposed in these two major literature, that we derive the estimating equation using the efficient score. So that's just some derivation. And then similarly, we can derive the estimating equation for this gamma. And when solving for this set, And when solving for this set of equations, we will need this conditional exploitation, and we will also need this theta prime. And somewhere in this equation, we also need a theta quantity, which is not sure, but trust me, it's there somewhere. So how to solve this conditional exploitation? Because we do not observe this x in another. These are X, and another thing we don't know is this conditional distribution. So that's kind of difficult to calculate. So, our strategy is: we propose a working model for this eta, which is the conditional expectation of X, given its complete and sufficient statistic and Z. And then the way of doing this, it's more computationally efficient. Efficients. And we do not need to compute the conditional expectation. And the good thing is that by using this working model, we ended up having a locally efficient estimator for the beta. We summarize that in the theorem. And to deal with this unknown function theta and its partial derivative, we propose to use a non-parametric kernel method to estimate them. So, by using adopting this estimating strategy, we come to this set of estimating equations. So, here we solve this set of equations for beta, gamma, and also this theta, which is theta, which is the unknown function theta here, and its partial derivative. And then this. And then this is just a symptomic result showing that the beta in the beta coefficient in the error-contaminated covariate is locally efficient. If we are lucky and then the working model is correct, then we will get an efficient estimator. If not, we still get a consistent point. And then the inverse probability estimator, inverse probability estimator we used, we also show that it's a prism estimator in this theorem. Now I want to show you some simulation study we did for this inverse probability estimator. We consider so many different settings hopefully to capture the real life scenario. So we consider different degrees of measurement error, load, moderate and high and And high, and the X, which is the dynamic feature, it could be univariate, or most of the time, it's multivariate. And this X and Z they can be independent or they can be dependent. And the outcome model, which is the reliability, we consider many different complicated models because in real life, reliability is very, it depends on so many things. And this data, we can see. And this theta, we can see the nonlinear function, which is a combination of gamma transport and. So that's what we try to do in simulation. So here are just settings of setting in our simulation. So setting one and two, these are the most simplest case, and we gradually make it more complicated. More complicated. So 3 and 4 is considering. We consider X1 and X2 both are from normal, and the level of measurement errors are 0.5 and 0.9. And Z, that's the error-free variable. That's mostly reflect the system hardware thing. And X4, setting 4, we increase. We increase the complexity of the model six. And setting five and six, we consider the correlated x and z. So x and z, they are no longer independent of each other anymore. And in this simulation, so that's a simulation one. We try to use a different method to correct those measurement errors. Those measurement error problems in X. So, here what we do is we adopt a regression calibration method, assuming that we know the true data follows a normal distribution. So, that's RCR norm. And we also did another one, so that's regression calibration using a uniform distribution. And semi-1 and semi-2, that means we propose a different working model for the conditional expectation. Of expectation. And we show the empirical standard deviation and the asymptotic standard deviation. And our results shows that they match quite well in this case. And in this simple setting, it looks like if we ignore the measurement error, the coverage rate for the naive estimator is not too bad, ninety-two percent to capturing the true. Okay. Capturing the tube. But our method, the proposed method performs better. But when we increase, make increase the level, increase the degree of measurement error, then the naive method, it performs poorly. Only like about 9% of the estimator can capture the true quantity. But the method we propose here, it's still close to the 95% nominal confidence interval. Confidence answerable. Yes. Thanks. These were also really good. I'm wondering about the estimated standard error being so much larger for the semi-grammetric ones. So, is that way that you estimate the standard error going to make a difference in terms of the coverage probability? And if you could just define it? Those are larger than the empirical standard edge, right? Yeah, it's slightly larger than the empirical standard. Semi 2 than solid. Oh, semi-2? Yeah, what? I guess I'm just asking. I guess I'm just asking, how did you estimate the standard error? And then, like, is that method do you think good or take that little factor colours probability to be? Oh, yeah, this estimated standard error. So, that's the asymptotic one. We show it's analytic form in 0 or 1 here, which I quickly show you there. And then we hope to see this empirical standard error and estimated standard error, they are close to each other. So that means. Each other. So that means the estimator is good. But of course, when these are asymptotic standard errors are too big or too large, it definitely affects our coverage rate. Did I answer your question? That's right. Another question. Yes. Do you expect it to be slightly larger than the empirical standard error? What I expected, the empirical standard error is... I don't think I expect that, but it happens to be a little bit larger in this case. The sample size, oh sorry, I did not put the sample size in the setting. But I believe the sample size in this uh simulation two, that's about three hundred and the number of uh um numerical experience. Numerical simulation is 200 with this particular case. And when in setting five and six, we try to reduce the sample size to see how well our method worked. Yes. So I actually have a question about the regression calibration piece because, and I could be wrong on this, I thought that regression calibration can't, like, it's not consistent in nonlinear models. And so given that you have a legitimate And so, given that you have a logistic regression as your outcome model here, I was kind of surprised to see it doing as well as it is. Especially, in my experience, the farther the beta gets from the null, you can make regression calibration really quite biased here. So, I wondered if you saw that in any settings where maybe this worked out well for regression calibration. Well, regression calibration, when the setting is simple, I think most of the phenomenon And most of the phenomena we observe that its performance is not that bad. But when the setting is complicated, then regression calibration can perform poorly. And that's what I am trying to show you in later of my settings. So you see, this is setting 3. So the setting 3, let me see what did I say, setting 3. So here we consider these x1, x2, both are normal, and the degrees of measurement errors are a little bit larger than. Areas are a little bit larger than segment one and two. And in this case, as we can see here, the coverage rate dropped below 90%. So we will say that the regression calibration is not that reliable when the degrees of measurement errors are large. One more question. Because I ran into this recently with regression calibration. And so in this case, you're assuming the regression calibration model, right? Calibration model, right? Like, because you don't have x for anyone, so or for any observations, any plants, right? So, this is like an equation that you specify. The calibration equation is not being estimated from the data in any way. Well, when we implement this, the regression calibration normal means that we assume we know the true distributions of that x. So, that means this is the best we can get by using the regression calibration. So, we just replace that x. Replace that X star with the conditional explanations of that quantity. That's what I was asking. Because if you estimate it, then you do see this, because there's like a missing source of uncertainty if you had to estimate the calibration equations. Thank you. Oh, I just observed. So the regression calibration on this table, it has a smaller bias than semi-1 or semi-2. Yes. It also has a smaller standard error than semi-1. Standard error, then send me one more sample two. So his MSE is quite small. But why it has a very bad coverage? Why that has that small coverage? Well, you are right. The regression calibration, the bias here is relatively small compared to the SEMI method, but we try to look at the coverage rates. The coverage rates in this performance. And as we see when we try to make the data generating process more complicated, the regression calibration is just the bias could be large and the standard error can be bad in our setting. What is the difference between 71 and 72? Oh, yeah, 71 and 72, that means we use different white. that means we use a different working model for this different working model for the conditional expectation. So semi-1 I believe that's the we we try to replace this uh cosine of delta plus 0.1 with to estimate the conditional expectations of x given x star and the z. Star and Z. And similarly, semic2 is try to use another working model. So we tried two different models. And the way to, so you may wonder how do we choose this working model? Because based on the setting, we can use some sort of guideline. Let me see if I have that here. I did not put the how do we choose the working model. Choose the working model in this slide here, but we can use the relationship between the y, the observed, the observed outcome. Sorry, that's the A, the flexibility, given the X star and C as a dying light to help us select a working model based on our derivation thing. And then this is the setting three, and this is the And this is the result for setting four five and six. So and then at the end of the day, we try to estimate the average treatment effects. So each this is the setting one to Each this is the setting one, two, three, and four, and five, and six. So the last two is the causal average treatment effect estimated by using the proposed propensity score. And this one naïve means we ignore the measurement error in the X. We just use whatever data we have. And RC1 and RC2, that's just using regression calibration, assuming different models. Assuming different models, either normal or uniform. Well, we could try to do more experiments, but from the experiment we did here, the performance is quite similar to what we see in this graph. And another estimator we try to use is the augmented inverse probability inverse. Inverse augmented inverse probability weighted estimator. And the benefit of using this AIPW is that it provides double protection over model mispecification. So in this estimator here, pi is still the propensity score, and M1 and M0, this is the outcome regression model. So that's the that we estimate. That that big we estimate these are m1 and m0. And then, um, oh, five minutes. Okay, sorry, I need to speed up. Sorry, I can't completely lost chat. Okay, so that's a double robust estimated. Well, the challenge here is that reliability is a double-bounded quantity. So we cannot just use linear regression. Otherwise, the predicted result may. The predicted result may go outside 0 and 1. So that's one challenge. And another challenge is that the target reliability depends on high dimensional covariates. So we try to reduce the dimension using some dimension reduction technique. And in dimension reduction literature, here, a popular one and easy to implement is principal component analysis, but we all know that it. But we all know that this method ignores the correlation between the outcome y, which is the reliability in our case, with the covariate x and z. So we cannot use that. And another type of method, which is easy for a beta practitioner, is the inverse type of regression approach. They are simple to use, but they require linearity condition and constant variant assumptions. Constant variant assumption. So, and also in our case, it may not provide some insight on those boundary response outcome model. And a third method is a semi-parametric approach. And we propose to, oh, let me talk about the beta regression first. So, we try, since our outcome reliability, that's a bounding response. So, we propose to model this. Propose to model this outcome model using beta regression model. And then we well, that's just the literature. So, and then in most of the existing literature, beta regression model, there's a little work considered high-dimensional covariate with a mixture of continuous and discrete variable. Most of the paper we find, they kind of deal with a low. Fund they kind of deal with a low-dimensional covariant. They require all of them to be continuous, and they cannot deal with those binary or categorical. But in our data, we have a lot of binary and categorical ones. So we need to come up with a better way to reduce the dimensionality. So we propose to estimate the outcome model using the beta regression model, and in the literature, there are many. And in the literature, there are many different methods, different ways of modeling the beta regression model. What we use is we use this parameterization. So we model the outcome with our mean and the precision parameter, where mean is the conditional expectations of y given the covariate w. So this w may contain both x and z. So it contains both a dynamic feature and Both are dynamic features and the static features. And the benefit of using this method, this way of modeling is that we directly see the mean of the reliability and we can easily calculate the variance of the reliability. So this is our dimension reduction model. Model, we try to work with to reduce the dimension in reliability model. So we assume that this y, the reliability is independent of the covariate given the linear combinations of eta transport times W. So, this is the standard dimension reduction model in the literature. So, we try to use the semi-use. Use the geometry approach by deriving the efficient estimating equation and solve for the parameters. So, because of time, I'll quickly go through this slice. And then we just solve this two set of equations for our beta parameter here. And in this estimator, we have some unknown quantity, which is this conditional. Conditional expectations of W given beta transfer W. If we have the linearity condition, then we will just use this result to estimate the conditional expectation. If we do not have the linearity condition, then we will just use a non-parametric kernel estimator to estimate that quantity. Okay, so now the simulation study here, I want to show you the low structure. I want to show you the low structure dimension. So, here we have W1 to W11. So, we have a mixture of continuous and discrete, and they are correlated with each other. And the dimension is assumed to be 1. Now, this is the result. So, the initial estimator means that we just use a beta reg. So, that's the beta regression without doing anything. So, that's the estimator. anything. So that's the estimator. So SIR, that's the slice inverse regression estimator and SABE save is another inverse type of estimator. These two estimator is very easy to implement but they do not provide the insight for the position parameter in the beta regression model. And the linear estimator means that we use the linearity condition so that we use that condition to We use that condition to estimate the conditional expectation. And the non-parametric estimator is the one that we use a non-parametric estimator to estimate the conditional expectations of W given beta trend scope times W. And another one is greater structured dimension. So in this case, the two dimension is 11 and it's not reducible. And it's not reducible. And the sample size we try to work with is 50 and 100, and we want to see how well our method works for this smaller sample size problem. And it looks here, we look at the mean and standard deviation of the residual from the predicted result. So it looks like the proposed estimator. The proposed estimator results in the smallest mean of the residual piece. And it's kind of as well, it's a little bit worse than the linear one, but we do not need that linearity condition. So we think our method is a better solution. And then this is the average treatment rep using all the methods. So two rep means that we know. means that we know we get to observe all the true covariance and we know the true model. And the S-diff, that's just taking the sample average of the both treatment group, both flexibility group. Naive, this is the inverse probability estimator, but we ignore the measurement error. This one is naive. We ignore the measurement error and we use the wrong outcome regression model in the AITW estimator. W estimator. And this one, DR, means a double robust estimator, but we ignore the measurement error similarly. These three, IPW and AIPW, but wrong model. And I want to show you this three method. So SAMI, that's the IPW estimator we propose. So we just try to correct the measurement error in the flexibility model, but we do not do anything. But we do not do anything with the outcome regression model, which is the reliability. And this one, we implement the AIPEW estimator, but we use a linear regression model to estimate the reliability. And the last one, that's the AIPEW, but we use a beta regression model. So as we can see here, the variance of this DR semi is slightly smaller than this one because in the literature we see that in the literature we see that if we in the literature the AIPW has the benefit of when we when we when we include the the outcome regression model, we we get to have a smaller variance. Smaller variants. Yes. I'm not sure you might have talked about it, but when you correct, when you correct some measurement error, the IPW, are you using a correct model for the calibration? Assuming something, a model assumption? Oh, so when we do the regression calibration here, here we assume the true model. I did not The true model. I did not for the regression calibration. Okay. Just for comparison for the regulation. When you're doing I DW, are you using a model for x for x term? What are you are you using a model assumption when you're correcting for the measurement general? Oh, when we try to do the uh cor do the error corre correction using the Error correction using the parametric method here, we only assume that the measurement error follows a normal distribution. Okay, it has to be assumption we make in the semi-parametric method. Have you looked at whether this assumption is correct? Um well I I did not thank you. I did not answer look at that one. Yes. Is the variance of the measurement error something you know Is a measurement error something you know beforehand, or something you can assume using? Well, that's a good question. In the simulation, we assume that we know the true variant, but in the real data, if we don't know the variance, we can use replicative data to estimate the variance. Wooly, we try to rush a little bit. Okay, sorry, having all the time. Okay, give us four minutes. Oh, my God, I gave us four minutes more. Okay, sorry. Okay, so I'll try to quickly wrap up my talk. So, this is what we try to do in this project. We focus on doing a dimension reduction for a boundary outcome model. And we try to estimate the average treatment effect using the robust estimator and double robust estimator. But the limitation is that in this project, in the current work, we did not. work we we did not uh we did not work on the we did not consider the operation uh cost because in those are large infrastructure when we switch from one decision like flexibility to non-flexib when we switch from non-flexibility to flexibility there is always some cost it's not free but we did not consider that as well or we did not consider that and there's a switching cost that's something I'm I'm still trying to learn from my I'm still trying to learn from my colleague. And our ultimate goal is to try to find the optimal flexibility, but with minimum cost. So people are kind of so greedy, so we want to get most things out, but spending less to do it. And another challenge is that there's some spatial connection in the power grid because if your neighbor lost power, it could be Lost power, it could be it's likely you will lose your power as well. So these are spatial connections we did not consider in the current project. So I would like to thank my colleague, my collaborator, Sarah. She's an assistant professor. She's an associate professor in power engineering at Syracuse University. And I also thank her for her lab, which provides data, and this is an interesting problem. Interesting problem. So, without her, there is no, this project won't exist. So, thank you all for listening. Yeah, then over time. No, you're fine. You're fine. We have plenty of room. So, just so you all know, let's get there. We do have lots of flexibility. So, right now it's lunch, and you have basically free time until at two o'clock here in the foyer. Here in the foyer, whatever that means, there's a group photo. So I'd love, please come. I'd love everyone present. So, yes, two o'clock. Give you a little bit more people for the photo. All right, let's all enjoy lunch. Thank you, everyone. For those of you that have pressed me, you can stay. Yeah, sure. So, a key professional. Also, of course, I can look at the. um of course because i keep in the beginning of the play it at every similar to what i'm doing yeah oh yeah which the difference it just it does that holds on the website right now there's my website that's what i did yeah  I tried, I tried shape. I think after I could be next up, there's not that much. Yeah, I know. I think that's kind of the way I call it. I see what I'm saying. I understand. Yeah, I guess I should have always thought that the title is like a little bit of a pilot. Yeah, great. And I was a little nervous because I was like, I was in Miami, and you just didn't go home for my episodes. It doesn't count me or anything. So I pulled out a 79 on Miami. And I was like looking at Fireware to see that watch. They're solely brainstorming. Like eight on the last 12 days. And I was, and my husband was like, oh, it's because it's 7.30 max. But like 12 hours earlier, Alaska had brought it. So literally the morning out, I got up at like three. Literally the morning out, I got up at like 3:45 and I was like, Well, I still have a flight, we will see how it goes. Well, it does not require novel, but I have to be here. So he also maybe also come back around the three beds right. Yeah, I started around my home at 12 o'clock. 18 and that's what I'm doing. That's like I just, I'm not that bad about the mornings, but like three and four o'clock. I don't even, like, I don't know twice.