All right, excellent. So I will talk about factorial lower bounds for random order streams, or almost that. Joint work with Ashish Shiplankar, John Gallagher, and Eric Price. So in this talk, I am interested in solving graph problems in the streaming model of computation. You know what it is, but I will remind you what the model is anyway. So our input here is a graph G with n vertices and m edges. With n vertices and m edges pictured on the slide, the edges are presented in a stream in some order. And in this talk, we'll be interested in a random order of arrival. We will be interested in single pass algorithms, so our algorithm should ingest this term and at the end of the day, output something useful about the graph. Now, classically, in the graph streaming model, one has been thinking about a linear amount of space in the number of vertices. Of space in the number of vertices, which is sometimes very often needed. But in this talk, we're interested in, so to speak, truly sublinear space graph parameter estimation algorithms. Namely, we want to understand how much, which graph parameters can be estimated using space sublinear in the number of vertices of the graph. Quite a few things can indeed be estimated in this amount of space. Matching size can be estimated using polylogarithmic space. Using polylogarithmic space, connected numbers of connected components can be estimated well. I'll talk about that later. One can generate random walks in sublinear and space. And of course, the line of work that Madhou was talking about in the morning very much fits into this framework. Although here, sometimes one doesn't even need the randomness in the arrival order of the edges of the graph. Okay, good. So we're interested in space complexity. interested in the space complexity of graph parameter estimation in random order streams. The two most relevant works to this talk are the very beautiful result of Pan Pang and Christian Zoller from SODA 18 that shows that the number of connected components in a graph on n vertices can be estimated up to an additive epsilon n error using space proportional to the function of just epsilon, the precision parameter. Just epsilon, the precision parameter, as long as the graph is given to the algorithm as a randomly ordered stream of edges. Okay, so this is fantastic, works for general graphs. However, the dependence on epsilon is quite bad. It is exponential and even worse than that. It's one over epsilon to the one over epsilon cubed. Okay. Another relevant result to this talk is a result of John Callagher, myself, and Eric Price from SODA22. There we show that There we show that one can simulate B random walks of length k up to T V D precision epsilon, which is not so important, in space proportional, in space essentially exponential in k squared. Again, this is assuming that the edges of the graph are presented to the algorithm in a random way. And here we're interested in understanding whether this exponential dependence on the exploration depth at the algorithm Exploration depth that the algorithm needs to perform is necessary. So for the random walk generation result, it is clear what I mean by exploration depth. We generate random walks of length k and we take space exponential in k squared, actually even length squared. The result of Peng and Zoller on random walk estimation on component counted, counting can really be thought of as doing one over epsilon length exploration in the graph because. Latency exploration in the graph because you approximate number of connected components up to an additive epsilon and term. It's enough to look at sort of one of epsilon size connected components. Good. So one notes that the space complexity depends exponentially in the exploration depth in these cases, and in fact, even worse than that. We will show later that component counting can actually be done. Counting can actually be done using factorial dependence on the parameter epsilon, and we'll prove tichlower bonds for that. So, we're interested in factorial dependence on aspiration depth and why is it necessary. Good. All right. A very specific problem that we will get tight bounds for in this talk is something we call the component finding problem. The problem is this. The problem is this. We're given a graph G as a randomly ordered stream of edges, and we're also given a promise that a constant fraction of vertices of G reside in connected components of size L for some parameter L. And the task of our algorithm upon ingesting this randomly ordered stream of edges of G is to find one such component. Then at the end of the stream, the algorithm should produce a collection of vertices and A collection of vertices and correctly claim that this collection is a connected component of size at most L in G. Okay, so the result of Pengenzola from Sodo 18 that I mentioned actually first designs a component finding primitive that takes space L to the order L cubed, and then uses this component finding primitive for estimating the number of connected components. In this talk, we'll get tight bounds for components. Get tight bounds for component counting and almost random order streams. Are there any questions about the problems that we would like to study? Okay, great. So we'll get our lower bound by introducing and analyzing a new communication problem, which we call streaming cycles. So let me tell you about this problem and then say something about its analysis. About its analysis. So, what is the streaming cycles problem? The problem here is the following. It is a communication problem, as it will be clear in a second. Here, our input is a graph G, pictured on the slide. This graph consists of n by L. Think of L as a large constant or slowly growing function of N. This graph consists of N by L disjoint cycles of length L. So, here they are pictured on the slide. Now, the edges of these cycles are presented to Of these cycles are presented to the algorithm in a stream in a random order, although not quite. The edges come with bits. Every edge is annotated with a random bit. Here they are, these bits shown in red on the slide. Good. So what is the task of the algorithm? At the end of the stream, our algorithm should output the parity of the bits on some specific site. C. Cycle C. Basically, the algorithm should point to a cycle of its choosing, some cycle C, and output the parity, the sum of the bits of the edges of the cycle, Mach 2. Okay? Good. So for simplicity, it's good to think of this process as follows. Really, think of the n-player communication problem, where every player gets an edge together with a bit written on that edge. And we should think of the bit. And we should think of the bits as private inputs to the players. So, those are the hard parts of this communication problem. And the edges of the underlying graph are just posted on a common board, at least for the previous players, for all the players to see. Okay, good. Good. So, let me illustrate this. So, there is some underlying cycle structure of the graph, and the hard part about solving the Steam cycles problem is. About solving the steering cycles problem is that at every point in time, the algorithm does not really know how the edges appearing on the board fit into cycles. For example, the first edge arrives, and the edges will arrive with random vertex IDs. So here's this black edge with a bit one. Okay, then maybe some other edge arrives, the bit is written, the edge remains on the board, and this proceeds. Okay, so the structure of the underlying cycles is slowly revealed to the algorithm, and at the end of To the algorithm, and at the end of the stream, the algorithm should output the parity of one of the cycles of its choosing. Okay, good. As I said, the hard part in this problem is that the algorithm at no point in time knows how the edges fit into cycles. So, really, to do this formally, we need to make sure that the edges arriving on the board have random vertex IDs. So, here they are with these random vertex IDs. So, really, what we get in the stream is something. Really, what we get in the stream is something like the first edge E1 is an edge vertex 234 with vertex 345. The corresponding bit is a zero, and this proceeds like this. Good. All right. So this is the streaming cycles problem, and we want to understand the space complexity of this problem. Any questions about the setup? There, how we will identify the cycle? Sorry, say again. I don't how would we identify the cycle? Ah, great, uh, good. Uh, so the question was, how do we identify the cycle? But as the edges come in, the edges that have come so far stay on the board. So, at the very end of the stream, like this proceeds over n time steps, you actually see all the edges of the graph on the board. So, you know what the cycles are at the very The cycles are at the very end of the stream, but you don't know the bits. So at that point, it's too late because the bits do not stay on the board. Does that help? Yeah, great. Okay, good. So basically note that at the beginning of the stream, right, so somehow a bunch of isolated edges probably sit on the board, and you really have no idea which edge belongs to which cycle. But at the end of the stream, the structure of the graph is very clear. Guarantee. Perfect. Thanks for that question. Guarantee, perfect. Thanks for that question. Great. So let's try to design a simple protocol for the simple streaming algorithm for this problem. So here is one. I will simply choose my favorite vertex, vertex one. Here it is, this vertex shown as green on the slide. And my algorithm will be a space one algorithm. I'll store one bit. Once, and the bit will be this. I observe the edges. I observe the edges arriving in the stream, and if I think that the edge arriving right now extends the component of my green vertex, I simply add the corresponding bit to my counter. So let's say here I chose the green vertex, the first edge arrives with a bit of one, I add a counter one to my bit. And this edge here, another edge arrives. It's not even connected to my component so far, so I simply disregard it. And some other edges arrive. Here, the edge with a Arrive here, the edge with a zero bit arrived, and it's extending my component the way I think it extends my component. That is true. So I add it to my counter. I ignore the edges that don't extend my current component, etc. So here I add one to my counter, and this proceeds. Okay, so note that in this particular case, I got lucky. And the green vertex was the arrival sequence was such that whenever an edge from the component Whenever an edge from the component of my green vertex was arriving, I knew that it was relevant to me. Basically, the component was arriving in order. In this case, I indeed succeed in computing the parity using a single bit. All right. On the other hand, I don't always get lucky. So suppose that my favorite vertex was not vertex one, but vertex 10. So here it is shown on the slide as the green blob. And again, I run the very same arrival sequence. So here, if the first stage I ignored. Here, the first edge I ignored. The second edge is incident in my green vertex, so I add one to my counter. Great. Then the third edge arrives, and it actually is part of the underlying cycle, but I don't know it yet. It is this disconnected edge from my component. I ignore it, and it's basically over. So this vertex got unlucky. Okay, so the component somehow grows and finally it merges into a single cycle, but I lost already. Good. Already. Good. So one can ask oneself: what is the success probability of this protocol that I just defined? And a moment of contemplation reveals that the success probability is basically one over L factorial. This will succeed if and only if the component of my favorite vertex arrives in a collectible order. The very basic collectible order is just one, two, three, four, five, sort of say going around the cycle. There are more collectible orders. There are more collectible orders, but it's still one over theta alpha tori, please. Good. All right. Now, of course, I can turn this simple algorithm into an algorithm that succeeds with high probability. Simply sample L to the order L seed vertices uniformly at random and keep counters for all of them. At least one of them will succeed with very high probability. And this gives me a This gives me a bona fidel algorithm for streaming cycles. Our main result in this work is that this is tight up to constant factors in the exponent. So L to the omega L space is indeed needed to solve streaming segments. Okay, great. So this is basically what we prove. Before I tell you something about the proof, let me show the The proof, let me show the applications. So, I wanted to understand the space complexity of component finding. Recall, component finding is, and given a randomly ordered stream of edges of a graph G, a good fraction, a constant fraction of whose vertices reside in components of length at most L, find one such component. If the reduction is very basic, so here I have a cycle from the streaming cycles problem on the left, the edges annotated with bits. Uh, the edges annotated with bits, I will turn it into uh either two or two cycles of length L or one cycle of length 2l as follows. Every vertex becomes present in my graph via the zero copy and the one copy. So here we have it. The zero copies are the inner cycle on the right, and the one copies are the outer cycles on the right. And whenever an edge ij arrives in the streaming cycle problem, I connect the Problem, I connect the I0 to the J0 and I1 to J1 if the corresponding bit on the edge is zero, and I cross the edges otherwise. One can observe that if the parity of my cycle in the streaming cycles problem was zero, then this produces two disjoint cycles of length L. Otherwise, and here's the modification, if the parity is odd, it produces one cycle of length 2L. Uh, cycle of length 2l. Okay, and one can verify that this is generally true. Okay, so this is the reduction, all right. Great. So given the simple reduction, we obtain the following corollary component finding requires L to the omega L space because streaming cycles does. As I mentioned before, the SODA 18 paper of Peng and Zoller gives a component finding primitive that uses L2. Finding primitive that uses L to the order L cubed space. So given that our result seems nice, but not particularly tight. Actually, we show that component finding can be solved in L to the order L space. This makes our bound tight. Okay, good. I also mentioned applications to random walk generation in random order streams. So, here what you can So, here we can prove the following. One of the lower bounds that we can prove is the following. Suppose that you would like to generate, let's say, an exponential in L, number of walks of length L or 2L or something. And then this requires at least a factorial in L amount of space. Why is that? Well, that is because if I look at the graph that I obtained through Look at the graph that I obtained through my reduction. The graph contains components of size L or 2L. And if I run an exponential number of random walks on these components, at least one of these random walks will loop around the corresponding cycle. So in particular, I will exactly know the length of the cycle that I hold in my hands. And I can map it back to a streaming cycles problem and get the corresponding parity. Okay. Great. Great. All right. So these are the types of lower bounds we get, except I have to be careful here. There's something important that I completely brushed under the rug, namely, which arrival order does the lower bound apply to? So I would like to say that component finding in random order streams requires L to the omega L space, but my reduction broke this. Because while the arrival order of edges in the streaming cycles problem is uniformly random, the reduction that I Random, the reduction that I used creates some amount of correlation because for every edge in streaming cycles, I actually inject two edges into the stream. So, what is going on here? Well, this brings us to a philosophical question of why do we study certain models of arrival in the streaming model? We study the adversarial model because it's very general. Results in this model apply to just about everything. On the other hand, real arrival sequence. On the other hand, real arrival sequences are probably not adversarial, so we use the random order streaming model to reflect this somehow. But there is a danger of our algorithms, essentially, for random order streaming overfitting to the distribution. And to answer this question, we introduce what we call the hidden batch random order streaming model that accounts for correlations in the arrival order of edges in the stream. In this hidden batch, In this hidden batch model, the arrival order is as follows. The edges of the graph G are first partitioned into batches of size bounded by B. And this partitioning is adversarial. Batches are then ordered randomly, and then edges arrive in batch-induced order. And the model is very general. One can do various adversarial reorderings and intra-leavings. But the idea here is that the algorithm is oblivious to the Is oblivious to the correlations to the partitioning of edges into batches, hence the hidden batch random order streaming model. One can think of the arrival order generated by this model as being correlated through external events, which are unknown to the algorithm. So imagine the stream of edges on Facebook as follows. It is reasonable to say that there are different parties happening pretty randomly and uncorrelatedly. And at every party, something... And at every party, some people go to the party and the friendships are formed. So, what we observe is a stream of friendships on a Facebook graph. The parties that generated the stream are pretty uncorrelated, yet the edges generated by the parties are. And we don't really know which edge came from what. Good. So we show that existing component counting and component finding algorithms actually do not overfit to the random order streaming model. Random order streaming model. In particular, they seamlessly extend to this model of ours with hidden batches. So that's good news. And now I can state our lower bound. Our lower bound says that component finding in the two hidden batch random order streaming model requires L to the omega L space, which is tight, and up to this log factor. Great. Okay. Good. So in the remaining Good. So in the remaining 10 minutes, I will any questions about the statement of the results? Otherwise, I'll say a few words about the analysis. Maria, a question about this model. Do you know any algorithm that overfits actually to random arrival? Is there a separation between the two, for instance? I do not know of any algorithm that overfits. Yeah, we somehow didn't do the legwork. We somehow didn't do the legwork. It does take work to support existing random order streaming algorithms to this model. It'll be great to do more along these lines, but yeah, I don't know of any. Is it possible that there's a compiler basically that takes any normal random order streaming algorithm and just runs it? Do you have to change the algorithm or like the same algorithm works basically? No, so here, yeah, you have to change it on pretty, kind of pretty obvious ways. Basically, the idea is that even though you don't know the batches, basically the correlations, you can kind of guess them. Because sort of if two edges arrive in a certain window, sort of weirdly close to each other, it is most often because they are in the same batch. Batch. So that's somehow the thing. That said, there were some issues that we faced in porting our random walk generation algorithm to this model. So we had almost the entire proof and couldn't like there is certain coupling stuff that we couldn't push through here. So yeah, it's interesting. I think it should work. Yeah. All right. So Michael, the model is that the graph as well as the partition of edges into batches. The partition of edges into batches is adversarial. Yeah, the ordering within a batch is adversarial, the ordering of the batches is random. Yes, to be even more, I'm simplifying it here, but basically, yeah, our model even allows interleaving, which I think is important. For example, if let's say if you have two batches with corresponding edges, the corresponding edges don't have to arrive one batch after the other. Basically, the way the model really The other. The way the model really works is that you first generate arrival times for the batches, and then you can think of these as random numbers between zero and one. And then you're given some window, and you're allowed to adversarially perturb the arrival, adversarially choose the arrival times of the edges within the window. So it's pretty, pretty general. Yeah, yeah. But in short, yes, the graph is adversarial, partitioning is adversarial. Adversarial, partitioning is adversarial. The only random thing is the arrival times of the batches. Everything else is adversarial. Okay, any other questions? All right, good. So in the remaining time, I'll say a few words about the analysis. Our analysis uses the Fourier analytic machinery that has been very successful in streaming lower bonds recently. So I'll talk about the existing machinery and talk about the existing machinery and why it's not adequate and what we do. Good. So the most relevant Fourier analytic, Fourier analytic, most relevant communication problem to our analysis is the Boolean hidden hypermatching problem, which was introduced by the beautiful and very influential work of Gavinsky, Kempek, Karendis, Rausen, DeWolf, and then Bergman and Yu. So the problem, I'll remind you, is the following, it's a two-party communication problem. Communication problem. There is Alice who is given a binary string X. Alice looks at the string, sends a message to Bob. Bob is given a hypermatching M with about n over L edges, hyper edges of size L. So here they are shown on the slide. And you should think of these hyper edges as essentially random. The hypermatching is basically random. Good. And Bob's task upon receiving the message from Alice is to output the parity of is to output the parity of some hyper edge in M. I'm simplifying here, I'm stating the decision version of the search version of the problem as opposed to the decision version of the problem, but this search version is more relevant to our work. And then using a beautiful Fourier analytic approach, one can show that n to the 1 minus 1 over L space is necessary and sufficient here, where L is the hyper-H size. Okay, let me just say a few words about how this works. Let me just say a few words about how this works and then why this does not immediately translate to our problem. Well, actually, before I do that, I should say why it is reasonable to think of Boolean hidden hypermatching in the context of our problem. Note that in Boolean hidden hypermatching, the bits assigned to the vertices are first given to Alice, and then the important sets, the hyperedges, are presented to Bob. Our problem is essentially similar, but it's an highly interleaved. But it's a highly interleaved version of Boolean hidden hypermatching, where the structure of the hyperedges, namely of the cycles, is revealed slowly to the output. And the bits x are revealed in an interleaved fashion as well. So Boolean-hidden hypermatching is basically a very extreme case of alcohol. Okay, so how does one analyze Boolean-hidden hypermatching? Well, suppose that Bob received a message from Alice. This message constrains X. Constrains X, the bit string of L is to live in some subset of the hypercube, call it A. Here it is shown on the slide. And one wants to understand what is the distribution of the parities of the hyperedges. So m times x, m here is the edge incidence matrix of the hypermatching conditioned on Alice's message. Right, and one can ask oneself, given a hyperedge E in Bob's hypermatching, what is the chance? Hypermatching, what is the chance of guessing the parity given Alice's message? Well, the chance of that is basically on a half plus is related to the Fourier transform of Alice's message on that hyperage. Here I'm using a normalized version of the Fourier transform F tilde. And it is possible to show, and that's what Gilinski and others and Verden and Hugh do, that in expectation, the sum of squared Fourier coefficients and all the Of squared Fourier coefficients on all the hyper-edges in M is very small, unless a lot of communication happened. And this is somehow a consequence of hyperconstructivity and the fact that the matching was rapid. So let me not dwell on this. Good. So now, suppose that we would like to apply Fourier analytic techniques to our streaming cycles problem. Let me remind you what the problem is. Again, there's an underlying graph that is a disjoint union of n by. is a disjoint union of n by l cycles of length l here they are shown on the slide in a fainted manner okay and then they arrive in a random order annotated with random bits wonderful and we should out the algorithm should output the parody of some cycle c at the end it's quite similar to boolean hidden hypermatching um good well how can we apply fourier analytic methods from uh boolean hidden hypermatching here well in fact many Hypermatching here. Well, in fact, many recent works, including the ones that Madhu talked about in the morning, have shown how to extend this Fourier analytic machinery to analyze problems with a large but constant number of players. And that is great. However, this runs into serious trouble here for streaming cycles because one can verify that one cannot partition this stream into fewer than n to the one over l or so players. And to the one over L or so players without revealing the answer to one of the players just immediately. There's L edges in every cycle. If there's fewer parties, then one of the parties will have a complete cycle. Okay. Furthermore, even if we do somehow, even if we analyze a game with n to the one over L players, we absolutely cannot lose anything in our bound as a function of the number of players. As a function of the number of players or anything linear, just because the number of players is way bigger than the space lower bound that we're trying to prove, at least for values of L that we care about. Good. So as a consequence, we need to design techniques that let us analyze the knowledge of the streaming algorithm, so to speak, in a continuous manner on a per-edge basis. That's something that Madhu mentioned in his talk. And that's basically our main question. And that's basically our main contribution here. Okay, good. So let me see. How much time do I have? Oh, yeah, how much time? Maybe five minutes. Sorry, say again? Maybe five minutes. Oh, yeah, excellent. That's the time of the delta, I think. So try to run. Okay, I heard five minutes. This is great. Good. All right. So, this is our problem, right? And Our problem, right? And remember that when we analyzed a very basic protocol for the problem, we essentially realized that certain cycles are easier to guess than other cycles. And in particular, let's say this left cycle on the slide is an easy one because if you start with the right vertex, its edges arrive in order. You can collect it as the edges come in. We call such cycles single seed cycles. And the other cycles we call multi-seed cycles, the one that somehow Multi-seed cycles, the one that somehow grow out of more than one component, which then get merged. So here is a merge. Okay, great. And basically our intuition is that the algorithm's best chance is to track the single seed cycles. And we'll prove this by bounding the evolution of Fourier coefficients of the algorithm as time goes on. Remember, time goes from one to n. Good. Good. Let's try this on a very basic protocol, the one that I presented at the beginning, where you just sample a bunch of starting vertices, little C of those, and then grow paths out of these vertices for as long as you like. And basically, so let for j between 1 and L, let y tj be the number of collected length j components that we're able to collect. We're able to collect. For example, at the beginning, at time zero, one can think of we hold in our hands C, which is our communication budget, components of size zero. And then one can look, let's say, for simplicity of combinatorics for the first half of the stream, just see how these quantities evolve. At every point in time, if I hold a length j component in my hand, then with probability 2 over n minus t, Over n minus t, the remainder of the stream, this component gets extended by one edge, and then I suddenly hold a length j plus one, size j plus one component. So these y tj's satisfy very natural equations. There is also a contribution of these mergers, but we hope that it's small. And if we solve these equations, we'll realize that the number of components of length j that will hold is basically at time t is basically Poisson. It's essentially a Poisson. It's essentially a Poisson distribution, something of this nature. And some amount of algebra reveals that, in particular, we can get the L to the L, the vectorial lower bound, out of this Poisson distribution. So at the end of the day, we prove that the Fourier coefficients of an actual general protocol are upper bounded by essentially that. So we let f tilde t of z be the knowledge of the algorithm about some collection z of bits at Some collection z of bits at time t. And at the end of the day, our task is to prove that at the final step, at step n, the sum of squared Fourier coefficients of all the cycles is small, similarly to Boolean-Hidden hypermatching. Okay, how do we do this? I will not go over the details much, but essentially we prove some upper bound on if we classify Fourier coefficients according to the lengths of the paths that comprise them. And then Comprise them. And then the upper bound on the amount of L2 squared Fourier mass on these coefficients is basically given by these Poisson terms. So some kinetorics involved. Good. So what is the main insight that lets us carry such an analysis through? Let me spend two slides on this. Essentially, we should ask ourselves the following simple question. For example, what does it take for the algorithm to have a lot of Fourier mass, let's say on Have a lot of real mass, let's say on paths of length four. So, this is h sub four. All right, well, how can you form a path of length four? In order to, there are several ways of doing this, and they are listed here on the slide. Well, maybe you first held an edge in your hand, and this edge got extended to a path of length two, then to a path of length three, then put to a path of length four. So that's the first line on the slide. Or perhaps you have You held two edges in your hand, then they got merged into a component of length three, which then got extended. That's another way of getting this. And there are some other methods. But the gist of this is that in order to construct a, in order to know something about a path of length four at time t, you should have known something about smaller paths at some previous time step. And then certain combinatorial events, such as the paths getting merged, should have happened. Getting merged should have happened. And here's the how do we make this formal in the language of Fourier analysis? Well, the main observation that powers our analysis is a decomposition of the Fourier transform of the knowledge of the algorithm at time t, called as f tilde sub t, into almost a product of its knowledge at time s for some s. At time s for some s less than t, and it's knowledge between time s and times, t plus one, and time t. So here's a key statement. Here it's not quite a product, of course, but that's basically the key insight into Fourier analysis that lets us prove such bounds. Good. So that's what I wanted to say about the analysis. Let me just mention some open problems. So here So, here, the most exciting open problem perhaps is to design techniques that will let us handle decision versions of streaming cycles. So here, we prove tight bounds on the component finding problem. That is, find a cycle and correctly say what its parity is. But as of now, we don't know how to prove, let's say, tight bounds on component counting. So, how do you analyze? Uh, counting. So, how do you analyze gapped versions in this continuous setting where you have n players and you can't partition into a small number of players? And a related question is the random walk generation result that I mentioned somehow has a space dependency which is exponential in the squared length of the random walk. So, if you look at the algorithm, in fact, the hardness comes from a different source, it's not really the hardness of. It's not really the hardness of exploration, so there's some other thing going on. So it would be nice to understand what the right answer for random log generation is. And thank you for your attention. Maybe we can ask one question while Robert's asked. Sorry, second. One quick question while the next set start. While there was negative sets out. So, Michael, why not the obvious open question of actual lower ones for random order, purely random order? Oh, yes, of course. No, I forgot about that one. That's the obvious one. Yes, exactly. Yeah, yeah, that's, yes. How do you not use this reduction? This would be fantastic. Okay. Yes. Yeah, we had, yeah, we had some approach there, but yeah, it also seemed related to how you. Related to how you handle correlation on the bits. So, this basically works very much uses the fact that the bits are uncorrelated. If you put a promise, then it's harder. Okay, thanks again, Michael. Thank you.