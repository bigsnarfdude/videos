Down uh the past two or three years, uh mostly using Simpag. So all the inversions I will be presenting here are uh are done were done using Simpag. And the for visualization we use either Matplotmap or PyVista. So these are two main open source packages we use. So the motivation here is to take one step further beyond just physical geophysical versions. So I will see more about it. So this work was mostly done by one of my former students. This work was mostly done by one of my former students, Sharon Wei, currently a postdoc at Stanford. So, the first part of the talk is from chapter 2 and 3 from his treatise. Okay, I will talk about two things, building probabilistic quasi-geor model, and second part is predicting mineral resources, both using airborne geophysical data set, and plus some geological information. Geological information. Airborne geophysics, we can collect multiple types of data, widely used. So the current standard workflow is to take the, assume that you have the airborne graph reading data or magnet data, then the standard workflow is to take them and using and do some inversions, right? For example, you can get a 3D assembly model and 3D sustainability and 3D desk model, and this can be done either separately or jointly. You can use, in this case, we're using SIMPAC. Of course, you can use some other packages. So this is pretty much the current workflow. But we asked the question, can we do more than just the physical problem models? Because what geologists, they don't care about the susceptibilities or data contrast, right? They are more interesting for They are more interesting, for example, geology, right? How many geographies in this area? Where are my faults? And where are the potential locations for minerals? So, the motivation for our work is to just can we extract more information from the physical property models that we obtain from inversions so that we can maximize the value of geophysical data and hopefully geophysicists. So, in this version, So, in this work, we focus on two things. One is that we propose to build something called classic George model. I will explain that later, through a process called George differentiation. So essentially, what this does is to take the inverted models and then you classify the inverted values into different groups. Each group is characterized by unique ranges of. By unique ranges of, for example, densities and scalabilities. So each unit has a very unique range. Each unit is defined by a unique range of density values and a unique range of accessibility values. And then I think it's reasonable to assume that each unit, each group, maybe represent some kind of GR unit. So that's the first thing we do. And the second thing is that we take the physical problem models and we try to Physical problem models, we try to make some predictions about where are the locations for potential mineral resources. So both of these two models, what we call classic engineering model and mineral prediction models, are arguably more useful than just the ceramic model and testing models. Because that model here on the upper right corner, it tells you the distribution. It tells you the distribution. So, different colors you have in different units that we identified based on the birth values. So, that model is potentially useful for geologists because it tells you the spatial distribution of different units that we can identify based on just geophysical data. And then that model is obviously useful because it tells you the locations for maybe some follow-up data collection or maybe more detailed. Data collection or maybe more detailed geophysical data collection analysis. Why do we care about minerals? Because if we are serious about energy transition and meeting the net mineral goals in the next couple of decades, in the next decade, we need to mine a lot more minerals. And we I think currently we are far behind behind that. Okay, moving on to the more technical part. Moving on to the more technical part. Oh, sorry. Yeah, some of the symbols are not quite right here. So this is from Dropbox. Apparently, they didn't do a good job with the symbols. But anyway, so the inversion method we used developed by Fonier and Oldenburg back in 2019, it was already implemented in Simpak. It was pretty well developed. We have been using that for years. It's pretty stable. It's called. So it's called a mixed output norm inversion. Essentially, the basic idea is to apply different norms to different components in the regular term. So typically in gravity magnetic inversion, for the regular term, we typically have four components. The smallest components and three smoothest components in X, Y, and Z directions. And it turns out that you can apply different norms to different components, and that will give you different models. They all fit the data, but they look They all fit the data, but they look different. And then we extend this idea to join inversion. Just to give you some idea here, yeah, so looks like some of the animations is not available here. Sorry, just need a segment. I download it online for you. Did you download PTF? Can we download here? I think we need PowerPoint. I'm going to make a little bit of a picture of the colour.  Yeah. Okay. Oh, I think it came. It came through somewhere. Yeah, we got it. Yes. Need somebody to come up with some music to go while I download. So I upgraded this to the But I probably choose a burst. Oh, we probably could. Oh. Yeah. Sorry. You should have coordinated on this before. Almost there. All right. We'll be back momentarily. Momentarily. So, don't present your Hobbes lights from Tobox. It doesn't work. So, we'll catch you back and we can show you beautiful equations. And we're back. Thank you regularly. Thank you very much, Sandy. Thank you, Sandy. So, yeah, this was the objective function I meant to show you earlier. Turns out that you can have different norms, you can have alpha norm here and maybe a zero norm here. Different combinations give you different models that look different, but they all fit the same. So, here are some examples. If you set, for example, P and Q both equal to 2, meaning you apply autonomous to both all the four terms, all the four components. Both all the four terms, all the four components, then you're essentially performing smoothness inversion, the standard smoothness inversion, and this is what you should expect: overly smooth nuclear boundaries, and also the estimate values are way lower than the true values because you smooth everything out, right? So that brings the inverted values lower than the true values. But if you apply, for example, here's another example where we apply R1R to all the four components. To all the four components. So, this is essentially the standard R1 on the version. And you get something different. This looks much more blocky, and you get some sense of the boundaries. And also, you can apply R1 long maybe to the first term, and R2 long to all the three smoothness components, right? And this is what one looks like. Compared with this one, it's much more compact. It also You recover these different structures. So, the whole point here is that we want to explore the model space to get a sense of the uncertainties. Because when it comes to, remember our goal is to present geologists, some kind of geological model, right? And when you present this to geologists, they always ask, hey, what is the uncertainty? How how much can I trust the volume, the dip, the depth, all that? So All that. So that's why we want to look at the different combinations of the P and Q values that allow us to have some sense of the uncertainties of the geophysical inversion blocks. If you want to perform rigorous uncertainty analysis, you have to go with the Monte Carlo sampling. Charlotte has done some work on that. Or nowadays you can use some deep geometry models to assess uncertainties. Both are at the forefront of the At the forefront of the research now. But in this case, we just want to see what we can do with existing deterministic inversion software, for example, Simpag or maybe Pachimly. So this is what we decided to do, to look at the variability of the inverted features by looking at, by just exploring different values for P and Q. Okay, so geology. Okay, so geology differentiation are a simple idea. So once you finish the geography versions, you just cross-plot them and then you look at the distribution and then you classify them into different units. And remember, each dot here has a special location. So once you finish the classification, you can map all the points back to the spatial domain and you end up with something like that. So that's what we call geology different geology. So here's a case study from Northeast Iowa. So this is the measured graphic gradient data, a little bit noisy. And the magnet data both are airborne data. We also have some 2D geological model. This is interpreted geological model based on geophysical data by Greg et al. in 2015. Before that, there should no geologic map. This is not the standard. This is not the standard traditional George model you expect. This is called interpreted George model. So from this model we know there are several units. For example, this is sign colour. So in the background it's Yabapai country rock, mostly gneiss and granite. And also we have some Vithic intrusions here, and there might be some other intermediate intrusion there. Intermediate intrusion there. Also, in this case, we have a one bottle here, called B01. From that bottle, we have some physical problem measurements, susceptibilities, the blue dots here, and also some density values, these purple dots. So, later we're going to use the physical point measurement to help us accept or reject the working models. So probabilistic geology differentiation. I'll start with the geology differentiation instead of probabilistic. Again, the basic idea is pretty... I already explained that. You just look at the distribution, right? And then you decide how you want to go about classification. So just want to give you a quick idea here. For example, we always start from unit one. You see unit one is always a background image, which always corresponding to Which always corresponds to near-zero density contrast and near-zero susceptibility. This is the easiest unit you can identify. So we always start from there. You probably ask, oh, how are you guys determining the boundaries? The boundaries are determined in such a way that when you visualize this unit one with a spatial domain, this should not contain any obvious density anomalies or susceptibility anomalies. So when we determine this function, So, when we determine this bound field here, there's a lot of 3D visualization involved using PyBaster and Matt Glauc, because we start with some initial gas, right? And then we visualize it in 3D. And then we look at, okay, with current classification, with the current boundary, do we see any obvious decimal nominates or certain non-linears? If yes, then that means that the current boundaries for unit Y need to be adjusted. Needs to be adjusted. So we just keep doing that adjustment and visualization until at one point we have unit one when visualizing speed domain does not contain any density or stable amount. And then we move on to unit two. For unit two, we are looking at near-zero density contrast values. Values and negative sensibilities. So, in this case, we know that when we visualize unit 2 in a special domain, we should not see any obvious density analogies. So, and one question people often ask is, oh, how did you determine the boundary between unit 2 and 3? What if I move this boundary a little bit to the left? What would happen? Well, what would happen is that if you move the boundary a little to the left, guess what? Left, guess what? In the spatial domain, this is an outline of unit 2 in the spatial domain. If you move this point very good to the left, then in the spatial domain, some of the negative density normalities will be included as part of unit 2. But we know that that's not what we want. So then we adjust the boundaries between unit 2 and 3 until a point where in spirit domain, Where in spirit domain, this unit 2 does not contain any obvious density nominals. While at the same time, we do expect to see some intermediate negative susceptibility denominating unit 2. So yeah, there are a certain level of subjectivity involved, but we do have some strong guidelines in determining the boundaries. If you move the boundary a little to the left, a little to the right, that's fine, but if you move too far to the left or to the right, You move too far to the left or to the right, you know that you are doing something wrong. So that's the basic idea. Just in the interest of time, I'm just going to skip the other units. But there's a lot of visualization involved. And you have to know what you are looking for for each unit. So that's the basic idea. So that is the um that is the det so-called deterministic um George differentiation because we we begin with one set of um density models, every model we end up with one quasi-geord model. So how about the probabilistic? Well, it's basically very very simple. We just randomly sample a lot of p values and alpha values. These are two new parameters in the regular term. Parameters in the regular term. We don't know what value to use, so we just decide to cast a white net, right? We just try. So, p-value is typically rating from 0 to 2. And 1, alpha s, that is the weight on the smallest term, typically rating from 0 to 2. You can go above 1, typically rating from 0 to 1. You can go above 1, but in this case, we just limit ourselves to 0 to 1. And then we just randomly sample 160 points, 62 points, and for each point, we perform one. Point, we performed one inversion. So, in the end, we performed 162 joint inversions. Shalom did that. I didn't do that. And then, for each inversion, he did the George differentiation. So, he performed 162 joint inversions and he developed 162 quasi-geor models. So, in the end, that allows us to do some statistics, but not all the models are consistently. All the models are consistent with the physical problem environments that I showed you earlier. Remember, we have one drill hole. We have some sense of the densities and sensibilities in this area for the basement. It turns out that for some of the models here, so this is the mean value, mean decimal value from this location where we have drill holes. We calculate the mean, we also calculate the standard deviation, and we compare. So remember this. Compare, so remember, these are the values from inversion. We compare the inverted values with the border measurement, and that will only accept those models whose mean values are within this upper and lower limit determined from drill holds. All the other models are rejected because all the other models they can feed the model, they can feed the data, but they're not consistent with um with the physical problem variants. With the physical problem variants. For example, for all the models here, their values are underestimated. Their inverted values are way lower than the lower bound, lower limit of the maverick values. But for those guys, their values are overestimated. Their values are larger than the upper bound, upper limit of the magic values. So everything above is green. Above this green box and below green box are rejected. So, easy hand, we have 37 accepted models, and then we do some statistics. Let me just skip this. So, in the end, this is what we get. For unit 2, we have some probability. So, this shows the probability of the spatial distribution. Probability of the spatial distribution of unit 2. So, for example, at this location, we are more confident about the presence of unit 2. We have almost 90% probability of unit 2. But in those areas, some models say, oh, there is unit 2 here, but some models say, oh, I'm not sure. So, in the end, the probability is around, I mean, maybe 30% or 20%. So, this gives you some sense of the confidence that. Confidence that you can have when integrating unit two. And the same is true for all the other units. We also can calculate the probability of the mythological or geological units at each set. Remember, we have 37 different classification models, right? So at each location, we have 37 labels or classifications. For example, in this simple case, For example, in this simple case, at the central location, from our first quasi-joint model, we have unit one assigned to this cell. The same is true for second quasi-theory model, but for the third quasi-joint model, it assigns unit two to the same location. And for the next one, it assigns a different label. So, just by doing some simple statistics, we can see that based on our model, for this cell, there's a fifth. For this cell, there is a 50% probability of unit one, and 25% probability for unit 23, and 0% for all their units. So, this is just some very empirical estimates on uncertainties using simpac inversion results. So, in the end, this is what you get. At any location, you can have a probabilistic classification of the units. For example, at this Units. For example, at this location here, we are, there is 93% probability of this unit belonging to unit 7. But there is also like 5% probability of this unit belonging to a country block. So this kind of information can be very useful when it comes to, after you version down, when it comes to the into pretition stage. So here, just moving to pre-space. Just a movie to pretty much show the same thing. You move to different locations, and we can show you the probability of any location with respect to all the units. Okay, so that's the first part. Second part, real quick, is again, we want to, once the inversion is done, we want to take advantage of the inversion results and maybe some existing geological information to make predictions of the location. Of the locations of potential mineral resources. So our study area is called Quest in this blue area. British Columbia has a lot of mineral resources. So in this case, the triangles, upside-down triangle, and triangle shows you the locations of the porphyry copper deposits for many of them. Our study area is in the test project area. What's interesting here is that in the north, we have the The north, this is our study area. In the north, in the south of our study area, we have some well-known copper-gold deposits. For example, this is Lauren, this is Mountain Milligan, this is Mountain Poly. But in the middle part, it's covered by a thick glacial layer. So, okay. I think I will just stop here. So, what we do is that we just I'm not not going to show you We're not going to show you the slides, but the basic idea is once the inversion is done. So, this is a huge area. We just do a lot of inversions. And then we cross-plot it. Then, in this case, we also have a huge database from British Columbia Geological Survey. So, they documented every single occurrence of mineral resources in British Columbia. For copper deposit, there are one hundred eighty two of them. So, we selected twenty of them, and then we look at their density and systemic values. Their density and systemic values. And then we map them out. And based on that, we identify some areas where we might have some areas where the inverted physical probability values are consistent with these known depods. So in the end, we make some predictions. Yeah, so all our work we've done in SimpAg, all the inversions, and the visualization in Visda. So Simpag and others, open source packages, really accelerate. Open source package really accelerated our research. So, thanks to all the developers and contributors. I think this is something that we should do moving forward as a community. It's really, really helpful for graduate students. Okay, thank you. Um, around your clustering it seemed like it was a pretty labour-intensive process. Process. Did you try any like K-Me? I have, yeah, we try that. We try that, and the result is pretty promising. We use just Q-minus clustering. The classification looks actually pretty similar. So we did that after we finished all the process. But the problem is this. First of all, with clustering, you still need to determine how many clusters you are looking at. There are some mathematical methods you can use, but this mathematical... Mathematical method you can use, but this mathematical method is just purely based on some mathematical criteria. Whether it's consistent with our geological perception, we don't know. So, even with clustering, you still need to decide how many clusters you're going to use. And also, second thing is that there's so many different clustering algorithms out there. The differences are the distance measures you used. For example, if you use p-minis clustering versus fuzzy sick, Fuzzy linear regression model, you get different models. So, again, you also need to determine what distance memory you're going to use. So, even with the unsupervised machine learning, you still have a lot of decisions you need to make. So, in the paper, we did it, because reviewers asked for that, we also explained the advantage and the limitations of the so-called automative methods. It's not fully automated, you still need to make a lot of decisions. So, I think that's what. A lot of decisions. So, I think that's one of the research going forward, research directions: is how to maybe quote-unquote automate and maybe make this process less labor-intensive. I guess you can use your geology as a starting point. Say we've got five main block types to see if we can get five plots to some plots. If you know that geology. Yeah, you have to have some geology in code tools. Okay, thank you. Thank you, Judge. So I know we have some questions for timeline on Zoom, but we run out of time, sadly. So again, post it on Slack and we can keep the conversation in there. So right now we are going to have a coffee break and hopefully at uh ten thirty we have two group 10:30 we have to group for the breakup discussion groups. We can shift it like five minutes, 10:35, so we can get some rest. Yeah, I think it's correct. So, see you in a bit. And with that description with that method, Yeah, because uh so that is the workout.