So, but Sarah, um, she was working on Google Brain until some very recently and uh developing models that go beyond test accuracy and especially with an interesting interpretability and developing co-factors and robust models. Also very interesting, Sarah has helped uh has funded a nonprofit uh harmonization called Delta Analytics in the in the Bay Area that works with non profits uh that works with non-profits for non-profit organizations for to try to enable technical activities in data analysis. And also she is one of the co-founders in the Trustworthy ML initiative, which also promotes seminars and forums related to Trustworthy AI. So today she will be talking about through the looking glass understanding model in paper and we are really looking forward to the talk. Thank you very much Sarah. Thank you so much. It's so lovely. Thank you so much. It's so lovely to be here. It is, I am admittedly very jealous of those of you there in person. So please enjoy the wonderful surroundings for all of us coming in virtually. But I was really delighted to receive this invitation because it's always really interesting having these spaces to think about what we mean by interpretability. And so I thought maybe I could use the time today. I thought maybe I could use the time today to share some of the ideas that I'm particularly interested in, but I believe we have an hour. So I'm going to share, I think that the talk I prepared will be 30 to 40 minutes, but I'm hoping we can open up for some fun discussion and just general hearing what ideas are interesting to all of you. So let's see how it goes, but do you feel free? So, let's see how it goes. But do feel free to also ask questions throughout. I'm happy to stop and chat more. So, as was mentioned, a lot of my research agenda is focused on how do we train models beyond just top-line metrics. So, maybe we have multiple desirable properties. This could be something like efficiency, but also robustness, fairness, interpretability. And as was mentioned, I, up until recently, was at Google Brain. I'm now thinking a lot about. I'm now thinking a lot about how we could democratize access to machine learning research and bring in new talent to the field. So I think for the next few months, I'm trying to chat to as many people as possible who have interesting ideas in these directions. So definitely feel free to reach out. I'm happy to chat with all of you in the context of this workshop or even once you have all gotten back home. If you want to find time, I've left my contact details there. Time. I've left my contact details there. But today, I want to talk about three concepts. So, one is that interpretability has often and is central to machine learning. The second, that top line metrics are not well correlated with other design objectives that we would like to fulfill. And that the third one is that we increasingly want to train models to fulfill multiple criteria, but this presents new and interesting challenges, and interpretability has a meaningful role. And interpretability has a meaningful role to play here. I'm going to be mentioning research with many different collaborators, so I'll try and emphasize which works correspond to which ideas as we go. But to begin with, I'd like to kick off with the reflection that discussion of AI progress has always been centered around comparisons to human performance. In the Dartmouth conference, the mandate was to impart machines with the ability to solve problems previously. With the ability to solve problems previously reserved for humans, with an emphasis on finding out how to make machines use language, form abstractions, and concepts. So very much the anchor is the human and the human's way of perceiving the world. We also see this in our first benchmark. So we see with the Turing test really the premises on how do you fool a human. This is somewhat interesting because the first bot to pass the First, bot to pass the Turing test exploited the foibles of what it means to be a human. So, pose as a 13-year-old Ukrainian boy where English was a second language. So, in many ways, what we're asking is for the model to capture human limitations as well as human achievements. And since then, the overwhelming focus of AI research has been on comparisons. So, many of our benchmarks from checkers. Benchmarks from checkers to deep blue to modern day. So go, Dota. It's worth thinking about the reasons which motivate this. We need a way to measure progress. And I do like this phrase by Turin. The extent to which we regress something as behaving in an intelligent manner is determined as much by our own state of mind and training as by the properties of the object under consideration. This, I often I often reflect on as being a double-edged sword because we often overfit to demanding that model behavior aligns with our value judgment. But in many ways, it's equally interesting and provides insight where it doesn't, the same way that there are interesting examples of intelligence which do not align with our own. So the mantle shrimp has 12 color channels instead of R3. R3. Hummingbirds see worlds that we do not, ultraviolet color. And the same for snakes, which use infrared to detect prey. Cows on average face north, natural magnetic impulse. And this is just to say that I think that in the field of interpretability, it is often interesting to take a step back. And the places where model behavior do not align with our own can be just as. Own can be just as interesting at teaching us something new about how to approach a task. Many Go players, for example, perceived Asian performance as beautiful as something to learn from. And even with recent studies, so we've learned from the retinopathy diagnosis of AI that often features can be exploited which we cannot see with the naked eye. So things like being able to extract whether a patient is a smoker or a non-smoker. Whether a patient is a smoker or a non-smoker. And I think this is an interesting lens to revisit because, firstly, regardless of whether we are lying or not, the emphasis of modern interpretability has always been a core building block of how we measure progress in machine learning. I think now, in many ways, it's just taken on renewed importance. And part of this has been the shift from handcrafted feature engineering to deep neural networks, where we delegate representation. Networks where we delegate representation of the task, and we see this huge gain in top-line metrics, but we give up full specification of the function. And this is, of course, the subject of much of our work in this room. But it is this trade-off, which I think is at the heart of the challenge that faces researchers working on interpretability. And the urgency to this is that, of course, we know that top line metrics May often fail to correlate with model behavior that we seek as desirable. So I call this accuracy without true learning. I really like to think of to introduce this Clever Hans the Horse. So Hans was a very famous horse in Berlin between 1891 and 1907. Hans was remarkable because not only could the horse compute simple arithmetic functions, Compute simple arithmetic functions, identify colors, count the crowd, but it was unclear exactly where the trick was. And so it took a very strongly grounded empirical experiment to discover that Hans the horse could not answer questions successfully if the human did not know the answer. So Hans the Horse was in fact picking up on microscopic facial clues. A classic example of high accuracy with Example of high accuracy without true learning. We often see where delegating learning of the function to the model has led to these clever hands moments. So the first two images you see is of a cow. The image on the right of the cow on the beach is predicted incorrectly versus the cow on the right. For the limo, the limo on the ice is predicted incorrectly versus the limo. Versus the limo on the left. And I think here this will be familiar to many as the composition of our training data leads to these spurious correlations with background being one and the over-indexing on patterns where because we have so many training data examples of a cow with grass, it's the model behavior breaks down in a different setting. We also see it in released models. We also see it in released models, so captioning systems where the captioning specifies sheep with this luscious green landscape or specifies dog with these children perhaps holding in the training data more likely a dog. And this is not specific to subfields. So we see these overfitting to spurious correlations in things like the leakage of private text. Of private text due to memorization or the generation of factually incorrect statements. And so this may be interesting or curious, but when Clever Hands moments happen in sensitive domains, there can be a huge cost to human welfare. For example, studies on skin lesion identification show leakage. So the presence of a ruler in certain data to measure certain types of tumors led to breaking down of the model when that wasn't present. The model when that wasn't present in the generalization. And even with pneumonia, so the presence of a metal tag in images from one hospital led to a correlation with the fraction of cases from that hospital. In both cases, this meant that we have these cliffs in generalization performance that lead to failure in model behavior. So we know that top-line metrics often hide critical model behavior. High-critical model behavior. And we also know that in deployment settings, it's necessary to go beyond these top five metrics. This might involve questions like: how does my model perform or how might my model perform? But in fact, this is only the starting point because it's often the core challenge is many of these questions depend on the prevalence of highly curated annotated data sets. And one of the reasons why we see Of the reasons why we see these breakdowns in performance on top line metrics versus what we expect in terms of other dimensions of model behavior is that our loss functions impose no preference for functions that are interpretable, robust, or guarantee privacy. I like this quote by Donald Knuth, who said that computers do exactly what they're told, no more, no less. And I really would reframe this as a model can. I would reframe this as a model can fulfill an objective in many ways while violating the spirit of said objective. So, this is where I think our work comes in. And in many ways, the goal of interpretability is to provide intuition and to aid the auditing of model behavior. It's a lens to these other properties and to understand if we are fulfilling multiple desirable criteria. With this in mind, I'd love to use this space to talk about a few ideas. Use this space to talk about a few ideas that I'm excited about right now, and then maybe we can open up because I would love to also learn a little bit about what ideas are exciting to you. So one is how do we evaluate interpretability methods? And I'll talk a little bit of my work, which has been focused on local interpretability methods. But second, a large thread of my research agenda recently has been on how do we efficiently How do we efficiently rank the data distribution? And how do we empower really researchers, but also deployment engineers to inspect slices of the data distribution that are of interest? And third, I think as we increasingly face larger and larger models, we need to have a better understanding and better interpretability tools for understanding how capacity impacts generalization properties because we know. Generalization properties because we know that generalization properties are altered as we vary the capacity of models. So, to begin with, to share some of my previous work on interpretability, a lot of it was centered on how do we compare the relative merits of different methods. So, local feature importance attribution are inherently local because they're Local because they're providing an explanation of a model behavior given a single example. And there are many, many such methods. This has been a subject of considerable interest in the field. It raises the conundrum, of course, of how do we choose which interpretability method to use? So, and of course, that's compounded by the fact that since we are seeking an interpretability method, we don't actually have any ground. Building method, we don't actually have any ground truth to anchor whether we've correctly interpreted. And this was a subject that I spent, along with collaborators, quite some time thinking about. And in many ways, a key criteria has to be that we want explanations to be meaningful, but there must also be fidelity. Because if we cannot guarantee fidelity, then Then, in some ways, we've compounded the problem. We're providing an incorrect explanation. So, starting from this premise that we want interpretability tools to be reliable and we want this to be consistent across models and data set, some paths are to define desirable properties. In some ways, these are to articulate properties or settings in which we expect certain behavior. Behavior. And these would be edge cases where there's constructive ground truth, and to iterate from there to compare different methods. And some work in this vein is edge case one. So this was work where we specifically looked at this premise that interpretability methods should be more approximately correct than a random guess. So remove and retrain, the premise of this work was that if information is truly important, remove. Truly important. Removing it from the data set and retraining the model should result in a worse model. Kind of asking your mechanic to guess what parts of the car should be removed to cause the maximum damage. And in many ways, the point of comparison would be if you were to randomly remove features. And so we show that, firstly, this has the requirement that we do. The requirement that we do retrain. And this is important because otherwise we're almost creating perturbations that add a kind of tricky underlying variable of are we being, is the difference in performance we're capturing because we're perturbing the distribution such that it's different from what the model was trained on. But we also were quite invested in making sure that we're controlling for things like feature. We're controlling for things like feature redundancy. So we evaluate over a range of different degradations and controlling for variance in performance. So we retrain several times to understand whether we have an accurate confidence bound for each level of degradation method. And the retraining is crucial because, as I mentioned, if we don't, we've deviated from the original distribution the model is training on. So it's unclear whether then we're Clear whether then we're conflating kind of sensitivity to perturbations with the actual method performance. So recent work has adapted this to NLP and again compared to different methods. And in fact, in this work, they recursively re-ranked to identify what words should be removed given all prior removals. But in fact, this work was interesting because it really showcased that many men. Showcased that many methods fail to fulfill this criteria of comparison. So, many methods do worse than just a random selection of features. And the second edge case I'll talk about is that crafting an edge case with the assumption that interpretability methods should be sensitive to factors that do impact the model. So, this scientific checks for saliency maps was a work in this vein. And in fact, this premise was. In fact, this premise was that interpretability methods should provide similar methods should not provide similar explanations when the network is progressively randomized, because at that point we have a junk network. So we should have sensitivity in the interpretability method if network is now junk and progressively randomized. The final edge case I'll talk about is that interpretability methods should not be sensitive to factors that do not impact the model. This is work we did where we This is work we did where we showed that if you have a common pre-processing step and you structure it so that it does not impact the model at all. So the gradients are completely the same. You should not see any impact on the attribution methods. And in fact, if you do, this is a point of failure. We actually saw you can explicitly craft an attack to manipulate the method, and that several popular methods are sensitive to this. Methods are sensitive to this. I think that this is an important direction of work because it's very important that we are not, we develop a more precise language around how we evaluate contributions to our field in this regard. However, I also think that after having worked on local explanation methods for some time, I've grown more considerate in what I'm doing. More considered in what I see as the ability for these methods to be totally faithful. Because in some ways, we're controlling, we're adding a lot of constraints. We're insisting that a local function or a local measure of importance be a correct estimator for model contributions. And I think that's always going to be harder. In fact, a lot of Going to be harder. In fact, a lot of my research lately has centered on it may be more powerful, and use cases will be different. There will be always the end consumer who will need their local explanation method. But I think for deployment engineers or for people who are ordinary models, it is likely just as important that we develop rich methods that have a more global view of the data set and, in fact, can surface subsections of the data for human or. Subsections of the data for human auditing. I think that the conundrum with local explanation methods is often what next? So, for someone who is faced with auditing a model, it is very tricky to go through millions of individual local explanations. And this is, you know, I'd be curious when we open up for discussion if there's any interesting or even strong opinions on this. But I think that we need complementary threads of work that are equally focused on how do we efficiently. Focused on how do we efficiently rank the distribution as a whole? And how do we empower people to look at slices of the distribution in meaningful ways? And so, recently, I've been working on how do we rank data sets by which are most challenging from the perspective of the model. And this can be either used to clean order the data set or used to improve training. So, there's many different use cases. And some of these subfields are well established with rich research threats, but Threats, but a lot of our work was on how do we do this cheaply. So, one direction I find interesting is that by now we know there's been interesting work showing that feature importance forms over the course of training in very interesting ways. We know that there's that easy examples tend to be learned very early on, that we have coherent gradients, so that there's certain features which we That there's certain features which we have effective representations for early on, and then we learn more challenging examples. We also know there's critical learning periods. So, if you remove certain features early in training, the model cannot recoup performance on those features if introduced later. And this will suggest that training signal in itself is a really interesting lever to use in terms of understanding what parts of our distribution are more challenging. This was a premise of the work. This was a premise of the work on estimating example difficulty using variance of gradients. And in this, what we did was we computed the average variance of gradients for an image over training. And so really the premise was that for many examples, the variance is low if there's coherent gradients. So if the gradients stabilize early in training, and these are early examples. But for much more challenging examples, the gradients tend to be very erratic as the model. Gradients tend to be very erratic as the model places it in different parts of the decision boundary. And so it's a nice lever to sort the examples by difficulty. And we found it was very effective in terms of discerning semantic grouping. So you can see these low VOG images tend to be much less cluttered and tend to be well positioned, well-centered, more prototypical. And the highest VOG tend to be much more, I would say, unusual vantage points. I would say unusual vantage points, maybe more cluttered backgrounds. We also found that it effectively discriminated between easy and challenging examples. So when we looked at top one error on these examples, we saw that high VOG tended to correspond with much higher error and low VOG with much lower. And we also found that these Also, we found that these, this, the what VOG showed was that the easy examples were learned early in training and harder examples required memorization later in training. The third idea I want to talk about is really a lot of my work on trying to measure how capacity impacts generalization properties. And in some ways, this thread of work is related to when we have multiple desirable properties, such as efficiency, high. Such as efficiency, high performance, and fairness, how do we measure trade-offs between them? So, this is a particularly pertinent question because we are in the midst of a bigger, better race in the number of parameters. So, it's a surprisingly compelling formula, but we are seeing larger and larger models. And this is across many subfields, but But I will say there's an argument in favor of this approach. So it's an interesting path of research, I think, to establish how differences in generalization occur. And also, I think there's many people interested in it because it's a very simple formula. So perhaps it's in some ways due risk. But I think it makes it increasingly urgent to understand how capacity impacts. To understand how capacity impacts generalization, mainly because, in some ways, this is establishing the Pareto frontier, but I suspect the hard work will be how do we do this far more efficiently? So it's important to understand, well, what are we getting for capacity? And how do generalization properties change as models get bigger and bigger? The second reason why it's urgent is that it also has put immense pressure on design. Immense pressure on design choices at inference time or deployment time that are applied often without consideration. So, a larger and larger models means that we're compressing at test time, we're pruning, we're quantizing, we're fine-tuning, but often the change in properties is not well-ordered because it's often seen as like the last thing that you do. So, considering how these design properties impact the model is very important. So, this is So, this is particularly true because most of the world uses ML in a resource-constrained environment. So, we are often forced to make choices for the efficiency design at test time. So, I'm going to talk about two papers that really convey these themes. And then I think I would love to open up for discussion after that. So, in these words, So, in these works, we asked how does model behavior diverge as we vary the level of compression? And so, we looked at this along a few different dimensions. One was robustness to certain types of distribution shift. And the other was to measure divergence at a class level and exemplar classification performance. So, I think that sparsity is a very useful mechanism to understand the role of capacity, but also to provide interpretability into an almost Into an almost counterfactual because you have the same structure of model, you have very similar levels of top line performance, but you have very different levels of overall parameters. So we can precisely vary the level of sparsity. So that's one nice aspect is that we can precisely control the end sparsity. And also, we it's kind of interesting, but sparse models easily outcompete dense models at similar parameter counts. So we're actually holding Parameter counts. So we're actually holding model quality fairly stable, even though we're radically changing the number of parameters. And what's interesting is that we find that low, that the majority of weights are used to memorize these very rare examples in the data set. So in fact, like one of the key findings from this series of work was that what we're doing with most of our capacity in these large models is spending a lot of Lot of compute and parameters to memorize the long tail of the data set. And so when we remove these weights, what we do lose is performance on the rare examples. And we can see this when we look at what a compressed deep neural node forgets. So we call these pruny identified exemplars, and these are the examples where performance diverges the most. And we can see, oh, good, I did include some of these. Excellent. So I don't know if anyone wants to venture a guess. What is this? Venture guests, what is this? What is the true label of this image? I think if you shout it out, I may be able to hear it, but I'm not certain. We need one brave volunteer to try throw something into the vest. And let's see if I can hear you. A round pardon? A ground or a disc? A disc? Okay. A disk? Okay, close. It's a toilet seat. So, could I understand where you're going with there? How about this one? This one looks like something all of you might enjoy later. Okay, excellent, Pierre. I would have said something similar. So, this is espresso. So, and these are some examples. It looks like I didn't include the extended set in this talk, but what you may have noticed is that they were quite challenging. So, we in fact found that they were more challenging even for a non-compressed model to identify. And what this was really showing was that we did a human study to evaluate what was the composition of these images. And they tended to feature both, did I include that? Did I include that? Okay, I didn't include that chart about the human study, but we found that they tended to include or over-index in images where multiple labels were appropriate, as well as these atypical examples where it's very much at the decision boundary between different classes, also incorrectly labeled examples. So, really, what was being lost, what was disproportionately impacted by changes of capacity, was the long tail. The long tail. And often, how a model treats underrepresented features in the long tail coincides with notions of fairness. So, we've seen this in previous work. And this is really because shortcut learning is very often due to relative under-representation. For example, in something like Celebe, we could structure it so you're predicting blonde versus dark-haired, and the model. Versus darkhead, and the model may learn to correlate blonde with being a female because there's far fewer blonde males in the data set. This results in higher error in the long tail, and it also can lead to undesirable spurious correlations. So what we found when we structured this, we also found that compression disproportionately impacts underrepresented features. And if these are protected attributes, it amplifies notions of disparate error rates. So it amplifies error on the unprotected. So, it amplifies error on the underrepresented protected attribute. And so, we show this using a few different data sets: Celeb A and then civil commons data set. I do want to mention another work because I think it provides a more nuanced framework to understand what exactly is happening. So, we did a work where we looked at the impact. Where we looked at the impact of pruning for low-resource machine translation. In some ways, this was a really interesting double-bind scenario. So, we have a limited data regime and we have compute resource constraints. And what we found was, in many ways, this reinforced our previous findings. So sparsity disproportionately impacted performance on the long tail. However, we also found that for out-of-distribution data sets, Distribution data sets, high levels of sparsity consistently improved generalization. And this was really interesting. And in many ways, it was because of the composition of the training data for these low-resource languages. So for low-resource languages, one of the most prevalent data sets in use is Jehovah's Witness. So JW300, it's a very specialized religious corpus, but it's Specialized religious corpus, but it's because many of the texts were translated by missionaries in different languages. And what this means is that rare artifacts in this data set are even rare in other settings we wish to generalize to. And so in many ways, this relates to a wider question: when do we want to curb memorization of rare features? And when is memorization beneficial versus harmful? Harmful. So, whether pruning aids or impedes performance depends on how relevant learning the long tail is for the task. And so, it depends on the source of uncertainty in the long tail. We often find two common types of examples in the long tail, these noisy examples and atypical. And so for in distribution, if your long tail is dominated by noisy, Tails dominated by noisy, it's almost bad memorization, and pruning can help. But often, our long tail is dominated by these atypical examples, and this is good memorization. This is when we have, like, for example, an underrepresented feature that we want to correctly represent. And for added distribution considerations, which is the low resource setting that we looked at, if the data is very different from the training distribution, then it's a misuse of parameters to overfit to the training distribution. Use of parameters to overfit to the training distribution, and we see this lift that we saw with the JW300. But if the data set is very similar, then you've looked at the ray artifacts still remain relevant. And I think this is a conversation which is still not quite nuanced enough when we talk about memorization. We typically talk about it as in without the nuance that really this can sometimes be beneficial versus harmful. Is harmful. What is coherent across all these settings is that memorization is currently painfully expensive. So we're using the majority of weights to memorize the long tail. So what I like about this direction of work is that when we have interpretability into what is the cost of learning representation for different instances, it becomes very clear there's very interesting directions for efficiency and understanding better our data distribution. And understanding better our data distribution and revisiting what we should spend more capacity on. And this has far-ranging implications because most of the data in the real world follows this distribution and is low frequency. And so if we want to model the world, we need to design models that can efficiently navigate low frequency events. I'm going to pause there. I really do want to open up for discussion, and I'm glad. It looks like I've left us 20 minutes to hear your thoughts, and I would just love to hear. Your thoughts, and I would just love to hear what ideas you are all excited about. I'll leave this here, but maybe I'll actually turn off the slides and we can chat unless there was, I'll keep it up for now in case people had specific questions about the slides, but maybe yeah, we can see how that goes. But yeah, I'd love to open up for a discussion. Thank you, Venus. Now we open the whole discussions. Can you hear me? I can hear you. Excellent. Thank you for the talk. It was super interesting. And I have actually two questions. So the first one is about the feature importance variance that you studied. Yeah. So what's your view? What's your view on how why the uncommon, sorry, the hard example have more variance? You think that it's, because I studied like uncertainty. So my question is, you think that they have more variance because they are close to the boundary and the the silency tends to have like label specific feature importance or you think that they are they have uh more variance because they are away? more variance because they are away from the in-distribution data of the lab and I think they are two cases. Yeah, so it's interesting. So we actually don't make any assumptions about the relation with the decision boundary, but I agree that's like a separate, really interesting thread of research. And I could, I think there's several interesting comments to be said there because it depends a lot on the source of uncertainty. So I suspect it's a it's So, I suspect it's not a single answer. In fact, I think we need more work there distinguishing between sources of uncertainty, because I often think we too crudely make it a binary problem of hard versus easy. But even within hard, we have these subsets of noisy examples versus fine-grained examples, which would be close to the boundary, but in between different decision boundaries. But let me put that to the side for the moment. So, here we make no assumptions about So, here we make no assumptions about the relationship with the decision boundary because we're leveraging variance across time steps, which I think is a very useful lever to explore because so in many ways, our assumption is that easy examples have coherent gradients over time. So, the model learns essentially how to place that example on the That example on the decision boundary, and as such, the gradients become much more consistent over time, which means the variance is low. Whereas hard examples, we already know that hard examples, most the majority of the last stage of training is in the memorization of hard examples. So those gradient updates are going to be, at least the magnitude is going to change far more in the later stages of training. And so that's why we see this high variance. So we don't need to say anything about the relationship of this. Need to say anything about the relationship with the decision boundary, we only need to say about the relationship with the time of training, which is quite fun in a way if you think about it. But go ahead, yeah? Very interesting, yeah, thank you. And yeah, yeah, I think that would be very interesting to see the relation between this study during training and the final result in the uncertainty. Also, using Also, using your metrics, or the variance of the surgery. And so, the second question was about the latest things you showed. So, about the long tail. So, I was into some work on grooming and the lottery ticket and those kind of things. So, my question is: if you tested something about the lottery ticket, so if you find a good sparse model, Good sparse model. Do you expect that it still knows the long tail or that? Yeah, that's an excellent question. Find a good spa model, I would expect that it also knows the long tail in some sense. Yeah, so it's not clear. So in some ways, the objectives of lottery tickets are the same as the objectives. So here, the detail I omitted was we use magnitude iterative pruning. So the difference, magnitude iterative. The difference, magnitude iterative pruning kind of slowly introduces sparsity. Lottery ticket almost rewinds to a good initialization and introduces sparsity, but you're still constraining capacity. So your lottery ticket is ideally much smaller in the number of parameters, but converges to the same performance. So similar to magnitude pruning, lottery ticket is exciting for different reasons. The dream is you can start sparse and you can just find the best in the ship. Find the best initialization. But I suspect the disparate impact will still be the same. In some ways, much of the focus on pruning efficiency has established a trade-off only with respect to top-line metrics. So, for all these methods, whether it be lottery, ticket, or magnitude pruning, there has been the takeaway, we have almost negligible changes in top line. Changes in top line accuracy for fairly radical changes in parameters. And I think what this thread of work has shown is that the issue is we've been focused on top line metrics. And so if you look at, in fact, what parts of the distribution are disproportionately impacted, you do see something like this, where it's impacting the long tail. So I do suspect we would see something similar. Although I will say this. So informally, I have been told by another researcher that they Another researcher that they have been working, they found in the course of their experiments that variational dropout as a technique, so pruning technique, does appear to have a slightly improved robustness. So one of the results I didn't show was how it amplifies sensitivity to adversary examples. There is also subsequent work that has come out that has been designing efficiency techniques which should achieve a better trade-off. And I think this is a natural like This is a natural, like, subsequent chapter to this work. If we know that we have this disparate impact that it can amplify fairness concerns, we can design techniques that achieve a better trade-off in operetto frontier of things that we want to optimize for. So that I think is very promising. Thank you. Yeah, oh, and by the way, I do think it's very interesting. The idea direction that you were talking about. How does establishing what is challenging over time correspond to a relationship with the decision boundary? That could be fun. Thank you. Thank you. So one question for me related to this special energy models are getting larger and larger, as you mentioned before. So do you expect that at some point they will start growing and maybe getting sparser? Do you think that we can achieve? Getting faster? Do you think that we can achieve similar accuracies for, let's say, automatic translation with the smaller models? Yeah, so I think there's a few questions baked in there. So right now we show that we can sparsify considerably using iterative techniques. So you don't prune, you don't start sparse, but you can introduce sparsity and you can achieve at least using topi-metric similar performance. Taltimetric similar performance, of course, you give up some considerations on your rare features. So, that I think is a well-established literature which says these are, you know, sparsity is a very promising way to do this and to try and achieve efficiency trade-offs. There's a when I think a question which is very exciting for efficiency researchers right now is: can you start sparse? Because in many ways, the limitation that we have to sparse. The limitation that we have to start dense is a considerable one. It changes the computational cost, it makes it less accessible for some groups to train these models. So if we could strive sparse and we could achieve a similar representation, that's very exciting. That's the lottery ticket hypothesis vein. But also, there's a growing body of research that I think is quite exciting about dynamic networks, which is we should be able to adapt the representation. Adapt the representation as we optimize. This relates to an underlying thread in my research, which is clearly not all data points are created equal. And understanding what data points are more challenging is very interesting from an interpretability auditing perspective, but also in terms of how we make computation more adaptive. I think this is one of the richest threads of research, which is if we say that we care about model behavior. About model behavior on certain parts of the distribution, we should make that explicit. And I think that there are many threads that are interesting in that respect. The question is how to make it more cheap to do so, to leverage signal that tells us that some parts of the distribution are of interest. That's a challenge because a lot of the work is still too expensive to extend over ranking the large data sets that we have. That we have, so that's interesting to me. Okay, thank you. Any more questions? Maybe I have one. Can you hear me? Yeah, I can hear you. Awesome. So, do you have a question regarding the examples difficulty work? I was wondering first, like, how, like, on which model are you computing the gradient? Like, is this something only for a complex model? So, you could use maybe also adding a model. You could use maybe also an inner model as a proxy, like look at the gradients of an inner model as a proxy of the difficulty. Oh, okay. So, this was the estimating example difficulty using the variance work. Yeah, so we tried it on a few different models. So I think it ranged from like wide ResNet to we used another ResNet formulation and then we used a simple, simple modeler. We used a simple, simple modeler. I think it was like three layer deep. So I think your question was: could you use an auxiliary model? Was that it? Or I'm not sure I understood the second part of the question. I'm wondering if you see the same effect on using a simple model because it sounds like if you use a complex model, then it would be like quite time consuming and quite a lot of computation to just estimate the implicity of the examples, right? Yeah, I wonder if that's, I think that's really interesting. If that's um, I think that's a really interesting uh perspective. Um, yeah, I could see that there has been some work in that vein, so not necessarily seeing if it would be a good proxy for the gradient from the larger model, but there has been work showing that you could use a much simpler model and then just classification error to try and determine whether an example is challenging or not. There was another paper which was trying to leverage the depth of correct classification. The depth of correct classification to understand example difficulties. So, I do like that idea of maybe leveraging capacity itself as a way to see if it's a good proxy without going through the full network. Yeah, that could be fun. Thanks. Hi. So, I you talk about the role of capacity, of network capacity for the problems of the network. The performance of the network and particularly in the long tail part. I was wondering if you have considered the role of the training policies that you use. So I see a lot of learning rates decreasing and weight decay and all this kind of stuff. It seems to me that by the time the network tries to stop learning, we keep pushing it to learn until it overfits to these few examples that we eventually are complaining about. That's what you're complaining about. Yeah, this is such an excellent paper. I have been thinking about this ever since a paper. So, what was the, it was, it was a paper that came out two years ago, and they were looking at memorization in particular. And they found that the propensity for memorization was very impacted by not only learning rate, but the optimizer in question. But it was very interesting because they only had it as like, you know, when people have that small subsection. You know, when people have that small subsection of subsection of experiments, and ever since I've been like someone needs to do this proper justice and exploration. So, yeah, do email me and I'll try and find the paper and dig it up. But yes, I think it's, we know certain things. So we know that high learning rate results in less memorization in general, whereas increasing your learning rate, I mean, lowering your learning rate earlier results in much more propensity. But the role to optimize. Propensity, but the role of the optimizer itself is also underexplored. So I think that's, yeah, excellent research question. I feel like it's been on my list of things I wish someone would explore more properly. Yeah. Yeah, so I was thinking about this good versus bad memorization that you introduced before, and it's super exciting because I think coming like working on Because I think coming like working mostly in the medical field, we're all afraid of memorization. For us, memorization is bad, right? We're showing these images, especially for things that we don't know how to do yet. So we're showing these images with the retina and saying it predicts blood pressure. And I was wondering how can we characterize what is bad memorization, if the ability can help us for that? Because we could eventually look at that. Yeah, so this relates to a wider, I think, very urgent question for the machine learning field is we need a more precise language to discuss uncertainty because really right now we just have Right now, we just have really favored probabilistic uncertainty in machine learning, which means we accord, you know, kind of a ranking, which the magnitude tells us what is certain and what's uncertain. But in fact, even within high uncertainty examples, we have different sources of uncertainty. And we don't have a mature language in machine learning to discuss this, but it's so important. To discuss this, but it's so important because good and bad memorization relates very much to sources of uncertainty. So, everything in the long trail of the model, because we've trained models using empirical risk minimization, everything in the long trail is highly uncertain from the model's perspective, but the sources of uncertainty are very different. Like for atypical, the source of uncertainty is a frequency problem, but we want the model to learn it, you know, for many cases, because we want the model to learn rare instances. Learn rare instances. But often, if it's noise, it's garbage, or if it's incorrectly labeled, you know, we have, we often term that under noise, noisy problems, but we treat it entirely separately from how we treat, you know, learning the long tail. But these are very much in the same vein. They're just different sources of uncertainty. And there, it's bad memorization, you know, for many different reasons, but it fails to generalize. Fails to generalize. So I think that having a more mature language about irreducible, reducible error, but also about how to leverage model signal to distinguish between these is very important. It's something I've been thinking a lot about recently, but for me, that's the crux of how we distinguish between good and bad. We need to have better tools to almost disentangle the sources of. Disentangle the sources of uncertainty and not just talk about it as a single ranking. And yeah, I've been thinking about a few ideas around that problem. But yeah, if anyone else has ideas, I'd be delighted to talk about it more. Okay, so with that, let's close the discussion and let's thank Sarah again for the presentation. Thank you very much. Okay, and with that, have now a flash talk by Diana.