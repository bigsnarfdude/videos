Uh, model machine learning for uh spatial data. Um, the first general category of approaches, I call them residual crigging. What they do is that they ignore the spatial information altogether and just use the covariates and fit like a non-linear machine learning method. It can be a random forest, a boosted tree, or a neural network, and it gets the estimate of y as some non-linear function of x. But because the But because the data is spatial in nature, there is spatial covariance or spatial dependence to account for. And to do this, what they do is that they take the residuals from this non-linear regression and then do like a fit a Gaussian process on these residuals. And so then they can create anonymous locations using just the standard Gaussian process methods. It has been done for random forests, and these papers have been like pretty well cited and quite popular. The advantage of this, it's very simple to use. You can just fill This is it's very simple to use. You can just fit, use any like neural network or random software, fit it, take the residuals, use any creating software to do creating. So it's very simple to use. The downside of this is that during the estimation of the mean part, it completely lose the spatial dependence. So it doesn't take into account any spatial information, which we will show that it impacts the estimation of the mean. The estimation of the mean, and the poor estimation of the mean will, in turn, impact the predictions. But the impact on the predictions is much less because often the spatial, the cradium kind of tends to explain some of the things that were not explained by the mean. But generally, it will affect the mean estimation part quite severely. The other approach, and again, it's a very reasonable approach, is to create a set of spatial features or spatial basis functions. Spatial basis functions. And instead of just using the covariates x, you use an augmented set of covariates where you will have x as well as this set of spatial basis functions. And then you fit a neural network or random forest on your response with this augmented set of overs. And this has been done for random forests, for neural networks. Again, it's a very simple idea and the one that can be implemented using existing software, so which are really nice. The downside is. The downside is that this is a, it becomes a fully machine learning type of method. It kind of leaves the traditional mixed effects spatial model. Well, that itself is not a problem, but then it doesn't give you a separate estimate of the non-spatial effect of the covariant effect. The mixed effects model separates the fixed effect and the random effect. So you can infer on the effect of the covariance through the estimate of the fixed effect. You can infer on the effect of the spatial dependence through the random effect. This one doesn't give you that. This one doesn't give you that, it will give you a joint predictive function as a function of both the covariate, the non-spatial covariates, and the added spatial basis functions. And also, this is very sensitive to the type and number of spatial features added in the sense that if you just add the latitude and longitude, it will generally not do well, although all basis functions are functions of the latitude and longitude, and you are doing non-linear modeling. So, in theory, it should do well, but it doesn't. Well, but it doesn't. If you add too many basis functions, it can drown out the effect of the non-spatial components. There is a balance here that needs so you need to sort of carefully create and curate the number of basis functions. And the third method, which is what I'm going to be talking about today, is sort of embedding machine learning directly within the spatial mixedFX model. So we have this spatial, we had the linear mixed effects model. Now I don't use the linear part. Use the linear part and then simply consider a non-linear least effect model where here m is going to be a non-linear function which I'm going to estimate using a machine learning method. Everything else is the same as a mixed effects model in the sense we have the spatial random effects model using Gaussian process. We'll have your suitable link function depending on your data types. Everything else is the same, keeping the traditional structure of the mixed effects model, just swapping out the linear regression with the non-linear regression. Regression with a non-linear regression, and that and we'll use machine learning to do that. So, it, as I said, it retains all the advantages. It is additive in terms of the non-spatial effect, which is a fixed effect, and the spatial random effect. We continue to use Gaussian processes to model the spatial random effects. It will have the same nice coverage of functions with interpretable parameters. We'll have the same advantage of being able to do predictions of new locations using CRID. Newgate patients using CRIP. Okay, but the question is: how to use like a random forest or neural network when you know there is this high-dimensional parameter WS, which is a spatial random effect to model dependence in the data. You cannot just simply ignore it and use off-the-shelf algorithms for this machine learning method. So, you have to kind of create specialized versions of these machine learning methods that adjust for this spatial event. Methods that adjust for the spatial dependence. So, in this third line of work, there has been quite a lot of recent interest. We had this paper where we used random force to do this back in 2021, and we recently did one for binary data, which is quite different because of the nature of binary data requiring a different link function and different loss function. Fabio Sigrist did a similar thing with. Did a similar thing with gradient boosted trees, which came out in JMLR in 2022. Again, a very nice way of doing boosting while you account for the spatial dependence through a Gaussian process covariance matrix. And then the work I'm going to be talking about here today is how to use neural networks in this setting when accounting for the spatial dependence. So before I talk about this work, I'll just present one result from this random first paper that shows how this Paper that shows how this method can sort of sort of address some of the shortcomings of the two other approaches. So these are the three schools of thought that I talked about. This is the random forest residual crigging, which ignores the spatial dependence when fitting the mean, and then the screen of the residuals. This is the spatial random forest, which adds a bunch of spatial basis functions with additional covariates and then fits the random forest. And this is the random forest GLS. And this is the random forest GLS method that we did. And these are the prediction mean square errors. And the x-axis is a function of the spatial signal to the covariate or the non-spatial signal ratio. So when the x-axis is large, the spatial signal dominates the data. When the x-axis is small, the covariate signal or the fixed effect part dominates the data. And you can see there's a very nice phase transition between these two different approaches. Two different approaches. So, when the spatial signal is small, the random forest residual trigging tends to do well because it ignores the spatial dependence when estimating the mean, and it is fine because the spatial signal is small. So it tends to do relatively well, but then as the spatial signal increases, its performance worsens because it's ignoring that strong spatial signal in the data when estimating green. On the other hand, the added spatial signal is not a signal. On the other hand, the added spatial features method, the spatial random forest, when the spatial signal is really weak, it is adding a bunch of basis functions, and that is kind of drowning out the non-spatial covariates. And so it tends to do, because it's kind of creating an unnecessary high-dimensional set of covariates, out of which only a small part, which is a non-spatial covariates, are relevant. There is not much spatial signal in the data. It tends to do really poorly. It tends to do really poorly, but then as the spatial signal increases, you would expect it to do well and it does well. And then this is the baseline, which is our method, which tends to do well across the spectrum because it is using the Lausanne process to model the covariance. And so the estimates of the parameters will determine whether the spatial signal is strong or weak. And so it tends to be able to estimate the spatial parameter in a data-driven manner to determine the degree. In a data-driven manner to determine the degree of spatial correlation. So that's the motivation of why we want to also do this for other methods like neural networks, which are much faster than random force. So as I said, I'll just focus on this one for our talk today. So I'll start with what neural networks are for the task of regression. So neural networks are simply a non-linear class of functions for regression. So this is a very simple linear. So, this is a very simple linear neural network. It's called a single-layer perceptron. It simply starts with your X, then multiplies it with the weight matrix to get Wx. And so you have your X, which is a multivariate covariate. So these are the components of X. It multiplies with the weight matrix to get the Z's. And these weights are just a linear transformation. And then it uses a non-linear transformation to go from the Z's to the hidden layer A's. And this non-linear transformation. A's and this non-linear transformation will be like a sigmoid transformation or a piecewise linear transformation, and it's typically known. And then from the A's, you then go to your final layer, which is the output layer, with again a linear transformation. So this is the beta part. So this is linear, non-linear, linear. Okay, so because of this non-linear part here, the whole function class becomes non-linear. If g was a linear function, then this will just become a reparameterization of this. Become a reparametrization of the linear equation. Okay, so you go from your covert x to your output O. O is simply m of x, where m is this function here. And so you fit the o's to your y's using some loss function, and you estimate your unknowns, which are these weight parameters here and here. The activation parameters are usually health fix for neural networks. And you can extend this idea to more than one hidden layer. So, this is the multi-layer perceptron, which simply uses the same idea and uses composition L times to create a sort of a larger class of non-linear functions. So, again, you go from your original coverts X, a linear transformation to get the Z's, a non-linear transformation to get the first hidden layer A, and then you kind of keep repeating the same thing to get your, so this will be an L-layer neural network, which will. This will be an L-layer neural network, which will have L many hidden layers, and then a final linear transformation on the final hidden layer to get your output layer O. And you compare that with your response. So it's just a class of nonlinear functions with certain parameters that you estimate using the same methods as you would do for any other parametric problem. So, the loss function that's typically used is the squared error loss function, which is the y minus the m of xi, and m is the output layer. So, it's yi minus y. Parameters are updated usually using like a local, like a gradient descent where you update it. The t at the t plus one at iteration, it's the t at iteration minus the some constant times the gradient. Gradient. And these gradients are often available in closed form using a chain rule, which is often termed as bracket propagation in the machine learning literature. And the reason neural networks have become very popular is because of what they use called mini-batching, which is basically just sub-sampling, but in a way where you partition the data into small sub-samples. And in each iteration, you don't use the whole data, but you just use one of those mini-batches, and then you cycle through all the mini-batches. So at each path, Batches. So at each pass, you're not looking at the entire data, just using a sub-sample of the data, and then you take a cycle along all the mini-batches. And stochastic related descent is a particular case of mini-batching where you use a batch size of one. So at each pass, you're just using the fit on one data point, and then you circulate, you cycle through all of your data points. Right. So our focus here is this loss function. Focus here is this loss function, which is the OLS loss. And if you recall, in linear models, when you have dependence in data, we'll never use the OLS loss. So, what do we do in linear models? So, suppose this is the linear mixed effects model. We will marginalize it out. This is our model. If this is our model, we'll rarely use the least square or the ordinary least square loss to estimate data, right? We'll use the generalized least square loss that uses this spatial covalence function. So, we want to do the same thing here. The only difference is that instead of the linear part, we have the non-linear model, which belongs to the neural network function class. But we want to use the same technique in the sense that we will, in the linear case, if m was a linear model, we will use the generalized least square loss to estimate beta. It's the maximum likelihood estimate of beta condition on sigma. In the non-linear case, we'd also want to use this as our loss. We'd also want to use this as our loss function instead of the ordinary least square loss because the ordinary least square loss will ignore any information about the spatial coefficients. But it's easier said than done because this is expensive. The computing sigma inverse is order n cubed, which is also what Doc talked about yesterday. Unlike the ordinary least squared loss, which is additive over the data points, the GLS loss is non-additive, so it's not amenable to mini-batching. It's not amenable to mini-batching. So it loses one of the main powerful aspects of neural networks, which is mini-batching and working on small subsets of the data. And of course, the covariance matrix sigma will contain these unknown spatial parameters that needs to be estimated. Okay, so to work with all of these issues, we use a particular kind of Gaussian process covariance called the nearest neighbor Gaussian process and Doug introduced this yesterday. It's based on the Vecchier's approximation. On the Vecchia's approximation. And again, I think we had a detailed discussion of this yesterday, but basically, if this is the full GP model, Vecchia's approximation, which is, I think, one of the most seminal works for big spatial data, reduces the conditioning set to only M many nearest neighbors, and that reduces the computation from order n cubed to order n. So we showed in this nearest neighbor Gaussian process paper that the Neighbor Gaussian process paper: that the Vecchia's approximation corresponds to a valid probability distribution, and not only that, it can be extended to a valid Gaussian process, which we call the nearest neighbor Gaussian process. And I think Raphael said yesterday, one of the nice things about Vecchia's approximation is that it's not only an approximation, but it's so much more in the sense it's a valid probability distribution. So, you can do so many more things with it. And which is what we do here: is that if you have this nearest neighbor Gaussian process covariance. Nearest neighbor Gaussian process covariance metric sigma, then sigma inverse factorizes on the nearest neighbor directed action graph you're using to create the neighbor sets. In the sense that the Cholesky factor of the inverse covariance matrix is a sparse lower triangular matrix, which will have non-zero entries only if there is a directed edge from the eye location to the j location. Why this is useful here, if you look at the GLS law, If you look at the GLS loss, a GLS loss with a covariance matrix sigma is simply an OLS loss between the decorrelated response sigma inverse half y and the decorrelated output sigma inverse half O. And so we can think of GLS loss as simply an OLS loss between two decorlated things. And if you're using a nearest neighbor Gaussian process, this decorrelation is simply a convolution on this directed acyclic graph because you're just using the weights. Because you are just using the weights that correspond to your directed neighbors. And so a decorrelation in a nearest neighbor Gaussian process is simply a convolution on the graph. And there is a vast literature on graph neural networks, which uses convolution on graph layers, on graph nodes to create further layers in the neural network. And so we can write down NNGLS as a graph neural network when you're using NNGP for the covariance matrix. For the covariance matrix. So we start with X, we have the L layer neural network to get the output O, and we can do this for all of the nodes in our nearest neighbor DAG. And then from the raw output, you can go to the decorrelated output, which is simply a graph convolution using the nearest neighbor creating weights. You do the same for your raw response. The raw response lies on the directed acyclic graph, so you can create the decorated response. Graph so you can create the decorated response using the creating weight. So these are simply multiplication using the Cholesky factors represented as graph convolutions. And once you do that, you can do everything that you have been doing in neural networks, like mini-batching. Now this is an OLS loss between the decorated response and the decrypted output. So instead of using the entire data, you can use mini-batches of the data and get your scalability back. So can you? So, can you? So, you know, I typically just work with linear algebra. And so you have a sparse Cholesky factor. You're multiplying it times something. End of story. Why call this a graph convolution? What's special about that? That's a very good question. It's the reason we want to use neural networks, the features of neural network, which is the mini-batching. So this is one of the The mini batching. So, this is one of the reasons we can then use mini batches instead of using the whole batch because now this is an OLS loss. And maybe let me show you the couple other things. So, the next one is parameter estimation. So, now all the parameters are now weights in the different neural network layers. So, the L layer neural network weights are here, and the Krigging weights that are used in the sparse tolestifactor are here. So, you can use the Here, so you can use the same method to update all of your parameters. But you can also choose to update your spatial parameters in the way you do using just the likelihood-based method. That's fine too, but this gives an unified framework to treat all the parameters in the same way. And then lastly, for Krigging, so if we want to predict at a new location S new and we are given the set of coverts X new, what happens is that we pass X new through the neural network. We pass X nu through the neural network to get the O nu, which is the output layer. We use the graph convolution, which is our estimate of the decorated response. And since convolution is simply a multiplication with a lower triangular matrix, it's an invertible operation. So you can do deconvolution and get the raw response back. So you can do a lot of these things within the graph neural network. And I have a slide at the end why the graph neural network is more general in the sense. Is more general in the sense it will allow future extensions that are not in the traditional geospatial framework. So, just to summarize, we work using the same geospatial model, just replace the linear regression with the non-linear model, which we estimate using neural networks. The only difference is that we use the generalized least square loss, which is what we'll be using in a linear model, anyways, but which is not. Anyways, but which is not what the standard neural networks use. So we use the generalized least square loss and then we frame it as a graph neural network that allows sort of parameter estimation and all these standard tricks like mini-batching, back propagation within the standard neural network. Easier to code up as a software. And the complex time complexities is linear in n. I'll skip the theory slide, but there was. The theory slide, but there was very little theory for neural networks for dependent data. I didn't see anything when we started working on this paper. Even in a non-spatial setup, there is very little theory. This was the recent work that came out, but for IIT data and for OLS laws. So we were able to extend this for dependent error processes and using the GLS laws. We showed that if the data is generated from a matter and GP on an irregular set of locations, then NNGLS is going to give you a consistent estimator. You have a consistent estimator of your non-linear mean function. Just a brief look at simulations. So, for estimation, we compare with the non-spatial model and then GLS, because all the other methods do not give an estimation of the mean function. For prediction, we compare various approaches that uses the spatial information, like the residual crigging or adding locations to the covariate list, or adding basis functions to the covariate list. This is the estimated. This is the estimation performance. This is kind of expected because the non-spatial model does not use any spatial information. So we are kind of beating up on that. But it's important to know that the other methods do not give an estimate of the mean function m. So this is kind of the only one that we can compare to. For predictions, this is the non-spatial model. So we can ignore that. We can focus on these four. So the brown is our method. The yellow. The yellow is the non-spatial model plus residual crigging, which tends to do pretty well, at least in the simulations. And then the two shades of blue are different approaches of using additional basis functions in your set of covariates. And that sometimes they do pretty well, but it often depends on what basis functions you choose and how these are the runtime for our different components of our model. Different components of our model, just verifying that the runtimes agree with our theoretical runtime, which is linear in the sample size, which it does. Just one slide on the PM2.5 data analysis. So we were predicting PM2.5 using six meteorological covariates, and we compared the same methods as we compared in our simulations. So for this prediction performance, the NMS plans, which uses basis functions, does very well. Function does very well, very similar to our method, whereas the other methods tend to do poorly. What's the time scale of that data example? Is it monthly data or? This is daily data and we use one year from 2019. We did it for various years, one for 2019, and then we did it for 2019 because it was used in the deep green paper who did it for 2019. Creaming paper who did it for 2019, but because it's we now have more data, we also did it for recent, like 2022, and the results are similar. I went over all of this, so I wouldn't go over all of these things, but basically, if you're using machine learning, you do not need to throw away all of your statistical well-established model. You can embed machine learning within your geospatial model and do Special model and do run neural networks or random forests using this idea of generalizing square loss. We have done some first theory for neural networks for irregular spatial data. And in terms of empirical performance, it's very competitive for prediction, but it also offers estimation of the mean just as a function of covariance, which is something many of the other methods do not do. So, why do we formulate an NGLS as a GNN? GNN has been used widely for Has been used widely for geospatial data, but if you look into it, they use very simple methods like just take the average of the neighbors without considering how far they are or what's the correlation between them. So this, we show that the geospatial model becomes a GNN with weights that are determined by trigger, which is how it should be because it takes into account the relative orientation of the spatial locations where the data is collected. So I think for irregular spatial data, if you want to do GNM, you should use Kriging. Do GNN, you should use Krigging weights because it agrees with your geospatial model. It's exactly the same thing. There are some applications of spatial data like disease mapping, where there is no explicit model. It's just specified through graphs. Like in the disease mapping, there's often this conditional autoregressive model and simultaneous autoregressive model that are just specified through graphs. So the graph formulation will allow extending this approach to those settings. Those settings. And then one of the things we're also working on is doing multivariate responses where we use co-gridding rates of multivariate data. We use stationary Gaussian processes, but you can use non-stationary Gaussian processes if there are possibilities. So Andrew has this very nice work on using deep Gaussian process covariance kernel. So there's a possibility of using a non-stationary covariance kernel for the covariance. A non-stationary covariance kernel for the covariance and then a non-linear model for the mean. So, combining these two ideas, and similarly, we can also add a set of small set of spatial basis functions in the mean while still using Gaussian process for the covariance. Again, this is often done in the linear setup where we use the latitude and longitude in the mean plus use a covariance function. And we can also do that same thing here. So, this is work with my two of my PhD students and my collaborator at Cornell. These are the papers. These are the papers. This is the neural network paper, and these were the random forest papers and so forth. Thank you.