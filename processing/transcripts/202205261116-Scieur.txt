Just a quick survey, who who would have would have been present tomorrow, actually? One from three or four? Okay, yeah, that was a very good idea to put for today. Okay, so today I'm going to talk about average case analysis and optimization. So it's like quite different from the talk we've got during this week where we talk about mostly theory in a deep neural network. Here I'm mostly focusing. Network. Here I'm mostly focusing on the optimization part. So I'm going to try to talk about these three papers, all of them submitted at ICMF and mostly also written with Fabien Pegoza. Maybe the most important one is the first one where we introduce the framework about average case analysis. Then the other area. Uh then the other are like extensions to uh other settings. Okay, so I'm gonna put some uh code here, so don't be afraid. I know it is usually a very bad idea. Sorry. Looks like you're talking festival. Yeah, so I noticed it's a very bad idea to put some prone on slides, but this is what I'm gonna do. So here N will be the number of samples. Here, n will be the number of samples of some machine learning program, and will be the language. So, usually, when you try to benchmark optimization algorithms, you just pick at first random matrices. So, I generate a random matrix of features A, then I generate a random solution X star, and I just define some the B in reach regression to be A times X star. But then I define some objective function. Some objective function here to be simply the L2 L2 norm and its region H transpose times the residual. And then I'm solving this problem using region descent. So it is a random problem and I just would like to have an idea on how fast vagin descent will be. So I'm gonna run this program many, many, many, many times. And this is what you're gonna obtain here. So all these gray lines Grey lines is one instance of a region designed over time, and I've just run my program several times. And in orange here, this is the average of the curves over time. So this would be the average case complexity of Bradjan descent, and I'm comparing it with the worst case of Bradjan descent. So the advantage of the So the advantage of the worst case complexity, so when you try to do some kind of vertical round about it, is that it will be valid for every input. And as you can see here, the blue curve is indeed above all the grey curves here. But unfortunately, this is quite conservative. So there is quite a big gap between your curve here, the the worst case curve, and any of the practical instance of projects. So unfortunately this is not really So unfortunately this is not really informative about the typical runtime of your algorithm. Instead the average case is a little bit more representative. So you see that roughly all your curves will follow this orange one. But unfortunately to do some average case analysis over something, you need some prior distribution over the problem. And also you need a way to generate your problem randomly. So this is quite an unknown question. Quite an unknown question, and we try to answer to this question in the paper. How can we perform average case analysis on optimization? So we took the first step of analyzing very simple functions. So instead of analyzing very complex uh deep neural networks, we just focus on simple quadratic functions here and see what we can say about that. So See what we can say about that. So I just wrote the function here just for notation, so that you can remember notation throughout the presentation. But here I'm just looking at this simple quadratic function, and I'm assuming that the solution x star and the matrix H are generated at random. So for instance, the random V square program I presented you with the lines of code. This is one instance of a random quadratic program. But now the question is, But now the question is what is the distribution over H and what should be the distribution over X star what you need to take into consideration? And actually the only thing that matters is the distribution of the eigenvalue over H, and that's all. So here I'm displaying two curves. So the first curve would be what I call the empirical spectral density of H. So this means we have one instance of your H and H. means we have one instance of your H and H and you just plot the eigenvalues here. So this is the empirical spectrum, so this is a discrete function, but it's also a random function because h is generated at random. So if I'm taking the expectation of this function, I obtain here the blue density function here, what I call the expected spectral density. So this is my function cas. Instead of the worst case where we consider every metric stage such that such that H is bounded, the eigenvalue of H are bounded between a positive constant and another positive constant, so the strong velocity and the smoothness constant. I'm assuming instead that the empirical spectral density of H is sampled at random using this density. Using this nasty function. So this is a very important concept here. So if we have any questions, it would be a very good time to ask questions about it if it isn't clear. So so this is your model, correct? Yeah, so this is my generative model. So I generate random matrix H such that in expectation the spectrum is The spectrum is equal to the expected spectrum of C. The the size is uh square, darl square, H is Del Sh, so yes it's always square. Why not just generate a covariance the covariance of a matrix? You can generate H as a covariance between two matrices. And to make the point actually, so remember the piece of code I showed you to you at the beginning of the presentation. I generate my matrix H as H1. H as H transpose A, where A is a random matrix. In that case, it's the same thing as generating at random matrix H, where the spectral density of H is a matchable password distribution, because it's the product of two random matrices. And product of two random matrices, if you look at the spectrum, it tends to look something like this, a matchable pastoral distribution. So in the limit, in the limit case, it's exactly that. In expectation, it's exactly that also. Okay. Yeah, so in limit case or expectation, basically this is. In limit case or expectation, basically this will be the same. That in the worst case you could have this uh extreme data out. You could have something like this in extreme case. So we have to think that T is the large. Yes. Yeah. Okay. So once we've done this generative model over H, we can compute the X. H, we can compute the expected error for first-order method on quadratic function. And actually, the formula is quite simple. So we have like a division between three terms: one which is linked to the initialization, another to the efficiency of the algorithm, and the third one to the complexity of the problem. So R and mu are fixed. So we consider this the problem we'd like to solve. So the only way we can optimize this expectation. The only way we can optimize this expected error would be to play on the algorithm we would like to use, and it is actually a polynomial. So I'm going to explain this very quickly. So the only degree of freedom we have is choosing the right polynomial to minimize this expected error. So why I'm talking about polynomials, this is because there is a very strong link between first-order methods on quantity and polynomials. So of course, here the proposition is as follows. And the proposition is as follows. If you're using a first-order method, so basically something that uses gradients, on a quadratic function, the error is written as this. So the error after t-iteration of some algorithm can be written as a polynomial in the matrix stage. So I'm going to give the quick example of region descent so that you're all convinced that this is the case. Okay, so that's minus my region with the state as gamma and this is the iterative of region descent. I'm gonna just remove x star on both sides. And so here I have a very nice recursion. And you see here the polynomial that appears magically. So this is P of X, which is defined as 1 minus gamma X or T. So when I'm talking about Rajal descent, this is the same as talking about this algorithm. And vice versa. If I'm talking about algorithms such that you have this condition, this is the same as talking about one specific algorithm. So, as I just showed you, Glenn is an 56 size, we can associate an equivalent to that. Okay, so our goal here would be to find the optimal first-order method to minimize this error out. So, I'm going to minimize over that polynomial such that the polynomial is equal to 1. And I can do that because minimizing over the algorithm is the same as minimizing over polynomials. And the solution is actually quite simple. So remember that's very nice expected petroleum that we have here. We can use it to design the optimal polynomial. And the optimal one would be a polynomial of degree t. And the sequence of those polynomials should be orthogonal to lambda times the spectral loss distribution. And there is a very huge theory about orthogonal polynomials. Orthogonal polynomials. So you just basically go on Wikipedia, see what the polynomial you are interested in, and then you derive the sequence of polynomials. And then you can find back the optimal first order like a also one very nice thing about orthogonal polynomials is that the recurrence is very simple. So you can build a sequence of orthogonal polynomials using only a three-term recurrence. And if you cast back this polynomial into an algorithm, And if you cast back this polynomial into an algorithm, this is how it looks like. It's basically a Gradient descent with momentum. So you see here the momentum term and right here the Gradient step size. And maybe that explains why using Graduate Disandrum works so well on uh deep neural networks. It's uh probably because while we have seen that uh neural networks sometimes behave like quadratics. Behave like roughly like quadratics, and using something that looks like red and momentum is optimal, actually, for optimizing such functions. And one very big advantage of this approach is that you don't need the store connected parameter to actually having an accelerator to rate of convergence. So, in the case of the magical passive distribution, so if you're assuming that your model is generated at random, so the expected. Data at random, so the expected spectral density is a match equal by distribution. We only need the mean and the variance of the distribution. And this is very easy to compute using the trace of the Hessian rather than computing the minimum and the maximum eigenvalue compared to the worst case method. So that means that you're able to have accelerated rates of convergence without the knowledge of the strong complexity. So here are some practical examples. Practical examples, and I'm comparing many different algorithms. So, in blue, this is the instance of my problem, well the spectrum of my problem, and in orange, this is my prior over the distribution of ligand values of my edge. So, that means that in orange, this is my expected spectral density distribution. And then I'm comparing this algorithm. So, matching capacity simplifies. So machine copy simplifies just as the asymptotic version of the machine copy optimal algorithm, and we have something that converges quite quickly to the optimal, but that's not the best algorithm. Compared to the one which use exactly the matching capacity distribution, so where we estimate the mean and the variance of the MPL gas spectral distribution, then you obtain that orange curve that converges quite quickly. Orange curve that converges quite quickly. And we can compare it to. Yes? What you say is that with only knowing the mean and the variant of the matching capacitor distribution mu, you can derive AP and BT, the time-dependent parameters of your orthogonal polynomial. Exactly, yes. And you can compute it using the trace and the trace square of the HN. And this is something you can estimate also very efficiently using random vectors. Like you multiply matrix H with some random vectors and this gives you estimates of the trace and trace square so that you don't have to store the entire matrix H. You just need the product between the matrix vector, matrix vector products. I'm also comparing here with conjugate region method. That's the most optimal method you can use. That's the most optimal method you can imagine for quadratic problem. And you see that the gap is actually super close. So that means that our analysis is quite tight. And actually, all the bonds that we have are actually equalities. So that's why we are so close to conjugate coagents in terms of complexity. And then there's a bunch of other methods, but maybe an important one would be the Nesterov method, which is optimal also for smooth and strongly convex functions. So something a bit more generic than the quadratic problem. But still, Radotti problem, but still it's not converging that fast compared to average case accelerated methods. So in terms of application, for instance here we talked about, in this week we have talked a lot about for instance the neural tangent kernel. So this one example of application, since if you initialize your If you initialize your deep neural network at random, you're most likely to have an engine that looks like a matching copy distribution, so you can use this kind of algorithm to have a faster conversion. It happens that we have also some other applications, for instance, the design of accelerated Gussie algorithm, where you have some matrix and that matrix represents a network. And the distribution of the eigenvalue in your network is important, and you can exploit this spatial structure to have accelerated. Structure to have accelerated parallel algorithms. In that case, they use the Jacobi measure, so the mu would be the Jacobi measure instead of the magical pasture distribution. We have also other applications such as the random matrix sketching for solving this square. Again, by using the random sketching, also there's some special structure inside the sketching matrices, and they can exploit this again to permash a bit faster. And also, it's possible to design basically an optimal method for any distribution which have more complex structure than just the market robust. So, as the first take-home message of this presentation, that would be that average case time is actually possible in complex optimization. Because usually we're using worst case bounds, but average case bounds are usually more representative. And I think this is a very good direction of. And I think this is a very good direction of research for context optimization. And as we've seen, there are many applications to that, not only just randomly squares. It's also possible to accelerate without knowing complex constants such as a strong convexity. And it seems to be faster than worst case beta, but it's also the expectation, of course. Yes. And for any distribution, how do you uh compute uh your uh sequence of particle and predict? So there is generic um There is generic um there is generic uh method to do that using the Graham Smith algorithm. Uh usually you just cross the finger to search that your solutions. Because I know Pan read the paper of Rafe and it was exactly this and used Jacobi polynomials because the only parameter he can estimate is uh is the the mass near zero. This is why he he says Jacobi polynomials, but instead um Jacobic groups, but instead, I mean here's a generic recursion. So I wanted to know whether you had a procedure if you don't know a quantity. So if you don't know the distribution at the world, then you cannot do anything because that's very hard to estimate the distribution of the metrics. So instead, you have either a prior over the distribution, or in that case, you just use a worst-case algorithm. Use a worst-case algorithm. But in machine learning, for instance, you usually know the distribution of your data set and you can exploit this to have a fast convergence of your algorithm. So that's one thing, but also you will see that actually it's not that important to know the distribution. So I'm going to present it a bit later. Knowing the exact distributions is not super important. So what is really important actually, so in well I'm spoiling a Well, I'm spoiling a little bit, but in a strongly convex program, actually, the only thing which is important is just giving a good idea of the actually strong convexity constant and smoothness. You have a spectral gap in this case? You have a spectral gap so we can measure. Yeah, so either you have a spectral gap, and in that case, it's not very important to know distribution because you will see that things simplify in asymptotic regime. And if you don't have spectral gap, the only thing which is important is the constant. The only thing which is important is the concentration that one cell. Yeah, so that was the first take-home message. And now I'm going back to my list of different applications. And as you said, they've noticed very interesting things, especially when the number of different goes to infinity. And it happened that when you look at this. That, when you look at this recurrence, okay, so you're time varying momentum and time varying step size here, and you look at t goes to infinity, there is one thing very interesting that happened. The first thing is that the momentum converts to a fixed parameter, which is the same momentum as in the Polyak-Heavyball method. So, Polyak-Heavyball method is the worst-case optimal algorithm on quadratic functions with a constant. function with constant parameters. The one thing which is interesting is that the step size also converges to a fixed parameter, which not that surprisingly finally, is the same as the step size of the polyak people method as well. And it happens for all densities u. Under some mild assumption, we need to assume that the spectral density function is strictly positive on the interval Be positive on the pental small n and big L, where small n and big L would be the bound on eigenvalue of the matrix H. So if it satisfies this, then these both parameters converge to the same one, to the one of Podemic amount of terms. So you have to be sure when you say average optimal is for each T, or T is the expectation. The expectation of the P T is the smallest for this method. Expectation of the objective is the smallest of expected, yes. And similarly, well, this would be like a corollary of the previous result. If you're looking at the average rate of convergence, then it's also the same rate of convergence of productible methods. So that's a relative rate of convergence, so that gives you just an idea on how much will decrease at each iteration as an exponential decay. This expectation of the probability. This expectation of the program instances, and that's the rate of polycommand method. So we basically converge to the worst-case convergence bound of the optimization program. So I'm going to just write it over here. So in average case, we also have a rate which is similar to the worst case one particular change. And that's a great. And that's the problem, yes? In this case, the sequence of orthogonal polynomial is a known family. Now, it's any distribution, so it's any sequence of orthogonal polynomial which is orthogonal to this density. But the density can be any density as long as this would be positive in the interval and the n. I don't know if I answered your question actually. You don't need to know the performance. You don't need to know the density. So, well, it's not that I don't need to know the density, but let's assume that we have an optimal method with respect to some density, and you look at t goes to infinity. It always converges to the same method which is polyacomato for all densities. And that's surprising. So that means that in the end, it's not that useful to have the density, to have average case acceleration. No, that's not the thing matters at us. So maybe I can go back with my here. So you can see here that that's basically the asymptotic version of Machacobastor, and this is the optimal Macho Cobasture, and this is where you're actually gaining something. This is the non asymptotic region. Something, this is the non-asymptotic routine. But asymptotically, the methods are basically that polycommon is actually universally optimal. So if you're losing polygomentum, you're gonna have the same rate of convergence asymptotically. You're just gonna lose a constant at the at the beginning. At the beginning. So I know that basically I just shoot myself in the foot because, well, I'm just saying first, oh, that's awesome. I worked case analysis is better than worse case analysis because it's more representative, but in the end, we see that polymorphism is always good, which is actually a worse case method. So it's not in the world of smooth and strong context function that we can actually have again. So to do some Some better analysis and more insightful analysis, we need to move to the world of context function. So I'm doing the right world. So here I'm assuming I was assuming that my expected spectral non-st belongs to where it's definite. In some interval which is away from zero. But now the question is, what happens if I'm looking to a spectral density which is defined from zero to some smoothness on some L? So this is a smooth convex function. And here what happens if I'm looking at only a convex function with no upper bound on the largest second. Upward bound on the larger second value. So, is it possible to guide something in that case? So, I recall here the worst case conversions rate. In the case of the worst case for smooth complex function, it's 1 over t square, and for non-smooth function, it's 1 over square root. So for For complex functions, I'm gonna look at a very specific distribution, which are actually Jacobi polynomials, as you just mentioned. I'm looking at this specific distribution, so mu of lambda would be lambda power psi times l minus lambda over two. So that's that will be my expected spectral density. Be my expected spectral density. So I'm gonna just give some examples here. So in fact, the two is not that important, so the two will play something in the constant, but it won't play something in the rate of conversion. So what is really important is the psi here. So if psi, for instance, is less than zero, you're gonna have a lot of concentration. Around the level. And so here I'm putting basically the density, the expected vector density. And lambda is bigger than zero, then here we have almost no zero convection. So it's like the strictly convex region. So we can have something that looks like this. And if psi is equal to zero, it would be something that looks like the uniform super. Be something that looks like the uniform solution. And so it happened that if you're computing the optimal algorithms with respect to this expected spectral density, then we have a very nice rate of conversion that I'm going to write it right here, just to remember. So yes, it's y divided by t or times psi is plus two. And psi has to be, I forgot to mention that psi has to be larger than minus one. So here we see that we have a huge gain in terms of rate of convergence. So once you're looking to a convex function and you try to design an average case of Timan algorithm, there is a very big gap. There is a very big gap between what we have in the worst case and what we have in the average case. And again, the algorithm looks like the region descent if one of those. Okay, so that was for the convex and smooth functions. The performance of the worst case method is it actually 1 over t squared actually. Is it uh actually one of those query just yes. Uh I mean the average performance of the worst case instance. I'm gonna data. So that was for the yeah, that was for the worst case method on uh the worst case instance and this is average case optimal on average case. So that's not exactly symbols but we will see later then actually the performance of worst case metadata quite fine also, but still some optimal. So now So now I'm going to look at this case where we have just information that you have a complex problem, but you don't even know what is the largest n value. That seems quite complicated because usually you need to know that a largest n value to just one gradient descent for instance. So in that case, the expected exponentium I'm going to consider here will be lambda power alpha times exponential of minus. Exponential of minus lambda. So this is scored as the Laguerre distribution, I believe, if I'm not mistaken. And again, alpha has to be bigger than minus 1. So here I have no idea what would be the largest n value, but still we have an exponential decay of the magnitude of the value. And what happened in that case? Again, if you perform the average case analysis, If you perform the average case analysis, we have a huge difference between what's the worst case and the average case. So, in average case, you're going to obtain something which is 1 divided by alpha plus 2. So, even in the worst case of the average case, so in the worst case of our alpha, we still have a 1 over t convergence rate compared to the 1 over square root of t for a non-smooth problem. And this is surprising because it looks like Disappointing because it looks like this is an accelerator method, and still we have no information at all about the minimum and maximum eigenvalue of the machine. So it is a huge improvement compared to the worst case. So that would be basically the third take-home message here, is that if you're moving to the average case analysis rather than the worst case analysis, The worst case analysis. For context function, you have those improvements which are quite consequent if you just look at the bounds. Yes. So they are universal. So they are universal. So actually what's the only important thing here? So why we anticipate distribution particularly is because what is important? It's because what is important is just the concentration of the eigenvalue around the bound of the spectrum. And we can show that whatever happens in between, as long as asymptotically it behaves like a polynomial at the edge of the spectrum, it's fine. And many distributions satisfy this assumption. So it's basically asking that your expected spectral density is analytic R10 and R110. Completely fine assumption for any distribution. For any distribution μ, you take the Jacobin polynomial coming from this specific distribution. Sorry, can you repeat? For any distribution mu that has a psi mass around zero, you take the Jacob realm. Yeah, that's exact. That's correct. And we don't even really need to know what happened here. So again, this will play the constant. The only thing that plays in the rate is this term. So that's the concentration. Term. So that's the concentration of around zero eigenvalue of your distribution. For this, it's always better to know L, to have worse case guarantee as well. In practice, if you're really interested about the expected rate of conversions, you don't need to know because the only thing you need to know in that case, I think, is like the again the variance and the expectation of the but knowing L is not a big deal. But knowing L is not a big deal. Most uh most cases. It's uh mostly the the the mu that we don't. Yes. Oh. Does it matter if the density has disjoint support? Sorry? Does it matter that the density view has disjoint support, for example? Disgion. Very interesting question. It matters. And this paper is submitted in 2021 about what happens if you have digital support. In that case, in summary, what we showed is Well, in summary, what we showed is that you need to have a cycle in your step sizes. So instead of converging with, if you look at the average case optimal method in that case, the step size is set to converging to one specific value, which will converge to a cycle of value. And it justifies somehow why people are using alternating step sizes for minimizing deep neural networks. It's because if you look at the option of deep neural networks, you have a huge concentration of eigenvalues of zero, and then a few. Values of zero, and then a few outliers. And so you have a huge gap between the two parts of the spectrum. And therefore, alternating between small and large step sizes enables also an acceptable loot of convergence. So this is a paper that was submitted with Batris Buru, Felipe Pedregoza, I think Corinne, no, Adrian Tevor, and Enrique. Yeah, so it's about uh cycling successes. Other questions? Do you have to be analytic in the support? Doesn't matter. What really matters is what happened on the screen. Okay, so last question. So I'm an answer to your question about what happened for the average case analysis for worst case methods. And in the paper, we've analyzed what happened with the nest error file. Analyze what happened with the Nestoroff algorithm. So I recall quickly Nestoroff algorithm. So this is xt plus 1 is equal to y t minus 1 divided by f region of f of y t. So this is a version of the sense step. And then we have an extrapolation step. Or two this one times So we've looked at this algorithm and the average case analysis of this algorithm on that specific problem. Okay, because this is the optimal algorithm for a smooth and complex problem. And we have like three different cases. The first case The first case is what we call the easy case, so when psi is larger than a minus one half. In that case, the rate of convergence is one divided by t power psi plus seven divided by two. So this is actually quite, not quite the Actually, quite not quite the optimal rate of harm margins for the average case analysis, but it's still something which beats the 1 over t square and still very fast. And since psi has to be larger than 1 half, it's like 1 over t power 3, so it's still quite a fast fleet of convergence. Then we have the special case where xi is equal to minus 1. Psi is equal to minus one half. That gives it's t one over t minus three times the regime of t so the psi equals to minus one half is basically the worst distribution you can imagine for over the eigenvalue. Over the eigenvalue, so that you basically attain the worst case convergence. And then we have the heart regime where xi is less than one half, minus one half, and in that case surprisingly, we have exactly the Average case, the average case rate of convergence. So that means that for art instances, if you're using Nesterof algorithm, you're actually going to converge at an optimal rate of convergence in average case. So that would be, I think, the last take-home message of my presentation. Using Nestorof algorithm is again universally optimal, but only for our But only for hard instances of quadratic functions. And if you're using it at easy instances, it's not going to be optimal, but the speed is quite fast or movie, so it's not available. Yeah? Thank you. I have a kind of related question, because I was trying to think about what happens in non-quadratic case. And one thing I was thinking about, are you familiar with this average? Yes, I've managed a bit, but I was just wondering if there's a connection between your studying and let's say you were studying some non-quadratic objectives. This so I felt like if the hash, if you're a non-quadrotic function, then the hash it has some properties. And it seems like you can reverse engineer a way to represent your objective function. Oh, okay. That kind of also seems simple. Not like the angle convolutions. Not sure, actually. I'll just write it on the board. One of the ways to get a smooth On the board. One of the ways to get a smooth function from a non-smooth function, you just pick some usually some metric distribution, and instead of optimizing f, you optimize this thing. This is how all zeroth order optimization works. Okay, so I was wondering if for certain functions f that are not quadratics, an assumption on the Hessian having some distribution is equivalent to Is equivalent to you effectively optimizing something like this, where you can reverse engineer a distribution on PC based on the assumptions on age. Okay, that's a good question. I'm not default to any answer about that because I'm not that familiar with these things. This was a year ago I came up with during your talk where I felt that's why I wasn't so surprised that you were getting all these races. I felt like it was inducing all sorts of accuracy because I felt like he was doing this. I felt like it was doing this. Okay, I'm not trying to. Okay, okay. Why would that put me something? I know the other one which does the proximal method, which will do a convolution. Oh, look, this is like from Egypt. You know, this has been rediscovered like a thousand times, but this is the main way of doing zero-to-order optimization. So let me see. So one way to do zero-the-order optimization is to pick a random direction and do a Direction and do a difference equation in that, and that's an estimate of the gradient. Unbiased estimate of the gradient, it's also the exact gradient obtained by taking the gradient of this and using a sample. From those zero-order methods, you're actually effectively doing well. That's been rediscovered like a thousand times. Has many names because it's been rediscovered. I can send you a lot of papers. Like, um, the nicest paper in my case for this is a paper from like uh Joe, but From like uh I have a question. Um maybe you mentioned that I miss it, but uh if we do the expected worst-case performance, how does it compare to the uh performance? Um must improve a little bit, right? Yeah, yeah, so it does improve it a little bit. So when you say expected worst case, you mean you pick the worst case algorithm and then You pick the worst case algorithm and then you apply it to some instant of problem with some expected spectral density, and you estimate the rate of convergence of that pattern? It's like you have an algorithm, the standard analysis. We know it's largest. Okay, so let's say we have quasi-descent, for instance, and we know the largest and smallest eigenvalue. Okay. Uh these are random objects, so the performance guarantee we get are random and it will be worse than yours because you have It would be worse than yours, it would be better. Maybe, I don't know, I was wondering in the first case. Um, so there is analysis. So, in in the paper about the context things, so we analyze a mess of migration descent. I know there's also a paper from Perthegoza that also analyze the parting time of optimization algorithm again in some very similar setting. So, that also will give you average case performance of a worse. Case performance over worst case algorithm, but in terms of averaging the worst case performance, that's a very big question. To me, that would be the naive things to compare to. But I don't think it will improve that much again because in the asymptotic regime, then the concentration around for difficult problems, the concentration around the extreme eigenvalue is. Extreme eigenvalue is usually quite high. So I don't think analyzing the expected worst case would give you a very big improvement compared to the actual worst case. But maybe it's useful. But like the thing I was thinking about was the generic uh smooth optimization guarantee of if the beta actually lies in zero to eighty. So, if beta actually lies in 0 to 180, then being able to plug in the expected smoothness would be an object. The infinity down to beta isn't as good of a rate as yours, but it would still be something non-trivial or non-quadratic. Yeah, sure. But I have no idea how to extend it to non-quadratics, by the way. So, phenomenon phenomenon is like outside that. I'm currently trying to send it to one quality, but it's quite complicated. So I assume that the Linux comment would be the mic. I would do this in the end. So that is assuming, okay, if you want error random objects, then what would be the regular smooth function you use the step size beta? But now beta is infinity. So what step size do you take? You can't just like sample from this distribution I say step size. So Right, we might have more questions. So, before thinking them again, I would like to thank all of you for being here, and especially those who are here on TBN. Several people in the way. I'd like also to thank the Girl Center, and it was an amazing opportunity to be here. Very happy to be with all of you, and we can thank again the you and we can take again Daniel for this.