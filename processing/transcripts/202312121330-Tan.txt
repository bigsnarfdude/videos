So, today I want to present on some work that is in preparation. And this is joint work with my wonderful collaborators, Omar, Dio, James, and my post-supervisor, Eny. So, all of them are currently at UC Berkeley. All right, so first. As we all know, deep learning has all the hype at the moment. But last year in Europe, there was this paper that came out that generated a lot of attention. And so the title of the paper gives away the message that it wants to say. And so they compared story-based methods against different deep learning architectures, and they showed that for That for these sorts of data sets, the tablet data sets that are moderately sized, free-based methods perform better than different types of deep learning architecture. Okay, so this paper generated a lot of tension, but this is actually not a new observation. So many times people have observed that comparing three-base methods against other machine learning methods, they usually perform. They usually perform better. So, not just against deep learning, but also against more classical methods like SVM, et cetera. So, here's one quote from a paper from 2008. Here's another survey paper from 2014, and yet another paper, 2018. Okay, so again and again, people observed that trees and tree ensembles perform really well on moderated sized tablet data sets. And so, these observations have led me. Observations have led me to spend the last two years or so doing research on trees and tree ensembles. And I believe more people should study this topic because it's very interesting and important. All right, so let's start with the basics. What is a decision tree? So it's very simple. It's a piecewise constant model that you obtain by performing recursive partitioning of a covariate space. So what do I mean by this? So consider this two-dimensional space. We have Dimensional space. We have two classes or two labels, white and blue. And we start with the model that is constant. That's a constant model over the whole space. And then we can update this model by splitting the space into two parts. We have the bottom part and the top part. And we can represent this using a decision stump. Over each of the two parts of the coverage space, you again predict the mean response over this. mean response over that over the labels in that portion okay so we can keep going we can split further split portion at the bottom and again predict the mean response over each of the two resulting leaves and again we can represent this by further drawing this tree structure okay so a decision tree is just the piece which forms the model all right so now when we consider random forest When we consider random forests, the grandports are trees. So these are ensembles of trees. What this means is that we just add up a whole bunch of individual decision tree models. Okay, so these are very popular, very powerful, but there are some drawbacks. So what are these drawbacks? First, if you want to do statistics, we often are interested in quantifying the uncertainty of your predictions. And for these methods, because they are algorithms. methods because they are algorithmic it's sort of clear unclear and difficult to perform any ancillary quantification for your predictions secondly these three and some of methods usually make use of some really splitting principle to make it computationally efficient or tractable and usually this works fairly well but it may also lead your algorithm to be stuck in some local optimum Local optimum. And then, thirdly, there is also some inefficiency when you fit trees, so tree ensembles, to data that has some additive structure. Okay, so this was found in one of my previous works from two years back. Okay, so how do we overcome these drawbacks or how can we try to overcome them? One way is to change from a frequency approach to a Bayesian approach and use a Bayesian formulation. And use a Bayesian formulation of three ensembles. So, this direction was first proposed in this paper in 2010. Okay, so these authors introduced a new algorithm called Bayesian Additive Regression Trees, or BUT for short. Okay, so how does this work? So, I'm going to spend a few more slides later on on this talk explaining this more. But for now, just one slide, at a very high level, what buttons. At a very high level, what Bubb is. So, one interpretation of this is that it's simply a Bayesian non-parametric regression model. Okay, so it's a bit like a Gaussian process model. And here, we have the usual three steps of Bayesian inference. In the first step, we define the prior, but your prior is going to be on a non-parametric space. This is going to be in the space of regression functions. Secondly, we combine the prior and the data likelihoods. The prior and the data likelihood to get the posterior. And then, thirdly, we need to be able to sample it for the posterior. As usual, we don't have access to the closed-formed formula for the posterior, so we need to use MCMC to do approximate sampling. All right, so this is one view of what BUT does. There's another view, which is to look at this at a purely algorithmic point of view and treat bud as a randomized tree on some. But as a randomized tree ensemble learning method. Okay, so again, you have this tree ensemble, and as you learn the trees, you are sampling from the posterior. So these are equivalent. And so you can think of this sampling as a way of trying to find the tree structures in a probabilistic way. Okay, so since that 2010 paper, BERT has become increasingly popular. All right, so he has been one. All right, so he has been widely used in the applied statistics community and has been applied in many settings, ranging from the social sciences to biostatistics, and essentially in any subfield that makes use of causal inference. So for some reason, people in causal inference love to use BET. And every year, I think they have this competition where they compare different methods on some simulated data sets. And usually, BERT or some other method is derived. Or some other method that's derived from it has the best performance. Okay, and naturally, there are quite a few different packages that implement this algorithm with many, many users. And here, if you look at the Google Scroller page for this paper, it now has more than 1,700 citations. So I took this screenshot last year, so I believe there should be more by now. Now. Okay, so naturally, people have tried to study, but, and here I'm going to give you one-slight overview of what has been done. The central message from all of this work has been the following. So the posterior that you get from this model actually has very good prediction and inference properties. Okay, so what do I mean by this? So first, we have posterior concentration. Posterior concentration. So, this means that if you have some generative model, okay, a frequentest generative model with a true regression function, then if you run but, the posterior of but is going to concentrate around the true regression function. And the rate at which this happens is the minimax rate for the problem. Okay, so we have the usual setting of Serbolov or other smoothness conditions, and then on top of that, we have On top of that, we have some more fancy conditions like anisotropic heterogeneous smoothness, but also achieves the minimax rate. Okay, so this was a problem that was studied by Donahoe for wavelets. So wavelets are well known to be spatially adaptive. So it turns out that the butt posterior is also spatially adaptive. So that's a very nice property. Okay, on top of that, but has also been shown to have variable selection consistency. To have variable selection consistency. So, if you have some true signal features and a lot of noisy features, but posterior would select true features. Okay, so here I want to note that a lot of work in this direction has been driven by Veronica Rokova. So she's great. She has a long line of work on this topic. Okay, so there's a problem though. So there's a hole in the literature. And this is that all of this work, all the work listed here, is only a Work listed here is only about the posterior. But we don't actually have access to the posterior. So, as I said, we can only sample approximately from the posterior using MCMC. Okay, so naturally, we need to ask some further questions. We want to know how close is this approximate posterior to the true posterior for bot. And then, secondly, as we run the MCMC, we want it to converge quickly enough. So, how long must we run? Converge quickly enough. So, how long must we run the chain in order to guarantee that we actually have achieved good enough conversions? Okay, so for those in the field, the technical question is, what is the mixing time for the bud simpler? All right, and so at this point, I want to give you some bad news. So, there's a problem. So, actually, mixing does not actually happen very well. Okay, it doesn't happen fast. It doesn't happen fast. In fact, it may not even happen at all on a reasonable time scale. So here's a survey paper about that from 2020. And here's this code over here. So it says that, okay, the algorithm is good, but it doesn't always mix well. And so here it says you should run many chains and average the results over the chains. And then another paper says that here you should use some warm starts. Okay, and in both these cases, we need to do this additional bells and whistles. Need to do this additional thousands and wizards because the chain does not make swallow. If it did, you wouldn't have to do these addition steps. Okay, so this belief that but sampler makes it slowly, so it's sort of well known in the community and many people allude to it, but it seems to have this nature of folklore. So nobody has actually tried to study this rigorously. And so this is the goal of our current work. Okay, so for the rest of this talk, I'm going to do the following. So I'm going to explain a bit more about how the BART algorithm works. Secondly, I'm going to talk about the framing for this problem. Okay, so I want to say that the algorithm doesn't perform so well, computationally speaking. And it turns out that matching time is not the right framing. And so we need to derive the right framing for those. And the punchline is that we should know at The punchline is that we should know at hitting times instead. And what we're going to do is to derive hitting time lower bounds for the bot algorithm. And also, from those hitting time lower bounds, there are going to be some takeaways that practitioners can use. Okay, so this is the plan for the rest of this talk. So, part one, how does part work? So, first, how to harmonize the space of regression trees. Secondly, what Regression trees, secondly, what are the priors and likelihoods? And thirdly, what is the MCMC algorithm? Okay, so how do we parametrizize progression trees? Okay, so we start with the tree structure. The tree structure comprises the topology of trees. Okay, so think of just a graph. And then it also includes the splitting rooms for each of the internal nodes of this rooted binary tree. Binary tree. Okay, the split splitting rules are these labels of the nodes. Okay, so it tells you what feature to split on and what threshold to split on. Okay, so together, these comprise the free structure, T. Okay, in addition to this, I need another parameter, data. And data is going to comprise a vector of values, one for each of the leaf nodes. And these are going to be the prediction values at each of the leaf nodes in this tree. Leaf nodes in this tree. Okay, and this tuple key, data, is going to parameterize your entire regression tree. Okay, and for subtractability, so I want this parameter space to be nice. So I want it to be a finite space. And one way to do it is to assume that your covariate space. So covariate space for the regression problem is discrete. And we're just going to assume that it's a product space. Space. Okay, so in practice, when you run birds, it automatically does some sort of disregardization. So this is not too far away from how it works in practice. Okay, so now let's go on to priors. So what's the prior for the tree structure? Okay, so for our theory, this is actually not super important, but let me just tell you what it is. So it's defined in terms of a stochastic process. So we start with the empty tree, a tribute tree, which is one node. Node and then with some probability, we're going to split the root nodes. And if we flip a coin, so if it's heads, we split the root node, if it's tails, we don't split, and then we just stop. And then if we split the nodes, we're going to construct a random split label. Okay, so we're going to draw the feature and the threshold uniformly at random. Okay, so we pick the feature i here and then the threshold a. Okay, and then we can keep going. So, for every new node that you construct in this tree, you again flip a coin with a different probability, and then if it's hit, we split, if it's tails, we stop. Okay, and so this is going to give you a regularization prior on the space of trees. All right, so now for the lead parameter data. So, what is it? So, it's actually the simplest possible parametrization or simplest possible process. Or simplest possible prior. So it's just a regular Gaussian prior. Okay, so this is the tree structure. Okay. And then for the leaf values, we're going to draw it from a normal prior that is independent. Okay, so all the leaf values are going to be independent of each other. And now using the tree structure and the lead parameters, I get a function. Okay, so this would be the regression tree function. Okay, so this would be the regression tree function. All right, and now I have my observed data sets. Okay, I have the covariance and the responses. And now I, once you have a likelihood, it's just going to be the Gaussian likelihood. Okay, so condition on the observed covariates and the function, the responses are just going to be normally distributed according to the function value at each of the covariates plus some Gaussian noise. Gaussian noise. Okay, so very simple, very straightforward. Okay, so sigma is just a noise parameter. So we assume this to be fixed. In the actual algorithm, it's allowed to vary. Okay, so now MCMC algorithm. So this is the main character of a story. Okay, so I'm going to be a bit careful about this. So this is the About this. So, this is the posterior that we want to sample from: the joint posterior over the tree structures and the leaf parameters conditioned on the observed data. So, what we do is to decompose this, decompose this into a product comprising the conditional posterior on the leaf parameters times the marginal posterior on the space of the three structures. And the simple observation is that this conditional posterior on the leaf parameters actually is a closed-form expression, right? Closed from expression, right? And this is not surprising, okay, because of the normal likelihood and prior, right? So, this is actually just Gaussian linear Bayesian linear regression, and we understand this very well. So, all we really need to do for the MCMC is to sample from the second term. Okay, we need to sample a tree structure from the space of trees. Okay, so how do we do this? We're going to use metropolis histance. So, for metropolitan sales things, we need the proposal distribution and then the sensor. Okay, so what is the proposal? We have four possible moves that are all local in nature. So, first move is to grow a tree. So, let's say we start with this tree over here. When we grow the tree, we just take one of the nodes, one of the leaf nodes, and then split it into two further leaves. We can prune the tree. So, starting from this, we can just remove one of the This we can just remove one of the terminal splits. We can change one of the split labels. Okay, so the tree structure, the topology remains the same, but the labels, one of the internal nodes, gets changed to something else. And lastly, we can do a swap. So swap means that we take a pair of parents and daughter nodes, and we switch the labels of these two nodes. Okay, so four different local moves. Different local moves. And after, so these are the four possibilities. And what we do is to pick one of these moves at random and we apply the usual metropolitan acceptance reject building. Okay, so that was but for one tree and now with multiple trees. Okay, so actually everything was the same, so same parametrization. And then regression function is just defined as the sum of the different functions. As the sum of the different functions for each tree, and for MCMC, for each tree, it's the same algorithm, but we embed this into a GIPS sampler. Okay, so what this means is that we have 200 trees, for instance, and every iteration recycle through all the trees and we update them one after the other. Okay, so for instance, iteration one, we could split the first tree, do nothing to the second one, split the last tree. And then the second iteration, we prune the first tree, do nothing to the Do nothing to this, grow this one, and then grow the last one as well, and then we keep going. Okay, by convention for the MC sampler, we have some don't samples, so we run it a few times without doing anything, and then we take the last bit of the sample trajectory as our approximate posterior. Okay, now part two: how do we frame combination lower bounds for the BAT algorithm? Okay, so I'm going to start by talking about. Okay, so I'm going to start by talking about some prior work that we did last year, and this ran into some problems. So, what are those problems and how should we fix it? Okay, so we're going to analyze combination load bonds from a frequency perspective, which I understand is a little bit strange because this is a Bayesian algorithm. Okay, but this is just necessary for the analysis. So, we're going to assume that we observe a data set, and the data set is to run IID from some addition. set is drawn RID from some distribution. Okay, so the covariates xi are going to be drawn for some distribution on the covariate space. And then yi is going to be given by a true regression function f star applied to the covariates plus some additive noise. And then we just need to assume that this noise is sub-kelogy and independent noise. Okay, so the warning here is that this frequency generated distribution need not be the same. Need not be the same as the Bayesian formulation, okay, and often will be different from the Bayesian formulation. All right, so mixing time. So, this is what most of you think about when you study computational properties of MCMC. And so, what's the mix in time? Okay, so I'm going to define it for you. So, we start with this value here. So, we look at the teeth time step of the Markov chain, and then we look at the transition. Chain and then we look at the transition control which we label q and we initialize this at some state okay the mt3 at um yeah so at t which would be the mt3 and then we subtract from this the stationary distribution we take the total deviation distance and now we consider the maximum over all initializations and the state space and then we consider all times for which this maximum value is less than one quarter Is less than one quarter. Okay, so why one quarter is just a convention, not important. And then the mixing time is the minimum over all of these values, all these time points for which this maximum value is less than one quarter. Okay, so this is the first time in which those two distributions are close enough and total variation distance. All right, so here's our result from last year. Okay, so we prove that major time for about one. Or one tree grows exponentially in the sample size. Okay, so here's where the curse of big data first arises. If you have more data, you think that your performance gets better, but actually in this case, something goes a bit wrong and the mix of time grows exponentially. Okay, so we thought that this was a nice result. We submitted this to AI stats. You got a rejection. Okay, and here I reproduce the quote from the reviewer. This is reviewer tree. This is reviewer 3. So he said that he or she said this paper has potentially to be significant. However, it's missing something. So it's missing this discussion about whether this problem is even relevant in the first place. So whether basing time in treating space is practically relevant or even necessary. So we're saying that we just studied the wrong problem. Okay, so we're not very happy with this review, but then after thinking for some time, actually, the review has a point. So there is some problem. So there are some problems with this formulation. Okay, so what are these problems? So, first, the Mexican time is defined as the worst case of all possible initializations. But when we run but, we don't actually take a worst case initialization. We always use the same one, which is that we start from the empty tree ensemble. Okay, so we use the worst case balance. This is too pessimistic. Secondly, over here, Over here, okay, so the maximum is taken. So, this MCMC, right, this sampling is taking place over the space of three parameters. Okay, so the tree structure and also the leaf parameters. But in practice, what you really care about is the regression function and not the tree structure. All right. And for this, the butt-based model, the tree structure is actually non-identifiable from the model. Fireball from the model. Okay, so here's an example. So consider these two trees. So for the first tree, you split an x1 and then an x2. And then for the second tree, you split an x2 and then an x1. So these are different trees, but they give the same function. Okay, so these two trees cannot be distinguished from any observed data set. And so if you're not able to mix between these two states, it actually doesn't really matter for practice. It actually doesn't really matter for practice. Okay, so this is indeed a problem. So, how do we overcome this? So, instead of using mixing times, we're going to use hitting times, and we're going to study hitting times for a specific region in the space of tree structures. Okay, we're going to do this for the highest posterior density region, or the HPDR. Okay, so let's consider this set, which I call opt. Which are called opt. So, this will comprise all the three structures that have no bias and also the minimum degrees of freedom or the smallest complexity. So, what we are able to show in this paper is that this subset is an HPDR, and the bud posterior is going to concentrate on this nice set as the number of training samples goes to infinity. And so, then we just want to study the hitting time for the set. We just want to study the hidden time for the set. Okay, so we first look at our times for which a Markov chain lies inside the set. We take the minimum of these times, and then this is defined to be the hidden time for the set. Okay, so if we can bound the hidden time from below, this means that it takes a long time for the Markov chain to reach this nice set, which means that the bot algorithm does not converge to a nice solution within that amount of time. Okay, so this is a meaningful way to quantify. Meaningful way to quantify how good this algorithm is from a computational perspective. All right. Okay, the last part of this talk. What are the lower bounds we have and how to take away from these lower bounds? Okay, so we have three different lower bounds, and then I'm going to show you the general proof recipe and then some takeaways. Okay, so three bounds. This is the first one. So it's going to apply to the setting where our data is generated from an added. Where our data is generated from an additive model. Once an additive model, so this is where the true model can be written as a sum of univariate component functions, one for m prime difference features. Okay, so let's do some recall. The bot model is also as follows. It's also additive, but each of these is not a univariate component. So each of these is just a tree that can be multivariate. Okay, and m over here is different from this m prime. So, what's the theorem? Okay, so stated informally is as follows. So, given this additive model, and suppose the number of trees is less than or equal to the number of additive components, then the expected hitting time for the HP DR set is bounded below by n to the one to the one-half. Okay, so in this case, it's not exponential anymore, but it's still growing in the trading sample size. Okay, so the trading sample size becomes larger. So, the training summer size becomes larger, this hidden time becomes worse and worse. And that's the second part to this theorem. Okay, so we've assumed even stronger assumptions. Okay, if the number of trees is strictly less than the number of components, and we restrict the moves to only grow and prune, then we actually have a bound that is bigger than the square root. So it's actually n to the a over 2, where a is this quantity here, which depends on the complexity of each of the Depends on the complexity of each of the component functions. So, degree over here just means number of piecewise constant pieces. Okay, so if we have many piecewise constant pieces, this is going to be a large degree polynomial. Okay, so this is the first instance of this competition because of dimensionality of big data for this algorithm. Okay, so now let's look at a simulation to understand this bound a bit better. So, I'm going to simulate some F star. Some f star that is linear with five different component functions. What I'm going to do is to fit but to this data from this model and then plot the RMSE and how it depends on the number of trees and the butt model. Okay, we're going to vary the number of trees. And I'm going to do this with four different sizes of the training data sets. Okay, so these are the results. So what you can see is that as you increase the number of trees, the RMSC goes down and it keeps going down. And I see it goes down and it keeps going down, and it should probably go down a bit more as you go beyond this plot. Okay, so let's try to interpret this a bit more. So, why is the performance suffering when you have too few trees? So, there are a few different regimes. So, first, when m is strictly less than m prime, the number of trees is less than the number of additive components, then what's happening is that you actually have an inefficient representation for this data process. So, this was some. Data process. So, this was something that I showed in some previous work. And then, in addition to that, so in addition to this n efficient representation, if m is less than or equal to m prime, then our theorem shows that there's also a computational problem. So although there may be a good representation for five trees, but the sampler finds it very difficult to reach that efficient representation. And what this simulation shows is that even beyond this theoretical Even beyond this theoretical guarantee, okay, so as you go further out, there still seems to be a computation of bottom egg. Okay, so the performance still decreases after the number of trees being equal to five. Okay, so here are two other results. Okay, so just very quickly, so let's assume that you only have current prune moves. Okay, we have two for the theorems. So the first one is for the case of a pure interaction. Okay, pure interaction. Interaction, okay, pure interaction. So the exact definition is a bit complicated, but what it means is that it's essentially you have something like a ZOR function, okay, which is one one minus one minus one. Okay, so this is notoriously hard for trees. And then here we upgrade the optimal set to a set comprising all three structures with no bias. So that's the second theorem. Okay, so if you have pure interaction, you have a lower bound of n to the one-half. You have a little bound of n to one-half. And then the third theorem is that: okay, so now let's go back to division setting with only one tree. Then we can obtain this lower bound that is exponential once again. Okay, so almost out of time, so I'm gonna skip this. So, what do our results mean for practice? So, what are the takeaways? So, over the short term, what it means is that K should That Kev should do some of these trips. We should run multiple runs of the bud chain and average the results in order to get better performance because we know that one chain does not make so well. And secondly, when we get credible endures from the bud algorithm, we should not take them at face value because they may not actually reflect the true posterior. They only reflect the approximate posterior. But over the long term, there are even more interesting questions. Okay, so this means. Okay, so this means that our results mean that the sampler has a lot of room for improvement. Okay, so what are the ways in which you can improve it? So, first, temperature control. So, we saw that the lower bounds increased with the training data size. So, this is because for some reason, the temperature for the chain is tied strictly to this sample size. So, we don't need to do it. It's unnecessary, and perhaps we should try to decouple the two things. Secondly, instead of using uniform proposals of the different moves, we should use an informed proposal. If we know what the promising move is, we should try to choose it more often. And lastly, instead of making local moves, we should try to be more global about the proposed moves. All right, and here's the last slide. What are our contributions and summary for this talk? Okay, so we created the framework for meaningful computational load bounds. Okay, so we provide the first analysis of bus. Okay, so we provide the first analysis of but with multiple trees. We provided hidden time load bounds under different settings which showed that it growed to the sample size. We did simulation that this in the paper. Lastly, we have these analysis, so why the sample performs poorly and how we should overcome them. Okay, and all of these results will appear in the paper in a few weeks. Thank you.