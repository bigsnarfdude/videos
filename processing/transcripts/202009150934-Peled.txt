On the Beers webpage. It sounds like we're going to move the schedule on Thursday back by half an hour. So we'll start probably half an hour later because at the moment there's a clash with the one world probability seminar. And it'd be a shame for that to overlap with Duncan's talk because I think the audiences will potentially overlap. So we'll confirm that on the webpage, so just check that before you come on Thursday, but it's likely to be half an hour later. Before you come on Thursday, but it's likely to be half an hour later. Good, and I it's should I um are you happy to get going? Uh, yes, uh, shall we record the talk? Is it recorded already? I started the recording, yes. All right, so yes, uh, thank you everyone. Fantastic to have um have Ron here. Um, it's really good he can join us, and um, yeah, he's gonna you can see his title on the screen. Um, go for it, right? So, uh, thank you very much. So, thank you very much, James Omer, and Ballint for having me. So, I'd like to tell you about the cycle structure of Euclidean random permutations. And this is based on joint works with two of my former master's students, Dor Elboim and Alexei Glatki. Dor is now a PhD student at Princeton. All right, so let me start by telling you what are these Euclidean random permutations. This is a general. Permutations, this is a general name. Let's start with something else: spatial random permutations. That would be more general. So, a spatial random permutation is some random permutation biased towards the identity in an underlying geometry. This really doesn't say much. It's just some idea. It's not a specific distribution. Here is an example to have in mind so that you can feel this concept. Suppose you have some fine. Concept: Suppose you have some finite metric space and you consider a random permutation of the points of the space. And you choose the random permutation with probability proportional to e to the minus the sum over all points in space of the distance traveled by that point for the permutation. So you discourage points to move far away according to the metric. Maybe the metric is very loose, so you don't discourage much, but generally speaking, Don't discourage much, but generally speaking, a point doesn't want to move too far away, and you just sum or sum these distances over all the points in space. So indeed, if the metric is not too loose, if it indeed makes the points not travel too far, then random permutations of this type will have a band structure. What do I mean by a band structure? It means that the typical displacement of a point will be smaller than. point will be smaller than it would be if there was no geometric background so smaller than the size of the space say than the diameter maybe small smaller than in a uniform permutation and the questions that we would like to study today is how does such a geometric structure affect the permutation statistics and our focus today is on the cycle structure of that the permutation there are other things of interest such as the longest increasing subsidy Such as the longest increasing sub-sequence, for instance, but we will not touch upon that to date. This is one motivation for studying it. It's a natural model of random permutations with a geometry. And another motivation is that there is some relation with phase transitions in other models. I'll touch upon that, I think, later on. Maybe I'll just show a picture before I go on. Ah, well, okay. Let me say that we focus on the case of Euclidean geometry. So actually, this. Geometry. So actually, this very general idea of metric space will not so much feature for us. We will have either R D or Z D. And in fact, we're not even going to discuss models of the type which is given in the example. We're going to discuss other models which are similar in spirit but have some integrable structure, which makes the analysis much facilitates finer analysis. So there are results on models of this kind by Armandaris, Betz, D. By Armandaris, Betz, Bisco, Ferrari, Fyodorov, Gandolf, or Grossmann, Lonardi, Moirhe, Wichtman, Hammer, Rus, Tagi, and Ulchi. And I'm not sure this is a comprehensive list. But still, the integrable structure will help a lot. Let me now show a picture. So I'd like to illustrate for you what I mean when I write a band structure. And I'll illustrate that by showing a permutation in one dimension. So when the underlying space is Z, the one-dimensional. Z, the one-dimensional integer lattice. This is the identity. Okay, I'm showing the graph of a permutation. Maybe I'll show the next picture. I'm showing the graph of a permutation. This is a permutation on 1000 points. And by the graph, I mean that what you're seeing here is x comma pi of x. Okay, so you have a point at x, pi of x for every x between 1 and 1000. And 1000 for this permutation. And what you can see is that the points are closer to the diagonal than you'd expect in a uniform permutation. They seem to lie mostly in a diagonal strip of width 200 in this case. So this is what I mean by a band permutation. It lies in a band. You can also do that in two dimensions, but then it would be hard for me to draw a picture. The input x will be two-dimensional and the output would also be. Two-dimensional, and the output would also be two-dimensional, but in some sense, it lies close to the diagonals. Points do not move too much. So, here I've just shown some progression. You start with the identity permutation, where all the points are exactly on the diagonal. And then, when you somehow make whatever model you're looking at more loose, allowing points to move a bit further, in this case, this is a sample of the Mallow's distribution with Q being 0.99. Then the points are. Then the points spread a bit, and now they've spread more. And if you were to allow them to spread without any geometric structure, you'd eventually get a uniform permutation, which looks like that. So you may think of the models we're studying as interpolating between the identity permutation and the uniform permutation according to some geometry. And you'd like to understand how the cycle structure depends on that. Before starting to discuss Before starting to discuss these spatial structures, these geometric structures, let's first discuss the case when there is no geometry, the so-called uniform and Ewens permutations. So suppose that pi is a uniform random permutation on endpoints, then as is well known, the cycle structure of pi is described by a stick-breaking construction. What is a stick-breaking construction? You may think about this interval here as the interval. This interval here is the interval from 1 up to n. To decide the length of the cycle that contains the first point, you take the interval from 1 up to n, you pick a uniform position along it. In this case, I've picked this position. And then the length of the cycle containing 1 is the length of this interval between 1 up to the point you've picked. This is how you've broken a piece of the stick. You had a stick of length n. You had a stick of length n, you've broken a piece at the uniform position, and what remains, meaning this part over here, is a smaller stick. And now in this smaller stick, you again pick a uniform position and break. And now you again pick a uniform position and break. And you do that until there is no stick left to break. And then these are exactly the lengths of the cycles in a uniform permutation according to the following rule. According to the following rule, first you add the length of the cycle containing one, then the length of the first point, of the cycle containing the first point not in the cycle of one, then the next point not in the previous cycles, and so on and so forth. So this is the stick-breaking construction. So this is one way to describe the cycles of a uniform permutation. Here is one more thing you can say. Suppose you have C of pi cycles and you sort Cycles and you sort them. You take the largest length, the second largest length, until the least largest length. And you may consider this as a vector in a C of pi-dimensional space. If you normalize the vector to sum to one, the length of all cycles is sums to n, and I divide by n, you normalize it to sum to one, and now you take n to infinity. So typically you have more and more cycles. So the length of this vector grows with n. This vector grows with n. Eventually, in the limit, it's going to be an infinite vector. But it turns out that the length of the longest cycle, for instance, takes some specific fraction. It has a certain distribution of what fraction it takes from one, from the whole thing. And so on for the second largest and third largest. And the resulting distribution is called the Poisson-Dir-Richelet distribution with parameter theta equal one. We'll denote it by PD1. So Pd1 is a distribution. So, Pd1 is a distribution on infinitely long vectors of non-negative quantities which sum to one, and these are the fractions that cycles take of the full permutation, the longest, the second longest, and so on. Now, similarly, I'll just go over this quickly. Similarly, you may define the so-called Ewens distribution. This is, if you wish, a generalization of the uniform distribution, the case state A equals one will be the uniform. You have some parameter. You have some parameter theta, and the probability of pi is proportional to theta to the number of cycles of pi. This was introduced initially in the biological context, but it has many applications. It's a very useful and famous distribution. Really, you may in fact see it as a spatial random permutation on the complete graph. I won't dwell on that, but if one looks closely, it is the Mallows distribution on the complete graph. I'll talk about Mallows later when Q is. Mallows later when q is 1 over theta. But in any case, it turns out that this generalization of the uniform permutation has a very similar cycle structure, where instead of the uniform distribution of position where you break the stick, you break according to a certain beta distribution. And if you do all that, then the sorted cycle lengths that we discussed before will converge to Poisson de Richley with parameter theta. Richley with parameter theta instead of parameter one, which we have in the uniform. So, this is a generalization. Okay, so this was all I wanted to say about permutations with no geometry. And now I'd like to talk about permutations with geometry. Initially, I thought I'll tell you first about the interchange process, so-called interchange or stirring process. This picture is taken from Gold Schmidt, Wolsche, and Windridge. But then I thought I won't have the time, so maybe I'll come back to it later. I'll go come back to it later, but for now I just move on. And I'll just say that it's very interesting to study the interchange process, and there is a big conjecture, and it's related to the phase transition of the quantum ferromagnetic Heisenberg model, and this is a fundamental open problem. But I won't talk about it. I think other people in this workshop will talk about it. Okay, so I move on to a different model. I move on to a different model, and this is one of these integrable models I mentioned, the so-called Mallows model. Let me introduce the model to you, tell you what it is. So I introduce it for a general graph, but we will later restrict to the one-dimensional case. That's the only case that's integrable, except maybe the UN experimentation. The only case I know. So you start with some finite graph, and you have a parameter q between 0 and 1. And now, for And now, for a given permutation, you measure the distance of the permutation to the identity in adjacent transpositions along the edges of the graph. So you have a permutation of the vertices of the graph, and you're allowed to swap positions of this permutation where there is an edge of the graph. You may swap, and by using such adjacent transpositions, you Transpositions, you wish to go to the identity permutation on the vertices. And the question is: what's the minimal number of steps you need to take? This could be a non-trivial computation, but we will not dwell on the computational side. We just define d adjacent to be this minimal number. Okay, so it is a metric, in fact, on if you define it between any two permutations, it is a metric on permutations. Now, the math. Now, if G is connected, the Mallow's model with parameter Q on this permutation is the model in which you sample a probability, a permutation with probability proportional to. You have the base is Q, and then you raise Q to the number of adjacent transpositions required to go to the identity. So it has this spatial structure, which I described earlier. If your permutation. Earlier, if your permutation is very close to the identity in this metric, then the exponent is small, and then the probability will not be so small because here is a number between 0 and 1, but you raise it to something small, it's not too small. But if instead your permutation is very far from the identity, then this could be a very small probability here. It's just proportional to that. And it's still unclear how many such permutations there are at this distance, so you don't know. There are this distance, so you don't know exactly what you'll get. But this is a way to bias the permutation towards the identity in the metric of adjacent transpositions. And of course, it depends on the graph that you start with. If you take q to 0, then you concentrate on the identity permutation. If instead you take q equal 1, then the model is exactly the uniform distribution because then the geometry plays no role. So this is one of those spatial random. So, this is one of those spatial random permutations. Now, we will only analyze this model, we'll only discuss it in dimension one, when the graph G is an interval in the one-dimensional lattice Z, say the interval from one up to N, for instance. And then adjacent transpositions means adjacent integers. You can swap pi of i and pi of j. And what's good about that case is that there is then an explicit formula for the distance in adjacent transpositions to be. In adjacent transpositions to the identity, it equals the number of inversions in the permutations. That is, it equals the number of pairs of inputs, i, j, so that on the one hand, i is less than j, but on the other hand, pi of i is bigger than pi of j. So it reverses the order of i and j in pi. That's why it's called an inversion. You count all the pairs like this, and that's exactly the distance in adjacent transpositions. And as it turns out, this And as it turns out, this equality implies a relatively simple sampling algorithm for the model. I don't know if I'll have a chance to explain to you the algorithm, but this is the key to the study of the model. This algorithm, already discussed in fact, by Mallows in 1957 when he introduced the model, is the key to the solvability of the model. The model was originally introduced as a statistical model for rankings. And let me mention. And let me mention, though I will not use it, that there is an extension of the definition from a finite interval in Z to an infinite interval, either one-sided infinite or two-sided infinite. This was found by Gnadin and Olshansky. They call such things Q-exchangeability. Okay, so what do we know about the Malus model? Let me now discuss some results. And I don't know, maybe at the end I'll talk about proofs. We'll see how time goes. So the first thing I So, the first thing I'd like to discuss with you is something relatively simple: the displacement of the Mallow's model. And you see it in the picture below. When I set the parameter Q to some number between 0 and 1, then points will travel more and more on average as Q approaches 1, as you become the uniform distribution. This is visible in the picture. This is the identity. This is Q equals 0. This is Q equals 0.99. Q equals 0.99, this is Q equals 0.995, and this is Q equals 1, the uniform. So they will travel more. Let us quantify this displacement. This will be useful for us also later on. So one can prove, and this was shown, various properties of this kind were shown by Braverman in Mossel, 2009, Nadino Shansky, 2012, and myself with the name Tarabatnagar 2015. So one can show. So one can show that the expected distance that the point travels, whichever point, is more or less q over 1 minus q minimum with n. So how to understand this? I don't care so much about the case that q is small. Let's only focus on the case that q is close to 1. When q is close to 1, the numerator is order 1. So really you should have in mind the two quantities 1 over 1 minus q. minus q and this is the one quantity sorry one over one minus q and n and what this is saying is that the distance that the point travels is one over one minus q unless this distance is bigger than the size of the whole permutation unless it's more than n and then it travels order n because it can't travel anymore so the graph of a malus permutation becomes closer to that of a uniform permutation as q goes to one form permutation as q goes to one and their displacements are similar when one minus q is of order one over n or even smaller than that and in fact there is a permuton limit for those who know what that is of the the graph that you see here when one minus q is of order one over n and this was found by star in 2009. And let me just mention that although the graph becomes graphically similar when one minus q is 1 minus q is little of 1 over n, the total variation distance is still not very small between uniform and mallows until 1 minus q becomes even smaller than 1 over n. So it must be, I don't remember the order of magnitude, maybe 1 over n squared, maybe something else. But in any case, it's not sufficient just for the graph to look like this. Okay, very good. By the way, I don't monitor the chat, but there was something there now. Something there now? If it's okay, if there was something important, let me okay. So we've discussed now the displacement results, and the next thing, and this is the main thing I'd like to discuss with you today, is the results on the cycle structure. So I would like to tell you about results obtained with my then master's student Alexei Gladk, published 2018. Okay, so let us study the Mallows permutation. So let us study the Mallow's permutation with parameter q between 0 and 1, and let's consider the length of the cycle containing a given point. Then, as it turns out, this length is up to universal constants 1 over 1 minus q squared, or n, if it cannot go beyond n, because there is only n points in the permutation. So it is of the order of the displacement square of Square of order of displacement squared. Okay, so this is something to take into account. So it is somehow interesting that the displacement can be very small, but the cycles are not too short anyway. Okay, displacement squared. Squared. Yeah, I shouldn't have even attempted this. So, for instance, you look in this graph here, the points don't travel much, but still they form relatively long cycles. They go up to the length of the displacement squared or n, whichever is the smallest of them. Okay, this is now you have in mind that there is, in fact, a subcritical and a supercritical regime when the lengths of the When the length of the cycle is still little O of n, that's when we say it's subcritical. We say the cycles are microscopic or mesoscopic, but they are not macroscopic. Macroscopic means of order n. So if the 1 minus Q is still above 1 over root N, then the cycles are mesoscopic. It's a subcritical regime. Afterwards, the cycles are macroscopic. And what is interesting is that. And what is interesting is that when the cycles become macroscopic, then suddenly, although the permutation is very far from being the uniform permutation, even the displacement is not yet right, still the longest cycles, the lengths of the longest cycles, look like the uniform distribution. They converge when normalized to the Poisson Dirichlet distribution with theta equal one. So the longest cycles become like the uniform permission. Become like the uniform permutation much, much before the permutation itself looks like the uniform. Maybe the graph of the permutation is still something like this picture over here, but already, which is very different from uniform, but already the lengths of the longest cycles looks exactly in the limit, exactly as it is in the uniform permutation. Okay, so this is what's written here. These are the main results that I'd like to say about the Mallow's model. Say about the Malos model. And let me mention that there are also other results about the Malos model, which I will not go into. For instance, Schumet Mukherjee found the Poisson limit laws for the short cycle, the lengths of the short, sorry, the number of short cycles in the regime where the permutation looks a bit more like uniform, when there is the permutant limit. Other things you may study about the one-dimensional Malus distribution is, for instance, to show that the descents form a determinantal That the descents form a determinental process. This is a result of Borden, the Connectent Fuhlmann. And myself and Naentara Batnagar, and then Naentara with Rede Bassu, studied the longest monotone sub-sequences. Because in a one-dimensional permutation, you can talk about monotone sub-sequences in the graph. I think we'll hear more about that later in the workshop. And not this result, but other results on increasing sub-sequences. This is all I'd like to tell you about the Mallows model for now. To tell you about the Mallows model for now. And now, let me move on to a second model, which has some integrability properties for which we can have results. The second model, it doesn't actually have a name. You may call it the permutation representation of the non-interacting Bose gas, but that wouldn't be so wieldy. In our paper, we just said Euclidean random permutations, but it's also not so precise. In any case, it's a In any case, it's a model coming from physics origin. Matsubara and Feynman noted that you may use it to study the so-called non-interacting Bose gas. And they wanted to understand it better to understand the phenomena of Bose-Einstein condensation. I won't go into that, but in fact, proving mathematically the existence of Bose-Einstein condensation, the so-called interacting, not non-interacting, the interacting Bose gas is a very important problem. Gas is a very important problem, problem, but I won't go into that. So, how does this model go? It's actually not so simple to describe, so please listen carefully. There are five parameters to the model, including the dimension. You have n points, and you put them in a box of side length n in dimension d. That's three parameters, and you need to place the points. You need to place the points somewhere. They're not the points of the lattice. They are somewhere in the box. And you will sample a permutation of them. Both the point locations and the permutation are random and they will be sampled jointly. So to be more formal, a configuration is a pair where x is the point positions and points in 0L to the b, and the permutation pi is a permutation on the points. And it is sampled from the following. And it is sampled from the following density. So, not just the permutation is sampled, the whole pair x is sampled from this density with respect to the begg times counting measure. So, you sample them together. There is another parameter, theta. It is not so important to us. You may set it to one if you don't like it. So, you have theta to the number of cycles of pi, and afterwards, you have the main term you penalize according to how far points move. If a point Points move. If a point is mapped very far in the square 0L to the d, you give a big penalty. If it's mapped not so far, you give a small penalty. So in this sense, it is like these spatial cases we discussed. You may think of u as x squared. That would be a good thing to have in mind. But in fact, we cannot exactly do that because we need some periodicity, as I will mention. periodicity as I will mention but still you you may have in mind u of x equals the norm of x squared and that would be a very good idea okay so what is this function u it governs the distance that points typically travel and we need some periodicity so the way we are defining u is the following take x to be a random variable with a density Variable with a density. For instance, the Gaussian. Gaussian is okay. Gaussian in Rd, standard Gaussian. Assume that it has mean zero and that the density is a good function. It doesn't really matter. The analysis uses something, but it doesn't have to be so strong as Schwartz, a good function. And now the way you define u is the following. You want to know the penalty given to y. So how much will you pay for a jump? How much will you pay for a jump of size y? Well, think of y as a jump from zero, from zero to y. And now imagine that the random variable x was the jump distribution going from zero to y. But if x happened to land not at y, but at some offset of y by L times an integer, so you think of the box as being wrapped around periodically in space. 0 L to the D is tiling the space. L to the d is tiling the space. So if you jump from zero not to y but to some translate of y by l times a point in z d to some other translate, then it's okay. Then you arrived at y as well because there is periodic boundary condition. So all in all, the probability, the density to land in y is the sum over all integer points of phi of y plus l dot k. So this is a difficult, somewhat difficult definition, and really it's not important. This is really a technicality. So just have in mind u of x. So just have in mind u of x equals norm of x squared if this seems difficult. And this is the definition of the model. Let me go back to the density. Let me show some pictures. So what do we have then? So here in these pictures we took in the x here Gaussian, there is a question. Yeah. Can we think of the points as lying in a three-dimensional torus instead? Yes, yes, definitely. Yes, yes, definitely. It is like that. Just that I needed to introduce X, which is on Rd, so I needed to explain a bit more. We actually use the density phi on Rd because we're going to change L. And then the way in which U changes as you change L will be by this formula. So for that reason, there is some importance, but it's not so important. Okay. So I go back to the So, I go back to the simulation results. So, here we see a two-dimensional simulation. We've put 50 points. This was done by Dor Elboen. We put 50 points. We took theta equal one. And you see the location of the points is random. And each point, and the points lie in cycles of a permutation. There could be fixed points. For instance, this is a fixed point over here, and there are many fixed points. Points. Now, what I'm showing you is a sequence of pictures, a sequence of samples where I've changed a parameter that's called rho. Let me now explain what is rho and how we should we think of this change. So I go back one slide. Rho is the density of particles. It's the amount of particles per unit area. Because I'm fixing for the moment the number of particles, the number of points, then the way I can change the density is to squeeze them in a Density is to squeeze them in a smaller box that will increase the density, or to put them in a larger box that would decrease the density. And the idea is the following: I'm going to fix you. You may think of this U. And then points usually travel to distance one because it fixed you, so order one. If I squeeze the points in a small box, then at distance one, you can find many other places to jump to. And if I spread the points in a big box, then at distance one, you don't. Box, then at distance one, you don't find so many points to jump to, and maybe you just stay as a fixed point yourself because there is nobody to go to. So the idea is that when the density is large, then the number of points per unit, yeah, when the density is small, sorry, when you have a big box and the density is small, you have many fixed points. But when the density is large, then things will start to travel more. Will start to travel more. So it's really analogous to this displacement concept that I've mentioned earlier. So here you see that we've put the 50 points in a big box and the density is only 0.2. And indeed, they don't form big cycles. They don't tend to jump too much. Now we've increased the density a little and maybe the cycles grew a little. We've increased a bit more and they grew a bit more. And so it seems to continue. But then some at some point. But then, at some point, well, I made a lot of jumps here, but still, at some point, suddenly the cycles seem much bigger than they used to seem. Maybe they feel the extent of the box, and you increase a bit more, and maybe it looks even almost like a uniform permutation. Maybe there is one cycle taking up half of the points. So, increasing rho is analogous to this displacement idea. And the results I will show show a phase transition should happen in this model. Transition should happen in this model roughly at 0.623. So you see a bit above this, a bit below that. Okay, it's not, I didn't, in these pictures, we didn't capture the exact. Okay, so now I will tell you about what results happen for this Bose gas in various dimensions. So I will have now three slides of results, one for dimension one, like the Mallow's model, you can do this in one. Mallows model: You can do this in one-dimensional box, the next one on two dimensions, and the next one on three and higher dimensions. The results in dimension one are especially interesting somehow because of the way they look. So, we are in dimension one, and we assume now that the density will go to infinity. We can analyze the case of constant density, but it's not so interesting. So, we take it to infinity, and also the number of points goes to infinity, so it means also the box size changes. Size changes in the right way. And let L1 be the length of the cycle containing the first point. So we identify three regimes: the subcritical, the critical, and the supercritical. It turns out that what's important is whether the density is smaller than the threshold square root n, or it is of order square root n, or it's more than square root n. If the density is smaller than square root n, then the length of the cycle is as The cycle is, as was before, if you recall from the Mallows model, L1 is of the order of the displacement. This will turn out to be the displacement in one dimension. So it's of the order of the displacement squared, just like in the Mallow's model. But we can say more than just the order of magnitude of the length of the cycle. We can also give a limiting distribution. It goes to the gamma distribution after appropriate scaling, which means this density. This is the gamma, gamma one-half one. Gamma, gamma one half one. This is the subcritical case. If rho is exactly of the order squared n, so now the macroscopic cycles start to appear. Now, length of the cycle starts to be of order n, as you see over here. And you divide L1 by n, then it goes to a very funny density between zero and one. We have not seen this density anywhere, but it's a big expression. We cannot even prove directly that for general parameters that it integrates to one. That it integrates to one, but it is because we can prove it. So it's this very big and funny density that I don't have much, something intelligent to say about. It involves the Jacobi theta function, meaning the sum of e to the minus n squared, or here, for instance. But we haven't seen it in the literature elsewhere. Bian, Pittman, and yours survey some probability distributions related to Jacobita function, but we haven't found this one there. But we haven't found this one there. We found cousins, maybe. I don't know if they're related or not. So, this is a very explicit expression. And in the supercritical regime, and for some technical reason we put this, but it's not needed for the result probably, then again, once the cycles are macroscopic, once you're supercritical, then the permutation is very different from uniform. The displacement is only square root n or a bit more, but 10 or a bit more, but the lengths of the longest cycles go to what they do in the uniform, or in this case, the Ewens permutation, to Poisson de Richlet theta. This density, which is difficult to read from here, maybe you'll find it nicer in this picture, where you can see it for the various values of the limiting constant alpha. Really, as alpha increases from zero to one, it interpolates between a delta mass at zero and the uniform distribution on zero. 0 and the uniform distribution on 0, 1. So this is how you may understand this, but it's an explicit and difficult expression. Okay, let me say now about dimension 2. In dimension 2, then again, we assume the density goes to infinity. There is a certain critical number that is given by a simple formula. I just take the covariance matrix of the random variable x from two slides ago and the determinant and so on. The determinant and so on. It turns out that what's important is the ratio between œÅ and logarithm of n. That's the quantity to look at. If that converges in the limit to a constant which is smaller than the critical alpha, smaller or equal, it could in particular go to zero. So if rho is, I don't know, square root of log n, it goes to zero. That's okay. Then what is the length of the cycle? The length of the cycle is, roughly speaking, it's e to the Picking its e to the row times a certain constant times a uniform random variable. So this is an interesting result. The length of the cycle is not concentrated. The length of the cycle containing one, it could be very big, could be very small, not very concentrated. It's to a uniform distribution. For instance, if rho is of order logarithm n, then you put in logarithm n for. You put in logarithm n for this, then you get n to the power uniform, n to a uniform. It's a very big spread, but this is what it is. Okay? And this holds all the way to the critical density. The critical density is no different in this case than the subcritical case. And when you're supercritical and your rho divided by log n goes to a number which is more than alpha c, that's the definition of supercritical, and this technical condition which you've Critical and this technical condition, which you may ignore, then asymptotically one minus alpha c over alpha fraction of the points lie in the macroscopic cycles, meaning those that can go to Poisson de Richle, those of order n, or lengths of order n. And indeed, if you take the longest cycles and divide them by the proportion of points in them times n, then you get to the Poisson de Richlet distribution. So this time a different phenomena happens. Not all the points converge to Poisson de Richlet. Points converge to Poisson de Richlet, just a fraction of the points there go there, and a certain other fraction remains in mesoscopic cycles. Indeed, when you ask what's the length of the cycle containing one, it's either macroscopic, so this ratio would be order one, would be one, or it's mesoscopic, uniform between zero and one. So we really understand the distribution very well. And I hope you can get a picture from what's Can get a picture from what's displayed. Before I run out of time, I go to the dimensions three and higher. So, in dimension three and higher, now we assume that rho is fixed because the phase transition happens at the constant rho. And there is a certain critical number, critical rho, and it's just given by the j's convolution of phi width itself at zero and the sum over all the j's. So, a certain number. If rho is Number. If rho is smaller than the critical point, then L1 converges to some integer-valued random variable with exponential tail decay. If it equals the critical point, then the length of the cycle containing one has only a power low decay, no longer exponential decay, but still it converges to some number. And that means, in particular, that the fraction of points in big cycles is zero, because this is a probability distribution. A probability distribution. And if you have more density than the critical density, then a certain fraction of the points will lie in the macroscopic cycles and again will converge to Poisson de Richlein. And the other fraction of the points will lie in order one cycles. So in this case, you either have cycles which are very short, order one, or you have cycles which are very long, order n. You know exactly the fraction in the big ones. The big ones goes to Poisson to each theta, and the rest of order. Rich late theta and the rest of order one, and we actually have explicit expressions for everything here. These are the results for the model. Let me mention that there were many previous results on this. The mathematical analysis started with Chupteau and continued with great work of Betts and Ulchi. However, their focus was dimensions d greater or equal to three. So they mostly obtained the result which is written here in the dimension bigger or equal to three. Dimension bigger or equal to three, although there are some differences between their results and ours in the class of distribution study, the X study. Bets and Ultra also showed that in dimension one and two, you can get results like in dimension three. If X has heavy tails, we don't do that. Still, we know more in some cases. So, first of all, dimensions one and two, what I showed you is new. And secondly, in dimension D greater or equal to three, we saw and we proved certain limit laws which were not in the Limit laws which were not in the previous analysis, except the Poisson de Richlet, which was in the previous analysis. We heavily rely on previous works of Bogachov and Zeindler. They analyzed the related surrogate spatial model, which has similar integrability, and we are inspired by their techniques, though we need additional innovations in dimension 1, 2, and in the critical cases, in dimension 1, 2, 3, and 4. Okay, so this is what I wanted to tell you here. To tell you here, and I think I'm already out of time, so I don't know. Maybe I can mention something about the proof, maybe not. What you say, and I finish now, or I go for two more minutes. There is the open questions I can mention, or I can also say something about the proof. And that's it. You've certainly got another couple of minutes if you'd like to. All right. Okay, so I'll just show something. I'll just show something regarding the proof. I'll share with you a different screen. This one, a different screen. Where is it? Here it is. Okay. So I'll just tell you what makes the model, which I just showed you, the model of this non-interacting Bose gas, what makes it solvable? And the idea is, and this is an easy calculation. Is, and this is an easy calculation that although initially I told you that you sample both the point locations and a permutation, you can take the marginal distribution just on the permutation, and it is explicit. So you can anneal away, you can integrate away the point positions, and everything looks nice. Indeed, the probability to see a permutation pi is just a product over cycle weights. It's just a product over the lengths of the cycles of pi. For each length, you have a certain weight. For each length, you have a certain weight. That's all it is. And this is pretty easy to derive. I don't have the time, so I just flash this. I just say it's pretty easy because initially the probability of permutation is the product over how far a point jumps. But indeed, when you anneal all the points, when you integrate x, then this becomes a product over cycles. Each cycle gets its own part of the integral. The idea is that if you remove a cycle from the permutation, then the remaining Permutation, then the remaining points behave exactly the same. So you really, it becomes kind of independent cycles, except that their length has to sum to something. And then you can just do an exact calculation and you reach this. So this is the source of the integrability. Once you have this, the rest is analysis, which I would like to tell you something about, but I don't have the time, so I'm not going to. And I go back to the open questions and conjectures, and the one interesting thing you can take. One interesting thing you can take out of this talk, especially if you like the interchange model, is that although I looked at models which have integrability, it could be that their properties give predictions to the behavior of other models without integrability. That is, it is possible, not proven certainly, but possible that there is some universality, that other models behave similarly. For instance, the Poisson de Richle limiting distribution is expected to be universal. Be universal. Another instance, if you measure the typical displacement of a point, what the previous results tell you when they hold for the mallows and for this Boseguest thing, it tells you that when the displacement is of order square root n, that's when you see a transition in dimension one. In dimension two, you need to go to square root of log n, then you see a transition. In dimension three, you should go to constant order, only then will macroscopic cycles appear. And before you reach there, in the sub-grid. And before you reach there, in the subcritical regime, the length of cycle is the displacement squared in dimension one and is of order e to the c to the d squared, maybe c is random, in dimension two. And such results in dimension one were proved for the interchange model by Cosmo and Sidorovichius, done maybe six years ago, still not published. There is some analogy with the behavior of random band matrices. I don't have the time to talk. And there is a question whether they mentioned two really easy. Whether dimension two really is universal, because in some models, for instance, Betts has numerical simulations, in some models there appear to be a Berezinski cluster its Taulus transition, not in the model of the Bose gas that I showed you. That model doesn't. Okay, thank you very much, and sorry for going over time. Thank you very much indeed, Your Honor. We can virtually. Anybody got any questions they'd like to ask? Any questions they'd like to ask? We're slightly behind, but if people have got a question they'd like to put to Rona, then please do go ahead. Feel free to jump in or hey, Alan. Your name for the interchange model on trees, but didn't get to show it. No, no, not at all. I just was checked by the very last thing you said, actually. I wondered myself about whether you could find. Whether you could find with the Kersalips Tules transition in 2D, one of these models where somehow you might have S and E curves. Right. I never quite managed to figure out what the parameters would be though. Okay, so in fact, there is a spatial random permutation model when you take a permutation on the points of the two D lattice and you insist that each point is mapped to a nearest neighbor. Is mapped to a nearest neighbor. So you don't just take a jump to arbitrary distance. Each point goes to the nearest neighbor. Indeed, the uniform distribution on such permutations in a domain, if you think about it, because each point is mapped to a nearest neighbor, the cycles are either a double edge going to a neighbor and back, or they form some big loop. That's the only way. And indeed, this is exactly the model of the double dimer, the double dimer model in dimension two. Module in dimension two. And that's conjectured to have SLE4 for the cycles, the loops. CLE4, I don't know about, I just say CLE4. I know you can be more precise, but I can't. I don't know rho and so on. So at least one model of a similar nature has it, and that's was simulating spatial random permutations where the jump distribution is e to the minus x squared. And also, apparently, when you give a small constant. Apparently, when you give a small constant in front of the x squared in the exponent, you can see similar SLE type things, at least our law behaviors, but I don't know much more than that. So it seems they can appear when you talk about lattice permutations. They don't appear in the Bose-gas picture. I mean, you have this for gasity, is that the right word, or anyway, you have this doubling up right of entropy from the ability to reverse cycles. So that Reverse cycles. So that's right. The double dimer model has that because when you take the union of two dimer configurations, then each thing that you get, which is not just one doubled edge, has two possibilities for how you get. So it's exactly the double dimension. Yeah. Yeah, there was some reason I've forgotten now why I thought that was CLE4, and you're basically saying the same thing. And of course, the dimer model is very specific, and that reduction. Very specific, and that reduction isn't completely convincing that it means that it will manifest that characteristic more generally. But you're saying that doing simulations that seem to indicate that CAPRIS4 is coming out? I can't tell you. I know Betts has simulations, but I didn't have the time to go into them in depth. I don't know if Betz is here to tell us, but he has tried to understand this on the numerical side. On the numerical side. I think once they look, there's some physics article. Anyway, sorry. Yeah, anyway. Perhaps we continue this discussion offline or afterwards. Are there other questions? Okay, so I will stop the recording and we can.