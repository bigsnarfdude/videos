I'd like to thank the for um for all four of us. And one of the organisers, Jen, is not here, so I particularly want to thank her in her absence. Okay, so um this is actually not just a David talk, it's the kind of applications to physics talk. Uh and I want to talk about low problems. So um kind of got three pieces to the talk and how this is up into three fairly This is up into three fairly equal pieces, if I can. So, the general topic is finite methods for incompressible flow. That's my PhD topic. You'll hear more about that in a moment. And the three kind of themes to the talk. So, I'm interested in trying to better conserve mass, what kind of thing. And then, I'm interested in computation with this thing called the input constant. We've heard about that in a couple of Constant. We've heard about that in a couple of talks earlier. Say more about that as we go along. And then finally, I want to talk about fast linear algebra of solving problems. Okay, so operator free condition. And if you want to know more, then there are two papers. So this conveniently brings in one of the other co-authors. So this is going to build part two, the bit about. Build part two, the bit about UM subconstants, builds on work I did at Valeria over ten years ago and a method of computing UM subconstants that most of you won't have heard about. And the work itself with Jen, so we posted that earlier this year, so you can look at it on archive. And I have to stress this is pretty incomplete work, other than that there is a paper about it. Unlike Javchek, there are more. There are more questions than there are answers for elaborate as we go along. I'm not afraid to throw out questions to people. And I do so without actually knowing the answers, unlike some people. So, well, where should we start? Well, Howard motivated this very nicely in his remarks yesterday. So you might have seen this book. It's actually the first edition of this book, 2005. This is kind of the starting point. This is kind of the starting point. And the starting point is this picture here. So we want to do flow over a step. Now this stokes flow Reynolds number 100. The dimensions aren't quite correct here because this length is 1 and it's 5 across. It's kind of a long, thin channel. Just a two-dimensional slice through a 3D duct. And we've got fully developed flow coming in here. That's channel flow. Here, that's channel flow, that's called Pusato flow. And we've got fully developed flow, this is the outflow at the end, and the flow comes over at the step, and we have a nice big recirculating bubble, which you would expect to see behind the step. Now, this is actually just the first recirculation. There's going to be an infinite sequence of recirculating bubbles going into the corner. You can't see them there. They're called mophidics. So, this is the picture of. So this is the picture of the streamlines of the flow. This tells you that a particle coming in here will come through here. And it also tells you that there's a region of trapped flow. That is that if you're here, you just go around in an orbit. It all looks pretty good. I mean, it's how you sell a book, right? A nice picture on the front, specially chosen. This is the pressure approximation that goes with this. So we have, essentially, if you have channel. Essentially, if you have channel flow, you have pressure which is decreasing linearly. You can see this at the start. And then we go over the step. There's actually a point singularity at the re-entry corner. The pressure is infinite there, or minus infinity. And then it recovers and then goes back to channel flow. It looks good. But actually there's a problem. So what is the problem? The problem is we haven't really considered mass. is we haven't really conserved mass. What we've done is we've used standard finite element, so P2, P1. This was the first finite element method that was provably stable. Proved by engineers in 19, sorry, introduced by engineers, Taylor and Hood, in the mid-1980s, just before I started my PhD, and later shown to be a stable approximation. So Peter, find an L1 to 4. So we find an L14. So we're going to do this from triangles. This is piecewise quadratic, continuous polynomials, and so you define 60 degrees of freedom. Okay, so this is my velocity approximation. And I've got two components at each vertex, horizontal and vertex. Quadrastic velocity over all the triangles joining up continuously. And you combine that with P1. Okay, so P1 is my pressure. Okay, so P1 is my pressure. Also continuous. This is a scalar quantity. That's Taylor Wood approximation. So that's what was used to generate this picture. But it's a kind of an illusion. Because I've got a certain amount of fluid which is coming in on the left. And that has to balance, it does balance, with the amount of fluid flowing out from the right. There's a global mass balance. But if you have no global males, But you have no local conservation of mass. So, if you were to draw a line between here and here and calculate the volume of fluid crossing that line, it would be different from the volume of fluid which came in. You can't see it on this picture because I post-processed the velocities to generate a streamline plot, so it's hidden. Why should we care about this? Well, and what can we do to fix it? And what can we do to fix it? Well, one kind of slightly mad way of fixing it is to make the pressure discontinuous. So, how can we make the pressure discontinuous? Well, why don't we have a special super linear pressure and let's add a P0 approximation, one degree of freedom center. So, we're going to have a pressure approximation which isn't just linear, we've had a constant. So, it's a bit bigger than. Okay, so it's a bit bigger than linear. This is going to make the approximation discontinuous because you're going to have a different constant in every triangle. But it's not completely discontinuous because they have to line up. Think about having two planes, but you shift them. But they're not arbitrarily discontinuous, you're just shifting the planes up. So let's solve that problem using this method, P2, and then P1 together with P0. Together with P0, we've got a slightly bigger linear algebra system to solve and see what we get. And we see this is the pressure. We see this is the P1 pressure, which looks like the one we saw before using Taylor Hood. And then we get a correction, which is very nice, at the centroids. It's smaller at the scale. This is a 0.2 difference here. This is 0.04. So you can now view the P0 pressure as giving you a correction to the solution. Correction to the solution so that we have local conservation mass. And where the contours are high is telling you where we lost the mass in the first place, i.e., around the corner. So this is what I want to talk about, linear algebra associated with this augmented formulation. So you can do it in general. You don't have to start with t2, t1. So the general formulation would So, the general formulation with 2D and 3D, we start with continuous velocity approximation, degree pk or qk, so that's lead polynomials degree k on triangles or tetrahedra, or tensor product polynomials on rectangles or hexi. And you combine that with a continuous pressure approximation, degree one less, that gives you stability. One less, that gives you stability, which is degree one less. Those are called tailorhood elements, but then we're going to augment it with these guys constant approximation. And so the result is we've got mass conserved relics. This builds on our discussion from yesterday, personal story. So, where did I come across this approximation? I knew about it in 1987. I was a lecturer at Eumist, and I knew about Taylor Hood, and I knew about the possibility of getting local constellation of Mass. And there was a knock on my door for a young person, this is an important story, and it was a guy from Jack Sabatia. He'd been in Manchester for at least 18 months. He was a two-year postdoc. Postdoc and his job was to write a code to do flow in a chemical mixer. The chemical mixer was a toroidal device with inlet pipes, stuff coming in, it also had exit pipes, and then it had a thing at the bottom where stuff came out. But there was more than one way for the fluid to come out of this device, and it was swirly. So his job was to write a code to simulate flow. To simulate flow in this real mixer. And he'd written a code, but unfortunately, he'd come from the Czech Republic where he'd learned about finite elements. He decided to write a finite element code. And the only method he knew that was stable was this one. It was a two-dimensional, axisymmetric, so it's P2, P1. And he spent 18 months. He'd written his code. And his boss, the professor, had looked at his results constantly. Had looked at his results constantly and said it's all wrong. Whatever he did in terms of putting fluid into this device, he could never get fluid to leave through the bottom. It always came out the size. And he was comparing with experiments. It shouldn't have happened. And he knocked on my door and said, what can I do? Or I said, well, why don't you try adding this constant pressure? It's easy to code. And two days later, a big grin on his face, you know. Big grin on his face, you know, knocks on my door. He got results with just running the same mesh, now the fluid is coming out the bottom. So the moral of the story is you don't know what you're doing trying to find somebody who can help. It kind of, you know, it saved his postdoc. He was literally about to quit because he couldn't get it to work. Okay. Okay. So, linear algebra. So, what do we have here? We have a complicated mass matrix. We've got a two-level mass matrix. It's a two-by-two block. The 1-1 block is like table hood. These are inner products of, say, linear basis functions. This is a sparse matrix on a certain grid. We've got a Q0 block. These are piecewise constant basis functions. That would be a diagonal matrix. And then we've got Diagonal matrix, and then we've got cross-semes associated with the products of these two things. So, why is this kind of important? Well, it's important because what we've done is slightly mad. Just now we have two ways of representing a constant pressure, two ways of representing all pressures. So, we've got a singular mass matrix and actually ill-conditioned as well, not just secum. Okay, can we still run with this approximation? Can we still run with this approximation? Well, what we need to assess first of all is how stable we are. So, this is where interceptability comes along. So, this tells you, in fact, whether this approximation will work. We know it's going to work. How do you prove it's going to work? Well, you prove it using mathematics, but then you do some numerical kind of experiments just to justify your thinking process. So, here's the Stokes system now, not Maddie-Stokes. So, A here is the best of the plastic. So, A here is the vector of the Placian, and then B here has got these two components. V1 is the linear bit, V0 is the constant bit. Okay, and so in some stability in a matrix sense is pretty straightforward. You construct this 2x2, this share complement, BA minus BT. That's what fills in the bottom block when you do Gauss elimination, block Gauss elimination. You compare it with the mass matrix. You can show analytically that. You can show analytically that the upper bound is always going to be one from the continuous problem to in the discrete sense, and then you have a lower bound which you want to be bounded away from zero. Okay, so when you do your calculations, if you want to set up the matrices and compute gamma, you want it to be bounded away from zero as you refine the mesh. It's important. Obviously, it's bounded away from zero for any given mesh, but as you refine, it's got to stay bounded away from zero. You refine, it's got to stay bounded away from zero. And the extra complication here: if we're in closed flow, then this matrix Bt is singular, so the constant pressure is in the low space. So typically, we're always solving singular systems when we solve enclosed flow problems. Well, this vector is in the null space now. The vector which is ones at the vertices, that's the piecewise constant. That's the piecewise constant approximation defined by this approximation. And we've got a piecewise constant approximation defined by the elements. So this vector is in the null space of Q star. It's also in the null space of Vc. So you might think, okay, surely I can deal with this. No. Okay, so Ix just doesn't work. It's a complicated eigenvalue problem. Eigenvalue problem. It has got a common null space. You just throw these matrices at it without doing any, without knowledge of this null vector, you run its problem. So here's the bit that I can talk about in my work with Daleria. We have a simple method for calculating sub constant. And we do it by running minres. Whenever we solve a problem, Stokes problem, using MinRes, we use a preconditioner of this type. We use a preconditioner of this type where M, the one one block is spectrally equivalent to A with X to the plastic. You could use a multiple recycle here, and this block is Q star, or you could use a spectrally equivalent approximation to it, but I don't know how to construct a spectrally equivalent approximation to this ill condition operator. So we have to use the real Q star. You can't use the diagonal Q star, for example. It's not spectrally equivalent to it. Spectrally equivalent to it. And then you run MinRES. This runs a LandsOS process, and on the interior, you're generating Ritz values or harmonic Ritz values. You monitor those. They give you estimates of the interior eigenvalues as you run the process. And you can simply post-process those and get an estimate of delta squared. That's the discrete one. Yeah? Sorry, this is just for the discrete problem? This is just for the discrete problem? This is not the mesh independent. Well, we're hoping it'll get numbers which converge to something which is mesh independent. So this is what happens in two dimensions on a square. So this is standard Taylor Hood. So we've just got a toll on, so we're stopping this process. This is the associated linear system. We're doing driven cavity flow here. So it's tolerant of 10 to the minus 6. And this is the both. This is just uniform refinement of the triangular grid. Uniform refinements of the triangular grid as we go along, and this is my estimate of the sub constant squared. Basically, it stays nicely bounded away from zero. Numerical evidence that Taylor Hood is stable. P2, P1 star, well a slightly bigger system, same velocity space, you've got more pressures now, quite a lot more pressures, and then you see, interestingly, you get converged to a different number. So, like, how can there be two different in subconstants? And which one's the right one? Well, it turns out that this is actually an unsolved problem. Nobody knows what the inf constant is for a square, or any geometry which apart from like a circle. There are bounds on where the in-sub constants are, you can derive by clever Fourier analysis, but it's quite a deep problem in mathematics to compute in sub constants on even very simple dimensions. Constants on even very simple domains. What we do know is that the numerical calculation of the in sub constant is, which way around is it? This one is closer to the true solution than this one. So it's an upper bound on the true, too small. Okay, so the real one should be bigger than this. So we know that this. This. So we know that this must be closer to the true inf than this one. There are also intervals tabulated you can find about where the in-sub constant should be based on Fourier analysis. It turns out that this value is in that interval and this value is not. So we've learned two things. One is that this is stable, and also that if you want to get a better estimation of the incept constant, you should use this rather than this. This rather than this. David, can I ask, how important is it to get it right? In a way, I want it to be bounded away from zero and maybe to estimate it quickly. So how is that more of an, you know, what is the motivation to really... The motivation is that the whole theory of the convergence of mixed finite elements relies, will have the unsub constant in the bound. Right. So if you're going to do some kind of error estimation, aquatic error estimation, the unit constant is going to feature. Estimation uses a constant than a feature. Okay? I think the question David, which I got the same question, is whether 0.14 versus 0.19 mark depends how precise you are. I'm a fairly precise person, so for me it matters, but other people, engineers, who cares, right? It's less than, it's bigger than... I'm just starting to understand. I mean, it's going to change by iteration or by a half iteration? Iterations. I understand what you mean, sorry, to say that. So you said that the convergence is going to depend on this concept, right? Convergence of the finite element approximation to solution. Right, so the underlying convergence, the finite element method is bounded. It may not depend on it. The bound that Bob will prove has got the inverse constant in it. Discrete towards the continuum solution. Discrete solution towards. It's a great solution towards the continuous work. Yeah, if you wanted to do our estimation, then it's an important thing to know. And in fact, for designing a stopping criteria, this is the part of our work, you need an estimate of the influence constant. That relates residuals to energy norms of the solution. This is back to what John was talking about. You've got a bound on the residual who is guaranteed by minres. You know, you've got residual reduction. That doesn't tell you how close U is to UH or P is to P. Close u is to uh or p is to pH, the connection comes through the insert constant. That's 2D, and then 3D, interestingly, we can do the same thing, and now we get closer agreement. The pattern of convergence is also interesting because we see this thing doesn't really settle down here, but with the augmented approximation, we could have started here after 20 iterations and got the same answer on any of the grids. 3D, this is how it looks. This is how it looks, but we can't do very fine grids here. So we're constrained by Jen's confusing facilities in Glasgow. And so this is where we got to anyway. And what we see here is that we can estimate the incept constant on this grid here and get the same value as the estimation on the final grid here. This looks like this, this looks like this. Okay, so that's the stability issue. So we know that triangular P2P1 star is numerically stable. We also know the hexahedral Q2Q1 star is numerically stable. On a tetrahedra, P2P1 star is not stable. But there's a dimension change as we go from 2D to 3D. Only on a hexahedra can you do this augmentation. Length wide outside of this. Why outside of this? Okay, so now hidden question here: what's the point? Is it worth the effort? Other than my experience with my colleague from the Czech Republic who saved his postdoc, in fact his academic career. Then he went to another university with a permanent position. So this takes us to that era, that dark time in our lives, 2020. 20. I got completely, this is Morpho, following on from yesterday's discussion, I got completely disillusioned with research in the summer of 2020. I could not see the point of going to any conferences, listening to any online videos, writing any papers. My life, academic life, was kind of shattered by this experience. What I did decide to do was maybe I can practice. Do was maybe I can practice making videos. So I knew this was coming. So I wrote some videos about fluid mechanics. In fact, about 50. They're short things, 20 minutes. And in the process of writing these videos, I got myself re-energized about doing research again. And this is from one of the videos that I wrote. I was investigating the flow over a step even before, and I was interested in what different methods could What different methods could do in terms of a really important quantity of interest. And a really important quantity of interest is this quantity, the wall shear stress. And this is the region here where the fluid is coming from this direction. So this is important, for example, public blood flow. If you're worried about the force on the wall, this is the thing that determines it. So can we calculate this average wall shear stress in the Wall shear stress in the region where the flow is going from this direction. And part of the procedure was to calculate this change in sign and this change in sign. And I know that augmented tail hood will give me better estimates of these numbers than standard tail, a lot better estimates. I think how important conservation of mass is if you're trying to estimate this, where this recirculating eddy reassurance. Recirculating any reassurance. So I just said, okay, can I compute this solution? And I compared three methods. So if you want to know more, you have to watch the video. Okay, so it's out there on my web page. And this is the comparison, which is quite interesting. So there was a lot of hype over the last five years about the way you should solve fluid dynamics problems, and in particular, a big push towards. And in particular, a big push towards things called pressure-robust approximation. So, a pressure-robust approximation is point-wise divergence-free. Not just divergence-free on average. You insist that you really do have pointwise divergence-free approximation. For reasons I'm not going to go into, there's a Simon Review paper on this topic where the authors evangelicize this point, say we should all be doing it. And one such a All be doing it. And one such approximation is this one here in black. That's for Scott Pegilius. So I'm comparing something which isn't mass conserving at all with something which is mass conserving on average against Scott Legelius, which is point-wise mass conserving. There's pretty similar costs. I'm not going to. There's the same grid levels here. They've got different numbers of degrees of freedom, but let's call them the same costs. And these are the numbers. The same cost, and these are the numbers that is quantity of interest, like the thing I described on the previous page. So the Taylor Hood converges from below to this value. Scott Megalius converges from above, interestingly, and somehow, this wasn't any, I didn't fix this, but this method is kind of right just about here. And then it converges to this value. To this value. Well, this is pretty stimulating, right? There must be something in this. Well, it was during lockdown, so maybe I got carried away. So I started talking to Jen now about, let's think about the linear algebra. Now things start to get a bit hairy. So from here on in, the story is not necessarily good news. Obviously, where I need help from you guys. Okay, so Stokes. Okay, so Stokes flow is fine, okay? We want to do now do Stokes flow. Oh, and what I should have said is for these results, I'm doing backslash. Okay, 2D. So it's 2D calculation, backslash. And maybe taking a leaf out of John's book, I'm believing the numbers that I get rather than doing any er estimation. So backslash, the residuals are 10 to the minus 14. I don't actually know where that means the UH is the right numbers, but anyway, those are the numbers. Is the right numbers, but anyway, those are the numbers confused with backslash. But if we were in 3D, we wouldn't use backslash, we need to use solver, and we know how to do it. We wrote a book about this, for goodness sake. So let's take what we learned in 2005, all that expertise, and let's apply it to this. And I kind of knew that there's going to be a problem here. I was forewarned. Because this method suddenly commented Taylor Hood. The commented Taylor Hood got popular for a while, about ten years ago. And I went to some talks like at MathLab, and there was a German group who'd rediscovered it and they were using it. It's better than standard sailor hood. You should use this because these things reoccur, right? They didn't know that it had been invented some 15 years earlier. But anyway, and I talked to a postdoc in that meeting, and he said, I'm using, I think I went to his talk, and I went up to. I went to his talk and I went up to him at the end and said, Your method isn't really that new. You know, I have actually seen it before. And he said, Oh, that's good, you can help me because I can't get any linear solvers to work. I've been trying to use your methods that are in your book, and nothing seems to work. So I said, come to Manchester. And he came to Manchester for two weeks, and it still didn't work. So, what's the problem? Why can't it get it to work? Okay, so. Okay, so here's the Navy-Stokes problem now. So, all we do is change FA to F. Okay, so F is now convection diffusion frequency components. Everything else is the same. It's linearized, so we're doing Navier Sturpis is a non-linear thing, so we're going to have to do a fixed point association or something. So, it's linearized Navier Stokes. I call that OC. Alright, so we want to solve this system in this method. It's not symmetric, we're going to use GMRS, we like GMRS, but it's using. GMRS, everybody's been using it so far this week, no mention of any other non-symmetric distribution solver. Okay, so preconditioning, this is the standard strategy advocated by Alvin, Sylvester, Watham, Locktime, the preconditioning. So M is the approximation to F, VT is just a matrix vector multiplier when we'll do the inverse. Matrix vector multiplying when we'll do the inverse. So it's this triangle precondition. And then this is the key thing. This has to be spectrally equivalent to the shear complement, which is more complicated now, because the thing in the middle is a convective diffusion operator on a Laplacian. Okay, so that's what we're trying to do. So I'm not going outside the established framework in any way. And how do we do it? How do we do it? Well, one thing that does work, and I knew would work, Howard's staring at me, is just take MS to be scaled version of the mass matrix. So Howard's visit to Manchester, he talked about yesterday, was when this paper was written. So take the mass matrix, and what you can show, what's shown in the paper, is that you'll get convergence independence of the matrix. That you'll get convergence independence of the mesh size when you do this. As you find the mesh, the number of GM res iterations won't grow unboundedly. So it's robust with respect to the mesh size, but it's not robust with respect to the viscosity. There's no viscosity. Well, there is some viscosity, it's just scaling counts here. So as you increase the Reynolds number, the iteration counts grow. And that might, I'm using GMRAs after all, so you don't want them to be too many. However, this should work, okay? So that's, and it does work, okay. Okay, so that's and it does work, okay, as said on the tin. But when we were writing the book, we wanted to emphasise methods which are more robust with respect to viscosity parameters, that is the Reynolds. So we have two strategies in the book, which came from his papers. This is the first strategy, so and this really is Kay Logan, I would say, who are responsible for this. So we call this pressure convection diffusion. Diffusion. And we want to apply this pressure convection diffusion approximation to this two-field approximation. Now it's a bit complicated because you've got two fields to approximate. You need to construct operators which are pressure versions of the velocity convection diffusion operators and you apply these as a matrix multiply in the precondition step. In the preconditions there. So, this would be the Taylor Hood approximation, the continuous approximation of the convection diffusion operator. We're also going to need a constant approximation. We know how to do that by calculating jumps. So we can calculate these. This is the Q1, the Taylor Hood mass matrix. And then you want to connect them together. And you can't unravel this matrix. Bm minus 1 BT, that's the Laplacian. M is the belonging. M is the velocity mass matrix, not the pressure mass matrix. B is divergence, Bt is gradient, but this is a Laplacian, but we can't break it into pieces and we can't approximate it because of our mixture of B T. B T is kind of like a really ill-conditioned thing. So we've got to keep it all together. So, like my Q star had to have the off-diagonal blocks, you can't decouple this into a Laplace. Decouple this into a Laplacian for the vertices and the Laplacian for the centers. So you've got to keep it all together. Okay, so those are my kind of two methods. We have got a third method, which I won't dwell on, which is Howard's method, so we'll skip that. The whole story associated with it. Okay, so we won't go there. So we'll just compare these two simple methods. Okay, so we've got M1 is the So we've got M1 is the original Elm Sylvester with scaling, and M2 is the thing which I'm hoping will be much faster when the Reynolds number is large. And then you run your experiments. So this one is, so this is two grids. I'm trying to show mesh independence, convergence of GMRS. And what we see is this one here, magenta lines, they're about the same, right? So this is just a simple precondition. Simple precondition. So we can reduce the residual by almost two orders of magnitude in about 70 steps. That's exactly what you see if you're just doing P2P1 with this precondition. So that is kind of working. David, there is a stagnation going on there. I'm talking about this one. I'm talking ahead. Well, this is where you. I mean, this is ill-conditioning, right? This is ill-conditioning. And in my working life, this is really the worst I've ever experienced, ill-conditioning. It's hard to get to grips with what is actually going on here. But what's interesting is, here's the good news. So what you you're running this precondition and you're hoping it goes down like this, okay? You get convergence about thirty steps. You get convergence about 30 steps. That's what we get if you just did these two. Okay? But what happens, it starts out fine, and then, oh my god, it's not going to work, okay? But actually, if you refine the grid, you can get further. So one solution is if you don't, if this one doesn't converge on the coarser grid, maybe you can go to a finer grid and keep going. And we're talking about engineering tolerances here. Talking about engineering tolerances here. That's the other thing, right? So we're, you know, this is like five orders of magnitude, four orders of magnitude. You know, we typically start when you're out of 10 to the minus 6, okay? There is another pattern there, is that the beginning the blue curve goes like the magenta more and more as you refine the strength. So this means that you refine some more, the blue curve would follow more the magenta. Blue card would follow more the magenta one and take longer to. I don't think so. You mean just this just the startup? I don't think you're surprised to see. Nevertheless, you'll see a sharp drop-off. The next grid, it would come to do this and from right down to here. I'm not showing you the results, but you definitely get better results the final grid. So the ill conditioning seems to go away. Isn't this contradictory? As you refine the grid, you get better condition. Okay, so what can we do? Okay, so now it's up to David, that's me, to try and come up with a fix. So, this is my fix, which isn't very elaborate. So, we'll do a two-level pre-conditioning strategy. So, what we'll do is we'll start using, and I've tried many, many things. FaceOp, this is not my first attempt at fixing this. This is my last attempt. I've given up now. So, what we're going to do is we know that if we used is we know that if we used P2P1 using the PCD preconditioner, we get fast convergence. So why don't we start by solving this problem, columns that we want, and then use that as a starting guess for the more general problem, but then switch to the slower converging method. But that's what's described here. So you start by solving the system, which is the Taylor Hood system. Which is the Taylor Hood system to your tolerance. You've got a factor here, 10. So you solve using the fast method, and then you generate a refined initial guess. So we've got an approximation for the velocity, the linear pressure, and initial guess for the new pressure is zero, and just run it. Run it with the thing that we know is working. So we're going to run it with this method after that. After that. So, can you just remind us which one is which, which you moment? Because this magenta one is this one, which is mass matrix scaling. Slow, but steady. This one, the erric one, is PCD. Well, I'm quite impressed by these results, but probably you won't think. Okay, so here goes. This was the original. Okay, so here goes. This was the original result. Okay, this was using the standard mass matrix scale, the one I showed you before, the one which is making steady progress, and now we've got a new method. So, what we do is this is fast convergence. We're only using PCD on the Taylor Hood approximation. We get to here, and then we use this as a starting guess, which takes us back to here, and then we apply this method, and we get nice reduction to start with, and then it follows this. So, I kind of have a way of solving the problem. Kind of have a way of solving the problem to a reasonable accuracy, right? Or doesn't matter to digital production. Which is faster than this, because this would take this many iterations. As I say, at this point, we've got more questions than answers, but that's the story that we've had. So, what have we learned? Well, here are some lessons, in particular. Basically, this ill-conditioned operators need careful management. On my tombstone, maybe we've got some good news, okay, so we know how to solve these problems. So the problem where the student who came to Manchester couldn't do it, at least I solved his problems now. What I should say is that this isn't just P2, P1, but it's true the hierarchy of methods. It's a hierarchy of methods, you know, P3, P2. If you're using a P fabricator method, you typically use a sequence of Taylor Hood approximations because they're all stable. But maybe at the last step, you might want to make it mass-conserving. And then what's next? Well, this is something which reawakened my awareness of this relationship to continuous and discrete constants and then replacing sparse matrix solves. Sparse matrix solves. This is not so difficult to do. We just hadn't done it when I wrote the paper. But this is something which we're working on actually. So thank you all for listening. Yeah, I have a question. So I learned throughout the years that finite elements are not a good idea because the basis is I mean from the standpoint of having I mean, from the standpoint of having these bases, formal basis, frontal elements are not even worth you. So that's why I ask, I mean, why don't you re-orphogenalize your tiny basis functions? For instance, like slightly changing one of the spaces? You want the balance between the spaces are important, okay? Between the paces are important, okay, in terms of the overall level approximation. So, like, you want to balance like order H to some P, to achieve that, because you've got H1 velocities and L2 pressures. That gives you this requirement that this degree should be one order less than this. Okay? So that's kind of like, you don't really want to throw that away. This would be stable, but you wouldn't use it because you're wasting your time using P2 velocity. Unfortunately, P1, P0 is also unstable, so you're in this kind of trap. Unstable, so you're in this kind of trap, really. I really like P2 velocity though because it recovers pleasar flow exactly. And P-Star flow is important in pipes and channels, and there's no error when you compute P-star flow. If you use linear approximation, you don't get the right answer. You have to refine the grid to get P-star flow. Whereas here, it's built in. It's such a fundamental flow. I like the starting point for velocities to be P2 or Q2 or whatever you want it to be. But yeah, I mean, the question is that we had this very simple way of changing this space, okay? Which came from the fact that I need to think in a hurry when this guy came into my room and said, what can I do? Well, this was the answer. You could use a completely discontinuous approximation, P1, three degrees of freedom anywhere in the triangle, but then that's not stable with P2 anymore. Now you have to add double functions. Now you have to add double functions to the velocity approximation. So you start chasing your tail if you're not careful. So irresistible stability is this very delicate mix between these two approximations. But maybe I can... So long. I wouldn't change the spaces. So you keep the velocity space, you keep the pressure space, you just choose basis functions for the pressure space. But I just think that a way that's I actually think that I mean, but a way that's that's well cond you see this is a well an ill-conditioned basis for that space. What we want is you s I'm sure you're right, we want a well-conditioned basis for this space. But it's a replicated space. Okay, that's the struggles. Let's say everything is replicated twice. All functions you can represent in two different ways. Okay, the term is conditioned operators part. The term if condition operators puzzles me a little bit, because I think there is something else going on, just not the condition number of the matrix, which has nothing to do with the mask. So I was wondering, maybe the killing reconditioner, the one that kills the first part, is capturing part of the spectral properties of the matrix and then doesn't do anything to the rest which the other recognition. Which the other preconditioner takes care of. Do you have spectral results for these preconditioners? Non-symmetric regime anyway. I mean it's assuming that the eigenvectors don't play a role. So do you have results on the spectrum of the precondition model? That would be interesting to see where the new precondition is. Where the new recognitioner acts, I don't have results, but the original methodology did have results, which wasn't specific for P2, P1. So what I mean by the classical preconditioner operates on the whole spectrum, the new preconditioner seems to operate on part of the spectrum very well. What do you mean the new preconditioner? The new split. Preconditioner that split the one that goes down, but that's not new. This is TCD applied to a different star. M2 star. M2 star is what I call the new recognition, compared to M1. M1 is the classical. Yeah, okay, sorry, yeah. M2 is the beloved, as you put the heart, is beloved. Yeah, but this is really just this with a different start of guess. I understand. Go to the previous picture, please. Here. So the blue curve tells me that the below precondition M2 acts on part of the spectrum very well and then doesn't do anything else. That seems reasonable. So that's why I was asking whether you have spectral analysis for an That would be nice. That would be nice. I mean, there is analysis of this preconditioner, okay, which is general for all stable approximations. But there are assumptions made in that. M2 or M1? Both. But M2 is fine, okay? So M1 is fine, okay? I showed. But M2, well, the analysis is a bit flaky, I would say. Not completely understood about the convergence. About the convergence of PCD? I mean, Howard might want to make a comment. Well, the results for the violet one say that the convergence is end of the mesh and iterations grow like one over the viscosity, or the Reynolds meaning. The results for the blue one are not complete. They suggest that the growth is like the square root of the rounds. Is like the square root of the round summer, but it's not full. It's incomplete. That's what we know about it. So, probably the incompleteness is related to the behavior. I mean, what you know probably is about the nice part of convergence, but then maybe something missing. I'm just guessing the first time I see the picture. I mean, I know a little bit about this fast and then slower convergence. The fast convergence. Convergence. The fast convergence is when it's resolving the eigenvalues that are sitting in a fairly tight region, but there are a handful of outliers, and it takes more time to resolve those outliers, and that's what's happening in this plateau. That's empirical results. Okay, we know it from experiment. What I was asking whether your precondition may Your precondition matrix can be analyzed in terms of spectrum to see whether there is a special distribution of the eigenvalues of the precondition matrix. This was my question. And we do have some not a complete analysis. Okay, some analysis, but it's not complete. This is a paid by the three of us. Formal questions? In the stirred tank reactor, you said the liquid started going out the bottom. Were they going out the bottom at the right rate? Yeah, it was completely close to experiment. The problem was solved overnight, as it were. It was like a miracle. It really was. I can't emphasise enough. I mean, like, he bought me a bottle of. He bought me a bottle of whiskey, I think, the next day or something. And then we kept meeting in conferences and he kept coming up to me and buying me drinks. Okay, let's thank David again.