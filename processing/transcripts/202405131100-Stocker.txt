 Uh embedded text method. Uh we are starting with our DG method and uh to do text detection. And to do TrefCG, we're looking for at least a finite dimensional subset of our kernel of the operator that has optimal approximation properties with much fewer degrees of freedom than the full polynomial space. Just as a standard example that I'm thinking of is, for example, Laplace equation, our standard in zero penalty form. And as a basis functions, instead of the full And as the basis functions, instead of the full polynomials, we would consider the harmonic polynomials. And the first goal in my talk is now that instead of going into our DG code and having to implement this set of harmonic polynomials in order to use as a basis function, we want to find it as a subset of our polynomial space and use an embedding to construct those. This is the main idea of the embedded draft method that I want to present now. So, as I was saying, we want our draft spaces. We can think of Draft spaces, we can think of only polynomials, but in fact, the general draft spaces, and we want to represent it in the terms of a standard DG basis, so the full polynomial space. And we want to do this via an embedding operator T. This will map our full polynomials to our test space. And then instead of solving our classic DG scheme with our matrix, our sparse matrix A, we will condense this matrix down to the traffic space by. Down to the draft space by the embedding, and then solving the scheme only on the few degrees of freedom of the draft space, right? Okay, so how do we find this embedding? So, this is the crucial idea on how to find the embedding. We want our operator to be zero. This is, of course, equivalent with the test and trial functions. That is a strong operator, think of Plasma, is zero for all our test functions. So we can construct a matrix W with the polymers as test and file functions. And then we will simply look for the kernel of this matrix and of course include element wise. Since our basis functions in the beach method are defined element wise, this operator parallel also on small local matrices. And we can do this via singular value. Using a SPD, we can look at the singular decomposition of this small element matrix, choose our zero singular values, and then our embedding matrix will be simply the The matrix corresponding to the singular values mapping onto this subspace, right? And as I was saying, this is done element-wise, and we can construct this. The pool operator is the blocked eigen matrix that maps down to the path. Okay, as a quick numeric example, we will stick to the Laplace for the time. And comparing here the pool polynomials, the actual implemented harmonic polynomials, depth space, and the embedded left space, we can see we have optimized quite replace all the three methods with the embedded left method as the harmonic polynomial. The harmonic polynomials. So, in terms of P and measuring the degrees of freedom here on the axis end of, we can see that we are using much fewer degrees of freedom than the full polynomial. Okay, so this was the general idea. So far, we have only circumvented the construction of the harmonic polynomial. What else can we do? Now, we already have the singular valid composition on one element, right? On each of the elements. And we can also use this to And we can also use this to construct a particular solution. So consider we have a problem with the right-hand side F. Then we can use the upper part of this singular valid composition to construct a particular solution using the pseudo-inverse. Sorry, UHF. UHF would be my particular solution. And with this, I can homogenize my system, right? I plug this in, subtract from the right-hand side, and then I can solve again on each element homogeneous with my trash spaces function. So this way. Spaces functions. So, this way I can treat problems with the right-hand side with this method. So, an example. Again, Roissau equation. And of course, now I cannot compare to a left space anymore. So, we are only looking to compare the full polynomial method to the embedded left method. And we can see that we have optimal convergence rates. And the local degrees of freedom of the system that we solve is again the one of the harmonic polynomials. Polynomials. Okay, so this was the first step for the right-hand side. The next thing that we want to look at is what happens if our draft space is not a subset of the polynomials, right? Yes. Oh, okay. Okay. For participants online, I know that there's a problem with sharing the screens. So we're going to. So we're going to take a quick break to change the computers.    The slide will be open on the random stuff, running the slides. Students read out and follow up on comment.   You just have some random amount of time. At least on my time. So people are like, can hear me? Ah, the audio was okay. Okay, great. Okay, wonderful. We are just getting back to where we left off and we will get restarted. Left off, and we will get restarted. At this point, this one is yeah, I use this. Yeah, okay. Uh yeah, okay, thanks. So, yes, yes, I will move the image, yes. Okay, so yes, okay, we get back into it. So we have seen we can find the harmonic polynomials inside the polynomials using the embedding, and we can treat the right-hand side by constructing a particular solution element by. Okay, the next part of the talk, I want to address the problem with our The problem is if our reft space is not polynomial, so if we cannot find it as a subset of the polynomial function, this is, of course, the case, for example, for Helmholtz, where we know we have the frame waves, or if we have a varying coefficient inside our equation. The idea to treat this problem is to relax our condition that we have on our embedding. So instead of using a full draft space, we introduce this kind of weaker condition. We introduce this kind of weaker condition inside our draft space. So, let me start with the second point here, actually, which is introducing enforcing that our solution has to be zero with the operator L, instead of that, we want this only to be true under a projection, a projection onto a smaller polynomial subspace. So, what we want to do is instead of using LULD equals zero for all d in our polynomial space, we use the relaxed condition where we simply put the projection inside here. Put the projection inside here, meaning, of course, that we can test with the space W that we are projecting onto, right? So the new condition to construct our matrix W and find the kernel is L U tested with W inside a smaller space WH. We can find again that the same, so we can recover our previous embeddings. So for example, if you think Laplace, it's of course equivalent to take a double. To take a WH as the plus of the polynomials of P, which are polynomials of P minus 2, meaning that this is exactly equivalent. But in the case where we, for example, have a varying coefficient inside here, this will give us something new. Okay, with this, we proceed as before. We construct this matrix W element-wise and find the kernel, and this will give us again our embedding. How does this work? So, the first test case here is Helmholtz. So, of course, as you all know, for Helmholtz, our standard For Helmholtz, our standard left space is made up by these plane wave functions, for example, which are non-polynomials, so these exponential functions here. And now I'm comparing this to the embedded method where we require that the operator projected onto the polynomials of p minus two has to be zero. So here I have to admit, so the theory is not yet complete. And this is the guess that gives us still best of... The gas that gives us still best approximation properties, and the same order of reduction as the plane waves is this projection to p minus two. Okay, this is the one that works. The idea is that the highest order of the operator is the De Laplacian. So this is why we want to use something similar as we did for the harmonic polynomial. Okay, so let's have a look at the numerical results, which are kind of surprising to me. So we are comparing again to the pool polynomials, the TG method pool polynomials, TrefCG method with a TrevDG method with the plane waves and the embedded draft one. And the first surprising thing that we observe is for all the four and five, for example, so we have optimal convergence rates, but the error of the embeddings, of the embedded left space in orange, matches the error of the flame waves instead of the error of the polynomials. So, by a constant factor, we gain some sort of stability, even though we are on a subset of the full polynomials. As you can see here. So, yes, we. As you can see here, so yes, we get some stability property from the plane waves via this embedding. Then on the right-hand side, I'm plotting the arrow with respect to the degrees of freedom or with respect to the key polynomial degree or number of plane waves. And we can see here, so we again have the same error as the plane waves, and we're going down nicely. Of course, the plane waves at some point or another, I didn't implement anything fancy, but still they will ill condition. So of course you can do many small. So of course you can do many smart things and have some preconditionals. I did not. I simply used the vanilla plane waves. And we can see here that the plane waves show some ill conditioning, so the error starts to go up again. While for the embedded theraft space, it's still a subspace of the polynomials. And in fact, we simply continue to converge. And I can tell you something about the conditioning of our space. Since the embedding matrices we are getting by an orthogonal decomposition, we know that the conditioning of our embedded REFS method, so the embedding applied to our system matrix. Um, embedding applied to our system matrix will be bounded since these matrices are orthogonal by the conditioning of our polynomials. So, if we have well-conditioned polynomials, also our embedded refts method will be well-conditioned. Okay, so now we get to the nice pictures of embedded refts. Here is another example. Again, with this projection, now we have the first-order operator, and the projection that I use here is onto the problems of p minus one, since our operator is first-order. I'm looking at this transport operator. This is the transport operator with a flow field B that is non-constant inside the element. And I just wanted to show some nice pictures here of the flow field curving inside the element and plotting the spaces functions that we achieve after the embedding. And you can nicely see that they try to, of course, it will be not exact, but they try to be constant along these flow lines of our field so that they satisfy this stress condition. Okay. Yet another example, the acoustic wave. Uh, yet another example, the acoustic wave equation, uh, with a varying coefficient here. Again, I'm comparing to full polynomials and then also to the quasitreft space. Uh, quasar draft, another method of treating varying coefficients. I will not talk more about these because there are many great talks ahead of us that will explain this. But we can see that we have again optimal convergence rate with also the embedded raft method. And here I show some comparison of the runtime. And we have a yes, we have. Have a yes, we are close to the runtime of the Decositre space. Of course, we have much few degrees of freedom, and the polynomial space takes long. Okay, um, yes, I will leave you with some analytical results. So, we have a CS lemma, TRS-type lemma for our embedded left method. If our original PDE problem is coercive and continuous, then we can get this at CS Lemma. So, you can see we have this infrom or the full penombins with this draft condition of our space. With this draft condition of our space. So, first, the draft condition on the left-hand side for the weak draft space, we can put this projection that I was talking about, and similar for the right-hand side. So, we can treat these inhomogeneous problems. Okay, finally, I also want to compare some in a very good way some algorithmic complexity between these methods. So, I'm comparing here to the standard DG, an actual DreftDG method where we An actual Dreft VG method where we implement the basic functions, hard code them, the embedded REFC method, and also hype REFCG. And here you can see, for example, that the total degrees of freedom that we have to store, since we are still in a polynomial setting, are still the size of the full polynomial case of the DT functions, which we can only reduce by using dread functions, of course. Only there we are fully working only on the pref. This similar is similar with the HTTP method where we still have to store. HT method where we still have to store also the volume degrees of freedom and not only the faster degrees of freedom that we use in solving the system after static connotation. So for the setup time, of course, again, the Drefts method has a smaller setup time as we are only setting up the degrees of freedom for the Drefts method. Here we still have to kind of construct the, well, part of the full matrix at least. And then we have this additional time of solving these local problems. Solving these local problems with the singularity composition. This is similar to HG, where we have to use static condensation, right? And the timing is similar. Now, for the final solve of the linear system, we only consider the global degrees of freedom here. And here, the thing that I want to point out, of course, the draft, the embedded and HET all have a similar order of reduction in our degrees of freedom for the global system. But the HET method will scale with the. But the HTT method will scale with the facets in our element, while the other ones have 30 degrees of freedom inside the elements, right? So, like in the DT method. Okay, so with this, I come to my conclusion. We've seen a new embedded REFTS method, so we can construct these test and trial spaces using a projection that will give us some drafts-like properties. And it works for homogeneous PDEs and non-constant coefficients. We have extended or applied this. Extended or applied this method to several other problems. And you will hear the next speaker, Ego, will present this part of this for the Stokes problem. And then Christoph tomorrow, I think, will also talk about this paper that we have to get on unfiltered method for the draft VTMA. Okay, this is the reference for the embedded left and for the code. The code is online and you can test it out and you can test it out also tomorrow in the workshop that we have together. Please bring a laptop and Please please bring a laptop and I guess I hope we can manage to test it out together. Okay, thank you. Thank you, Paul. So do we have any questions in the room or online? There's people online that would like to hear your question. So here might be when you compare that like yes standard I mean full polynomials. The full polynomials based on each element. Of course the so the spaces in this sense are kind of independent but Is in this sense kind of independent, but yes, I was using a symmetric interior penalty. Yeah, but for the embedded drafts, you could do also some altruist method, for example. Yeah, yeah, this is the slide on the yeah, this is a refinement with the LTO error. Yeah, and you can, yes. Error, yeah, and you can, yes, uh, yes, that we have the same approximation uh properties, so same convergence rate, same convergence rate, yeah. Thank you, What do you mean by that? Now you have a question. Does it mean that they are different? Or varying globally? Globally, but inside the element probably I need some sort of regularity. So I wouldn't want them to jump inside an element. Yeah. Why did the rows to jump in there? We have to do it Yes, so the um the difference the difference to the main trafts method was is similar also to this quasi-thraft business, so that we want to not have this strong condition of the operator of our function to be zero, which we cannot satisfy with the polynomials. So we are relaxing the condition with this projection. This is the main difference, and yes, yes, this is the main difference. The yes, this is the this is the main difference, and then we can observe again the same out of reduction that we would, for example, if you look at the Laplace varying coefficients, you get the same out of reduction as harmonic polynomials. You get the, yes, the harmonic polynomial-like basis functions, but they satisfy the weaker condition with this projection inside. Yes, degrees of freedom is the same, yes. But the theory here is lacking on the best approximation estimates. Yes, but we observe it. Yes, but we observe it. Helmholtz said did not try. I'm sorry to say. But we can do it tomorrow in the workshop. Yes, but I expect something similar for the acoustic wave that I tried with the Erin Coefficient. I think for the Helmos, it was already, the challenge I had in mind was already that the standard space is not polynomial. Regarding the acoustic wave equation, how do you deal with the time variago? Is the time variable? Sorry, yes. This is the space-time method that you're well familiar with, like the elastic paper. I really solve the chart of the no, no, on this on this example, no. I really solved the Solved the implicitly the whole time slab. Yeah, it's fully space-time method. Yeah. Okay, we have a question online from Ralph. Do you want to go ahead, Ralph? Yes, sir. I would like to ask a question, but I could not understand the previous questions. Maybe it has already been asked. No worries. No worries. Oh, can you hear me? Yes. All right, good. When you have a translation invariant differential operator, can you compute the embedded draft space once and for all and then use it for all elements? Yes. Yes, I think so. Yes. Yes. No, no, a translation invariant operator. No, this is what he said. Yes. Okay, so this could save a lot of work then. Yes, this could also, yes, it would. Yeah. All right, I was mainly asking this question. The work of constructing this embedding is not what is the most expensive in the sense that we can do it in parallel also. So it will save some work for sure. But yeah. For sure, but yeah, since inverting the full system is the most expensive, yeah, but it could, yes, I agree. Okay, thank you. I just wanted to make sure that I got the main points correctly. Thank you. Thank you. Okay, do we have any? Yes? This live, but I guess we have to move the video because I wanted to see the comparison with HDG. Uh, it seems that it's equivalent to HDG, but it scales better. What yes, an excellent question. If I could, no, I cannot move the slides, but I have backup slides of numbers. Yeah, cool. Just, yes. Backup, backup, backup. Yes. Okay, cool. Yes, okay, cool. Comparison. Yes, so here is, I'm just showing some. We recently computed some explicit numbers for this comparison. Here was the comparing to a virtual element method. And yes, so the order of reduction, this was the main message of the first, that is comparable. Then on, for example, a triangle mesh, I'm considering a periodic triangle mesh. We can see, okay, here are the different methods. I'm comparing two. Methods. I'm comparing to HG condensed, much lament methods condensed, and different order of polynomials. And for the simple, I'm considering two edge cases. I want to see only triangles, so very small amount of facets inside the space compared to the number of elements. And here, for the standard graphs, let's say a monopoly normal, it will be hard to beat the H3 or the virtual element method. The HHC or the virtual element method. Here we are not even comparing to HO, for example. So we can see that yes, we scale by factor we scale a worse than these methods. And here are the number of non-zero entries in the system matrix where we can also see that we are not beating them. Interestingly enough, of course, if you consider a first-order operator, this is what I mean by PDP2 here is the second order operator. The first order operator is, of course, a much smaller kernel. And there we will gain a lot. But yes, so for the standard example of the second order operator, For the standard example of the second operator, we will think we are lacking behind this example. But now to consider the other edge case, a mesh of a truncated optohedron mesh where we have a lot of faces per element, you can see that the numbers change completely. So as we are only scaling with the volumes inside the other two methods, degrees of freedom on the faces and even edges of the mesh, we can see that the degrees of freedom here four times. Four times or yes, of hours in the fixed, for example, and here the number of non-zero entries in the matrix is in our favor, I would say. Yeah. Okay, so before we thank Paul again, I want to remind you that in our program, we have two activities that are going to be led by Paul. One is going to be tonight, and it's going to be about NGSOL, the code that Peter mentioned earlier, in general. And then the next one that they're going to be tomorrow, probably, is. Tomorrow, probably is going to be more focused about codes that Paul has developed for interest. Test 23, can the online world hear me? Yes. Thank you very much. It's this one called Talk E5 to me. Coke anybody. Wonderful. See the slides, they can hear me, but more is there if you wish. Can you click on the slides, please? With the mouse, when you click on the slides, is working everywhere? Perfect. It's working everywhere. Perfect. We're set to go. All right. So we're now set up for the next talk. So, our next speaker is Igor Wulis from GÃ¶ttingen, who's obviously from the same collaboration group. And so, Igor is going to tell us about stocks products. Yes, thank you very much for having me.