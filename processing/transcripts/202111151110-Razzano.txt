Okay, hello everyone. I guess you can hear me. Maybe I'll stop the video because I'm home, so I don't want to have a slow connection. So okay, just a second. Okay, so good afternoon, good morning to you. Thanks for the invitation. Uh, thanks for the invitation. I'm very sorry that I couldn't be there in person, uh, but today I try to do my best to give you an overview about some possible methods related to how we can use machine learning to investigate the noise, particular glitches in our interferometers. Um, so clearly, first of all, we all know that we are in a very exciting era and we have a bunch of data coming. And we have a bunch of data coming, and we have close first three first runs, so one or two or three. And we are clearly looking forward to four. And we would have a much larger network, but also we would have Cagra and in the future we will be RIGOIndia. So we are all very enthusiastic and very happy to get this data. But of course, one important problem is try to understand the noise in this detection. The noise in these detectors. And clearly, one of the most important milestones in this era is the beginning of the multi-messenger astronomy down with the gravitational wave. So you see also these things in previous talk. So I will go super fast about this. The key point, takeaway message is that it's very important to have fast alert and it's very important to have a fast detector characterization. This is extremely important to Important for the whole chain of the electromagnetic follow-up. So, when we are talking about big data, it's interesting to compare this with the whole context of the big data in the world. So, I got this slide, this picture that gives you what happens every single minute. This is updated to July 2021, and you can see that there's a bunch of data flowing in from Facebook and YouTube. You know, Facebook, YouTube, Zoom, etc. We are all generating data every minute, every second, and clearly, machine learning is one of the most important way to tackle this big data. When we're talking about big data in astronomy, we are talking about not so big data, but still it's huge because here you can see some numbers. You can see the Sloan Digital Sky Survey. The Sloan Dishon Sky Survey, it's 50 terabytes. But if we're looking into the future, the near future, if you look at the LST, we're talking about petabytes. When we're talking about SKA, it's even more because of course the radius survey are producing a huge amount of data. And of course, all the community, including our community, is moving toward a more systematic approach to analyzing all these data. This data. In our field, when we are talking about this big data and noise, mostly noise means order of 0.5 terabyte per day that are collected from thousand channels that are monitoring the detector. Every single part of the detector subsystem is monitored by sensors. And basically, most of these things, it's noise. So the biggest amount. The most biggest amount of data is just noise, but we want to be sure that we know this noise and we can remove this noise and we can get the signals that we want. So during these days, you will see many different approaches related to the low latency analysis, detector control. I'm going to talk a bit more about the detector characterization. And of course, machine learning can be also used on longer time scales when you want to look for new sources. Scales when you want to look for new sources, when you want to go for multi-messenger analysis and combining signals coming from different detectors, not just gravitational waves. So Elena will talk about this in the next day. So there will be very different approaches using machine learning. So why machine learning and deep learning is extremely interesting for us, of course, it's a frontier of data analysis because. here of data analysis because we have all these important all this this network of layers of perceptron, the single unit, the single artificial neuron. And of course, the key point is that if you build an extremely complex network, you can basically do whatever you want. You can approximate very complicated formulas. So you can start from an image, you can start. Start from an image, you can start from a time series, and you can get and you can extract information out of this data. This is an example of a deep layer. So, why this is important for the characterization and classification? So, we are limited by stationary noise, stationary noise, and in particular, all the transient noise events that record glitches and impacts on data quality and can mimic the real astrophysics. And it can mimic the real astrophysical signals. So it's important to understand where this glitch comes from and understand their morphology. And of course, in some cases, this is not easy because they can come from different parts of the instrument. So this is sort of one of the first topic where people started using machine learning. So there's a bunch of works in the literature out there. And of course, you can find You can find a lot of papers tackling this problem from different points of view, but still, there is a lot of work that we can do in this field. When we are talking about big data, when we're talking about machine learning, we cannot ignore this very important project, which is Gravity SPY. Because every time you want to classify data, you want to train your machine learning algorithm, you need to have labeled data. Of data in the case, you want to use the supervised learning approach. So that's basically what Gravity Spy is doing. So thanks to Gravity Spy, we have a bunch of people, volunteers that are looking at this data and they are classifying data. So we can use this output classification and we can use to train our machine learning algorithms. I will go back in a while in In a while, in citizen science, in the citizen science approach, because we've been working on another project that is complementary to gravity spy, and I will discuss about this later. So when we are talking about morphology, clearly, this is how glitches look like. So this is a blip. You can see other morphologies like the high-frequency whistle glitches. And as you can see, and as you know, the main way of The main way of representing these things is using time frequency representation. So, you can use, for instance, a Q-transform, you can use just FFT pure spectrograms. And in some cases, it's even more complex than morphology. You can see the one called Koi Fish, you can see the elixir. And in some cases, you can see that some of these glitches are repeating. So, it's also important to understand how many. And how many times these glitches are repeating, and if they change in time, they change their shape, etc. So, what we've been doing, of course, is using a convolutional network. There are many groups working on that. So, I will give you some examples just to give you an example of how powerful can be these techniques. And in Leiden Virgo, we are using many groups are using. Using that many groups are using this approach. Of course, as I was mentioning before, deep learning gave us the possibility of basically classification of, for instance, a time series, of an images, and basically any arbitrary complex formula. So in our case, this formula can basically go from the gravitational weight data to a class of glitches. A class of glitches. We would like to use images because it's sort of an obvious choice because these are easy to train to build a training data set because you can just look at morphology, you can spot images, and you can see the shape of this stuff. And of course, you can also compress long data stream. And of course, you need to do some pre-processing. And of course, after that, you can use machine learning and a convolutional network to extract features. To extract features. So we therefore started using CNN2D, which is basically a set of neural networks used to work on images, extract features, classify images, etc. And I will show we also using CNN1G as well. As happens in this sort of stuff, you need to start from some simulation. So we prepared a basic simulation. A basic basic simulator that can basically give us a possibility of simulating color noise. And you can, we prepared a set of sort of standard simulation with sort of a standard rich shape. So similar to blips, similar to the scatter lights, as you can see in this case. And once we have done this, you can build images. You can do whatever you want. You can build spectrograms, like in this example. Spectrograms, like in this example, or you can build Q-transforms. I will show you other examples later on. In this case, we decided to use two seconds because in this case, you can get the shorter glitches like blip, but you can also get the longer ones. And you can even get some sort of transient signal that resembles a chirp. So you can also test how the Also, test how these algorithms are robust against the detection of a real signal versus, of course, a noise. You can apply different image analysis techniques. You can do, of course, you need to start with the sort of widening and then you can do contrast analysis. And just to make these things available, if anybody's interested, all this data, this simulated data that we All this data, this simulated data that we prepared a few years ago are now on Fixhare. So you can go there, you can download these images. If you want to use these to train your own network, you can also get the original time series. And these are available on Fix Chairs. Okay, so from this, of course, this is an example of the architecture that you can build. What you can get, you can basically What you can get, you can basically start from gravitational wave data, and then you can do a sort of, you can select the time series related to your glitch, so related to the trigger you want to analyze, and then you do some standard pre-processing. You can reduce the image size in order to avoid using too much memory in your system. And of course, one of the typical The typical structure that you can use, you can put everything into an HDF5 file. The classification works in the sense that it gives the probability for each image to belong to each class. And of course, you can take the maximum or you can decide to put a threshold and then in this way select the purity of your classification. Usually we also add a noise, which basically means no glitches. Means no glitches, also to test how robust is your code to detect actual glitch. So you're doing basically anomaly detection in this way. And here I will show you some examples of some tests related to different sort of architecture, like one block, convolutional network, three blocks, or standard shadow. And of course, the nice part is that you can develop everything on path. Can develop everything on Python thanks to all the various libraries, including TensorFlow, Keras, etc. And we did all these tests using standard GPUs from NVIDIA. So we had a bunch of different cards that we're using to do our tests. Okay, so this is a standard training and evolution curve. This comes from a paper that we have written with ANA a few years ago. Here you can see the performance. Here you can see the performance compared between the shallow network, deeper network, and this compared to a support vector machine. And you can see that, of course, the CNN is performing extremely well. You can see all the relevant figures here. Of course, you can also use confusion metrics and you can see that, of course, this network is performing very well. Performing very well is, of course, is much better in some cases, especially when you have images that have a very similar morphology. The CNN is working slightly better in distinguishing different morphologies. So like, I don't know, in this case, like Gussian and Syngashan, for example. And of course, one of the most interesting things about CNN is that it can extract features. It can extract features and they are basically independent on the position of the feature. So, if your glitch is in the center or it's not in the center, they can pick up the right class of your glitches. So, this is an example of a case where we put together some glitches that are very close, just to test how robust they are in distinguishing an image with different sorts of glitches. And this worked pretty fine. Pretty fun. Of course, from the very first test, we went on. So we started using a more realistic data set. So instead of just using the simulations, we wanted to test how well we can classify data. So we used the Gravity SPY database levels, and this gave results that are consistent with what was found in. Consistent, which was found by the gravity spy team. After that, we wanted to go deeper into exploring how these samples are pure. So we can do a cross-match with the results coming from the database of Rabbit Spy. And this was particularly interesting for the Virgo side rather than LIGO because Rather than LIGO, because the glitches can be slightly different. So, we wanted to explore how well we could classify Virgo data using as a starting point the LIGO data. And here you can see an example of glitches that have been classified, like scatter lights. And you can see that these are more or less very, very similar. When you're doing the class of the repeating blips, Repeating blips, it's of course depending on how much they are repeating. So it's nice to see the variety between the among the same class. So in this way, you can go deeper in investigating the different class. Maybe you can introduce subclasses if you want. And clearly, going on preparing for a tree. So from one and two, we wanted to go, we wanted to use this stuff for classification. We wanted to use this stuff for classification of a tree. And this also was very interesting to check the purity of the sample. So, how well this pipeline that we had classified these glitches. And of course, you can see that in many cases like BLIP, it's of course extremely easy. Also, because blips are the most frequent glitches, so it's much easier to spot and to classify these. To spot and to classify these sort of glitches. And we were also interested to see and to explore what happens in the none of the above. Again, this was mostly to understand if we can see the different subclasses in verbal ages also. And here you can see that the one that were labeled as none of the above within these subclasses by classification of these classes, we can find some. Of these classes, we can find some sub-virupes that could be interesting to go and look deeper to see that maybe in some cases something that was classified as none of the above was scratchy or something else. And this happens mostly also because the initial precipitation from the citizen scientists could raise some uncertainty, et cetera. So this is something that could be interesting to explore as well. That could be interesting to explore as well. We also try to see if we could use the CNN1D, and this is also the other standard approach that is used in this sort of analysis. Of course, you have some advantages here. The first one is that you don't have to build tons and tons of images, so you can, of course, save space. Um and then yeah, uh okay, I see uh a question, so I will go and answer after I finish this. Of course, the problem is that it's not as easy to spot the morphologies by eye. So you need to start from some classification that is done somewhere else. So we did some some tests by doing some some deeper Some deeper CNN1D, the performance were very good, and what we are doing is integrating with the CNN2D. Of course, the issue here is that we want to have something that in this way can compare results from the CNN1DN2D. And this was, by the way, was a content on a master thesis that we have of students, Jacobo Talpini at the University of Milan, that worked. University of Milan to work with our team in Virgo. So, okay, so yeah, I see a question here about the problem of overfitting. Well, it depends because we have a bunch of data. So what we are doing, we are basically what we are doing here is to do different approaches. One is to have some data augmentation also. And so far, And so far, there is no, we haven't seen so much overfitting. We can keep this in control, of course, by using the standard techniques of dropout, etc. Okay, so I'd like to go on after I presented the standard approaches that are being used by our group, but not just our group. Group, but not just our group because different groups are using a similar approach in like Virgo. I'd like to go back to the citizen science because, uh, in this way, this gives me the opportunity to introduce a new project that we are working on. As I was mentioning, citizen science has been considered a very valid support to prepare a set of label data sets in the classification of noise. In the classification of noise features, and in particular in glitches. So, here you can on the right, you can see a sort of supervised learning for dummies. And of course, the gravity spy story showed that citizen scientists can help a lot. And thanks to, I will give you some figures later on. So, the gravity spy teams demonstrated clearly that thanks to the help of these volunteers, we can do a lot of stuff. We can do a lot of stuff and we can train better models thanks to them. And so recently, in this last couple of years, we started working on a project, on a European project called Rainforest, which is being published in a call within the Horizon 2020 European program. And the idea is to bring together scientists working on different fields of physics. Fields of physics in order to build novel citizen science projects with the ultimate goal of engaging 100,000 citizens in projects related to the large research infrastructures in Europe. So there is Virgo, there is also LHC, there is KM Trinidad for Neutrinos, and this project has been led by Sabros Carzanevas, the director of EGO. We wanted to, what we What we want to propose and to reinforce is not just the development of citizen science projects, but also to provide a context that support and improve the engagement of people. So together with this project, this citizen science project comes also a set of activities related to studying how to improve the engagement of different target groups. So for instance, So for instance, also doing some target activities related to students, to senior citizen scientists, and also to people that have some impairments. So for instance, we have a bunch of activities that are related to sonification and how to use sonification methodologies to increase the senses and increase the inclusions. So here you can see that these Here you can see the institutions that are involved in this project. And the reinforced project is focused around four demonstrators, we call demonstrators, the citizen science projects. We have one related to gravitational waves. And of course, among these four other projects, we are the luckiest one because we have more experience than gravity spy. The other three are one related to Are one related to neutrinos, so we call deep sea enters. Then there is another project related to the search for new particles at the LEC. And then there is a project related to the muon tomography, so cosmic muon images. And as I was mentioning, our gravitational wave package is involving different institutions, including, of course, University of Pisa, IEGO, Oxford. Oxford Open University. And this has been, all these projects are being developed in close collaboration with the universe, which is the leading platform for citizen science. And we want to improve the engagement of citizens that already knows the gravitational waves thanks to gravity spy. And we wanted to support also this activity and have more and more people involved in this. More people involved in this activity, and as it happens also with gravity spy, the outputs will be used for training machine learning models. Just to give an example, in these last weeks, one of these projects was launched officially, the project related to new particle search at CERN. You might notice that the name slightly changed. Name slightly changed because, of course, the one before were the initial names when we initially propose this project. Now, of course, these are the final names. For instance, in this case, citizen scientists are asked to look for vertex and to look for the signatures of new particles and at the LAC data, in particular with ALOS. And you can see the screenshot of the home page of the Of the LIC project. And then after that, I'd like to show you what we're doing with gravitational waves. We are also, we have been preparing our own demonstrator. So we've been doing data selection, data preparation. We've been developing a set of pipelines to do the analysis of these data that are flowing. And of course, we have been developing the whole Developing the whole universe demonstrator. Here you can see the people involved in the work package three, which is gravitational waves. And here is the output. So I'm very happy to introduce our new project, which is called Witch Hunters. And here you can see the URL. So as I was mentioning, now I will shortly describe what people will be doing within this project. Doing within this project. The project is already available, it's still in VITA, in VITA review, but we are expecting to officially launch it very soon. And then I'd like to give you an overview about what we are doing within this project and what are the results that we are obtained so far. So the idea is to try to offer a new way for engaging people in gravitational wave science with citizen science. Science to with citizen science. We know from Gravity Spy that this can be extremely successful. We have, for instance, I grabbed some numbers before. So we have for Gravity SPY, there are 26,000 people, more than 5 million classifications. So we ask ourselves, can we complement this approach? Because it's very hard to do better than this. So can we do something that can be complementary to this? So we started exploring. So we started exploring new frontiers, so not just glitches. And also, we wanted to decided to include the signals coming from the sensor in the detector, so the auxiliary channels. So not just showing the main and the strained data, but also having other auxiliar channels. And also, we developed this sort of a project also to be run on mobile devices as well, because the universe has as a Universe has an app for smartphones. So we developed some specific set of tasks that you can run on your smartphone if you want. So this is how Google Hunters is organized. You can see there are some desktop level. There are four desktop, three desktop level plus playground level. And then there are two mobile challenges. So if you look at Gravity Spider is a system. At Gravity Spider is a system that's where people can do more and more classification and unlock more difficult levels. In this case, we decided to do slightly different stuff in the sense that we have levels, but people can basically access whatever they want. Because of course, we know that people know very well already gravity spy. So we don't want that people start playing and doing classification when they know they done this. When they know they've done this already with Gravity Spy, so in this way, they can keep doing stuff with Gravity Spy, and then here they can do something different, something that I will show you in a bit. And then, as I was mentioning, we had also a couple of tasks that are related to mobiles. And in this case, these are related to not just to classification, but to other stuff. So, what are the challenges that we have? What are the challenges that we have in our demonstrator? We want to attract more gravitational wave funds, of course, and we want to engage people that are citizen scientists, and maybe they are excited and are interested in, I don't know, planet hunting on other stuff. And we wanted to also offer to them the possibility of knowing more about gravitational ways. I was mentioning before auxiliary channels because we decided to We decided to have not just the strain data, but also including the auxiliary channels coming from the detector. Of course, because more data is supposed to mean more fun. And so we prepared an initial data set based on Virgo. And since the auxiliary channels are now public, we prepared an agreement between EGO and the Reinforce Consortium to use the data from the auxiliary channels. The data from the auxiliary channels for these, uh, for this detector, for this project. Of course, I mean, there is only the images, there's nothing related to the GPSM, etc. It's just the images. And we were trying to develop a workflow that is flexible and eventually, one day, can accommodate other noise features and not just the glitches. Even if this is probably not easy, we need to think about a little bit more, but in pretty much. To think about a little bit more, but in principle, this could be possible. So, just to give you some example, we started with basic levels, which is basically very similar to what you can do. It's even easier to what we can do gratify. So you do classification. The idea of these levels are for people that they don't know anything. So, we have a playground level where people can have a sort of a feedback. So, they click. Sort of a feedback. So they click on a glitch class, and if the glitch class is wrong, there is a message saying, oh, this is wrong, try again, et cetera. So in this way, they can get used to learn how to auto-play and how to classify these glitches. And then later on, you can have more classes and this way you get used to work with gravitational waves. Of course, people that know already gravity spy, they can skip this part. Gravity spy, they can skip this bar, of course. Then there is another set of tasks, another task, which is coming basically from level two, where you have more classes that we observe in our detector, that we observe in Virgo. And then the citizens are asked to draw rectangles to isolate where there is a glitch. You can have more than one glitch, so in this case, you draw more than one rectangle. Draw more than one rectangle, you can just skip this part because maybe you don't see anything. And in this way, you can get more details about the shape of this detector. Of course, Zoo Universe offers the possibility of having a field guide where you can see information on riches, you can see that there is all the tutorial stuff, etc. And then there's the last level, level two, where you can basically go and Where you can basically go and use auxiliary channels. So, the idea here is that you are presented with the HFT image, the red one, like before, and then you can compare this image with the spectrograms in four auxiliary channels. So, in this case, here you can see four examples, and then the citizens are asked to click on the one or more. One or more on these blue images, where they think that there is a similarity in the morphology. And here you can see that the red level curves gives the shape of the HFT, and then you basically compare these things. We started with the H channels, but we have discussed with people working in the data characterization, and of course we will add more channels. add more channels so that we can improve the number of channels that are included in this step. And then there are the two mobile levels, which are basically one related to the level two, like this one, where on your smartphone you draw a rectangle. It's called different. The name is different. It's called Las Foude Glitch because it's It's called last photo that glitch because it's likely you're taking at the last of your glitch, and then there is another one for auxiliary channels. But instead of having four images, because they are difficult to see all four images on your smartphone, because of course the screen size is not as big as your laptop, for example. You have only one image. So you just have to say, oh, these two morphologies are similar, yes or no? And that's it. And then you go to another one. Go to another one. So, these are basically the main tasks that we have on these projects and how they integrate in the whole environment of Virgo. So, the idea is that we have created a database, a Virgo Rich database, where we basically store the triggers. And then with Gwich Hunters, we can basically get the triggers, we can build images, we can put on Gwich Hunters, and then we can use this and the And then we can use this, and these output coming from the citizen scientists to train the thanks to our algorithm, our machine learning algorithm, and then we can store in the pitch database all the information and the labels. So not just the labeling, but also the position, etc. Of course, we started in August the first set of the beta tests. This means that the project was submitted to. The project was submitted to a set of citizen scientists that acted as a beta tester. So they started playing with this. They gave us a bunch of comments and feedbacks. And now we have implemented all this feedback. Just to give you some very super preliminary results, just basically on the statistics, we haven't launched this officially. So far, we present. So, so far, we presented this project in some specific occasion, like the European Research Nights, some science course that we did, Open Science Fair, etc. So, of course, the number of people that started playing with this are of course limited. You can see a number of classifications. You can see that there is a bunch of people trying the playground, which means that these are people that probably they didn't know. People that probably didn't know these projects before, and this is also probably related to the fact that we started offering these things in events located, for instance, in Italy, and maybe they didn't know about this whole citizen design stuff before. This is something that these are just our guesses. We need to, of course, to understand this with the more details. You can see that the beta test was. You can see that the beta test was in August, and then you can see the peak here on the right picture. Okay, so these are some preliminary results. You can see some confusion metrics just based on playground, because for playground, we have the truth. And of course, we can check these results against what the people classify. Classified and decided, you can see that there are some confusion between blips and coys, and this is can be it's realistic because, in some cases, it's R to the side, and but of course, the numbers here are not yet very large. So, of course, these are, as I was mentioning, very, very preliminary. Okay, next step. So, the rechanters, the demonstrator has been finalized, has been delivered at the end of May. End of May is on the universe, and we did many rounds of tests because the whole universe policy states that there is a set of rounds of tests. So, first you have an internal test by the universe people, then there is a beta test with people, a bunch of citizen scientists that are looking and playing with your projects. And then in the meantime, we have. And then, in the meantime, we have updated our data sets. So far, we have included glitches in Ltray A, and then we will soon include other glitches as well. So, the next step is that we have been basically implementing all the comments that we've been receiving. And so we are ready for the official launch in the universe that will be on November 16, which, by the way, is tomorrow. So, if you go now, you will still. You go now, you will still see the beta label, and then hopefully from tomorrow, this will become an official project in the universe. And okay, conclusion. So I gave you a quick overview about what we can do with the image short-time series and doing machine learning. And the performance are extremely good, are very promising. So, we are, in fact, we are using already this in Ligen Virgo. These were This worked pretty smoothly on both simulation and data. And now, later on, I give you an overview about this new citizen reinforced citizen science projects, in particular this Greek chunter. So we are very excited to see how this will go. And we hope that this will also help not just to have more data sets with labels, but also to have a large. Possible also to have a larger engagement for people with gravitational waves. And I think that's all for me. So I'll stop. Thank you very much. Thank you, Max. Any questions from here or through Zoom? There is a question. No, no, that that I think was answered. Yeah. So I have a quick question. You add auxiliary channels. You said that for the moment you had eight auxiliary channels. How scalable is adding auxiliary channels to the project, right? To the project, right? Even in the reduced set of channels in LIGO, we have about I think a thousand channels. Is it feasible to how many channels, auxiliary channels, will you end up putting the project? Well, this is an interesting question. We have started with a set of channels that are That are interesting for detector characterization purposes. So, this is a subset of channels. There are some tens of channels. So, I think that the goal initially will be to have orders of, let's say, a few tens of these, and then include more and more. So far, we started with A channels just to see if the whole stuff is working. The whole stuff is working. But in this sort of stuff, the real answer, you will have the real answer as soon as the citizens will start playing with this. But the initial goal will be, I would say, having some tens of these and then improving and adding more and more of the light channels. Also, because this is supposed to have a strong integration with people working on a technical characterization. So, if there are something that is particularly interesting. That is particularly interesting, some channels that is very interesting to study. We can include and we can prepare images, and we can include in the data set. And is there a way you can avoid generating all these images? For example, when you do Q scans, you vary slightly the parameters and the shape of the glitch in the scan channel. The in the scan changes a lot, right? For I always had the suspicion that coefficient and bleep were the same kind of bleaches, right? And it seems that you find something like that because the confusion between the two is high. Maybe can you say a little bit more about avoiding making the spectrograms and finding an alternative way? Yeah, we yeah, we have. Yeah, we have done some tests by changing the parameters, and also we are analyzing the first feedbacks that come from the citizens. So this is something that I showed before. So we plan to use this first, as soon as the project will be launched. So hopefully, we hope that we will get more input. And then we would like to. We would like to see the different parameters that we use for each of these glitches and see if there is a way to optimize these things. The thing is that from the point of view of the interface, there is a set of stuff that you have to do to be compliant with the universe. But before you can do some study a priori where you can optimize. Or where you can optimize the way that you present the images. So we've done this work already a little bit, but of course, the ultimate results, the ultimate answer will come when we will have feedback from citizens. Any other questions? So thank you very much. So I was wondering about the amplitude of the glitch versus the noise amplitude. So is there any impact of the, let's say, glitch to noise amplitude on the performance of the deep learning models. Yep. Yeah, that's basically the question. Yes, so what we did, I haven't showed the results here, but what we did is to try to see how the performance changes if you vary the signal-to-noise ratio. But what we have seen back in also already back in time where we've done these simulations as well is that once you are doing the So, once you are doing the whitenings part, you are able to remove most of the noise, and then the shape of the glitch you can classify very well, even for fainter ones. Okay, thank you. And the other question is: did you test after the model is trained, did you test input in gravitational? Gravitational weight signals. So, are they classified as noise, as glitch? Yeah, what we have done, yeah, yeah, when including the, we call it chirp-like to train some stuff. Yeah, you can see the real data, you can see the signal. There are actually other works, there are many, some papers. many some papers that are that have been focusing on the detection of church-like signals. There's a recent work by the group of Barcelona, for instance. And yeah, they show that these approaches is pretty good in distinguishing real signals. For us, we were more focused on the noise part, but we demonstrated that this approach is robust in the technical. Technical, so share. Thank you. Thank you. Okay, thank you. Any yes? Hi, thanks for the nice talk. I was wondering, kind of along the lines of the last question, how do you make sure that you're conservative about not Conservative about not misclassifying signals as glitches? Is this something that you can just build into the loss function? Yeah, this is more important when you are analyzing only one detector, because when you have more than one detector, of course, I haven't mentioned this here, but we did some tests with more than one detector. And when you have more than one detector, clearly it's easy to spot. It's easy to spot a signal because you can see it in both detectors. Regarding a single detector, you can adjust the probability, the threshold of the detection so that you can reach the level you want of misclassification. But in principle, at least for those that are There are no misclassifications. Cool, thanks. And the other question I had is how you're deciding what windows of data to show users in the app. And I was wondering if like your choice, your choice about this can bias the training data that you're later feeding into your neural networks. Feeding into your neural networks? Well, yeah, this is something that we discussed a lot when we're also developing the Glitch Hunter project because we have some, of course, some glitches that are very short, others that are longs. So far, we have been decided to use it to using a standard so that you can get both long and short. But yeah, this is, for instance, in Gravity Spy, they Graphic Spy, they also have developed a multi-window, multi-time range, I would call it, image. So you have four images with different time range, and then the neural network analyze all these four time images together.