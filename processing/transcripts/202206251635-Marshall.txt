In this session today, it's a privilege for me to be talking during this workshop amongst some really wonderful talks today. So today I'll be discussing to you a model class that I created, which is amenable to multi-taper spectral analysis. The model class is built using stochastic processes that contain elements of cycle. They contain elements of cyclist stationary and stationary processes. Now, that might sound like an easy exercise because we have all seen these consistently just in the past session, let alone in the field. The reason I'm going to be presenting this new model class is that I think it has applicability in the sense that it can justify a lot why models work, why models work. Models work why multi-taper spectral analysis should be expected to work. Over the years, since the seminal 1982 paper and even since spectroanalysis began, modern methods and spectral analysis have enjoyed a lot of success. We look at statistical analyses to test the model performance, and it always seems to show satisfying results for physical data sets. Data sets. So, why does it work so well? And when does it break down? When do the asymptotic assumptions about the distributions of our reconstructed signals, when do they break down? And why do they often seem to be working so well? Why do we often get Gaussian statistics? So, I'll start off with a model. It'll be based on physical foundations, and then I'll explain how from the physical. How, from the physically realistic model, we can change that model a little bit so that it stops being quite as physically plausible, but more generally applicable to a wide variety of data sets. So that is called a step of robust modeling. The way I'm going to depart from physical reality with the perturbation into the land of mathematical wonderland is I'm going to be making my model. Making my model such that you get eigencofficients that have nice distributional properties. I'm going to describe two ways that you can model the DTFT spectrum. The first is directly trying to model the DTFT spectrum. And then you can get its statistics and its finite dimensional distribution information that way. So that's pretty obvious. The second way is less. The second way is less obvious. We're not going to model the DTFT spectrum directly. We are going to use another frequency domain process that is random. And that's going to be the way that we get our DTFT statistics. Similar to PCA or something, where you're finding out information about a signal by going into a space where the statistics work well, and then coming back again at the end and saying, here's the statistics of the temporal model. Statistics of the temporal model. So we begin with the motivation for the physical process. We have a black voltage time series. It is periodic with a period of one day. You can probably guess what the source of one day periodicity is. It's called Earth's rotation with respect to the stars. So So, because of that, you think, well, surely if we just fit a Fourier series in red, it should explain what's going on. Well, apparently not. So, it's not going too well. Now, I could increase the sensitivity of my detector for the F test. I don't feel like doing that. I think it's better to stick with this sensitivity that I'm using here. So, what I'm going to do is accept that there is residual periodicity. Where does this Where does this residual periodicity come from, then? To answer this, we look at the power spectrum on the right here of the black curve of the data, and we note that there are a number of conspicuous spectral peaks. Those peaks characterize the periodicity of the red curve here. So, So, so far, nothing very interesting. We have red noise, colored noise, so that produces a log linear trend. Again, we've all seen that before. Now, I'm going to subtract out these peaks by subtracting the red curve from the black curve and looking at the residuals. What does that do? The power spectrum now has a similar kind of baseline to what it did before. So, here's the baseline. Now, let's look at these residual peaks. Why are they there? Why did my Why are they there? Why did my red curve not do the job? The reason I propose is possibly just a guess, but it helps for my modeling to have this physical motivation. I propose that there's been a filtering step that affected these peaks. A peak broadening, we call that. So we had an original signal that was like the red one, but we passed it through a linear filter. And that produced something similar. And that produced something similar to the effect of what Middleton calls a half-wave rectification, a linear rectification in this case. And that means we have broader peaks than we should have if we just had a nice red curve that explained everything. So that's why these peaks don't completely disappear when you subtract the red curve from the black curve. That's why you've got these little peaks down here, these bumps. So that's one possible mechanism. Mechanism. Is there any other non-linearity in the system? Yes, I think there is. So let's look at the log linear trend. What could have caused that? We look at this curve here from Middleton, and it seems to provide us with some information. So we have a black curve here with peaks. This is similar to this because it's on linear frequency, not log frequency. So this curve here, I procoles, might be similar to this. But this black curve can be related. But this black curve can be related to a curve that has lower power and is a bit smoother, and this dashed curve here. And that corresponds to a transfer function that is associated with a quadratic nonlinearity. So, all right, that's two possible mechanisms. And Feynman shows that if you consider the linear and nonlinear responses, we often see a discrepancy in the two curves that result, similar to what we see here. Similar to what we see here. So, now what I'm going to do is look at and see if I can model this residual periodicity that produces this and occurs when I subtract the red curve from the black curve. And I'm going to do that using a quadratic nonlinearity. But I don't think that's enough myself, because when I read Karl Margarov's seminal paper on the F to the minus five-thirds phenomenon, he basically said, All right, well, we might have quadratic nonlinear. Said, all right, well, we might have quadratic nonlinearity, but the higher order nonlinearity is a complete mess. So, what I'm going to do is just toss out the neat nonlinear theory and replace it with stochasticity. And often what people do is they look at cyclostationary processes. This is a tidy, convenient way to deal with things. It's a contrast to what BIR2012 does, which is a nice enough method, I think, in itself. It's an iterative subtraction of harmonic red curve. Of harmonic red curves one after another. But I think this is more tidy because it produces stationary states as its base. So what do we do in spectral modeling? Well, we need to know what the spectral quantities are that we're after. What are we trying to explain with the power spectrum, with the bifrequency spectrum? What are we actually looking at in the frequency domain? So there are two ways, I think. I think there are only two. Ways, I think. I think there are only two quantities that we ever look at in the spectral domain. And I think this is what we're trying to reconstruct. In fact, I think what we're trying to do in spectral analysis at the end of the day is we're trying to reconstruct non-random frequency index signals that explain the finite dimensional distributions of the DTFT of the time process that we're interested in. The two spectral processes that are round. The two spectral processes that are random that we're going to use to infer these non-random FDD signals is we're going to use the first one, the integrated spectrum. And that integrated spectrum is something that quantifies a spectral measure that occurs in the spectral representation. The second quantity I'm going to consider is an approximation of the integrated spectrum. And this is called the normalized integrated spectrum. It's nothing fancy. It's just an average of the integrated spectrum over a local neighborhood. And this is nice because it's a direct function of this quantity. Function of this quantity, but now it doesn't contain any singular components. So it means we can use it for stationary analysis. So this is actually, I think, what is implicitly used in Thomson 1982 when he's expanding this quantity in terms of DPSWs. To estimate either of these two, we use the DFT eigencoefficient processes, the YK, and we specify X so that the YK have an asymptotically complex Gaussian. Have an asymptotically complex Gaussian behavior, and so that they have a dimension between them of 2 and W. So now to make a model that incorporates all of this and has all these satisfying frequency domain properties about the DTFT spectrum, how can we do this from physical first principles? Let's start off with Middleton. Middleton looks at a noise process, some kind of A noise process, some kind of input, and I'm going to define a stationary state based on Middleton's idea. I'm going to start off with a particle in three dimensions and let it traverse along a random walk with certain regularity conditions. This produces a sequence of standardized displacements over time. And because the displacements are ordered, and because they're ordered by time, this sequence weakly converges. This sequence weakly converges by Lyapunov's theorem to a Gaussian process. And it actually happens to be that this Gaussian process is non-stationary. So what I'm going to do now is perturb that Gaussian process by a compensator, and that is going to produce a stationary output, which I'll call the input of my general process and the And the input here, I'll assume it doesn't have to be Gaussian. So I'll send this now through a non-linear functional GNLN. And that will assume we'll produce a driver. So far, all I've done is I've taken Middleton's theory and theory from Feynman about nonlinear driving functions for stochastic wave equations. I send this through a stochastic wave equation integrator. It gives me a Wave equation integrator, it gives me a solution. You can think of this as putting a couple of pendulum in a tank of water with turbulent forcing. This solution is then passed through an LTI filter, which everybody knows about, more or less, who does imaging. And that accounts for geometric distortion, like for an array, like the seismic arrays we've been looking at, or attenuation. So that's a nice, plausible physical model, nothing too fancy so far. And now I'm going to show that we can adjust that model a little bit, and then it's going to give us all the nice spectral properties that we're looking for. So the first thing I want to have is that my model on the right here, the physically plausible one, is such that you get complex Gaussian DFT. DFT, eigen coefficient, spectra. So, to do this, it's a nice prescription that was being provided in the literature, but it was scattered over bits and pieces. So, I tried to pull these things together into one cohesive whole, and that produced my PhD dissertation in 2020. I said that the input on discrete time was IID. I said that the nonlinear functional could be approximated by an AR1 integrator. By an AR1 integrator. I said that the driver was an AR1 as a result. I estimated the ODE integrator of my wave equation by an AR2 filter. That is something that's been done in the literature before as well. And then that, of course, is a cascade filter, so that gives me a narrow sense stationary process according to Brillinger's theorem. And then Brockwell and Davis tell me that I can use an ARMRPQ integrator for the LTI filter. Integrator for the LTI filter. No problem. At the end, I get an aerosense stationary process, so it's not quite satisfying because it's too simple. But look at what Jaguar shows. Here he's got an example of fading radio intensity where he's got stationary stuff going on here, stationary stuff at the end, and in the middle he's got a bit of a frequency modulation going on. So that's not technically stationary, but when you look at the statistic, the reconstructed signals, the PSD and the other The PSD and the other correlation that are non-random and characterize the FDDs, the finite dimensional distributions of the stationary process. These appear to be like what you'd expect of a stationary process. And why is that? It's because these things smooth over realizations of that stochastic process. So it makes sense that there's a resistance of these signals to a minimal amount of non-stationarity. So, why am I? So, why am I caring about these stationary models? It's because there are theorems that show that you can get very close to complex Gaussian eigencoefficient spectra when you do this. The reason I do this is because now I know how to specify the different filters along the way in order to ensure complex some form of complex normality in the eigen coefficient spectra. So, I'm going to have an MAQ for this point. I'm going to have an Q for this point, I'm going to have an IID input, I'm going to have an ARMQ filter for the solution, and then an arbitrary filter for the output. If I I'm not going to change one thing, I'm going to change, I'll stop this being AR1 for the moment, this non-linear function for the driver. And by doing by making this just a quadratic function as opposed to like what I Function as opposed to like what I was showing previously with a square wave rectifier. This actually produces a cyclostationary component, which is the almost cyclostationary, almost periodic component here. And then you pass that through the AR1 of the ODE. That gives you an LAPTV process, a linear almost periodic time variant process, plus stationary noise. So now we're getting a really nice looking model. Getting a really nice-looking model, and I can specify to you what distributions need to be present for the IID process to confirm that we have Gaussian behavior and the eigen coefficients under the stationary approximation of the previous slide. And I was able to do that by simulation study for the more complicated processes in my dissertation. The degree of error is presented there. Finally, I conclude this talk by presenting a model class that is amenable to multi-test. That is amenable to multi-taper spectral analysis. We'll be trying to infer information about the finite dimensional distributions of the DTFT spectrum of a time-varying stochastic process. There are two ways to do this. The first way is to directly try to model the DTFT spectrum. And the second way is to look at a And the second way is to look at a different spectrum altogether, like in PCA when you look at the principal components, in a model space where this inference is straightforward. We get our estimators for the reconstructions for the relevant signals that we want in reality about the DTFT, but in this new space. And once we get those estimators from that new space, we get better quality and better control of what we can do. Of what we can do with these reconstructed signals, than we would if we were just trying your direct characterization of the DTFT spectrum. And it's surprisingly simple how you do this. So we have a state-space model, as we saw in the previous presentation. The output process X, which I said was narrow sense stationary, that's like the Yaguan noise or with the perturbation of a cyclist stationarity. This output here is at the This output here is at the observable layer. We have two hidden layers, and they are specified by the integrated spectrum I discussed in the previous slide. And it so happens under the first assumption that the integrated spectrum is equal to the DTFT. So all I'm doing is I'm specifying the DTFT as being a hidden layer in each of these. And you can see an example of a simulated DTFT spectrum for the process X. If I do this, the restriction that I have is unfortunately that the limitations on the types of models I can consider are quite severe. And as a result, it's harder to test for the Gaussian property of the eigencoefficient spectra. It is also harder for me to specify elaborate models. I have to only consider the output here and the driver. I've removed the The driver. I've removed the base states, which I thought were stationary, and I don't know anything about the filters in between. So there's a lack of control, there's a lack of distributional verification. It's a whole other theory. It really requires a whole new line of research, and I think it's quite daunting. What I did is I looked at something a little bit. It also required time limitation of the original time series, which is by no means a guarantee when you look at the date. When you look at the data, when you expand this integrated spectrum in the DPSWs, you do get a bounded truncation error. So that is good news. But in general, you can use this model and it's okay, but it is restrictive. What I prefer is this other model I was considering, where we don't assume that the integrated spectrum is equal to the DTFT. So now instead of directly modeling the DTFT, I'm going to just Modeling the DTFT. I'm going to just directly model the normalized integrated spectrum, that one I showed with the averaging filter. You can think about the intuitive merits of this integrated spectrum by looking at the power spectrum and comparing it with this curve I plotted here. This is the real axis, this is the imaginary axis, and this is the frequency axis. And it looks like a Brownian motion with really high jumps where the peaks are occurring. Where the peaks are occurring in the spectrum. So that's a way you can interpret what I'm doing. Under this different type of model, all of a sudden, I've got my model back. I've got that physical model I showed you with all the nice interpretable features. This all comes back because now I can look at stationary processes. I can look at cyclostationary processes. I don't have to worry about these jump discontinuities that are so bothering me when I look. So bothering me when I looked at the direct attempt to model the DTFT spectrum. Again, if I do an expansion and Slepian functions of the relevant spectral quantity, I get an OP1 error again. So it's a bounded truncation error. And I'm not even expanding a spectral process anymore. What I'm doing now is I'm expanding a rectangle function that occurs inside a convolution expression. Inside a convolution expression for the DFT eigencoefficient spectrum. Naturally, expanding a rectangle function that is non-random in DPSWs is much easier than expanding the DTFT, which is random in these functions. It's much easier for me to say what the errors are in the truncation. So here's the two different models: direct modeling of the DTFT. All right, it's fine, but it's All right, it's fine, but it's a bit limited and it's hard to control things. If we model the integrated, the normalized integrated spectrum instead, kind of like your PC signal that you get in a singular value decomposition, then you get a much neater looking time domain model. So there's like a balance between nice modeling in the time domain and nice modeling in the frequency domain. And since the frequency domain is already abstract, why not make it? Already abstract, why not make it a little bit more abstract and have an easier time domain model to work with and be able to control all the model elements? So that's my amenable time series model. And there's a lot of references. If you'd like me to send them to you, please let me know. And a lot of people helped in this work. It wasn't just my idea. So I can send you the collage if you'd like to see who the contacts were. So thank you for. Contact as well. So, thank you very much for your time and hope your evening goes well. If you have any questions, I'll take them.