Okay, um so yeah, so the work I'm talking about today um is very much work in progress. Um, so this is work with Raphael Husa and Jonathan Torne, looking at trying to capture um varied extremal dependence structures using mixtures of conditional extremes models. And so, even though it's work in progress, hopefully, I can say something interesting. Hopefully, I can say something interesting about what we've done so far and also feature directions that we'd like to take this in. So, we'll just start off with the motivation. This is quite similar to some stuff that we've already heard in previous extremes talks this week. But essentially, in multivariate extremes, we often try and think about things in terms of asymptotic dependence and asymptotic independence. Independence. So asymptotic dependence would be where we, if we keep taking lots and lots of data, then we'd expect to see the largest values happening simultaneously in our different variables, whereas asymptotic independence would be the largest values happening separately across the different variables. And this is important to think about because different models for multivariate extremes are able to capture. Variety extremes are able to capture different structures in terms of asymptotic dependence or asymptotic independence. But actually, we could have something more complicated than either of those two categories. So under the in the bivariate case, under asymptotic dependence, we'd see both the variables being large together, but we could also see one of the variables being large and the other small. And the other small, or both of them being large, and while the other small. So, this more complicated structure requires us to think about things a little bit more carefully. So, I've just got some examples of some data, and I'll keep coming back to these examples during the presentation. And so, these kind of in the bivariate setting kind of cover the different structures that we can see in terms of. That we can see in terms of the extremal dependence. So, the first one up here is a Gaussian copula where we have asymptotic independence. So, it's known that we have asymptotic independence. And we can see here that the largest values in X correspond with, well, not the largest values in Y, and vice versa. And I'll just say that this is for data on Laplace margins. So, we've heard a few times. We've heard a few times already that we kind of often transform the margins and just focus on the dependence when we're looking at multivariate extremes. So, throughout this talk, we'll just be focusing on data on the cost margin. So, the second data set is simulated from a bivariate extreme value distribution with a logistic model, which is asymptotically dependent. And we can kind of see here that the biggest values are tending to happen simultaneously. Are tending to happen simultaneously across the two variables. And then on the bottom row, we've got two examples of asymmetric logistic models which can have combinations of these two things. So the bottom right actually has is kind of a combination of both of the top two. So we have some case some data where X and Y are both big at the same time, and some where X is big while Y is not big. Is not big and y is big, and x is x is not, whereas the bottom left has x big on its own and x and y big together. So, in these examples, I'd kind of picked the parameters that control the dependence so that it was quite easy for us to see what that looks like for this particular data. Data, but each of them does have a parameter that controls that dependence. And if we change that, the underlying extreme dependence structure is the same. But just looking at things by eye, it's not necessarily so easy to see what's going on, which means that we need kind of methodology to detect the structure for us. So, this problem has been looked at before. I've just got a few examples on this. Few examples on this slide of work that's looked at this problem. So, trying to estimate the structure in terms of identifying subsets of variables that can be large simultaneously while others are of smaller order. And there's a nice review paper by two of our in-the-room participants that covers this as well. So, the idea here is that if we know this structure, then we can either choose or construct an appropriate structure. Choose or construct an appropriate model for the extremes that is able to capture that particular structure. So, what we're doing in this work at the moment is instead trying to both estimate the structure and create a model for the extreme at the same time. And we're doing that by exploiting mixtures of conditional extremes models. So, for most of the talk, I'll be focusing. Um, for most of the talk, I'll be focusing on the bivariate case, but I'll get to some trivariate extensions at the end. And so, in this bivariate setting, there are three subsets of interest. As we've seen, X big on its own, Y big on its own, or both of them big together. And one thing to bear in mind is that each variable should be represented at least once in this extremal dependence structure. Essentially, that's just because each variable has to be large at some point. Okay, so I'll just give a quick introduction to conditional extremes models. And so I think Anthony briefly mentioned these in his talk on Monday. But yeah, these models originated with Heffman and Torn in 2004 and for variables with exponential of the tails, which we have in the case total class margins, which I've said and I'm going to focus on throughout the talk. Um, throughout the talk, the idea is to condition on one of the variables being above some high threshold, which we're calling u, and look at a normalized version of the other variable, normalized by two functions a and b. So this is this y minus a over b. And look at the distribution of that, as well as the exceedances above the threshold of the conditioning variable. And if we take that threshold towards infinity, Take that threshold towards infinity, then these things tend in distribution towards a Z and an E, which are going to be independent in the limit. So this E is going to be an exponential distribution, a standard exponential distribution, and that's just because we started with our exponential of the tails. And as I said, it's independent of their. It's independent of Z in the limit, and so Z has some restrictions on it, so it has to be non-degenerate and place no mass on infinity. So, because we get this independence in the limit, that means that we can just kind of focus on the first thing, so this normalized y. And it turns out that the well, so Hesman and Anton proposed. Proposed taking A as alpha times X and B as X to the beta, and they showed that these are sensible choices for a wide range of known distributions. And this model is particularly nice because it's able to capture both asymptotic dependence, well, either asymptotic dependence or asymptotic independence. So, if we set the alpha to one and the beta to zero, To one and the beta to zero, we're in the asymptotic dependence setting. Whereas, if alpha is less than one, then we have asymptotic independence. So, we talked about this residual distribution Z, and so it's common to set that as a working assumption as a normal distribution. And if we do that and kind of undo the normalization on y, we get a model that looks like this. So, y given that x is And y given that x is some value little x above a high threshold u is going to be normally distributed with these parameters. So just to note, the variable y itself isn't normally distributed. This is conditional on x being equal to little x. So in general, we'd have these four parameters to estimate. So alpha, beta, mu, and sigma. Sigma. But under asymptotic dependence, this simplifies to just having the mean as x plus mu and a variance of sigma squared. So thinking back to trying to capture the different dependent structures, if we want to use these conditional models, there are three particular models that we'd want to compare. So if we have only asymptotic dependence, so Dependence and so we can think of that as only x and y being big together. Then we'd want to do the alpha equals one, beta equals zero case. So that gives us our first model. The second model would be to allow these alpha and beta term parameters into the model to give us that possibility of asymptotic independence. And if we set the alpha parameter as being less than one, that can be our asymptotic independence. That can be our asymptotic independence model. So x big while y is smaller. And we can also have a mixture of these two things. So there's going to be three models that we want to compare when we're conditioning on X being above some high threshold U. I will just mention that mixtures of bivariate conditional extremes have been looked at by one of John's PhD students recently, but in that paper they were looking at allowing for mixture. Looking at allowing for mixtures of asymptotically independent components, and it was just in the bivariate setting, so not extending it higher than that. Okay, so in terms of fitting the models, it's quite straightforward to fit the first two. Just using standard maximum likelihood estimation works fine. For the mixture model, I've so far been using just an EM algorithm. Raphael did code up a lot of other options. Raphael did code up a lot of other options as well. But the EM seems to work pretty well, and it's also quite a bit quicker than some of the MCMC-based approaches. But we did come across one issue. So sometimes when we were fitting the mixture model, if we take our threshold, so I've just picked a threshold here for X, and then if we flip the mixture model, we can The mixture model: We can assign each of the points where X is above the threshold as either being in the asymptotic dependence component or in the asymptotic independence component. And it turns out that rarely, but it did happen, and if we did that assignment, then sometimes we'd get this weird kind of overlap of the clusters. So we don't really want this sort of thing to happen because the asymptotic dependence component should really. Asymptotic dependence component should really be X big and Y big at the same time. So it doesn't really make sense that these points down here are assigned to that cluster. So to try and avoid that, we've kind of put, well, we've tried a few different things, but what we've settled on at the moment is to try and put a constraint in the likelihood that stops this happening. So if we just zoom in on those points where X is above the threshold, Threshold. What we've done is essentially to look at windows across the value of x, and within each window, check whether there are any values of y assigned to the asymptotically dependent component that are smaller than those in the asymptotically independent component. And if they are, then we basically put a really big penalty into the likelihood to try and stop this happening. So when I write, So, when I run it again with that penalty and the likelihood, we get a better result for that data. Okay, so that this slide is just summarizing what I just said. So, maximum likelihood estimation for the two one component models and the EM algorithm for the mixture model, but with this penalty in the likelihood to avoid the cluster overlap. So, once we've got our three fitted models, Our three bits of models, and there are different things that we might want to do. So, yesterday in Jenny's talk, she mentioned that once you have a model, you might want to simulate extra data to try and estimate probabilities that extrapolate beyond the question. I don't know if it's okay. Yeah, to try and estimate probabilities that allow us to extrapolate. That allows us to extrapolate. So, this is just an example where I've fit the three different models to some data and then, using the fitted model, simulated lots more data. So, you can kind of see here that the type of model that we actually use does matter. So, in the asymptotically dependent AM only case, we are getting quite a different shape to the asymptotically independent only case and again to the mixture. And again to the mixture. And we can actually put the original data on top of those. And it looks as though the asymptotically dependent one isn't doing so well because we're getting a lot of simulated points up here, but that wasn't really the shape of the original data. And this was actually Gaussian data, so I know that it's asymptotically independent. Another thing to note is that if you fit the model for some threshold U. The model for some threshold u, it should also, or it is also, um, yeah, you can assume that you've got the same model for a higher threshold, and say B. So, this is just an example of that. If we simulate, choose a higher threshold and then simulate some data, we could also look at what happens. So, that's a point that I'll come back to in a moment. Okay, so of course, we've got these three models. Of course, we've got these three models. We want to choose what our favourite model would be in practice. So, one option that's quite obvious would be to use VIC or some other model selection criteria. So, we tried this, but we found that it often gave us issues where the mixture model was being preferred when we know that the truth is actually that there's only one component. There's only one component. So, again, we tried a few things, but one thing that we found to work quite well is that if we bear in mind that this model, we fit the model at a threshold U, but it's also a relevant model for higher thresholds. So, if we take that into account, we can look at calculating a VIC that's just based on points above a higher threshold and try and do that comparison above. Do that comparison above the higher threshold instead. So, this is just one example of what we get if we do that. So, I'm calling this BIC star just to show that it's not exactly the BIC for the fitted models because we're just using a different subset of the data. So, the QV on the x-axis here is a quantile that I'm using. A quantile that I'm using to choose the threshold V. And then in this case, this was again a Gaussian distribution. So I'm subtracting the true, the BIC for the true model, which is the asymptotically independent model. So we want to minimize the BIC star. So at the lower thresholds, actually, we can see that the mixture model. Actually, we can see that the mixture model is being preferred, but as we increase this threshold using the BIC star, and we do just get the asymptotic independence model being preferred. So, this is obviously just one data example, but it was something that we saw across a wide range of different models, and it seems to work pretty well in practice if we fit our model at a lower threshold to allow us to have kind of To allow us to have kind of enough data to get a good fit, but then actually choose the model at a higher threshold so that it's kind of really taking into account the behaviour at the extreme. Okay, so another thing to think about in the bivariate case is that so far we've just looked at a single conditioning variable, so we just looked at conditioning on X, but of course we could choose either variable as the conditioning variable. Variable as the conditioning variable. So, really, we want to ensure that we've got consistency across those two variables. So, if we fit the models and do the model selection based on X being large and do it for Y being large as well, we want to make sure that they agree. So, for example, we don't want one model to say that X and Y can be big together and the other to say that they can't. So, what we've proposed to do in this case Do in this case is to instead minimise the sum of the BIC star values across the two different conditioning variables, but we only allow it to select a feasible combination of structures. So if X is telling us that it can only be big on its own, then conditioning on Y has to tell us that they can't be big together. So I've just got a few simulations. So, I've just got a few simulation results. I don't know how long I've got for that. Okay, and so I'll just quickly go through some simulations and then talk about the tri-variate case. So, this is some simulated data from Gaussian distributions where we know that we've really got asymptotic independence. So, I've done 100 iterations running the full approach and looked at how. Approach and looked at how often it tells us that we have truly got asymptotic independence. So this is for different values of the correlation, and you can see that it's doing pretty well, especially actually for higher correlations, which is a little bit strange. But yeah, this is something that we need to look into a little bit more. So for the logistic model, we have just the asymptotic dependence component should be. Dependence component should be being selected. And for the dependence parameter in that model being 0.25 or 0.5, we're doing pretty well. As we move up to 0.75, things get a little bit more difficult. So it's perhaps not so surprising that we're not doing very well in that case. One thing that I'll just mention here is that when we're getting it wrong, we're distinguishing between orange, which is Between orange, which is when it tells us that we've got x1 and x2 big together, but is also giving us some extra components, and red, which is when it's just saying we don't have x1 and x2 at all. So, in this 0.75 case, most of the time it's just telling us that we've got asymptotic independence, which if you look at some data simulated from that model, perhaps isn't so surprising. And the results are kind of similar for the two case. Also, kind of similar for the two asymmetric logistic models, so I'll just skip over those. Okay, so I'll move on to the tri-variate case, which we haven't done so much work on, but we do have kind of a few ideas that I think are quite interesting about how we can extend the approach to the tri-variate case. So, if we go back to considering just a single conditioning variable, so X being above. X being above again a high threshold U. There are now four mixture components that we'd want to consider. So essentially, in the tri-variate conditional extremes approach, there's an alpha and a beta associated with each of the other two variables. So with Y and with Z in this case. And each of those can be controlled to allow for that variable being large or small. Variable being large or small while X is big. So the four components that we'd want to consider are X large while Y and Z are small, X large and exactly one of the others being large with it, or all three being large simultaneously. So because you've got these four different components that we want to consider, that gives us 15 possible mixture models. Because obviously we don't want to have one where we don't. Because obviously, we don't want to have one where we don't have any components at all. So, having 15 models to compare, and even in the trivariate case, is perhaps too many. And as we increase to higher dimension, that's obviously going to get even worse. So, we've been thinking about how the bivariate results can tell us something about the trivariate structure. So, in particular, if we think about fitting the bivariate models to Variant models to y, given that x is large. If that tells us that we only have the asymptotically independent model in that case, that's telling us that x and y can't be large together, which means that we can remove options two and four from this tri-variant model, because in those cases, x and y are large at the same time. Similarly, if doing the model selection. And doing the model section for y given x is large gives us the asymptotically dependent only model, and then we can remove options one and three. And in a similar way, if it tells us that we've got a mixture model, that means we've got to include at least one of the components that has x and y big together, and at least one of them that has x large while y is small. So we can. So we can kind of, oh, yeah, and the same thing is true if we look at Z conditioning on X greater than E as well. So we can kind of combine that information into a table. So if we've done our two bivariate fits and chosen one of the three models, we can look at which components we're actually left with for the trivariate case. So they're given by the blue volume. Blue values in the table. So, for example, if we look at the case where both bivariate fits give us asymptotic independence, then that's only going to leave us with mixture component one for the trivariate case. So it's only leaving us with one possible model for the trivariate case. If we look at, say, one of the variables giving us a mixture. Giving as a mixture structure in the bivariate case and the other being asymptotic independent or asymptotic dependent, then again it actually reduces it to only one possible model. So we have to have so sorry, so we're given that in this case, for example, we can only use variable mixture components one and three because the y given x was asymptotic dependent. X was asymptotic dependent, so we can remove two and four. But then, because this was a mixture for Z, given X greater than U, we have to have at least two components in the model. So, that's the orange line, orange value, sorry. So, that again is telling us exactly what the structure should be. So, in eight out of these nine combinations, the two bivariate fits actually tell us exactly what the trivariate fit should be. What the trivariate pit should be. And in the other one, if both of them have mixture structures in the bivariate case, then we can reduce it down to seven possible models instead of 15. Okay, so just to wrap up, I'll just say a few things about what we still need to think about. So obviously, it's nice if the bivariate results can tell us something about the trivariate structure, but we maybe need to think about how much. But we maybe need to think about how much we actually want to rely on those bivariate results. Like, do we really want to throw away all the other models before we pick the tri-variate case? Maybe we can just try and remove some models where we're really sure that it looks like they wouldn't be a good choice. So, as well as thinking about the actual structure, it would be nice to think about whether the mixture probabilities in the Whether the mixture probabilities in the bivariate cases gave us any information about the mixture probabilities in the trivariate case. Obviously, it'd be nice to look at comparing the dependent structure estimation to some of those existing methods that I mentioned before, checking sensitivity to the different thresholds, so where we fit the model and where we choose the model, and then of course extensions to higher and free dimensions. Extensions to higher in three dimensions as well. Everything for me, and thank you.