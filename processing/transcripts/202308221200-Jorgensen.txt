Some slide here, and it is explaining a little bit about the background of the problem. And the background problem actually is mathematical physics, and more particularly quantum field theory, and the early attempts by Arthur Jaffe, Ostevar Dan Swada, and many others to do a renormalization of. Station of the Hilbert space where the observables and the algebras are acting. Of course, in quantum field theory, you know, you have algebras of unbounded operators and they're non-commuting, and that's why quantum field theory is hard. And the idea, going back to the early days, was to renormalize the Hilbert space so that you get a different set of data in the renormalized setting. Normalized setting. And so to do the parallel list of new data is that the Lorentz group or Point Boy group becomes Euclidean group and the quantum fields become random fields. And they have a property that is named after Charleston Schwarter. And in a special case, the Markov property that was studied early on by Ed Nelson. We will see. Nelson, we will see that many of the random fields in the reflected picture will not be Markov fields, but they would have a weaker property that's named after Orsoval and Swadel. And then the other direction, many directions in the meantime, geometry and Riemann surfaces and web reflection obviously. Uh, web reflection obviously have obvious connections and operator theory and spectral theory because you have Wien's functions and they have reflected versions of them. But the part that I was, I've been involved in several of these parts, but the one that I want to focus on now is based on joint work with two co-authors, Gesta Wolofn and Carlam Nieb, and that's in And that's in the framework of representations of Lie groups. And in the example I mentioned a minute ago, you already had two Lie groups, conquer group and Euclidian group. And you know from the definition that you get from one to the other by flipping the time variable with a minus sign. And since i squared is minus one, you'd see that you have to work in complexer form. You have to work in complexified Lie algebras and Lie groups. And so that's what we'll take a closer look at. But before I do it, I want to illustrate some of the main ideas in the absolute simplest case of all. And that is where you fix the Hilbert space where you work. And then we study those kinds of re-normalizations which yield new Hilbert space. Which yield new Hilbert spaces, but the subspaces in the initial Hilbert space would be indexed by projections. You know that in random fields, projections are conditional expectations, and they will take that form when we will be discussing marker properties and related. But here then, Here, then I would like to introduce the definitions in the simplest possible framework, and then I would expand to get a wider setting after we discuss the simplest case. And in the simplest case, you have three subspaces indexed by zero and plus and minus, but you know that closed subspaces in bijective correspondence with projections. So I write them as projections again with subscripts. Again, with subscripts plus, zero and plus and minus, and plus would refer to the forward direction and minus the negative direction or the future and the past. And so we would have to introduce some extra structures so that we can make sense of that. And I already mentioned Markov and OS for the random fields, and we'll come back to that shortly. But the simplest case is where, in addition to the three subspaces, we have operators. And of course, I'll be mainly interested in unitary representation. But if you have a single unitary operator, you can think about that as a representation of the group of the integers. And then you would have a forward direction and a backward direction, and so on and so forth. And reflection is just a self-adjoint unitary operator. That means it has eigen spaces. Eigen spaces plus one and minus one, and the relationship between the unitary and the reflection is that when you reflect the unitary, you get its inverse, but unitary means that the inverse is the adjoint or Pareto. And I'm almost ready to define reflect OS reflection positivity in the simplest case. And so the And so the subspaces that we use for creating a renormalization have to satisfy two properties that are defined in terms of u and theta. And the first property is that you are, let's see, it will be help me if I close here. The first one is that you are invariant in the forward. Invariant in the forward direction, so that's 0.3. And the second property is that when you take the inner product on that subspace and then apply the reflection, then that would be positive. So that gives you then another semi-definite inner product. And we would like to compare the two, but you keep in mind that the new renormalized inner product would refer only to the subspace. Refer only to the subspace that we denote with a sub subscript plus. And this is just a reminder of what a reflection is. Eigenvalue is plus one and minus one. And here write the subspace corresponding to eigenvalue plus one. And the projection can give it a name. And if you give it a name P, then the reflection is to be minus the identity of P minus one minus P, if you like. And this one here. And this one here is meant just as a super quick summary of some details that are listed in the details that are included in the list of references that I'll mention at the end of the lecture here. So we mentioned the closed subspaces and but we can localize them in different ways. Three examples are illustrated here, inside and outside. Inside and outside. In the first example, inside refers to the complex disk and outside the exterior. And for the Scotty dual of a bordered Riemann surface, you have a circle that separates in an inside and an outside, or a left and a right, if you like. And then you introduce local coordinates so that you can make precise reflection as a geometric trend. As a geometric transformation, and that's fine. And the simplest case is where we're just on the real line, then the forward time direction is clear enough. But we would look at some cases where the inside and the outside refers to an interval between plus one and minus one. So those are three geometric examples that you can use for visualizing some of the operated. Some of the operator theory that I'll now discuss in the next slide here. So, now having done the simplest case here with a single unitary operator, I will turn now instead to the case where you will refer to representation of the Lie group. In some cases, when I study representation, I use the letter pi for the name of the representation, and so that's what I use here. So, the pi. So that's what I use here. So the pi here corresponds to the u in the previous slide. And in some cases, theta, when it's realized as an operator, is often denoted by the letter J. That's just historic tradition for how we denote things in the various contexts where we're working. So now I have to make precise what it means that the reflection on That the reflection operator J and the unitary representation have reflection symmetry. And to do that, you have to first introduce a reflection in the Lie group. In the case of the Lie group being the real line, the reflection is obviously plus t to minus t. But in a Lie group, you have many candidates for reflection. The reflections are done from abelian subalgebra so. Sub-algebras of the Lie algebra. And there are various ways for semi-simple Lie groups to introduce parameters so that you get reflections in the Lie group simply being a period two automorphisms. And if you have seen semi-simple Lie groups before, you know precisely what they are. If not, just use the geometry from the previous slide to help you visualize a geometric reflection. A geometric reflection. And so that would be the definition. One way to see when it might be satisfied is that you might have a representation pi as a direct sum of two of them, plus and minus. And then if one of them is unitarily equivalent to the other composed with the reflection tau, then you get the condition we're talking about in the previous. The condition we're talking about in the previous line. And in addition to that, since we are talking about reflection, of course, we need to talk about a subgroup that corresponds to the boundary between one side and the other side of the reflection. And that would be the subgroup that's fixed on the period two automorphism tau that defines the reflection. And so the second, the last condition three. The last condition three is then the same as I talked about before. In some cases, the subspace that generates a renormalization would have a subscript zero, depending on what's convenient for the context. And so that's what we have here. And so again, reminding us of the relationship between the two data points, J and P. points j and pi conditions one and two is what we have here but now i'd like to elaborate a little bit more on the nature of the reflection as as it relates to the lie group in question because you know that that when you have automorphisms in in lie groups or differentiable maps in lie groups they pass to the lie algebra the lie algebra would be written with gothic german letters Gothic German letters and the two sub spaces of the Lie algebra, Gothic G. Actually, the Lie algebra of H, the points that are fixed, is a Lie algebra, but the ones that flip y to minus y is not a Lie algebra. So I didn't, if I call it Q, then the commutator bracket. The commutator bracket of points, two points in Q will be contained in H. And so that's just what reflections look like when they're translated to the framework of Lie groups and Lie algebras. So far, so good. And the next one is: I guess if you're familiar with semi-symbol Lie groups and their Cartan decompositions, K A N, that would be fine. A n, that would be fine. If not, I'll just do a little bit of hand waving. The reflections in the Lie algebra can often be gotten by reflecting in an abelian subalgebra A of the Lie algebra using an element in the K part of the K A and decomposition. And those elements are called Weyl elements and they're used in the study of highest weight representation. Weight representations and they refer to positive weights. And so you have a number L with subscript referring to positive weights and another number nu referring to the parameter of the so-called complementary series of unitary representations. So the language of complementary series and principal series and so on is language going back to Harish Chandra. To Aristandra for the classification of unitary representation for Sim Sim Lie groups, and so far, so good. But now we go back to the forward direction. And so we specify the forward direction by a cone in the Q part of the Lie algebra. And so that would be the cone that specifies the forward direction. So if you put that into the exponential function and multiply, then you get a sub. Then you get a sub-semi-group of the Lie group. And the idea with the renormalization is that we re-normalize the inner product so that the modified representation in the renormalized Hilbert space would be a contractive semi-group as it is restricted to the semi-group of the Lie group corresponding to the forward direction. Corresponding to the forward direction, and the reflection can be written this way here in the bottom line. And it refers this for this particular example here, it refers both to tau and to the while element w. And there's also some of the examples I showed you with omega in a complex form referring to the disk or the half-plane. Or the half-plane or the Riemann surface in the Lie setting here, it can be realized in the coordinate system for the Lie algebras. And that would be the omega that we'll be referring to. And so we will be completing in the renormalized subspace coming, the one that we denoted, K0. And so the next slide. The next slide here is making precise what the representation looks like because when we renormalized, remember we changed the Lie algebra from one type to another type. In the simplest case, we looked at it was from the Lorentz to the Euclidean by flipping with a minus sign in the time direction. But in the Lie algebra, we would be multiplying by i because i squared is minus one. And so you can do that. Minus one. And so you can do that for any kind of splitting h comma p of the Lie algebra. And so now what we see is that we can complexify both the Lie algebra and the corresponding representations. And so we saw that the representation, when it's passed to the renormalized Hilbert space, becomes a Space becomes a contractive representation, so the generator would be self-adjoint and semi-bounded, and we put an I in front of it, and you get then a skew adjoint operator, and that would be the generator of the complexified Lie algebra. And so that's what conditions Roman 1 and Roman 2 refer to in part 3. We don't change the representation. Change the representation on the H part, but we change it on the P part. And that's what condition two and part three looks like. And in the special case of the Lorentz group and the Euclidean group, that's precisely what you have to do to modify it with the time direction, the way we talked about it. The way we talked about it. And so that was representation theory. And now I want to say a little bit more about the Ostevalda-Swada approach to the axioms of quantum field theory, usually referred to as the Whiteman axioms. And so again, I remind you of what we talked about. We talked about a Q-part in the Lie algebra and we talked about a cone. Cone for the forward direction with non-empty interior, and it's called hyperbolic if the add x transformations in the Lie algebra have real eigenvalues for every element in the interior of the chrome. Add x is just the name we use for the operator that takes commutator with x. And so the And so the properties we talked about referred to how we get a semigroup from that cone and how the representation restricts to become contractive on the semigroup and the properties that are required for the reflection operator J referring to a subspace in the forward direction. So when we write PR3, that is saying that That is saying that the subspace is pointing in the forward direction with reference to the representation pi and the column C. So that is what that refers to. And so one of the theorems in my joint paper with Olafson here is that when we have representations of Lie groups with reflection positivity, then we can go. Then we can go from the group G to the complexified part of it. And so I remind you again that if we have a splitting of the Lie algebra, then you put an I on the second part in the Lie algebra, the Q part, and then you get a new Lie algebra. And you might know that you can characterize a compact Lie algebra cosplay. Lie algebra corresponding to a compact Lie group by having its killing form to be strictly negative. And so if you have a strictly negative killing form, then you'd have a compact case. But if you flip a sign in the queue, then you'd get a non-compact case. And but we'll be paying attention to the interplay between the two representations, one before we renormalize and the other after we renormalize. After we normalize, and the superscript C is referring to the latter. And the reason we use a superscript C is that it takes place in the complexification of the other Lie group. And so again, if we are interested in cones in the Lie algebra, they would have to refer to the representations because the properties we want for the operators will refer to. Want for the operators would have referred to the representation, and this is just a bit of notation that's reminding us about that. And so, the next thing here is just for folks who might not be familiar with the language of the identification of representations of Lie groups in Hilbert space. And in this case, here we'll be paying attention to these. Attention to these combs in the four directions and the Lie group and the subgroup before we do the complexification, after we do the complication, and the action of the Lie algebra coming from just differentiating the action in the Lie group. And so that's just a bit of notation here. The other bit of notation is the way the subgroup The subgroup K is acting on elements in the representation space. So if capital X is a Lie algebra element, and if little K is a Lie group element, then the extension of the action of the Lie algebra on the vector space is that the K action on a vector of the form x dot v becomes The action of the transformed Lie element acting on the transformed vector in the Hilbert space. And if you write down the definitions of Lie actions, that's the only thing you can really do. And the next one here is again, highest weight representation and the language for that. If you've never seen them before, just take that as a definition. As a definition, but you can also, in case you're familiar with Fox Base and raising and lowering operators, you'd be able to see a parallel between that and the different three properties that I'm talking about here. And having that in mind, then we're able to state the first theorem about existence of reflection representations coming from a highest weight module. And so, as I said, the weights are the eigenvalues in the Lie algebra. And highest weight is that there is such a parameter nu referring to a highest weight, which means that anytime you take these combinations of the other weights, then the difference is positive. So that means that the weight new is highest in the order. Nu is highest in the ordering of the weights referring to linear functionals on the elements in the sub-algebra of the Lie algebra of the representation in question. And so the theorem says that we can do the reflection positivity construction from one representation to the other. For example, if the representation you start with is one of the principal series representation. One of the principal series representations. Then, after you do the renormalization, you typically get a complementary series representation that has a complex structure. And so that refers to the spectrum of the generators of the mass operator being positive so that you can do an analytic continuation. And so that would. And so that was Li theory. But I mentioned that we also then want to establish that we can then, after having passed to the reflection picture, then instead of looking at quantum fields, we can look at random fields such as Markov fields or more general OS positive fields. Those would be systems of random. Systems of random verbs. So random verbs are just measurable functions related to your probability space here. And so they form abelian algebras as opposed to quantum field theory where you have non-abelian algebra. So that's one of the things we accomplish by passing to the reflection picture. And the purpose of this one here is just to flesh out a little bit more what we mean by reflection. Bit more what we mean by reflection. In other words, we have, let me take some more again. We have these subspaces and the reflection operator. But if we have to create a new Hilbert space, we have to pass to the quotient by this n, and then we do a Hilbert completion of the quotient. And so that's what we mean. And so that's what we mean by the renormalized Hilbert space. And so all the operators we had in the initial Hilbert space would then be translated into the operators in the renormalized Hilbert space. So, so far, so good. And then the thing that we'll be looking at in the re-normalized Hilbert space is we Space is we're looking at Markov properties or OS properties or Stavaldesvader type properties, which is a bigger family of random processes. But the definition of the properties we'll be looking for in the renormalized random field framework can be defined in terms of conditional expectations. In terms of conditional expectations. So I'll just mention the easiest one first, which is this Marko property. And if you heard about a little bit about random verbs, random processes, you would know that Marko means forgetting the past. And so, what it says is that if you condition the past, if you go to the future and condition by the past, then you. Then you only need the present information, so that means subscript zero. And so it says that all everything you do by, if you go to the future and condition by the past, you only need information about the present. Sorry, can I ask a question? Yes, yes. Yeah, sorry about that, yeah, but so just to make it clear. But so just to make it clear, where is the randomness here? I mean, I mean, the random variables would come towards the end. Maybe I'll just go to the end here. A few more examples. Because this looks that we can do a lot of things. We can do a lot of things in the re-normalized Hilbert space, and we can represent the vectors in the space by two parameters, V and an element from the group. And then the random variable system is that I think in this example you were just talking about. You were just talking about, you have some scattering process in mind. I didn't introduce it yet, and then the reflection becomes d to d inverse, and the covariance would then be specified by a covariance operator indexed by the two group elements, and it'll be contractive. And if you are in a forward direction in one real variable here, then you would have it be positive definite. And if it's Positive definite, and if it's also positive definite in the plus form, then that corresponds to reflection positivity. And the marker property corresponds to the covariance operator being a semi-group of operators. So that's what we're aiming for. But I should probably have mentioned that earlier. So I'm glad you asked. So that's good. Thank you for explaining. Yeah. So there's a little bit of more time. So, there's a little bit of more technical points here. Again, when we introduce the renormalization, we go to the quotient and that's written by Q. And then you restrict Q to that subspace and we give it a name gamma. And so if we cut down with the projection, you get a multiplicative operator both before and after you re-normalize. And so that's handy to have. And so that's handy to have, and we definitely need that as a lemma when we're going to talk about the random fields towards the end. And this one here is again kind of the beginning of the construction of the random fields. And the construction of the random fields is done the usual way with the Kolmogorov consistency and extending from cylinder sets. Right, and so that's the technical details that's required for constructing a probability space so that the operators before we were normalized then induce new operators in this probability space here. And so, here I'm just stating it as a theorem without proofs. But one of the forms that it takes is that the semigroup in the probability space can be made precise as a convolution semi-group, as a heat semigroup in some special cases, but the semi-groups of measures are more general classes of measures. The case when they are semi-groups is a Markov and R semi-groups is a Markov, and we'll see that there are many examples of non-Markov. And I'll the so right, so I'll simply list some of the generating covariance operators when they are semi-groups and not semigroups. And so the semi-groups with the Markov property and the non-semi-groups. The Markov property, and the non-semi groups would be the Osterwalder Swada, and it's only a relatively small family that have the Markov property here. And so this is again a little bit more terminology of the induced operators in the probability space. For example, the first triangle here is The first triangle here is the plus subspace. And you know, in probability theory, you define a forward direction by the sigma algebra generated by the random fields in the forward direction. And so that's what the subspace looks like in the probabilistic setting in the renormalized space. And similarly, with zero referring to the present. The present and then cutting down with the conditional expectation this way here is what we do. And so the next one is simply just that the matrix corner of the unitary action in the forward time direction becomes a semigroup in the case of. Semi-group in the case of Markov fields. And so that's one more piece of information, but without proofs. And the next focus here would be comparing different random fields, the ones that are Markov and the ones that are not. And this one here, I should have mentioned earlier, is the diagram. Diagram that gives you all the technical steps going into renormalization. On the first line, you have the unitary operator, and on the second line, you have the subspace, and on the third line, you have what happens when you pass to the quotient by n and do the Hilbert completion. So that means that the representations in the top line become representations in the Become representations in the bottom line. And so if the top line, you have come, if you have reflection positivity in the top line in the forward direction, then in the bottom line, that becomes a self-adjoint contracting semi-group. And so, but it'll be different operators, both different Hilbert space and different, well, with the Hilbert space before and after you re-normalize is different. Before and after you re-normalize is different. And so the operators are different. And that's a fairly direct way of understanding them. It's highly non-trivial to show that if U is reflection positivity is positive relative to a forward direction, then when you construct the corresponding U tilde, that becomes self-adjoint and contracted. That's highly non-trivial, but it's part of the general construction. Part of the general construction that goes into this. And so the purpose of the next one here is just to give you one particular example of relating to everything we talk about to unitary representations of Lie groups. And so this is the baby example principal series representations of the Lie group SL2R, which can also be thought of as a quadratic. A quadratic form with a plus and a minus sign. And so you will notice here that this is a singular integral and so it requires a bit of careful analysis to justify the Hilbert completion with respect to a singular integral. And again, SL2I is a semi-simple group, so that's a KAN representation. K is the rotation. This is the rotation, one-dimensional rotation group, and A is just a multiplicative scalar with A being positive. And so the unitary representation restricted to the A part is given by 4.2. And you will see that if you then take test functions supported in interval for minus one to one, that was the inside versus outside that I'm. The inside versus outside that I mentioned in the beginning. Then these operators will be invariant under the forward action in 4.2. And they will have the positivity condition if the reflection is defined by 4.3. And so we have all the axioms satisfied, but not only that, you can calculate directly what happens to the inner product when. Uh, when it's initially given in 4.6, then with the kicking in the reflection here, it simply just corresponds to x into 1 divided by x. That means that the singular kernel in 4.6 becomes a fractional real version of the Zieger kernel. There's also a version of that in the complex case where instead of an interval for minus one. An interval from minus one to one, you have a disk with a radius one, and then you'll be a complex Zigger kernel. And both of them are interesting, but just to make it simple, stick with the real case. And again, you can write it in the Fourier transform picture in equivalent form. And so here you then have a group G and the new group. The new group G C, superscript C, and you have the quotient map, and you have the two Hilbert spaces: one a Hilbert space from a singular kernel, and the other from the EC contractive Zigger kernel. And this is just writing down that this example, you have the conditions satisfied. The conditions satisfied that we talked about. And maybe I say that the OS with parentheses theta is referring to 4.11, because you might have OS for some theta, but not for other thetas. So we'll be paying attention to systems of projections and whether they have good properties for some reflections. For some reflections and not for others. The next one here is pure Hebrew. I'm sorry to interrupt. You have like five minutes including questions. Including questions. Okay, very good. I think I'm fine. So I'll skip the one about Hilbert spaces, the contractions, and how contractions give us new reflection operators. So the main thing I want to do in the remaining couple of minutes. To do in the remaining couple of minutes is point out that some examples might be OS but not Markov. So I mentioned Markov is 4.18 and reflection positive is in the previous page. And so I mentioned that 4.18 implies the Markov and then as a DeMarkov, and then there's another theorem that says that if you have a projection system that has the OS property for all reflections and then intersect over that, then you get Markov. But you can easily have some Markov and some reflections that don't have the Markov property. And so this is again repeating the definitions here. Repeating the definitions here of Markov versus OS. And this is the proof that Markov implies OS, because to have OS, you have to be positive when you squeeze the reflection operator between the two, between E plus on the left and the right. And then if you use the axioms we talked about, you would see that E plus theta E minus. That e plus theta e minus by a little bit of algebra is the same as e plus e zero e minus. So you'd see that that Markov implies OS, but not the other way around. And so that's that. And the next one is what they look like when they come from an action on a vector space in the re-normalized setting. Reflection is G to D. Reflection is g to d inverse, and you get random fields with covariance operators. We talked about them. And in the case of the real line, you get positive definite and the reflection positive definite. And this one here was a beautiful example by Abel Flein in the late 70s. And so again, semi-group. Again, semi-group corresponds to Markov, again motivated by Ed Nelson where the Markov fields for more general cases here. And here, finally, we have four easy to understand examples. Number one is the famous Ornstein-Uhlenbeck process with its kernel, and that is Markov. As we would, anyone who's worked with random fields would know that this is. Random fields would know that this is the simplest Markov. But then you have the other three, and the other three come up in the Osterwalder-Swader and Giaffe analysis in the simplest form. The other three come up as covariance kernels this way here. And you can easily check that they are definitely not Markov because we have classificational Markov semi-groups, but they come up. But they come up in a natural framework for this setting here, where they are the equivalents for the one parameter systems correspond to Walter-Swada theory. And so that's what I wanted to mention here. And here's a little bit more about the kernel operators more generally being indexed by. Being indexed by a semi-group inside the Lie group that we mentioned in the beginning, and the properties are what you would expect them to be. And then finally, the papers that you can refer to if you want to study the proofs of some of the things we talked about. There's the most recent paper with my two co-authors, Nieb and Olafsson, is number one, and a paper going a while back that I have together. A while back that I have together with Olafsson is in GIFA, and there's also one in a memorial volume in honor of Harris Chandra and Vic Siegel. And that's this one here in the Proceedings of the Symposium Pure Math. And finally, one that I had in the GAFA that was actually also a memorial issue of GIFA in honor of Insell. And then And then finally, a more recent one with my co-author Zhang, where we classified these reflection systems purely in the language of spectral theory in Hilbert space. And then the last one is that beautiful paper with the very detailed computations and examples by Abel Klein, again motivated by a paper a little earlier in JFA. In JFA by Ed Nelson. This one here is in the Journal of Probability Theory, but it was in the 70s where it still had a German title, Warscheinisch Keitu Wee und verman gibied. Okay, so that's all. So thank you for your attention. Sorry for rushing. Thank you very much. Any question? Rather quick. Rather quick, yeah. I just had one question. Is it possible to define reflection positivity for actions of von Neumann algebras or sister algebra? Yes, it is. Yes, it is. Excellent question. And Roberto Longo is one of the pioneers in that direction. Oh, I see. Okay. And also, so Khan's spectral triples. Yes, absolutely. Yes, absolutely. I see. Very good. This could be useful for action of quantum groups, for example, stuff like that. I'm so happy to ask. I've worked a little bit on that, but the pioneers there would include Bobato Longo and other of his collaborators who formulate this in the language of concept. Very nice. Okay, thank you. Yeah, very good. Thank you. There is a question from the audience here. Yeah. Yeah, just how much of this goes true for coset spaces rather than groups? Oh, almost everything. Sorry? Almost everything. So the papers by Olafsson and myself, they do the coset spaces. Yeah, so the coset spaces are modeled for the geometries that. geometries that that that that are at the heart of of these constructions very much yeah so that would include the siter and the oh yes yeah and coarsal we talk about coarsal uh spaces so those are homogeneous spaces that have a coarsal direction so a coarsal refers to a forward time direction so that's very important part in my work with osteval with the olafs and and also all of uh and and And also, all of the ones with two co-authors, Olofs and Nipa. I'm glad you mentioned that, yeah. And especially those two have even a whole book on causal symmetric spaces. Okay. Okay, thank you very much. And we move to the next talk by Raphael Pons: No Commutative Geometry and Semi-Classical Analysis. Please share. 