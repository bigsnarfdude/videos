Conserving solutions to be generalized, Gangbacks are equation. So, um when I originally planned this talk, I thought I was going to be um before Fiona. I'm now after Fiona, so a lot of the stuff you have seen before, I hope you like it because you're going to see it again. Right, so I'd just like to get the notation and this general basics very solid. So, here in our case, a Yang-Baxter operator given a vector space, V, a Yang-Baxter operator is a transformation. We're going to call it an automorphism of the tensor square. We're going to call it an automorphism of the tensor square of V, linear pedestrian, which satisfies the following. We define R1 as R tensor with the identity of V on the left. R2 is R tensor with the identity on the R2 is on the left, R1 is on the right. R is a Yang-Baxter operator if it satisfies the Yang-Baxter equation here, which is R1, R2, R1 equals R2, R1, R2. Again, very similar to the Brave relation. In fact, these yield representations of the Bray group, the N for any N. Group, the n for any n, which given here is just this algebraic presentation, which of course you've seen before today, as generated by sigma 1 through sigma n minus 1 with these braid relations and the far complementation relations. So all you need to do is send sigma i to a copy of R tensored with an appropriate amount of identities on the left and the right, depending on how many strands you're using. A generalized Yang vector operator is as follows. So given a vector space V of dimension n and two positive integers. dimension m and two positive integers m and k. A generalized n-vector operator is a transformation on not the tensor square but the nth tensor power of v, for which if r1 is our tensor with the identity on the right not just once but k times, and r2 is our tensor with the identity on the left, again k times, we have the familiar r1, r2, r1 equals r2, r1, r2. We call this for m and k the generalizing maxim equation. Again, these with a little caveat will yield With a little caveat, we'll yield representations of the break group. We can see that I satisfy our break relation right here if we send all of our generators to R tension with appropriate amounts of k times the identity on the left and right. However, in the usual Yang-Baxter case, we get all our far commutation relations for free, basically because naturality of the tensor product. Here, this is not the case, sometimes. You must also verify these far commutation relations separately, essentially because. Mutation relations separately, essentially because in some of these equations, your two R's are going to overlap on some tensor factors. And so you just need to verify separately that those are true. Eventually, if you get far enough commutation away, they will act independently on separate tensor factors and you'll be fine. So for today, for just my talk, we'll note that R1 Rj equals Rj R1. It's going to be non-trivial if J times K is less than M. For this talk, we're going to define generalized J max drag. Talk: We're going to define generalized A-max-der operators, use notation a little bit, just say that they do satisfy all relevant Farcon notation predictions. So now we're talking about charge-conserving versions of those things. So what is a charge-conserving matrix? It is, first we consider a basis given by ket0 through ket n minus 1 for a vector space v. We'll call it a product basis of the tensor, the amp tensor power of v, just given again through rocket notation. So, a linear transformation on the amp tensor power PPB is going to be charge conserving if, let's see, if it fixes the span of, leaves invariant the span of this following set. So essentially, this is what we get if we take a basis element and we're allowed to permute all the indices inside that basis element. Again, we've seen this before, so hopefully, I can glance over it just a little bit. Hopefully, I can gloss over it just a little bit. We take that set of vectors, we take its span, and all those spans are invariant. I'll get an example, which we've seen this exact example for today, on for dimension v equals 3 and on acting on two tensor powers. Each subspace, ket ij and ketji, that span is going to be left invariant. So there are some one-dimensional subspaces and some two-dimensional subspaces. And naturally, we can see that if we start acting on more tensor powers, Start acting on more tensor powers, this gets a little more complicated. So, say on three tensor powers, we might see, say, ket000 spans just a one-dimensional invariant subspace. Ket112 might, so you can take that two and permute it to either middle or the left. So there's a three-dimensional invariant subspace there. And if they're all different, so if I take ket 0, 1, 2, that's going to be part of a six-dimensional subspace. That's left invariant. Invariant. Let's see. So, summarizing the results of Ralan Martin on classifying charge-conserving amvextor operators. So, for a charge-conserving operator R, let U be some subset of the relevant indices here, so a subset of the integers from 1 to n. Let v sub u, again just a notation, be the subspace spanned by the product basis vectors only containing indices in u. So by the charge conserving property, these things are all going to be invariant. These things are all going to be invariant. They're unions of things that are left invariant by the property itself. So, if R is itself a charge-conserving ang vexator operator, then its restriction to any one of these subspaces given by U is, of course, is going to be a charge-conserving ang vexor operator. The really interesting part in the power here we get is that if R sub U is a charge-conserving ang vexor operator, for all U, and we only have to check size of U up to 3. Size of U up to 3, then R itself is going to be a charge-conserving Yangmaxer operator. You don't need to deal with any more indices than 3 at a time, essentially. Right, and again, we've gone over why this is true, but just to recap, when we verify the Yang-Baxter equation on something like this, we're doing it on three tensor factors. So, three is the maximum amount of distinct indices we could see at any given one time. So that's why we only need to check restrictions on the subspaces up to three indices. In the generalized Jangbaxer operator case, we can do very much the same thing. So if R is a charge-conserving generalized Jang-baxter operator on M and K, then R restricted to U for a similarly defined U is a charge-conserving generalized Jang-bax-to-operator for all those U with the same M and K. Now we get another interesting converse here, where if R sub U is a charge-conserving Yangbaxter operator for all subsets up to a certain size, which I'll describe what that is in just a second. This is also written wrong, so please ignore this just a little bit. Then R itself is going to be a charge-conserving ambax or operator. We don't need to go any further. Now, some intuition on why this is true. Well, so. This is true. Well, so the case I'm mostly considered with right now is m equals 3, so we act on three tensor factors, and k equals 1. So the distance between each sigma is just going to be one tensor factor. This is the generalized Yang vector equation in that setting, and we can see that it's verified on four tensor factors. So the amount of distinct indices we can see at any given time verifying this is four. We need to check up to dimension four just for this generalized Jang-Maxer equation. This generalized Jang-Maxer equation. Now, of course, there are, in this case, again, m equals 1, m equals 3, k equals 1. There is a far commutation relation we need to check as well. So R1 and R3 do not necessarily commute, just for free. That equation gets verified on five tensor factors. So really what we need to do is you need to take your biggest far computation relation that needs to be checked on its own. It doesn't come for free. Checked on its own. It doesn't come for free. And you need to see how many tensor factors that acts on. That's the maximum size of subset you need to take, and that's what this expression is supposed to give you. Right, results. This is currently in progress. I've checked everything up to dimension 4 and have some code to check dimension 5. So it's very close. So, to begin with, if R is a charge-conserving Yang max or operator, usually Yang Vaxter operator, not generalized, with dimension v equals 2. With dimension v equals 2, that is a 4x4 matrix. So if I take its direct sum with itself, I'll get a block matrix with two 4x4s. It's an 8x8 in total. Which is exactly, since I'm acting on three tensor factors here, if I have dimension d equals 2, I'm looking for 8x8 matrices. So you can ask, is that a charge-conserving, generalizing matching operator here? And in fact, it is. So solutions of this type kind of give rise to a family of solutions that looks very, very similar to the ones that we saw. To the ones that we saw in REL and written classification. In fact, they have basically the same combinatorics. A priori, you might not expect them to, let's see, you might expect some additional relations from, or additional constraints from the additional equations we'll be able to satisfy. But indeed, fairly certain there are none. There is also, I'm going to give you one explicit matrix here, and if you would like more, feel free to come talk more about the root. Feel free to come talk about the root. This new solution, which is not a direct sum of something coming from a previous result, basically you've got these three free variables here. And you can verify, if you have maple or what have you, that this satisfies the equation as well. Let's see, we have our reference right here, Martin-Rawls classification, and thank y'all so much. Thank you for the ones. We have time for the question. Why are they called charge-conservative? Let's see. Someone answered this very adeptly after Theodore's talk, and I'm blanking a little bit right now. Sorry. So, the result you had, what, two slides ago about this direct sum. Do you like expect this to generalize? I mean, this is very specific numbers, this two, three varying. Like, do you expect this to? Let's see. I don't for the following reason. So, we're taking a direct sum here of, so dimension v equals two, so this two. equals 2. So this 2 squared is 4, direct sum of 2, 4 is going to be 8. So this is, we can all kind of do this because like 4 times 2 is equal to 2 to the third, if that makes any sense. And let's see. To build, it just kind of seems like a specific consequence of the specific numbers involved. The specific numbers involved. It's something I do want to think about a little more, however.