About measuring conditional independence, which is one very particular definition of group type fairness reduces to the particular kind of conditional independence I'm going to talk about. So I'll mention that in a minute. Before that, this conditional independence thing also shows up in other cases. So I'm going to motivate this by an out-of-distribution generalization type problem because we had slides and pictures for it, and I wasn't going to make slides and pictures for the fairness version. Before I start, Before I start, this is about two papers. One is this. The slides are online, all right. The other paper we submitted on Saturday morning. It'll be on archive probably later this week. Didn't realize that the Google, we have to get the Google approval process for archives separately from Submitted. So we'll get a little bit. But yeah, work with a bunch of good students and a couple professors. Yeah. Here's yeah. Okay, so there we go. So here's the out-of-distribution generalization motivation that I'll start with. So imagine we have a car driving around, and one like small sub-problem you might want to solve in a car is, oh, maybe my GPS isn't working well, so I'd like to be able to look around and tell where I am in the city. It's a thing people can do, so maybe it's a thing that'll be useful for your car to do. So you drive around, you start in the morning, you're driving over there, you're in the You start in the morning, you're driving over there. In the afternoon, you're over here, in the evening, you're over there. And you, you know, you keep your pictures, and then those are your pictures. You learn a model to go from location to position. But then the next day, you start in the afternoon, and the lighting is all off. So your predictive model thinks, oh yeah, this part of the city is dark, and this part of the city, it's bright. So now your model is totally wrong, right? Because you had distribution jet. So, how do you? So, how do you avoid that? One way to avoid that, well, one way to avoid that would be to drive around the city at lots of times a day, right? But another way to avoid that would be if you realize this is a problem, you could try to explicitly enforce, I would like my prediction of the location to be conditionally independent of the time of day, given where I am. If you just make the prediction independent of the time of day, given this training set, then you're going. Given this training set, then you're going to break everything because the position and the time of day are dependent on each other because you are in different positions at different times of day. So, but if you do this conditional independence, then it's good. And then you learn a predictive model that doesn't incorporate the time of day into its predictions if you manage to enforce this condition. This also comes up in fairness. This is the, oh, sorry. The, oh, sorry, before I get the fairness, there's something called domain invariance, which is almost the same thing as out of distribution generalizations, just like a particular, slightly different framing. In that setting, it's the same structure. We would want to learn features, which are independent of which domain we trained, like this input data is coming from, given the predictive label. If that doesn't make sense, don't worry about it. I'm not going to talk about it in that. Fairness. One of the most popular group fairness type structures. Group fairness type structures these days. It's called equalized odds. And one version of it is to state that the predictions of your model should be independent of whatever protected attribute you're trying to protect against, given the true label. So if you're trying to do some kind of predictive problem, like you're doing some medical problem, maybe the true need for this treatment differs based on sex, but you don't want to discriminate against. But you don't want to discriminate against anyone based on sex, you just want to have good predictions. This would allow you to say, well, given that I need the treatment, the prediction of whether I need the treatment doesn't depend on my source. So that's one of many incompatible definitions of fairness, but this is one of the popular ones these days. Again, also, as Sammy pointed out, and it was mentioned a couple times today, just to reiterate, if you manage to perfectly Reiterate: If you manage to perfectly enforce this, that doesn't mean your model is fair in fairness, right? There's all kinds of things that go with the fairness. This is one mathematical framing. But given this mathematical framing, I'd like to be able to find a model or, et cetera, some kind of features or predictive model or something that satisfies this condition at least approximately. Also, this is what the second project is about. I'd also like to be able to test this. About. I'd also like to be able to test this, right? Given some data, either someone else trained a model and I'd like to know whether it's fair, or I have some data. I guess this is kind of an instance of that, but we ran on this paper on an example of some data some people had made of car insurance prices, whether the price that you pay for car insurance is independent of whether you live in a minority neighborhood with some particular definition of what minority is. With some particular definition of what minority neighborhood means, given your risk as a driver, again, with some definition of what risk means, right? So that data set basically determined that these things are not conditionally independent and pricing models seem to be discriminating on the basis of living in a minority neighborhood. But that's a different thing that we would have to look at the data and tell whether that's true. Whether that's true or not, right? So, how do we tell, looking at some data, whether this conditional dependence? Problem is, that's hard. So, I'm going to use, this notation's a little bit weird. It's motivated by this predictive case. I'm going to use x being independent of z given y. So I'm always going to be conditioning on y. So, if I'm conditioning on a discrete variable, and there's only a few values of that discrete variable, like it's, okay, yeah, either it's. Either it's male or female. In my data set, I have binary markers for that, and that's what I conditional. In that case, you could just take the subset of the male data, the subset of the female data, and in each of those, do a marginal dependence test or measurement or whatever for these two variables, like in each of those subsets, and then you aggregate between those subsets in whatever appropriate. But if you have a lot of categories, Categories, this becomes less practical, right? Also, if you don't have very much data in each category, this becomes not practical. So, even if you have a discrete value, but you're trying to do this, like you're training a deep network, and you would like your deep network to satisfy this conditional dependence as you train, then probably you need to be checking this measurement at every mini-batch during training, right? And even if you have a ton of total data available, measuring this on a mini-batch. Measuring this on a mini batch of whatever 64 samples or something, it might be really noisy to measure, and so then hard to estimate this efficiently, right? The other thing is that a lot of times you're conditioning on things that aren't actually discrete, right? You're conditioning on some continuous variable. Maybe if it's a 1D variable, you can bin it appropriately, but then the binning is usually going to introduce some dependence of its own, and so you have to be really careful about all that kinds of stuff. Or you can make really strong assumptions. Or you can make really strong assumptions about the form of your data. So, if you assume everything is jointly Gaussian, then this reduces to checking the partial correlation, which is also equivalent to you just take the precision matrix and you look whether a particular entry is zero or not. If you assume everything is Gaussian, then you can do stuff with that. That's relatively easy. It's not trivial, but you can define a statistical test that's kind of right. But if you don't want to assume Gaussianity, and if you're training a deep network, probably you don't want to be. Training a deep network, probably you don't want to be assuming calciumity, right? Then, what do you do? There are approaches to do it. I'm going to talk about one family of approaches. There are other ones, of course. Okay, so to start with, let's think about how, okay, so let's think about maybe this case where I'm doing conditional independence based on marginal independence in a subset. And I'll ask, okay, how do I measure marginal dependence? Right, this will be a warm. A warm-up for what we're going to do conditionally because a lot of the same tools will apply. So here is some particular model. Don't worry about the exact specifics of the model, but you know, here is the model. Here are samples from this model. I have some X and I have some Z. These are not correlated, but they are clearly dependent. And so how do I tell that they're dependent? So one thing we can do is we can look for some nonlinear function f of x and some nonlinear function g of z. And some nonlinear function g of z, that then the outcomes of both, of the f of x and g of z, would then be incorporated, right? If x and z are independent, then there won't be any such function. I won't be able to find one, right? But it turns out, if they are, there are sorry, if they are dependent, there always exists such a pair of functions that will satisfy this. And there will even be L2 functions, like square integral functions. So here is one. So here is one such example. There are many for this problem. f equals x here. I don't have to transform x, but I'm going to square z. So I transform from this problem to this problem, and now there's a very clear coordination here, right? So by transforming this data, I can now easily tell that the stuff is dependent. Okay, so x is independent of z if and only if all such L2 functions are unbelievable. Two functions are uncorrelated. So if I can find a particular function that gives me correlation, then I can be sure that they're not dependent, as long, you know, up to issues and not like overfitting type issues. But how can I be relatively confident that they're not, right? How do I check every L2 function? Well, obviously I can't check every L2 function, but I can check enough nonlinear functions to Nonlinear functions to then be confident that no LL2 function exists. And the way I can do that is with kernels. So some of you who are old enough to remember sport vector machines know what kernels are. Some of you might not. So if you're not familiar, one way to think about kernels is that they, so a reproducing kernel Hilbert space in RKHS is a space of functions that That more or less is functions that look like this. If I choose some k that is some pairwise similarity function, maybe like this, this is like a Gaussian kernel or a squared exponential kernel or an exponentiate quadratic kernel, depending on who you are. There's probably a million other names too. And then I can take linear combinations of these functions centered at different points. I add those up. You can have negative weights on the alphas too. I don't know why. We only do them positively. Too. I don't know why we only drew them positive. And then the RTHS is a Hilbert space of functions that is all functions that look like this, plus a few more to make it Hilbert space. So depending on my choice of kernel, this then gives me a set of smooth functions. And if I use a rich enough kernel, it's a really big Hilbert space of functions. It also turns out from our gay. It also turns out from ArcAHS properties, we could go through it in five minutes, but I don't have time. Oh wow, I really don't have time. It turns out that if you take any two functions f and g in the RK address, there is an operator, so operator being with the Hilbert space analog of a matrix, there is some linear operator that looks like this, such that the covariance between f of x and g of z is exactly this like bilinear form. Is exactly this bilinear form with this operator. There's some mild assumptions about the distribution of the kernel here, but almost hopeless this is true. So we can evaluate this covariance that we're looking for by doing this bilinear product. This matrix, if I use a linear kernel, then the RKHS is the space of linear functions. And this matrix is just the, this operator is just the normal cross-covariance matrix, if you mess on something. Okay. Okay, so if I look at this operator, and this operator is the zero operator, like it's the linear operator that always maps anything to zero, then that tells us that every function f and every function g in the RKHS, their covariance is zero, right? Because if I plug in zero here, then this bilinear form is always zero. So that would then mean everything in the RPHS is uncorrelated. In the RPHS is uncorrelated, and so if we have a big enough RPHS, so something like a Gaussian kernel, this is true, this then implies that every L2 function is uncorrelated, and so then these things are implied. So there's something called the Hilbert-Schmidt independence criterion, HSIC, which is exactly a statistical test based on this, or a measurement of dependence that you can construct a test out of. It just measures the squared Hilbert-Schmidt norm of this operator. Hilbert-Schmidt norm is just the first. Operator. I would try it normally. It's just the Houvenius norm, but in infinite dimensions, etc. So this is a measurement which at the population level is zero if and only if these random variables are implied. So then I can estimate this thing. Here's the estimator, it doesn't really matter. And then if the estimate is small enough that I think, you know, if the estimate is large enough that I'm confident the true value isn't zero, then I can be confident perfect. Isn't zero, then I could be content perfect. Okay, so if I want to train a deep network so that these features are approximately independent of Z, then I can add this as a regularizer during my training to try to keep the HSIC value low. And there's some details here about proving that this actually works, but intuitively, if you have a reasonable value of gamma here, then this is going to give you a deep network whose features are approximately. Okay, um what's the conditional version of this? Well, here's a problem now where I've made a conditional, it's the same model, actually, but now I'm thinking about this y as a thing that I'm conditioning on. And then here's what this looks like, where I'm now also plotting y based on color, right? So now x and z are dependent. They're also conditionally dependent given y. And that conditional dependence goes. And that conditional dependence goes through this, like psi one. So, how do we detect this? Or how do we characterize when things are conditionally independent? I'm gonna go maybe a little bit too quickly, but there's this really nice. So here's the previous thing where we said dependence, like marginal dependence, is characterized by correlation of L2 functions. So, conditional dependence, you know. So, conditional dependence, there's details to work out, but it would make sense that conditional dependence then could be characterized like this, where I choose a different function depending on y. And then I take conditional expectations depending on y. And so if these things are equal for every y, or almost every y, then we would call this conditionally independent, right? Because they're dependent for any value of y. Now I can take this, and I can, instead of choosing a different I could take this and I can, instead of choosing a different function, I could just make it a two-argument function, right? Depends on both x and y. Instead of having a different function for each y, it's the same thing. I'm just writing it differently. Or almost the same thing. I mean, the definition is a little bit different, but it's almost the same. So there's this classic result, Godin 1980, that says this condition is equivalent to saying to this condition. So what is this condition? So this f tilde is a function. So this f tilde is a function that is conditionally centered in y. It's a function of both x and y. For almost any value of y, its expectation, its conditional expectation is 0. Same thing for g, except it's a function of z and y. If you have these conditionally centered functions, then you have this conditional independence if and only if you have like marginal, I mean, as written here, it's just the expectation of their product, but they're each mean zero because they're centered. They're each mean zero because they're centered. So, this also means the covariance of these functions is zero. It turns out you can also drop one, like for one of these functions, so without lots of generally, let's call it the x one, one of them doesn't have to depend on y. It's okay to have f just be any function of x, and then the other thing needs to be centered, and then it's still the same. Okay, so we can use this characterization of conditional independence. Characterization of conditional independence to then build our conditional independence test in exactly the same way. We can find an f and a g. So now f is a function of x again, g is a function of both z and y, but this function ignores y, because in this case you don't have to depend on y, although in general you might have to. And then you get some correlation here. You center this in terms of y, which for this function, because it doesn't depend on y, is just like subtracting it, whatever, it depends on the y. Subtract it, whatever, it depends on the y instead of the, you center it in terms of y, and then you get this correlation. And in this case, you still again have a correlation there. So we can check exactly the same thing by constructing a different operator in the RPHS than the one we had before, but it's the same idea, right? We construct this operator. We call this the CERC operator because we came up with this bad acronym. And this operator. And this operator is zero if and only if they're conditionally independent. Okay. Here is like a special case of this in the special case of product kernels. So if you're a kernel for both Z and Y, you have a kernel on ZY pairs. Normally, if you're using a Gaussian kernel or something, you'd separate that into the product of a kernel on Z with a kernel on Y. That's like the usual case. In that case, it simplifies a little bit, and this is the expression. Little bit, and this is the expression. So we have this like tensor outer product of the kernel on X, the kernel on Y, and the centered kernel on Z. Okay, not gonna talk about how you estimate it. Well, okay, I'll talk a little bit about how you're estimated. What you have to do, basically, is you have to estimate, you have to center it. So you have to estimate what's called a conditional mean embedding. This is an object that's been studied before. That's been studied before. It is essentially just the if I write it, I mean it's this, except only for z. Okay, here it is. So it's the expectation of the features of z for a particular y. This is a, you can estimate this with kernel retrogression, but it's infinite-dimensional kernel retrogression. Like the labels, so the inputs is. So the inputs are in some whatever space. You map that to infinite dimensions, and then your outputs are also infinite dimensional, because this object is the feature, the infinite dimensional features of Z. But it's just kernel ridge regression. It's familiar formulas, except some of the labels, instead of being scalars, are space objects. You can use this to construct an estimator. I'm not going to talk about the details of the estimator, but it's relatively simple. Simple. And then this is something that you compute based on kernel matrices. Given a particular sample, you compute your x-to-x kernel, your y-to-y kernel, and then your centered z to z, and that centering is based on your estimate of this machine. Okay, so this is nice. One thing is it's consistent. So the estimation goes to zero if and only if they're conditionally independent. The rate of that is known, but it's a little bit complicated. Is known, but it's a little bit complicated to state. The other nice thing that's really nice about this for deep learning purposes is this regression is a regression from Z to Y. So if X is our deep network stuff, Z and Y are fixed, then we don't have to redo this regression as we change the network. So we do this regression once in advance based on our training data, and we just do a regression, it's a normal regression. Normal regression. And then, as we're training the deep network, we use the output of that regression over and over again, but we don't have to rerun it. So, this makes it computationally feasible to do, like, with a reasonable sample size as you're training your network. Okay, so in the interest of time, I'm not even going to go through the experiments. Basically, it works better than comparisons on some very synthetic data sets and some relatively simple, like, real. Like real, real-ish data sets controlled cases where we know whether conditional is true or not. Okay, so two minutes about testing. So the last bit is, okay, so we do this, we train a deep model, we say, oh yeah, we trained it with CERCE, so we expect that it will be roughly conditionally dependent. But is it? Right? Like we've done this, we have a model. Is it actually conditionally dependent? Or maybe we just have nothing to do with the deep model. Have nothing to do with the deep model. We just want to run a conditional compliance system. One particular thing to note about this is that, like, here, this driver risk score is like a scalar thing that we're conditioning on. But by doing this regression framing, we allow ourselves to handle much more complicated-wise. You could imagine conditioning on an image. Like, you know, am I fair with respect to a picture of this application, right? Where I don't have explicit labels, and I don't want to run through the explicit property. I don't want to run through the explicit proxy setting. I just directly go from the kind of table. Anyway, so we're going to do null hypothesis significance testing, which has its problems, but that's what we're going to do. And our null hypothesis is that they're conditionally independent. Alternative is that they're not. And so this is equivalent to asking whether this CERC operator is zero or not. We compute its norm, we estimate it from data. We estimated from data. Is the estimate big enough that we'd be confident the true one isn't? Problem is: this regression is really hard. Even in the best case, the mini-max rate is m to the minus 1 fourth compared to normal regression is m to the minus 1 half, right? So this is a lot worse. And that's the best case mini max rate. It can actually be arbitrarily slow, depending on the thing you're trying to estimate. So that's bad. So that's bad. So, our regression here is probably going to be wrong. But if we have the correct regression, everything else is good. So, what happens when we have the wrong regression? So, I'm not even going to go through this, too many equations anyway. But when we have the wrong regression, it turns out to add a bunch of bias to our estimate based on the error in our regression. So, if our regression If our regression is wrong, then no matter how many samples we have for the rest of the test, we're still going to be wrong based on those values. Yeah, okay. So there's another thing called KCI, which is a very similar operator. It's a little bit older that requires centering both of them. And so what we do is basically just to correct some of this bias in one of these regressions, we Split, we do data splitting, we have two regressions, and then we use one of the regressions in one place and one of the others in the other. By decorrelating these errors, then this really reduces the bias, and our overall test works way better. We also do a smarter way to do the test, but I'll skip that. Yeah, so here's an example. This is the type 1 error. This is the, like, I want my test to reject under the null 5% of the time. How often does it actually? The null 5% of the time, how often does it actually reject in some particular setting? R tests, a split KCI, it's pretty close. It's not perfect, but it's relatively close to the stated level. KCI is pretty bad, and then plain CERC testing is awful. Because KCI is less biased than CERC because it has the two regressions anymore. Also, comparing to some competitor methods that assume things, we can be way more powerful. So, here's an example. Way more powerful. So, here's an example on synthetic data where these two models: this RBPT2 is like a deep learning kind of thing, GCM is like a linear, like assuming things are roughly Gaussian kind of thing, we could be way more powerful than them. So here this is the type 2 error. So like GCM almost never rejects in this case, in a case where they actually are different, whereas we reject almost all the time. Okay, so this doesn't solve everything, but accounting for this bias. Doesn't solve everything, but accounting for this bias really helps. And then this allows you to do conditional independence testing somewhat, and allows you to train models that are roughly conditionally independent somewhat. There's practical, you know, these are our first papers in both of these things, and so we've done mostly like synthetic experiment type stuff. I'm certainly not claiming, like, even if this worked perfectly, it wouldn't solve fairness, but it, you know, it certainly doesn't work perfectly, right? So that's the thing. Perfectly. So that's the thing we're continuing to work on, is especially cases like where if you're like conditioning on an image, then you can't just use a Gaussian kernel in images. You need to learn something relevant to the data. That side happens to be relatively straightforward. We have some experiments that show that works. Conditioning on Z turns out to be harder, but we have some ideas. So that's the thing we're working on as well, is learning the appropriate kernel to use in these measurements. Okay, that's it. Thank you. Thank you. Sorry, I'm waiting on what's wrong with that. Yeah, according to my understanding for this type of method is basically when because what kind of method is the dimension is relatively high, you always suffer some. Right, okay, sorry, so let me just make a point that I should have made about this that I didn't. So for the CERC setting, the assumption here is that y and z are both relatively low. Y and Z are both relatively low-dimensional, so that kernels work. But you can have a deep network that does whatever you want. It can be, you know, the input to X can be absolutely anything, just as long as the outputs are relatively low-dimensional. So that could be representation learning where you're learning like a 256-dimensional representation or something, and in that case, it will work okay. It'll work pretty well if X is your decisions, like the output of your network, because then this conditional independence that you're actually measuring, everything's low-dimensional. Actually, measuring everything is low-dimensional. Even though there's a huge deep network going on inside of it, the kernel isn't looking at that. That's another level of questions I always ask myself, because when you use deep learning to do the projections, right? And also, you also want to use the to do the testing. That's the double job. Look, this is trivial when dimension is high. And then you couple them together. You couple them together, this could have made the situation even worse. So, if you use the same kernel to test, like you train it with some setting, and then you say, Okay, yeah, now I'm gonna test based on this same kernel, yeah, that would be a problem. Um, but if you use a separate kernel, like you learn a separate kernel for testing, which is a thing that we, you know, my group has done a lot of, um, is learning kernels for particular tests. And that can work really, really well by parameterizing the kernels in terms of deep networks and then optimizing things. In other settings, In other settings, we haven't done that in this setting yet, but in related settings, it works way better than doing a direct deep learning kind of test or than doing any other kind of test we know about. Thank you so much, Danica, for a great talk. So, near the beginning, you mentioned that equalize odds means that predictions are independent of projected through the condition of the true label. Labeled. And if I understand the RKHS construction is being used to determine if this conditional independence is true. But if it's not true, how do you use the true label to sort of transform the predictions to make them independent? And do ARCAHSs come into play there as well? Yeah, so I think there's two ways. Yeah, so I think there's two ways to think about that. One way is in terms of interpretability in how is this dependent. So the amount that these kinds of methods can answer that is you can look at these functions that you learn. And sometimes looking at those functions that you learn can be helpful for understanding what's going on. And sometimes it can kind of be a slip. So that's definitely an important area to figure out a little more. In terms of fixing it, you can frame fixing it as exactly the same problem as learning a model. The same problem as learning a model with Circe, right? You can say, I want to, I have some model that I trained not thinking about fairness at all, and now I want to post-process it into something fair. Well, you can, you know, that's this problem, right? You take your model outputs, you do some, you learn some transformation of them to then make them more thorough, right? Or more conditionally independent anymore. Go ahead. So your concept. So in in your construction of the proof of independence, an RDC uses covariance. So are you testing only for linear independence? No, we're doing the covariance of non-linear functions of the data. That's the point. If you used a linear kernel so that these functions were linear, then it would only be linear to come out. Yeah. Yeah, it's a great talk. You you use the kernel approach to test the independence. Test the independence. So, another approach is using distance covariance. Yes, I meant to mention this. Distance covariance is actually a special case of HSEC with a particular choice. Oh, it's a special case? Cool. Okay, so just trying to relay this with the result by Shrine and Peters. Yes. Okay, so Sean Peters have a paper on the hardness of conditional intense testing that basically says this problem is impossible. That basically says this problem is impossible, and it's true that this problem is impossible. The way that, oh, so for a particular definition, impossible, right? The way that their result plays into ours, to my understanding, is that the cases that are impossible to test are exactly the cases where the dependence, like the convergence of the conditional mean estimation is arbitrarily slow. So if you get a good conditional mean estimate, then everything works. But it's not always possible to get a good conditional mean estimate. So your choice of kernel is So your choice of kernel is really important in framing the form of the dependence as something that is possible. That sounds to me like you are not as busy description. Oh yeah, we are. That's our first step is we do a kernel progression testament. No more questions than our just give it a second. So 