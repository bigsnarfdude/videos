So I'm here on behalf of Simon, who's graciously extended his invite to me. This is going to be a presentation of more exploratory work. I met Alex at a Welcome to Your PhD event in London. And so this is quite still a work in progress, but we're quite excited with the preliminary results, which I'm going to be presenting today. Okay, so just a couple of motivational slides. Alex comes from a background. Alex comes from a background in AI for health, and he's the one who actually has my tie towards the medical imaging side of this. So we're looking at how does stay look like in the real world? Well, realistically, we don't have nice grids or meshes or nice Cartesian planes. More oftentimes, this is just a stump that I found was quite a good example. We have sparse sensing, which was mentioned earlier in the days before. Oftentimes, you can't get Oftentimes, you can't get a realistic feel for all of the important points of data. I drew this from a graphics paper actually, but I think it shows it quite sufficiently. This is maybe closer to what Alex does. I'm afraid I don't do cardiac signals, but he's looking at electrical signals on the heart. And so that was one of the prime motivators towards looking into this field of signals on non-Euclidean geometries. Euclidean geometries. And then how does data behave in the real world? So oftentimes data is physically meaningful. We look at a lot of things in physics that are described by mathematical equations. And these tend to also be spatiotemporal. This is, of course, just a subset of all of the problems that we do look at. And that kind of leads us into partial differential equations. This is the focus of my PhD, and this is also why I ended up working with Alex. I ended up working with Alex. He presents a very interesting problem field, and I am looking at something on the math side related. I'm using the formulation given by one of the motivating papers that we were reading. So we're looking at PDEs of this form where we have the flux and the divergence of the flux. You can find PDEs pretty much everywhere. I don't think I need to explain that too far. Yeah, okay, so we were considering that we're looking at non-Euclidean. Considering that we're looking at non-Euclidean domains, well, what already exists? So, graph neural networks have kind of taken the world by storm a little bit. And all of the various iterations of the graph neural networks, including the convolutional neural network and the attention, graph attention network, have been used in a variety of fields, including medical imaging and as well as all of the graphics fields and other more geometrically inclined forms of data. Forms of data. This is kind of a good visual of how graph networks typically work. You have some point cloud, which is the common way of intaking data, and then you need to be able to form the connections in somehow. And then the further data processing happens throughout the graph neural network, commonly through message passing or other iterations of message passing. Right, so graph neural networks are discrete representations. So, graph neural networks are discrete representations. I know there is now some work done in being able to add and remove nodes, but I haven't seen that with a very, very common widely uptake. So more continuous representations, this is what I'm going to go into a little bit more detail on. But this is the generalized INR. This actually takes the data in the form of a point cloud. It creates In the form of a point cloud, it creates nearest neighbor connections to create a graph. From there, the graph is used to calculate the graph Laplacian, which is that third step in the process. And then the graph Laplacian, or a truncated form of the eigencomposition, is used as input to a neural network which predicts the signal. So, why are we looking at continuous representations? Well, nice things about continuous representations is that you can Continuous representations is that you can do some arbitrary sampling of solutions. So oftentimes your data is not spread completely over the entire domain. And if you do want to be able to look at the end value on a specific point, then on a graph, you might not be able to get the same discretization. As well, the continuous representation should be discretization invariant. Of course, that also depends on how well it actually trains. It depends on how well it actually trains. And then it learns the manifold and not the discrete approximation and the interpolations of said manifold, which would be the result from a graph-based method. And then when I say it pulls from multiple disciplines, I think this is also something that I'll explain a little bit more later. But our approaches so far include some of the mathematics results that you've seen actually also in the previous couple of days, but some algebraic geometry. And then we're also looking at And then we're also looking at representation learning more in the AI side and computer graphics techniques, as well as the scientific ML, given that manifolds and topology are very, very common in the graphics domains. Okay, so our current working name for the project is manifold pins. And this is because we're looking at pins that are basically used to learn manifolds as well as the signal on the manifold. On the manifold. Our current exploration is the Graph Laplacian, which is, as I mentioned, inspired by the GINR paper that you just saw. The end goal, we kind of have two motivations aside from being able to predict a signal on a domain. The second one I think is quite interesting. You can potentially learn the manifold that the PDE sits on. So oftentimes, if you're just collecting data, you might not know exactly the spatial. The spatial dependencies, for example, of the equation. And if you're simultaneously learning the manifold, which best contains, I suppose, the PDE, then that can give you some valuable insight about the data set itself. Okay. Right, so this is back to the GINR that I just mentioned. The implicit neural representation, which this paper also builds off of, is a class of techniques that Of is a class of techniques that parametrize signals using a neural network. And because of the neural network itself, it's a continuous representation of said signal. It's essentially just like a neural network that maps from points on a given domain to the signal at those points. A common example of where INRs are used is in graphics, where you would use the 2D coordinates of pixels to correspond to Coordinates of pixels to correspond to their RGB values or some such similar problem. By expressing a signal as an INR, you get the continuous approximation and therefore you get the arbitrary sampling that I mentioned earlier. The generalized form is, as I just went through, instead of just taking the 2D coordinates directly, it is capable of, I think, as long as it's a Ring Manny manifold, you can discretize it into a graph and then take this. Discretize it into a graph and then take the spectral embeddings and so on and so forth. And that way you can use it on arbitrary topological spaces so long as you do these three pre-processing steps. This is again, it's a very nice generalization because in a point cloud, you can do all of the pre-processing, and this is a very common mathematical workflow. The spectral graph embeddings, which are used as you see in the third part. As you see in the third part, they are usually of size k for the node n, and they're calculated from the eigenbomposition of the generalized Laplacian, which is then ordered by the respective eigenvalues. And in practice, you would truncate that by the first k top k eigenvalues. That way, that also decreases the computation of the pre-processing, which is more of a practical benefit. Then the other part that we're looking at, because as I mentioned, we're looking at electrical signals, which electromagnetics are dominated by PDEs in general. Neural networks for PDEs, this is the most ubiquitous type that took over in 2017, I think, by Ricey et al. And this is the physics-informed neural network. One of the main benefits is that you can use physics-informed networks on very, very small data regimes. Regimes. And while there was previous success using Gaussian processes to encode prior information, similar to what this is doing with PDEs, the Gaussian process approach is not particularly well suited toward nonlinear problems, which is more common in the real world. Essentially, what happens here is that it exploits like auto diff, which is now implemented in every single library ever, to back. ever to back differentiate the output u with respect to the input x via the computation tree. So you can use very simple feed-forward neural networks, and there are now also convolutional and graph neural network implementations, as well as many, many others of PINs. And this way you can basically constrain the output with respect to some PDE involving the input, which Which, for example, might be coordinate spaces. So, we propose to use the Laplace-Belchami operator as our kind of pin regularizer, and you'll see the pin loss shortly after this. We're proposing a change of coordinates, which is learned using the neural network H theta in the beginning. This essentially replaces the first three steps of the GINR by By mapping the Euclidean coordinates to some latent representation, which we train using the spectral embeddings currently, and then we regularize according to the Laplace-Beltrami operator. Then the second step is very similar to the GINR, where you have an implicit neural representation which maps from the latent representation to the end signal. The pin loss operates over the entire... The pin loss operates over the entire two networks, so there's kind of like a double loss going on in the first one. Because we want this to be fully differentiable from start to finish, such that we can back such that we can differentiate fully, you have to use smooth activation functions. So commonly it's the tangent, but also there's something called siren, which uses the sinus, which is a form of the sinusoidal function. There's also the sinusoidal. There's also the soft plus activation function, which is a smooth value in essence. And so we're looking at testing with these different activation functions as well to see what might work better or worse. Right, so learning the graph spectral embedding encourages the embedding network to learn the manifold by proxy. And so because we're also regularizing according to the, oh, I suppose I should. So, this is the formulation that we're using. This is the multivariable composite function at the top, just taken from a previous paper. And then we can formulate the Laplace-Beltrami operator according to this kind of coordinate change as the trace of S, which is defined right there using the Jacobians and the Hessians. And this is, we use this because we're moving from Euclidean to some latent space representation. To some latent space representation, which is another, let's say, coordinate space, to the final end result. So, why do we use Laplace Beltrami? I think this was also mentioned earlier. So this is actually the pin loss formulation. The third term here is your typical loss. This would probably just be your MSE loss for some observed signal and your predicted signal. But these two right here. Two right here, this would be your PD directly. So you can, because you can differentiate the final signal with respect to the input coordinates, you can imagine it's quite simple in theory to rewrite that according to the partial differential equation, which we've just shown as the Laplace-Beltromi operator. And then the second is the boundary conditions, which we've used Dirichlet boundary conditions. We've used Dirichlet boundary conditions in our current test, but theoretically, you could define that however way you want. Why Laplace, that's as simple as one, two, and three. The first paper is basically showing the equivalence between spectral embeddings and the Laplacian eigenmaps with the projection computed by a kernel PCA method. So this kind of motivates. This kind of motivates the connection between those two, and then from the Laplacian eigenmaps, you can show that they're a discrete representation of the Laplace-Beltramy operator. I seem to have something against discrete representations because I'm working with continuous. And so because of that, we use the Laplace-Beltramy operator directly, which guarantees an optimal embedding. And number three, this is obviously not the original proof of the fact that it's an optimal embedding, but it does give the Optimal embedding, but it does give the mathematical proof in a nicely written manner. So I've used that as my third point. Right, so yes, some very preliminary results. This is kind of like a modified Swiss rule. This was something, a very simple test, given that we're looking at the end signal should be very. The end signal should be very, very dependent on the manifold. First of all, we don't want like a planar manifold because that would also be a little bit redundant to test, and so we use a very common swiss roll idea. And then we've used the Poisson equation with, let's see, oh, sorry. We've got like x times y evaluated on the boundaries, which is our boundary condition. Is our boundary condition? And we've truncated to the first eight eigenfunctions. We found that that was sufficient, whereas the GINR paper used the first 100. Because the domain is relatively simple and so is the PDE, we testing it on the simple case here. Firstly, a call for help. I've never actually seen these camel helps right here in other training. It used to be much, much worse before we ran some hyperparameters. Before we ran some hyperparameter tuning, so I'm not quite sure still what exactly causes the hump, but one of our major concerns was that pin loss is notoriously hard to get to converge. That's because pins create a very, very complex loss landscape. And so if you just happen to fall into a local minimum, they refuse to get out of them. It turns out that ours does converge after we. Ours does converge after we did a little bit of fiddling with both the layers and the activation function. We've actually used the soft plus here, which is the continuous RELI that I've mentioned. And our operator is constructed from many, many partial terms, which also makes the lost landscape a little bit difficult to learn. So far, we've avoided that. But I can't tell you exactly whether this will happen for every case. This converged quite nicely, as I said. Said. And so we've chosen to, I've well, I've used the MSC loss as well, but the R2 score I think is a good measure of how this has performed. I've included the GINR R2 score, which we've tested on the same, I guess, cylinder with a gap or our modified Swiss roll using the spectral embedding, the same as the paper. And that resulted in a 0.6. And that resulted in the 0.675 R2 score. The R2 score ranges from negative one to one, and zero would be if you predicted the average of all of the data. And so this is somewhat promising. It means that it's modeling the solution somewhat okay. But I think Alex and I have said that our target might be close to the 0.8 level because that way you're getting a quite good representation of the output signal. Even though our pin enabled Even though our pin-enabled R2 score is a little bit lower, we're actually quite happy with it so far. As I've written below, our pin-enabled version only uses 2% of the observed signal, whereas the GINR uses the full 100%, I suppose. Let's see. Okay, so we've also tested a second architecture. And this was basically using an autoencoder to we take the latent, sorry, the choke point of the autoencoder to be the number of dimensions that we want as the latent representation. In our case, we go from 3D to 2D, so that choke point would be a length of two. And if you can faithfully compress it into a two. Compress it into a 2D coordinate space that also creates some kind of embedded manifold, ideally. And these are the visualized results of that. It doesn't quite learn the full what you would imagine should be the cis rule, which would be a 2D plane. But I've got the predicted signal on the predicted latent representation on the left. You can see it's a quite a smooth varying function from the yellow to the dark purple. The real signal is a little bit. The real signal is a little bit different, where you can see the dark purple kind of takes like two paths like this when plotted onto our predicted manifold. But the lighter colors do come from the top. I'm not quite sure why we have a tail either. So this is also something that we're looking into why that's happened. The axes are quite arbitrary. So we've actually just taken the latent representation as the two as the vector. As the vector in the middle of the autoencoder. So I can't really tell you what exactly they are. But yeah, it does actually start to broaden out. I have plotted the entire training process of those, of the auto, sorry, of the latent representation, and it goes from a line to start to broaden out like this. I think also because our PDE is quite simple, it doesn't finally need to completely turn into a 2D plane. Turn into a 2D plane, and so it stops training after maybe halfway, and it ends up like this quite soon. It converges to this shape. But likewise, I think our R2 score was close to 0.56 or 58, something like that, slightly worse than the previous one that I've shown. But this also does have the benefit of having a very convenient visualization. Whereas when you're using the eigenvectors, you have eight vectors. Eigenvectors, you have eight vectors, and we've played around with plotting them, but you do have to basically plot all of them and be selective when it comes to presenting a visualization. So, in essence, this abstracts the latent representation a little bit better, albeit with a lower R2 performance. So, kind of a summary of the current work. We've written the PDE according to the latent representation, and that's using the Laplace-Beltrami operator to converge towards. Operator to converge towards the optimal embedding. Some ongoing directions. We're still looking at other ways that manifold learning is implemented. So in classical methods, we've got the locally linear embeddings as well as other dimensionality reduction techniques. Also, from the more math side, we're looking at the metric tensor, and there's some preliminary work in machine learning to learn the metric tensor directly. We found that it was. Directly. We found that it was very non-trivial to implement. So we haven't quite tested that just yet. And then the interesting part would be to use, as I've mentioned, there's a lot of ties to other domains to see whether we can bring those into a more, let's say, scientific ML approach. I've written CNEX slide because I found this graphic, which really demonstrates how manifold learning is done in graphics. You've got the point cloud, which is quite noisy and messy. Noisy and messy, and you can see there's a lot of like scattering along the edges here. And these are five common techniques in the computer graphics domain, which start out not quite too promising, but the points to surf in the GT, for example, or sorry, the GT should be the actual. But this ends up in a quite nice learned representation from the point cloud. So if you're able to learn the math. Cloud. So, if you're able to learn the manifold in these ways, then it might be worth considering to see whether we can incorporate these techniques into the more scientific ML or learned PDEs regime. Okay, I think I've just run under as well. Thank you. So much for the talk. Are there questions? Yes. Thanks for your talk. You said you have this problem in convergence with the loss function. Yeah. So can you show it again? Did you pick actually the last point as your solution? Because this seems to have a higher loss than in between point, right? So this low point is actually slightly lower than that low point. So yes, we've chosen. So, yes, we've chosen. I think we've taken the average of the last from 400 to 500 because it does shift around a little bit. So, this behavior you usually see if you have a too strong momentum term in. Oh, I see. Okay. You know, it's like gravity, you have a rolling ball and it starts to oscillate. Right, yeah. And maybe if you decrease the momentum, I guess you're using Adam or something similar for that. Yes, Adam. Reduce the momentum. Yes, Adam. Reduce the momentum and then you won't see it. We've found that. So we've run the tuning as well on Adam with the momentum term as something that we're optimizing over. And this is post-tuning. So I'm not quite sure it's still what's happening. I'll take a look at the momentum again to see whether we can mitigate. But this is a typical behavior of momentum terms. So if you have an algorithm with a momentum, there's no monotone decrease. That's clear. Okay, perfect. So you overshoot. Yeah. Thank you. So you overshoot. Thank you. I don't see anyone. Oh, yes. So going back to the scatter plot I asked you about, since you're reporting R squared, I guess you want that scatter plot to be linear. But what physically is the reason why these learned representations should be linear? I might have missed something, but we don't quite want it to be linear. Oh, okay. The R2 is. The R2 is, we've used an n-dimensional, I suppose, R2. And so it should, the latent representation we would expect from the, let's see here. Ideally, it would be just like a square, I suppose, or a rectangle, I suppose, because it's rolled up here. Yeah, so in essence, when we're talking about the data generation, at least, we've We've basically generated on like a 2D plane and then curled that. So the data is, sorry, the PDE is defined with respect to a 2D plane in essence. And so if you've got the 3D representation like this, you can see like in 3D, you might a common, like the reason why the Swiss world is a good data set to be used for manifold learning, for example, is because, you know, these two points are, I suppose. You know, these two points are, I suppose, close together, but they have no actual relationship. Likewise, along the curve and along the bends, that happens. And so, if you're able to unravel, I suppose, or find the another common part of manifold learning is like signals on surfaces not necessarily like even uniformly scattered in 3D space. And so, if you can learn the surface that it sits on, then the data oftentimes makes more sense. 