Tell us about anything at once. Thanks for the vague introduction. So yeah, thanks for organizing and for the opportunity to speak. So I want to tell you guys about some work, so soon hopefully to be online, about generalizing techniques for analyzing low-degree polynomial algorithms to kind of a much broader class. Much broader class of kind of distributions and situations than we've been able to deal with before. So, the kind of starting point, I want to remind you of one of the kind of main calculations in this area, which is about detection in Gaussian additive models. So what are these models? What are these models? So, this is a detection problem, right? So, it's hypothesis testing. And so, we'll have two distributions. Under the null model, we'll make a bunch of observations that are IID Gaussian. And I'll put a variance here for a reason we'll see in a second to draw an analogy under the Planted model. So, we'll draw some vector from some distribution. Vector from some distribution. This I'll call the prior. So this is in dimension n, and then what we'll observe is these y i's now recentered, so you can imagine that we x plus Gaussian noise. And so the question I want to ask about this, so you can, you can, these are all as well. So these are independent. This includes things like. This includes things like the standard formulations of like spec matrix models, like tensor model, planted submatrix, etc. So you can ask, how well do low-degree polynomials do at this? So there's a standard quantity, maybe probably Alex mentioned earlier, that we like to compute. Some people have started to call this the advantage to some degree. So it depends on this prior distribution. And here I'll emphasize it. And here I'll emphasize its dependence on this Gaussian. And this is an optimization problem where we try to maximize the expectation under the Planted model over low-degree polynomials. That are small in this kind of L2 sense under the null element. So probably this is fairly familiar. So we're trying to find a polynomial that is kind of big under one distribution, small under the other, and maybe we're thinking of thresholding it or something. So the kind of rough conjecture, this is the Hopkins hypothesis, right? Just teasing. Is that so? Depending on the degree, right, this tells us something about algorithms. So if this thing, so for some growing d, and as n goes to infinity, is O of 1, then we think there should be no algorithm that runs in time sort of almost n to the g. So almost n to the d. Like this would be the runtime of just brute force calculating a degree d polynomial. Or sorry, big n to the d. But here, to be safe, you can put a polynomial event. And this kind of lets us capture spectral and things like that. And then you can also maybe, so this is maybe people talking about this as less plausible, but I think deep down we believe it, that if this is omega 1, then there should be. That if this is omega one, then there should be, you should be able to calculate kind of the optimizer and threshold it, and that should distinguish between these two things. So in this case, there exists an end to the D algorithm. So in any case, computing this thing gives us some kind of diagnosis of the hardness of detection, strong detection in this model. Now, there is a very nice There is a very nice calculation, an exact calculation of this, thanks to this kind of Gaussian setting. So, in fact, this is exactly equal to an expectation over, so this will be over two draws from this prior distribution of some function. I'll write this down in a second. or 1 over sigma squared times the inner product of these things. And this x less than or equal to d, so let me put it maybe here. This is the truncated Taylor series. Of the exponential function. And so, again, I want to emphasize this is an exact formula, right? So, we know exactly the, then we know the solution to this optimization problem and everything. And this is very nice because so all of the kind of ingredients in the setting play a very kind of clear and separated role here, right? So, the degree, kind of the runtime we're allowing ourselves, is in the truncation. The amount of noise is down here in the denominator. Is down here in the denominator. And maybe most importantly, so the prior, only the prior is an n-dimensional distribution, but the only role it plays is through this scalar kind of distributional summary statistic, this inner product of two independent copies drawn from it. So these are the independent things. And this is, so this is just kind of formally nice, right? But also is a useful tool for proving, let's say, lower bounds. For proving, let's say, lower bounds, proving this kind of thing. Because now, instead of dealing with this entire big distribution, you just have to understand the scalar random variable. Ask questions if anything that's confusing. And let me mention, so here we know, right, that the proof of this is that the optimizer f star is the projection. The orthogonal projection to low-degree polynomials of the likelihood ratio. So dp dq, or think of it as if they're continuous, the ratio of the densities. And to understand the kind of objective function, you can decompose this in In permeate polynomials. These are orthogonal polynomials with respect to this null distribution. So somehow that helps us here. So what I'm going to tell you about is how we can generalize this story to a much sort of broader setting. Any questions about this for the links? Yeah. For this um the other expansion, the the last one was the one was typically you know. Order of magnitude or yeah, so okay, either it's constant or these are kind of growing exponentially. That's right. Possibly exponentially kind of growing. The log term will determine whether it is O1 or omega 1, right? Sure, unless this is extremely small, in which case it'll be the first one, which it's always at least one, right? So it's kind of roughly max of one and the last term. In the last term. Okay, so the setting that I'll be working in is what I'll call latent variable models. And I'm going to ask the same question about these things. So I'm going to generalize everything about this except for these independencies. So again, we'll have these two distributions. Under Q, we'll draw these YIs IID from some distribution. I'll call it P0. Under P, we'll draw this signal, and then we'll draw the Yi from some distribution PXI. And these, we'll see, so right now. And these, we'll see, so right now these are totally arbitrary, and they will be essentially almost completely arbitrary. And the things that we contribute together, these things are referred to as the channel. So some people here have worked on kind of similar results in this direction. This is the terminology that they use, I think, from kind of information theory. I'm also going to talk about a more general class of algorithms than low-degree polynomials. So I'm going to ask. So I'm going to ask whether low coordinate degree functions can distinguish and detect. So this is an idea that is around in kind of statistics world. It showed up in this context. World, it showed up in this context in Sam's thesis as I think maybe a more natural class of computations than just low-degree polynomials. And then it kind of disappeared because I think we all collectively decided it's easier to deal with polynomials. But hopefully, I'll convince you that that's actually totally not true. These are much more convenient in certain ways. So, what are these things? Let me write the tax a little bit. So I'll write this Vt for functions in Y. So functions such that f of y only depends on the coordinates in a particular subset. I think this is actually like in our low degree versus statistical quarry paper, we're also using some stuff. Okay, okay, interesting. Now that I see it again, shouldn't we have called it something like something following the word junta should have been? Something involved in the word junta should have been involved in the naming of this, but that's maybe not for now. Okay, you can tell me about. It's just like, you know, used for the same reason. It's the same reason you're about to use it, I think. Yeah. Okay, we'll find out. Okay, interesting. So, right, depends only on the coordinates that are in this set. And then, so this subspace. This subspace of these things will be all possible linear combinations of functions that depend only on a subset of size at most t. So you can take any arbitrary computation of these small subsets of coordinates and then take linear combinations of them. That's the kind of computational model. So this in particular contains what did we call non-belos. And so now, okay, we can define a kind of analog of the advantage. I'll call this the coordinated advantage of these two things, which is just the same thing. But restricted to this setting. I'll just set instead of whatever you want away. So why is this nice? You know, using these tools that are about low-degree polynomials, in case I totally missed something very important in your paper, there are a lot of things that we can't handle. So think about this as the observations kind of coming from. Of coming from, you know, if x is here, we can observe a Gaussian-centered at x, right? That's kind of the noise model that we know how to handle. And there are very few kind of, you know, there's no even small perturbation of this that we know how to deal with in the low-degree polynomials sort of framework. So, for example, I want to handle something with some small little perturbation like this. Perturbation like this. If this kind of little squiggle is very tiny, obviously this shouldn't make a difference for the complexity of this problem, but we have no idea how to do this because we don't have these nice explicit orthogonal polynomials and identities for them and things like that to work for models like this. Okay, so let me state the main theorems. So, first of all, So, first of all, sorry, so I mean, is the point you're trying to make there that it's easier to compute what the C advantage is in settings where we don't have to do it? It will be. It will be. Or, yes, I mean, it will be to some, depending on kind of what you mean by sort of closed form, I guess. But, but, yeah. It'll certainly be easier to bound. So, let me write this down. So, for some, in some sense, nice. In some sense, nice kind of settings of this problem, so nice prior and channel. So, in particular, the condition on the channel let me call smooth, and the condition on the prior, let me call small. I'll be more specific a little later. We can bound the coordinate advantage by some constant times the exact same thing, almost. Same thing, almost. And here, there's some other, so the role of 1 over sigma squared is played by, remember, so 1 over sigma squared was the thing here that depended on the channel. So something else plays that role here. The rest of the formula is the same. This is something called the Fisher information, which Something called the Fisher Information, which, if people have read, so there's a paper by Alex and Unkwer and their collaborators, and some things by Paunt and Jaming. But this is played a role in these kinds of universality results, but hopefully I'll get to kind of defining it. But this is some function depending on just this family of distributions. And in fact, you can also prove a partial converse, and this is why I was curious about the connection with the statistical. The connection with the statistical query paper. Because here you can show that if this prior is also dilute, and if people have read this paper, this is in exactly the same sense as there. You can prove a matching lower college times the same thing. So So, usually, what we do with low-degree polynomials is prove upper bounds on the advantage, which are lower bounds against the class of algorithms. And this kind of says that those lower bounds against low-degree polynomials behave universally over some big class of channels. This says that under this extra assumption, really the behavior, kind of the convergence or divergence, doesn't depend essentially on the channel, except through this one scalar. Through this one scalar parameter. So under both of these, this course bubble of behavior is channel universal. Are you going to pull the curtain back on what smooth and small and deadly mean? Yeah, but so let me give an application to convince you that there's nothing, that it's useful for real problems. As long as you get to the eventual end. Sure, yeah. And if I don't, then I'll take you next. Yeah, you remind me of that. Wait, sorry, did you say this already? So the Fisher information, I mean, how are you defining this? So. I mean, usually I think about Fisher information as defining a different type of object. So it's the Fisher information of this family. It's the Fisher information of this family at zero. At zero, yeah. Okay. And the reason that at zero is relevant is because the prior is small in some sense, right? So we only care about the behavior near zero. It's a very similar argument ultimately to the thing you put in for the Sintaylor expansion of something and the addition of the OK, great. So let me give an example of what this kind of thing tells us. And this will be relating to this paper I keep mentioning by Anker and Alex. Mentioning by Anker and Alex and others. So, this will be about general spiked matrix models. So, what happens here, so our channel will be kind of like I was alluding to over there, it'll be additive noise. So, this is, this has to be a distribution, it's the law x plus. X plus Z, and this Z has some nice density. And we really don't need very much, like, let's say a smooth density that's symmetric. And that's it, I think, infinitely fab. So that's that. And the thing that we're observing, this thing, is the log of, how do I want to say? How do I want to scale? So let's say lambda over root n times sum xx transpose. And so in fact, really, I want to think about the upper triangle of this. And you can imagine this as being like a symmetric matrix plus symmetric notice or images, the way that people did this. And then x will be, so x. So X will have its entries be of order 1, and then its norm kind of roughly N. So lambda is a parameter, and there's a transition where detection becomes possible information theoretically and computationally and so on. And so originally this was studied for just the Gaussian noise distribution, and then these guys figured out how this depends on what this is kind of. How this depends. I mean, this is kind of based on this other work in physics about the information theoretic threshold, but how some algorithms kind of depend on this more general distribution. So in particular, what that paper shows is that if this lambda is bigger than 1 over the square root of this f parameter, which is Of this f parameter, which is just something. So, for this particular example, it's something you can calculate. It's just some integral involving the density of z. Then, there is an efficient test to distinguish these things. So, the test is an entry-wise transformation, followed by PCA in the sense of computing the largest object values and thresholding it. And so, we'll hope there this succeeds. And so, So the corollary of this stuff is that in the other regime, this coordinate advantage is bounded until quite high degree. So like let's say n over log squared n or something. This is O of 1. And according to the conjecture, what this means. According to the conjecture, what this means is that you need nearly exponential time in this other region to solve this. I think, by the way, Florant discovered this before us. Right, right, right. Yeah, so I mean, I think there were physics conjectures about the computational side, and then he and Jia Ming, right, and then others proved the information theoretic side of this rigorously, as far as I understand. Unless I'm maybe analyzed AMP. I don't know. Tell me if I want to let me. No. Tell me if I'm, I want to, let me be just to, right? So an interesting sub-example is, so I won't explain this on the board, but it's a sparse PCA, where this distribution of x, so here there's some kind of distribution of x, and then I'm building this from that distribution. If this has a kind of constant sparsity, right, and that sparsity is high enough, if it's sparse enough, then there's a statistical computational gap where Statistical computational gap, where in part of this regime, you can kind of by some brute force calculation actually achieve detection. And this also implies that that statistical computational gap is universal with respect to all beats and place distributions. Okay, questions? So, what I guess I will try to tell you about the rest of the time is some of the proof. Time is some of the proof ideas and why it turns out that these low coordinate degree functions are easier to work with than low-degree polynomials. P refers to this family. It's some function of this family of distributions. Because you said it's universal, so somehow it meant that it doesn't depend on everything. It depends right. It's universal in that it depends on one number. This is a In that it depends on one number. This is just a number. So it doesn't depend on the entire family of distributions, it depends on this number through distributions. So, for example, okay, a simple example is that one thing I can treat is essentially any exponential family in which you make observations. It's like Bernoulli or Poisson or exponential or whatever. The tilts of a particular reference measure. And in that case, that parameter is always this, it's one over the variance. It's one over the variance of the kind of base distribution. A lot of times you also refer to it as a universality class. Like once you tell me this property of the instance, nothing else matters. You have questions about the terminology or? No, I mean. No, like the advantage is also the number. So why is it sure? So here, okay, here's how you would use this result, right? Let's say I've proved Let's say I've proved for a Gaussian, additive Gaussian model, some low-degree lower bound. Now I take some other model, some other noise model, let's say it's structured like this, it has a lambda in it. Then what's going to happen is I just have to rescale that lambda by something to do with this FP, and I immediately get a corresponding low-degree lower bound for many other noise distributions for free. Up to this adjustment, because this is some kind of up to this adjustment because this is some kind of natural scaling parameter associated to a particular distribution. We can talk more about that here. Okay, so okay so let me say a little bit about some proof techniques. So the main thing is it to compute the Fisher information of this like config Gaussian if you look at this density is Okay, so if this density is some function p, right, then this fp is, I believe, integral of p prime of y squared over p of y. So it's just a one variable integral, right? Could be it doesn't depend on prior chi or it doesn't depend on the prior chi, right? Kind of like I was. On the prior cut, right? Kind of like I was referring to here, the ingredients all sort of separated. That's one of the nice things about this kind of formula. Oh, so this is like the small in parentheses pi. Right, so there's an extra assumption, which is that pi is, so its entries should look basically no worse than like this in magnitude, which is square root, which is the fourth root of capital N. It's one over the fourth root of capital N, because here capital N is little n squared. Your cup of minus little n squared. Oh, the prior over the, okay, I guess the prior over the X shows up on the outside, like that equation. And in particular, through the distribution of this thing. Okay. So the main thing that probably is mysterious. The main thing that probably is mysterious is what am I doing instead of decomposing in Hermite polynomials? That's the, like, what do we do when we don't have these orthogonal polynomial twos? So the kind of substitute for this has a bunch of different names. The people in statistics have been called the Efron Stein decomposition or the ANOVA decomposition sometimes. So, what is this thing? So, what is this thing? This is a more general kind of decomposition of the sort of space of functions with reference to a particular measure into ones that kind of respect this dependence on different subsets of coordinates. So there are some of these sort of function spaces. I'll call them v hat t. them v hat t. And actually they're something simple. So they're just these vt, the functions that depend on just a particular subset of coordinates. But a subset of that where I ask them to be orthogonal, and here orthogonal like all the inner products are with respect to the Q distribution. I ask them to be orthogonal to everything depending on a strictly smaller set of coordinates. They're kind of all of the new functions. All of the new functions that depend on this big set of coordinates that aren't captured by all the previous stuff, and in fact, are orthogonal to all of that. So these have some nice properties. First of all, these are all orthogonal to each other. Also, you can decompose Each Vt as the sum of these for all subsets containing it, right, which is just kind of going backwards from this formula. And lastly, so because they're all so because they're all orthogonal to each other and this kind of thing holds, we can, I'm sorry, I shouldn't have to hat. We can write down, actually, so let me also write it down. Actually, wait, so let me also write here. This big subspace of all the low coordinate degree functions is the sum of all of these kind of special subspaces indexed by sets of at most that size. And we can project to these because, so this gives us some relationship about the sums of that. Does that mean I should stop saying? Does that mean I should stop soon? Oh, um, but okay, sure. Does that work? Uh, yeah, I think so. So we can project to these by doing sort of Mobius inversion or like inclusion-exclusion on this formula. So this is uh this So this is nice. This is kind of an explicit, an explicit formula. It's maybe useful to note an example which is from kind of Boolean Fourier analysis, which is the case where this measure is, let's say, The case where this measure is, let's say, where the Q measure is uniform on plus minus one. So in that case, what happens is that, okay, Vt is the span of X to the S over all S subset of T. These are those Fourier characters. This is the functions that only depend on a particular subset. It must be polynomials and all of the smaller stuff. And this is just the span. This is just the span of that 1x to the t. And so, indeed, this is the sum of all of these spans. And they're all orthogonal and so forth. Is there anything that would be, okay, like, I guess that I can sort of, you know, run Grim Schmidt on some sort of function. I imagine that I would get something kind of like this. So, like, what does this? Maybe I, so, okay, these are sub, so every function decomposes as a kind of sum of elements of one of these, right? And I guess you're asking how do you get those, maybe? Like, right, yeah. I mean, I guess, okay, I guess the, you know, the buoyant example is, like, probably maybe doesn't highlight the strength of this, like, basic composition. No, and it's a little tricky to say what happens in June. So, okay, let me, maybe it'll, it'll clarify to say. So, what happens when you It'll clarify to say. So, what happens when you project to this VT? This is just kind of conditional expectation on the stuff in T. You're sort of integrating away all of the other coordinates. This is like the interpretation of conditional expectation as a projection. So, and therefore, you can calculate this as some linear. Calculate this as some linear combination of these conditional expectations. Here's a nice thing. So, okay, this is kind of general theory about this decomposition. What does it do? So, like here, we need to do these projections to the likelihood ratio. So, the question that comes up is what happens when I project the likelihood ratio like this. And here is something really nice happens, which is, so this of the light. The likelihood ratio is just the marginal likelihood ratio. And this maybe, I mean, it's a super simple calculation to check that if I kind of integrate out a bunch of coordinates from the likelihood ratio, I just get the likelihood ratio on the margin. So, in fact, right, unlike in the polynomial situation, I can write down this low coordinate degree likelihood ratio and Coordinate degree likelihood ratio in pretty much totally closed form in terms of these for any model. So here I'm not even using this latent variable model assumption. So I can, you know, I won't do it explicitly, but you just take this and plug it in here and then take a big sum of all of those. And it's some linear combination of this one. Let me pick one more battle to fight. Battle to fight. And then I'll leave you alone. So let me maybe say where this Fisher information comes from. Then there's a question. Oh, yeah. Go ahead. Let's see. When we have to tease a few usually with our audience at the beginning, if we The problem is I'm thinking if we have x1, 2, xn, which aren't quite independent, but as they sum to zero, then like x1 and x2 wouldn't be orthogonal. So yeah, so this is an example just with respect to a particular distribution q. Like this notion, so this symbol in particular, right, this part of the definition depends on q. Right, and so the fact that these subspaces work out like this is specific to the fact that I'm talking about Q being like uniform over the hypercube. So more generally, this will be a more complicated thing I'm asking. But they will, I can show you maybe after, but they will always be mutually orthogonal so long as they're defined as well. Yeah, you need our fast afterwards. Sure. Okay, so like roughly, very roughly speaking, what happens? So first of all, right, so we only have to deal with these marginal likelihood ratios. And now this assumption, with all this factorization, becomes useful. So what are these things? We can show they're just expectations over the prior of the product. So, this like as a function of some y. It's a product of a bunch of kind of mini likelihood ratios from the From the family. So, this is a pretty calculation. And ultimately, what happens is you can show that this coordinate of advantage, and this is by similar things, so people have seen how you can bound a Bernoulli low-degree calculation by a Gaussian one. This is a very similar thing to that. You can bound this by This by this kind of thing. And then in here, you get something which is like, it's kind of like an inner product of these little inner things. One more long formula and So minus one times. Okay, so what is this, right? This thing on the inside is some big complicated function. So it's like an overlap, but that's specific to the channel. It's some function that depends on the channel of these two coordinates. Of these two coordinates. And I'm summing it over all of these i's. And the way, basically, what happens, like the place where this Fisher information comes from, is that by a Taylor expansion, and this is what happens also in all of these papers by Florent and Jenlin and Alex, what happens is that this is roughly just this fisher information times xi1, xi2. The lower order partial derivatives vanish. Partial derivatives damage. And then using these kind of smoothness, so okay, you need the smoothness of the prior. So the smoothness basically means think of it as the smoothness of this likelihood ratio as a function of x, so that I can do this kind of expansion. And the smallness of the smoothness of the channel and the smallness of the prior tells me that this stuff is happening close enough to zero that I can do this Taylor extension. And then kind of further details of that. And then, further details of that let you correct the error in this approximation. And plugging this in, you get exactly that type of bound. Let me, as one last thing, emphasize, so this idea of diluteness is exactly taking sort of some, what, a prior that used to generate some x, and instead generating a bunch of copies of x. K copies that are all scaled down by 1 over root K. And this, for some mysterious reason, is exactly what you guys ended up looking at in the statistical query paper, right, to make these kinds of models make sense for that. And so, okay, I won't get into it, but there's some sense in which doing this transformation sort of brings these things closer to the Gaussian model that used to be here, kind of because if you get a bunch of independent observations of something. A bunch of independent observations of something like this.