This is I don't have to do my usual lead-in, which is this is I'm going to be talking about theory. If you want to leave now, please, please do. There's nothing I think real, that's usually what I do. Yeah, so the work I'm going to be talking to you about today is some work that I've been doing with Lobster Club for various historical reasons. I haven't turned it back. Most of the heavy lifting was done by John McNamara for all of the stuff that I'm going to be talking about today, but Ola Lehmar and Alison Houston were. Erla Emar and Alisa Houston were involved, have been heavily involved with this project. So, the other thing I'll say is that, unlike probably most of the other talks, I'm not really going to get very technical because I don't know a lot of the really nitty-gritty details of exactly how John did all the evolutionary simulations. So, bear that in mind and lunch. Okay, so learning and evolution. Learning, something that's very taxonomically widespread, lots of things to do it, it's been studied a lot. It's been studied a lot, but most people have studied it kind of at approximate level to understand how phenotypic variation, behavioural variation develops and the mechanisms underpinning it. And then when people have thought evolutionarily about learning, they focus mainly on when learning evolves as a strategy. But they're also, I mean, of course, an interesting question is: once it's evolved, what are the... It's evolved. Does it have any evolutionary consequences? Are there interesting evolutionary consequences in systems where you've got learning? Now, this question has been addressed, of course, to a certain degree, but a lot of the work is focused in abiotic and ecological contexts. And the answer is usually when you've got adaptive learning, when you've got a learning process where animals kind of reinforce their behavior in a way that leads them to do things that give them higher payoffs, you tend to minimize phenotypic variation. Phenotypic variation, like everybody starts to do equally well. So it tends to slow down evolution. That's the answer for a lot of the stuff. That's a short answer with lots of details, clearly. In social sexual contexts, there has been a reasonable amount of stuff done, but it's usually focused on social learning and the role of social learning in kind of maintaining phenotypic variation. Phenotypic variation and its inheritance, and how that can lead to co-evolution with you know at a kind of genetic level. And you know, so gene culture co-evolution, I don't want to tell too many people, you guys, too much about that because you've come from, you know, a lot of you have come from the world of the guy who kind of started thinking about this. There's been a lot of stuff done on social learning and how it influences sexual selection, but what I'm going to focus on. Selection, but what I'm going to focus on here is how kind of asocial learning processes, so not non-social learning processes, so just purely learning processes that adjust behavior according to the kind of the returns that the animals get from performing an action, how that affects things evolutionarily in social contexts. And so, I mean, we focus. So, I mean, we focus particularly on competition. It's the kind of probably the most widespread form of social interaction out there, so it's kind of a nice reason to focus on it. And the key thing that we really focused on is when you've got negative frequency-dependent return-ups, because you often get a lot of negative frequency frequency-dependent chaos when you've got competitive interactions. And we focus on associative learning, very taxonomically widespread. I mean, one of the model systems is Seahawks. I mean, one of the model systems is Seahair, so lots of things do it, even fairly unsophisticated things, and a lot of those things interact socially in this kind of most basic level, you know, competing with other individuals within their species. And I mean, you're likely to see learning involved with some of the kind of gains that you get that result from competitive interactions because often these kind of competitive games are played repeatedly over individual lifetimes. Repeatedly over individual lifetimes, and under conditions, under conditions, and the payoffs, if you like, can change according to different ecological conditions and different changes to population composition, demographic composition, and the rest of it. So, likely, I mean, we're not covering the evolution of learning in the analyses I'm going to be talking to you about here. We're going to assume that learning exists and it's being deployed. It's being deployed in these kind of negative frequency-dependent and competitive interactive contexts. Okay, so oh, hilariously, for some reason, it's giving me one of those random usually a spermy, this one's a little question mark. But yeah, I'm going to focus on a case study for everything here, but I will say that what we have done is we've looked at every single two-action negative frequency independent game that is out there. Game that is out there, and the kinds of things that we get, kinds of outcomes that I'm going to talk to you about here, you know, kind of emerge in all of those situations. Some to a greater or lesser, the strength of selection can differ in different games, but largely this seems to generalise to all. We haven't proved any of this, you know, technically, but it seems like it's a fairly common outcome, at least, when you've got maybe frequency patterns in these situations. So I'm going to focus on the producer scrounge again. So, I'm going to focus on the producer scrounger game. It's a collective parasitic game, so it's a game of nicking stuff, a foraging game where there's the ability of individuals to come along and attempt to nick resources from individuals that have found resources. So, you typically have producers that find food with a probability. Question mark. When they find the food, they can consume V1 amount of the food before anybody else turns up, but then. Before anybody else turns up, but then some individuals can scrounge, other individuals can scrounge on that food. And we're going to assume that there's kind of a maximum number of scroungers that can never turn up. And when they arrive, everybody shares V2 amount of food, amounts of resource. So you get a finder's advantage, but then everybody comes along and you share it with any scroungers that come along. And the payoffs in these games tend to be negatively frequent dependent. Games tend to be negatively frequency dependent, so it's just the number of other producers here on the x-axis and the payoff to if you produce and if you scrounge. In both cases, even if you produce, the payoffs are going up because basically there are fewer and fewer scroungers here in the population, so everybody's getting more food, but the lines cross here. And so producers do best when they're rare, better than scramblers. Better than scroungers when scroungers are rare and when scroungers are common. No, it's the other way around. Sorry, I've got to put this properly. So, when scroungers are rare and there are lots of other producers, scroungers do well, but when scroungers are common, they're nicking food, there's no food to share. So you've just got a fatigue kind of negative frequency at what we've done in this is we've suggested that individuals are going to be playing T rounds of this game and they're T rounds of this game, and they're interacting on a simple network. So you've got groups of n, but you interact with k neighbors, if you like. So you don't necessarily have to interact with everybody in your group. And what we've added to this is simple, I can't remember what that's supposed to be, action value learning. So the results, we've done learning in. The results, we've done learning in two ways. We've got Act Critic and Action Value Learning. Action-value learning, I'm just going to tell you because I find it more intuitive to explain how it works. We've done it with Act Critic because it allows you to investigate the evolution of different types of mechanisms within the learning process. Action-value learning kind of matches what psychologists have described using. Have described using in the Raskola-Wagner equation to describe how associative strength changes with experience. And basically, individuals have kind of subjective estimates of the current reward of choosing one or other of the actions, P or S, QP and S I T, to choose and choose to produce with probability P IT and S otherwise. IT and S otherwise. And basically, you update QPIT and S, Q, S, IT according to the reward that you get. So P is updated in this way, as you experience things, and these subjective valuations that you take are updated according to these equations. And the key thing to know here is this parameter. And the key thing to note here is this parameter alpha, which is the learning rate. Just basically how strongly you're influenced, how much you value the different subjectively, the different rewards, taking the different actions, how much that's dependent on recent experience, how strongly it's dependent on recent experience. So the higher alpha is, the faster the learning rate. Usually it's interpreted as the learning rate parameter. So, first of all, in order to understand the evolutionary consequences, I just have to walk you through the kind of developmental consequences of having, which was the subject of a paper we published in 2022 in Proceedings. But it's important to understand the evolutionary consequences, to understand these developmental consequences. So, we ran these on different kinds of networks, you know, the kind of small world network here. Network here and just a kind of regular network where basically the K number of neighbors is just the same for everybody. You know, it's just evenly spread. So even though there are lots of individuals in the group here, you're only interacting with four individuals in both of these networks. The small world networks just has a probability of rewiring one of those. So you're not necessarily just kind of, there's a bit of stochasticity. Kind of, there's a bit of stochasticity in the way that the networks are distributed within the group. And what you typically find when you've got reinforcement learning of any kind, developmentally, are things like this. This is just showing the probability of producing as a function of the rounds playing the game. So essentially, when you start, you pick one of the actions at random, so to produce or round. As soon as the first As soon as the first person has picked, the first individual has picked their choice because of the negative frequency dependence, it changes the reinforcement that the neighbours are going to get. And it makes the alternative strategy, so if you've picked to produce, it'll make scrounging more valuable, a little bit more valuable. And then what that means is that your neighbours will tend to then start to produce at a relatively high probability, which then At a relatively high probability, which then, through the negative frequency dependence, means that the reinforcement you get from producing, if you started producing, will just increase. But this tends to iterate, and you end up with kind of specialization, if you like, or what we call polarization. And that typically just robustly happens. But, and so this just illustrates, I mean, the key kind of controlling, it's not surprising, and we're going to talk you through this, it's a bit complicated, but this just shows you. But this just shows you when you're in very large networks, so you're interacting with lots of other individuals and you've got very slow learning rates. This is just the probability density function associated with the probability associated with plane P. There isn't really any specialization. But when you've got very small or relatively small networks, tight networks, and fast learning, individuals cluster, you get individuals ending up eventually always playing. Always playing producer or never playing producer. Okay, and that's, and you can look at the subjective valuations in those two cases, and you can see that the subjective valuations of producer, of some individuals for producing is high, and for other individuals, the subjective valuation of scrounging is high. Okay, and that's it's driven by this, the fact that these returns are negatively frequent, even though everybody's starting off, you know, this. Everybody's starting off, you know, this is in an unbiased world, everybody's starting off with an equal probability of playing either of the tactics. But because of negative groups, independence and learning, you end up, this simple asocial learning, you end up polarizing. But small groups, tight interaction networks, this effect is the strongest because the negative frequency dependence is the strongest. Feedback is the strongest, right? Feedback is the strongest, right? Any given individual's behaviour will have more of an effect, a stronger effect, on the other individuals around them in a tighter network. And that just starts to relax. And fast learning, again, does the same thing. It just increases the rate at which that kind of feedback process gets started and is maintained. And stockasticity doesn't, you know, any of the stockasticity that can come in from the fact that. Stochasticity that can come in from the fact that some individuals have started in different situations, the individuals that you're interacting with, that tends to erode, not, you know, have less of an eroding effect when you've got a faster learning rate. Okay? Okay, so you're happy with the notion that in these kind of games, you'll end up developmentally, through a process, ending up at some sort of DSS to coin data. To coin Dave Stevens' developmental stable strategy, if you like, developmental stable outcome, where you've got a lot of polarization. And what this means is individuals are either playing, producing, or scrounging. But the key thing when you're trying to think about the evolutionary consequences of this is that in any of these games, these negative frequency dependence games, at equilibrium, at this DSS, the individuals that have been. The individuals that are stuck playing one of the strategies are going to get a lower payoff than the individuals that are stuck at the other. Because in all of these games, there's one strategy that's slightly more pro-social than the other. Okay, so in the producer-scranger game, the producer is more pro-social than the scrounger. Hawk Dove game, Dove is more pro-social. And it's the more pro-social strategy. If you're stuck playing that, you end up having a lower payoff when you reach these polarised averages. When you reach these polarized outcomes, there's no incentive to change your behavior still because you'll get even lower if you did something different, but you end up getting lower payoffs at this DSS. And so what this does is it generates selection on biasing your behavior so that you're not likely to play, end up stuck playing the pro-social behavior. Okay, and that's the root of the evolutionary consequences. I'm just going to go through. These are the kind of I'm just going to go through, these are the kind of newer results. So, this shows you actor-value learning and actor-critic learning. The difference is that what you can look at is the evolution of the bias in the rate at which you value the, you know, how much value you get to returns that you get from playing one strategy over the other. You can get a bias in that, so it becomes unequal. And actually, this diagram just shows you that the bias. Just shows you that the bias, you get a bias against producing, evolving, and it's evolutionarily stable. And the reason you can do that, we tried this in Act Critic too is because there's a different thing here. It's like the initial probability of playing, the initial bias towards playing one or the other. And again, you get this bias away from it. But probably the more interesting thing about this is that even though these are ESSs, they're equal-evolutionary minima. Evolutionary minima. Okay? And that's because of the negative frequency dependence. Because even though the ESS, there is this biased ESS kind of playing strategy, it pays mutants to have an even more extreme kind of bias in either direction because it forces your opponents to play the opposite, which is what you want them to do, negative frequency dependence. Frequency can. Okay, so this is very, very general. It's strong. Some of these kind of dips and things, the minima are shallower and deeper in different games. So for example, the Hawk Dub game, it's not quite as deep as this. It's not quite as nice as one of the reasons that I didn't do the Hawk Dub game. It doesn't look quite as pretty, but you certainly get it. We've also done it, tried it in sexual selection games, like Corner Satellite kind of games, and things like that. And you get the same result. You get the same result. And then basically, this drive maintains a lot of variation. And this just illustrates some evolutionary simulations. With sex, thinking with testimonial inheritance in 50-50, you end up with unimodal distributions around the ESS, but you get quite a substantial amount of variation maintained. And if you do it asexually, you end up with polymorphisms. These things are not stable. Stable. The probabilities of being at any of these different things kind of fluctuate, but you end up, and that's both for both kinds of learning. But it's driven by this very general process that you're forced, learning generates this developmentally driven biasing of behavior that generates a risk of being stuck, being the nice guy, and then that. And then that generates kind of evolutionary selection, you know, evolutionary pressures to sort of bias yourself away from being stuck there. And because of the negative frequency components, they're evolutionary minimum. So that's it. I mean, I kind of finished very quickly, remarkably quickly, but hopefully we can have a bit of discussion about this. So, you know, the take-home messages: asocial reinforcement learning with negative frequency-dependent rewards, these initial arbitrary. You get initial arbitrary choices of actions, reinforces the value of the original action because of the impacts on what your neighbours are valuing. And polarization or specialization develops. It's stronger in smaller groups and with faster learning rates. So this we would then predict from this kind of analysis that you would expect these kind of pressures from learning to evolve in systems where you've got relatively tight interaction rates and you want to be adjusting your behaviour. And you want to be adjusting your behavior very strongly to recent experience. And biases evolve to avoid pro-social trap, and they tend to be better to be more extreme than ESS. And it only happens with learning. So this need to be more extreme than the ESS and these biases won't happen without learning. Learning is really crucial to this simple extension. So, I mean, what we're selling the idea is that learning may be a substantial source of Source of variation in a lot of different systems in these relatively simple social interactions. I mean, social interactions are going to be very general because you interact in conspecifics to gain resources, so you expect these kind of social interactions to be fairly general, and learning as a mechanism is very taxonomically widespread. So hopefully, this will kind of splurge people to think a little bit more. So, oh sorry, that's the polarization metric. That's the polarization metric that we use to describe polarization. But anyway, it's basically something that you guys might like write fixation index. I didn't do this on purpose, but it's just I tend to have it at the end of the slide just in case anybody asks me technical questions. That's it. Thank you very much. You brought up nicely with the inevitable session may start. So, oh, fascinating. So, oh, fascinating stuff. Did you look at the evolution of learning rate? What if you allowed learning rate? Yeah, we didn't because we couldn't think of a way of kind of doing it in the same framework. So, I mean, it would take, I mean, it is an interesting thing and it is something that we're attempting to follow it up with, but we're struggling to come up with a nice, a neat way of doing it. The reason I'm interested is because it reminds me of the Red King effect. Where Red King effect, you've got two individual sides. You've got two sides involved in mutualism. The more slowly evolving one often gets the much better deal because its hands are tied. And so, here, one form of biasing yourself is starting out fairly neutral but learning slowly. Yes, no, no, sure. No, I mean, that's interesting, actually. Yeah. I don't think we looked at one way we could do that is allow the learning rate parameter to evolve. Yes, I don't think that that's. I don't think that's something that we did. I mean, it would be very interesting to see you because you would have a. This feels like a situation that's just designed for getting you in one of these ripped king kind of. I mean, one of the things I will say is that we also did it with skill. So we can, you know, even if it's not learning, you can also get, if you allow there to be some sort of, you know, a parameter that explains how good you are at playing those different things, you will also get biases. It just shows you that. It just shows you that this should lead to a co-evolution of mechanisms controlling behaviour as well as the behaviour itself and variation. But yes, that's good. I mean, it's an interesting thing. Well, I think we'll definitely kind of have a other questions. Mark the video button.