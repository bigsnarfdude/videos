Many thanks for the invitation, the introduction. Good morning, everyone. First, I want to apologize for being online due to some personal reasons. Today, I will talk about a statistical approach for simulating the density solution of a McKinney-soft equation. This is a joint work with Mark Hoffman. I will start with the definition of the McKinney-soft equation. The McKinley-of equation that we consider in our paper has this form where the drift B depends. the drift V depends not only on the position of Xt but also on its probability distribution mu t. Here mu t is the law of X t. So with Mark we asked ourselves how to simulate the density of mu t. This question is interesting because the density will be the solution of the making mass of PDE. So in this presentation this question will be decomposed into two parts. In the first part In the first part, we will discuss how to construct an estimator for the density μt. And in the second part, we will discuss how do we prove the convergence rate of this estimator. In this talk, we set the dimension d equals to 1 to simplify the notation, but the method presented here also work for high-dimensional settings. Before we start, let's take a quick look at the assumption. Let's take a quick look at the assumptions. We assume that A1, the law of X0 is sub-Gaussian. A2, the dependence on the major argument in the drift B is the integral of another function B tilde with respect to mu. Moreover, we assume that this function B tilde is regular. A3, we assume that the sigma is uniformly elliptic and The assumptions A1, A2, and A3 guarantee the existence and uniqueness of the solution, and of course its density to the McKinley Lasso equation. Now, let's start part one, the construction of an estimator of the density. So, this is a McKinley-soft equation. In this part, we will In this part, we will discuss how to construct an estimator for the density of μt. To answer this question, there are three steps. Step one, we use a particle method to simulate the probability distribution of Xt, which is inspired by the propagation recalls property of the McKinley-Soviet equation. After this step, the probability distribution μt is probability distribution μ t is simulated by this described measure, which is an empirical measure over a particle system. The second step is the bridge from particles to the density function. Similar to the classical statistic theory, we will use a kernel function to estimate the density from the particle system. The third step is model. The third step is more technical. We try to figure out how to choose that kernel function k here. Can you see? And the bandwidth eta under appropriate conditions. Now let's start step one, the definition of the particle system. So we start with the Machinasov equation. First, we consider a time discreditation. a time discretization. We choose the capital M as the time discretion number and define H as the time step. The Euler scheme is defined here, which shares the same idea as an Euler scheme for a standard diffusion process. I always call this Euler scheme a theoretical scheme because we can notice that here we cannot We cannot simulate the mu bar t directly. To do this, we need to add a special discretization, which is similar to the publication Michael's property, but it is in the discrete version. So we define this n-particle system where in the major argument here, instead of mu bar Tm, we consider an empirical major over the particles. Major over the particles at each time tm. So, with this definition, we have the convergence with respect to the Washachein distance when the time step H here converts to zero and the number of the particles n converts to infinity. Now, step two from particles to density. After the particle method, we have an n-particle system with a described improgmator. Described improgmator which is close to μt with respect to the Vashakstan distance. Now, the question becomes: how to simulate the density of μt from this particle system? Recall the classical kernel density estimation here. If we have an IID sample following a probability density function f, we can estimate the density f by using the By using the f hat eta defined here, where k is a kernel function and eta is a bandwidth, which is a smoothing parameter. Similarly, here we have a particle system where the particles are identically distributed but not independent, but not far from independent. Far from independent, especially when the number of the particle n is large. So to simulate the density of μt, we can still apply the same idea as before, that is using a kernel function and bandwidth to define the estimator of the density function. Here, of course, our estimator will have nh eta. Depends on the choice of the current. Depends on the choice of the kernel k, the bandwidth eta, and also depends on the time step h and the number of the particles n. Once we define this estimator, mu hat and h eta of the density function mu t, our first main result is a concentration inequality of this estimator. So for a fixed xn for h and eta small. X and for H and eta small enough, we have the probability of the distance between our estimator and the true density larger than epsilon, smaller than some exponential of minus n epsilon square. So here the constant in this inequality depends on the kernel k and the bandwidth eta. So further question will be So, further question will be which kernel K and bandwidth eta will choose. I call this question a crucial question because even in the simplest case, the choice of the kernel and the bandwidth will change the simulation result. Here is a trivial example. We take an IAD sample of the standard normal distribution. Distribution. On the left-hand side, we use the same bandwidth, but different kernels, the Gaussian kernel and the cosine kernel for the density estimation. And on the right-hand side, we use the same Gaussian kernel, but two different bandwidths, 0.2 and 0.7. We can clearly remark the difference between the different simulation results by using. Different simulation results by using different kernels and different bandwidths. So, this is just a simple example. If we return back to our Machine-Lassov equation, we still have the same question, which kernel k and bandwidth eta will we choose. In the paper, Bosi Dale in 1970, the author studied the density estimation with a Gaussian kernel and discussed the kernel and discuss the convergence rate with respect to the bandwidth epsilon. In another paper, this one, the authors still use the Gaussian kernel, but in this paper, they also choose the bandwidth eta same as the time step H in the order scheme. Of course, Gaussian kernel is a common choice for the density estimation, but we ask ourselves: is there any better? Ask ourselves: Is there any better choice for the kernel K and the bandwidth eta? So, in our paper, we use high-order kernels. What is it? The definition of the order of the kernel is here. So we denote by L the order of the kernel. A kernel of order L is a bounded function k such that the integral k such that the integral of x power kx equals to zero for k between one to l and the integral of kx itself equals one. Here are some example of Gaussian based high order kernels. We can see that Gaussian kernel is of order one and we can produce high order kernels based on the Gaussian kernel. So So on this page, I will show the figures of Gaussian-based high-order kernels. So the first one is Gaussian, which has order one, and then order three, order five, seven, and nine. Now, the question is why we consider high-order kernels? In our paper, we prove the benefits of using high-order kernels. Benefits of using high-order kernels, the convergence rate. I will directly present the corollary 2.11 because this one is easier to read. So under assumptions A1, A2, and A3, assume moreover that the true density that we want to estimate is k times differentiable, and we will use a kernel of order L. In this case, In this case, we can compute explicitly optimization of the bandwidth eta here, which is the number of the particles n power minus 1 over 2 times minus k L plus 1 plus D. And this choice, this optimization of the bandwidths leads to this convergence rate where the convergence rate in n, the number of n N, the number of the particles, depends on the order of the kernel L here. So, which means, for example, assume that the true density is regular enough, k equals to infinity, then if we use a kernel of order one, for example, the Gaussian kernel that everyone use, we get a convergence rate n power minus four over five. minus 4 over 5. And if we use a kernel order 3, we get a convergence rate n power minus 9 minus 8 over 9, which is faster than the first convergence rate. Now I will show you a simulation example to confirm the relation between the convergence rate and the order of the kernel. So in this example, we consider this special matching less of equation. x0 is a Gaussian. Equation, x0 is a Gaussian random variable means ray and variance are half. This example is artificial, which is a modified stationary Einstein Ublimbik process. We choose this example since at each time t we know xt is still Gaussian since it is stationary. So we can compute the simulation errors. As we want to study the convergence rate in N. We fix the time discretization parameters. We set t equals to 1, m equals to 100. We then compute the particle system with different particle numbers from 2 power 7 to 2 power 15. Then we compute the estimator by using different Gaussian-based high-order kernels and optimal bandwidths that we saw before. We saw before. The simulation error is computed by this formula. We compute the distance between the simulated and true density function and repeat 30 times for the multiplical computation of the expectation. On the left-hand side, we show the relation between the log error and log n, n is the number of the particle. So, first, we can Particle. So, first, we can remark that the error decrease for all kernels when the number of the particles increases. But a kernel of high order, for example, this one, the green one, order nine, has a higher convergence rate. So, if we take a look at the slope of this log-log error on the right-hand side, we can remark that, for example, the kernel of order 9 has the kernel of order 9 as the largest absolute value of your slope. So this example clearly shows the benefits of using high order kernels when the density is regular enough. However, there is a problem. The regularity of the density mu t is usually unknown or difficult to compute from mu0, v and sigma. So without this information, we propose another data-driven band. We propose another data-driven bandwidth selection method, which is a variant of the classical Golden-Sugar-Lipsky's method. So, the idea is we select the bandwidth eta depending on the stochastic particle system itself in order to optimize a kind of trade-off between the bias and the variance term. Here, there are many notations on this page, but I will show you the idea. We pick a five. pick a finite set for the bandwidths and define these two terms A eta n and V eta n. The term A eta n represents the bias and the term V eta n represents the variance. So the data dragon bandwidth is the eta such that the sum of these two values reaches the minimum. So with the definition of this data So with the definition of this data-driven bandwidth, we obtain inequality for the simulation error in the paper. Here, our limitation of this method is this pay factor omega, which is which theoretically depends on some information of new t itself, so it is unknown. But this pay factor comes from the original golden sugar lip skis method, so we cannot avoid it here. But we provide some numerical experiences. But we provide some numerical experiments to test the result with the different pay factors. So, here I will present a quick cross-experiment for the selection of the data-driven bandwidths. We compare these two examples. Experiment one, we consider a double-layer potential equation with a drift u prime smooth energy, the blue one. And in the second example, we add a non-smooth force V in the drift. So the V is in the second figure here, which is known smoother than Lipschitz continuous. We add this term to reduce the smoothness of the density that we want to estimate. So the objective is to compare the bandwidth selection result with these two settings. For these two For these two experiments, we try with different particle numbers, p factors, and the positions of x around zero. Here is a bandwidth selection result. Recall that experiment one is with the smooth drift, and in experiment two, we add a non-smooth force. So, compare these two figures, we can get an empirical conclusion, which is a Conclusion, which is data-driven bandwidth method, but just a smaller bandwidth when the true density is less smooth. Now, I will show the figure of the density estimation of the double layer potential equation, which is experiment one. This equation has no explicit solution, so we cannot compute the convergence rate of the error as before, but here we can still compare. But here we can still compare these two figures. In the second figure, we simulate the density with 2 power 15 particles, and we can remark that the simulation results are close to each other with different kernels. So we may consider it as an almost true density. If we go back to the first figure, we can remark that the estimator with older Estimator with order nine, for example, is closer to the almost the true density below than other simulation results using lower order kronos. So this comparison shows that with a super choice of the bandwidth, we still have the benefits of using high-order kernel, even we don't know the true density exactly. Now I will quickly present how we I will quickly present how we prove the convergence rate of this estimator. Sorry, no question? Okay. In this part, our objective is to prove the convergence rate of our estimator mu hat and h eta to the true density in the To the true density in the LP sense. So, the idea is that we decompose this error into three parts: part one, part two, and part three. So, the k eta is defined here. The major mu bar and h in part one is the imperfect major on the described particle system. And mu bar h in part one and part two is the law of x bar h, which is defined. Our H, which is defined by this abstract Euler scheme. We call it abstract because this scheme is not for simulation, it's only a two-fold proof. And mu bar T in part two and part three is the true density of the solution to the making mass of equation. The idea is part one is the error between the described particle system and the abstract Euler scheme, which is an error in n. Which is an error in n, the number of the particles. Part 2 is an error between the density of abstract order scheme and the true density, which will be an error in H, with time discretization step. And part 3 is an error between the kernel estimation density and the true density, which will be an error in k and eta, the kernel and bandwidths. For part one, we prove about For part one, we prove a Beinstein type inequality for any bounded function phi and then apply this inequality to the kernel function k eta. So the idea is first we apply a change of probability measure by using Jazanov's theorem and makes a connection between the empirical measure of the particle system and the empirical measure of this system Y without interaction. Y without interaction. Next, as the particle system Y are id, we can apply the classical Beinstein inequality for independent and bounded learning variable to this system Y. For part two, the K is to control the uniform convergence rate of the density of the Euler scale. In the paper, Gobella Bora, 2000. Per Gobella Bora 2008, I think this result has already been established for diffusion process. For us, if we want to apply this result, we need to consider the Kim-Lassov equation as a diffusion process. And this is straightforward. In fact, under assumptions A1, A2, A3 that we made, these two assumptions imply the existence and uniqueness of the solution. And of course, Uniqueness of the solution, and of course, its marginal distributions. So, we only need to consider the drift of v as a function of t and x for a fixed plug-in marginal distribution mu t, which is the true marginal distribution of the unique solution. So, for part three, this term is controlled by a standard kernel approximation. For example, Approximation. For example, for curl 2.11, the idea is to use a classical Tyler development, and this part will disappear by using the definition of the order of the kernel. A short summary of my presentation. In summary, we study a kernel type estimator for the density of the solution to the McIntyre-Lassoff equation, and we prove and show empirical evidence of the benefits. Evidence of the benefits of using high-order kernels and the data-driven bandwidths. The limitation of the method. First, the regularity of the density that we want to estimate is hard to know from the data mu0, B and sigma. Another limitation is the choice of the p factor in the data driven bandwidth selection process. It depends on somehow the upper bound of mu t locally around x, which is normally. Locally around x, which is normally unknown. But we still believe that it is a good way to estimate the density by using the high-order kernels and, of course, the data-driven bandwidth. And the assumptions that we made in this paper may be too strong. Here, I will show you another example for this point. For example, we consider the Burger equations defined here. First, there exists a closed form formula. Exists a closed form formula for the density, which can be found in the paper POC dialnine 187. In this equation, the drift here is not even continuous, which rules out our assumptions. However, for the density estimation, for example, if we take 2 power 10 particles in this figure, we can still remark that the estimator with higher order kernels, for example, the green. Order kernels, for example, the green one of order nine is closer to the true density, the black one, than other lower order kernels. So the result is in line with the result of our paper. Even the drift itself rules out our assumptions. I think I'll stop here and thank you for your attention. Thank you, Milati. I think we have time to take one. I think we have time to take any questions in the audience. Is it crucial to assume B is a linear in mu? Sorry, I can't hear clearly. So can you repeat your question? You assume P is a linear in terms of the major value. Is that in there? You mean this one? Yeah, yeah. Yeah, yeah. So the question is: Is this necessary? Right, right. Is it important for your result? It is important. We can generalize this assumptions. In fact, how to say it, this assumption is made for the application of the, sorry. Sorry, later. This assumption is made for the application of this result, the result of the uniform convergence rate for density. It is made for to study the regularity with respect to t in mu t. So, any generalized condition which allows us to get the same regularity in t can be used. In T can be used instead of the linear dependence in the major mu. Okay, thank you. I have a quick question. So for this uniform convergence rate of the density estimation, do you have any insight on whether the rate you have is tight or not? Whether the rate is rate is tight, optimal. Oh, this uh this can uh Uh, this can uh so you mean if the age here is optimal or not, yeah. Um, I think uh under the assumptions made in this paper, it it is optimal, but maybe we, if we make stronger conditions on the drift and sigma, we may get the convergence rate, for example, h square. H square. But under the same conditions made in this paper, this is all ocular. Okay. Very nice. Thank you.