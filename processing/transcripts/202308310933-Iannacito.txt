And Iga were both current or former members of Pruden Ebach. So first I will introduce you what is this blind search separation problem because it's a very specific topic in CINAC processing. Then I will move like the results that we have in this context into the context of the canonical polyadic. The canonical polyadic tensor decomposition that Michelle just introduced before. And finally, I will give you the outline of the algorithms and how we try to improve it because it's going to have quite a bit of problems. So the blind surface operation problem is a classical problem in the signal processing field. So the idea is that we have a matrix where we have basically Matrix where we have basically the observation of different signals and we want to reconstruct the source signal, so to separate the original signals. So if we look at the problem on a mathematical viewpoint, we have a matrix where we store these observed signals, and this is actually equal to the product of a mixing matrix that's unknown and a source signal matrix that's unknown. And the idea is that we want to re-obtain this factorization. So to do it, So, to do it, we can impose some deterministic uniqueness condition. And by deterministic, I mean that this is a condition on a matrix that is always true. So, if we want to make some example, we can impose, for example, that the search signals are statistically independent, and so we fall into the independent component analysis, or we can impose that both of the mixing metrics and the search signal metrics are non-negative. Signal matrix are non-negative. Or we can impose that the source signals are actually can be modeled, their matrix is actually sparse. And in all these cases, we obtain uniqueness for this factorization. However, as you can imagine, in general, this is not true. So, in general, we can always multiply by an invertible matrix both the mixing matrix and the third signal matrix and obtain a new factorization. And this is a big problem. New factorization. And this is a big problem because in the senior processing field, we want uniqueness. Like it's a very important constraint that we have. So what we can do is we can move to generic uniqueness. And by generic uniqueness, I mean that we impose condition on a matrix that depends on a parameter that lives in a certain subspace of a vector space. And this condition has to hold almost everywhere. Almost everywhere. And by almost everywhere, I mean that there is just a set where this property does not hold, and this set is measured zero with respect to an absolute continuous measure, with respect to the back measure. Okay, so once we have this definition for generic condition, we can rephrase the problem of blind source separation. So again, we have our matrix X and we want to factorize it as the product of the matrix. Factorize it as a product of the matrix Z, and that's the mixing matrix, and the source signal matrix S. But both these matrix have entries that are function, that depends on this parameter set. And we can impose conditions on the functions that modelize the entries of these two metrics. So, in particular, we impose that the columns of the mixing matrix that depends on this parameter are linearly independent. And these are just all the conditions we have for the mixing metrics. We are going to have more conditions for the source signal metrics. So we want that all the column of the source signal matrix depend on L independent parameters. So we have L parameters for the first column, L parameters for the second column, and so on for all the columns. But that's not all. Like we want also that all the columns are modelized by are modelized by Are modelized by the composition of a rational function, so a ratio of two polynomial composed with a vectorial function. And it depends on L independent parameters. So this is the general framework. And you can expect it to be extremely, I would say, abstract and not really practical, but that's not really the case. Can I ask? Yes. Is this S intentionally as well? Intentionally S of XIR or is it SR of XIR? Can you? Here's a missing R, right? This is your question. Yeah, it's intentional because each column has its own family of parameters. So this is why the R moves to the C. And the structure of all the columns has to be the same. So maybe we have different polynomials. No, so we have the same. No, so we have the we have the same polynomials, but the the parameters that we are going to put inside are different. So at the end stuff will change, but I expect it to be a bit more clear with the following example. So we go back to the example I showed you at the very beginning and in this case it was constructed on purpose from the mixing matrix and the third signals and then I re-obtained third one. Okay, so in this case we Okay, so in this case, in this very particular example, the matrix and is constant and is full rank, so it perfectly fits the hypothesis we had before. And the search signals that you have on the picture on the vertical right are actually samples of this polynomial, of this rational function, where the parameters in this case are the coefficient of the polynomial. So those signals are just evaluation of. Of functions that have this structure but with different values of the coefficient. So the coefficients are going to be our parameters in this case and this coefficient, if we count them, are the sum of the degree of the two polynomial plus two. And in this case, the vectorial function we are composing our rational function with is just the identity. Yes? Yes. Is uh A1 in the denominator a typo, or is that something that's a little bit of a data? Oh, no, that's a typo. Sorry, no, no, no, it's a typo. Okay, so once we are in this framework, like in 2016, Dominov and Delatauer came up with this cookbook recipe. So a series of conditions that once we have modelized a problem, we can check. And if these six Check and if these six conditions that are pretty technical hold, we can conclude that the factorization we're going to obtain is actually unique, generic unique. So unique up to a set of measuring zero. And I would not really focus on all these conditions because they appear very technical and very analytical, but actually you will see that most of the case of the signal processing problems are naturally satisfied. One clear One clear example is, like in the previous case, the function f was the identity. So you see that clearly point two and three that are pretty technical are always satisfied. So just a quick remark in the case of blind source separation. So previously I told you that the source signal columns, the source signals, are modelized by the position of a rational function. The position of a rational function and a vectorial function. And once we have this type of structure, actually, what we are saying is that the columns of the surf signal matrix belong to an algebraic variety. So they can be seen as the points that satisfy a family of polynomials that describe this algebraic variety. And this is very important because it's the bridge with the canonical polyadic decomposition. So as mission. So, as Michel previously stated, the canonical poleadic decomposition is the decomposition that writes a given tensor as a sum of R, a tensor of rank one. So, visually, this is what we have. And what we can do is matricize this tensor, so I'm holding it with respect to one of the possible directions, and we simply replace, once we know, the canonical polyethylene composition. Know the canonical polyethylene decomposition, the tensor product that we have between B and C with the chronocker product. And we obtain this type of equation. And this type of equation is telling us that B, I, Kronecker, CI are all vectors that if they are matricized correspond to a matrix of rank one. And this is actually identifying a specific calgebright variety. And we can try to apply and And we can try to apply and work in this direction to obtain the CP decomposition. So we can try to use the results we have from the theorem to implement an algorithm. And the idea is that we start with the observed signal matrix X, or the matricization of our tensor, and we try to use this known data to recompute the mixing. Recompute the mixing inverse of the mixing matrix. And once we have the inverse of the mixing matrix, we can apply it to the known matrix 8 and reconstruct the third signal matrix. So how we can do it? Well, we know that A is a column of the inverse of the mixing matrix if and only if X transpose, so the matrix or the observed. Tensor or the observed signal matrix time this column A is a column of S. And being a column of S implies belonging to the algebraic variety. And belonging to the algebraic variety is an equivalent condition for saying that we satisfy these points are roots of a family of polynomials. So in particularly, this can be seen as a vector of inner product, and when we evaluate the polynomial, product and when we evaluate the polynomials that describe the algebraic variety in this inner product we obtain zeros because they have to belong to the variety. And then we can do some manipulation. So in particular we can decide to formally replace the variable of this polynomial by the column of the matrix X and we can replace the multiplication, the scalar multiplication. The multiplication, the scalar multiplication by the tensor product. We proceed in our equivalence, and when we matricise or when we vectorize that structure, that's actually a tensor structure, we end up having a matrix that has this shape applied to the vectorization of a symmetric tensor. And this has to be equal to a vector of zero. So this we can reinterpret it simply, this long chain of equivalence, we can reinterpret. chain of equivalence we can interpret it by saying that a vector is a column of the matrix m power minus 1 only if it belongs to the intersection of the kernel of this matrix q and the subspace of the vectorized symmetric tensors and this is a very i would say abstract result so it's not easy to obtain an algorithm directly from this structure so Structure. So we arrive at saying this. So we need to compute this matrix Q and we need to compute the kernel of this matrix Q and its intersection with the space of vectorized symmetric transfers. These are the new steps in our algorithm. Okay, the very bottleneck of this algorithm is this matrix Q. So Actually, in a paper from 2017 or 2014, I don't remember exactly, IGNAT proved that the columns of these metrics Q, the metrics I showed you before, in the case of the canonical polyadic decomposition, have this very structure. So they are equal to vectorization of the second compound matrices of linear combination of the slice of our tensors. So one of the important terms. One of the important like there are two important points to highlight here. So first is that the dimensions of this matrix Q are actually product of binomials coefficient. So they grow extremely fast and this matrix is going to be extremely quickly, very large. And on the other side, these compound matrices, the entries of these compound matrices are binors of the terminals. So yeah, that's the right reaction. It's expensive and complicated. So, we can try to use some algebraic trick to overcome this problem. So, just to give you an idea of what does it mean, let's assume we have a tensor of size i trying 3 times 2. So, it means that all the frontal slice have we have two frontal slice, all of them made up by three columns. And then the matrix Q has three. And then the matrix Q has three different columns. And we can see that the matrix Q1 and Q3 are the columns given simply by the compound matrix of first or the second slice, respectively, while the second column is given by a combination of compound matrix of a combination of the two possible slice. So we can elaborate a bit on the compound metrics and we obtain that they have that all the three possible That all the three possible compound metrics that we need to construct Q have three columns. And once we vectorize and reassemble together all this column, we obtain that the matrix Q has this shaped, which means that the matrix Q is highly structured, as you can see, because we have roll-wise the same structure, and column-wise, we can see that there are relationships linking the first and the third column. So we can try to exploit this. So we can try to exploit this structure to make our algorithm feasible. And the tool we are going to use is exterior algebra. So the exterior product simply associates to a couple of factors the difference between A time B transpose minus B A transpose. So the outer product of A time B minus B time B. So if we compute from So if we compute, for example, the outer product between these two vectors given there, we obtain a very particular matrix. So we see that on the diagonal we have just zeros, that this matrix is skew symmetric. And the other important point is that the entries we find on the upper triangular part are actually the minor, the determinant of all the possible minors that we have in the matrix that has just the. Have in the matrix that has just the column A and column B as columns. And we are going to use this property to make our algorithm more affordable. So in particular, we can observe that when we compute the vector product between the compound matrix of the matrix that has just A and B as columns, this happens to be equal at twice the vector product, the inner product between the Between the exterior product. And if we develop a bit this equation, we end up that this inner product just depends on the norm of the columns minus the inner product of the two columns. So what we can do to improve our algorithm is not constructing Q explicitly and rather and moving directly to the step where we compute the kernel of Q. And rather than computing the kernel of Q, we compute the kernel. The kernel of Q, we compute the kernel of the grand matrix associated with Q. And what we see, and we are doing it because we notice that when we make the product, the inner product between two columns of Q, we are actually computing the inner product between two vectorized compound matrices. And we can reapply the exterior algebra product to make this computation more affordable. So, the steps we are going to take is pre-computing all the possible inner products. Computing all the possible inner products between the slides of our answer, and we end up having just six functions that depend on all this inner product. And using this function together with the pre-computed inner product of slices to compute and to make the multiplication, say the algorithm we are going to use to compute the kernel of this matrix Q transpose Q. So we have some preliminary enumeration. We have some preliminary numerical results, and I would really like to stress that this is a work I just started doing. So, in this slide, I'm just showing you the results without the optimization steps that we can do using the exterior algebra product. So, in this case, we have five tensors of given dimension and given rank. And in this case, we show the third column of the table how. Third column of the table, how much CPU time we need to recompute the CPD decomposition using the algorithm. And we can see that, for example, just at the last row, we can recompute exactly the CPD decomposition, but it takes seven minutes without the optimization. So our expectation is that that in introducing this exterior algebra tool and this optimization that we can have on the algorithm, we are going to reduce significantly this time. Significantly this time. So, as I say, this is working for us. Okay, so just a bit of remarks. So, I presented you some condition that guarantees this generic uniqueness, explaining you what generic uniqueness is. I show you how we moved from a theorem, so from something that's very abstract, to something that's more concrete, like an algorithm, and I highlighted you the light. I highlighted you the really bottleneck of this algorithm and some possible optimization steps that we are going to take to make this algorithm more efficient. Thank you for your attention. Yeah, so I remember the result of Dominov, what it was saying. I mean, it was very interesting. It was very interesting, but computationally enlightening. So can you say, when you say you're optimized, can you say something about what you expect, or maybe an improvement, the runtime of the optimized algorithm, if you just compute the null space kernel of Q transpose Q with this nice addition of the optimization? So I didn't run so far any experiments, but I can already see. Experiments, but I can already tell you that this is really a worst result. This is the worst result I think you can get because this is an implementation where every minor determinant is computed through the determinant MATLAB function. Like, yeah, exactly. That's my reaction when I read the code. So already, like, replacing that fun that very naive way of computing the determinant by a recurrent way made the code extremely faster. Made the code extremely faster. So I really believe that with the implementation we can do even better because we are pre-computing stuff and after we expect to just have a smart way of implementing the matrix product, the vector, with the use of the C. Because we end up having that all the components of Q transpose Q depend just on C, like can be expressed by one of these six functions. So we have six functions, we pass the parameter that we already pre-computed, and this Pre-computed and this should make the process possible. Is the bottom line the pre-computation of these vectors or? Yes, right now, like indeed, like we moved the problem from computing Q to computing this quantity. And I'm thinking a bit on how can if, how and if, in case, if and how, this quantity can be actually split up. Because, like, the interesting factor is that. That's some background. So these slides are actually a column of a matrix that's orthogonal. Oh, sorry. So we have this XH is a slice of my tensor and actually it came from a column of a matrix. And these columns are mutually orthogonal. But what we have to do, since these are rearranged as a metric and we have to compute inner product between like columns of this matrix, we have to compute inner product. Of this matrix, we have to compute the inner product between sub-vectors containing this vector. And my impression is that thanks to this orthogonality and this particular structure that these metrics have, there is still some space for improvement. So that's the next like. But there are several next steps like implementing first, but I I have the impression that also on this part there is some space for improvement and that's something I would like to work on. That's something I would like to work on in the next couple months. Questions? If not, I'll thank Fatina and for this human input.