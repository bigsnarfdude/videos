Department of Sila. So, I'll present LLP for analoging phenotype information. This is my outline, very short talk. So, I'll start with some textual data linked with Biobank and then some traditional methods doing NLP to extract that information out of Twinkle Notes. And since now we all move to generative AI, I think I have to include something about the recent work on the larger model for this kind of work. And then And then talk a little bit about some community effort building more standards to help for using those NOD technology in the real world study, things like that. So I think thanks to Neil's talk, we'll... I'm sorry, I've lost sound. Oh, really? Can you hear me? Can anybody tell me what happened? Is it working now? Can you hear us? Can you hear us? Hello? Oh, okay, people from downstairs, yeah. Okay. Should I keep talking or say something? Can you hear us? Okay. Good morning, everyone at Burt. Good morning, everyone at first. Yes, if you have a computer and you have the audio on in the room and you are also in the meeting, that is when you're gonna get that weird feedback loop that sounds like so please please just keep that in mind for about the week. Just make sure that if you're gonna meet things, just mute your audio, otherwise, you're gonna have the loop forever to just creating that sound. Otherwise, everything should sound great. Sounds working now, right? Okay, I'll just continue. Can people online hear us? Yes, we can hear you. You sound very nice and very clear. Yeah, thank you. Okay, great. I'll continue. I think the previous talk already convinced us we are adding more comprehensive and heterogeneous data to link with BioBand. One of the examples of us is connecting different types of data, electronic health record, questionnaire. Health record, questionnaire, physical evaluation, even variables, and also environment data together with all the biospacing life. So, one particular challenge that you deal with those heterogeneous data is actually there's a textured data embedded. For example, EHR has a lot of people notes. Questionnaire has all those narrative answers. So, I put an example here with the notes from EHR. Talk about patients, 70-year-old women with a history of diabetes, maladies, hypotension. Of diabetes, maladies, hypertension, things like that. As you can see, actually, the notes provide a lot of details: medical history, social history. That also might be useful for risk factors or study variable, including your analysis. So that's why we want to develop a method to extract those information out of the notes. That's where the NLP comes from. So you can simply think NLP is any system to manipulate or understand the text data. Text data. And it could be different text data. And there's also different tasks of an opinion formation which is like a permit, a search engine against a large collection of text. And there's also question answering about all those machine translation. One specific task I list here is information extraction. Give you a document. We try to extract the specific information out of that document, like what kind of disease the patient has, what kind of drug the patient takes. So that's what we have been working on. So that's what we have been working on for mostly, I think, 70-80% of Kenka NOK work is on this task. I'll show you what are the brief, like a sub-task and what we call information extraction. The first one is the entity recognition. Basically, what it does is you want to recognize in this sentence, like MI of abdominal is the entity of a test. And then June 18, 2008, is the entity of a tempo test. Is the entity of a tempo expression, right? You recognize all those different entities. Second one is called relation instruction, basically trying to understand the context of entity. So you want to know this June 18, the tempo modifier is modified MRI. So you want to know the relation between those two entities. The third one is what we call concept normalization. So you have the term here, Randall Cell Cassinorma, but you then want to map to a standard common knowledge code so you can do some analysis, right? But the code might be. But the code might be not the same as what is describing the text. In this case, malignant neoplasma of kidney is actually the text for code. So you need to map to those standard concept ID, and you can do further analysis. Those are three things we're mainly working on. And over the last 20 years, different NLP challenges opened around those weekly tasks as you can see. And I just have a very brief view of the history. View of the history, what we have done in the past, and I'll talk more about the larger basement. Previous work, like Medali, MetaMap, 1990s or early 2000s, mainly rule-based system. You have a dictionary of all the medical disease terms, and we try and do dictionary local. And later in 2010, we're moving to a machine learning-based approach. So, in this example, patient report with recent GI bleeding. GI bleeding. Basically, you want to look at GI bleeding as a medical problem. How did you do it? You're trying to label a beginning of the entity as B, intermediate entity as I, or other outside as O. Then you assign a label to each of the words. You convert this to a sequence labeling task, right? You can machine learning task. And we have developed all the kind of machine learning methods. And then in 2020, I think many of you also worked on deep learning, and we are talking. Learning, and we are talking about all the contextual vending, like a bird model. And we're doing a lot of pre-training on the Twinkle domain to show the performance improve. And this is actually our work on same data set, trying to extract medical problem treatment tests with different technology, different algorithms. Earliest is more machine learning, the best one is actually 85. But move to the BERT model with machine learning without much feature engineering, you can actually reach 90%. That's what you find. Has reached 90%. That's what you're 5% jump. But now we are moving to more we call generative AI WD. And how this is going to change, we practice our information extraction task. So you probably heard a lot of news about TED GPT, how it outperforms human, like past medical licensing LAM, things like that. But then we are more interested, okay, although GPT is more defined for tax. GPT isn't more defined for text generation tasks, but people also show you can understand text, so we can also use for information extraction. And we did several studies on using different GPS different large energy models for information retraction paths. So there's different ways you can actually interact with those large energy models. The first one is usually we call zero shot or field shot setting. You focus on prompt. You just use the large engine as a blank part. The large engine as a black box. Then you're trying to manipulate the text, the prompt, you give to the model to get the result you want. So the first study we did is on the GPT-3.5 and GPT-4, we're trying to develop a right prompt to extract medical problem treatment tests, the same task I described before. Then we develop a prompt kind of framework for doing this kind of extraction. So it looks like this. So you tell the model what kind of tasks you want. Model what kind of tasks you want. I want to exchange medical entities, then what kind of format you want to get it out. Also, you give it definitions: like what is a medical problem, okay? And then you even can give a few examples. That's what we call future learning, so that the model can learn in the context. And with that, we did a formal evaluation. Actually, when you compare to the state of our Perl model on the exact match, it's still GPT for you with five shots. GPT for you with five shots then is 0.59 versus 0.78 on the clinical burn model. But you think about that model is trained on hundreds, thousands of examples, but GPT5 example. So from that sense, I still think it's impressive. But it's not like with GPT with large angle, we just can just do everything 100% accurate, okay? And then the second thing is. So, paper spaces here, you only should act your five shaft button. Why don't you keep trying? Oh, that's right. So, you're doing a few hundred, just a few shots learning, okay? GPD also have a way. Now, you can actually, instruction turning, you provide hundreds of examples of GP GPT. But then they also charge for that. But then later, we have this, at least it's expensive. When we do first our GPT experiment, When we first start our GPT experiment, we did 12 data sets, not the previous on the biomicro data set. With a few short learning, zero, two, five, and twelve different data sets, all the different settings, cost $20,000 to do all the experiment logic. So we have LAMA later, right? It's open source. So with open source LLMA, you get all the weight. You can do two types of things. One is instruction turning. You give more examples. Instruction training. We give more examples, okay? We do the instruction like the Stanford Alpaca way. The other one is the pre-training, which is more expensive. I'll describe later. So I first described the instruction turning. So basically, we have the same task, but previous ChatGPT, we cannot actually send actual clinical notes today. We use fake notes. Here, with LAMA, we do locally, we can use actual clinical notes. So with Beauty Physician Nodes as a training, a few. Notes as a trending, some a few other data sets. I2B2 is just for the testing. We do the same thing, then we do this what we call instruction turning. You're converting all those annotated data to an instruction. Then you give those hundreds, thousands of examples to the model. Sorry about the internet. So which size Lava model do you use? I have three models, seven billion, seventeen billion, seven billion. We use them, all three of them. I have the All the real stuff. I have the results. Yeah. Because, sorry, because you were mentioning that you didn't have to be things expensively API. But running DRAM has the... I'll talk about that later. Okay, sorry. No problem. So I talk about performance first, then I'll talk about GPU usage. So before, remember, I show you, just give a few examples to GPT, the performance is not as good as fine-tuned supervised model, you implement. But over here, we do see several goods. We do see 7 billion, 13 billion, 70 billion. It actually, with those old examples, do the fan tune, we reached the similar performance back to incorporate or even better performance compared to that. But this one, especially on I2B2, I think that's interesting because it's just a testing set. The model didn't see any upgraded from that data source. It's actually showing 2% better than the BAL queuer-burned model. I feel like it's the generalizability actually. Feel like this is the generalizability, actually, it might be pretty good. But still, think about all the other costs we're still debating. I'll talk about later when I finish the pre-training. This is another example of instruction turning on Lama for different types of entity. This is for social determinants, health, like geographic location, employee status, social support, isolation. Status, social support, isolation, things like that, which is actually part of the CAS project that Sila just mentioned. We're trying to extract those social determinants information out of the Trinodes. And you see actually Lama's performance, in this case, we actually have four different sites, okay. Harris County, this is UT Federation, this is Mimic, this is the Mayo Economics. The LAMA performance is actually showed much better performance compared to the previous one. And this is also convinced us because LAMA is trained. Convenience because LAMA is trained on open domain data, which may already seem a lot of those social determinants from other documents. That's another interesting actually show. You may can use those large informal for those actually non-clinical entities, maybe even perform better. And then I mentioned there's a third way to improve the performance, to continuously pre-train. LAMA model is open, you have all the weights, you can do continuous pre-train, use a lot of tangible nodes. Use a lot of Tinker notes. That's what we are doing. We will collect 129 billion of tokens from literature, from Tinker Notes, from Mimic, feed it to that, do continuous pre-training. This one is very consuming in terms of computational resource. It requires 2008100 GPU running for a month. So it's about 100,000 GPU hours. If you go to Amazon, they charge $4 per hour, might be 400 KE. Might be 400k easily with, you also have to do a lot of experiment to really make it work. And then we still do instructional finding, you found medical knowledge to build the question answering here, appears, we can do the fine-tune again. So this one is smaller. The fine-tune is like an 8 GPU run for a week. It's more affordable. Then you also can do past-specific fine-tune, like I showed it for social determinant health for medical entities. It's also very It's also very reasonable computation. But we actually showed this actually, we call this Miyama, but actually quite good performance on different NLP tasks. But I have to point out actually the improvement on the NER task specifically is not much better compared to just fine-tuning ARMA. So that's related to the question you asked in the nugget. What's the trade-off? You have different ways to do pre-training, very costly. Costly, instruction fine-tuned or using GPT. And so basically, this is all the choices you have. I think for information instruction, right now, the recommendation might be just start with open source to instruction finder. You don't need much resources, GPU resources, to train it, but the performance is pretty reasonable. But then you still have to compare this LAMA-based approach with previous. Based approach was previously like a bird-based approach. I have a lot of collaborators that want to use the Lama-based pipeline because everyone wants to claim I'm using generated larger for this. But my suggestion is that you have to consider your local environment. A lot of clinical parameters, they don't have GPUs at all. And how can I run this model? And then with the GPU, still, it's slower right now, now. Okay? Glama-based model right now, running a node sweep of. Model right now running on notes, we probably need four to five seconds. Previous one, one to two seconds. And performance gain is maybe two percent maximal. So, which one you want to use for this information question, I think, still question. It has to be depends on your needs. All right, I don't know how much time I have, but I'll do quickly. So, now we have the methods, we have the tool, but the question is: we show the success. We showed the success on MPPGO study, but now we often have to do study cross-sites, right? You want to, like, say, federated learning, you ask the question about each site on the same data model, so we can run them. So then, how can we standardize this process, how you store the texture data, how you run NLP, how do we do quality control on the NLP regard? Because there's different NLP pipeline solutions. So that's why we started this community effort, actually, trying to spend. This community effort actually trying to standardize this process and to make the study become more reproducible. And this is actually, I don't know if anyone, I think at least someone heard of this OMAP common data model. The idea is that for the different data sources, you convert to a common data model with standard vocabulary to define all the codes. Then you can run that as a process within one set of uh uh code. And uh we we didn't So we're trying to develop a stand-up representation, build a method and tool, and conduct uh conduct the studies actually use those texture data in the typical study. And we propose two tables in this OMAP command data model. One is the notes table, one is the notes NLP table, which is showing you how you should store all the texture data. What are the meta information you should include in? And when you run NLP, what are the meta-information, the the system name, the version, the output. The version, the output map to the standard concept, things like that. And we also actually, in addition to the representation, we actually also prepared a workflow with script showing you, if you use this NLP system, we have this wrapper to help you to convert the output to the standard format. Then how to convert from no NLP to the final clinical tables. We have a paper on this. And then I just show you this model. This model is also used in the OBAS. I was a part of OBAS DRC at the Vandago until last month because the budget cutting will slow down the NLP process. But this is what we're proposing, this whole, the back end of the DRC curation, where I should have NLP workflow. Just follow what the NOTS table, NOTS NLP table, and then to extract all the concept of that. And then this is another example in the M3C. I don't know if you In the M3C, I don't know if you heard, it's not, I wouldn't call it bow band. I don't know if there's a bowel specimen collected. It's more like the COVID cohort. But in this case, it's slight different from the all of us deserve. We also talked the previous little about different data models. All of us, all the data go to centralized site, DRC and Vandigo. And then this one is that generally you process, it's still data goes to the centralized Goes to the centralized N3C repository. But the difference is that the NLP process is doing locally on each site. That's why you need to standardize this process and then put the codes out. The code actually goes to the centralized site. So that's a slight difference. I'll just show you some use cases. Finishing up soon. Yeah. Last slide. Just rich information in the texture data and the NLP and the large algorithm might be helpful. And I think it's. Might be helpful. And I think it's still important to actually streamline all the methods that have workflows so we can actually really reproduce the study efficiently. Thanks. Thanks for the team. Thank you. Thank you. We probably have time for one quick question.