Who are selfish, self-interested, and they play according to a Nash equilibrium. So, this could be in systemic risk, contract theory with a large number of employees. It could be when you try to incentivize people to transition to less carbon emissions. And there are other applications, including to epidemics. So, I will maybe mention some examples at the end. I will probably not show the numerical results, but if you are interested, I can share more examples. And here is a And here is a very brief literature review, but there are other papers. So, the first two papers are about contract theory and mainframe games. And then we had this paper with Gokce, Andreen Carmona, and Alexander Orrell on applications to epidemics. And what I'm going to present today is somehow a generalization of the algorithm we had in that paper, where we tackle now continuous space problems and we also have some analysis of the algorithm. Analysis of the algorithm. So I'm going to briefly explain what the Steichelberg Metry game, how to rewrite it as a single-level problem, how to apply deep learning to this single-level problem, and then I will mention some numerical examples. So let's start with Stuckerberg Minfield games. And so, as I've said, the idea is that you have a leader and many followers. And before that, this idea was already studied a lot in contract theory. A lot in contract theory, in leader follower games, and typically this is formulated as a bi-level optimization problem because the leader makes a choice. This choice influences the follower's problem. The follower is going to perform some optimization, and the result of this optimization will influence the leader. So, this is really a bi-level problem, which is different from a Nash equilibrium. Here, we are interested in situations where you have a principal and many agents, and these many agents will. And these many agents will react to the policy decided by the principal or by the leader, will play a Nash equilibrium, and the Nash equilibrium result will influence the principal. And what we are going to do is we are going to replace the population with many agents by a mean field games. So mean field games were introduced by Juan Keynes and Malame and by Zrillion about 10 years ago, 20 years ago, sorry. And so the idea is. 20 years ago, sorry. And so the idea is that you let the number of agents go to infinity, and then you study the interactions between one representative agent and the population distribution. So each agent is infinitesimal. The agents are somehow identical and they interact through the population distribution. So this is for basic mean field game, which focuses on a Nash equilibrium. But here we are interested in the incentives given by the principal. So the picture is the following. You have incentives. You have incentives, let's say lambda, chosen by the principal. Given this incentive, the population reacts and plays a mean field Nash equilibrium. A mean field Nash equilibrium means that you are looking for a pair composed of a control, alpha hat, lambda, and a population distribution, mu hat lambda, which is in general time dependent. This pair is time dependent. And the goal is given lambda to solve the optimization problem for the individual. Optimization problem for the individual, but so that the control you obtain is also generating the population distribution. So this is basically a mean field game problem, but parameterized by lambda. And of course, you could say, well, I'm going to solve it for every possible lambda. For every possible lambda, I get the population distribution mu hat lambda. And then this mu hat lambda influences the payoff or the cost of the principal. So the cost of the principal depends on. Principle depends on lambda, but also depends on mu hat lambda, which is a Nash equilibrium resulting from lambda. So this becomes an optimization problem, but under a pretty complicated constraint, which is that alpha hat lambda, mu hat lambda must come from a mean field game Nash equilibrium parameterized by lambda. So you can rewrite this problem in the language of forward-backward stochastic differential equations by. Stochastic differential equations by parametrizing the Nash equilibrium constraint as an FBSD of MacInvelis type, where x is a state, y is the value function for a representative player, and mu lambda is a law of x lambda. So this becomes an optimal control problem under a Mike Involazov FBSD constraint, but it's still somehow a bi-level problem because this FBSD has a forward-backward structure and the backward is in. Forward backward structure, and the backward is in general hard to solve. So, the approach that we propose is to build on the literature to solve FBSDs, including machinery of FBSDs, using a kind of shooting method. So replacing the backward SDE by a forward SDE, and then writing the whole problem as a single-level optimization problem. So, let's see how to. So let's see how to do that. And I'm sorry if you are familiar with the framework, maybe you understand notations. If not, maybe it's a bit fast. But please feel free to check the paper later if you're interested in the details. I just want to provide the main ideas. So we rewrite our problem, which is the cost of the principle, with the forward SD and the backward SD here, because you see that it starts from a terminal value. And what you do in the shooting method is that. What you do in the shooting method is that you rewrite this BSD as a forward SD starting from y0, but you impose a constraint which is that yt should be equal to the g, which is the terminal cost for the individual agent. So so far we didn't change anything. It's just that we have this extra constraint. But now we can optimize over lambda and also optimize over z and y zero. So we need to choose the initial value of y as well as the z. Of y as well as a z component in front of dw in order to ensure that the terminal constraint is indeed satisfied. So making sure that the constraint is satisfied exactly is difficult, but you can replace that by a penalty and add this penalty in the cost of the principal. So this is how you obtain a penalized problem where nu is the penalization coefficient here and we should take nu large enough in order to ensure that the penalization Enough in order to ensure that the penalty is indeed satisfied or as much as possible. And if the penalty is zero, then yt is equal to g which is what you want for the constraint of the BSD, which means you have solved the BSD correctly, and which means that the population is really following a Nash equilibrium. I mean free game Nash equilibrium. So now it becomes really a single level optimization problem where the controls Problem where the controls are lambda for the principal and zy0 for the population. We look for lambda either as a function of time only or time xt and mu t as well, depending on the kind of policy you want to allow. z is a function of t and xt and y0 is a function of x0. So these are three functions that we can then replace by neural networks. So we approximate the So, we approximate the new controls, lambda, z, and y0 by neural networks. We replace the mean field by some approximation, for example, an empirical distribution with a finite number of samples. We discretize time, and from here we can simulate trajectories and compute the empirical cost. So, we have a result in the paper where we show that under some suitable assumptions, the new problem, which is discretized, penalized, Discretized, penalized, and with neural networks approximates indeed the original problem, at least as far as the value function is concerned. And this result still holds if lambda is not a function just of time, as I did for simplicity in the slide, but also of xt and mu t potentially. Numerically, we also have some examples where the principles policy can depend on the whole trajectory and On the whole trajectory, and the interactions can be through the distribution of control and state. So, there is a special case of interest, which is in contract theory. In that case, you just have a terminal payment, U of C, where C can depend on the whole trajectory. And in that case, actually, you can get rid completely of the penalty because the constraint can be embedded in the choice of the terminal cost and why. Terminal cost and why. So I think I don't have much time for the numerical results, but you can still have a lyric for the computational computer problems. No, no, but yeah, okay, thank you. Thank you. So very briefly, we have an example which is a systemic risk model with a regulator. So this systemic risk model of Carmona, Foucault, and SUN has been studied a lot in the literature. And the idea is that there is. Literature. And the idea is that there is one of the parameters that can be interpreted as a parameter chosen by the central bank in order to incentivize borrowing and lending or discourage borrowing and lending. And so here we add this layer of complexity by adding a regulator to the model who can control this extra parameter. And the model for the banks is linear quadratic, but once you add the regulator's problem, it's not that simple anymore and doesn't have a closed form solution anymore. So So, this is one of the examples that we studied. Maybe for the sake of time, I will move on to the next one. The next one is from contract theory, in which case the cost of the principal is relatively simple, and the cost of the population is relatively simple as well. But here, Xi is a payment that is given by the principal to the agent, and this Xi could be trajectory-dependent. So, for this one, instead of using a standard feature. Of using a standard feed-for-wide fully connected neural network, we use a recurrent neural network which can take into account the whole trajectory. So, this problem was actually studied by one of the first paper on the topic that I mentioned before by Eli Mastrolia and Posamai. Okay, so here we can recover actually the explicit alpha that was obtained in the paper of the three authors I just mentioned. And there is a third example which I mentioned as Third example, which I mentioned at the beginning, which was from epidemics. So, this is an example we studied in a previous paper. This one is actually a finite state. There are only three states here, S, I, and R. And the transition rates can be influenced by the players by choosing somehow their socialization level. So, we also introduce a regulator here, and the regulator can incentivize the level of socialization, for example, by penalizing a large level of socialization. A large level of socialization in the midst of the epidemic in order to somehow lower the curve or flatten the curve. So, we studied this example and also another example more complicated with more states with the same algorithm as what I described today. So, I'm going to stop here. We covered mid-field games and FPSDEs very briefly, Stakerberg Minfield Games, how to go from the bi-level optimization to constraint optimization problem with a terminal constraint, and then the Problem with a terminal constraint and then the single-level optimization problem. And then we use deep learning algorithm and showed some numerical examples. In the future direction, there are still many theoretical questions about existence, uniqueness of solutions for general Stacker-Bergman-free games. The question of convergence to Nash equilibrium when we use the shooting method I presented. More applications in economics and finance. And I want to mention that not only the paper is a joint work with Gokcev, but also the slides are. Work with Goche, but also the slides are a joint work because actually they mostly come from Goche. And you can check the paper if you are interested online. Thank you very much. Okay, thank you for this nice talk and for being on time. So we still have a few minutes for questions. Can you go back to that example by Caimona, Fuk, and Zoom? I just wanted to understand what is the difference in the behavior. What is the difference in the behavior of the regulator? Because they already have a regulator in that page, right? So, yeah, thank you for the question indeed. Can you see the slide? Is it working? Okay, great. So, yeah, so what we see here is that there is this formula for the cost of representative agent, which involves the mean of the population. And we have this parameter lambda here, which in their paper is. Which in their paper is fixed, and they say that it can be interpreted.