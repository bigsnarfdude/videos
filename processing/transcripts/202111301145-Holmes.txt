Thank you. Thanks, everyone. It's great to be here. Great to be here in person as well. First scientific conference, like I'm sure it is for many of you in a couple of years. So really enjoying the discussion and the presentations so far. So I'm going to talk today about some joint work with Edwin. Joint work with Edwin Fong, who's a PhD student just finishing up, and this is one aspect of his doctoral work in his thesis. And it's joint with Stephen Walker, who many of us know from UT Austin. And Edwin's thesis has been looking at different aspects of predictive inference, and I'll explain what we mean by that. And we're quite excited by this particular viewpoint of Bayesian inference. Inference and so I say a view towards objectivity, meaning that I feel that predictive inference can give us insight into how we might produce procedures which have certain, let's say, objective inference properties. Okay, so I'm going to start off with discussing exactly what I mean by predictive inference, provide simple examples, some extensions, before covering some examples. Covering some examples. So, motivation, just to get us on the right kind of starting point, is that Bayesian inference typically is it possible to remove the talk? Otherwise, you'll have to guess what's in there. I think you can guess on this one. So, Bayesian inference typically begins with the specification of a prior of a With the specification of a prior of a likelihood function, a sampling distribution for data y and a prior on the parameters of that sampling distribution, following which using Bayes' rule, we get the posterior distribution on the parameters of the sampling distribution. And so we'll start with the simplest case. We're going to consider y iid from some distribution. From some distribution, we're going to assume the model is true here. So we have some true theta naught, and we have the Bayesian posterior. And of course, from that, we get the prior predictive, we get the posterior predictive. And kind of here's a question which, you know, for us that work in applied settings is kind of often of interest: is what is represented in this posterior distribution? Yeah, so we've got prior uncertainty about theta, we get data. About theta, we get data, we update using Bayes' rule, and we get this object here, this probability measure here. And what is the source of the uncertainty in that measure? And the predictive point says that it's the missing information. So, if you define a likelihood function, you're assuming an infinite population. It's a mathematical form for It's a mathematical form for assuming some infinite population. And it's the missing or not yet observed yn plus one to infinity that is the source of the uncertainty because if I really had observed the complete population then any target S demand or any parameter of interest would of course be known exactly so I'm going to use like theta over Like theta open bracket y1 to infinity to know any kind of consistent estimator for the parameter or estimand of interest. And here, in this setting, the parameter of interest is the thing that indexes the likelihood function. But that doesn't necessarily have to be that doesn't have to be the case. So we're interested in some population statistic. Okay, so. Okay, so the predictive inference approach is going to focus on the joint predictive for observables as the primary tool for analysis. And we're going to work directly with the joint predictive to define statements of inference on parameters of interest. This will be defined without respect to the theta. And that will allow us, we'll see, to we don't have to go through a likelihood. To, we don't have to go through a likelihood and prior construction, we argue, in order to do interesting inference on a parameter theta. And of course, this approach has its roots in Definetti predictive inference, but also Geisser and others. And there's some references at the end, recent work by Sonia Petroni and others. Okay. So I think it's really helpful to go back to Dave Finetti here and. To Dave Finetti here and really enjoyed Igor's talk yesterday on the predictive viewpoint in Bayesian on parametrics. So, and I defer to others in the room who know Dave Fernetti's work much better than me. But the starting point for Bayesian statistics is the subjective specification of a joint probability distribution on observables. So, this is where we start. It's not with the likelihood and the prior. We start with this object here. Start with this object here. That's the task. Can we define a subjective probability measure on observables? But this is hard. For any capital M and any little N, you have to define that joint probability statement or measure. So how do we define such a probability measure over all possible outcomes? And then how do we make uncertainty? How do we make uncertainty quantification on statistics of interest using this object? And of course, as we all know, De Finetti recognized the challenge and showed remarkably how certain symmetries, if you're willing to impose certain symmetries on that probability measure, you get an incredible simplification in the inference. So the representation theorem says. So, the representation theorem says if we assume exchangeability in the underlying data generating process, then every joint distribution on exchangeable data has this decomposition into this mixing measure and the sampling distribution under which the data is IID, as if it's IID. And this theta here is this kind of lurking parameter, which is defined via symmetry and sufficient. Find via symmetry and sufficiency in the limit as little n goes to infinity. And from that, of course, it motivates a likelihood and prior construction, because when you see this, this is as if it is a prior distribution on a parameter that indexes a likelihood function. And moreover, for any conditional predictive probability, Predictive probability, we can write it as this mixture of the likelihood function times the posterior distribution on the parameters which are conditional on y1 to n. Okay, so this is well known. So the likelihood prior provides this kind of super convenient vehicle to construct this probability measure under exchangeability. And what we see is that. And what we see is that conditional inference or learning on data, essentially, we can pass through, we can use this posterior probability on the parameters to do essentially to do any of the learning that we want to do on parameters of interest. And moreover, this posterior is obtained through a shortcut. So we don't have to work on the original joint probability on observables. We get a shortcut. We get a shortcut through this likelihood prior, which is updated through Bayes rule. And that shortcut is so useful that it gives Bayesian statistics its name. I mean, there's nothing, as we know, kind of Bayesian about Bayes rule. It's just an extremely useful shortcut when we have exchangeability. And the work that really Edwin's driven in his thesis is: well, are there other approaches that can work? Approaches that can work directly with P, with the joint observables, and perhaps we can relax some of the assumptions of Bayesian inference and therefore maybe do some more interesting or other interesting models. So the first question is, if we're going to work with the predictive, how do we reason about a parameter of interest if we're just solely working with this predictive density? And so consider any. And so consider any statistic, any estimand of interest for a study. And that, again, just to denote, the theta does not have to index the probability model here. As I said on the kind of first slide, the kind of thought experiment is if I had all of the data, so if we're assuming an infinite probability, an infinite population, if I really had observed the whole population, then the Whole population, then the statistic or parameter would be known exactly. Okay, there's no uncertainty left. And so, of course, what it means, since it's obvious, it's the finite nature of the observed data that is the source of the uncertainty in the parameter of interest. So, if it's so the so the finite nature. So, the finite nature leads to the uncertainty in the theta. In which case, if the uncertainty in theta is a consequence of the missing part of the population, then let's make this yn plus one to infinity the starting point for inference. Because if I knew it, I'd know the statistic. The fact I don't know it means that there's uncertainty in the parameter. So, let's use the predictive. So let's use the predictive, the infinite population predictive to impute the missing part. So if I could impute n plus one to infinity, then I could just pick off the theta. And then I could impute another missing part of the population and pick off another theta. And those thetas would be exact because, as we know, if you had the complete data, there's no uncertainty. No, there's no uncertainty, and in this way, this posterior predictive will provide us a direct characterization of Bayesian uncertainty. I don't know how to raise my hand. Can you hear me? Hi, Kat. Okay, sorry, Chris, to interrupt. So I'm already a bit confused about zero being a function of all the sequence. Sequence. So, you know, I like this predictive approach. But so, do you mean? Can you return to the previous slide? Oh, it's also here. Yes, yes, yes, it was, it was the next one, next slide again. So, theta zero. Oh, no, this is the infinity now. No, you wrote the tad zero is equal to ah here. to ah here so theta zero is a theta of the sequence y one infinity small small meaning realization but then for any little omega say for any sequence so y one infinity you would have a different theta zero no or are you thinking of theta zero as i don't know the quantile or no no i'm thinking of something with uh with uh exchangeability With exchangeability, then the limit that theta is a function indeed, as you write, of the wise, but it's random and indeed as a prior. So if I assumed that there really was an underlying population with outcomes y1 to infinity, I mean, if we think of the finite sample, a finite population case, if I Population case. If I really observed the whole population, then any parameter of interest would be known. Well, yeah, the frequency. So if I have an urn and I'm picking balls, then I would see the urn composition, I guess. But if I'm flipping a coin, what is this that had zero? Well, if you're thinking about. you're what I think about flipping a coin would be rather that a coin has been flipped but but you haven't been revealed you you aren't revealed the outcomes of that coin and then the experiment releases or observes certain outcomes of that coin i think if i if i keep going it might yes sorry chris sorry no no no it's okay Go on, go on. Yes, yes. Sorry. So the key aspect, when we look at this left-hand side, that seems quite challenging to work with this joint distribution from n plus one to infinity, but we're going to factorize it in a particular way. We're going to factorize it into a series of one-step predictives, you know, using this kind of prequential factorization. Prequential factorization. So that takes us, that's just by the chain rule. And then the computational mechanism we're going to use is we're simply going to impute n plus 1 to infinity in the following manner. We're going to draw y n plus 1 from the posterior predictive. And then this is important. We're going to then condition on that random variable. Well, we're going to draw the random variable. Then we're going to condition on it. And then I'm going to draw. And then I'm going to draw another random variable, and then I'm going to condition on that. Yeah, so there's this recursive resampling from this predictive distribution. And from that predictive, from these draws, capital Y1, n plus 1 to infinity, we're then going to pick off the statistic of interest. And with slight abuse of notation, this. abusive notation this the the the values of one to n the first n values in this y will be the observed data yeah and clearly this theta infinity which is the parameter of interest used in these imputed y's is a random and it has a distribution which is conditional both on the first n values that you've seen and the n plus one to infinity And the n plus one to infinity values that you've imputed, and the key results very good, thank you. It's clarifies. Theta zero is the true unknown population parameter. So that would be, say, some functional under F naught if F naught was the sampling distribution of the data. So the difference is because you are not using the right predictor? Well, the difference is this is random. But this is random because you are not using. This is random because it's drawn. This is calculated using imputed values of y. I have a, okay, maybe, okay, this might be a kind of, this is a cartoon. So imagine. So imagine that there is a true, unobserved nature's kind of population. Yeah. So again, maybe think of the finite sample case. So suppose that you have some clinical trial that is using looking at outcomes on patients of age 60 and above. So this end would be. So, this n would be a few million. There is some true, you have to think of nature's kind of true outcomes for those samples. Now, you undergo a study, which is to actually measure a finite number, little n, of this population data table. So, what you observe is in your study, you observed the first unit, you didn't observe the second or the third. You didn't observe the second or the third, but you observed the fourth. If you'd observed the whole population table, you'd have your statistic. There'd be no uncertainty. That would be theta naught. If you really had completed the full study, you would know the parameter of interest. There would be no uncertainty. Yeah, the uncertainty comes because of the question marks. You only had two samples. You only had two samples. The approach we're going to suggest is under exchangeability, we can, of course, just reorder these indices. So now your study revealed two observations. If you had all of the observations, capital N going to infinity, you'd have the parameter of interest for any statistic. What we're going to do is we're going to impute the data tables. Impute the data tables and then pick off the parameters, the theta infinities from this data table, the theta infinity from this data table. And the key result from Duob is that if you do this, and if this predictive is the posterior predictive from your Bayesian model, then you get back the Bayes posterior. The Bayes posterior. So, this is a really important result. It says that if I go through my likelihood and prior construction and I construct this joint predictive as the posterior predictive for my Bayesian model, and I do this forward simulation, you'll get back the posterior distribution. Yeah. And the key result there is. The key result there is Doobes paper on the application of the theory of martingales. Okay. So people are aware, like, they're with me on this approach, which is we're just going to impute. We're going to and we're going to do this recursion. We're going to impute, take the value, stuff it back into the model, update. You've got to update the predictive and then impute another one. Update the predictive and then impute. Then impute. Yeah. Just one slide back. So table four, table five are just two samples. Thank you. Yeah, these would give me two values of theta. And if I kept going, then just like Monte Carlo, I get a sample of thetas from the posterior. You, of course, cheated a little bit, you know, by assuming the finite population in general. In general, I calculate it all up to infinity, right? Exactly. So, in general, I'd like to do this. Well, we'll see in some special cases we can. But in general, you're absolutely right that there will be some finite sample approximation that comes because, of course, whilst I would like to be able to impute to infinity, often that is computationally challenging. Computationally challenging. But we can think about Monte Carlo error in not being able to run our MCMC to infinity as something a little bit akin. We're going to have to take some computational, but we know we're heading towards the right place. Yeah, so Dube's result is specifically about the posterior, about taking the posterior amino. Posterior about taking the posterior mean and so in consistency. Okay, so if you ran this, if you did this approach, what I've suggested is get your posterior predictive, impute from n plus one to infinity, yeah, and then you'll get back the base posterior. And so you would argue at that point, this seems like a lot of trouble to get back to something that I've already got. To something that I've already got. Because to impute that posterior predictive, you would use the posterior distribution on your parameter of interest. So, what's been gained, if anything, here? Given that this imputation tool where I use the likelihood prior construction just gets me back base. Well, a few points here. So, I think one thing is interesting, it's always useful to view things from different It's useful to view things from different angles. And so this angle shows that in Bayesian inference, writing it in this way, we see that the source of the uncertainty in the Bayes posterior arises from the missing data. That that is the source of the uncertainty that's captured in the posterior. And it flows from the missing population. If you're assuming a likelihood function, if you're assuming an infinite population for mathematics. Population for mathematical convenience, then we see that the missing population is the result of here. It makes clearer the Bayes-Frequentist distinction. This is something I really like, which is what is frequentist uncertainty? Frequentist uncertainty is where you sample the Y's, IID, hypothetically, hypothetical repeat experiments where you would draw observations. Would draw observations, IID of size N, and then you'd calculate your statistic of interest using an estimator. And then you'd repeat that and repeat that and repeat that. And that would give you your confidence intervals. Okay. Only if you knew the true value of the parameter. If you knew, no, no, I mean, this is how you would, this is how you would get a confidence interval. We'll show like the bootstrap is. We'll show like the bootstrap is precisely this. So, what does Bayes do? The predictive representation of what Bayes is doing in the posterior distribution is you're actually akin to sampling the infinite population jointly and then picking off the parameter of interest. And that's formally equivalent to what you're doing when you use this posterior distribution. You use this posterior distribution, say through a likelihood and prior. So, what we can see is that they're both kind of targeting in the same. You could use an estimator here, consistent estimator. It's just this one is driven by akin to hypothetically repeat experiments of size n, and the Bayesian considers the missing data. So, in this predictive viewpoint, it changes. So, in this predictive viewpoint, it changes the nature of the task from specifying a likelihood and a prior to specifying this one step ahead predictive and an update. So, there's still two components to do. You need the predictive for n plus one. I need to sample that, and then I need to be able to update my predictive. And this is something constitutional: what you're probably going to do next. When you do that predictive simulation, it's under an assumed model. And what you're going to do next, you're probably going to introduce somehow that assumed model through the back door? Not always. So, of course, you need a predictive model, but we don't have to construct this predictive using a likelihood and a prior. Prior, yeah. So, so if you construct this predictive using a likelihood and a prior, then all of this framework tells you is just a nice way to interpret that posterior. So, if you're using a conventional likelihood prior, when you present this posterior, I can think of this as, oh, this is akin to imputing the missing population and then picking off the parameter of interest. Of interest. But we're going to look at more general ways of constructing that predictive. And it opens up the possibility of using modern machine learning algorithms. But because you need to update the predictive, it's this recursion. They need to be online or continual. So you have to be able to impute and then learn from that imputed value. But it also points to generalities, which is, or generalizations, which is we can relax certain of the assumptions in conventional Bayesian learning and allow for more general predictive machines. So clearly, this parameter of interest in this predictive approach doesn't have to index the likelihood function. Yeah, it's just any statistic that says if you had an infinite amount of data, then I'd know the value. Data, then I'd know the value. And we're not going to assume that the model is true, that the predictive model is true. We're just going to assume it's an accurate simulation for the population. Computationally, it might be kind of easier to simulate wise from a joint predictive and then pick off the theta rather than simulating directly from that posterior. Directly from that posterior. We're swapping in the kind of MCMC task of generating thetas from this to this task of being able to simulate pseudo-observations and then plug those into an estimator, consistent estimator. And it has flavors as well, something a bit like ABC, where you simulate Y's and then have a way of picking off thetas. It's also just to note that this method is triggerly parallelizable because you can just separate this out to simulate, impute different populations, and pick off the parameters. And what we've swapped, likelihood and prior construction, we've swapped in predictive model. And therefore, prior elicitation becomes predictive model selection. And my kind of tenuous And my kind of tenuous connection to objectivity is that I feel, well, I don't feel, but they're really well-developed and accepted methods for comparing predictive models. Okay, so here's the algorithm. And quite rightly, somebody pointed out, well, the kind of the elephant in the room is we can't go to infinity. But what we can do is we can go to capital N and make that capital N very big. That capital N very big. So here's the algorithm. We're going to call this algorithm predictive resampling for what should be obvious is we're going to, first of all, we're going to get a predictive. Oh, I wish it hadn't stayed there. Yeah. We're going to get a predictive Pn, little n, which is now the predictive for n plus one, given the observed data. And for some very, very large number, capital N, I'm going to just do this recursion or this. Just do this recursion or this loop, which says: first of all, sample a random Y from the predictive and then update the predictive. So you then push this YI into this predictive machine and update it, keep going around, and then pick off the parameter using an estimator given the sample that you've drawn. The first little n samples are the observed data. Little n samples of the observed data. Now, two things to note here. First of all, my starting point is having observed n data points. So we're already outside of Bayes because this algorithm is going to violate the coherency of belief updating, which says you can never specify, you shouldn't specify the predictive based on what you've observed. Yeah, hence you specify your prior and off you go. And formally, you're never allowed. You go, and formally, you're never allowed to touch the predictive machine or the update, having specified your prior. Now, I would argue in practice that we never do that. You know, you would never not check your model against data and then maybe update your model. But formally, here, the PN is that having observed all the data, build the best predictive model you can, and then off you go. And the other thing to note, of course, And the other thing to note, of course, is we're going to need to this updating because I want to get n large, it's got to be pretty efficient. So you're going to need efficient mechanisms for up this for this task. This one is pretty easy. This thing might be challenging is this online learning. We're going to be learning from, you're going to be learning from observations which are drawn from the model itself. Okay, let's look at a very simple example just to convince you that Doob was right. Hi, Louise. Can you go back to the previous slide? Okay, probably you will say this later, but you say that you forget about the base, just not with the predictive. Yeah. How do you update the P I minus one to the P I? We'll see. Yeah. See, yeah, this is a key part: is that you can't do anything. Yeah, so there's going to be some conditions. You'll want to impose some conditions on how you do that update. And we're going to come to some, what we think are good conditions on how to, what to impose the constraints on that update. But here's a really simple example. So, this is a likelihood function for y. We're going to assume that y is normal. We're going to assume that y is normal, mean theta, standard deviation one. We've got a prior on there. And we've generated some data of size n, little n. And I think the posterior mean we got out was like 1.76. All of these sample paths start at that point. So this is y bar from your data. Now I'm going to simulate an Simulate an n plus one, and now I'm going to update. Now, the first update is going to move you quite a lot because I'm going from n to n plus one. I'm adding an extra observation. So, this sample path probably I generated an X or Edwin generated, sorry, a Y that was quite high. And then it moves the posterior mean up here. And now I generate, I update my Bayes, and then I generate another sample. And then I generate another sample. And so you get these random paths. Here, these are of the statistic of interest, theta, which is just the mean of the y's. But what you see is over time, they settle down. And of course, that's because, you know, if I get more data, I get less information from the next draw. Yeah. And it, and so each one of these paths is just a separate draw where I've started. Just a separate draw where I've started off at the same point, I just draw a different y, I update, I draw another y from that predictive, and push that back into the model. So I'm doing these Bayesian updates on these pseudo-observations, which are drawn from the actual model themselves. And if you follow these paths to the end, here we drew a thousand, and I take this as a Monte Carlo draw, what you see is you get almost an exact representation of the bayes posterior, which you knew here. Of the Bayes posterior, which you knew here. So, this is just showing you, this is a computational, trivial example of Dube's theorem, which says if I do this kind of posterior recursive sampling, I'll get back Bayes. What is the true value of theta? I can't remember. The key thing here, it doesn't matter, is that the predictive resampling gives you back the base posterior. Yeah. So this is just persuade. This would be a trivial exercise because you say, well, I've done a lot of work to get something I knew right from the start. But the key thing is that you do get back the posterior under this. So this is the equivalence. This example, how do you define this predictive? This is just the posterior predictive under the likelihood prior. So a standard like Bayes linear model. Standard like Bayes linear model with known noise. Okay, let's take the sit, let's now think about this PN. Yeah, I now want to go outside of conventional Bayes. I'm going to take the simplest non-parametric predictive, which is going to be the empirical distribution function. So now I'm going to take the empirical distribution function at n, and I'm going to use that as my predictive. So, what would I do to draw a sample from this predictive at n plus one? Well, that would just be the atomic measure of the values that I've previously seen. That's the empirical distribution function. And of course, what I can do then is I can draw a predictive. Well, that's going to be one of the data points I've previously seen. Yeah, and now I'm going to put that back in. I'm going to update. So, I'm now going to have a new empirical measure at end. Going to have a new empirical measure at n plus one, but it's using one of the samples that was drawn. And now I'm going to get a new, I'm going to draw from that new predictive, which is going to be another empirical distribution given n plus two, and so forth. And if you do that, what we know, of course, and many people in this room will know, is that this is simply a polyurn scheme with replacement, and we get back this. This random measure, which because we've got an atomic predictive, will always just reproduce the atoms. But in the limit, we're going to have random weights. These are kind of normalized exponential ones. We keep doing this, we'll get exponential weights, which normalizes the Dirichlet 111. And then picking off any parameter of interest is going to give me the Bayesian bootstrap. Sorry. Sorry. So the Bayesian bootstrap has precisely this predictive inference measure of sampling a Y, putting it back in, sampling another Y, putting that back in and conditioning. And so I quite like this viewpoint that compares the Bayesian bootstrap on the left with Efron's bootstrap on the right. So here's the pseudocode. Both the Bayesian bootstrap and Efron's bootstrap. Bayesian bootstrap and Efron's bootstrap start from the same predictive, the empirical distribution function of n. They're both going to draw B samples, capital B samples. For the Bayesian bootstrap, I'm going to go from n plus one to infinity because I need infinite data to pick off the parameter of interest. It's the population parameter that I'm interested in. So I'm going to sample yi from the empirical distribution function, then I'm going to put that back in. Function, then I'm going to put that back in in the update. And I'm going to repeat. And I'm going to do that an infinite amount of time. That's then going to give me the empirical distribution function from this infinite data set. And then I'm just going to evaluate any statistic that I want. So, what does Efron's bootstrap do? Because it's a frequentis measure. It says, well, what I want to kind of replicate is not the missing population. Is not the missing population, it's experiments that I might have done, but I didn't. So I'm just going to draw n samples, and I'm going to do that by first of all sampling a Y from the predictive, but I'm not going to update the predictive. You're going to condition on the predictive. And of course, if you do that n times, you get the base, you get Efron's bootstrap. So that's just a different, that's a way of viewing these two approaches. Approaches. I think I've said all of this, which is predictive resampling, draw n plus one, n plus two, recurse, pick off the parameter of interest. I'm just looking at the time. Yeah. Of course, you need to be careful. If we're going to start to generate more general predictives, you have to be careful to make sure that. You have to be careful to make sure that what you get at the end is going to be, is going to give you something meaningful. And so we have a couple of conditions in there. One is that the limiting empirical distribution function exists, it's a random measure. And secondly, we have a martingale condition on the unbiasedness, which says essentially you shouldn't be able to add information. If I'm doing this predictive and recursion, you shouldn't be adding any information into the data. Information into the data. And these two things we talk about as predictive coherence in the paper. So the important thing is under those two conditions are satisfied if this sequence follows something called this conditionally identically distributed distributions as introduced by a paper in 2004. And it's a weaker condition. Condition than exchangeability. So, just again to note that we shouldn't add information by doing this decomposition. And different forms of predictives lead us to different situations. So, if we take the conventional posterior predictive under a likelihood prior, I just get back my Bayes posterior. If I take the empirical distribution, If I take the empirical distribution function as my predictive, I get the Bayesian bootstrap. And under more general CID distributions, we're going to get things we call martingale posteriors. I've only got a few minutes, so I'm going to go very quickly through one, which is there's a really nice paper by Buja and others, including Ed George, about considering the functional Considering the functional kind of viewpoint or projection viewpoints of models. So let's take a classical linear regression model. And now we think about theta naught as the expectation under some population F naught of the value of theta that minimizes the L2 norm. So this isn't making any statements about the linear model being true. I'm just interested in making inference about that theory. In making inference about that theta norm. Well, if we take the empirical CDF as our predictive, what we're going to get is something like a weighted likelihood bootstrap. In order to make, now I want to make Bayesian posterior inference about this value theta naught, this unknown theta naught. The key thing is when people talk about weighted likelihood bootstraps, they tend to talk about them as approximations. As approximations. We don't view this as an approximation. This is a kind of very precise, valid Bayesian non-parametric inference on that parameter of interest. And moreover, what we can show is that this Bayesian bootstrap or the weighted likelihood bootstrap has some really nice properties, better properties than the Bayes approach in M open. And you're always in M open. If the models miss. You're always in M open. If the model's misspecified, the Bayesian bootstrap gets better frequentist coverage for that theta naught and it has lower predictive risk. And so this is a kind of a cartoon that Edwin put together, which is what we're showing here is a few data points and the kind of true regression line, which is the black. And then if you're fitting. And then, if you're fitting a linear regression, here are the weights associated with the data points. As you might be able to see, as you update certain weights, you'll tilt the line to try and model those data more closely. And if you do that over and over again, you get a Bayesian bootstrap posterior for the distribution. And you can pick off credible intervals, etc. Etc. The thing I won't have time to show you, but you can read in the paper, is other constructions using copulars. And the thing to note is that all Bayesian updates have a copular representation. So this looks slightly weird, but what you can do is you can write the posterior predictive at i plus one in terms of the posterior predictive at time. Time i times this kind of copular update for the two predictives at the point that you drew and any other value y. And we use this in the paper to do some Bayesian non-parametric kernel density estimation, which seems to show some kind of nice properties. We get faster models. Faster models using this copular update than using conventional kind of MCMC, and we seem to gain some kind of accuracy as well. I'm just going to conclude with one slide. So the nub of what we've done, or the heart of what we've done, is say taking the predictive framework, which is we're going to think of the missing data as the source of uncertainty. So we're thinking carefully. Of uncertainty, so we're thinking carefully about the data that wasn't observed but might have been as the source of Bayesian uncertainty. And what this means is that you can really think about we treat all of Bayesian inference as a missing data problem. It emphasizes the close link between Bayesian inference and the ability to accurately simulate or model a system of interest. And I think this gives us new angles for objectivity of thinking about. For objectivity, you're thinking about predictive model evaluation. It places the frequentist uncertainty and Bayesian uncertainty on equal footing. You know, the frequentist uncertainty is about data sets of size little n and properties of estimators, finite sample estimators, and the Bayes is about the same estimators, but with infinite data. And we think it's interesting to consider kind of online approaches. To consider kind of online approaches to this, and there's a paper there that has all of this in. Thank you. Yeah, I mean, the simplest way of working with things like generalize, you know, with estimating equations is through the Bayesian bootstrap, where that theta is targeted via a loss or some kind of score function by the estimating equation, and then imputing the missing whys using the empirical. Wise using the empirical measure. So I think that would be the way that I would suggest to kind of think about it. And then you can use more general kind of bootstraps if you wanted to introduce prime information. The way of thinking about it is to say, I've observed data of site of little n. What's the missing information needed for me to answer the question of interest? Interest. Yeah, so what's the and just focus on the missing information needed to answer the question and then impute it, predict it. Take a kind of Bayesian approach. And the key difference for the Bayesian is the joint prediction. Yeah, if you don't predict jointly, you'll just get back the same theta after every value. So it's this joint prediction of the missing information needed to answer the question of interest. So if you said to me, Of interest. If you said to me, I want a hazard rate, I'd ask you, what's the missing information needed for you to pin down that particular parameter? And then let's impute that missing information given the data that we've got.