My name is Melissa Lopez. I'm a postdoc at Yichurk University and NICAF as well. So I wanted to thank the organizers for the invitation. So today we'll be talking about the detection of anomalies amongst micro-school population with auto-codds. So in this minutes of fame, I wanted to promote the machine learning informal group. We have a Wikipedia, a mailing list, Madermos, the Centanos whereas I said before. So we will be organizing talks. Also, I think it's important that we organize the work within the It's important that we organize the work within the collaboration, and we exchange ideas. And this is the space that we want this to happen, right? So, previously this week, we had these very nice talks, among others, with Gabele that he talked about nice abstraction, and then Derek highlighted the problem of goods mitigation. Also, we have this very interesting round table of challenges of detector characterization, among other related talks that we have today. Related talks that we had today about glitches itself. And indeed, glitches are a problem. We have repeatedly said so. They are caused, as you know, they are caused by the instruments or the environment. Some of them, we know why they happen, some of us we don't really know why they happen. The problem is that there's minus amount of scientific data available, and they hinder gravitational wave detection as they can mass remove gravitational waves. We have seen conflicts of time the only 17 by 17 during this week. And over here, we have We have the different families of glitches by gravity spine. So you can see that they come in different time and frequency morphologies. And I also wrote a few things about here, about some opinions I have regarding this topic. You can try to change my mind. I'm very open to discussion. But you can probably not see it from there, so I will zoom in. So rotation with its external eyes is non-Daussian and non-stationary. I think this is a super important point. I think this is a super important point that a lot of people are doing very interesting work by in Gaussian noise probable concept, but then scaling up to actual detector noise is a highly non-trivial task. Something else is that glee identification and mitigation is a fundamental problem for gravitational wave data analysis. And this is something that has been tackled from different perspectives by various groups. But it's still something that we need to, as a collaboration, understand. Something else is that it's reasonable to think that feature detection. Reasonable to think that future detectors will contain glitzes, such as LISA, Eisen Tesla, Cosmic Explorer, on top of all the problems that people have been mentioning throughout this week, about what are the challenges of these third-generation experiments. So we need to be prepared for this. Alright, since this is a machine learning conference, then we are going to try to use machine learning to solve some of these problems, not all of them, of course. But first, I wanted to give a flash introduction to machine learning applications and then. To machine learning applications, and then learned that you guys really like cats, so it will be with cats. So, when we talk about detection and identification, the question that we are answering is, is there a cat on the picture? So, essentially, we have this cat over here, and then we pass it through our black box machine learning model, and then we have an output that says this is certain values. When we do parameter estimation, it's what color is the cat. We have this very nice picture, we pass it again to some other imaginary. We pass it against some other machine learning backsbook algorithm and then we output orange. But when we go and do unsupervised learning, maybe we have a lot of cats and we don't really know what to do with them. So we essentially group them, right? So for that, we need to feed our machine learning method, the space of cat, whatever that is, right? So it passes through our machine learning algorithm, and suddenly it groups it in black cats, sorry, white cats, orange cats, and black cats, among others. So, this is something similar to what people have been doing with machine learning. So, most of people, I think, they have been thinking about more classification as in supervised. Again, I'm going to highlight Gravity Spy because we use Gravity Spy in our work. But there have been other people that have been also thinking along the slides. The problem with this is that there's several challenges that are non-trivial. So, for example, relative spy in particular, others as well, user representation of the main. User representation of the main strain of the detector. The way that it works in my head, and maybe it's not the way that it works, is that glitches are actually projections in the H of T. They don't happen in the H of T. There's a physical process that's happening somewhere in the detector, and that's something that we are really addressing if we only look at H of T. So, also something else is that the classes are rated and labels are actually expensive. So, gratifying circumvents this issue using humans and This issue using humans and as well as machine learning. But the question, open question, I think, is like, what happens when you don't have enough resources to properly train the scientists that are labeling all these things, right? Something that's also non-trivial is that the tactor evolved over time. We have seen while Derek was highlighting how we have new glitches in the neurons and also other glitches that they could appear and disappear instantaneously for X and Way reason. So the first idea is: can we use information? So the first idea is can we use information from the detector itself? It's also a channel, there's 10 to 6 channels to process, roughly. And then the second idea is that we want to let the data speak for itself. And for that, we want to do an useful and supervised learning. So over here, we have a global problem, we have a high-performance computing problem, and a structured discovery problem as well. Someone earlier this week asked the question: like, very few people. The question: Like, very few papers method development under supervised machine learning, and why? So, the fast answer is: it's actually hard. One problem is explainability that is essentially pretty difficult, but also on top of that for this particular problem is that we lack ground truth, among other problems that we will discuss in this talk. So, what are we going to do to solve this problem of reducing 10 to the power of 6 oscillated channels? Well, we are going Well, we are going to essentially encode, encode, and encode. So, we have some raw data that's huge, and then we want to encode it in some useful information. So, what is the first thing that we do? Well, we have domain knowledge, right? So, with domain knowledge, we can actually reduce this list to 350 Azure channels. And a trigger warning is that this is not an extensive dead term list. This is some list that we got from somewhere. But we should do a better job of this. And we want to expand this to a full dead term list. To the full detail list. Something else is that we can encode the data into data representation. So for this, we're going to use fractal dimension, which you can understand as a measure of complexity of the data. So over here, we have the fractal dimension, and this is in the x-axis as the time. So you can see several points that have this mean and invariant, and then some points that essentially go over, like somewhere far away. Like somewhere far away. So, what you're essentially observing here is one observed tunnel. So, one auxiliary tunnel that has been processed with the fractal domain. We take one second of this observed tunnel and we have this one value. So, essentially, each point here is one second of this observed tunnel. Most of them, they are well-behaved, and they are within this mean invariance, but some of them they are actually outside. And this point signs with the whistle's beetle of the observing runs. Of the observing runs. So, actually, this idea is not new. This comes from Marco's paper from 2022, where he included one hour of this channel in one hour of time, of processing time. And then he explored how the splits essentially deviate from the mean, showing that essentially your data becomes more complex because you have more extractor. And that's why the fractal emission shifts from the average. What actually my student Robin did is to make this work for. Make this work faster. Because if we want to scale it up to all the subscribed channels that we have, we actually need to solve this high-performance computing problem by encoding one hour of data in 11 seconds of processing time, making this in almost real time. Something else to continue to encode our problem is, of course, to use machine learning. I said it before. So what my student Poloma did is to use convolutional alternators. And let's talk a bit before about what the input looks like. About what the input looks like. So I said that we have 350 oxid channels, and actually, we're going to use 350 of these. We essentially select 50 oxid channels. And by this 50, what we do to choose them is to see what is the highest variance of them. So we compare their variances and essentially see that some 50 of them have a pretty higher variance with respect to the others. So this is what an image of the fractal dimension looks like. Of the fractal dimension looks like. So, in the y-axis, you have the list of channels. The z-axis is essentially the fractal dimension, as you can see there. And you can see that this side over here looks a bit more bright, and this side over here looks a bit more intense red. And that's because these are different time windows. So, essentially, from the pixel 0 to 31, we are using a time window of 0.25 seconds to encode 8 seconds of data. From the pixel 31, From the pixel 31 to 46, we're using 0.5 seconds to incorporate seconds of data. Then, for 46 to 53, we use one second, and then from 53 to the end, we use two seconds. And then, also what data do we encode, actually? I said before that we have all these classes of glitches, but this is just a proof of concept work. So, essentially, we select these three different glitches. We have a skylight, taunting, and whistles. Because the skylight near Long, near Because the scarlite near long and they're low frequency, so it's a good pearl for low frequency, essentially. Then we have tanti that they are a bit higher frequency, but they are very brief, and also they are very common as well. Whistles, they are very high frequency, and their durations can be very short, and very bit, they can be a bit longer as well. So, I think, like, overall, these are like three very different types of blitzes to see if we can get some structure out of this. All right, so let's talk about the algorithm. Alright, so let's talk about the algorithm. So, an altocolor in one sentence is a convolutional neural network for reconstruction of an input or a value. So, from Thompson Marco Stas talks, we have seen that we have an input that's essentially the noise and the signal. It can be a gravitational wave, it can be a glitch, and then the model essentially has a target that's noise itself. However, you can also have an input and then pass it through your model and get your input. Model and get your input. So, this is an example. So, this is a handwritten digit. This is a number seven. So, we have this image, and then we encode it into what we call embedded space, and other people call it latent space. And then we decode it and get the reconstructed set. To some degree, essentially. And some people might ask, but you already have your input, why do you want to reconstruct it? And that's a very good question. So, essentially, what people do with this handwritten data set is to Data set is to take this embedded space, project it in two dimensions, into something that we can understand, and essentially have these clusters of data. So you can see over here, let me point, yeah. You can see over here that the ones are quite close to the seven because they're written kind of the same. Then you have the nine that is also quite close. You have the zeros that they look close to the sixes, and a bunch of anomalies because some people don't know how to write numbers, essentially. To write numbers, essentially. So, this is an interesting idea. So, we can actually do the same thing. So, we have our original fractal dimension image, we encode it into the embedded space, then we reconstruct it, we decode it to reconstruct it to some level of accuracy that it can be actually pretty high. And then we essentially project it using t-SNE. t-SNE is actually a quite established tool within machine learning, and we have something like this. Like this. So, what you can see here is different clusters, and each color represents Whistles, Tony, and Scarlet from Gravity Spine. These are just to benchmark, actually. So, we are benchmarking against Gravity Spine, despite the lack of ground truth, because this is essentially what we have to understand around supervised government. So, as you can see, we have a cluster for Tony over here, we have a cluster for Scarlight over here, this is a whistle cluster. Here, this is a whistle cluster, and then something that's kind of like a mixture, and then something that looks pretty anomalous. And a bunch of anomalies in the middle as well. The take-home message here is that the clusters are consistent with gravity spy outputs, which is good news. But gravity spy uses only spectrograms in H of T. Well, we use only fractal emission with ophoxylic channels. And the question is, let's try to interpret what is going on and why we have. What is going on, and why we have all this bunch of anomalies by using spectrograms. So, let's go to section one, which was actually this Tomty cluster. So, you can see over here one of the anomalies that my student picked, but there's many more. So, just I'm showing some examples. So, over here, we have something that was labeled as a whistle. And for context, a whistle looks something like this. This is subject cuts, actually, it's higher frequency. But you can see that it's not looking quite right. See that it's not looking quite right as a whistle. It looks pretty anomalous. Something else, we can go to cluster two. That's this mixture over here. And then you can see here that we found this tum tea that actually is not, it's a ton tea with scratchy, right? So it's an overlap of glitches. And the standard tum teeth looks something like this. If we go to cluster three, which is this anomalous guy over here, so we can actually, we actually found a lot of anomalies in between. Actually, found a lot of anomalies within the cluster. So, we have this, this was actually labeled as a scarlite, but it's actually an overlap between a Tom T and a Scratchy. And for context, a Scarlite looks something like this. Then if we go to cluster 4, which is the Scarlite cluster, we found this Tomte with, sorry, it was labeled as Tompty, but it was actually a clayfish. And again, this is a Tomte, what it looks like. And then finally, five, which is more like a whistle. Which is more like a whistle, we found this anomaly that honestly doesn't look like a skylight. It was labeled as a skylight, but this is a skylight, this is something else. All right, so to summarize these results, so in total in our data set, I was a bit limited, we found 177 anomalies, which constitute 7.6% of the data. Again, this is a first-order approximation only using three different classes of glitzes, but it's Suplicis, but it's to my knowledge is like the first world that tries to somehow estimate what is the degree of misclassification of privatized bias as a supervised learning algorithm with a lack of ground truth, right? So we could also be biased as well. So what my student found is that for whistles we have 49 anomalies, 45% of them. For non-morphologies, 28% of them were misclassifications, 27% overlaps. For Tompties, 32% of them 32% of them were non-moor qualities, so less than wisdoms. 21% of them misclassifications and 47 overlaps. That's because the Tomti are actually quite common and they are also quite short, so it's quite normal that they overlap with the silver glitters. The scan light, on the other hand, it had 20 unknowns, 72 misclassifications, and that's because essentially they are in the same frequency band as all the glitters, and then one album. And the paper you can scan there, or you have the archive. You can scan it there, or you have the archive number. And okay, we did a bunch of stuff to understand this and supervised learning algorithm. So, the question is: if we can go to explainable machine learning, this is what I hope, actually. So, the take-home message here is that we learned from the original to the reconstructed, so we learned in that direction to exploit the embedded space and project it into dimensions and construct this space. So, if you remember from the digits, Space. So if you remember from the digits, different digits were located in different parts of the perimeter space according to their morphology. So the question now is if we can learn backwards. And what I mean by that is essentially, if I have, for example, in the case of the digits, because it's easier to understand, if I have a one that's located somewhere in the space, I have a zero that's located somewhere else, that's because the image learning algorithm said, ooh, this is kind of flat, and this is kind of round. So they are actually quite distinct. Of browns. So they are actually quite distinct. So I'm going to put them in different locations in the embedded space. So we can always map back the activation function and be like, oh, okay, so in our case, if our glit is in this location and this other glit is in this other location, it might be because some of these channels, they have distinct features. And we can even say which of these channels have this distinct feature. So that's something that Jess also talked about a bit, saliency maps. So this is just one long example. One dog example. So, if you have an image of a dog, the machine learning algorithm can tell you what are the main features they found interesting. So, in the case over here, you can see that it's highlighting essentially, so this bar represents the activation, so what calls the most attention to the neural network. You can see the eyes and essentially the nose, they are the most activated because that's what it used to say that this was a dog rather than anything else. Anything else. And I guess the often question that we are trying to solve here is: can our non-linear model tell us what is the most relevant channel? And again, for interpretability, because this is unsupervised learning, we need the help of instrumentalists to be able to get conclusions out of this. So, conclusions and future work, what we have shown here is that fractal dimension representation is complementary to H of T, as you saw from the agreement of gravity spin. From the agreement of Perthus Pine with the work. Something else is that unsupervised learning can reveal misclassifications of supervised learning, as we proved, glitch overlaps, and the one morphologies as well. Something that we want to do, of course, is to extend this to other glitch populations, uneven gravitational detectors. The question is, if this is scalable, that's something that we need to answer. We can also relate glitches to auster channels we explained in machine learning, which is the final goal of this project. And now we have three problems. And now we have three problems. That will be high performance computing, machine learning and interoperability. And that's everything from my side. I will be taking questions. Thank you so much for listening. Questions? I'm getting a sense for how much your choice of this sort of track doll's representation. Do you have a sense for how much? Do you have a sense for how much that matters? How much try to keep it in the answer? That's a very good question. I don't think I have a good answer for it. So I think it's very important the representation, for sure. I would say that's the key part on everything that we have. On everything that we have done so far. Because if you try to encode so much data, let's say that instead of taking factor dimension, you take a standard time series, right? Your algorithm will need to scale up super much. And at the end of the day, you lose the whole point, which is interpretability behind it. So I think that having better representations, it's essentially what it will make machine learning algorithms in general make it to the next level. Make it to the next level. Yeah, I'm pretty sure you can do it with emitters as well. My problem with that, our problem with that, is that it's much more expensive, right? Because the gravity spy takes four emitters per glitch. We kinda do the same. We do some sort of fusion that. But if you scale it up to 50 tunnels, it's way more expensive. And if you scale it up, It's way more expensive. And if you scale it up to seven hundred tunnels that charge tunneling, it's even more. As you could imagine, spiking the subsystems and clustering your channels, right? Because the only reason I like images, right, is that searches and sub-versions of CBC searches. That is the representation. It is harder to understand from a person's perspective. Understands from a search perspective of that. Yeah, 100%. And I think that's why in the last part we took essential spectrograms to understand what was going on, for sure. So I think there will need to be a mix. Like there's some information that the practical mission is giving and there's some other information that we can understand better in spectrum. If I can answer a little bit, I mean, I don't think the fractal dimension. The fractal dimension is the ultimate answer, and there may be better ways. The motivation was that you can monitor the order of 100 channels in real time. So, if you need to do 100 images, you cannot do it in real time. So, we were able to run this second by second and know if there is an anomaly. I'll just plug to that to that end. I mean, in Malcolm GW now, we have a torch representation of Q transforms that's real fast. I'm not sure if it's 100 channels at a time fast, but it employs it fast. And so I think it might be possible now. And of course, this we did this in the last month or two ago. So this is not this is relatively important. So I would love if folks were interested in any construction just interested in any construction, we just want to just sort of give a benchmark and shot there, because we might not actually be too far from the GPUs we have and that's where the fortune trees are. Um I was just wondering about your use of TSNE. I mean TSNE is a very nice visualization tool and it gives you all these but um maybe for for for one of interpretation One of interpretation. But I mean, my understanding is that if you just add five more data points, that whole classification picture will change again. Yes. Are you aware of other similar tools that maybe are a bit more robust or state really? I like TSNE, but it's very much like a visualization thing. It's not like a thing that we can make part of a pipeline because new data will come in and. Because new data will come in and everything will be in classic differentiation. Um not from the top of my head. Because I think, yeah, exactly as you said when I was trying like PCA, like TSME change. Tom, do you want to say that? Well, just on this point, I mean, I suppose it doesn't really matter what you use. If you use PCA, you should see similar clusters show up. It's kind of arbitrary. The fact that you can use, say, The fact that you can use safe auxiliary channels and get these clusters that are separable according to the gravity spike classes indicates that these glitches can be, there's something going on in those auxiliary channels that you can exploit that information without any knowledge of HFT to call something a scattered light or a Tom T or a whistle. So, yeah, TISNE is arbitrary, though not to your degree, Damonism. The not you'd agree down a little. Um, it's just to show, yeah, to visualize that, yes, these are separable, these three different events, and it's just a convenient way to visualize more or less. Yeah, to add a bit more on that, I think the also main point here, of course we couldn't do it, but only two since it's to find a metric, the same method. Because t-SNE is parallelistic, sure, but at the end of the day, what you want to do is To do is this, these points are located in this bit of the space that to some extent as well, it's like sure, but at least they are more consistent and repurposable. So, that will be the ultimate goal of this, to find the metric. Right, you can try using UMAP. It's almost the exact same diagram, but it does behave differently. But it the clusters would stay in the same place? I think it's a little more scalable. I don't know how robust it is supposed to be. Robustiness of this change. Can I just also add: if you ran that same Disney algorithm again on that data, it wouldn't look the same, but the clusters would be less identical. But that's, I just want to know if there's a tool that wouldn't be criticism. Oh, no, no, sure, sure. Yeah, good question. Um yeah, I mean, ties back to the discussion we were having uh yeah before. To the discussion we were having before this about the features. So you had like 50 auxiliary channels finally, right? In your cleaning model. I would be thinking that maybe you don't need all 50 of them. If you have some kind of idea which features are more important, things like that, you can increase the speed even more. What do you have in common on that? So if I got your question correctly, essentially I mean that n all not all channels are Assuming that not all channels are relevant. Yeah, because I think if I remember you said, like, you had chosen the ones with the most variable variance, I think, like, variance. It might be that even though some of them are varying, they have less contribution to the anomaly which we're trying to detect. Yeah, that's exactly the case. There's tunnels that they will be more useful for certain glitches than others, but we input the same thing to be more flexible, right? Like more flexible, right? Because, for example, let's say that Ponty and Scarlight have different physical processes, you still want to have that information there. The only problem is that your information will be sparse, in a sense, right? So your machine learning model will need to take this into account. And it's for 50 channels, it's been an okay job. To scale up to more study channels, this is going to be a challenge, for sure. Any final questions? No. No, let's thank Melissa.