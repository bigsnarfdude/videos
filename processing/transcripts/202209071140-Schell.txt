In the audience, and I'm talking about two-dimensional sums signature. And I changed the title, but quasi-isometric functions appear, so don't be worried. But they are under the hood, so I define, don't define them. So this is why I change the title. So let me recall the motive, as a motivation, recall the one-dimensional setting and especially time-warping invariances. So, time-woping invariances are defined on eventually constant functions. So, here we have a time series with discrete data, discrete time, so the natural numbers, and they map into the complex number field. You can think in this talk of the complex numbers as pixels, colors. This intuition works nicely, and later you can rechange this to a vector of. Change this to a vector of arbitrary dimension. But for this talk, think of colors. This will be fine. And it will be eventually constant, meaning that the data is unbounded, but there's only finitely many data points in each time series. So this is only for convenience. And warping is now a function from the eventually constant functions to the eventually constant functions. And it has Constant functions. And it has an index k. And this index k specifies at which position you warp or at which position you repeat the entry. So here the position would be two. So at position two, you would copy your entry and you shift all the remaining entries by one. So no information is lost. And the warping invariant is now a function from the time series to the complex numbers. To the complex numbers, and it should not see this warping. So, this is a classical invariant, how you think an invariant should be. So, it's a function. And here's an example. This is quite clear. So it takes the first entry, this is the two of this time series, and subtracts the last entry. So, there is not really a last, but the limit. And I assume that the time series are eventually constant, so the limit exists. Constant, so the limit exists. And then, yeah, what you obtain is two minus one. This is what the invariant should be. And dynamic time warping is huge in literature. It sees the data at different clocks. Okay, so this is the motivation. And now the title suggests that we talk about two dimensions. So this is a two-dimensional setting. So the two-dimensional setting has now data. Data in two directions. So either you have columns or you have rows, and the data is now eventually constant functions indexed by two entries. And the warping has an additional second argument or indicates whether you warp on the columns or on the rows. So the first example here. So, the first example here, you have a one. So, this means on axis one, you warp. So, axis one means columns, means you repeat at the second position, you repeat this row. So, in the columns direction, you repeat the row. So, this is the repetition here, and you shift everything. And the second example here: the axis index is. The axis index is at two. So you have the same game, but now you have rows. So you have the rows you shift. So columns you copy. And again, on position two, so this is the column you shift. Okay, and this is the warping in two directions. And now the invariances are functions. Are functions which are independent of this warping. And what is important is that this warping is in both directions independently. So you can warp either on columns or rows, and the invariants should not see this. Or this is what we are interested in. So this is what psi equals psi concatenation with this warping means. And again, And again, you have this warping on rows and columns. And imagine that this is your original picture, like a very high resolution of a dock. Then you can copy each line and each column, and you obtain a much bigger dock. And it looks the same, like if you consider it as a picture, because your eyes will maybe not see the copies. But also, you can do it on certain parts of the On certain parts of the image more than on others, and this is how you can stretch this dog to a very long dog. And the invariants should not see this. So this is the motivation. So this is what I'm going to talk about. And for this, I will introduce signatures. But before I will do this, I will need one more ingredients. And this is a difference operator. So if you have a signature in mind with integral. If you have a signature in mind with integrals and everything, you take the derivative and in some signature, you take a difference operator. So, difference, differential, or difference, this is the same thing, but discrete or non-discrete. And this difference operator is here, this delta, which takes an eventually constant function, so a picture, so to say, and maps it to an eventually zero function, a function of finite support. And the definition is what you see in this line. See in this line, this is, of course, linear, and its kernel is given by all constant functions. So this is, yeah, if you plug in a constant function, then you will obtain a zero function. And also, it's a search active. So you have a sigma here, and you can think of this as, I don't know, text, as a come sum. So come sum, maybe reverse. Maybe reversed, but this is how you can think of this. It does not really matter for now. It is only a third right inverse, and so delta is surjective, and you get a partition. So this is linear algebra one of all the eventually constant functions, and you get a normal form. And this normal form is what you would get, what you would You would get what you would hope. So, you have a picture and you have eventually constant two on all boundaries, and then you map this to an eventually zero function. So, maybe we go to this definition, just at least two or three numerical examples. So, this is the delta, and this is the eventually constant functions. You have twos at all the boundaries, and here is. And here is what where something happens, right? So, this small two by two matrix is where there is really something happening. And then, this seven here, this first entry is mapped with this. So, this is a forward difference. This maps this seven to this two because you add seven plus three minus five minus three, and this is what you obtain, the two. And on the boundary, you have always And on the boundary, you have always constant functions, and then you get a 2. What is also interesting is this locally warped part here, this locally 2 by 2 matrix, where you have a 3 plus 2 minus 3 minus 2, and then you get a 0. But you don't get a 0 line because you have this 2's here in the limit. So you have a 3 point plus 2 minus 2 minus 2. 2 minus 2 minus 2. Okay, so this is the difference operator, and with this, we have everything together to define the two-dimensional signature, some signature. And you can think of the input as a difference signal. So this is what we will do later. So Z. So in order to keep the notation clear, I define it for eventually zero functions. And you can think of this as a different signal of a picture. This is a different signal of a picture or something. And this is a function and it takes pictures and maps it to complex numbers. So this is at least how I've written down. So it is a function which has a linear component in the second or a linear slot in the second component, but it maps to complex numbers. So it's the coefficients in the signature, as you think, in the integral in some signature. Think in the integral and some signature. And well, on signatures are defined on words, but for two-dimensional objects, you have two-dimensional words. So this is something very natural. So we have to define matrix compositions. They are exponent matrices, and they are analogous to words in the one-dimensional setting. And everything else. Dimensional setting. And everything else looks very similar, right? You have the sum over the simplex or this increasing chain of exponents here, and then a big product where you multiply all these together. And I have two examples to make it visually. I have maybe a little bit time to go through one of them. So here you have a one times one matrix. So this is a format one times one. format one times one. So the product here vanishes, so you have only one entry and you sum over two indices which do not see each other. So the i chain here is only of length one and the j k chain is also of length one. So you have only two indices which don't see each other. And the exponent here is one, so you raise everything to the power of one. And this means that you add all entries. So two plus one plus three plus two. plus three plus two. More interesting are two by two matrices where you have like four variables in the simplex here and you have a product of two elements. And the exponents here or the entries in this matrix compositions, they are precisely the exponents here. And you have only two of them because you have so many zeros here. So this is the first, this triangle. This triangle does not appear because here is. This triangle does not appear because here is zero, so in the product that vanishes, but you have also one, two, one. So this is a simplex. So yeah, only increasing change. So I1 would be one, I2 would be three in this case. Okay, and now I collect the properties to call this signature that this is a justified name, and the quasi-shuffle is one of them. I don't defy the quasi-shuffle, so the quasi-shuffle lives on. So the quasar shuffle lives on the vector space spent by all these matrix compositions. It takes matrices. And this is an example. They get quite large soon, but due to time considerations, I won't go into much details. But it is designed in such a way that this algebraic identity holds. And this even works for formal expressions. This even works for formal expressions. This also works for quasi-isometric functions. But it is, yeah, it works for the signature. And so the input or the product here is this A quasar shuffle B and the product of these signatures on the left-hand side. So you all know this. And the second property is chance identity. And for this, we equip this matrix. This matrix composition space, this vector space with a coproduct. And this coproduct is very intuitive. So you diagonalize along or diagonalize along block matrices where no zeros appear. And with this, you equip this space. This space of all matrix compositions, or you obtain a graded by algebra. This graded by algebra is a Hopf algebra, so this is always the case. And with this, you obtain a weak form of chance identity. Why do I call it weak? Because you have no restrictions on the words. So this works for all. So this works for all compositions. This is nice. And the structure looks exactly as Chen as we would hope. So it is a Chen's identity, but the concatenation of pictures is a bit not clear how to do this. And here the concatenation of pictures, which would be concatenation of path in the classical setting, is on the diagonal. So you have all these zeros, and this makes this work. So there are other. So there are other forms of concatenation along one axis, but because you have images, it is not clear on which part you concatenate. But this is chance identity, and this is why we call this a signature, because the two main properties are satisfied: the shuffle relations and the The shuffle relations and the signature. And now, the third and the motivation from the beginning, we return back to invariances. So we have this warping invariances, and we want to use signatures in order to define these warping invariances, to use them in data science or wherever. And as I mentioned in the beginning, or defined in the beginning, we have this difference operator and we use first. Operator, and we use first the difference operator, and then we compute the signature. And therefore, we cannot suspect that something the difference operator already puts to zero will be seen later on. And therefore, we have this, so we define this equivalence relation on eventually constant functions. And there we have this modulo constant component because the difference operator already sets. Operator already sets functions which are equal up to a constants both to the same object. But except this, we have everything we want. And by the way, this is also natural in the one-dimensional setting, right? So you have also this equivalence modulo, the starting position or something. And then we have everything as we wish. So the first So the first two matrices are equivalent just because of the constant shift. So you have 7, 2, and 2, 0. So the difference is only 2. So these are equivalent modulo, the kernel of data. And the second here is where something happens. This is even a very special case where this matrix can be derived by stuttering from the first. But all these three are equivalent. So, but all these three are equivalent, and the signature tells you exactly when such functions are equivalent. So, you have two pictures and you compute its signatures. And when the signatures are all equal, the coefficients of the signatures are all equal, then the original data was equal up to start bopping. Start boping and this constant shift. And yeah, with this, I would finish. I have some final remarks. So some of you have maybe noticed that the iterative component was missing. So this is something quite challenging. So we suspect that, so we have a coefficient. We have coefficients which we can compute iteratively, but it is a subclass, and we suspect that this subclass is smaller than all coefficients. So this is something on the computational side. As I mentioned in the beginning, you can replace the complex number by any vector. So then you can really think of colors like a three-vector or something for red, blue, and I don't know. Red, blue, and I don't know. And so, this is this is only bookkeeping. It would be nice to have a deconcatenation along one axis. So, I've made a picture here where you have the dog and the bird next to each other. And if you want to compute this efficiently with dynamic programming principle, then you would have to deconcatenate along this axis. Axis. And there is a nice approach, a nice formula in this second paper here by Herbert Oberhauser or Derek Lee and co-authors, where they do exactly this. So they modify Chen a little bit here and there. So they add a little sum over one axis or over one time. But essentially, they are asking or doing precisely what I've mentioned earlier, so that you have concatenation, not yeah, so that is not clear. Yeah, so this is not clear, and they have concatenation on both axes. So, this is very nice. And the last point is more from a practical point of view. So if you, these three pictures are equal, modulo stuttering, so this warping, but we would maybe think of local warping also, right? So that you could stretch, like if you use Photoshop, you stretch only at certain points, but here the warp. But here the warping takes place on the entire axis. So, this is how they are designed, and this is how it would be natural in the one-dimensional setting or something. But the other axis are warped. And here, where I produce this curve here, we have this warping here quite a lot and here a little bit less. And therefore, you get this curve from this diagonal thing. But when you have this curve, this When you have this curve, this curve appears, of course, in the later curves as well. So it was quite challenging to readapt this curve here from there and so on. And their local operation would be nice, but this is at the moment not how it is. So, and yeah, I mentioned the paper already. The second one, the third by Sami Tindle, I think he's also in the audience. They use signatures, certain signatures already for texture classification. So there might be nice or there are nice applications, and I would be very interested in further of those. And the fourth is a radically different approach, which interprets pictures as a time series. So yeah, so three directions. And yeah, with this, I would like to finish. Yeah, with this, I would like to finish. Yeah, thank you. Thank you very much. Are there any questions? Second off your talk. If you go to the continuous limits, sort of, you know, you shrink things just by a hands-on calculation, what iterated integrals show up. Did you think a bit about this? No, not much. No, not much. So, there is this paper by Yosha Diel where they do this at one dimension. So, I haven't thought about the two-dimensional at all. No, I can't really answer. Yeah, thank you. Any other questions? Hi. Yes, we have a good representation of an image through your signatures, right? But what about low-dimensional things? Low dimensional things, yes, good question. So, you mean a low approximation of the image? So, or the numerical stability of the signature of this approach. The second one? Both, maybe. Pardon, the audio quality is a bit disturbed, so maybe both. Maybe both at your time. Both, both. Well, low rank representation of the image, I haven't thought of at all. So this is, and the numerical stability, this is, I think, not different from the first, from the one-dimensional setting. So where you use signatures in the one-dimensional setting, except that you have another blow-up, which is the second dimension, right? So even a signature. So let me go to the definition. Even a signature in one dimension has all these summits over all these different components. So the sum is quite large. And here, so you have, I don't know, t to the power of m or something, you know, binomial coefficients, something like that. But here you have, you screw. But here you square the number of summits. So it becomes even more challenging. But I think the complexity is still the same. So this is at least what I hope. And putting maybe many parts of the image via pre-processing or something to zero will help this. So I hope that the same methods as in the one-dimensional case hold here too. Does this answer the question? Does this answer the question? Maybe I have a quick question. I was wondering if you modify the trends identity, where you can sort of do it horizontally, like in the integral case, do you think this would also work in the case of your Yes, yes, it works. So I had not enough time to go into details. enough time to to go into details but um the thing the same um the same result holds for the um for the discrete case so you also have the sum over um so let me go to the gen so you have the sum then over um over the other axis where the concatenation does not occur and um it's it also holds for these diagonal compositions so this matrices which go along a diagonal and um or maybe the anti- And or maybe the anti-diagonal. And yeah, the same problem with permutations probably arises here too. So maybe one has to modify it even further in order to get the modification for all words or so. Yeah. Excellent. All right. Thank you very much. So let's take this meeting.