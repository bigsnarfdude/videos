Excellent. Okay. Well, it's a real honor to be invited here. This is my first time to Oaxaca, so it's beautiful and great to see all of you. Those of you I haven't met, looking forward to meeting you. So I'm going to talk to you about some work that my postdoc, Ali Zito, and I have done. Ali, he's not here, unfortunately, but he. Unfortunately, but he's a really kind of impressive guy. He's a semi-professional actor, at least formerly an actor. He's a painter, quite talented painter. And I recently learned that he's actually in a band and plays the bass. So he's a true Renaissance man. And that's some pretty cool stats. So I'm going to, did that work? Let me try this again. Do I need to? Do I need to click on the slides maybe? There we go. Okay. So, first, let me give you some of the background and motivation for this work. So, this comes from a problem in cancer genomics. And so, just a little bit of background first on where the data comes from and the interpretation. So, in all the cells in our bodies, there's the DNA, of course, which is a Of course, which is a sequence of A's, C's, G's, and T's. And, you know, just a double-stranded helix. So each A is paired with a T and each C is paired with a G. And this is all what you've learned from basic biology. And what happens in all the cells in your body all the time is that exposures occur, either they could be external exposures or internal processes that kind of Internal processes that kind of something happens, and you get, say, like UV radiation or maybe cigarette smoking, and that acts on the genome and introduces mutations. So if we have the DNA of a particular subject of interest, we can compare it to a reference genome and know kind of what should have been in that location. And then what happens is a mutation will show up as a difference in that person's genome versus the reference genome. Okay, so. The reference genome. Okay, so in this particular example, there was a T which mutated to a C, and of course, the complementary base, which in this case was T A, mutated to C G. Okay, so there's, you know, you could think about the different types of the mutations that occur, and we actually categorize them not just by the bases that mutated, but by looking at what was just adjacent to those bases as well. Adjacent to those bases as well. And this gives you some context for the mutation. And so for this example, we would have AC, sorry, ATC mutating to ACC. Okay, so this is sometimes denoted in this way. So A was the base just on the left, if you will. C was the base on the right, and the T mutated to a C. Okay, now, if you kind of count up the ways this You kind of count up the ways this can occur, and you have to account for the fact that it's complementary. So C always pairs with G and T always pairs with A. And if you kind of account for that complementarity, there's 96 possible types of mutations of this sort. So we call these single base substitutions or SBS mutations. Okay. And the reason, so it's, there's only six here in the middle because you. In the middle, because you distinguish the pyrimidine, which is the T or the C. And that's how you reduce that sort of complementarity. Okay, so now this gives rise to the data that we start with by taking for each subject, we have one column in the matrix and each of these 96 single-based substitution types, we have a row in the matrix, and each entry in the matrix is how many times that substitution type or that. That substitution type or that mutation type occurred in the genome of that patient. Okay, so this is based on the idea that you're going to take a biopsy of the cancer tumor, you're going to sequence it, you look at what mutations occurred, and then you count how many of each of these types you observed. So this gives rise to a count matrix of non-negative integers, which forms the data for this problem. Okay, so if we're going to denote that matrix. We're going to denote that matrix by x, and it's an i by j matrix. So i is 96, j is the number of patients. And the basic task that we're going to try to do here is what's called mutational signatures analysis. We didn't come up with it. It's been around for a little over 10 years or so. And the idea is that there are various processes that cause mutation, right? Different environmental exposures. Different environmental exposures, like I mentioned, or cellular dysregulation, just things that kind of either naturally or unnaturally go wrong that introduce these mutations. And the key kind of scientific basis for what we're going to do is that each of these mutational processes has been found to pretty consistently produce each mutation type at a relatively constant rate. Okay, so relatively speaking, like the number of A to T. The number of A to T mutations that occur from UV radiation tends to be the same across different individuals. Okay, so this is kind of the key basis for what we're going to do. And so in order to try to understand what mutational processes are occurring in this data and what each subject has been exposed to, we can do non-negative matrix factorization. Okay, so the idea here is. Okay, so the idea here is we're gonna take the X matrix and approximately factor it into a matrix R, which is a non-negative matrix of what we call signatures, and a matrix theta, which is another non-negative matrix of loadings or signature activities. Okay, so it's going to be I by K for R and K by J for theta. And this has been used, as I mentioned, for quite some time. For quite some time to do mutational signatures analysis. Right. Okay. And the word signature refers to each column in the R matrix, which represents the rates at which each of these mutation types is occurring due to a given process, a mutational process. So each column in this matrix of signatures corresponds to when one mutational. To in one mutational process, which has a signature. Okay. The inner rank of this factorization, the K, represents the number of mutational processes that are acting on the genome in this data. And so a question that always arises is, how do you choose K, right? Whether it's mixture models or factorization models or any kind of model selection sort of situation, you got to pick K. And right? So this is always the little Right, so this is always a little bit of a tricky issue. There's two main approaches in the literature for doing this. The first one is to fit an NMF model. That's NMF is non-negative matrix factorization for each of a range of values of K, and then pick K, say with the BIC or some other crazy neural network-based approach, which is very popular because the leading group who does this analysis introduced it. Group who does this analysis introduced it. Another sort of popular approach for this in mutational signatures is to do automatic selection of K with some kind of sparsity-inducing model. And one method for this, which is quite popular, is called signature analyzer, which uses a automatic relevance determination kind of approach. And another approach is using an infinite factorization model. Is using an infinite factorization model, like a Bayesian non-parametric kind of model, where you allow for infinitely many factors, but you have some shrinkage, which encourages only sort of a small or only the needed number of factors to be used. All right, now, but there's really no existing method that allows you to use an informative prior, which is really key to what we're going to do here, based on known signatures and automatic selection of pay. Selection of pay. So we're going to introduce a new approach, which is inspired by these kinds of automatic selection things, but is a fully Bayesian model. So it gives you uncertainty quantification, allows you to use informative priors, and is computationally relatively easy to use. So the basic model is not a new model, it's kind of the A new model. It's kind of the same model that everyone uses, which is a Poisson non-negative matrix factorization model. So the likelihood part of the model here is just that each entry of the matrix, the Xij, each Xij is Poisson distributed with a rate equal to like one of these entries. Let me go back. Like the corresponding entry in this product of. Of r times theta. So rather than just factoring it directly with some sort of like least squares criterion or whatever, we're going to use this likelihood, right? A probabilistic model. Okay, now as a Bayesian model, we're going to put priors on the signatures and the loadings. So for the signatures, it's actually quite natural to use a Deer Schlay prior because there A Dear Schlay prior because there's, as you can see, a non-negative, sorry, there's a non-identifiability in that a constant factor can be moved between these two, the r's and the thetas. So it's natural and common to normalize the columns of the signatures and use a Dirichlet prior. And then we're going to use a gamma prior on the loadings. Now, the way this is parametrized in terms of the gamma is that they have a mean mu k. And so this is the loadings for signature k. For signature k. And then that allows us to put a hyper prior on that common mean of the loadings. And then the choice of hyper prior is really the crux of the proposal that we're going to use, which is this what we call compressive hyper prior. So with this key choice of hyperprior that I'm going to tell you in a minute, this hierarchical model gives you uncertainty quantification with pretty straightforward Gibbs sampling updates. You have to do a little auxiliary variable. Updates, you have to do a little auxiliary variable augmentation trick, which has also been known for a while. And we get to do automatic relevance determination by shrinking the weights of unneeded factors to zero without having to do this combinatorial kind of complicated search over which factor, how many factors do we need, which factors should be included, or something like that. Okay, I haven't. Now, I mentioned that this hyperprior is kind of a key part of our proposal. So, what we're going to do is something which, on the face of it, looks like pretty innocuous and simple, but it has some really nice key features. So, we're going to use the following hyperprior on these mu k's. So, remember that the mu k is the common mean of the loadings or signature k. We're going to make it an inverse gamma. Now we're gonna make it an inverse gamma with these parameters here. So, J, remember, capital J is the number of samples. A and epsilon are hyper parameters or whatever settings that you can choose. And we're gonna always set epsilon to be pretty small. It doesn't really matter exactly what epsilon is, so we're gonna set it to 0.001 as a default. Now, this hyper prior has a couple nice properties. Has a couple nice properties. One is that a priori it's sparsity inducing, because if you work out what the mean of this prior, this prior of mu k, it's just epsilon, right? So if epsilon is small, a priori, it's trying to shrink things down so that these the means are close to zero. And as a result, just from that, it's going to tend to try to pull these relevance weights, these mu k's for unknown. These relevance weights, these Î¼Ks for unneeded signatures, down to something close to zero, right? To kind of push them out of the model, and then they won't be used in the inference. Another thing about this hyperprior, which is quite nice, is that it has what we call a strength matching property. And this is a little bit weird from a Bayesian kind of subjective prior perspective, because it's not a subjective prior that makes any sense, right? It's really just chosen. Makes any sense, right? It's really just chosen for the properties that it gives us. So, a posteriori, the prior and the likelihood have equal influence on the mu k's for this model. So, in other words, so the full conditional mean of the mu k's, this is that it's actually given everything, but that turns out to just be equivalent to conditioning on theta. It's just halfway between epsilon, which is something close to zero, and the average loadings for a signature k. Loadings for signature k. So the theta bar k is the average of the loadings for signature k. So, in other words, what this ends up doing is that if there's a signature that you really need, so that, and then so the loadings are going to have to be something non-negligible, then the relevance weights mu k end up being shrunk about halfway to epsilon. So it's pretty significant shrinkage, you know. But it turns out that the loadings themselves, the theta, That the loadings themselves, the theta k's, which are the things you really care about for inference, are only very slightly shrunk toward epsilon. And that's because the likelihood contribution from the Poisson thing, from the data, overwhelms the prior contribution from the mu k. So the loadings are still only negligibly affected by this very strong hyperpower. But it does have the effect of helping to shrink. Helping to shrink out any unneeded signature. Okay, so this gives us the additional benefit of giving robustness to mild overdispersion because the fact that this hyperprior is so sort of strong in some sense allows it to overwhelm or overcome the sort of bad sort of tendency of the likelihood to introduce more and more factors that are just being introduced to account for. Are just being introduced to account for model misspecification. So it's a common thing that, you know, in mixture models and factor models, if the model is even slightly misspecified, then as you get more and more data, you tend to introduce more, you know, K gets larger and larger because it's just trying to fit those little sort of misspecified aspects of the model. And this helps to overcome that. Let's see, how am I doing on time? I guess I'm doing okay. 20 minutes. All right, good. 20 minutes. All right, good. All right, now, any questions so far? Let me maybe pause here because that's, I kind of gave the model and some kind of basic motivation for it. And I'm going to dive into a little bit of theory now. So before we get a little kind of in the weeds on that, let me pause and see if anybody has questions. Okay. All right. Now, so Allie worked out this really kind of interesting theory, which Which we had no idea was going to happen when we first started exploring this model. So it turns out that you can, if you introduce these auxiliary variables, y, which are needed for Gibbs sampling, then some kind of nice theoretical properties pop out of this model. So the auxiliary variables we're going to introduce are, we're going to denote by yijk, and they have a natural interpretation, which is just Natural interpretation, which is just that yijk is the number of mutations due to signature k. So channel i is the ith type of mutation, so that's like the ith row of the matrix for patient j. And so you can rewrite this model in terms of by introducing the y's in the following way. So you make the y's just Poisson with the rate equal to sort of a given sort of like a given like product Rik times theta kj. And then you sum up the y's to get the overall xij, sort of the number of the counts that's due to adding up all the signature contributions. All right. Now, as I mentioned, this has the benefit of making Gibbs sampling easy, but it also ends up having a nice posterior characterization of the relevance weights mu. Relevance weights in terms of a new distribution, which we haven't seen before. So, if we define the inverse Kumer distribution to be a random variable with this density here, pointers are not really working. Then it turns out this is the inverse of what was called the Kumar distribution by Armero and Bayari in 1997. In a totally different context, they found this Kumar distribution, and it turns Distribution. And it turns out that so the inverse of that is what we're going to call inverse Kumar. And the posterior distribution or the conditional distribution of mu k given these auxiliary variables y, all the counts, is actually inverse Kumar. So it's pretty cool. It has a closed form expression in terms of this distribution. And that allows us to do some theory. Allows us to do some theory showing that if the average number of counts due to a given signature, so y bar k, converges to some constant as the number of samples grows, then the posterior on these, the corresponding relevance weight mu k converges to mu star. I didn't put the formula for mu star. It has a closed form expression, but it's a little Closed form expression, but it's a little bit weird. It's just something based on like the quadratic formula, and it's just kind of a wonky form. But it actually is very closely approximating something which is flat and then increases linearly. So it's approximately, so the linear part is that it's roughly like y minus a over 2. And then the sort of flat part is that it's, if you kind of squint, this is approximately epsilon. is approximately epsilon when epsilon is smaller than a significantly smaller. So this is, this kind of plot is, if you are familiar with lasso shrinkage, it looks a lot like what lasso does, right? Lasso is like an L1 regularization and it kind of shrinks your coefficients so that if you're close to zero, then it ends up being zeroed out. And then it's sort of linearly shrunk or sorry, it's shrunk by a constant factor. Or sorry, it's shrunk by a constant factor, or maybe shifted by a constant factor above that. And this is very similar, right? It's actually like a very similar looking plot. And so this is showing that as the sample size grows, the posterior on the relevance weight is concentrating to this kind of function. So the one here is not always one. It just ends up being A, whatever. So the A was this parameter in your model here, which is something you can choose. Which is something you can choose, and that controls basically the elbow of this curve. All right, so it's quite interesting, and this helps to contribute to the sparsity-inducing aspect of the prior, because basically, if the signature is not needed, then these counts are going to be small, and then the relevance weight gets shrunk to something very close to zero, epsilon. Very close to zero, epsilon. Whereas if it is needed, then these counts are going to be larger. And in that case, the relevance weight gets shrunk to like basically half of what it would have been. All right, so this leads to this compressive property, basically saying that if, in fact, this is kind of what I said in words, if these counts, if the average count for signature k is going to zero, then the relevance weight concentrates on any neighborhood. Weight concentrates on any neighborhood of zero that contains epsilon. So it's not concentrating at zero, but it's concentrating in a neighborhood that contains whatever epsilon you pick. So what this ends up doing is that unneeded signatures are effectively removed. And yeah, so that's the idea. It's kind of similar in spirit to the overfitted mixture models of Russo and Mangerson, which shrink out sort of any. Which shrink out sort of any excess mixture components that are not needed. But unfortunately, our theory is not as strong as theirs. We have this conditional theory given the whys, but what would be really nice is to extend this to just unconditionally given the data itself. Do you get that same sort of shrinkage of the unneeded component? All right, so now there's a really nice aspect from a Bayesian perspective. From a Bayesian perspective, one really cool thing about this model is that it makes it super easy to use informative priors. Now, for this problem, that's actually really useful because there's been a lot of work in the scientific literature characterizing a wide range of these mutational signatures that occur in different cancer subjects. And so, this catalog of somatic mutations in cancer or COSMIC, which has Cancer or COSMIC, which has been curated, which has a list of about 90 known mutational signatures, known sort of in quotes. Some of them are more well characterized than others, but they're putatively known. And so this is just some examples of different ones. So it's maybe a little hard to see, but each of the sort of entries of this graph is for one of these. Is for one of these 96 substitution types, and the vertical axis is the probability under that particular signature. And so you can see there's kind of a wide range of ones that occur. Some are affecting, you know, many, are causing many different types of mutations. Some are only causing a very small set of mutation types. And these rates are actually remarkably constant when you do inference across different subjects. It's across different subjects and groups of subjects and cancer types. It's actually kind of nice. Right, so how are we going to use this prior information? So we can actually just very easily extend that basic Poisson model to include an additional part. So this was the original part that we had in the mean of the Poisson. We're just going to add another part here, which is another factorization that has. Factorization that has the signatures given an informative prior centered at these known signatures. So for each known signature, we're going to put a row k signature in there, which is not exactly equal, but close to. So we're going to put a Dierchlet prior so that it's encouraged to be close to that known signature. And then for the weights, the omegas, the loadings will just be given gamma priors as before. Priors as before. Same as with the thetas. So this is similar to a model developed by Izzy Grabsky and actually in our department. But we also had to have this additional, so augmenting it in this way, similar to her recovery discovery model. But then we also have this compressive hyper prior, which makes it very nice to do inference in this model. All right. So simulations. So, simulations, we did some simulation studies under correct specification. So, when they did actually push on versus a little bit of over-dispersion in the form of negative binomial. And we simulated using some of these known signatures in this, like the row k part of the model, and also some just random, randomly generated simulated signatures for this other part, the arc k. All right. And then the first thing we looked at is how well do you estimate K? Do you recover K accurately or not? And we compared with a bunch of the leading methods here. And so the first two, the blue ones are our method with the informative prior. That's comp NMF plus COSMIC, and also our method without the informative prior. Signer, which is a Bayesian method using BIC to select K. SIGProfiler, which is the most SIG profiler, which is the most kind of popular method, which uses like least squares sort of estimation and then bootstrapping. SIG analyzer, which is an ARD maximum A posteriori approach, and then a infinite shrinkage or infinite factorization model, which is based on the cusp model, but we had to adapt it for this Poisson setting. And so what happens here is that when the model is correct, when it's actually Poisson, they all do really well. Actually, Poisson, they all do really well. Basically, everybody nails it, they all get the right answer. But when there's a little bit of over-dispersion, then some of these models just really totally break down. So, the signature analyzer, the MAP model, and the CUSP model just drastically overestimate the number of signatures. So, they're not sensitive to, sorry, so they're not robust to that misspecification. Our models are both, both of our models are more robust, and these other two actually do pretty well as well. Other two actually do pretty well as well, Signer and SIG profiler. In terms of the precision and sensitivity with which the true signatures are recovered, you can also see, so basically in this plot, we're blue, dark blue and blue, and you want to be in the upper right corner. That means you're getting high precision and high sensitivity. Basically, it's kind of same story when there's no over dispersion, when the Poisson model is correct, everybody's doing pretty decently. Everybody's doing pretty decently. Actually, interestingly, the SIG profiler model, which is the, that's like the kind of industry standard, kind of doesn't do as great. And especially when there's over dispersion, you can see that some of the methods really struggle. And like these are getting down to like a precision of 0.5, which is pretty bad. Whereas our methods are doing a lot better, especially the informative prior method, the dark blue one is just nailing it. Blue one is just nailing it, which, in some sense, you know, it's not too surprising because we're putting in these known, like known signatures to simulate the data and we're using this, whoa, I break this, simulate, and we're using the same known signatures in the model. So it's not too surprising that it does better, but it's nice to see that come out in the data, in the results. Okay, so lastly, I'm going to show you an application here to breast cancer data. To breast cancer data. And this is actually kind of like the first real data set that mutational signatures were applied to. And since then, it's become a benchmark for these types of methods. So it's just actually 21 samples. So J equals 21 from 21 different patients. And the original paper, The original paper that looked at this found five signatures. They just used non-negative matrix factorization and they estimated these five signatures, which you can more or less see here in the plot. So we did kind of ran all these existing methods on this data again for a comparison. And then to evaluate the performance, we looked at the cosine similarity, which is the kind of standard metric, distance metric used in this. Distance metric used in this application. The cosine similarity between the inferred signatures and the true signatures. Well, I can't say, they're not, I can't say true signatures. The nearest matching cosmic signatures, which are the nearest matching ones in this curated database of like currently nowadays, what's accepted to be kind of the gold standard of a true signature. So you do your inference, you estimate these signatures, you match them up with the sort of Signatures, you match them up with the sort of known true signatures, and how close do you get to a known true signature? That's what's represented here. So you can see that our comp NMF cosmic method is just kind of blowing everything out of the water with cosine similarities very close to one for many of these inferred signatures. The comp NMF, which is our other sort of non-informative prior-based approach. Approach does well relative to the existing methods, not as good as when you use the informative prior. And yeah, so the other ones, some of them just really struggle. Like SIG profiler, that's like the industry standard, just only picks up three signatures and they're not really great matches, honestly. Poisson Cusp picks up quite a few, but they're sort of lower cosine similarity in several cases. So overall, So, overall, in terms of this metric, it seems like our methods are doing pretty well. Also, something that's nice about the informative prior, even though it's sort of cheating in some sense, right? But the fact that we're using these brown truth signatures as an informative prior, that actually does something, a couple of things that are really helpful. Because when you do this analysis, you have to match up the inferred signatures with this curated set of like known signatures. Set of like known signatures. And there's always some uncertainty or questionable matches. But here we get actually really unambiguous matches to these known signatures, which is really helpful. And it allows you to prevent, maybe do I have it on the next slide? Yeah. So you can avoid mixing up signatures that should remain distinct. So for example, here, in this, these two signatures here have These two signatures here have, since this original data was collected over many, many studies, have been found that these are actually two distinct signatures. But if you just look at this data set, there's a strong correlation between the presence of these two signatures in the patients. And so they always get mixed up. If you don't use an informative prior, then these get combined or merged into one signature. But you wouldn't know that from just this data alone. So using the informative prior is really helpful for that. Is really helpful for that. Yeah, the other thing, which is nice, is that we get uncertainty quantification, and that helps you to flag kind of potentially spurious signatures. So like this one over here for like the informative prior method, it has a lower cosine similarity, but it also has like a quite high uncertainty. And so higher uncertainty can be used as kind of a flag of potentially spurious signatures. So we can also indicate the uncertainty in these plots here with like error bars on the These plots here with like error bars on the signatures themselves. All right, so just kind of wrapping up, I think this is a really interesting method. It's super simple in some sense. It's just a kind of pretty basic Bayesian model with a really peculiar choice of hyperprior. But it allows you to do automatic selection of the signatures and the number of signatures. Computation is pretty fast, actually. It's really. Pretty fast, actually. It's really easy to implement. The informative prior gives you these unambiguous matches to known signatures, gives you uncertainty quantification, and high precision and sensitivity for recovering those signatures. So some future directions, extending it to multi-study settings with covariates, better robustness to model specification, which actually I have a PhD student who's finishing a paper right now on that, and speeding up the And speeding up the inference algorithm, especially, you know, we're interested in extending this and to more complicated models using variational inference. So that's my talk. I'm happy to take any questions. Thanks for listening. Thank you, Jeff. Any quick questions, please, Jim. Thank you. Oh, do you want the mic? Yeah. Mike, yeah. Yeah, you're on time. Yeah, very cool stuff. Thank you. I have two questions. My first is about identifiability and your concentration results. Yeah. Yeah, I was a little confused about the concentration without like identifiability of like the different columns. Well, we cheated basically in the theory because there is non-identifiability, but it's actually very, we removed one source of non-identifiability, which is the multiplicative constant. Non-identifiability, which is the multiplicative constant by forcing normalization of the signatures. But these models are tricky with identifiability, as you probably realize. So what we did for this theory is we introduced these auxiliary variables, which they already kind of decompose the data into the k different signatures. Right. So, yeah, that makes it a lot easier. Yeah. Because, yeah, I. Um, because yeah, I was wondering if this possibility was helping with that or something like that. But no, I mean, it's curious because, like, I mean, empirically, it really does seem to concentrate even without enforcing other identifiability constraints. So it's an, I don't know if there's, yeah, maybe an interesting theoretical question there. Yeah, there are a lot of like anchor words, like anchor topics type stuff that with sposity and identifiability. That's really important. Oh, yeah, yeah. That's really good. Oh, yeah, yeah. Right. Assuming sparsity helps, yeah. And my second question was: I would like to get more intuition about why your prior setup is doing better at modal misfestation than other sposity-inducing priors. Like Spike and Slab hits you pretty hard with the sposity-inducing side. Do you have an issue? Yeah, so it's this is the particularly peculiar part about this model, which is that it's basically the strength matching thing. It's basically the strength matching thing here, right? The strength of this hyper prior is growing with the sample size. So it's as J increases, the number of samples grows, the hyperprior is more and more and more strongly concentrating at zero. And that helps to counterbalance any sort of bad things that are coming from the likelihood, right? So it's enough. I mean, it's in the hyper prior. It's enough, I mean, it's in the hyperprior, so it's kind of up there, but it's enough that it keeps misspecification from sort of introducing spurious signatures. It only works for small amounts of misspecification. If once you kind of go farther away from the model, yeah, then it's going to introduce more stuff. But yeah. Yeah. So how do you actually determine the number of factors at the end? Right. And so the way this works is that you choose this. You choose this epsilon, right? And the signatures which are not being used end up getting shrunk to have a mu k of very close to epsilon. It's pretty tight around epsilon. And so there's a really clear and easy sort of threshold. We always use something like five times epsilon, but it's always pretty unambiguous. Yeah. Yeah. Another question is probably irrelevant to our methodology, but at the end of the very end, you said the informative part is very useful, especially for the two signatures that is always simultaneously happening in the patient. So I'm just curious how the database, how did they figure out separate? Well, because they have more data since then. This was just like one study, but there have been so many studies now, thousands of different samples. Thousands, thousands of different samples, at least. And then, if you have enough data, then you can resolve these. Yeah. Yeah. Thanks for the talk. I'm just curious, is the Kumer distribution the same as the inverse Gaussian? I don't. Oh, is the Kumer the same? No, I'm almost positive it's not. Let's see. I mean, if we look. So, well. So, well, I mean, you get this one over mu in the exponent, which is, I think, not really looking inverse Gaussian. Oh, the inverse of this? Yeah, I don't think so. I mean, yeah, I'm pretty sure it's different. Yeah. Anybody know? Too bad Allie's not here. Or maybe he's on Zoom. I don't know. I am on Zoom actually. I am on Zoom actually. Oh, great. Now we can answer the question. Yeah. The question was: one, yeah, one over this is just a Kumar distribution. That's why we called it the inverse Kumar. It's from that paper by Heron Bayar in 1997. Totally different setting, but I haven't found this distribution anywhere, so we basically named it. Yeah. Yeah, and the Kumer is not the same as the inverse Gaussian. I think that was the same thing. No, no, no, no, no. It's different. Any more comments out there from the Zoomers online? Any Zoomer question? So Jeff, could you uh it's super super nice discussion, I really enjoyed it. It's it's beautiful, like straight after the region ali with informative fires and all that. We've been formatted fires and all that. I always found that there's always this discrepancy between actually variable selection versus just shrinking the coefficients. We obviously chose the letter because it was formative to Jank's question. It seems to work out just fine. But do you have any general comments on putting in a mixture model with the actual probability methods here because it's just a passive shrinkage? Right. I mean, sort of the original. Sort of the original motivation for not doing like a spike and slab type of thing is really computational. These automatic relevance determination approaches are just computationally so nice. And like it's a kind of complicated space, but by doing this continuous shrinkage and then like pulling it all the way to like epsilon, it just always like it just always works. Just always works. And it doesn't really take that long for it to converge. I mean, you know, you have to, it's a Gibbs sampler, so you have to run it, but it's pretty reliable. So that's the nice thing. Yeah. Other obvious question, dependence of the signatures? Oh, dependence, dependence on the signatures. In what way? Oh, a priority, like you make them a priori independent, right? Oh, in terms of this prior, the prior on the signature. Right. Could we think we learn about dependence potentially in terms of which ones are occurring together or separately or dependence within the yeah, yeah, yeah, right. I mean, there's a couple of ways in which dependence could be introduced here that might be interesting. One is, just look at some of these signatures. There's, well, you can't really see as much here, but there are clear dependencies within each. There are clear dependencies within each signature in terms of the types of mutations that tend to occur, right? So, like a C to T mutations here are tending to occur in this one, but not other stuff. So, you could build like a hierarchical model or prior here that accounts for that dependence. Another way which you could do dependence is in terms of, and this we actually observe very directly in the data. I mean, this example was. With, I just sort of alluded to it, is that some signatures tend to be correlated in terms of the loadings. And so if you look across patients, it'll be like, all right, these two signatures either tend to occur together or not. And so you could introduce dependence in terms of the loadings, like correlation of signature loadings. And that would be potentially useful as well. Yeah, yeah. I mean, it's cool. I mean, it's an interesting scientific question, actually, as well. Question, actually. The other obvious question: we have to go from there. So, in the end, you get like the inference of the signatures. We could use it now for potentially some other interesting things or like any tools. Oh, like other applications. The last slide was really convincing. You confess to the cheat. I think it's a beautiful cheat. But it was convincing, yeah. But what do we do with that kind of inference then in terms of going further to So, I mean, in this application, the signatures are used for understanding kind of the biology of cancer, right? Like, what are the processes that are giving rise? And these have been linked to, in many cases, the etiology of what was the kind of physical or mechanistic process that gave rise to this, just like not from this kind of analysis, but from. Just like not from this kind of analysis, but from doing further follow-up studies. Yeah, and then connecting this with kind of with medicine, and there's been some work on, for example, I was on a paper, collaborative paper on pancreatic cancer, and there's a certain signature, this HRD signature, which if you have that signature, then there's a treatment which is actually known to be more effective, right? And so looking at And so looking at, you know, can you detect which patients have that signature, then they're candidates for that treatment. So pretty cool. Yeah. Any more comments, reactions? Then thank you, Jeff. Hi. Thank you. We have twenty-five minutes break. I think there is four.