I have two goals in this book. One is to introduce a new type of online research citation problem that maybe you want to come up with if you're just kind of following a lineage of theory problems of how we typically generalize things and think of the next, you know, our next project. And this comes from a particular cloud computing application. And the second thing I want to, my goal is to show you a probabilistic tool that I think is not super well known in the theory community. And that's this idea of stochastic monotone matching. So I hopefully I'll give you a new tool in your toolkit. Give you a new tool in your toolkit. I think this could be useful for other online problems. So, this is a joint work with my co-authors, Konstantina Melu and Markovin Microsoft Research. And this was performed when I was an intern there last summer. Since we're part of the cloud computing group, let me first give you a bit of motivation. So what's our problem? We also have physical machines over here on the left, and we're serving virtual machines that our clients are requesting from us. So these are these little cloud computer things. And in principle, each physical machine can support multiple virtual machines. Machine can support multiple virtual machines subject to some constraints like memory, power, and synthesis. But here's the problem: what happens if a machine fails? So, suppose this first machine fails. Our clients are not going to be happy if we just let this in this state. In particular, this is a big goal in the cloud computing industry to maintain very high levels of reliability. So, this is how you convince people you should actually have a cloud computer and not just buy your own physical machine. So, after it's written into the contracts that we're going to have at most one hour or one day of downturn. Or one day of downtime per year. So, this is a very important problem in practice. And it turns out that assigning virtual machines to one physical machine provides too high a level of risk and it's too operationally expensive to figure out what to do with these stranded demands now if this machine fails. It's too complicated to figure it out. And it's too high of a risk. So, the idea, what we do, we want to change the assignment process to make this reassignment in case of failure easier. So, this is our new model. Here's the idea. We're going to add redundancy to this assignment process. So, this is inspired by real systems architectures. If you go more details, this is like a paper in the bottom which describes a particular architecture where this is implemented. Here's the idea. Instead of assigning a demand to a single, I'm going to call them demands, virtual machines or jobs, whatever you want to call them, demands, to a single machine, I will assign them to a pair of machines. I will split the resource consumption of this demand. Resource consumption of this demand in half, like in this little picture. So I have two half demands now, each of half the size. I want to assign each half to distinct machines, like in this picture on the bottom. So now what happens in a case of failure? So if a machine fails, suppose this machine on the left fails, you will reassign all the half demands that are assigned to this machine to their neighboring machine, the other half of that edge. Poss and this, you know, 'cause we're in an emergency situation, we can possibly increase the capacity on that. Can possibly increase the capacity of remaining machines that are still occupying. We call this a failover scenario. When a single machine fails and we have to do something about it. So, note this has some advantages. One, like the reassign process is fixed. You don't have to recompute it every time you change the initial assignment. As soon as you assign something, it's fixed wherever where this neighboring machine is. This is good for operational simplicity. And just to give you an example, what would happen in this case, this machine on the left fails, we're going to resend another half demand to this machine on the right. Okay, so let me just give you the commentarial online problem we're going to solve. We're going to call this online damage question with failover, hence the name of these failover scenarios. So we're given m machines. You can imagine they're the vertices of a graph, because I'm assigning things to edges of this graph. So it's a complete graph. I can assign jobs or demands to any pair of vertices, assign them to any edge on this graph. So n demands will arrive online with sizes. So I imagine just one resource, the most important one is power. Source, the most important one is power. So you can just imagine it's the power consumption of this virtual machine. And I'm going to think of the height of this rectangle as the size of that demand. And upon arrival, I've assigned each demand to a pair of machines upon arrival to satisfy some constraints. So let me just draw a few more demands for a second. So maybe the second demand arrives to assign it here. And so on. I've assigned a few demands. What are the constraints? So let me just... The first one is our typical storm capacity constraint, which I'm going to call the nominal constraint. Stream, which I'm going to call the non-linear screen. It says: if I look at each machine, each vertex, and I looked at all the edges instant upon it, the load instant to each machine should be at most one. So if I sum the height of all these rectangles, it's at most one. This is sort of our typical linear capacity constraint, so we're pretty familiar with it. The new thing is this fail-ever constraint. So in every fail ever scenario, I want to guarantee that, by fail-ever scenario, I mean if a single machine fails, so a single vertex fails, the load incident to each machine is at most B, which in general can be larger than one. So on the picture. Larger than one. So, in the picture, suppose this one relation there is this top left machine fails. I'm interested in what happens to this bottom left machine. Well, now this half demand that was assigned here is now reassigned down here. So, in fact, now I'm doubling the effect of these two demands that were assigned on this ninja. And if I sum up all this stuff, it should be almost e. And if you want to, like, a you know a mathematical formula for this developer, can you figure out this way? So, the load at every machine, so what's this first term? So, what's this first term for the machine? Every machine U, I sum over the load on every edge, and then I add the worst possible failure, right? So, the worst possible single-edge failure is better VMO speed. And what is my goal? My objective is I want to maximize the utilization of these machines. That is, I want to maximize the total size of assigned demands until the first demand needs to be rejected. So, whereas proceeding online, we have a fixed urban boundary capacity computed to these constraints. At some point, we might have to reject some. At this point, we just stop. At this point, we just stopped. And the reason we don't allow arbitrary rejections is because that's just not what you do to your clients, to be honest. So, this is all, is there any questions about this problem? Just a clarification, yes. You say half of, but it could be any fraction. Or is it half of? So, we're going to only focus on half path, but that's an interesting question. And the reason for that is because, like, operational simplicity, if you just fix half-path. But that is an interesting question that's probably worth thinking about in the future. If you allow an arbitrary If you allow arbitrary splitting work across more than two devices, one could imagine this as well. Yes? In the online model, you only have jobs arriving, right? So you won't have some demand that ends, like say a client doesn't need the prescription anymore or something like that. Oh, that's right. Demands are only arriving. Yeah, there's no like departure later. That's another question that could be worth thinking about. Any other questions? The failure listener is always just single machine. Failure listener is always just single machine failure? Yeah, yeah. So, again, this is another interesting direction of generalization. We only consider single machine failure because it turns out in practice that it's unlikely for multiple machines to fail at the same time. And I guess they've just determined, I can't know the details because they don't work there, but this is sufficient to achieve the levels of doability that they want. Because you can imagine that if a single machine fails, this is a very urgent thing. We want to fix it really fast, so it's unlikely that something else is going to fail during that small time. When you fail during that small period of time, that's the sort of theory. Anyone else? Any questions about this? Practices B, like. Great question. So practice B is like, it's not one, it's like something between like 1.2 or 1.5. It depends on the particular data center. You can imagine it's a bit bigger than 1. So there is some, you do get some extra capacity. Okay. Yeah, in general, feel free to. Yeah, in general, feel free to stop me in the future if there are questions. I probably won't use a full 30 minutes. I don't plan to. But we'll see what happens. So, just to give you a brief sense of the context of this work, so of course this, like, there's a long lineage of different sort of resource allocation problems, solid resource allocation problems. Probably the most relevant are like multiple knapstacks. You have multiple knapstacks at your disposal. You want to pack items in these knapstacks to maximize your total reward. Perhaps most related is this coupled placement type problem where, again, you're given, say, a hypergraph, and you Hypergraph, and you have items you want to pack in this hypergraph, but how do you do it? You assign items to edges, and this problem again kind of just had its non-listing. So every hyper edge incident to a vertex, you have some upper bound on what can be assigned there. But crucially, I think what's different about this problem than all these other previous resource allocation problems is this new addition of redundancy on this, and particularly scaleover scenarios. It's what makes it different. You have to keep track of sort of the max edge and stand edge vertex, which is sort of different than other ones. Is sort of different than other ones. What are our results for this problem? So, just one technical detail. If you want to ask what is opt, I'm going to do competitive analysis compared to opt an offline policy that knows all the demands, knows the whole demand sequence, but it also has to assign demands in the same order, right? It can't cheat on a small list demands at the end, which you'll never see. So, it has to assign demands in the same order until it rejects. But you don't have to worry too much about this. Not worry too much about this. Just for curious. And we consider two models for this demand security problem. Consider a worst-case model, where you get a one-half minus a little of one competitive algorithm. And this little of one is in terms of the number of machines down. So as the number of machines goes to infinity, the competitive ratio approaches one half. And there's no data terminal scenario from the script that are half-competitive, so it's as best possible. Second, perhaps more interestingly, consider a stochastic model of arrivals, so like an average case model. So if all demand slices are drawn IID, so the pumps. Sizes are drawn IID from some unknown distribution. I don't make any assumption on this distribution. They've actually approached the optimal solution because a one minus a little more competitive outcome. We're pretty good on time. So my goal for the rest of this talk is I want to give a sketch of the worst case just to get some intuition for the problems, to get a feel for it because it's a bit different. And I want to give a high-level overview of the Stochastica. We're going to show you how this new tool, well, not new tool, but this old rediscovered tool of Stochastica on. Rediscovered tool of stochastic proton matchings will help us get this one minus a little one guarantee. Let's begin with the worst case. So here's the idea. I've just transferred the fail over to the top right just so you can keep it in mind. So suppose you're looking at one vertex and you assign demand to have some tiny size epsilon to one of its edges. What does that fail over question going to tell you? It says you've got to be ready for this possible failure. It could be the case, you've got to be ready for this demand size to double. This is one possible failure. But, you know, suppose you have many demands of size epsilon. If you put them all on the same edge, this is very costly for your failure budget, right? Because you have to double the size of all of those demands. So the first intuition I want to give you is this constraint that tells you that you should spread out your demands. You shouldn't put too many on one edge. So this would be nicer, right? If you could put one edge, demand of size epsilon each. So then you only pay two times epsilon in case that two times the total space. So, I'm going, for the rest of this talk, I'm going to assume the failure of budget is one just for simplicity. This just makes the math easier. So, what you can imagine, what's the best case here? If I have all these maps of size epsilon, I really want to arrange one in each edge for one of our epsilon minus one edges. This is the max cost we can package. It's an easy calculation for the fill over constraint. So, what's, you know, what do I love is if I have enough demand subsidies epsilon to just fill a clique. Demands of size epsilon, so just fill a clique of size one over epsilon, such that every edge has exactly one demand of size epsilon. This is like by a volume argument, you can see that op can do no better than this up to small. So you know, you might want to keep building cliques of size epsilon. We're really spreading out your demands a lot if they're all size epsilon. Sorry, cliques of size one over epsilon is for demands of size epsilon. So we're really spreading out your demands. But we have a fixed number of machines, right? So we want to balance this idea of spreading out demands and not using up, you know, spreading them out too much, using too many machines. You know, it's buying a lot too much, you see too many machines. That's the idea. The trade-off the key is more than the algorithm is actually quite simple. It's basically built off this idea of reserving cliques of the appropriate size for demands of different sizes. So again, we're just going to assume b equals 1. What you don't, you know, all you have to know about that is we want to arrange demands of size one over k in a clique of size exactly k. Ideally, so we'll likely. And for simplicity, up to some standard dispersation ideas, I'm just going to assume all demands are size one over k right into k. Assume all demands of size 1 over k great interk. The algorithm is very simple. So if a demand of size 1 overk arrives, just assign it to any open engine and reserve a clique of size k for that particular size class. Otherwise, just open a new clique of size k and assign it there. Or in a picture, maybe the first, I'll denote different size demands by different colors. So maybe the first demand is blue, I open a blue clique, I assign the first demand there to some edge. Maybe I assign a second blue job arrives, I assign it to the same clique, but a different edge. Same clique, but a different edge. Maybe the next job derives is R, and so I have to open a new clique, so I'm spreading things out right. Perhaps a different size because these demands are larger, so I need a smaller clique. And so on, eventually maybe enough blue demands arrive to fill up this entire clique, one for edge. So if another blue demands arrives, I have to open it. And so on, eventually I'm going to run out of machines. For one, I'll go through the whole analysis, but the idea is quite simple. For all these cliques just by volume market that are filled, one per edge, we're happy. We're happy basically, we're doing as well as opt due to a volume argument, up to discretization. The only problem is, like, we might open a blue clique and only put a very few blue jobs in there before we have to fail due to some, you know. So I should be clear, we stop as soon as we can't reserve a clique that if we want to reserve a new clique of size like K and we don't have that many machines left, we just stop. Let's give them. So we're wasting all this space in these remaining cliques, right? They might not be full. The idea here is. And I D here is, again, like kind of standing. We want to handle very large K separately, so very small demands we're going to handle separately. So we can ensure that there are actually not too many different demand sizes. So K is gotten like little O of M. And each of these requires a small size set, little O of M. So and then, you know, if you tune these primitives correctly, you can show that you will only waste little O of M machines. And this is where the little O of one loss comes in the competitive ratio. Competitive ratio. Any questions about this analysis? This is all I want to say about the worst case. Don't do anything to mention that. That's hidden. I kind of brushed it over there. That's in the systemization thing. Yeah, so you're losing a little bit due to that. Because basically these are packed optimally, but we end up losing a half. So kind of by standard digitization ideas. Any other questions? Okay, then let's continue. So we handled the worst case. So we got one half. As M goes to infinity, this is best possible. Now let's talk about perhaps a more technical case, which is the stochastic model, where we get basically an optimal algorithm as M goes to infinity. And of course, your first thought is, I should learn this distribution implicitly somehow. So how are we going to do that? And this is where this idea of stochastic matching comes in. But here's the idea behind the stochastic model. Hand of stochastic model. So again, you know, each demand is drawn IID from some unknown distribution mu. So here's kind of a template algorithm for us. So suppose we've already signed the first n prime demands. We've already seen them, we already packed them somehow. Maybe not well, I don't know. We'll just pack them somehow, like in this patient. Now, what am I going to do? I'm going to say in hindsight, what should I have done for these first-hand prime demands? What was the optimal thing to do? So I'm going to compute a, you know, in general, a near-optimal assignment of the realized demand size is equal to the minimum number. Realize demand size is equal to the minimum number of possible machines. So maybe I did it in five machines, but if I would, you know, if I was a little smarter, I could have done it in four machines. I'm going to say for the next N prime online arrivals, I want to try to mimic this optimal solution. I'm going to use this as a template for my next n prime online arrivals. So I want to use this template as time to assign the next n prime things that I can arrive online. And how do you turn this into an overall algorithm? Well, see the first demand, you assign it somewhere. Demand, you assign it somewhere. Now you can use that to learn what to do with the second demand. Now you've seen the first two demands. Now you can use that to assign the next two demands, you've seen four demands, and a standard doubling argument will give you the whole output. So this is really the heart of the optimal. So there are two questions here. How do you keep this in your optimal assignment? I will maybe get into that later if I have more time. But really what I really think is the most interesting is how do you use this assignment to assign this subsequent upcoming online arrivals? Subsequent every time online arrivals? This is a really interesting question. Because we really, remember, we have to be really, we cannot be wasteful at all because we really want an optimal solution up to little of one factor. So, how do we do this with basically no loss? And this was the idea. So, if you haven't been paying attention or maybe taking a break, I encourage you to at least pay attention for one minute to look at this theorem because it might be useful for your maybe future research. I don't think it's super well known. So, this is a monotone matching theorem due to Riemann telegram. This actually arose from. This actually arose when they were studying the online bin packing problem in the same model where the items arrive IID. And here's the theorem. So we're given two sequences of random variables. You can imagine that the X's are given to offline, but the Y's arrive online one by one. And they're all drawn in IID from the same distribution. So you can handle different distributions up to some error, but I'll give you a simplified statement here. Both x's and y's are from the same distribution. The same distribution, and you know, we can compute a match from the y's to the x's online, such as with high probability. It's monotone that says that if yi is matched to xj, then yi is at most xj. This is what I mean by monotone. You only match the things bigger than you. And further, you match almost everything. Only advantagedly, only an advancingly small fraction of device remain not matched. So, the remarkable thing about this is you can do this online. Like, offline, you can probably imagine some passenger tracking to show that, like, these. Like some passenger tracking should show that these two things are close. But how do you do this online? And I won't talk about the proof of this. It's already uses some measure theoretic ideas and it gives an explicit algorithm. Yeah, they're all the same distribution. You can handle some discrepancy. If the distributions of the X's is slightly different from the Y's, you can do this with some error in this little n. But I'm suppressing that for the purposes of our. I'm suppressing that for the purposes of our application to the other. But how do we use this template? It's perhaps an interesting question. And it's pretty simple given this theorem in hand, right? So at the bottom, I've just drawn the same picture. I have my template for the first n prime. This is what I'd like to use. And the next n prime are going to arrive online. So imagine that the slots in this template are the X's, the online arrivals are the Y's. So just a peanut monotone matching from the next N prime online arrivals to the realized first n prime, online as the subscript. Online as the subsequent end prime array. And if you're matching upon arrival, just assign that arrival corresponding edge in the template. This monotonicity property ensures that feasibility right because you're only assigned to a larger slot in the template. So you're only using less of the capacity. So in a picture, what's it need? Maybe the first, the next command arrives, it's matched here. So I'm going to place this demand here. Second demand arrives, it's unmatched. Oh yeah, so the last thing is if something arrives is unmatched, you know, there are a few things unmatched that can be pretty wasteful with them. I'm just going to assign it to a disjoint edge by itself. I'm just going to assign to a disjoint edge by itself. This is OC. The second thing arrives, it's unmatched and it's dependent on its own edge. Third thing as well. And eventually assign almost everything in this way. And note that this assignment is always feasible on the right because we've only, due to this monogenist front, they're only decreasing the usage of the capacities. Just to understand. Yeah, this is using this half-half split. Yes, yes, yes, yes. In principle, I guess a template, if you In principle, I guess the template, if you were allowed to choose arbitrarily how to split things, the template can encode like, oh, if you want to put a demand on this edge, you should split it on third, two, thirds. You should probably have to generalize that. The template should just encode more information. Any other questions about this? Yeah. There's not a sort of change in the why wouldn't you just sort of suck it? Oh, that works. That works. I mean, that's the algorithm, which is the best of it. But I guess the question is, like, why does it algorithm? Yeah, so they analyze this algorithm directly using some measure theoretic ideas on D company. They analyze their purpose as a process, some measured theoretic ideas to show that that indeed gives this guarantee. The little n, what's the function? It's like square root n times polygon characters. Any other questions about this? Okay, so let's just zoom out a bit to complete the algorithm, right? So we had our template algorithm here. We want to be peering up from assignments. So I filled in the second guide how to use this assignment. We want to complete this online monotone matching thing to assign the next and prime theme online to our templates. I won't really talk too much about the first step. Really, talk too much about the first step about how do you compute this near offline assignment? Well, this is just an offline problem. I give you some deterministic demand sizes, tell me the minimum amount of machines you need to assign them all, subject to these constraints. But this is basically, this is very similar to the classic offline impacting problem, and you can use a configuration LP like that to solve this problem. But there is small loss, like little one loss as well. We'll have a few minutes, so um. Minutes, so I haven't really told you about the analysis of this procedure. Like, why should you expect this to work? I'll give you a bit of intuition. The idea is that, so now I've seen the first two n-prim demands, and I've assigned them, and I think I've assigned them pretty well. Why? Because this, by sort of induction, is assigned near optimal n to the minimum number of machines. I use this, I basically assign the subsequent n prime also near optimal to the optimal number of machines, plus some few unmatched things. But this is managing a small, so it's not a big deal. Vanishing is small, so it's not a big deal. And the idea is that let's think about the time our algorithm fails to assign the next demand. I've assigned everything up to now, basically, the minimum number of machines possible up to like little of one factor. So basically, opt is going to fail shortly after, right? I've barely wasted any space so far. And because we're in the IID model, like the utilization basically grows linearly with the number of rounds you survive. So you just want to survive as many rounds as possible, right? Because each round has basically expected. Because each round has basically expected the same increase in utilization. And this is why this kind of analysis doesn't work in the worst case, right? Because it's clearly not true in the worst case. And this is sort of the idea of the analysis, why this should be true. If we fail, we basically pack everything optimally, up to a little of one factor. So far, so optimal fails shortly after in a few more rounds. Little of m rounds, optimal failed. And this is where you lose a little of one factor in R, guaranteed. And there's one technical notice I want to share with you, which I want to share with you, which might be a little unintuitive. But note that when we do this template thing, we seem the first M', we assign the next M prime, we do use them on disjoint devices. We use fresh machines for every round, right? In particular, we never use these edges that cross these rounds. This seems pretty wasteful, right? Because we want to spread things out as much as possible. So, are we wasting edges? Is it really true that even though I've assigned this near-optimally, and I've assigned this near-optimally, is it true that the union of these instances? That the union of these instances I will still assign your optimally? I think this is not immediately clear. And indeed, we can show that this is true. And this is also kind of an interesting statement I want to show you. So let minimum machine N be the minimum number of machines needed to assign n demand strong IID from you. So this is a bit of a mouthful, just in a picture. You should imagine we basically assigned the first n prime and minimum machines, n prime, up to little of one factors. up to a little of one factors. The next subsequent m prime arrivals, we also assign the minimum machine n prime because that's the size of our templates, plus again a little of n prime factors. We really want to show that this is almost the same as the minimum machine 2n prime, right? And indeed, this is what we can show. We can show sort of a quantitative convergence that this minimum machine n is basically linear in n. That is, with high probability we have for all n, that the minimum machines need to show assign n demands as n. Need to assign n demand as n times some universal constant only depending on the distribution. You can imagine C mu as like the average number machines need for a single demand from the distribution as the number of demands you draw goes to infinity. That's really basically the definition of C mu. You can show this draw quantity convergence that the additive loss, sorry, that the additive deviation from this basic limited, from this limit is at most little. What does it mean for our picture? It means indeed we have actually signed everything so far into the minimum number. So far, into the minimum number of possible machines up to literal and prime factors. And this basically completes the analysis. Again, I won't go into proof at all. It's using a similar kind of tools as to the algorithm I should be, we're going to use some kind of stochastic matching to get a deterministic proxy, and then we're going to use a conversional key to understand the deterministic problem. Good, so that's all I want to say. Just to summarize, we introduced this new problem of introducing redundancy and these failures in errors to surround. Redundancy on these failover scenarios to sort of resource allocation problems to better model the situations seen in these cloud computing applications. This was the Sony Humanization with Failover problem. Consider the worst case of the stochastic model. Worst case, we get a competitive ratio of 1 half as m goes to infinity with a tight lower bound. And this was the idea of just reserving cliques, spreading things out as much as possible. The stochastic model, we gave a competitive ratio that approaches 1 as m goes to infinity. And the idea is that we want to learn from the past using these template assignments in this stochastic monitoring. Template assignments in this stochastic monitor matching right here. Yes, that's all I want to say. Thank you. I'll take any questions. Question or two before lunch? Yeah. So in the worst case setting, do you believe that randomization could let you do it a little bit better? Oh yeah, so we actually have a that's a great question. So we only have considered deterministic algorithms where the best possible one have. For randomized algorithms, we have a lower bound that's below one half, it's like four-thirds or something. We haven't thought about designing better randomized algorithms for this. That's definitely view in the beginning. Instead of just following the past, can you just complete the optimal thing given view and then use that information only for the past each patch? So I can mu mu upfront. New mu up front, what do you think? I guess my question is: how would you improve this result, right? Can you imagine this problem? I mean, isn't it only easier? Sorry, am I missing something? But then not using the not I see. Not using this like learning focus idea, just like yeah. I see, so you're just saying like, if I knew MealFrint, I'll just sample a bunch of like the optimal templates. Yeah, yeah, that's that's component and that's just that. Yes, that's even exactly template for each of the benchmarks. That should probably work because you can just, so maybe I'll go back to this. So basically, using this idea, if you knew, you can basically estimate CMU and you would know how many demands you should expect to be able to fit. And you can compute the desk template for that. And off to little, if that one factor is really fine. So yes, I think that would work. It would give a simpler algorithm. You don't have to do all this doubling nonsense. 