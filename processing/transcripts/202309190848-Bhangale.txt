AND is also a C S P, but the IT is K, a number. Think of K as like ten or fifteen, some fixed number, and each constraint is AND of K literal. And the goal here is to find an assignment to the variables that satisfies all the constraints. So we want to check if the whole system is satisfiable or not. Now, let me describe the optimization version of the. Let me describe the optimization version of CSP, which is called the Max CSP, and which is the main topic of this talk. So, here the input consists of a predicate. So, I'm defining the CSP as in a very general form. So, here we have a predicate P which is an assignment from some alphabet sigma to the k 201, which just tells you the set of satisfying assignments. Uh okay. Okay, right. And then you have a set of variables, say n variables, these are va variables taking value from the domain uh sigma and uh you have a set of constraints. Now here each constraint is you apply the predicate to a tuple of k variables and here as in case of the three side and k and you can also plug in a literal Uh you can also plug in a literal instead of a variable. Okay, so I won't go too much into it, but uh think of literal as some shift of a variable. Okay, and okay, so here the goal here is now to find an assignment to this variable that maximizes the number of satisfied constraints. And a constraint is satisfied if the predicate evaluates to 1. Okay, so formally for an assignment sigma, the value of an instance phi at sigma is the fraction of satisfied constraints and we want to maximize the value of the instance. And we want to maximize the value of the extra. That is the goal. So, okay, so what are we interested in? We are interested in characterizing the approximation threshold for every maxPCS. So, you give me a predicate and then I want to say that whether this predicate is hard, how hard is it to approximate and so on. So, more formally, if the instance is C satisfiable, which means that if I give an instance where there exists an assignment that satisfies at least C fraction of the constraint, so that's the guarantee you have on the input instance. The guarantee you have on the input instance. Then a relaxed goal may be: can you find an assignment that satisfies S fraction of the constraint? Think of S as a number less than C. And so the C is referred to as the completeness parameter and S is the soundness parameter. So let's look at a few results and the few known results about specific CSPs so that we get to know what are the results. We get to know what are the results that we know for these specific systems. So, max 3 sat again, predicate is or of 3 liters. If I give a satisfiable instance, then there is a very simple approximation algorithm which is just pick a random assignment. So, if you pick a random assignment, then the probability that each constraint is satisfied is 7 over 8. So, in expectation, you can hope to satisfy 7 over 8 factorial constraints, and this gives you 7 over 8 approximation, even if the instance is satisfied. Instance is satisfiable. Or even if the instance is not satisfiable, the same algorithm gives you 70 rate approximation. And the famous result of Australia says that this is the best that it can do for this. For max 3 lin over in general an abelian group G, where constraints are checking some group equation involving three variables. If the instance is satisfiable, then there is this algorithm of Gaussian. Algorithm of Gaussian elimination that gives you a satisfying assignment. And this algorithm is similar to the high-speed algorithm that you see to satisfy or to check to get the satisfying assignment of some linear set of equations. So the same algorithm works. So if the instance is satisfiable, you can find a satisfying assignment in polynomial time. But if the instance is almost satisfiable, if that's the only guarantee you know about the instance, then again in the same paper, Hasta showed that. In the same paper, Hasla showed that the random assignment is the best that you can do on this CS. So, there's a difference when the input instance is satisfiable versus the input instance is almost satisfiable. In all these talks, you should think of epsilon as arbitrary small constant. And there is a similar predicate, which is maxilline over a non-abelian group. Now, instead of the abline group, you have a non-abelian group, and a constraint is just checking a group equation involving 3D. Checking a group equation involving three variables. So, here, if dot is the G operation, each constraint is you take a product of three variables and you ask that whether that product is equal to some element or not. And in this case, the same result holds in the almost completeness case. And again, the random assignment gives you 1 over G, and that's the best that you can do. But the situation changes when the instance is satisfiable. So, if the instance in this case is satisfiable, In this case, it is satisfiable, then you can no longer find the satisfying assignment in polynomial time, it is NP hard. But at the same time, you can do better than just assigning a random, just getting a random assignment. So, the approximation factor in this case is 1 over the size of the commutator subgroup of G, and for many non-ablane groups, this size is less than the size of the group. So, this is a better algorithm than just random size. So, yeah, so the question was: you have to characterize the approximation threshold for every max PCSP. And there's a huge success in understanding this question. And this is by the result of Ragnar. So, for max CSP, you can write down a semi-finite program, just a relaxation like LP, but slightly stronger than LP, linear program. And then you can ask for the integrity gap for this semi-finite program relaxation for. This semi-definite program relaxation for a given max PCSP. So, the Aguinda's result says that if you give me a CS integrated gap instance of this basic STP, which means that there is an instance of this max PCSP where the integral value is S, but the STP value is C. There is a gap. Then this implies you can take that integrity gap and use it in the reduction to show that it is NP hard to find S plus epsilon satisfying assignment even if the instance epsilon satisfying assignment even if the instance is 200 epsilon satisfying. In other words, it says that and we also know an algorithm that gives you an S satisfying assignment if the instance is satisfyable. So we have an algorithm that matches the NGID gap of a max PCSP and this is just says that that is the best that you can do. So SDPs give you the best approximation algorithm for max PCSPs. Now there's a caveat here because Caveat here because I said that SDB gives you the best approximation algorithm. Usually, when we compute the approximation factor or the approximation quality of an algorithm, we compute this ratio which is the ratio between the optimum solution and what algorithm determines. So, in this case, the approximation, the hardness approximation we get is S over C. And that's how even if And that is how, even if you lose some epsilon CR, you still get the same ratio. So, in that case, this is a resolved table and it fully characterizes the approximation threshold of max species. But the main problem of this, and also I should mention that this is NP hardness assuming the unique aim schedule. But the main drawback of this result is that it loses perfect completeness. So, we can never get a hardness result on. Never get hardness result on instances that are satisfiable because you always use epsilon here. So you can't, so we can't use the Ragwinda's result to conclude anything or to say anything about you know if I give an instance is satisfiable, how better can you approximate it. And that is the main topic of this talk. So can we characterize the approximation threshold for every satisfiable CS? In other words, if the instance In other words, if the instance is unsatisfiable, can we find an assignment that satisfies at least s factor of the constraints in polynomial time? And again, we are interested in, from the hardness side, we are interested in finding the smallest s such that this can be hard. And from the algorithmic side, we are interested in finding the largest s so that it can be done in vulnerable form. Any questions so far? Okay. And again, for the threes are predicated, the soundness parameter is 708. For the three sides, 708 for the trillion or the subelian group, non-abelian group, the sum of the parameters is. And the question that we are interested in is like, you know, it's not just for specific predicates, but for like a class of predicates, or ideally for all the predicates. And this question is highly non-trivial because even if for a given predicate to understand if this optimal s value is less than 1 or not, it is the same as solving this dichotomy conjecture which. Dichotomy conjecture, which is now a theorem. So, which got recently resolved. So, answering this question is at least as hard as solving the dichotomy or proving the dichotomy theorem. So, now that we have the proof of the dichotomy theorem, so the natural question is: you know, now let's try to find the optimal value of this S for every particle. Okay, and in order to motivate the class of predicates we are interested in, I would like to focus on this algorithm. Again, this is a simple algorithm, it just causes an elimination. I want to spend some time on this algorithm so that we will capture some property of the predicate which is present here that we can use in getting this better approximation algorithm. And this property is called the linearity of a predicate. So, linearity of a predicate is which. Predicate. So, linearity of predicate is which should be used in both these algorithms. So, it's obvious here. And now I'm going to spend a few minutes on why this property got used getting better. Okay, so I need a few definitions. So, again, we are interested in this CSP where each constant is a group equation over this non-ablined group G. For a group, for two group elements G and H, the combinator of this pair, this pair. This pair, this pair is this element of this group, which is G inverse at inverse GH. It's always identity if G is abelian because you can switch these things, but if G is non-abelian, then you get different elements. And a commutator subgroup of a group which is generated by this G G is a subgroup generated by all these computers. So, you take all these commutators and you take the subgroup generated by those. So, the commutator subgroup of G L and Q is S L and Q. These are the matrices, N frozen matrices over FQ. N crossing matrices over FQ with non-zero determinant, and these are n-crossing matrices over FQ with determinant 1. The curved race of group of Sn which is the group of all permutations is A n. This is the alternating group, which is the group of all permutations with even parity. Now, the important property of this competitor subgroup is that not just the competitor subgroup, but any normal group, is that if you take the quotient of the group with this competitor subgroup, The quotient of the group with this computer subgroup, then the quotient group that you get is an abelian group. So, for these two examples that I mentioned, if you take the quotient of G Ln Q with S and N Q, you get this group which is isomorphic to FQ star, which is the multiplicative group of this field FQ. And it's a variant, right? It's just multiplication mod Q. And if you take mod of S m with an, you get F2 star, which is just a 01, Z2, a billion proof. Abelian proof. So, and this is the crucial property that we are going to use to get better approximation algorithm for filling over this tonal plane. And again, so this is the whole group, this is your competitor subgroup, and the quotes of G are the squares, and the quotient group is each element of this quotient group is just a square of these in this picture. And it satisfies. So, if you take any element from here and any element from here, and you do the normal G group operation, you get an element. G group operation, you get an element component. For every two square, the square where this product will land is fixed. And this is a billion. Now, it doesn't matter whether you force like this plus this or this plus this over this portion. So that's so this is the fauclo algorithm which gives you one over GT approximation. So let G be the abelian group at D get by modding G by the subgroup. Now let's convert the equations over G to equations over this abelian group H now. How? So we have Group H now. How? So we have this equation over G. Let's convert all of these group variables to the group variables, to the H group variables. So for every Ai, you take the coset containing Ai in H and just replace Ai with this capital here. So this is a group equation. This is an equation over G and this is an equation over H. And I'm just using fresh variables just to denote that these are like equations over the sublime group H. Over the sublime H, the y-variance. So now you can easily observe that if this system of linear equation is fully satisfiable, then this system of linear equation is also fully satisfiable over H. You just take the satisfying assignment and replace the value for Xi with the respective posit in this system of equation. So if this is satisfiable, this is also satisfiable. But now this is a system of equation over a period. Is a system of Big N equation over a billion group, and that's the important property that we want to use. So we can, in fact, find the full satisfying assignment in polynomial time using Gaussian elimination sort of. So we have this sigma i assigned to yi and this assignment satisfies all the equations over h. Okay, now to get an assignment over g, for each xi, you select a random element from the posit that you have assigned. Each y gets Coset that you have assigned. Each Y gets a cosette which is an element of the quotient group. And then to get the final assignment, you just select a random element from that coset. And again, you can easily check that this random process will end up satisfying a given equation with probability 1 or the size of the computer. Because no matter which element from this coset you choose and which element from this coset you choose, there is always an element from the third coset such that this whole equation is satisfied. So that this whole equation is satisfied over each. And there are only the size of the commutators of group many choices for each of these. So that gives you one or the size of the commutators of group approximation. Yes? It is a good question. You're finding a solution over H, is it possible to always lift a solution back up to G? Or are you doing it randomly? Yes. So find that there's any solution over H does it evolve to a fully solution? In general, it should not be because it's an NP-complete problem. Because it's an NP-complete problem. Because if you could do that, then you could solve. So, yeah, so here you could ask for more. Instead of just sampling a random element from this coset, can you do some clever rounding algorithm to get an assignment over G? But we have a matching hardness result that shows that this algorithm is also optimal if these are. So you can't do better than this algorithm in general. Any questions? Is it NP-hard for every non-abelian group? It is NP-hard for every non-abelian. It is NP hard for every normal equipment. Okay, so the linear property of the predicates. So now here, so now we are at a point where I can define the linear property of the predicates. So let's consider a group where the competitor subgroup is non-trivial. It's not the whole group. Then consider the natural map, which maps the group element. Map which maps the group elements to the respective quotient group, elements from the quotient group hatch. So for example, I'm mapping this element from G to this element of the quotient group and so on. Then when XYZ, I mean the product of XYZ is 1, which means that if these three variables they satisfy a group equation, then the corresponding map element they satisfy the group equation or H. Again, this is an abelian thing. It's an abelian thing. abelian thing. It's an abelian thing. So this is what I call a linear property of a predicate. So you can embed the domain of your predicate to some abelian group so that whenever the instant whenever the tuple is satisfying tuple then the corresponding you get a corresponding abelian group equation if you follow the map. It's only one direction. Sorry? It's it's only one direction, right? Oh sorry, it's it's only one direction. Yes, you only need one direction even yeah. So you only need one direction even yeah. Because you walk the phenomena. Sorry. Because you evolved in a form. Right. Yeah. So because even in the algorithm you only needed one direction. So you only wanted that the yeah, so the equation should be satisfiable on over this upload. That's all. So this is okay. Make sense? And this linear property got crucially used to get a better algorithm. Okay. So let's add. Better algorithm. Okay, so let's abstract out this linear property for general predicates. And that's the key definition. So a predicate which is from sigma, so I'm just focusing on three LD predicates now, which is from sigma 3 to 0, 1 has a linear property if there exists an ambient group H. And now we are considering three different mappings because of totality from sigma to H, such that at least one of the mappings needs to be non-constant because we can always assign everything 0. Because you can always assign everything 0 and then you will have the property. But for this property to be non-trivial, we want one of the maps to be non-trivial on constant. And whenever a tuple is present, is a satisfying assignment, the corresponding map group element satisfy the group equation. So, in picture, so I can write a predicate as you know, this is the domain sigma, the satisfying assignments are the same color, more black. This is one satisfying assignment, this is another satisfying assignment. This is one satisfying assignment, this is another satisfying assignment. So you can view your predicate like that. And in this case, you can embed this predicate into this group C file, which is I'm just assigning the elements from Z file to each of these elements from the domain. And now it has the property that if you look at any satisfying assignment, it satisfies the group condition. So the addition of these three group elements is 0. Same for here. And for a concrete example, if you look at this predicate, which is you take three permutations and they form accepting tuple, if the compulsion is the identity, then the mapping that maps the permutation to the parity of the permutation, it's an abelian embedding of this predicate. What is your group batch? Yes, uh it should be finite, yeah. Constant finite. It doesn't matter what your group batch is. Okay, and it and the same definition can be generalized to KD predicates, the same thing, you know. Now we'll have like k maps and so on and so forth. Mitchell's question. So we want h to be small because we want a 1 over h. I mean is that what we are aiming at? Ideally yes, you want h to be small, yes. But but I mean that was a specific predicate and that was a That was a specific predicate, and that was, I mean, it has a lot of structure, but in general, the predicates would be arbitrary. But yes, I mean, yes, having a small group helps, but what we are going to see, we are going to, yeah, okay, but that's a good question. Okay, so again, coming back to the question: can we understand the approximate of satisfiable max CSP when the predicate has some structure? Ideally, we want. The predicate has some structure. Ideally, we want to prove the result for all the predicates, but that's small. And the structure that we're going to focus on, or the kind of results that we got, is characterization for satisfiable theory predicates where the predicate does not have a linear embedding. If you give me a predicate, a theory predicate that does not have any linear embedding, then we have some hardness results for, we have like full characterization. For we have like full characterization, you know, this is an algorithm, and this is the best that you can do. Yes, so the algorithm that I described that had a linear embedding. But of course, if you take a non-obliging group where the size of the competitor subgroup is the whole group, then it does not have a linear embedding. This will work for that also. I mean, this. I mean this? Yes, yes. So translations are allowed. Any other questions? Okay. So the main thing, I mean, so this is really the key point here that linear property, the linear property of a predicate can lead to better approximation algorithm because there is this two logic. Algorithm because there is just two log houses in elimination, right? You can hope to apply that to reduce your search. Instead of getting a random assignment, you can cleverly go down to a small universe and then sample assignment from there. That's what we did for the Max3DNO and non-Ombi. And STPs, they don't capture Gaussian elimination. And so the hope is that if there is no linear property, it means that STPs use the best option. That STPs give the best optimal approximation algorithm. And that's what we show. So if the predicate does not have a linear property or linear embedding, then STPs, they give you the best approximation algorithm, even if the instance is satisfied. So just think that Don mentioned that if this one-sided so if you have some satisfiable uh segments does it mean there is a subset of those that satisfies like alpha a plus no you want there's a upper side of. No, you want the superset, yeah. You want superset. You want all the satisfying assignment to satisfying tuple to be more. Yeah, if it is more, then it's it's also fine. Yes? So yeah, that's not allowed. It's uh uh it the maps they need to be more constant. Okay, so in the next 15 minutes, so I want to spend some time on Ragman's calculation and then I'll just point out to one location where if we get a good results there or better theorem there, then it directly applies, it directly proves our theorem for these three areas, ESPs with more linear embedding. Okay, so again. Okay, so again, so what Ragminda showed is that if there is a, so now I'm just focusing on the case when the completeness is almost one. So if there exists a one s integrality gap instance for max PCSP, then Ragminda showed that it will actually be hard to find S plus epsilon satisfying assignment even if the instance is almost satisfiable. Again, this indeed it means that the S C P value is 1, but the actual integral value is S. And the main component in the reduction is these things called decreased tests. So, what are these tests? So, consider a function from sigma to the n to sigma. So, think of n as like large number. So, this is not related to the idea of the predicate. And so, a function f is for a dictator function, if the output depends only on the one variable, any of the n variables. So, there are like n different dictator functions. Dictator functions. And a function is like, say, a function is far from a dictator function. So if the output depends on too many papers. Very vague definitions, but at least for the purpose of this, this is enough. Okay. So the following was enough to prove Arbinda's theorem, which is that if there is a 1S integrated instance for max PCSP, then there is a dictatorship test. It tests whether a given function is a dictator or not by looking at k. not by looking at k by querying f at k locations if directly of the predicate is clean such that every dictator function passes the test with probability 1 minus epsilon so all the no matter which dictator function you give the test should pass on those functions with probability almost close to 1 whereas if a function is far from dictators then the test should reject and here the rejection probability is less plus 1 so this the completeness of the test is almost 1 the sum is also Of the test is almost 1, the soundness of the test is S, which is the parameter from the integrity app instance. And this is enough to get the actual hardness equal. And a small, yeah, so here we are interested in a test which queries f at k locations and the accepting criteria of the test should only be you know you apply this predicate on this k out k outputs. So it's a very special situation test. A very special decrease of test. So if your predicate P is KLE, then the test should vary F at K locations and the accepting credit should be this. Only then, if you have such a test which has this property, then you can directly translate that to a UG hardness design. Okay, but the thing is that for satisfiable CSPs, our hope is to not lose epsilon in the completeness. We want a test whose completeness is 1 and the soundness is optimal. And the soundness is optimal. So, okay, so how does a test, a typical dictatorship test looks like? So, again, you want to query f at k locations. Okay, so there are like k inputs, each of length n. But I want to sample this column-wise. So, each column, so if I look at the inputs as a matrix k cross n, then each column is of size k. Then each column is of size k. And so let's say I fix some distribution mu over sigma to the k and I sample each column from the same distribution independently. That gives me my k inputs. Then I query f on these k inputs and then my accepting criteria is this tuple whether it belongs to p inverse 1 or not. And okay, then that's that's that's that's the dictatorship test. Now the clever part of the test or designing a good dictatorship test is coming up with this distribution mu. Is coming up to this distribution mu. So that's the only variable in this test. So the correct distribution or the distribution that works for I'm going to result is the distribution mu that comes from the SGP solution of this integrated gap instance. So I'm not going to say too much about it, but you know this is so you have this integrated gap instance, you have the SDP solution for that whose value is one and the distribute the distribution mu is coming from the SP solution. Mu is coming from the SDP distribution. Just some distribution on. So the SDP value 1, it means that the support of the distribution has to be in P inverse 1. That's a property that follows from the SDP value, which means that the completeness is 1. Because if the support of the distribution is in new, then we are, for dictator functions, what we are really checking is whether the column belongs to pinball 1 or 1. And this tells you that if the support of new is in pinball 1, each column we are sampling it from the set of sites, right? Column, you are sampling it from the set of sites by a samples, right? And that's why the completeness of this test is one. But then why do we lose epsilon in the completeness? And that's the main point of this talk, right? So the reason we lose completeness, because we want to prove the soundness of this test, also, right? We want that if a function is far from dictator, then it should pass the test with product at most S or S plus also. So you can express the So, you can express the test classing property as you sample these inputs, this is the location for the same distribution, μ to the n, and then you check whether that tuple satisfies the predicate. And now you can basically split, so f is a function from sigma to the n to sigma, right? So, you can basically split this expression into two parts: low degree part and high degree part. So, I'm not going to say what exactly is low degree, but it's like a natural definition of a low degree function. Of a low degree function. Now, the low degree part gives the contribution s if f is far from the function. So that's really the right parameter. Which means that we are left with high degree terms and we need to show that the high degree terms are negligible in this expression. If we show that, then it will end up showing that if the function is far from dictator, then the test passing property is tested. Okay, and it's only this part that we work on. Okay, and it's only this part that we work on, you know. So we want to show that in this expression, the contribution that is coming from high-degree functions is negative. And this is where Ragmuna used this result by Mossel. This says that if the distribution mu is connected, then if you take high-degree functions and evaluate the product of the functions where you sample these k inputs from this distribution mu. Infosed from this distribution u, then you can bound this expression. You can say that this expression is negligible. Okay? And so I'm not going to, so the connectedness definition is here, so I'm not going to say too much about it, but one easy way to make a distribution connected is just, you know, you started with this original distribution mu, you just add a little bit of noise to it. So again, the distribution is reproductive 1 minus epsilon. I'm going to sample it according to the estimate. Epsilon, I am going to sample it according to the STP distribution, you know, and if per d epsilon, I am just going to sample a random string from sigma to the k. And now this is your new mu and you do your depletion test based on this. And this mu satisfies the connectedness property. So far, so good. And now we now finish the analysis. So, as I said, this mu tilde So as I said, this u tilde, it satisfies the connectedness property, so you can really control the high-degree term from this expression. So the soundness is S plus little bit, which is what we wanted. But the problem with this is that the completeness suffers because now we can no longer say that each column belongs to p minus 1 because there is this small chance that you know you go out of the set of accepting assignment. So the completeness or the so the dictator function, they pass the test with producing only 1 minus epsilon and not. Will be only 1 minus epsilon and not 1. So, if we really want our test to pass with probability 1, then it is enforced on us that we want the distribution mu to be fully supported on p inverse 1. It can be a subset of p inverse 1, but it cannot be like a superset of p inverse 1. So, we want a distribution such that it's fully supported on p inverse 1, and at the same time we want And at the same time, we want the distribution μ so that we can control the high decrease term. If we have these two properties, then it gives us a decreased test with completeness 1 because of this. And the soundness of the test will be S because we can control the high degree term. And the high degree terms, they are not always negligible. So, this is a very small example. So, consider a predicate which has a linear embedding. And alpha, beta, gamma, these are the linear embeddings. These are the linear embeddings. Let's say chi, let's into the sublient group H, let chi be any non-dual character of the sublient group H. Then define these three functions which is I am taking the input, okay, which is a string from sigma to the n. Then I am taking the input, which is from sigma to the n, then I am getting the elements from h to the n by mapping by following these maps. Following these maps, and then I'm evaluating my character on this string. That gives me a real or complex valued number, and that's my function g. Okay, similarly, I can define a function g2 where I use the embedding this, and g3 where I use this. And now, if you look at the product of these three, and again, these functions are high degree because you know the degree of these functions is all is n. So, if you take the product of three functions, If you take the product of three functions, you get this product, and now you can use the additive property of this character, and you get this addition over h. And now, if you relate this expression over this equal to the n, now we can use this property that since we are only sampling columns from p inverse 1, so this is always 0. That's the linear property of the predicate. So, you are evaluating this expression with the identity of the group, which is 1. So, here is an example of a predicate. So, here is an example of a predicate such that this term that we are interested in, it's not negligible, in fact, it's 1. So, we can't hope to get negligible contribution from high-degree terms for such predicates. And the main result that we show is that that's the only possibility for 3D predicates. If it satisfies linear embedding, then you can't control. But if it does not satisfy linear property, then we can. Linear property, then we can actually control the height equipment. So to state it fully, so take a predicate P which is not linearly embeddable, and now that mu we have a distribution which is fully supported on p inverse 1. So this is where we replace the connectedness property. Now we are not using the connectedness property of mu, but we are using the property that mu is supported on tuples that are not linearly. On tuples that are not linearly embedded. If we have that property, then you can say that the high-degree terms they are negligible. Now you can run the same analysis that Ragmenra did in his paper. You take the 1S integrity gap instance and design a declation test with that. So the soundness of the test is S plus negligible because of this. Because the low degree part still gives you the contribution S and the high degree part is negligible. S and the high degree part is negligible because of this analytical lemma. And yeah, in the earlier paper, we showed the same result, but some extra properties on the predicate, which is really like the predicate needs to be slightly dense, then we can show this. But in this paper this year, we got rid of the same indicator property. So it's just like purely a non-linearly embedded property. Yeah, so this is what the earlier work was. If you need the distribution new to be connected, we can configure the same thing if we just assume that the support of new is not linearly embeddable. And this is a stronger result than the connectedness property because there are predicates that are not linearly embedded and are not connected. So if the predicate is not connected, you can't use this lemma, but then you can use our lemma to. Use our lemma to control the hiding terms for this particular stuff. Then, otherwise, you are doing some invariance principle for these kinds of stuff. Not really the invariance principle. So, the invariance principle gets used in the low degree part, where you show that the contribution from the low degree part is S, right? So, there you replace the variables or Boolean variables with Gaussian, and then you need to show that. Gaussian, and then you need to show that you know you don't change a lot by moving the Gaussian. And then you use the Gaussian expression to come up with a rounding algorithm. So the invariance principle you use in that part. Here it's just some pure analytical thing that you need to show. If that probability is not linear at all, so suppose there exists a mu, F and G and S, then the contraposit is that you will find a linear element. Yeah, so that's that's the few yeah, so that we didn't yeah, so that's that's what we grew recently. So I'll I'll mention that uh on the last slide. So this is a probabilistic statement where the linear embedding is that it has a it has to work for all For all satisfiable things we need to have. Right. Okay. Right, right. No, no. So this lemma only talks about predicates that are not linearly embeddable. And here we saw an example where if the predicate is linearly embeddable, then this expression can be large. And we recently showed that, yeah, if this expression is large, then that is the only possibility. These functions, they behave like what we saw in the example. That's the only possibility. So again, I showed that. I was just saying something. This is counterpositive to this thing. I was not saying something. But yes, this is the art. And there must be a lot of things. The question was: was that how the proof goes by finding the linear embedding? No, that's not how the proof goes. The proof goes by some sort of induction. So you show it for the base case when you only have functions from sigma to 0, 1, you gain something, some 0.99 in there, you know. And then since these are like high degree functions, And then, since these are like higher degree functions, for each degree, you ideally want to gain something for each degree. We'll go next. So, what is the certificate of non-linear inverting? What is the certificate of non-linear empiricality? The base case. I mean, yeah, so if you have just function f gh from sigma to minus 1 plus 1, then expect a value of f of Expected value of f of x, g of y, h of z. Here you just sample x, y, z from just mu, right? So this is going to be 1 minus epsilon and not 1. I mean, this is not exactly what is true, but you know, we sort of use similar to this property. So you gain something in the base case when you have like one gradient function, and here we are talking about n. Function and here we are talking about n variate functions. And high degree just means that you know for each degree you are trying to gain 1 minus epsilon. So you gain something like this with the degree is like linear and then you get this. So this is where we use the non-linear property of the predicate. H minus 1 1 all that we need is that it's bounded? Yes, we need that. We need the boundedness. Otherwise it is not true. Okay, so as I mentioned, we can just use that and finish the dictatorship test. So we have this thing: if there is no linear embedding, and if you give me one SMD variety instance, then I can get a dictatorship test with completeness 1 and some of the S plus and so on. There's just one small caveat, which is I want that every local distribution to be fully supported on P plus 1 in this integration. One in this integrated gap instance because otherwise I won't be able to finish the dictation test analysis. So, except for this, we have a full calculation of except for this condition that every local distribution needs to be fully supported on V must one, we get the full result for predicates that are not linear compatible. Isn't that the meaning of SDP value being what? No, it can be the local distribution can be supported. No, it can be the local distribution can be supported on a subset of equal predictions. So, right, if the predicate, so these are like the satisfying assignment of a predicate. But if you give me a SDP solution where every local distribution only is supported on this, then my predicate is essentially, the distribution is essentially supported on this. But then these two plus they may satisfy the near everything. And then I can't use the analytical memory. So for the distribution, for yeah, that's why we need that the every local distribution, the support of that. Local distribution, the support of that must be not femalely available. So that's some extra technical condition that we need, and we don't know how to get rid of that. One perfect completeness. You can't say most constraints have this property, you're right. No, we want like all constraints, yes. Yes. Even if like one constraint has this, we can. And yeah, so we can use this lemma to get hardness result. Of course, we can't use unique games conjecture because we. Of course, we can't use uni games conjecture because we lose perfect completeness. But then there is this thing called Rich 221 games, which is similar to uni games. And it has the conjecture is that even with perfect completeness, if soundness is small, you can use that as a starting point and use this decrease to finish the reduction. So, this is the further progress that I just want to mention: that earlier we saw that the predicate is not linearly embeddable, then the expression is small. And recently, we showed that. And basically, we showed that if the expression if the now let's look at now, let's look at a predicate which is linearly embeddable. And what if the expression is large? Then we showed that the only way this can happen is that f looks like the character that we saw in the example, plus some small. So this is sort of the stability result. The only way that the expression can be large. So let me just one question. Here also you have. So, just one question. Here also you have the fully supported thing. So, here we do not prove any hardness result. Here it just. No, no, the hypothesis that you want to be fully supported. You need it here the because if you have a tiny, tiny mass in one point, it can't really help improve. Actually, for basically your pattern we knew a constant. A constant. N is going, the number of valids is going to infinity. So, like in mu, it's either constant probability or zero. I think so. So, yeah, so n is going to okay, got it. So, if it's if it's non-zero, it is green. Come back. Okay, so let me wrap up by mentioning an open problem. So, so far, whatever we discuss linear embedding, non-linear embedding, in the mass for that, if you only Embedding lemmas for that, we only showed those lemmas for theory predicates, okay. But then we can talk about internal theory predicate, and we don't know. So, the hypothesis is that the same thing holds for Kary predicate. So, if the predicate is not linearly embeddable, then for high-degree functions, this expression is small. And we don't know how to show this right now. And again, if we have this, then we get this hardness result by plugging it into the detection as a framework of R. Into the detection as a framework of algorithm. And another question is: if the predicate does satisfy some linear embedding, then can we use that to get a better approximation algorithm? We saw that for this max linear non-appliane group that for just a specific predicate, but can we use this in general? So can we use a combination of... So STPs, they give you the optimal algorithm for max PCSPs if you don't worry about epsilons. So if the predicate satisfies some linear embedding, can we use the combination of STP and Gaussian elimination to get like And thousand elimination to get like optimal approximation. It's like quite open problem. Okay, so let me end here. Let's just like give one quick question. 