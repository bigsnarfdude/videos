City of Linz. He is a PhD student there, and he will talk about non-optimal point sets for numerical integration. Thank you, Christian, for the nice introduction. And I want to thank the organizers for giving me the opportunity to talk here. And yes, I want to expand on that and talk a little bit about myself. So currently I'm a PhD student in Lindz under supervision of Eike Heinrichs and Josha Brokno. Hendricks and Joshu Brokno within GAARTS. And this year, at least most of the time, I was in GAARS doing some research on large deviations and limit theorems in high-dimensional geometry, but also working in parallel on some topics which I'll present in the following. And last year I completed my master in LINTS with the topic discrepancy and numerical integration on spheres, and this may explain why I And this may explain why using this rather special kind of discrepancy of which we already heard a lot by Uhu to motivate some questions later on. And in general, I'm interested in the geometry behind numerical problems. And today I will be going to talk about the problem of numerical integration. So let me share my screen. So, yeah, this is based on joint work with David Krieg and Fritz Pilichsammer from Lindz. And about the part with Fritz Pilichsammer, I talked in the already mentioned seminar organized by Damier, Ryan and Alex. So I only refer to there for the recorded talk and will focus on the other part. We'll focus on the other part in the following. So, as a general setup, I will consider the following. So, D will be a set, some bounded domain or manifold which we work on. Mu is, for example, the uniform measure on this domain. And F D will be a class of functions which are bounded on D, so that function evaluations make sense. And if we have a point set P, we want to approximate the integral. We want to approximate the integral with respect to μ using the function values of at this point set P. And we want to do this for every function and we want to use a linear algorithm. In fact, linear algorithms are optimal in this setting. So a linear algorithm is given by real weights w and is this quantity here. And the worst case error of this algorithm I denote by Algorithm, I denote by this quantity here, is the supremum over all functions in this function class of this error here. So the difference between the linear algorithm and the integral true value. So the quality of a point set for this problem can be, for example, defined by looking at equal weights, so the price of Monte Carlo error, so to speak, or also if we Or also, if we optimize our own weights, this can also be quality. So, the worst case error of the best linear algorithm, which uses function values at this point set. So, to motivate some questions, I will briefly introduce spherical cap discrepancy, which was already introduced. So, in this case, we have also the main D, the sphere. main D, the sphere, and mu will be sigma, the normalized area measure on the sphere. And as a class of functions, I will take the indicator functions of caps. So C D will be the family of caps, which are parameterized by the centers and the heights. These are just intersections of the sphere of half spaces. And in this case, if we write down the definition as before, we get some. We get something like this. So, this is somewhat weighted discrepancy. So, here the indicator function at c of xi returns one if the point xi is in the cap C. And we weight these contributions and sum them up and compare this to the area of the cap with respect to the surface area measure, so the integral. And now the spherical cap describes. And now the spherical cap discrepancy is obtained if you take equal weights here. So then you have the number of points divided by the total number of points, and this gives you then the normalized spherical cap discrepancy. And yes, for the spherical cap discrepancy, one can study some questions. So how good are optimal point sets? So the open problem of Hua mentioned consists of estimate Consists of estimating the minimal discrepancy, so finding at least asymptotically such equivalence for some function g n. And yes, this problem is still open and it's a hard problem. And in fact, the upper bound, which was obtained by Beck, can be also obtained using combinatorial discrepancy. So the dual shatter function lemma and the transference lemma adapted to this case. Adapted to this case. And one could also ask a similar question about the optimal weighted error. So if we take a general function class here and minimize all weights and the point sets and also look for an equivalence here. So in this case, there were some results obtained about Soble F spaces, for example, different kinds of spaces. Yes. Yes, but in general, for example, for circuit cap discrepancy, this is still an open problem. And also, how to find these near optimal points is not an easy question. And also, if there are weights involved, it may become harder or easier. This depends, and we'll see an instance later. And also, to find these optimal weights may be difficult, but Be difficult, but for example, if Fd is the unit ball of a reproducing kernel-Hilbert space, one has a nice representation of the error, and one can optimize this quite easily, for example. But instead of talking about optimal points, I will also focus on more general points and describe the motivation like this. So, I want to fantasize the quality of a point set for the The quality of a point set for the problem of numerical integration in terms of geometric quantities which are easy to analyze, easy to describe, for example. And why is this useful? So with this, one can obviously also study optimal point sets, because if one understands what geometric quantities are important for optimality, one can also look for the optimal point sets. But the original motivation. The original motivation David and I had was to look out for random points, so the quality of random points for numerical integration. This is known if we have equal weights, but for optimal weights this was unknown. The result I will present in a few moments. And yeah, this I will present after a brief. Sent after a brief excursion to circuit discrepancy again in order to motivate what I mean by geometric quantities and these worst case errors. So if we take this rather special kind of Sobolev space on the sphere, so the w2 of smoothness d plus 1 over 2, so these are slightly more than continuous functions. I don't give the definition here, I just use this as a motivation. And we consider As a motivation, and we consider the unit ball of this Sobolev space, then using reproducing Carnell-Hilbert space techniques, Johan Braucht and Josef Dick could show that for every finite point set we have the following. So this is the worst case error over the unit ball of the Soblar space of an equal weight algorithm. So we see this here, and it actually equals Equals up to a constant this geometric quantity. And this is related to the Stolasky invariance principle. And one can interpret this as an energy difference between continuous and discrete energy. And I want to mention that this gives a very nice geometric expression for the quality of the point set for numerical integration in this unit ball. So one can explicitly. So, one can explicitly compute this sum of distances and then check whether the point set is good for this problem or not. And with this motivation in mind, let me give you an overview of the result David and I obtained. So, in this case, we take D to be a bounded convex domain in R D and μ to be the And μ to be the Lebesgue measure on this domain. As a function class, we now take the unit ball of the usual solvus based on D. So these are the LP functions with big derivatives of order up to S, which are contained in L P. And if S is greater than D over P, which I will assume here, then we have continuous representatives and the function evaluation makes sense. Function evaluation makes sense. So if we consider a point set in this domain now, I want to introduce a geometric quantity which I will use to characterize the error in a moment. So the distance function is defined as follows. So you take a point x and you compute the distance to the closest point of p and with this distance function we can This distance function, we can state the following results. So, if p is between one and infinity, and one over p plus one over p star is equal to one, so p star is the conjugate index to p, and the smoothness is large enough, then we have for any finite point set with constants independent of this point set that the minimal error, if we optimize our all weights of linear algorithms. Weights of linear algorithms. So the optimal linear algorithm using the information at this point set of on the unit ball of the Sobolev space is asymptotically equal to a norm of the distance function. So if p is equal to 1, this is the s power of the supremum norm of the distance function, which some of you may know by the name. Know by the name covering radius. So, in fact, this is the minimal radius you can take such that the balls centered at the point set P cover the whole domain. And if p is greater than one, we obtain something which is smaller. So, often bounds were given in terms of the covering radius. But whenever you see such a bound, please think of this average distance because. average distance because in our case we managed to prove equivalence to this average distance which is the sp norm of the distance function to the power of s and in fact this is related to quantization theory so let me briefly talk about that and the quantization problem consists of quantizing a random vector x Using a random vector x, which is a distribution mu, and r will be some parameter. So we want to minimize among our all f which are measurable and have an image set of at most n elements the following error, this difference. And f is a random variable which takes only most n values. So this is the quantization of x. And this problem is, in fact, equivalent to the following problem. This is a result in quantization theory. So, if we minimize over all point sets which have at most endpoints, the following, that this distance integral, so the LR mu norm of the distance function to the R. These are two equivalent problems. Equivalent problems. So if we take μ to be the uniform measure on the convex domain, bounded convex domain, then we see that we can interpret theorem 2 in the following way, that the sp star quantization error, if we use such a point set P, in fact in quantization theory one is often concerned with optimal point sets, but one can do this for general point sets too. Do this for general points, it's two. And this error is in fact equivalent to the error of the optimal linear algorithm in this unit ball. And I also want to mention that this quantization problem is also related to a Wasserstein distance between the empirical measure, which is based on the points of P, and the uniform measure. But yeah, this will be also. But yeah, this will be also the topic of Stefan Steinerberger's talk, I think in about two and a half hours from now. So I'll refer to there for maybe more information on that. And I want to give an application now. Namely, our original motivation consisted in ascertaining the quality of random points for the integration problem. So if we have now So, if we have now I D uniform points on D and we take the random endpoint set consisting of the first nons, then we can write down the following results. So on average, this average distance is n to the minus 1 over d, which is optimal if the parameter is smaller than infinity and we have an additional log. And we have an additional logarithmic factor if the supremum norm is taken. So, this logarithmic factor was already known from research to Vetnikov and Saf, for example, can also be obtained via the coupon collector problem. And the upper part here, in fact, we deduced it from a result to the Pierre Corort from random limit. Yeah, a limit theorem for random quantizers, something like that. And with this in hand, one can prove that random points are asymptotically optimal for the numerical integration in the subo-left space WSP if p is greater than 1, if and only if p is greater than 1. So if you don't take weights, then it is known that random points are not optimal, but Not optimal, but before it was only known that you can at lose at most a logarithmic factor, and we close this gap using this characterization with the average distance. Okay, so let me state a few open questions to the end of my talk. So if you have, for example, the interval, unit interval, then we can write down Then we can write down the average distance as a sum of the spacings between the points, and we can have a discrete form which is easily computable in this sense. Because the distance integral may not be so easy to compute, so it's not an easy to compute quantity, but at least for random points, it's an easy to analyze quantity which. Analyze quantity, which we used to show the previous result. And the question would be: what are analogs of this in higher dimensions? So maybe you take the whole radii in some point set and take averages over them and you maybe have to be careful about the holes. I don't know, but this would be an interesting question, I think. Also, interesting would. Also, interesting would be to obtain a characterization as we did for the equal weight problem. So we used optimal weights, so we could choose our weights freely. And the question is, what happens if you take equal weights? What kind of characterizations can you get? And also, an interesting question, I think it's the relation of this distance function or Wasserstein distance in an Sostein distance in a sense I explained a bit before to discrepancy or weighted discrepancy or related concepts. So with this I want to close my talk and thank you for your attention. Thank you very much, Matthias, for this talk. I find it very interesting. And of course, the floor is open for questions. And of course, the floor is open for questions. Now, again, I'll remind you: please either use the chat or raise your hands if you want to ask questions. So I would again start with one of my questions. Do you have any idea regarding the answers to your open questions? Is there a discrete version in a higher dimension? What do you think? I mean, in our proof, we obtained a somewhat discrete version, but it was quite adapted to the point set and not very nice. So I don't know if there's a nice description in terms of largest holes maybe amidst the point sets. I thought about it, but I haven't come up with a solution yet. So there is a question by Sasho. There is a question by Sasho. Sasha, do you want to ask it yourself? Sure. Right, so my question is: if you have a sense whether for random point sets, what do optimal weights look like? Are they very far from uniform in some average sense? Yeah, we do not quite understand how the weights look like. So they are obtained quite indirectly. Quite indirectly, so we haven't analyzed it in detail, but it would certainly be interesting if they are around about equal weighted. But it depends very much on the point set, I guess. Are there more questions? I guess to clarify, I was thinking that for random point sets, you could also try computationally and see what you can. Maybe there can be some. Maybe there can be something done here. So, are there more questions for Matthias? I don't see any questions, so thank you very much again, Matthias, for your very nice talk. And I'll stop the recording. And I'll stop the recording of your talk now.