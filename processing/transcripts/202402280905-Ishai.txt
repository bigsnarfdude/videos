I won't take any more of your time since already starting a little late. Thanks for being here. Yeah, and thanks for inviting me. It's really a great pleasure. Okay, so I'll talk about cryptography from planted graphs. This is joint work with Damiano, Amos, Real, and Peron. And I guess this talk is about really incident, I would say, accidental discovery of new connections, simple new connections between cryptography and planted draft problems. Problems. It feels a bit like being in Russia. You ask the wrong questions, a few months later, you find yourself almost freezing to death in a place with minus 24 degrees. But anyway, I think beyond the fact that there are really nice new cryptographic applications of statistical inference problems, I think a major takeaway from this talk is a bunch of Is a bunch of uh you know open questions that I hope you guys will be interested in. Okay, so a bit more about the talk. I will show that seemingly conservative variants of standard conjectures about the hardness of the planted click problem give rise to new cryptographic applications in the domains of secret sharing and secure computation. Of secret sharing and secure computation. I'll explain what these terms are. And specifically, we show that if we allow additional public information, then we can get levels of succinctness that were not possible before, were not known before under any assumption. And once we look more closely into these applications and try to pinpoint the exact types of planet problems they give rise to, then we get something that's much Then we get something that's much more significantly different than the types of assumptions. These are still planted rough assumptions, but very differently flavor than the standard ones that this community studies. And here we have a two-way connection. So in a sense, you cannot escape these types of assumptions if you want to nail down the best complexity for the primitives that we will discuss. And I would like to say that, you know, surprisingly, there hasn't been that. Surprisingly, there hasn't been that much use of hardness of statistical inference problems in cryptography. And previous applications are very different in flavor from the ones I will present here. They're mostly about diversifying assumptions. For instance, recent work of Android provision alone shows how to get a public key encryption from a combination, from an assumption that includes some hardware. From an assumption that includes some hardness of a planted hypergraph problem. And there is a work in the context of planting backdoors in ML models by Vinod and others that is very different. It's not a traditional crypto task. It's more about helping the bad guys than the good guys. And it's more specialized in nature. Here we have really mainstream crypto free meetings. Okay, by the way, I'll try to make this talk as self-contained as possible, but of course As self-contained as possible, but of course, please stop me if anything I say is not clear, if you're assuming too much about the decline. Okay, cool. So let's start with a really basic question in cryptography, threshold secret sharing. Throughout this talk, a secret will be just one bit. And here we talk about the 2 out of n threshold case, which means that we want to split S into N bit strings called shares. Strings called shares with the requirement that a single share hides the secret. You could think of it as being independent of the secret. And if you combine two shares, then you can reconstruct the secret. Okay, this is a very, you know, like a bread and butter of many cryptographic applications. Okay, and I guess that most of you know about Shamir secretarian scheme where we use a random line to solve this problem and this. Problem, and this solution uses shares of size roughly log n. And there are some simple lower bounds that show that log n is the optimal threshold, at least in this case. So Andre has worked that it's far less trivial for some higher threshold, which is very close to n. But in this case, the lower bound is pretty simple, and it shows that log n is the right answer. Okay, so no, there's no constant in front of the lower bound. No, there's no constant in front of the log or there is no just additive, just maybe one plus minus one, like things related to MDS conjecture and things like that. Okay, so now, okay, so problem is solved. Well, the new question is what about computational hiding, right? If we settle for computational indistinguishability between the two possible secrets, and here, And here, throughout the talk, my default notion of computational hiding says that any polynomial time distinguisher that attempts to distinguish between s equals 0 to s equals 1 only get a little of one advantage. Okay, so in the terminology of Alex's talk for Monday, this would be hardness of the weak detection problem. Okay, so this is the notion of this. Okay, so this is the notion of distinctibility. And later in the talk, this is no, I will talk later about strengthening this in both directions, but this will be the default notion of security. And also, all of the firework that I describe will be with respect to this notion of indistinguishability. Any questions about the basic goal? Does it make sense to you? Does this question make sense? Okay, so it makes sense in general, but you're specifically. It makes sense in general, but here specifically doesn't make so much sense because if the share size is logarithmic, then we cannot really expect a computational statistical gap. You can just in polynomial time get a sufficiently good approximation of the distribution of each share under both possible secrets and use this to distinguish. Okay, so we really have no hope to improve it even in the computational setting unless we make one extra reduction. Unless we make one extra relaxation. And this extra relaxation is adding public information. What do I mean by public information? When you generate these secret shares, you together jointly generate another share, S0, which is public information. It's up in the slide for everyone to see. So reconstruction can use this public information. So if you have two private shares and the public share, you can reconstruct the secret. And of course, we want secrecy to hold even We want secrecy to hold even given this public information. So the joint distribution of S0 and any S sub i under secret 0 should be indistinguishable from the joint distribution, the same joint distribution under secret 1. Okay, so this is the new notion. Why is this notion interesting or motivated beyond being natural? So first of all, public information is a cheaper resource. You don't need to work to securely store it. You don't need to work to securely store it, you know, it can be posted online. Perhaps more importantly, you can generate and distribute it, post it, even before you know which parties will be involved. Right? And practical contexts like so-called pre-processing SNARKs in cryptography, where users download some public information, some big common reference ring to be later used to get very succinct proofs. Okay, so this is. Proofs. Okay, so this is something that's actually been deployed. And also, even though at this point we don't have efficient, like practically efficient solutions for this, but using high-end crypto, you can generically compress and reuse public information so that the amortized cost in terms of communication can go down to zero. Okay, so these are reasons why we care about crypto with public information and specifically secret sharing with public information. Specifically, secret sharing with public information. Maybe you said it already, but even with public information, the login robot calls for collection okay. Exactly, good point, great point, yeah. So in the information stored at extending, public information is not helpful. And perhaps one reason, I mean, this project was inspired, in fact, with previous work with others that will mention soon, about computational secret sharing, where we did not look at this simple question. Take a look at this simple question. We looked at very complicated access structures that we're trying to save, and some of the techniques involve public information. But normally, I think the community did not look at this question just because computational secret sharing didn't receive the attention it deserved until recently. So I think this is the main reason. And then the combination with public information just wasn't looked at. Again, for us, it looked like let's get this out of the way, show it impossible. Our initial Out of the way, show it impossible. Our initial hypothesis was that the same lower bound for the information theoretic setting would apply also for computational secretaries with public information, but this turned out not to be true. Yeah. Is to say what, like, when this kind of secret sharing comes up, just that you know that you can Yeah, I mean like in the like in which applications this is yeah. I mean the classical example is if you have a code for a nuclear bomb or Code for a nuclear bomb, or whatever, you don't want a single person, an irresponsible person, to use it. Maybe this is less meaningful for one-bit secret. At least, we hope the code is more than one bit, but yeah, there are many applications in secure computation and many other codes, it's distributed strongly and so on. Okay, cool. So, what can we say if we allow public information? Unfortunately, or fortunately, depending on the point of view, not much. The best lower bound we were able to show. The best lower bound we were able to show is a double lower bound. And this basically, you know, hiding some details, this essentially the argument is: okay, if you have a solution for n parties, then in particular it gives you a solution for the first square root log n parties, square root log n parties, then the public information will be logarithmic, in which case the brute force approach uh says that there is no comput uh statistical computational gap. So basically scaling down the problem. Basically, scaling down the problem. And why does the public information make it smaller? You guys can't. Okay, because without loss of generality, we'll get back to this later. The public information can just include for every pair of possible shares whether the secret is 0 or 1. So if you have a square root of log n parties, the public information without loss of generality is just log n. Okay? And this is the first open question in the talk and one of the biggest ones. It's very embarrassing, but we really don't know. Embarrassing, but we really don't know. We have this exponential gap for this very simple problem. Yeah, and I think that unlike many of the questions we face in our research, here we don't really have a guess where the true answer is. But that's what makes it interesting, I think. Maybe you guys have much better intuition that can help us get an informed guess. So, later in the talk, I will translate this to a question. Later in the talk, I will translate this to a question about planted graphs, but for now, let me do what we often do when we can solve a simple problem. We make it more complicated. So, let me give a more complicated version of the problem, or a more general version of the problem. It's also quite clear. And this is called forbidden graph secret sharing for historical reasons. I'll do my best to explain the term forbidden by using the word forbidden as much as possible in the following. So, basically, here we have. So basically, here we have a graph that represents. So, as before, a single share should not reveal the secret, but now we have a graph Q that tells us which are the edges of the graph are the qualified pairs. These pairs of shares should reconstruct the secret. And all the non-edges of the graph are forbidden pairs. Now, why do we not just call it graph secretariat? Because graph secret sharing refers to a stronger primitive where even bigger cells. Where even bigger sets are forbidden if they do not contain an edge. So it's sort of like the graph specifies the forbidden non-edge, the non-edges of the graph specify the forbidden sets of size two, but we make no requirements about the bigger sets. It's a promised version. So you're saying here, like, I'm allowed to have a set of size three regardless of what you measure. Exactly. So so a set of size three, I make no requirement whatsoever. A set of size one should not re learn the secret. Not learn the secret and a set of size two should learn the secret if and only if it's an edge in the graph. Okay, so this is again a problem, a simple promise, maybe a scaled-down version of the general question about secret theory for general access structures. There's been quite interesting recent developments. So, in the information theoretic setting, the naive solution has n bits per share. Then there was an intermediate phase where we knew how to get this down to Phase where we knew how to get this down to square root 10, and then a breakthrough result of Vinod with Tian Ren and Hotek. It showed, and this interestingly, it builds on the techniques from state-of-the-art locally decodable codes that we heard about from Frau Besh yesterday. The share size is somewhere between polylog and polynomial by 2 to the square root log m. This is the state of the R2D information, it's very exciting. In the computer, In the computational setting, this recent work that I mentioned with Vinod and others shows that if you assume a sub-exponentially secure one-way functions, we can get the share size to polylog. And again, all these statements are for general graphs. Okay, and the best lower bounds, as in the threshold case, are just logarithmic log and this is the best lower bound we have. And it's not clear that the lower bound is tight. In the setting room, yeah. Okay, so we again ask the question: can we improve polylog to log if we settle for computational security and allow public information? And here there is, in fact, a very natural approach to build it by using blended click style assumptions. Okay, so throughout the talk, little anything. Little n is a number of parties or shares, and big n is a bigger quantity that represents some ambient size, number of notes in some ambient graph. So we pick a standard Erdos-Reni ambient graph G. And now the shares will just be pointers to little n random nodes of G. So these are the secret shares, each of them of size log big n. Okay, now we need to decide which graph we want to plant. And we'll make this decision depending on the value of the secret. So in one case, we'll plant an anti-click, in another case, in the other case, we'll plant this access structure Q. Okay, and now if you're unauthorized, if a pair is forbidden, right, it's a non-edge in the graph, then in both cases, the corresponding pointers will not be connected in G. Connected in G, right in a planted well. Wait, but sorry, from the paper is not clear. So it's a pointer to a vertex or a point to vertex? It's a pointer to another. Yeah, to a single vertex. Right, and now because I just toggle the edges that are qualified, your only way of noticing a difference, if the graph is indeed hidden, this is the crucial assumption here, then your only way of learning. And your only way of learning distinction between the two cases is if you're qualified. Okay, because unqualified pair, their pointers will always be disconnected in both cases. And if you're qualified pair, then in one case they will be connected, in the other case they'll be disconnected. Okay, so if we want to argue that this is indeed computationally hiding, we need to make some assumptions. You call log n here is the information theoretic lower bound to log to n? So log 2n is the information theoretical. I mean the threshold is a special case, so yes. But also for other cases, like a hypercube, yeah, but log n is the best known lower bound. I mean log lo lo log n is is, yeah, it's the best known lower bound. There are no super log lower bounds for any axis factor. Lower bounds for any axis factor, for any forbidden graph axis top. Okay, so this has the flavor of a planted click assumption, but it's different in two ways. So let me start by reviewing my formulation of standard planted click assumptions. Again, there are many of them, but the one that's convenient here is to consider planting a so this is really a standard formulation of the planted clicos. Really, a standard formulation of the planted click assumption, except that we assume here that when the ambient graph is more than quadratically bigger than the planted click size, then you have this even weak detection is hard. So every polytime distinguisher between the case where we did plant a click versus we did not plant a click will have a little of one advantage. This is a conservative assumption by the standards of disk query lists. This is a standard plant. This is a standard line of jig style assumption. And we need to modify here in two ways. First of all, you know, in this case, we plant a general graph queue. Okay, so we need to replace a click by some fixed publicly known graph. Okay, intuitively, click is the easiest graph to detect. Okay, I'll talk more about this later. So this seems legit. So, this seems legit. The second change is that if you're a forbidden pair, you still learn as a hint the image of the two nodes of H in the graph, where they lie in G. So you basically get as a hint a pair of nodes of G that are inside the planted graph H. Okay, so this is a form. Okay, so this is a form of leakage, and a broad study of leakage in planting problems was made in some work by Brennan and Guy Bressler. And, you know, the specific combination of these two, we don't think it was covered by prior works. I'll say more about what we know about this assumption. But this is the informal version of the assumption. We change the standard planet click assumption in these two orthogonal ways. Okay, and a bit more formally, we call it the weak. And a bit more formally, we call it the weak PSH assumption that subgraph with hints. And the formal version of the assumption says the following: fix any two nodes, these correspond to a potential forbidden pair. That assumption, in fact, is not restricted to a non-edge of the graph of H. So pick any two nodes, and then you cannot distinguish between planting, yeah, this is for every graph H, between planting H. graph H between planting H in a random graph and the actual location of I and G in H versus just a random graph with a random edge or non-edge in G, right? Where the choice between edge and non-edge depends on whether I and J are in edge or non-edge of H. Okay, so you kind of give this leakage and you can generalize it to bigger hints and then you plant a subgraph. And then you plant a subgraph. If the hint is bigger, here it's just edge, not edge. Otherwise, you just plant the subgraph induced by the notes you leave. Why do you need leakage in your vertices when you precisely for one part? No, no, it's also two parties, right? That are non-edge. This is corrupting. I mean, that is the non-edge. Yeah. But more generally, you can generalize it to any insight, and the general formulation is slightly more involved. You need to plan. Is slightly more evolved, you need to plant the subrough industry the set of vertices that you need. Is there always a constant size? So, yeah, I'll say soon about this. We wish you could handle a bigger size, but up to log, it seems fine. Okay, so what can okay, so just to recall when I say indistinguishable, this is a default notion, polytime distinguisher, a little over advantage. What you can actually conjecture and is consistent with the evidence we have. With the evidence we have is that we get security against quasi-poly like n to the little of n-time distinguisher, and the advantage is some inverse polynomial that depends on the epsilon, right? This is basically the strongest possible conjecture. Well, the choice of G and half is arbitrary, right? And you could make the edge density. Yeah, you mean that you don't need to pick a random graph, this G? Yeah, excellent question. Yeah, the strongest possible. Also, this is the strongest plausible thing if you want it to work for every end node H, but it could be that some, that many H's, like bleak should be really easy compared to many H's. I think so. Maybe for some H's, you could do much better than end of the logo. Excellent. We'll get to this. But for the end to the log N, I think that most H will have a unique footprint once you look at the a set of ten log N uh purposes, right? So okay, if you're leaking up to yeah, up to log size, okay. A log besides. Can you just want to clarify for any plant edge, like for the non-edges in the patch? No, no, the non-edges of the edge will be non-edges. So they never touch edge. Okay. So I guess the empty is you have to click it. Yes, sorry. I mean that was right. That's awful. That should also be okay. So again, we were able to prove. So again, we we were we were able to prove, you know, to validate this uh conjecture with respect to low degree hardness, you know, even when the hint size is up to uh you know little of log m. And this is actually useful. The bigger hint is motivated by getting, you know, some progress in the direction of the strict graph secreturing where even bigger sets up to logarithmic size, if they don't contain an edge, then they cannot reconstruct the secret. This is the motivation. Construct the secret. This is the motivation for considering bigger hints, but beyond log n, then the problem becomes easy. Yes. Something probably. So in the construction I described before, like the two graphs you considered where, you know, in one case you plant a H, in the other case you plant an NT. Why does this not show up in your conjecture at least? So the assumption is for. So the assumption is for every graph, so this includes an entry click is a special case. So it's basically saying that you cannot distinguish from. Okay, so if both of them are distinguishable from gene-half, then you can distinguish. Okay, so again, we were able to validate this with respect to low-degree tests, even again when the little insize is up to logarithmic, and this is useful. And if you believe in this low-degree conjecture of cells, This low-degree conjecture of SEM, then you believe in this assumption. And regardless of whether you believe in the conjecture, the weak piece, the assumption, implies in the context of the application share size 2 plus epsilon log n. Okay, and a recurring theme in this talk is: can we do better? Okay. Yeah, question? Yeah. So this is quasi-collibropin, right? This is quasi-collibropid. So, if you work with higher edge densities, could you afford slow tension? Good question. The issue is that we need at least in one cases to plant a low edge density, but yeah, I'll get to it. There are enough degrees of freedom for this question to make sense. I'll go to some more aggressive general planting versions, but for now we're in the benign planting in Erdogan of Virginia. Oh, let's see, it's probably this. Uh can you go below if you wanted to do below log n for the two out of n threshold, right? Yeah, you could ask the same question. Yeah, you could ask the same question, but there, you know, the question is hard enough for two out of n. So now the aim is to get below two log n down to log n. Okay, this is what we're trying to do now. And for this, the idea is let's not play. the idea is let's not plan let's not plant some you know very you know dense or sparse graph like an independent set in the previous example let's try to plant a random graph what does it mean to plant a random graph inside a random graph and again you're not changing the densities you can think of it as just picking a set of little n-nodes in the in the big graph and you know considering the in the subgraph as a planted graph but now I want to But now I want to use this random planted subgraph R to mask the secret and release some more public information. So now if you think of these graphs as like n by n adjacency matrix, you can think of the secret times this axis structure graph, right, viewed as a matrix, plus, this is plus mod 2, you know, the incidence matrix of this random graph. Graph. Okay, and now the intuition is that if this R, right, that is used to mask the secret were perfectly independent of the thing on the left, then we would get perfect hiding because, you know, we're basically toggling the edges of the graph Q. Sorry, we're toggling the edges of R only. Edges of R only in the places that correspond to edges of Q. So changing the secret tobacco the edges of R only in places that correspond to edges of Q. And this means that you can easily see if you're a qualified pair, you can easily reconstruct the secret. And assuming that R looks totally random and independent of this uh planted you know planting experiment on the left, Planting experiment on the left, then you get perfect hiding. Q is known. Yes, Q is known. But if R is random, then this is enough to make everything secure. R is known as well. R is known as well. Well, I'm saying if the real R is not known, it's hidden, yeah, then. Okay, so we really want R to be computationally hidden, and then we pretend as if the R on the right. If the R on the right was chosen freshly from the R on the left. And this is good enough for security, right? This is a standard critical type hybrid argument. Publish S times Q, Q plus R, right? So I know that R is either one or another, right? S is a bit. Yes. So given the public information, I know one or two possibilities. But the difference with respect to the planted click is. The difference with respect to the planted click is that these two options will be typical graphs and not extreme graphs. This is the difference. So from the point of view of the kind of statistics problem, you're saying first we kind of both fix a random crap, and then you're asking me to detect whether I've planted that inside of another. Okay, but the kind of inner ones, no. Is it fixed? Yes. So it's really, you know. Yeah, so so so it's it's really you know uh uh avoiding the extreme cases where where you know uh getting quadratic improvement in the standard deviation species. If you use the previous scheme, but with Q being a typical graph, you might have also hoped for the independent set of one pole. You would need sort of Q versus a dependent set. So even if Q were a typical graph, you wouldn't be committing. And now you have that both cases are typical graphs, regardless of what Yeah. Yeah, so this is the formulation of the assumption. We call it the random PSH assumption. And again, there is also this hint, which is an orthogonal thing. But the main point is that we cannot distinguish between a planted random graph in the actual random graph versus just an independent random graph. And this is something we did not succeed. we were we did not succeed in in establishing low degree hardness and then Andre and authors managed to prove it so this is also now validated with respect to low degree tests okay so out low degree uh log uh maybe I think log square log square like if it's low square but not more because I guess you need Because I guess for arbitrary entirely you're given age, right? So you can always, there's always a distinguisher that checks for the presence of age somewhere. That has degree one squared. Oh, I see. But if you wanted to have more security, you could just pick, like, I mean, presumably, you could just have kind of a size. I mean, presumably we just have that little bigger and be bigger. I mean, right, like planted dense upgraph is supposed to be much harder than planted clean. So I think maybe I should have highlighted the difference. So previously the big graph was more than quadratically bigger than the small graph. Now it's just one plus. So the difference is that here you're given age. But in that subgraph, you're not given the subgraphics. Any subgraph that has this age. The dense subgraph is harder and sparser random numbers. But I think the end point is that once you know the graph that's planted, if you look at a log n-sized subgraph of that guy, then it's going to have sort of a unique fingerprint in the automobile without trading. Okay, fair. Yeah, so the conclusion is that if you believe in this random PSH assumption, then we get the share size down to Then we get the share size down to 1 plus epsilon lobby. So almost like the threshold case. Okay, cool. And again, a recurring theme is can we do even better? Okay, as I said, we have the already know how we can prove this log log. And here again we know, okay, so let's go to the threshold case. Okay, because again I Okay, because again this is the simplest case and it gives rise to the simplest formulation of the open question. And now let me attempt to give an equivalent and quite clean reformulation of this question in terms of planted crops. So let's assume that each share is from a domain of size k where k is much smaller than n, the number of shares. This is what we're trying to get. We're trying to beat the log n per net. Okay, now you can look at this reconstruction. This is what I alluded to earlier when I explained the lower bound, the log-log lower bound. You can basically just consider the reconstruction algorithm, which is defined by the public information, right? So the public information can define a different reconstruction algorithm every time. You can think of it as a K by M multipartite graph where each part represents the K. Part represents the k possible shares of one of the parties. And an edge says that attempting to reconstruct by using these two specific shares will yield secret one. And a non-edge means secretly zero. Can you sorry, can you just like draw so this is a like S1 S2 S1. S2, S3. And let's say that there are five possible shares. The shared domain is of size 5. Then, you know, this edge means that if these two, the second and third parties attempt to reconstruct with these shares, then the secret is one. And a non-edge means that the secret is zero. Okay, and we know that this graph, it's a distribution over multipartite graph, but each instance in the support of this distribution has the property that when you have a sharing, a valid sharing of s equals one, then the graph must have a full multi-party click, right? So it means that you need to pick. These correspond to the actual shares in the support of. To the actual shares in the support of S equals 1, and they need to form a multipartite clique so that every pair of shares will lead to the right secret. And similarly, every valid shares, set of shares in the support of secret zero correspond to an entry-click in this graph, like a full entry-click in this multi-party graph. Everybody can write down this graph. Yes. Yes, I mean reconstruction algorithm is efficient, so this is you can do it for a longer time. And now we can have this equivalent reformulation of the problem. We basically need a joint distribution over some ambient graph G. A full multipartite click in G, where G is a K by N multipartite graph. C is a full click containing one. You know, click containing one note from each part, and A is a full anti-clicking G. Okay, so G is a K by N multipartite graph, C is a click, you know, and the critical hiding property is that if you give me G together with a random node in the click, I cannot distinguish it from the same G together with the random node in the entry click. So in a sense, we're trying to hide both a click and an entry click in the same click. In the same graph, in a way that you cannot tell whether or not it is in the click or in the entity. Okay, it sounds a bit weird. I will say a bit more about the weirdness, but before talking about the weirdness, we can in fact use a random partitioning to switch from multipartite graphs to general graphs, right? Because a random partition, when you're less, you know, when k is less than n, then a random partition will be. Than n, then a random partition will basically put make things multipartite. So just forget about multipartite plots for now and think about the following formulation. We pick an ambient G, possibly from an exotic distribution, together with a click and an anti-click, and we want these two cases to be indistinguishable. Okay, and the crucial point here: to beat the log n bound, we need Bound, we need the size of G to be little over n squared, where n is the size of the clicking on the key. Well, again, we're back to the 2 out of n case, right? We're back to the 2 out of n. That's why it's $16. Okay, so this looks like a pretty clean question about planting graphs, and let's see a little And let's see a little bit. So, I'd like to argue this is very different from the types of planting problems like the previous one, like planting in an Erdoganic graph. And really, the main difference is that we can have an exotic distribution from the ambient graph and also jointly pick the click and the anti-click together with the tools of the graph. And let me explain why this is a big deal. First of all, when you allow arbitrary ambient disputes, When you allow arbitrary ambient distributions, it makes hiding easier. Why? For one thing, you can have a higher variance for the degrees, right? So you can, you know, one reason why a big click is easy to detect is just that the effect on the degree is noticeable. Now, you have plenty of graph models, and some of them have a high degree variance, right? If the standard deviation of the degree is linear, then even a linear size, you know, a smaller linear size click or anti-click will be locally undetected. Per entrance leak will get locally undetected. So it's not like you can take the node and look at the degree and decide whether it's part of GT equal to NTTL. And in fact, we rely on the strong NP hardness results for approximating max click, we know that we can efficiently generate a pair of indistinguishable distributions over graphs such that one of them will always have a very big click and the other will only have small clicks. Will only have small clips, right, with a very big gap. So we know that we can do strange things if we allow arbitrary ambient distributions. Unfortunately, none of these directly helps us in the sense that if you look, at least I ran by Hurifeg one candidate construction which he broke, but he still was not sure whether this is. Still was not sure whether this is a general phenomenon. So, in this graph, you have to ensure that the G is drawn from a distribution so that there is no way to significantly larger the, like, you know, make the click bigger or make the anti-click bigger, right? Like, if your graph is such that randomly it occurs, that I can grow C in compared to the graph. Yeah, so excellent question, and I will address it soon. So, another fundamental difference is that. Difference is that here there is no natural search version because, again, you can plant tons of destructors, right? It's really just about, you know, okay, so I generate it together with one click and one ante click. There could be many others that aim to confuse the distinguisher, right? Including bigger ones or different ones. And in fact, you know, every even though a click and an enter click and only have one node in the intersection, you can easily build graphs. You can easily build graphs, even small graphs, where every node is both part of a big click and a big anti-click. In fact, this is implicit in a previous work on the weak notion of secretary, and this is a graph with only four end nodes, where every node is part of both a click of size n and an anti-click of size n. Okay, so so again, this will not give us a solution because the the distributions are not what Are not what they should, but it does suggest that there is a lot of room for doing things of the type that Prabhesh was mentioning. Yeah, I can ask you offline, but there are semi-random versions of the TIC problem which had fit algorithms. And yeah, my guess for what an algorithm interesting algorithmic attack here would be, would be the algorithm for the semi-random versions of Kitchen Crumb. Okay, but but but okay, but one one let me say the final difference is that usually we need an exponential gap between the big graph and the planted graph to be in the impossible regime, right, where you get statistical hiding. Here the gap is only quadratic. And if you look at the you reframe Shamir signature and scheme in this framework, here you can get a perfect hiding, right? So you can think of here the clicks, here the graph is actually fixed. You don't change the graph. Fixed. You don't change the graph. The distribution is only over the click and anti-click. And really, the click will be a random line passing through the origin, the anti-click will be a random line passing through some other point. And you cannot distinguish between perfectly, it's perfectly the same distribution, a random note from a random click versus a random node from a random node. Would your log log and lower bound from before say that you can't make n as small as n types? Okay, yes, right. Yes. Okay, so this is another fundamental difference. Yes, so so so it's really some kind of a meeting point between the information theory type lower bounds and the statistical inference, again, which is a recurring theme, of course, of statistical inference, but this is another. Inference, but this is another domain we've discovered. And again, my main point is that this is a very different beast than the standard planting process. Okay, can we do even better? Going all the way. What is going all the way? One bit per part, one bit per share. I think. This is all the way. Trust me, there will be no more touches in that. Exactly, yeah. So we can't do better than low-growth threshold. Better than novel for threshold or non-trivial forbidden graph, but we can consider general access structures where you have a general collection of qualified sets that can be constructed secret, and the other sets are forbidden, right? So, this is like the strict notion of secret turing, where every set is either qualified or forbidden. So, the question is: can we have statistical computational gaps for some axis structure, right? Some access factor, right? Not the graph, not forbidden graph or threshold for some access factor with only one big shares? Can we improve over the state of the art with just one big shares? And the answer is no, that if you restrict yourself to one big, one big shares, then even if you relax the problem of secret sharing by settling for computational hiding and by allowing public information, you cannot do more. In fact, this is the most technically involved result from our work. The result from our work. And the proof goes via some new characterization of the type of access structures that can be realized. These are called perfect binary ideals. So with one bit shares when the secret is one bit. So we have this characterization. And then you can show that there is a certain combinatorial condition that is met, necessarily met, which gives rise to an effusion protection. Okay? So this is what we can prove on the negative side for general acceptance. Negative side for general structures: that if you insist on one bit shares, then there are no statistical computational paths. Gaps with two bit shares, we have no idea. Okay, so we don't know where for general, we know that the gaps can be, for the threshold case, so the graph case, the gaps can be, you know, have to be at least with log-log n share size. For general access structures, they could go as low as two bits, but not one bit. Okay, so let me briefly discuss this second application domain I mentioned, which is secure computation. Again, it's technically similar and even simpler in a sense, the application. In the general problem of secure computation, you know, it's like standard communication complexity. You want to compute some functional distributed inputs, except that you want the interaction to reveal nothing else beyond the output. Okay, so here, Ellis and Bob, this is a classical millionaire's problem, they want to know who has more money. Problem, they want to know who has more money without revealing any more information about how much money they have to each other beyond just the final bit. Okay, it's a very central problem in cryptography. And here we restrict ourselves to computing functions where x and y are from a domain of size n and the output is a single peak. Okay? So they have n possible values of how much money they have. Okay. Okay, and a simple model for secure computation is a so-called private simultaneous messages or PSM model. Here we have some external referee who should learn the output and only the output, and the protocol proceeds by just simultaneously sending messages to the referee. As I described the problem so far, this seems clearly impossible because, you know, to get the correctness, there'd better be some dependence between the input and the message. So to make this possible, So to make this possible, we assume that Alice and Bob share some common source of secret randomness which is unknown to the referee. Okay, and now the problem becomes possible to solve and the main goal is to minimize the length of the messages. Okay, what do we know? This is a morally harder problem than the secret sharing problem. Now, in particular, to give you just one indication, here there is a trivial low-hill lower bound just from communication. Log and lower bound just for communication complexity. So, even without sequencing, there is a trivial log and lower bound, which we don't have for the secreturing problem. In the information theoretic setting, the best upper bound we have is the square root of n, and the best lower bounds are log n, depending on whether you allow error or don't allow error, but it's slightly bigger by constant factor than the trivial log n lower bound. This is the best lower bounds we have. Low ones we have. And here we consider, as usual, the relaxation to computational security or computational hiding with public information. As I said, log n is the best we can hope for. And the question now, now the one plus epsilon log n, unlike the sequenturing problem, now it will be the latest optimum. Okay, so can we get it? And it turns out that we can, using similar ideas to the sequence sharing, in fact, here it. To the sequence sharing. In fact, here it's even a bit more conceptually straightforward. So you express the truth table of your function as a bipartisan graph, right? So this basically, the edges connect two inputs where f evaluates to one. Now you hide, you plant this bipartite graph in an Erdos-Reni graph and the secret randomness common to Alice and Bob. Randomness common to Alice and Bob and unknown to the referee will include pointers to the nodes of H. Then we don't care so much about the size of the secret randomness. And then the messages will just be, each party will send the node of G that corresponds to the input. Okay, and now under the previous weak psh assumption that I stated before, we get 2 plus epsilon log n message size. Log n message size. Right, this means that the ambient graph is more than slightly more than quadratically bigger than the planted graph. And here, we cannot do the same as we did. Here, really, unlike the sequence sharing problem, in the sequence-sharing problem, we were able to plant a random graph. And here, we cannot avoid planting the graph of f just because you can reconstruct the value of f from the two messages. In the secret sharing problem, Messages. In the secret sharing problem, given two shares, you cannot tell whether this pair is qualified or not. Okay? But now, given two messages, you can tell whether they form an edge in the graph corresponding to F. So this is a fundamental difference, but still we can randomize the graph by adding internal edges. For moving from bipartite to general graph, we're moving random internal edges in each part. And now we can argue that each. And now we can argue that if the function is typical, right, so if f is very dense or very sparse, this construction will be broken unless the ambient graph is quadratically bigger. But for a typical f, this is a slightly stronger version than the previous random PSH assumption, because now we say that even if the distinguisher can depend on f, even if you have a limited, basically unbounded computation power depending on f, you still cannot. Depending on f, you still cannot find a planted random f in a graph which is always slightly bigger. Wasn't the best, the best, the best upper bound if you wanted suggestible hiding was like polyn. Square root n? And was something known? You're talking about getting the medium two log n versus one's epsilon login, but it seems like it's already a huge improvement over the root n. Yeah, but they want to get close to the login log log, right? Want to get close to the log n lower bound, right? So the significance of improving two log n to one plus second log n is that the lower bound is log n. No, I understand, but what's the name of the name? Yeah, even the two log n, even the two log n is Eugene Foucault, yes. No, and then there was, there was no, before this, it wasn't known how to do anything better than the root n, even if I have a lapse of computation. No, no, you there is a cheating solution. So if you use a sub- if you use a an encryption scheme, you can just publish as public information some encrypted information and then let the parties uh give them secret keys. Give them secret keys that enables decryption. So, there is some trivial way of compiling existing PSM protocols into ones with polylog L message. So, the improvement really is from polylog L to log L. This is the improvement. Out of logs was not log. Yeah, even OSLog was not log. Yeah, just polylog was known. And even there for polylog, the assumption is incomparable. So, there you need sub-exponential secure one-off functions, and here you only need to assume a limited bonus. But I would say that the sub-exponential secure. But I would say that the sub-expunctions, if you're one of the functions, it's still a conservative assumption. Yeah, so basically, just again, I'm running out of time, but just to mention, you know, if you consider a practical-looking scenario of secure computation, then typically we have an offline phase that is executed before the inputs are known. And in this offline phase, Alice and Bob can generate both the public information and the secret information. The public information and the secret information, and post the public information online. This can be done much before they know the inputs. And then, when they know their inputs, only then they just need this, send these very short messages. So, it's non-interactive, near-optimal communication, the best you can hope for in this offline-only setting. So, really, planted rough assumptions give you a near-optimal protocol in a natural offline online setting for. Online setting for signal computation. Yeah, and again, the assumption this typical PSH is a strengthening of this round of PSH from before, but this was also validated by Andre Dahl. Again, the technical difference is just whether you allow the distinguisher to depend on the graph or the distinguisher is given the random graph as an input, right? We consider non-uniform distinguishers. Yeah, so to recap, for the secret sharing, this is the scale of different share sizes. You know, this is a result I did not mention. This is the headline result from the paper with Vilotal. We show that under the sub-exponential RSA assumption, you can in fact get, without public information, but with computational hiding, you can get polylog share size for every axis structure. Okay, but these approaches cannot give you log share size. Log share size. The best you can hope for is polylog. We showed impossibility. If you're super greedy and want to one big share, you don't have any gaps between statistical and computational hiding for secret sharing. For fully specified access structures, when you allow promise access structures, then it's different. With log log n, we don't have threshold two out of n schemes. On the positive side, if you're On the positive side, if you assume this weak PSH assumption, fairly conservative, I would say, like basically saying that click is the hardest graph to detect, then we get two log n, and if you make the stronger random PSH assumption, then you get log n, essentially, one plus epsilon number. For the PSM problem, now we look at message size. It's impossible below log n. You can use this trivial encrypted Use this trivial encrypted encryption using symmetric encryption, this simple solution to get polylog n share size assuming sub-exponents one function. We showed how to use weak PSH to get 2 plus epsilon log n and typical PSH to get 1 plus epsilon log n, which is here close to optimum. Okay, so there are tons of open questions. I would just mention them briefly. So there is this exponential gap for 12 n sequitures. Gap for 2-auto-grand sequenturing, which is really related to these exotic plant work problems. What do you believe would be the answer? Yeah, so I said I don't have, my weak belief is that the upper, the low, you can't get log below, log n, but I think, yeah, this would probably be difficult to prove, but yeah, this might be a trucking problem. Yeah, the note. Um yeah, the notion of indistinguishability we consider is is is quite far from the best we can hope for uh in two orthogonal ways. First of all, we only consider quasi-polytime. You know, can you make it say sub-exponential? And again, here you would need to use a clever ambient graph, right, with many destructors basically. Okay, and perhaps a more practical problem is to get, again, the best we can hope for for the existing construct. hope for for the existing constructions is n to the minus also inverse polynomial distinguishing distinguishing advantage in cryptography we typically want it to be negligible we can achieve this almost generically by just using some standard hard hardness simplification so if you're willing to tolerate any little omega of one multiplicative operator it will still keep the improvement over the state of the art right it will blur the distinction between log n and two log n, but still be much more Log n and two log n, but still be much better than the previous results, then we can get negligible advantage. But it seems like, you know, when I talked to Prasad Roda Vevra, he sounded optimistic about using some LP tricks to slightly tweak the distributions of ambient graphs to make uh the distinguishing advantage negative. I'm not sure. Yeah, I I'm not sure. It it wasn't a high confidence claim, but uh Wasn't a high confidence claim, but pretty sure this is doable. Okay, sounds great. And you know, this extreme version, we showed that one-bit shares there are no gaps. Even if the domain size of the shares is three, we do not know whether there are statistical computational gaps for any access factor. Okay, so the proof there is already quite hard, and two things break down in the proof. If you increase the shared domain size from two to three. You know, more cryptographic applications. We've seen some contexts like in works of Rachel and Vinod and Ayush, where a little bit of compression goes a long way, all the way to obfuscation. So, you know, we never know. I mean, this is maybe this is the end of the applications, and maybe this is the beginning of applications. We don't know. Yeah, and I guess more in the domain of this community give more evidence. We only show low degree. More evidence, we only show low-degree hardness, many more, you know, SOS lower ones, many more types of evidence. One can try to show. I don't think anyone wants to do that. It's yesterday's talk, huh? Okay. Ancor definitely doesn't want to do that. There are some students, mad students out there. But yeah, I agree. This is perhaps the least motivated problem in the sense that we pretty much. That we pretty much know what we expect there. But I would say that a more interesting question, more about the structural relation, this is a bit like in Guy's work about the leakage problem. So we have different orthogonal modifiers of these standard blending problems, right? Like allowing hint in the form of a small subgraph. Or, you know, even a search to decision for this general graph. know uh for these general graphs this this is could be a a more challenging problem you know for for for there are things that work structural properties for planted clicks that do not seem to work for that seem to break down for for for general graphs so better understanding this can can yield some some qualitative insights yeah and we have all this distinction between click and a random graph and a typical graph where the distinguisher can depend on the graph so showing equivalence maybe you know is the random graph version Maybe, you know, is the random graph version equivalent to the typical graph version, right? Is some graph-dependent advice helpful in detecting? Yeah, so I think this is it. Did you come for one question? There are more questions. So, about this Ambdian graph question, so if I ask something like, okay, you want to take any random graph distribution, and we're going to form this hypothesis testing question of distinguish between the distribution versus the distribution plus the tweak versus this kind of random graph, and then maybe subject to some density or something. So, are you saying that you don't think that a Roche-Ranye is the hardest distribution of those? There would be harder. So again, I mean, you can construct a pair of distributions over graphs such that one of them contains a click of size n to the graph with n nodes, such as one of them contains a click of size n to the 0.9, and the other, the maximum click is n to the point 9. Random is not the hardest instance for click. Okay, I'll ask you to go. In particular, hiding a click inside the click. But in particular, hiding a click inside the click is pretty easy. Sure, sure, sure. But what subconstraints like what's the density can't be? Yeah, but this is why I gave this example. A pair of distributions that are indistributable or one contains a huge click and the other only five clicks, right? We can build them in a formal way. Sorry if you address that already. Regarding your question of boosting the cost of the number of exponential, what you always boosting is like faster than which you can The issue is that we have a bit more constraints than just, you know, if you think of this planting both click and an anti-click, this is not the typical setup of these problems. Yeah, but for the problem. Let's try to get back on schedule. So back in half past two. Thank you. Not discretionary like traumatic. You're already told me it's not like that. Yeah, what the hell? Yeah, yeah, so we didn't know this was his problem. So the question I was asking for, right? And now of course you need some conditions so that there is no significant and make it manifest and gain some draft action. Otherwise there is no edge. One way to do that is to include edges from S and S for it is also ensured that there is no effect. At some point, you want the information being the location of the case. Yes, it should be identified when you do the same thing. So, if you have to do something to the cutoff as well, which I haven't thought about. Yeah, like it's information that determines exponentially many deflects and deflicts. It's all about this graph could have exponentially many large clicks. S cannot be significantly contained inside like a function. But like this seems to be starved. One we could ensure that we removed the edges from like set up the edges so that the ones that won't professor things are at random. This would ensure that there is no enlargement. In this case, my HTTP will just be because basically in this case there is a polynomial type of colour. As long as S is bigger than N, I did give you a list of size basically. Which contains this. So this seems like good enough and actually reliable enough. I mean, definitely. I think it would basically give you one over scroop and for anyway happy with an RPG. If you give me one vertex, you can realize this, but most, you know, in this most sector, you can see also. This basically shows that you get a tab if there's enough in the basement, you're saying it turns out to be very hard. So, just to confirm if you tell me one vertex in S, then I can tell you what S I can tell you. Okay, but then that notion of like four spread versus final. So the key point there is that if you're using the fact that these nodes cannot be contained exponentially, I think you're implicit there thinking of a search problem here, whereas here the goal is only when we could have exponentially many. Could have exponentially many clicks and anti-clicks, right? The goal is to just generate a joint distribution of G together with one choice of the click and anti-click, such that you cannot. Maybe like the key thing that I'm missing is... So, okay, I'm saying that the following two are conflicting requirements. To construct a graph with exponentially many clicks and anti-clicks, while at the same time ensuring that the click you are hiding cannot be significantly enhanced. Hiding cannot be significantly enlarged. I'm saying these two are, in principle, in conflict with each other. I'm saying, like, a natural way to somehow build a graph with many, many large clicks would, in my opinion, also. But even if the click can be significantly enlarged, it would not necessarily break this instruction. Ah, so but that's that's exactly, right? Like so so this is like this random click is like in some sort of a in the case sound it sounds like a very interesting step in this direction. Step in this direction. Uri basically, I don't remember, I communicated with him at a fairly early stage of this project, but I believe I did formulate this question to him and even posed a specific candidate for a graph model with high-degree variance. And basically, what he said, okay, you look at the sub-graph induced by low-degree nodes and you'll apply spectral methods. But again, he sounded like it sounded to him like a very interesting and difficult research problem. interesting and difficult research problems. So in Shamir Secretary you can draw a graph, it's an explicit graph, it's not a rental graph. So I think maybe one way to test it is whether like this spectral method would find a close structure. So this algorithm is not really spectral in the middle. Or whatever. I mean whether this only works in the regime of Ganesha or something like that. No, no, but there we have a quadratic size click, but it's not a random graph, right? It's an explicit. So that this shows that the choice of yeah, this shows that the choice of click and endo might be. What I'm saying is maybe you can test like the methods that we have for finding clicks in random graph into this explicit graph. They were not designed for it. Right, right. Yeah, this is a very good question. I did not know of a way to think of some Shamer's schema. I didn't either until you both told me about it. My point is that it's not a planting. I mean, it's a weird planting for him, right? So, you know, I told you this in a weird way, but like, you know, Uri's. Weird way, but like you know, Puri's motivation for even considering this model was to understand robustness of the algorithms for clicks. He wanted to phrase models that do not use randomness and allow for more general graphs. And I said, if you look at these distinguishable distributions that come out of the harnessing approximation results, do they have any interesting properties from uh you know uh right, so so so the difficulty here, so it's in some sense you can generically show that uh the clicks in those cannot be significantly enlarged. In those cannot be significantly enlarged. But this property is hard to algorithmically verify. Like, you know, I told you one very specific way to construct edges around S so that it cannot be enlarged. You may choose them randomly. And this is in principle like known to an algorithm. An algorithm can basically learn from this. But there are like, you know, weird ways to perhaps construct edges so that there is no way to enlarge S. But of course, also it's hard for algorithms to do anything about it. And I think one intuition that comes out of this line of. One intuition that comes out of this line of work is now like you know, the worst case hard to approximate versions of click must have the property that their cut is in some sense adversarially chosen. Like the rest of the edges do not matter. It's really the cut of S that matters. We're thinking of maybe using some kind of hardener like PCP-based hardness approximation for having both a click and an anti-click, but it's it's uh it's not clear. Again, Budi would be like at one point when you're talking about kind of getting this building what's absolutely.