So, what I would like to do is, I would quickly touch a certain issue which is related to optimal transport-based data analysis, which kind of became pretty popular in the last years. That's what I call optimal transport dependency. And I would like to discuss that as a kind of maybe interesting and novel method for modeling and measuring. Method for modeling and measuring statistical tendency. So, to link it to the topic of the workshop, latent structure would be here, in fact, what I introduce later as the optimal transport plane between marginals and couplings. And that will carry actually very important information on dependency to make, for example, prediction or to make use of it for other purposes. So, let me stress that this is joint work with Giacomo Nies, a PhD student of mine, and Thomas. PhD student of mine and Thomas Stout, who is a postdoc and loves to make handstands, as you can see on the right-hand side. So let me just start with saying something very elementary. We all have a clear notion, I think, of what is independence. So at least if you use the terminology of Kolmogorov, that independence is characterized by that the probability of a joint event is just the product. Of a joint event is just the product of its marginals. So, it is not so clear actually what is dependency. So, if I show you these little three toy data sets on the left-hand side, then you would probably immediately say that the left-hand side one and the middle one, there seems to be the X and Y relation completely deterministically dependent. And maybe the right-hand side one looks very And maybe the right-hand side one looks very uncorrelated or independent. So, in fact, I tell you the truth here. And the truth is that they are all deterministically coupled in a sense, completely. So, they are just sampled from these three admittedly very different functions. So, a monotone one, a kink one, and then this very wittly, almost looking maybe chaotic function, but it still has a certain structure. Still has a certain structure. It's continuous. Lipschitz is continuous, even. So, if you would use classical measures of dependency, and of course, we all know many of them, like, for example, I don't know, Pearson or Spearman's Wank correlation coefficient, then you would get for the left-hand side, obviously, that it's highly correlated. You get correlation one for the Spearman's and for Pearson's correlation close to one. The middle one, these two. The middle one, these two are not able, in a sense, to detect. It's almost empirically uncorrelated, and also for the right-hand side. And the reason is, of course, because, as you all know, of course, they measure specific features, let's say, of dependency or what we believe should be modeled as dependency, like the Pearson one measures effectively, as you all know, linear or deviation from a linear relationship. Deviation from a linear relationship and spearmen's from a monotone relationship. And that's obviously what is then violated on the middle and the right-hand side picture. So here is another example, which is a classical gene expression study to predict breast cancer, already dating back 20 years ago. Here, the data were collected from 98 patients, which you see actually at the leaves, along with six clinical Along with six clinical responses, I only show two responses here for illustration: angioinvasion, which is the spread of a tumor into blood vessels, and the development of metastasis on the right-hand side. So the expression levels were recorded or measured of about 5,000 genes. And this gives you a correlation matrix of these expressions. And that has been used as a, let's say, distance measure or correlation measure between genes. And then hierarchical cluster. And then hierarchical clustering, actually, tree has been extracted from that based on a single linkage clustering. And these trees, which you see, then has as leaves the patients and relevant groups of genes as nodes. So according to that clustering. And then what you see below here is binary responses whether cancer has evolved or not. Or not. So, for example, the dark bar then corresponds to having metastasis on the right-hand side. And if there is not a dark bar, then luckily the patient didn't exploit metastasis. So what you would also like to understand here is, of course, the dependency, let's say, between that tree, which you get from that correlation, or maybe from that correlation matrix, to these binary responses. So I show you this because here you see that depending. You this because here you see that dependency is obviously now a concept which where you act in very different spaces. One would be maybe the space of trees, and the other one would be just the space of binary values, which where you maybe want to make it both to a metric space. So you can take on the trees the past distance and you can take the matching distance or zero one distance on the binary responses. What I want to do in this talk, as I mentioned, What I want to do in this talk, as I mentioned, is I would like to come up with a notion of dependency, which is very general. And I think it has quite interesting meaning and interpretation. And that is based on optimal transport. So in the first part, I will just review a very little what we need from optimal transport. Then we'll also talk about some computational issues and then come back to this application. So, as you all know, So, as you all know, optimal transport is now a very well-established theory which originates back already to Gaspar Mosch about 250 years ago, who formulated as a physical, let's say, matching problem or mass transportation problem. And that leads to a very difficult, highly nonlinear variational optimization problem in general. And a lot has been contributed by mathematics and optimization. By mathematics and optimization to that. But then, and this is a formulation I want to pursue here: Leonid Kantorovich, in an ingenious way in the 30s already of the last century, relaxed that problem in the sense that he replaced functions actually by measures. This sounds at a first glance that it gets more complicated, but actually it gets much more simple. And the optimal transport problem in that formulation. In that formulation, then would just amount to the following, and we can do that in pretty general context. Let's say we have two underlying spaces, Polish spaces, X and Y, and then we have marginal measures, probability measures. You can generalize, but for our purpose, probability measure is fine, mu and new. And then, of course, we have couplings between these probability measures, as you all know, which we now call transport plans. So, but this is nothing else than a transition. But this is nothing else than a transition kernel or a coupling. And what you want to do is you want to transport one measure into the other or map one measure into the others with such a transport plan, but you want to do it in a most cost-efficient or energetic efficient way. And so that means you have a cost function which acts on the product of these spaces. And for mathematical reasons, you may assume that it's lower semi-continuous. In our case, it's always. In our case, it's always our situation, it's always the case. And then, actually, this highly non-linear problem becomes linear in the following sense. You want to minimize the expected or the average cost over all possible couplings between these measures. That means effectively that you pertain mass. Mass does not get lost. Mass is here standardized to one. These are probability measures. And then you try to seek for the optimal. For the optimal coupling, minimizing that expected cost. And obviously, you see immediately that this is a linear problem because you optimize over that. And actually, it's a convex domain of coupling. So you see it's a linear problem now over a convex domain, which is then much more easily accessible to theory, but also to algorithms. So, what has been the ingenious idea is here that I just allow in a Idea is here that I just allow, in a sense, mass splitting. And there are situations where this is maybe not the right thing to do from a practical point of view, but in many situations, it is the right thing to do. And then you can show actually easily that such a minimizer exists. And under reasonable assumption on the cost function, it is unique. So in most cases, the cost function will be actually a metric or the sum of metrics within these spaces x and y. This is what you should just take as the terminology. Just take as the terminology and which we need in the form. So, as I said, optimal transport has a long history, and only maybe more recently it actually entered actively into data analysis, in particular into data analysis where you can think of massive data, like maybe a few millions or so. And one reason is that there has been made very much progress in In computation in the last maybe 10, 15 years. Also, still classical and old algorithms are very, very, very useful. And we do that actually. This is one side of the metal. And the other is there has been also in the last years quite interesting progress in statistics and statistical inference and also in machine learning. It's nowadays used a lot, actually. Really. So, what I am in general interested is: is there a principle and also a computable way to use OT as a method for data analysis? And this is certainly not what it originally was designed for. Also, it, of course, occurs and plays an enormous role in many optimization tasks and transportation, real-world transportation tasks, for example. So, the talk in this The talk in this talk, I would just, as I said, look to a specific issue and that is statistical dependency. And actually, there is related literature in a sense in the statistical community. Also, they did not put it in the framework of optimal transport and usually look to Euclidean or even one-dimensional situation. But also, in the machine learning literature in the last years, there popped up actually this notion of. This notion of transport dependency, as I want to discuss it here, and maybe put it a little more on rigorous and solid grounds. So, the idea is just the following, and that's very simple. Imagine I have such a cost function. Think of maybe the sum of metrics in these spaces. Then I just do the following. I define a notion of dependency in the following way. I have potential. I have potentially a coupled measure, so X and Y are dependent observations. And what I just want to do is, I want to transform somehow or move it in the most cost-efficient way or cost-efficient way to its marginals or the product of its marginals. So I'm really working now in the product space. And obviously, if I need a lot of effort to do that, that should be intuitively. To do that, that should be intuitively a measure of large dependency. And if I can do that very quickly, or already the measure itself, maybe the product, then it's independence. So this is, let's say, the continuous formulation or the measure-theoretic formulation. On the empirical world, you can also do that, of course. So you would maybe take a two-dimensional, in this case, empirical measure, which I maybe call gamma hat n, and you take the product. And you take the product measures of the marginals. So, what you get to see just with a few points is here: the gamma and head would be the green dots. And then if you just take the marginal values, you would get all these blue dots. And then you would have to distribute the mass from the green ones to the blue ones in the most cost-efficient way. So, for example, if cost would be Euclidean distant, then this would just be the result here. You would distribute that money. Result here, you would distribute that mass to these three marginal points, or distribute this mass to these three neighboring points. And here you would just distribute it in y direction, and there is no movement in the x direction. So that's maybe the intuition of that transport measure. And of course, I did not specify the cost so far, and we will see that we get actually for different cost functions quite different notions of dependency. And you would be interested, of course, in whether it is possible to maybe standardize these kinds of quantities in a way that we get something which is beautifully nice, like a Pearson correlation coefficient or a Spearman correlation coefficient or whatever you want to think of. So you can standardize it maybe properly to an interval or unit interval. And when the value is then large, you can make a comparison between experiments. Comparison between experiments and to other settings. And when the value is close to zero, this is an indication for independence. So, very useful for that is a very simple upper bound, which I think goes back to a paper of Maury and Shekele in the Euclidean case, at least. And we generalized it then to the general setting of Polyspaces. And this is just the following. What I can always do is, of course, I can bound the option. Course, I can bound the optimal transport by or any transport by a less efficient one where I just look into one direction, for example, the X or the Y direction. So what I get is I get trivially, let's say, the following bound that if I have a cost function which depends on both coordinates, then I can bound it by just any transport by using, and also the optimal one by using the transport which only Using the transport, which only goes, for example, here in Y direction. But then the issue is that in the X direction, I don't spend anything, so the cost is zero. So this part just vanishes in a beautiful way, and then I get an upper bound. And I call that a universal upper bound because it does not depend locally on anything more. You can do more refined bounds, but this one is, in a sense, universal. And this is just the expectation of the y distance. Of the y distance of y and an independent copy. And this is just the upper bound, which is very useful for what I'm saying in the following. So you use that upper bound. Think of it in a similar spirit as you have an upper bound maybe for the Pearson correlation coefficient when the data are exactly on a linear surface in higher dimensions or just on a straight line in the. Dimensions or just on a straight line in dimension one. So you use that upper bound to bound the Pearson correlation coefficient, and you also can do that here. And the first thing what turns out is that if the cost function is just the additive split of the metric, so you may think, for example, of a Euclidean cost function or the square of Euclidean cost function, that would be an example. Then actually, you can show the following. This bound is achieved if... This bound is achieved if and only if y is deterministically coupled, and that would be really a transport map in the notion of optimal transport, to the random variable x. And this coupling phi has a specific structure. It's a one-Lipschitz function. And that's an if and only if. And of course, you can also do it vice versa if you would just take the upper bound in the x. upper bound in the in the x direction and then you would have that x is coupled in a lips way to y so that means here maximal let's say dependency would be achieved when they are coupled by a Lipschitz function so you can refine that and introduce what you know alpha Lipschitz functions which just in general metric spaces would amount to this inequality here Inequality here. So the Lipschitz coefficient is bounded by a number alpha, which is positive and at the moment is flexible. We don't fix it. And then you can introduce a cost, which in a sense re-rates, let's say, the X cost. You can also do it for the Y. It's not symmetric. And then you get the following. You get that this alpha cost and alpha equal one would be just correspond to what I said before in the previous slide. That this notion or this dependency, optimal transport. This dependency, optimal transport dependency, is zero exactly if x and y is independent, and it becomes maximal if y is alpha Lipschitz deterministic, or is alpha Lipschitz deterministic coupled with x. It's interesting to note that there is, let's say, a degenerate case or a limiting case, and this is when alpha equals infinity or tends to infinity. Then, obviously, the Lipschitz coefficient becomes a lengthy. Obviously, the Lipschitz coefficient becomes arbitrarily large. And so you can approximate, roughly speaking, any measurable function. And in fact, you can then just show for that specific case, and that has been suggested by Wiesel in a paper which just appeared in Bernoulli, that if you get the maximal correlation or transport correlation, then this is exactly the case when y is deterministically coupled to x by a measurable function. Measurable function. But let's say morally, you can think about it. What does that actually mean for concrete data analysis? Mathematically, you have now a measurable function, but in a sense, it's so general that you never can estimate it from data. So if you, for example, restrict to Lipschitz functions, then you know, of course, that you are able to estimate that from data, and then maybe you can use it for prediction. It's exactly what I suggested to do. So, but let me explore. So, but let me exploit a little more on this coefficient. You can make it nicely symmetric, and this is what we call the symmetric transport correlation. And what you just have to do is you have to standardize the metric on the X space and the Y space by their corresponding upper bounds. And then you get the following: you get a transport correlation or transport measure for dependency, which is first of all. Dependency, which is first of all symmetric in X and Y. The previous notions were not symmetric, which is zero if X exactly if X is independent from Y in the classical sense. And it becomes maximal if, in fact, Y and X are deterministically coupled by an isometry between the metric spaces. So that means, in a sense, this dependency would express that the underlying That the underlying variables or random variables look in a sense isomorphic from the viewpoint of these metric underlying spaces. And then you can show also that it is invariant actually with respect to all rescalings by such isometries, which can be very useful in practice, of course, if you think of that data are just metric, data are just stored in a different way, but effectively, it's an isometric. Effectively, it's an isometric story. So let me talk a little bit about estimation of that thing. And I have to be brief here because time is flying quickly. In general, it is true that in particular in high dimension, it is difficult to estimate the optimal transport dependence, or in general, optimal transport, and you have a curse of dimensionality, which Of dimensionality, which effectively says that the minimax rate scales with something like one divided by the dimension or two divided by the dimension. There is a very interesting observation which we finally then exploited and called lower complexity adaptation. I will be very brief here. And that actually says that automatically optimal transport, and in a sense that solves, let's say, the curves of dimensionality to some extent, adapts. To some extent, adapts to the lower dimensional structure of the measures. So that means if one measure is potentially living in a very high-dimensional space and the other measure is living or the support is in a very low-dimensional space, usually don't know maybe, then actually the minimax rate is completely dictated by the low-dimensional convergence rate. And the high-dimensional doesn't play any role. So this is what we call the lower complexity adaptation principle. And that also can be transferred to the situation. To the situation of dependency measuring. So that means, for example, if you have two marginals, or let me put it this way: if you have a coupling which is effectively concentrated on a low-dimensional space, also maybe one of the marginals or both are in a high-dimensional space, but that dependency is expressed in a low-dimensional structure, then you can show actually that the convergence rate or minimax convergence rate is of the order n. convergence rate is of the order n to the power of minus 2d and d is just the dimension of the low dimensional space here in the setting of a low dimensional manifold and this is very fruitful and useful because you see then that at least partially you can deal with high dimensional data to to to a certain certain extent as i said also computation plays a role and uh if you want to compute that optimal transport dependency that actually it turns out that That actually it turns out that if you take it vanilla as it is, then you have to match the two-dimensional, let's say, empirical distribution function where things are fully coupled with the marginals. And this is, let's say, of size n, and this is then obviously of size n square. But that would amount to a linear program where you match actually objects of n times n square, and the runtime would be roughly of the best. Roughly of the best available algorithms, let's say, n to the power of five, which is, of course, very large. There is a nice trick to reduce that, and this is a certain way of sub-sampling. So, what you do is, in a sense, you just take your independent data and you sample from them just to get end data. And if you do that, for example, by random sampling, because they are independent, then actually you almost pertain the information of the margin. Almost pertain the information of the marginals. And this is what you can prove, I show you on the next slide, maybe up to a constant. But more importantly, at the moment is in fact that this now reduce the size of the problem to an n times n problem. Now I compare the n data coupled data with n data which are uncoupled, which are independent. And so I can reduce the runtime of the algorithm significantly in the worst case, for example. Leap in the worst case, for example, to cubic algorithms. And nowadays, there is lots of improvement and it's a very active field. So you can certainly also go beyond, and you can then compute, let's say, problems with a few hundred thousand or a million of data points in a reasonable time. So as I said, surprisingly, you can show that this actually doesn't change the minimax rate. You lose a little efficiency in the constants, but not in the minimax rate if you do that. So if I So, if I come back to my data application I introduced at the beginning, you can now analyze it this way and consider one metric space as this free space and the other one would be just the binary outcomes. And we have actually analyzed this data with a pretty sophisticated method, which very particular takes care of the Which very particular takes care of the tree structure and so on in a paper. And there we found that there is a correlation to metastasis, but not to angioinvasion for that case. And this is exactly what we get here if we take the symmetric optimal transport correlation coefficient, where we find actually that if you do a permutation test, then you can show the validity of a permutation test, where we find actually Where we find actually that there is a strong indication for dependency in the metastasis case, but not so much in the angioinvasion case. What is interesting is that maybe if you just do the following, if you take the correlation matrix from the gene expression data as just as it is, and you do not go into the hierarchical clustering, which of course introduces the data. Clustering, which of course introduces additional sources of error and so on, and start to evaluate the data as I showed here. So you just take that matrix as a cost matrix and play the whole game. Then the cost is not a metric anymore, but you just play the whole game, you get exactly the same findings, actually, which we found pretty surprising. So it seems that this dependency concept also works even if you do not have this rigorous mathematical theory. Mathematical theory, which we developed in the situation when the underlying cost is the sum of metrics, let's say, or a linear combination of metrics. So to summarize, I would like to convince you somehow that optimal transport dependency can provide a kind of unifying concept for maybe several meaningful notions of dependency. Full notions of dependency. And this is just by choice of the distance. And in a sense, you can see that as a prior information, maybe which you feed into that dependency context where you believe that that type of dependency might be present or not. And more or less explicitly, all dependency concepts do that in a certain way. Sometimes we understand it well, like with Spearman's or with Pearson's correlation coffee. With others, sometimes we don't understand it. Correlation coffee with others, sometimes we don't understand very well. What I did not exploit here is that what you get is from that, you get, of course, also the possibility to estimate that transport plan or that transport map, for example, that Lipschitz function or that isometry. And you can use that, of course, for prediction. And you can relate very general and different variables, for example, trees with binary outcomes or whatever you can think of. It also. It also turns out, and you can show that it is pretty robust to convolution and let's say certain systematic measurement error in a sense, which I found very useful in applications I'm working on. And you can also use it, for example, for independence testing with permutation tests. We did a huge amount of simulation studies and compared it to many, many other measures of dependency. And we found that actually that that's working quite well. That actually, that's working quite well. There are issues which are still certainly not solved. And one is a computational bottleneck. Also, I suggested maybe an algorithm to speed up things quite a bit. And if you want to use the estimator, not so much the independence testing version, the estimator for that dependency in particular, there is still, of course, a curse of dimensionality, which is just Which is just unavoidable if truly the dependency measure is concentrated on a, let's say, high-dimensional joint support.