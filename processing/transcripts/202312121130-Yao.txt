So, my talk today is based on some of the recent works that have been done with my co-authors. And this is just one of the results we have been actually obtaining regarding the manifold fitting. So, my talk today is regarding how do we actually use manifold fittings or what it is and how this can be useful for people who are. Can be useful for people who are interested in machine learning or different kinds of applications. Personally, I don't really do the applications, but I think I'm actually going to give you very interesting results regarding the RNA clusterings. So some of the work has been done with my senior mentor at Tsinghua, Yao. I don't think I actually need to introduce him to most of the audience today. To introduce him to most of the audience today. So, some of other work that I've been done with my previous postdoc, Dr. Sha, who is now a lecturer actually in Hangzhou, and also my current students, Sujaji, at the NUS, and also with my current postdoc at the NUS too. So, my talk today is based on this 2023 manifold fitting with Cyclogan. And the core work has been done from my 2019. My 2019 or 2022 papers on manifold fittings. So, so, all right, so why this thing matters? Why do I actually want to do this? So, I think I got my degree around 2011. So, there's a bunch of people spent 20 years working on Masel and working on different dimensional reduction techniques. So, I think I'm not going to really compete with them. So, I actually decide to do something else. So, the philosophy here is. Else, so the philosophy here is that if you think about a very extreme case, so you have a lot of variables, basically how you measure all the data, you have p variable or d variables. Just think, just imagine a very simple case. So if at every single dimensions, right, of each of the variable you measure, right, if most of the, you know, if every single dimension are actually important, then you're actually seeing a Euclidean ball in a high-dimensional space. So in that sense, you just don't. So, in that sense, you just don't, you just don't, you just need to get a coffee and go home. You can't really do much work. So, mostly, when you measure high-dimensional dating, high-dimensional data, you're actually trying to see, at least in geometry, a very lower-dimensional structures, because not every single variable actually matters. Now, people will say, okay, well, that's why they wanted to do dimensional reductions. All right, okay, fine. But what I'm trying to actually advocate here is that. Is here is that removing variables from the whole data set sometimes is not very useful because you might actually remove some important variables. So what I want to try to do is recover the underlying structures, which I call the lower-dimensional manifold. And then somehow when I try to do this, I wanted to actually do, I want to do the manifold fitting within the original ambient space, basically within the original space that. The original space that the data has been collected, so which I call ambient space. So, the extremely simple case here that I also wanted to mention here is that these are the data sets people trying to simulate. For example, these are the Swiss road data set. You have a bunch of data sets, and then these are different colors representing different clusters. So, one thing people do in terms of from the way I see it in using geometry is that they basically flatten the whole Swiss rows. In that sense, The whole Swiss world. In that sense, they do dimensionality. They actually flatten the whole Swiss world, destroy the structures here. And then you flatten the whole thing, you become a two-dimensional plane, and you can apply different strategies, for example, classifications, clustering to classify them. But what actually matters to me is that if you sit standing here, for example, and then you're trying to go through the whole Swiss row and reach to this black or blue, whatever colors, these are the distance that you can't really use, for example, or the Euclidean distance. You have to actually use. You couldn't distinguish. You have to actually respect the geometry behind this whole thing. Then, using my terminologies, the thing I wanted to do is actually to recover the Swiss row itself on the data. Then, you can do whatever statistical methods or data science methods exactly on this manifold. Again, this Swiss row is two-dimensional. It's basically living in a three-dimensional Euclidean space, which I call the ambient space. So, I don't want to actually remove this geometries. Move this geometries. The second thing I wanted to motivate the project or the problem is that there's a very interesting application that I was involved, which is called Prime EMs. So people have been giving me a bunch of data. So these are the two-dimensional, let's say, images. So the pipeline here is that nowadays people can actually put this input two-dimensional data, which was actually a projection by these 3D parts. You know, by these 3D particles at unknown angles, so that they put these things in the pipeline, trying to come up with a 3D reconstruction of the particles in 3Ds. So now, these unknown rotation matrix actually matters, meaning that each of the rotation matrix essentially can be viewed as a point, let's say, in a very special manner. Or it's not a really manifold, it's actually a SO3. So if somebody has a way to uniquely or actually helpfully sample. Or actually helpfully sample these points in the SO3, then this can be actually be used in reconstructing the three particles. So this is why actually the method matters, because you don't actually want to just put these things into this pipelines without actually designing a very nice way to sample the manifold. All right, so these are the two motivations. And then I've actually kept talks at the different universities. keep keep giving talks at the different universities since you know i would say 10 months ago i was actually giving 10 in the states and five in europe until someday some big some big shot this you know he wrote to me and said maybe i should stop doing this just wrote a paper and publish on the ams and i actually or i should do a very extreme like uh apply works so i actually check down the the website to see what the ams is and actually decide okay this is not something And I actually decided, okay, this is not something I can do. So I actually decided to do the second things, which is try to apply my methods onto a much more application side. So this is something I wanted to share with you guys. And I will give some results by the end of this talk. So we have been collecting data sets, I would say, from the last 10 to 15 years that these papers that people have been dealing with as single cell. Dealing with single-cell RNA data sets. These data sets we have been collecting is from the nature science cell. We have been collecting so far 60 data sets. This is a 25 data set that I can put in the slide. So I will present some of the results that we actually been applying using our manifold fitting techniques into that kind of data sets. So that I'm going to show you why this thing matters. Again, I would say I'm trying to give you the I'm trying to give you a few heads up. One is that we're not doing dimensional reductions. The second is that we're not competing with different people. We were trying to polish these data sets such that everybody else can use it, in their own way, own method to do clustering. Clustering is not our goal. Our goal is trying to do the manifold fitting such that the data after our manifold fitting can be much more useful. Useful, right? So, so what is the so-called manifold fitting and why is it different from the so-called manifold learning in the literatures? I think around 2016, I was thinking about doing this kind of stuff, and I actually do a very detailed search and realize that, okay, there's the metaphor learning has been already reserved for these methods. Reserved for these methods, for example, called ISO map and local linear embeddings. Let's see, even one of my friends who actually developed the so-called local tangent space alignment, this paper has been highly cited, PCA or whatever, kernel PCA, partial maps, all kind of things. These are basically assuming that your data is actually in high-dimensional state, that's CRD. And they're trying, by assuming there's a lower dimensional structure, they're actually trying to estimate embeddings to reduce dimensions. The embeddings to reduce the dimensions, let's say, from R d to our small d all right. So, after the whole embeddings, they can't actually go back to the original space. So, in theory, this is not a really well, it is not a manifold learning because you don't actually estimate the manifold. So, that's why I'm actually focused on this much more relevant regime, such that given the data set coming from Rd, then can we actually estimate a manifold M with dimensions small d with? With dimension small d, which the small is less than capital D, such that you know we can approximate the data set very well in terms of some smoothness of conditions, right? So, so Christian University and also the Light Walshman has been working on this since 2012. So, the manifold learning literature starting from late 90s to 2010. So, after that, nobody has ever tried this because this is basically the end of it, right? So, but from 2012, But from twenty twenty twelve, the the a few people have been you know working on these things and and and and that they have they have been written let's see two or three papers on on on this on this topic. And then this somehow Charles Befferman got his interest on it and he is a fuse metallist and he wrote a paper with his book called Postdox and the Field of Laws on 2016 is trying to promote this sort of areas, I would say. Areas, I would say, very, very interesting, also challenging areas. So, and they also had another papers on 2018 and trying to provide a very, very theoretical framework of how can we actually do these things to get the balance as a manifold estimate. So, we actually come up with a paper in 2019 trying to improve their papers. But 2021, they have another full of papers. So, we've got another two papers this year. This year and also last year, I think. I mean, the manuscript. And then before I talk about, you know, this manifold fitting business, I just wanted to say, okay, if the manifold is known, if the manifold is known, then you can always extend the PCA or different methods on that manifold. The manifold living in the ambient space, right? So what I'm trying to do here is that if the manifold is not known, if we're given the data, let's say X living or Data at say X living or D, how do we actually estimate behind it? If we can do that with a very efficient algorithm, then we can always project the data points onto that manifold in the ambient space without removing the dimensions of the ambient space. All right. So this is sort of the sketch of the talk that I did. So the hypothesis. So, the hypothesis of the manifold fitting of the manifold is actually coming from a so-called geometric embedding problems. And this problem until recently has been answered by Charles Pfeffelman. He published a paper in AMS and that's basically saying that, okay, if we're giving a data, let's say A, then if the small D is very much less than the capital D, then we can actually construct a manifold estimator somehow to approximate. Manifold estimators, somehow to approximate that kind of metric space. And he is much more interested in actually dealing with this kind of problem in the metric space. To me, then because I'm not really a math guy, so we're interested in, say, given this manifold living in Rd space, and then giving a measure mu, let's say, on the manifold, then these are going to give us the x data. All right, so if you put some noise on it, shifting the x to the ambient space, then this is the y, what we got. So we want actually to construct an estimator, let's say, m hat. Again, we're not doing dimensional reductions. We're not relying on any isometric embeddings to reduce dimensions. So we're trying to directly come up with the manifold estimators such that to bond the intrinsic manifold, which we never know, and the estimators. The estimators. So, to give you a much more relaxed explanation, we're trying to push the points in high-dimensional space so that they can lie much closer to the underlying manifold without removing the dimension of this point. So, of course, we can still do some asymptotics if we wanted to publish a paper in the. A paper in it in the STEST journal, of course. And then this is the graphic illustrations. So, what do I mean by these different terminologies? So, first of all, these are the manifold. You never know that these are the points in a high-dimensional space. These points, the black dots can be images, can be vectors, can be matrix, can be graph that met. So, the so-called embedding approach or PCA, or all these kind of embedding-based approaches, they're trying to. approaches they're trying to you know uh transform these points to a uh to a lower dimensional space let's say rd okay still rd but it's not manifolds once you reduce dimensions and you are actually living or landing in this space with a small d then you can't really go back that's why i'm saying the manifold manifold learning literature are not really the manifold learners all right so if the the thing that we're going to talk about is that these are the original data points in These are the original data points in black. Then we're trying to push these black points so that the landing points, which are these red dots, are going to be lying much closer to the manifold. And then, okay, if you want to fit a Smooth manifold, and this is the fitting that you wanted to do. All right. So basically, if I can get this fitting, or if I can get this red dots with the reasonable conditions, then Dots with the reasonable conditions, then I can always visualize anything in the original space. It's just like I can do interpolations on a manifold that I fitted, then I can show you, I can visualize any images that you don't even get a chance to observe, basically. So, this is a small setup. I mean, this is a very simple setup. I mean, you can make the assumptions. You can make the assumptions much more complicated, it doesn't matter, but this is something we start from. So we assume that XI's are, you know, in Rd, but they lie exactly on the manifold. All right. So basically, there is no noise so that they lie exactly on the line manifold. The manifold was a small d-dimensions. So the reach of the manifold basically measures how flat the manifold is. For example, we're looking at this table, two-dimensional planes, right? Table two-dimensional planes, right? So the plane is flat, so the reach is infinity. So basically, the reach is that if the largest tau such that within that neighborhood of this manifold, right? So there's only one unique projection from any point in that neighborhood to the manifold. So this two-dimensional plane is of rich infinity because you can always increase the tau such that every single point in the space will have only one projection onto the two-dimensional point. Projection onto the two-dimensional plane. So the KCI setup is just any noise distributions. You can think about a bonded noise or you can think about a Gaussian noise. And the Gaussian noise is actually unbounded. So it's actually much more complicated. So a few people wrote papers on, say, the bonded noise. And we don't have to assume that this is bonded. So YI essentially the convolution of So YI essentially is the convolution of the measure that you actually put on X line on a manifold with the noise. Because of noise, then you actually can observe the Yi's. Yi's essentially are actually noisy. So the goal is trying to push the YI that we actually observe to the manifold, but in the ambient space, but in the ambient space. Okay? So this. So this is a very informative illustration of how the CF charge 2018 algorithm has been working out. So suppose this is the two-dimensional manifold, sorry, one-dimensional manifold in the two-dimensional space is just a circle. So we observe the points, the points in R2. And then so what you want to do to do is actually to get a bunch of epsilonet. To get a bunch of epsilonet, these are the circles in R2 to cover the whole data set. Now, this is just trying to find like the boundaries of these data points so that we can, before we construct the manifold, this is the regimes that we're trying to rely on. So these epsilon nets are consist of these circles with each of the circles that are centers give you the bones of your skeletons of these data sets. So now, if you want to do the geometries, so Do the geometries. So, within each of the circles, within each of the boards, whatever you call it, then you can estimate the local geometries. These are lines, these blue lines are representing the local geometry. Of course, you can do PCA, you can do whatever you wanted to do. You can do the code methods, KD, whatever. These are giving me some lower-dimensional, let's say, iPhone spaces, trying to approximate the local data points within each of the epsilon net. That's fine. Then, the goal is how do we actually try? Or is how do we actually try to connect or actually paste together these local geometries so that we can learn the circle itself? So the so-called 2018 papers from Charlotte Everman is that, okay, we just, after you fit every single lines of these local geometry, you can forget about all the data sets. So these are they call the disk, essentially just every kinds of these iPhone spaces, or PC, if you can think about it. You can think about it, right? So, so my job or their job is that suppose we have a point Y, the Y can be one of the data points, or just can be anything that you wanted to push to the manifold in the ambient space. So, this is the green point you wanted to process in such that to move this thing to the manifold because we've already estimated these discs, and then you can zoom in, and within a little circle, a neighborhood. Circle, a neighborhood, there are actually only three of them. This guy, that guy, that guy, where the centers are actually within a circle. The circle is a neighborhood you choose. All right. So in these pictures, you rely on this guy, this guy, that guy, these other spaces to decide how much or how you want to push this point to a manifold. Manifold, in the sense, is this pink curve. So I zoom in, I actually, I mean, you can, you can, if you. I mean, you can try to project this point based on this iPhone space, based on this iPhone space, based on this PC, whatever spaces. Because these are all vectors, and you can try to pull them together. So this is the width average of these circles. So these are the width average of these projections. So this arrow gives you how you should be any shooting to the manifold. Of course, this is not going to really let. Is not going to really let you land on the exactly manifolds. So you still have to do one more projection. So this is the drawback of the CF-18 papers. So anyway, I've actually plotted illustration of how you can think about these techniques, right? So every point in this two-dimensional plane can be pushed to the manifold. So these blue areas basically tell you the norm of this projective vectors pointing to the manifold. Vectors pointing to the manifold. If the norm of that vector is not going to be zero, it's not going to be very small, then you can't push them to the manifold. So, of course, for these areas, the brown areas, right, you can always very easily push them to the manifold. And for this one, it will be much difficult for you to push them to the manifold because the norm after the projection of the vectors is not going to be really close to zero. That's why we've got actually highlight them in blue. Got actually, you know, highlight them in blues. So, again, the idea is that how do you paste these local spaces such that they can give you the circle back? All right? So, so, so, my 819 paper said, okay, let's don't do this projections twice. I mean, let's just do one thing. So, basically, you can always combine the data set within the neighborhood by just. That within the neighborhood by just averaging these data points, you don't do the projections. After you finish this step, you're directly shooting your points, your Y to the manifold. This is much faster, actually, in terms of geometry, the bias is much, much smaller than the CF18 papers. So the theoretical results is the same as their 18 papers. Basically, you're looking at a round at a square root of the sigma such that. Around the square root of the sigma, such that the radius is actually sigma. So we can actually get after the manifold fitting, then the distance between the y's that you projected to the manifold with the m is essentially at the order of r square because the r is the square root of the sigma. So it's going to be sigma. So the sigma order in terms of the host of distance for the manifold fitting is the same as the CF18, but the performance in terms of the numerical results is actually much better. Is actually much better. I don't have enough time to actually talk about details, but I want to show you 2021, they have come up with a very interesting papers, but with some not very clear proof. So this is from my understanding of their papers from the 2021. So actually, they propose a very, very strong results, and that's the best that we can do, actually. And that's the best that we can do actually. So, previously, I said, okay, if we do the manifold fitting, then we can get a sigma host of distance bond. The sigma here is the level of the noise, right? 0.21, these mathematicians actually came up with the idea, okay, you don't want to actually, you know, using a local ball to pick up these points. Because for most of the people, if you think about localities, you think about Euclidean balls picking these points. About Euclidean balls picking these points, like nearest neighborhoods, stuff like that. But the goal is trying to approximate local geometries. But to do that, actually, this method will actually fall short. That's why, in order to improve the so-called sigma square bond, when sigma is very small, they came up with this ideology such that, okay, you don't use this circle, you don't use the ball, you use the cylinders. The cylinders, the idea is that if you show. Idea is that if you shrink the cylinders, then the direction of the cylinders will actually much, much better in terms of estimating normal space. So if you do that, then you don't really need to estimate tangent space. Because this is their ideas. But I think their proof has some problem because they're assuming that, let's say, these red dots are the XI, which is noiseless, and these black dots are the noisy points. Black dots are the noisy points. These are the Y eyes. They are actually in the proof, they're actually using the Y eyes, assuming that where they actually come from. They're actually basically assuming that they will have a flexibility to choose these points such that they know the online truth of these red dots, which doesn't make any sense to me because you don't assume that thing. Otherwise, you don't do manifold fitting because the red dots are not something you will be able to see. So, anyway, so we still. So, anyway, so we still think that getting the sigma square bound is actually hopeful. Then, that's why we come up with this local construction. So, the idea is still very interesting. So, you know, without actually knowing their method, for me, if I want to push this Y to the manifold, these are the data points, right? So, people were just going to pick up eight conditions and push them to the manifold to getting this. To getting this so-called green dots. That's all. If the green dots are very close to the manifold, you're fine. You publish a paper. Well, this is not going to actually improve the sigma bond. So one thing we do is that we first will decide to push this red dot a bit closer to the manifold. Let's say push this red dot becoming landing to the blue dot so that the blue dots can be sigma distance from the manifold. Sigma distance from the manifold. Once this approach is okay, then we can connect these two guys. So this is going to give us directions, say what kind of cylinders that we can actually construct. So we call this approach as a local construction. So only if we do this, we can actually do the second step to push forward the black dots to the manifold. So this guy is the green dots is actually very, very helpful. It's going to be lying very Very helpful is going to be lying very close to the manifold comparing to the first approaches, which is called the blue ones. So this becomes the backbone of our 2022 paper. All right. So of course, these are just some parameters. Like we were looking at, let's see, the cylinders with R1, R2 as the radius. And this N is the number of the points we need to achieve that bond. And the point basically depends on the Basically, it depends on the intrinsic dimension of the manifold, which is small D here, which is very good news. Otherwise, if this is going to be the capital D, then this is not good. So, the internal geometry is why this thing matters. Why we can't just use a PCA to do the local covariance and pick up a space and pick up a. Up a space and pick up a normal space to project because you need square root of the sigma. Okay, so you need a square root of sigma to have the eigengap. All right, so the thing we wanted to decide why this direction matters, the green dotted direction matters, is that we can actually control. We can actually control these directions. Okay. Okay, so after this, so geez, uh, this fy is the blue dots. So, if we actually control the blue dots with the y's, then this direction can be controlled by sigma bar. This is actually a high-order precision than the PCA. PCA can only get a square root of sigma bars. Okay, that's why over the years they actually can't get the thing correct. You can't actually improve the sigma bar, that's all. Sigma bar, that's all. So, because I have time limitations, I can just give you the result. The theoretical results is that so far we can do the manifold fittings with a sigma squared times of log turn. When the sigma is small, then this is, I would say, as good as the sigma squared. If, well, CF 2021, they've actually got a sigma square, but they are using extra information to achieve that value. Okay, anyway. So I really wanted to. I really wanted to go over this new things we applied to the GAN, the cycle GAM, the cycle GAM businesses. So in machine learning, people think about they've got a feature space which is small D. This is not manifold. There's just a lower dimension of Euclidean space. They think there's a mapping between Z and Y. That's why they wanted to be actually using the generator as the G X or G Y's to do the cycle generator. And they're actually putting the two additional discriminators to actually do to learn. Discriminators to actually do to learn this network. So, one thing I've been thinking is that maybe I should put, maybe I should put a manifold fitting approach after this Y mappings so that before we give this Y to the discriminators, we're actually trying to fitting this thing in the high dimensions and actually to give it to the Y's. So to give it to the discriminators. So actually this gave us a very nice results. Us a very nice results. So I don't have time to go over the details, but I just wanted to show you skipping these simulations, but I want to show you this thing that I promised that I wanted to talk about. We've got 60 data sets, and this data set is the first row of that screen that I show you, one of the 25. Okay, so people have been spending monies on this, and there are actually three clusters, no matter what. Three clusters, no matter what. The dimension is 38,000, and the sample is 700 stem cells from the mouth. And then they have applying actually the different methods, there's 10 different clustering methods. Then one of them are the best one, so so-called S C D H A. So it's an autoencoder approach that the RNA sequence, you know, people have. People have been using. So, what I did is that I just applied my manifold fitting on this data set with dimension 38,000. Before the fittings, this is the two-dimensional testing visualization of the data set. Of course, you can use different methods to improve your accuracies. Fine. I did a manifold fitting on the exact data set, returning the same dimension without throwing away any. Without throwing away any unuseful information, just the same matrix I will return to you. Then I still visualize them in the in using testning, okay, because it's a high-dimensional space. So I visualize them in the testing space. And you can see that you don't even need anything fancy to classify, to do any clustering for this particular data. And people say, okay, maybe this is a coincidence. So this is the result. So, this is the result we have been summarizing so far. So, we have tried this method for the 25 sRNA data set ranging from this. We've compared the results among 10 different clusterings, including the so-called best ones called the single-cell decomposition using hierarchy out input. This is the nature communication methods 2021. So, every single RNA clustering, you have to actually. Clustering that you have to actually compare with them. So, I've actually listed after my manifold fitting, I've tried their methods. These are the improvements. On average, over the 25 data sets, we've got an 18% improvement without doing anything. Again, we're not improving the clustering methods. We're improving the framework such that they can use on my framework so that they can achieve the better results. Better results. Again, once again, these are not accuracy. These are not correction accuracies. These are so-called random indexes. If we just simply compare with the accuracies, then you are expecting to see 35% more in terms of average classification errors. So I think that's basically everything I wanted to share with you guys.