Everyone to be here, and I also want to thank the organizers for this wonderful workshop. So, today I'd like to share with everyone our ongoing work. So, here, in addition to the keywords, deep learning, I also want to emphasize we are doing the testing for non-linear effects. And as this joint work with multiple students, I'll explain why this involves multiple students and also my long-time collaborator, Shao Hensei. So, I will give some brief introduction about the problem we are dealing with. Then, I'll review two methods: one is parametric, another is non-parametric. And then, finally, I'll introduce our new methods. I'll show you some numerical results. And then I will end with some short discussion. So, the problem we consider here is TWAS, or some of you are probably more familiar with MR. Personally, I don't think there is much difference between MR and TWAS. MR, millennial. And the TOAS, MR, millennial randomization. And then, also in the TOAS, also called predict scan. And then there are also, so other works called the protein-wide association studies. So there is your exposure is just proteins. You want to see whether protein is associated with some traits. Also, and some others have also done work to the image-wide association study. So, for example, you are interested in seeing whether some brain regions are related to some. Related to some disease. So, all of them share the same statistical approach. It's called IV regression, instrumental variable regression for causal inference. So, here I emphasize our purposes for causal inference. So, your goal is to identify genes associated putatively causal to some disease, for example, AD. All right. So, here actually just some kind of literature kind of review. And then for MR, there are just too many papers. For MR, there are just too many papers. So I would not bother to list all of them. So the application is like this: is that then we treat SNPs as IVs, and then we look at association between SNPs and genes in TWAS, or even IWAS, then we look at how SNPs actually associated with the brain regions. And then hopefully we can say which genes, which brain regions are causal or putatively causal to some treat. Be causal to some treats, for example, Alzheimer's disease. So, in my talk here, I'm going to use the G-Text data for the first part from SNP to gene, and then I'm going to use the UK biobank data for the second part. So, from gene to treat. Again, the goal here is kind of the causal inference. And of course, the challenge for causal inference, we might have hidden confounders. So, here I want to explain just a very So, here I want to explain just very briefly. So, here, suppose this is our true model. So, everything here actually is linear. So, what is the treat of interest? X, we can imagine X is a genes expression level. So, what we're really interested in is this beta parameter. So, this is the genes effect on a treat. But the challenge is that we have this U. U is a confounder, it's a hidden confounder. And then we know that if I just feed this model, but ignore you, I just feed this model. Ignore you. I just fade this model, you know, regress y on x, and then that will need to bias the estimation of beta. So, so positively, false positive, and so on. So, that's why we come up with, and people came up with this idea of two-stage least square in IV regression. Then you add another model, so-called stage one model, okay? And then you model the X, the gene expression, with some IVs. So, IV here is a SNPs. So, in genetics, people also call this to predict the gene. To predict the genetic components of G. But in IV regression, basically Z are just IVs. So I use IVs to predict X. And so I plug the predicted X into my second stage model, my model of interest. Then I can get unbiased and consistent estimate of the beta. So this is the whole idea of two-stage least square. And when X is a gene situation level, weight some treat. So this is called the TWAS. So, this is called the TWAS. And more generally, if X is just whatever, some exposure, and then this is like actually maintaining animation. All right. And then, here I want to deal with one possible limitation is that in stage two model, so the effects of X and Y actually is modeled as linear. So, we could imagine in some situations, maybe effects actually are not linear. Here, I also want to emphasize that in this talk, I'm going to assume that in stage one, everything is linear. One everything is linear, right? So, so the effects of the IVs on genes actually are linear. Okay, I for my application TWAS here, I think this assumption is reasonable for two reasons. One is we all know that typically SNPs have very small effect sizes. Okay, so if something is very small, linear properties are reasonable. And the second more important reason is that typically for stage one model, my sample size is very small for gene expression data or EQTL data. Sample size is typically just. Data. Sample size typically just a few hundred, for example, in G-Tex. On the other hand, for the second stage model, for example, I'm going to use a UK BioBank. My sample size is 200,000. So that's why there's an opportunity for me to detect nonlinear effects. All right. So before we look at the non-parametric models, then we realize that maybe we should look at some simple parametric models. So that's what we did just published earlier this year. This year. So, in addition to consider the linear effects of gene, we also consider the quadratic effects. So, that's why in the stage one model here, in addition to predict x, we also need to predict x squared. We just use two linear models, and then we plug it into the second stage model, and then we just test the corresponding regression coefficients. Here, I just want to emphasize, obviously, everyone knows that the expectation of x squared is not equals to the square of the expectation of x, right? The expectation of x, right? So that's why we needed this model x squared. We want to predict the x squared. So, throughout this talk, I'm going to consider two possible tests. One is a global test. So, global test is just saying whether there's any association between this genes expression and a treat. So, in my second stage model, that means it's both beta one, beta two equals to zero. So, I don't care about linear way, nonlinear. On the other hand, is that if I only care about nonlinear effects, so I only need to. Only care about non-linear effects, so I only need to test the beta tool. All right, and of course, the limitation here is my state-to-model. I only consider linear effects and the quadratic effects. So, if there is, for example, cubic effects, then this may or may not work, right? So, that's why then we have this, we want to consider some non-parametric models because we really don't know what's a function form of gene expression on a tree. So, again, I only consider stage two because stage one. I only consider stage two because stage one again is just a linear model. In stage two, here, I just want model X as a non-parametric non-linear function G, possibly non-linear, all right, G function, all right. And then again, so IB regression, the trick of IV regression is that if you just fit Y on X, you are going to get a biased estimate of G. So the trick is you just condition on your C V or condition on your IVs. So here are just condition, calculate the conditional expectation of. Calculate the conditional expectation of why given my IV Z. Z actually are my SNPs. So you want to do this way. Okay. And then, of course, one approach is to model this non-parametric function, we can use deep neural networks, artificial neural networks. So parametrized by theta, G theta. So this is this approach called deep IV, actually published a few years ago. So obviously here, because my y is actually continuous, so I'm going to use squared error loss. Okay. But in order to calculate. okay but in order to calculate this expected value so here i i have to i have to use actually the conditional distribution of x given z so x given gene expression given uh given i base and this is fine because because for gene expression we can we can assume it it has a normal distribution you know for most of the time not always right and then p here is uh just uh uh uh rigid penalty typically used in neural network also called the weight decay right so so this that looks very very strange So, this looks very, very straightforward. And then there are two issues here. The first issue is that we don't have a close-form solution for this expectation of G given Z, because I don't even know G. How I can calculate its expectation, conditional expectation. Because of that, then we have to use multicolor sampling because, again, I said based on the stage one model, we can reasonably assume X given Z has a normal distribution. So we can sample. You use multi-color sum to Follow some to approximate this integration. So, this can be dealt with in principle. But the challenge is that if, let's say, for each x, I use 100 points to approximate this integration, then I'll end up my sample size 100 times of whatever I had at its beginning. So, for UK biography data, my sample size is 200,000, then multiplied by 100, that will be huge. So, that will make algorithm very, very slow. Very, very slow. But that's another big deal. Okay. So nowadays, we always have high-performance computing. Also, we can do this in a parallel way and then so on. The more challenging actually is the second issue is that this actually turns out to be a U-pose problem, U-pose inverse problem. And if you look at this thing here, if you calculate actually the score equations, then you can imagine that this is an equation involving an integral, integral here. An integral integral here. Okay. It turns out actually, this is called actually Frey the Holmes integral equation of the first can. I didn't know this. I probably still don't know this, but I know now I know that this equation is very difficult to solve. This was actually explicitly pointed out in New West paper 2013. Okay, I noticed that paper, I read that paper, and I thought, well, of course, it's difficult because it's a non-parametric problem. It shouldn't be difficult. Okay, so that's why at this beginning, we want to try this method, and then we got one student to work on that. Then we got one student work on that, and then the students said, Yeah, you know, it took a lot of effort, you have to, you know, pay a lot of attention to fine-tune this thing because for inverse problems, the only way you can, for you pose the inverse problem, the only way you can deal with it through regularization. So that means the choice of tuning parameters, very important. So you have to have to patiently, you know, tune your model. Okay, so the student told me, I said, okay, that's okay. And then we just need to try one or two kind of examples. It seems to be working. So then the student left. And then we recruited another student. The student worked hard for several months and then came up saying, came back saying, well, sometimes we couldn't even fit a model with two function GX is quadratic. So I was very surprised. And then we tried to recruit another student with experience, with a lot of experience in deep learning. And then she tried the whole summer and then came back saying, yeah, it was difficult. So we tried many, many different ways. And at the end, all of them didn't work. So it took out. didn't work. So it took probably half a year, more than half a year, almost a year. So finally, we realized is this in here as well, because this is our model. And then the goal is to estimate the GX and then this estimate this GX because it involves an integral equation. So it's difficult. So then we realize actually, of course, in order to calculate the expectation of this unknown nonlinear function gx, it's different from nonlinear functions. From you know non-linear function of the expectation of x, right? So the expectation of gx not equals to g of the expectation of x. But nevertheless, we realize it is actually another function of the mu. And mu actually is the mean of the x. So here I use mu sub z to explicitly denote the conditional mean of x given z. So we know that x is different from g, okay. But on the other hand, if I just want to estimate the g I don't just want to estimate the g i don't i don't i don't i don't care about actually this integration anymore because i already i please take account of that right because because look at that's that's my definition here okay but of course then then then then the key question is other than whether we are going to to to deal with the original question using this approach so anyway so the key idea here is instead of estimating g function we just estimated this h function and then now we want to see whether this can solve And then now we want to see whether this can solve our original problem because at the beginning I mentioned to you, our goal is not necessarily to figure out what is a G. What our goal is to say whether this G is or is not a constant, or linear or non-linear, right? So here's actually our proposition. So at least in the normal situation here, if your GX is a constant, that means there's no association between X and Y, then this H function will also be a constant, will be another constant. Sorry, I should use another C. Okay. Okay, so that means if there is no association between X and Y, no matter you use G or you use H, you're going to reach the same conclusion. And similarly, if GX is linear, if and only if my H function is linear, okay. And more generally, of course, if Gx is a polynomial of degree K, and then my H function is also a polynomial of degree K. Okay. And then, of course, you're going to say, well, I mean, there are many non-linear functions. There are many non-linear functions, they are not polynomial. Well, if gx is smooth enough, and then we can do a Taylor expansion, then you can imagine that H can be also similarly approximated. So, and the end, then we say that, okay, we can do this that instead of estimating the G, we're going to estimate this H, and then we're going to use a neural network to estimate this H. So, I'll show you some results later on: is that estimating H makes life much easier. Much easier. Okay. And our goal is association testing, right? So, and the end, we needed to test in the association, right? So, how to do this, again, this is actually a lot of these popular approach to deal with non-parametric or adaptive methods, is that we do sample splitting, okay? So, we use a part of the sample to train our model, and then use another part of the sample to test hypothesis. Test hypothesis. So, suppose you use your training data to get the H hand, and then you just use a test data, put this H hand into your test data, then you can test the corresponding parameter. If we want to do a global test, I just put the H hand in the right-hand side of my equation. I'm going to just test whether beta equals to zero. If I want to test the non-linear effect, then I put the linear term there. So the beta tool will give me the adjusted effects, right? After adjusting for linear effects. Right, after adjusting for linear effects, so that means it will give me some non-linear effects. So, that's actually what we're going to do, and of course, you know, it's not very efficient if you do sample partitioning, results may depend, may depend on sample partitioning. So, that's why we have to do this many, many times. And then for each time, we get a p-value. And then at the end, we just combine the p-value. So, here we're going to just use the Cauchy combination methods. So, you can also use some other methods. Okay. So, here we apply to the real data. Okay. Real data. Okay. I skip the simulations because you can imagine it works for simulated data. I'll show you some examples later on if I have time. Yeah. So for the G-Text data, I use SNPs to predict genes expression level. So here, I just want to emphasize N is relatively small. So I only have less than 700 subjects. Okay. And then for the second stage, oh, and then for each gene, we just use a SNPs, nearby says SNPs. The SNPs, nearby sense SNPs, predict the gene suppression level. Okay, so here we use backward or we first module selection, then we use backward selection. So we end up with no more than 50 SNPs to predict the gene suppression level. Again, the reason is because our sample size is very small. It does not make sense to use many, many SNPs. Okay. And as in the key, here's the stage two model here. So we use UK biobank data. So here, actually, sorry, I didn't write. Oh, yeah. So my sample says for UK biobank data, the total sample says about 200,000. Okay, but as in record that as in for Okay, but as in recall that as in for deep learning approach or non-parametric approach, we have to do sample subject. So, what we do is that you use a half of sample, 50%, as our training, training, training sample to fit as a neural network model. We use another 10% as validation data. So we needed to monitor whether neural network estimation converges or not. And finally, we use remaining 40% to do testing. So at the end, we consider about 5,000 genes and we just use one for the correction. Just use one for the correction. So, here actually the results. So, we first consider the HDL, high-density lipoprotein cholesterol as our Y, our treat. So, we want to see which genes are possibly correlated with or putative causal to HDL. Okay, so here's Venn diagram. So, you can see here, this is standard TWAS. So, you only consider linear term. It actually detects the most associations. But if you consider quadratic terms, so that this is. Quadratic term, so that this is TWASLQ, and also, you know, we have two tests: one is global test, another is non-linear test. So if you do global test, actually, you may find actually a few extra genes, okay? Or deliver here, deep learning, IV regression. Deliver here is if you use our approach here, you can find identify actually 12 additional genes that would be missed by standard TWAS or quadratic TOAS. Or quadratic TOS. Okay. So this was HDO. So that this message is that if you use deep neural network, you can identify a few extra genes. And then for LDL, similar story. So you're looking at this product here. So this deliver, and then actually we can identify seven plus four, 11 additional genes that would be missed by other approaches. By the way, is that if you use deep IV. Then, if you use deep IV, so because the deep learning approach to estimate G, it did not identify any significant genes, okay, because it's not stable. Again, I'll show you just in a few minutes, all right? So, here actually just a real data analysis, and then you may see what is the what a fitted curve looks like, okay? So, here is actually our estimated h function or g function. So, so blue, blue dot the line corresponds to this TLQ. So, the parametric quadratic model. Q, so the parametric quadratic model, and then the red dashed line corresponds to our fitting model, you know, our estimated h function from our deep learning approach. And then here I have shaded area here because I run this many times. I think it's maybe about 25 times. Then the shaded areas give me actually the range. And then the static red line gives me actually the average. So you can see it's not necessarily a sample actually quadratic function. Are simple actually quadratic function. By the way, this is a gene actually only identified by our approach and is missed by standard TWAS or quadratic TWAS. Okay, so you can see here because it's probably not quadratic. And then here's another one here. This is close to quadratic, but not exactly quadratic again. Similarly, this one here is close to quadratic, but not exactly similar as quadratic, because this region here and this region here, you can see the quadratic curve is out of spawn. All right. And then this is what I promised I'll share with you. Is that this based on the simulation here? We use a gene, a real data from a gene here, and then we simulate the linear effects, quadratic effects, cubic effects. So you probably can see here, actually, I have the black line there, and also a red-dashed line there. Red dashed line is average. This is from, I think it's 100 simulations. The top is our deliver. So you can say that is a red. So you can say that the red line and the black line, they pretty much overlap. Again, the shaded area give you the range, give you the range from 100 ones. So you can say generally, our approach can capture linear quadratic cubic kind of two curves. And at the bottom is this deep IV approach. And then you can see there are a lot of variations jumping around. So wiggling here from the deep IV. You can also see actually variability. You can also say actually variability is also much bigger. By the way, the white scale is exactly the same. Okay, so this explains that why if you apply deep IV to our real data, it did not identify any associated genes because the estimated curve, just variability is too large. So at the end, there is no power. So here is actually our light work structure. We have maybe four or five actual hidden layers. One new thing here is because, you know, based on our One new thing here is because based on our experience, we know that for most of genes, maybe the association are roughly linear. So we add a skip layer here, such that then it can capture the linear effects. So that's only new thing here about the deep learning part. So I think my time is almost over, so I'll just end with this with a brief discussion. Okay, so again, so both good and bad of our approach is that we do not estimate G. Is that we do not estimate G. Instead, we estimate this H function. Okay, so if your interest is in G, then this approach cannot be used. But on the other hand, is that if your goal is for association testing, you don't necessarily care so much about the exact function form of G. What you care about is whether G is a constant or not constant, whether there's any association between your exposure and outcome. Then this approach can be used. After saying that, I will also say in our experience, the G function and H function actually are very similar. Actually, they are very similar. So, so they're not exactly the same, but at least if you draw out the H function, it will give you some idea about the functional form of the G function. Okay. And then one big limitation so far is that we assume that the IVs are all valid IVs. And of course, in practice, the valid IV assumptions probably are guaranteed to be validated. So what we can do is we can do either goodness of it test or we just extend the method to be robust. Method to be robust to the presence of invalid IVs. So, this is still going just like you know, many, many methods for MR and D1s. Okay. And we are also working on right now, only consider one gene. I think the neural network will be way more powerful if you consider multiple genes, many, many genes. So this corresponds to multivariate T1s we are currently pursuing. And then, of course, people will also ask how this compare with other non-parametric. Ask how this compare with other non-parametric methods. I can tell you is as we tried, for example, GAM, it didn't work very well. Okay, um, so all right, so so, so thanks, everyone. I also want to thank NIH and our Minnesota Supercompany Institute for offering our GPU support. Um, all right, thank you.