So yeah, some subjective history. My first encounter with bathrology and glystologist was IGS 2008 in Limerick, Ireland. Presented some work on a green and ice sheet. A green ice sheet model and its adjoint, and how we can use this to do perhaps some science with it. I was scared then as well. I thought they would just be put to shreds. But in fact, the audience was very generous. It was a great experience. And in particular, Richard Heindmarsch talked to me afterwards. Here he's teaching oceanographers on how to do sea-blowing oceanography during ACBC 2010. Some of you ACDC 2010, some of you were there. And so Richard comes to me and says, You and Gwendolyn Reiging are nearly will be working on the isochrome problem in the coming years. And I replied, what are isochromes? So I had no idea what he was talking about. And then a few years later, at MIT, Alex Woebel, who was still in Boston at the time, he actually pointed me to this 1991 paper by Dr. McHale. Dr. McHale, biothermometry by control methods. Who knows this paper? What person? Who doesn't? Who doesn't know this paper? Okay, most of those who didn't raise their hands go voting this year, so it's important, especially if you're in the US. But anyway, so it's actually really sort of the little cousin of the tutorial on control methods that's very well known in 1993. Very well known in 1993. This one's earlier, and it's actually really an elegant paper. And since I thought actually, I don't have much to say today, and in fact, Doug McKeo says everything that I could be saying so much better, actually I really pasted from this paper. It's really beautiful to read. So he says, you know, what are the concepts of paleoclimate inference and why do we care about it, why do we do inversion at all? And he says, you know, look, there's the forward problem. The forward problem. We model ice sheets, and these models are easy to do, but easy to solve, and particularly the forward problem. He says nature's evolution through history cannot be solved. In the last sentence, we are not privy to the boundary and initial conditions. It's exactly what we've been talking about earlier today, right? So, how do we initialize an ice sheet model given imperfect boundary conditions, initial conditions? So, that's how he lays out the problem, and he then basically. And he then basically lays out a forward problem for a warhole. So it's a one-point problem, which he frames as basically just a diffusion problem that evolves through time. And it's governed and it's forced at the surface by the surface climate, temperature that goes through time. And then basically the borehole has an expression on how surface temperature was at any given time in the past. So that's the forward problem. The inverse problem said, well, you know, ideally, Inverse problem said, Well, you know, ideally, you know, we'll just basically invert for this, given the data that we have. But in fact, the data usually are noisy, and in order to really fit the model exactly, they would have to fulfill that mathematically structured that's imposed by the forward model. That is usually not practical, and so it might just be impossible to solve that inverse problem. And so, basically, the inverse problem, this is the forward problem here. This doesn't work, okay. This doesn't work. Okay. So the forward problem: given surface temperature, you map to the borehole at temperature Z at the final time. So it's a final time problem where you have this temperature as a function of depth in the ice sheet at this final time. And you just write the inverse operator, but that actually doesn't work. So you can't do that. And he goes to describe why that doesn't work. But then this third ingredient, he said, well, They say, well, let's formulate this as a least squares inverse problem, which we actually can solve. We know how to solve this. It will be a non-unique surface, it will be a non-unique solution, but it's the best that we can do. And he formulates this, basically saying here the simulated temperature as a function of depth that we have, integrating our model from initial time to final time, compared to the To final time compared to the data that misfits. We want to minimize that misfit. He goes on to introduce the method of Lagrange multiplier, where he says, well, we impose, we basically extend our misfit function as a Lagrange function, and where we now basically impose our model equation and basically subject to these Lagrange multipliers. So, Mauro talked about this before. Interestingly, this It's really nice because he actually very acknowledges the connection to oceanography. He cites Stacker and Noam, 1988, that paper is one of the classic in introducing eye joints in oceanography. It's basically just this very elegant short title, Fitting Dynamics to Data. And then later on, he actually even cites my mentor, so to speak, Wunsch, 1988, who also. 1988, who also worked on tracer inverse problems. So it's all already connected to oceanography back then. So it's quite amazing. So yeah, so yeah, and then he presents a solution. So the outline of this paper here now is why am I talking about this? Well, first, you know, you should read this paper. It's nice. And then thinking about the from paleothermometry to paleochronometry, not sure if that expression exists, but it's Not sure if that expression exists, but it must exist probably. And then see how we might go about doing this problem using sort of transient inversion with an adjoint model, and then with one specific model where sort of an idea is to see whether we can do something useful with it. And so, you know, again, so you can tell me whether something, yeah, you basically say, well, this is impossible, but we'll see. So as I've said, so viewed as, we think about isochrones as kind of a three-dimensional. As kind of a three-dimensional extension of Barhaw. So we don't just have a history, a terminal constraint history at one point, but then if we have these isochrone layers, we basically begin to have sort of an entire ice sheet wide, or at least for a good part of the ice sheet where we have those. We actually have those data in hands that could tell us something about the internal structure of the ice sheet, which is right now we basically just have the surface, everything that we have is from the surface expressions. Problem of surface expressions. So, again, the analog for oceanography is something that we have been dealing with for a long time, is really the oceanic tracer data, right? So we measure, the ship goes to sea, we measure bomb C14, carbon-14, that's been basically fluxed to the surface of the ocean from atmospheric sub-bomb simulations, sub-bomb erosions in the atmosphere. Oceans in the atmosphere in the 50s that gives us a tracer, CFCs, a human-made tracer that we can measure now, and where we have a structure and where we could say what are the surface boundary conditions, so what are these fluxes of these tracers in the past that give us to the distribution today, given the ocean circulation that we impose, right? And in fact, again, Michael already draws this analog. And so the question is, how do we And so the question is: how can we formulate this problem? And just to acknowledge that, probably completely and entirely incomplete sort of list of references of people who have already worked on this problem, Waddington et al. Actually, these are amazingly all in 2007. Peranin and Heinmarsch, Leisener-Bieli et al. I'm not sure because it's probably just me who looked at this in 2000. Yeah, I don't know. Yeah, I don't know. There are some interesting papers too that give a probably a nice overview. I apologize for my lack of knowledge of the literature here, going further back in time. But sort of more recently, really, the one that is sort of been making a splash is the use of Operation Ice Bridge data and other data sets to really produce age data structure for a significant chunk of the green and ice sheets, and many people have been involved in this work. People have been involved in this work, and with one of the figures here, as a function of depth, these colors are supposed to mean anything specific except to identify same sort of age layer structures. And the question is, what can we do with this? So if we have these types of data now in hand for a significant part of, let's say, for example, the Greenland ice sheet, in conjunction with an appropriate estimation framework that would Estimation framework that would basically involve the model and some ways to do inverse or inference. So we can then use these isochrone data, but of course we can also use the present day, the other data that we have simultaneously, like artimetry data, the surface velocity data, to do a reconstruction or to actually run an ice sheet model and say, what is it? Ice sheet model and say what is it, how does the ice sheet model have to be forced in order to represent these present-day layers and other data? And how do we need to change, can we estimate, can we invert or infer the past climate evolution in order to get a better fit of that model to the present day data? An interesting byproduct, if we go with this approach, is that if we were managing to do this, Managing to do this, so we will be able to run a transient ice sheet model with the surface climate forcings and calibrated parameter initial condition to present-day state. There might at least be the chance that if we continue to integrate this model forward in time, that actually we would get a model that has a transient or a change that doesn't crazily drift like what's shown before. So maybe going this route might even be. Going this route might even be a way to solve, or one way to solve, the initialization problem, because you have a realistic transience from the past into the present, and so you might expect something semi-useful to occur if you now continue to integrate this in time. And so basically what I'm asking here is, can we simultaneously solve the parameter calibration problem? Because we would invert for the, of course, some of the internal model parameters, the path state reconstruction, and the initialization. State reconstruction and the initialization problem. Of course, does this all make sense? And probably maybe some of the modelers or glycologists here say this guy and he's nuts, and it might be true. But basically, at least it's worth to formulate this problem. The other thing I wanted to mention is that you are not alone in terms of the problems that you face with this initialization. It's a very similar problem in the climate modeling community, in the ocean modeling community. This is a paper from. Community. This is a paper from Meil, Jerry Mail, a senior scientist at NCAR 2014. It's about decadal climate prediction. And it's sort of a report from the trenches and sort of pointing out all the issues that we still have to do this decadal prediction problem. And he shows, basically, say, we have observation, let's say, in the ocean, usefully starting to have sort of a useful verbic system since the 1960s. Since the 1960s. That's the black line here. We have an ocean model or climate model that has its preferred sort of its climate state that it's comfortable with already. And that's this trajectory here, this gray state. And they, of course, are different. And so that would be an uninitialized one. And so now we say, okay, now we just do the initialization. So really the same as you. So what happens if they try this? Immediately, the model basically. Immediately, the model basically wants to go back into its preferred state that it's comfortable with. And so these are all these curves here, right? And so any of these transient, it's totally unrealistic. Some people call it initialization shock. People have been working on this and say, okay, well, let's try to figure out sort of this mean drift of the model. Let's correct for that. And then sort of this bias corrected drift, which might get to an improved hindcast. But it's still really not satisfactory, and the community is grappling And the community is grappling with this problem still today. So, yeah, so you're not alone. And so, the question between this sort of the snapshot versus transient calibration is again sort of shown here, that an ideal sort of realistic state coming with this blue curve here to present day here, where we have this sort of snapshot, a single point observation with which we initialize the model. With which we initialized the model, and we wanted to go here, but maybe sort of any other trajectory might have come to the same initial condition. That's kind of encapsulated in the snapshot inversion versus the transient calibration is where we actually try to use the data, if we have data in the past, and time-resolved. In the past, and time resolved to fit sort of an initial state that if we do integrate this forward in time, we might actually have some hope again that this would do something realistic. And so how do we formulate this? So again, so same, just quickly going through this, it's basically the same, similarly similar to what Mauro showed, but now with a time-stepping model. So we have the model state that's propagated. That's propagated forward by the model operator L from time xk sub k to k plus 1. We have at each of these times we have observations that are mapped from the model state space to the observation state by operator E and some noise at each of these times. And so we formulate a B squares model data misstep function, possibly with some more, you know, in the atmosphere and ocean we sometimes call sort of a balance operator or penalty term. Or penalty term. And we basically extend this to a Lagrange function with the Lagrange multiplier where we impose the model. And we find, now I apologize, I'm switching between k and t's. That's part of, I've been rushing to put together this talk, but I think I hope it's sufficiently clear. So when we take the derivative of L with respect to the Lagrange multiplier, we recover the model equations when we take the derivative. The model equations, where we take the derivatives of the L with respect to the model state, we basically derive the adjoint equations. These are the final time and present time boundary conditions. And so we use, for example, the final time. And we can then, it's a time-dependent problem. And we can see how at each intermediate time k, we get the Lagrange multiplier or the dual state or the algebra at time k via. k via the partial derivative of the misfit at time k, so that we that's an explicit expression, and everything in j up to time k plus 1. So that's the propagated by the adjoint model one time step. So this is the adjoint, so the transpose of the tangent linear model. So basically everything that happened at time k plus 1 in the future, and that's we already evaluated, this property. We already evaluated is propagated to time k with the adjoint plus this explicit. And so now we can play this game going to k plus 2. And so you begin to see sort of what in the AD part, so we see as the chain rule where we basically look at the how sensitivity, the gradient computes the sensitivity of model data misfit of something in the future to basically how that should change the initial condition. So we see that, that's nicely seen. So propagate algebra. That's nicely seen. So, propagate adjoint propagates information backwards in time. We also see that for these square model data misfit, they are basically driving terms. So, if the misfit is zero, then the adjoint is zero. So, they are kind of, you can think of it like as forcing terms. And so the gradient at time zero, so if this goes all the way to time zero, then basically we have sort of a weighted by these different adjoins. By these different adjoins, all of the misfits at all the different times. So dj, dx1, and dj, d dx2, they are propagated by sort of increasingly, you know, chain rules, sort of forward in time adjoint sensitivities. So very powerful. And so for us, one thing that's easy for the tonal constraint problem, we actually don't have to worry about all of these intermediate because we only have a misfit. Because we only have a misfit, we evaluate the misfit at time xn, and so everything here disappears. But then the other thing, so that part is easy, but now what we want to do to follow this idea of paleothermometry, we also not just control the initial condition or the additional model parameters, but we also want to control time-varying surface forcing conditions, because that's we want to invert for the time-varying climate in the past. Or the time-varying climate in the past. But that can be readily accomplished within the same framework by basically just considering the fact that this operator, the model operator L, it's not just a function of the model state, but it's also a function here of the known forcings and then the uncertainty, so the anomalies or the changes in that forcing. So basically, in addition to getting the adjoint equation or doing the derivative with respect to x, the With respect to x, the model state, we also here conduct the derivative with respect to the time-varying forcing. And what we see is that the adjoint, so at time, so the sensitivity of the distance at time t is actually, can be basically obtained through the adjoint propagated from final times, which is exactly what we want. We have our misfits in the radar layer. That's carried by the adjoint model. That's carried by the Android model and then basically projected at time t onto basically the forcing anomalies that we are basically trying to invert for. So that's our terminal constraint problem. We've actually, so we have done this, so this works because that's what we do in a major ocean, global ocean data assimilation project all the time, where we simultaneously invert for these initial conditions the surface forcing, like the atmospheric state, and internal model parameters. And internal model parameters. And the idea again goes back to some: yeah, we just implement this stuff. People before us have thought about this already. So that's something that they thought about in 82 when basically they said, well, we have all of these disparate observations. Think about what Mathieu was describing, all of these different, how do we make sense of these? And they were faced with the same problems. They had all these in situ measurements, current meters, chips. And then there was the prospect in the 80s. And then there was the prospect in the 80s that new satellites would come online, satellite optimometry, spectrometry. There was the idea that we can do acoustic tomography or thermometry, that acoustic wave propagation in the ocean tell us something about changes in the thermal structure of the ocean. How do we make sense of all of these data? And the idea is, you know, we basically use a model as the integrating framework through which we interpret the data. And that's what basically And that's what basically ECHO does, right? So we have a model data misfit function, and the first guess sort of, you know, the misfit is very large, the trajectory is far off from the observation through time, and we change the control space, and then sort of that misfit is included. So we've kind of done it. And one of the important ingredients here for at least for how we did it in the ocean is we used automatic or algorithmic differentiation and materials. And Mathieu said, you know, we talk about it. Indeed, I do. I can handle this very quickly because basically what it is that, you know, handwritten adjoints are very hard to do, and application of AD makes things very easy. Unfortunately, it's actually not that easy at all, but I don't have any time to go into this, except to say that so now we have recently kind of redeveloped the adjoint using open source. Open source algorithmic differentiation of the Secopolis ice sheet model, which is now called Secopolis ADV1. And work really spearheaded by Liz Logan here. And why Secupoulis, it seems like a good option. It's a 3D model with thermomechanical coupling. It has several choices of surface mass balance. It does already simulate. It does already simulate age layers, and it's probably efficient to run at the time scales over which these data are representative, which is millennial scale. And in terms of for AD, it's fairly straightforward F77, F90 code, so that lends itself really to AD. And so, you know, half of the people now say, hey, wait, didn't you do this already? Just in 2009, and the answer is sort of, but really, what's new here is. But really, what's new here is that, first of all, we're using a completely different AD tool, which is an open source tool. So, the idea being is that the wide community, you guys, can actually try this and use this. It is now the changes, code changes were required to actually really make this code be interpreted properly by the AD tool and deal with it. There are certain things that describe sort of the do's and don'ts. This is all integrated in the later. This is all integrated in the latest version of the trunk. And the latest C Corporate is so compared to the version that we had 2003 to the recent, has just many more features now available. And so that's my final slide. So where we go from here is basically we basically have the adjoint working. The primary focus now needs to be on sort of developing the data model misfit function, looking to implement this. To implement this, implement the control variables for time-dependent climate. And then, yeah, basically go through the steps to see what we can do with this. And thank you, happy to tell you to do that. I will means ask some questions or send them forward as we went. Helmar. I'm just curious: what do you do inversion and notion of the people? How many unknowns do we have? Do we have a similar situation that we have when we have distributed parameters? A or COVID-19. So we have a, I mean, we have, let's say, we have a very, very large number of unknowns because you can think of in the ocean also being words for the atmosphere before the ocean model. And in fact, some of them, there was a question. Mar and Blackboard, they differ widely. Exactly the same. They differ widely. Exactly the same problem in the ocean. We try to use the best re-analysis to force the model. They differ widely. They have a huge issue in the Arctic. So we invert, so we try, we basically use the adjoint to correct for time varying every 14 days the 2D surface field of... What about the model parameters? Model parameters, so we invert simultaneously for 3D mixing, 3D, three 3D mixing parameters. Vertical, diffusivity. Diffusivity, then the parametrized eddy diffusivity, and then we added a parametrized eddy voltage transport because we need to parametrize at the resolution that we have. There are 3D fields. So time mean, but 3D. Some people make an argument while they are in time varying, then there's some physical arguments why they might be, but we're at some point this gets so large dimension that you have to ask whether it still makes sense. I'll ask one question because we allowed to ask dumb questions and it's about automatic differentiation. So when you have a Ford model code where there is a discontinuity or just non-differentiability, so in the PDE this might happen on a surface measure zero or might be associated with a moving boundary. What does automatic differentiation actually do when you're functioning differentiating doesn't want to be differentiating? It doesn't want to be differentiated. Right. So there's a number, it's a long, the answer is long. So the one thing I think is, think of it as that's one side, at least it's one side differentiability, right? Sometimes you have something that is discontinuous at a certain, but if you are, as long as you are in one regime, you have a derivative, you have another regime, at that point it's not differentiable. But it may also be discontinuous. They could also be discontinuous. So you can, that's what we actually did to some extent in Cecopolis. You actually try to change the expression. So that's one thing. You can do brute force, in which case you're still.