Sick the whole day, so I was not really here. But, anyways, so today I'm going to talk about this ongoing project. We have me and my bus Peter on corsets for, well, it's core set. The idea is this data reduction methodology that works for spaces that are not just the EDM. So, okay, so I first want to start talking about the motivation behind this project. So, we have these collaborators at MD Anderson that have this big, very big project that MD Anderson is a very big cancer hospital in history, someone doesn't know, where basically they want to understand what mutations drive certain types of cancer, particularly stomach and liver cancer. So, what they did is they grabbed like about 20 patients, more or less. More or less, they took biopsies of the tumors and then they sampled, I guess, well, with those biopsies, then they used two different genome sequencing technologies, both single cell and both RNA sequencing. And with that, they identified within each individual cell of their like which mutations they had. So here in the is this working? How does this work? Okay, yeah. Here in the here, this thing is the UMAP plot, for instance, of the genome, of the gene expression profile for each mutation, like the average gene expression. So at the end, they have like what they identify was about 250,000 mutations from about 1 million cells, more or less, which is a lot. And then you can, of course, do some more like bioinformatics. Some more like bioinformatics and process a lot of features to try to do some, I guess, influence. So, there were several tasks that they asked of us. I guess there's more, but I guess the three key ones were this. The first one was to classify the mutations. So there's two types of mutations we can have, I think. I mean, I might be lying if there's a biologist here, but there's two. Yeah, there's basically two types of mutations, somatic and general mutation. Two types of mutations, somatic and germline mutations. So, germ-line mutations are the ones that you have since conception. So, it's the mutations that both the egg and the sperm carry it. And then somatic are the ones that you just acquire throughout your life. So, it can be just randomness, but it can also just be because, I don't know, exposure to chemicals, radiation, et cetera. And the idea is, so most of the mutations you have are germline. And then there's very few that are somatic, but these ones appear to be the key. These ones appear to be the key in, I guess, developing these very sort of very aggressive types of cancer. But then, again, it's not that, like, it's kind of hard to, it's kind of hard to see which ones are, you acquire afterwards and before. So, so yeah, so first of all, it's classified. I mean, that. Then there's also reconstructing the evolutionary tree of the tumor. So it's just like the phylogeny, which cells came from which cells. From which cells, and you can do that if you just basically look at the mutations, because cells that, you know, if you have like cells that share the similar profile of mutations, then they should be like, I guess, like in the same, at the same like evolutionary level, so to speak. And then the third one is to basically do GWASes, like a lot of them. GWAS stands for genome-wide association study, which is basically like a very fancy regression where they, it's kind of like, so I mean. It's kind of like so, and I have straight hair, so you can do regress having straight hair with like having like each gene, so that way you can identify which genes are associated with each trait you exhibit. This is something that pharmaceutical companies love to do. Anyways, so the problem is: well, first, we wanted to do patient inference in all of these things because, well, we're patients. I mean, I guess and I mean, I guess, and but this poses a bit of problem, a bit of a problem because the data is too big. Even if you just look at the if you just look at the, I guess, the reduced data set of just mutations, it's still about 250,000 rows. The number of columns, it's not a problem because that's something you define, like which coveries you want to use, but it's just too massive. So, as we all know, there's basically two ways of doing, you know, feeding a patient model. Feeding a patient model, the persistent CNC, it's not, that's not gonna work. I mean, I tried because I was just like, yeah, let's try. I mean, it never ends. Like, even if you use like stand or something that's already pre-programmed, but people that code way better than me, it's just not, it's too big. Like I get like overflows and unit like clusters. It's annoying. And then there's also variational inference, which has the advantage that you can use stochastic variational inference and then to work with mini-batches. Work with mini-batches. But that has the problem that if you don't, depending on the variation of families you choose, it might not be expressive enough to, I guess, approximate these very complex posteriors we're expecting to have. So basically, there were two choices because we had several different tasks. It was either we, for each of these, we generate this very new, fancy, computational way of doing stuff that it's a bit. Like doing stuff that it's a bit, that's very hard. Or we look back and then think of like how to reduce the data size. So, corsets are this idea of reducing that this family of methods that arise from information, computational geometry, that the idea is to just reduce the data, the size of your data sets. There are several ways of like, I guess, defining one, but I'm gonna work with like this one that I have. But I'm gonna work with like this one that I have here. So, naively, if you, I mean, if you give to any of you like this big data set, the first thing you can try is just to like get a sub-sample, right? Like, just say, okay, I mean, there's too many, too much data. So you have excess information. So you can just get like a, just at random, just sample like uniformly, say, 100 cells or 100 new pictures. That in theory works. I mean, it's not, I mean, I've done that before, and I mean, it passes the reviewers. It passes the reviewer, so it's fine. But then, the issue is, but you can do better than this. So, what you can do is, okay, sure, you grab the subsample, but you can, the idea is to find weights so that when you weigh this up, like when you weigh this subsample, you actually get a better approximation than without the weights. So, typically, you know, you assume a likelihood of this form, this form, which is like just like a sum. Which is like just like a sum of potentials. So, what you can do is for each of these, like begin, you find a set of weights that is sparse. So, you get a reduction of the data set. And the idea is just to basically approximate this pi theta by this weighted thing. The optimum and then the way to find these things is by typically so, like this is the most classical formulation. Is you basically just find the basically just find the uh just find the set of weights the set of sparse weights that minimize the coup back likelihood divergence between the likelihoods the problem with doing this is that if you just work on the likelihood well there's two problems first is you're ignoring the priors uh this is the so depending on how deep your hierarchy is that is an issue because sometimes the the key of the model is on how you set up the priors so if you're just ignoring the effect the prior So, if you're just ignoring the effect that priors have on the likelihood, then you're not actually, I guess, it's kind of like you're ignoring a big piece of information. So, the quality of the core set might not be that good. And then also, I mean, there's the classical problem that sometimes the likelihood is just not available, right? Like, like if you have no parametric models or like stuff that are like, I know graphs, you can't really write down the likelihood in a way or in a tractable way that allows the computer to solve this proximal optimization problem. So, what we think. So, what we thought of was: okay, why don't we look? Let's, you know, go back from the beginning and think of like on the ground up. So, assume you have a data set of size big n, and you, that's modeled with this pthera. Yeah, pthera. Then, as if you want to, in here, I'm going to fix the size of the corset support, like by core support, I mean like the subsample. So, as soon as you want, like, So assume you want, like, you know, you have this big n and you want a smaller, a smaller size, small n of the data set. And assume you want to, you use, you get the sub-sample and you want to, and you want to get a corset based on this. And by a corset, I mean the weights. So the idea is to hop on the strain of like the using like predictive like predictive structures to replace the pullback livelihood divergence between likelihoods with a notion of discrepancy between the predicted distributions. Discrepancy between the predicted distributions. So, the main idea is like of the contribution is instead of getting the instead of comparing likelihoods, you can compare the posterior predictives. So, we define a predicted core set as this set of weights that minimize like the, I guess, the posterior predictive between with the weighted data set and then with the weighted subset and then the full data set. And this D is just some notion of discrepancy between probability measures. Discrepancy between probability measures. Particularly, we choose the Basserstein distance, which, for those of you who are not familiar with it, it's like basically like, so you do this thing of like getting the minimum, the optimal coupling. And there's like a very big literature on this. The advantage of this is that the absorption distance allows you to compute distances between particles very easily. So you can work directly on the samples and it's a bit Directly on the samples, and it's a bit, it's just, there's a lot of like computationally advantages for those. Because, of course, the thing that I guess you might ask is: okay, sure, you want to compare posterior predictives, but then, but how do you get them? Because to get them, you need the posteriors. But then if you have the posteriors, there's no, I mean, why do you want to curse it? So, yeah, of course, we don't have access to this, but we can approximate them. So, the idea is. So, the idea is: there's this very nice trick by, I think his name is Simon, Leidon, Leidon, Chris Holmes, and Stephen Walker. They published it on New Reps like two years ago. No, three years ago. Crazy. Okay, three years ago. But the idea is, well, the likelihood is at the end a random measure because it depends on a random parameter. So, what you can do is put a prior on the likelihood. And if you put just if you assume that the And if you put just if you assume that the likelihood is a digital process, then this is very nice because, so if you center the digital process at the model, the family of models you want to estimate, then the posterior predictive is very, I mean, it's just a polyar. So it becomes, so basically to sample from the posterior predictive becomes just basically using this like interpolation between the Bayesian bootstrap and just sampling from your prior predictive. Your prior predictive. So, this allows us to approximate the posterior predictive, even if we don't have actual access to the posterior. So, now I guess the full algorithm is something like this. So, oh yeah, so well, so after you have this set of weights, you just multiply the weights by like you weight your data set. And then you can use whatever you want, right? You can use like any MCMC you want to write, any variational difference you want to. Any variational reference you want to use, like anything you want to use, you can just do it on there. So the idea is: so you first subsample, you know, the data. And then, of course, this P theta depends on a random parameter. So you sample from your hyperpriors. And then this Pn is the, by this Pn, I mean the posterior predictive motion from the polyarn. So basically, you sample from like a very long trajectory from the polyarn. Long trajectory from the polyarn based on the full data set. You also then you also sample one from the small data set. And then you just, once you have these two, I guess, big vectors, you just get the set of weights that minimize the distance between these things and you store these weights. And then at the end, you get a Monte Carlo estimate of these weights by just getting the average of them. This works because This works because, well, you assume that, yeah, so there's like several key things here that make it work. First of all, is the digital process is unbiased, so it's going to be centered at the family of models you want. But and then also, I mean, the Monte Carlo, and then also you have like this notion of the set of predictives force a martingale. So you can. Martingale, so you can well, so you can use like any like the Monte Carlo estimate is consistent because you have the loss of large numbers for martingale. So the only caveat is that you need this map to be smooth. Because of course, if the basically if the weights like this F hat is the empiricals, so if this is not smooth with respect to small changes in the weights, then you're not. To small changes in the weights, then you're not going to really, it's going to be kind of hard. You're going to need a lot of samples of the weights to be able to get a nice quality cursor. And then also, well, of course, this distance between like the vasosten distance can also be replaced with something else, depending on what of this exact application you want to use. So now I'm going to go on like three use cases we did. We did, or like we're doing, there's one that I'm currently trying to debug, but, anyways. So, the first one is density estimation. So, I know you can just simulate from a mix. I simulated from a mixture, this, I guess, thing. The true mixture is this like, is this blue density? And you want to fit a location scale, location-scale division process mixture with a Gaussian kernel, like the very, I guess, very classical infinite mixture. I guess very classical infinite mixture thing. So then we run this like with different values of that parameter that tunes, there's this parameter alpha that, where is it here? This parameter alpha that tunes the thing between, I guess, sampling new data points or just sampling from your current data set. So depending on that, you find different like approximations. So basically, Approximation. So basically, if you just don't wait anything, that's the orange one. You see that, well, you don't recover, you don't recover the modes here. But then if you actually do the samples, you still cannot recover. If you take the weights, you cannot recover a bit this bump here, which is nice. Then I also repeated it and then like computing the pullback library. Computing the Kulvak-Liibi divergence between the densities, the estimated densities, you also get like, I mean, it actually does decrease the distance between them and the true one. So the only thing is tuning these alpha is kind of weird because, so for instance, this one, I think I used alpha equal to one, and this one I use alpha equal to 0.1, where it's the upper one. So as you can see, like in the one where I chose alpha equal one, you really don't get any difference between really don't get any difference between the uh between the core set and just subsampling but then this one is not and i really don't have a good intuition on when that happens i assume it has to do with the uh i guess the not necessarily compared to like this the the size of your model space it might be to like if you have a very big things which is like a space of densities then you Cities, then you in my kind of, yeah, I don't, I'm not really sure why. That's something that we kind of want to investigate what's going on there. Find like a way of, I guess, tuning this thing. Then something that a lot of papers that you see on corsets for basic statistics do is they just use a lot, they just get a logistic regression to see if it works. So, this one, I mean, it's a bit easier in the sense that it's easier for the subsample to fit, right? Because it's a very Sample to fit right because it's a very like tiny parametric model. But still, we see that you can find a set of hyperparameters that allow you to get a better approximation with the corset than without the corset. By unit corset, I mean the subsampling, just like weights equal to one. This actually took me a while. It wasn't that simple to get something that looked different. Something that looks different than just with the subsampling, which I guess means that really the nice use case for this is when you have a very big model, I guess. Then the third one, which is what Peter was talking about in the introduction, is for random partitions. So this one is the one that's actually more attuned to what we want to do with like the The sequencing data we have. Because something that you kind of want to do is to cluster cells into types. That's something that it's a very like common thing you need to do, and it's not easy. There's this, I guess, whole field in special transcriptomics that it's not. Yeah, it's just like it's kind of complex because the gene expression profiles are like too diverse. It's like too heterogeneous to read this stuff. Anyways, so in this case, the random. Anyways, so in this case, the random partitions, the framework is a bit different because in the other examples I showed, you actually have the data and then I can compare the procedural predictive in the data. But if you want to compare the partitions, you don't observe partitions. Like you never, you don't go around in the wild and they're like, oh yeah, I observe this partition. You observe data and then you have indicators, like cluster indicators for each data port. So what you need to do is to input these things and basically expand the data. These things and basically expand the data. So if you start with YI, you need to augment the data to YI, SI, with where SI is the indicator. And you can do this by just basically sampling randomly from the EPPF, assuming you have like a DMP model. Yeah, and then, so the idea is, so because, so we can, instead of working with YI, you can kind of expand this, and then at each iteration. And then at each iteration, you just have to input the you just have to input the cluster assignments for each data point. And then for the trajectory that you sample, you can just get the posteriors for those. So here's like the modifier algorithm. So again, it's like you start sampling like the feras, like from your mixing measure, from the prior productive of these things. From the prior predictive of this thing. And then, when you want to sample the posterior predictive, you can, for these, you can just get them conditionally because typically things have like, you know, you have like a species sampling model type of thing, it's very simple. This, yeah, this is something I'm still kind of working on debugging, not working for some reason. But yeah, so the further, so the further work we need to do is, first of all, is we need to find a way of guide the hyperparameter exploration. Hyperparameter exploration. Because right now, all we do is say, oh, yeah, first to sample Feras, like your parameter, just sample them from the prior, from the hyperpriors. But this is kind of very naively done because, of course, you can get a better quality. It's kind of like when you do vanilla IBC that you just like sample. And that's not, it's not necessarily work that well in practice. So what you need to do is instead to do some sort of like more guided approach to that, which again, we can. To that, which again we can we can uh use like this, all this ABC literature to do. We also want to characterize the effect, the effect of how complex the model is on how to get an effective like the size, because for some corsets, for instance, for the logistic regression one, I had I simulated, I think, 10,000 points, and then I sample 50. And that worked like well, but sometimes it depends on the model of the size of the The model of the size of because it's like this always this tension between how big your model is and how much data you have. Because, of course, the bigger the model, the more data you need. But then it's like, you know, when can you actually get weights that mean some. Then we need to test it on real data. This is more a problem with pre-processing of the sequencing data that we've kind of spoken on some bioinformatics there, but anyways. Informatics there, but anyways, and then this is actually kind of slow because right now I just say, okay, I just use like, uh, just use like automatic differentiation for it, but that's very slow. So there's many, many ways to, there's, I mean, it was just like the, you know, the thing that you can write fast in PyTorch, but you can do better than that, which is something that we also want to try to accelerate it. Yeah. And so I guess. Yeah, and so I guess this was it. And if you have any questions, I'm here. Trevor, I have to go to building. Oh, wait a second. Yeah, you and turn the microphone on because we all did that mistake. No, but it's the switch thing. So, well, first of all, thank you for the very interesting talk. I will limit myself to just maybe one. Okay. But I would like to chat with you a lot afterwards. A lot of interesting ideas here. My question is: so you sort of compare, at least at the beginning, to existing Bayesian force set methods that attempt to sort of directly approximate the theory. And I'm kind of wondering if you looked at the weights and the core set points that your method produces and just see if it produces a good posterior core set as well. I mean, I know you were most interested in the posterior predictive. Oh, yeah, sorry. These are the posteriors. Yeah, yeah, yeah. No, these are the posteriors. Like, all of these are the posterior. Like, basically, once I got the core set, I just ran it with like PyMC and then saw what I got. Pi M C and then saw what I got. So these are the posterior. So I do get a good quality approximation. Okay. And then the other question is sort of converse of that, which is, did you use the more sort of like existing Bayesian corset methods that are targeting the posterior and see if they produce an equally good or better or worse? No, that's something I want to do, but it's just it's harder because then you need to make them work. But yeah, that's something I want to do. Okay. Thank you. Okay. It's really a nice talk. I have a lot of questions for you. I'll just ask one. So it's going to be a stupid question. So it looks like you're trying to find a high quality subset and with weights to really try to approximate the full likelihood because I guess evaluating the full likelihood using all data points will be too expensive computationally. So I'm wondering why you would focus on a subset of the data. Focus on a subset of the data instead of trying to approximate the likelihood function or approximate some kind of posterior distribution directly. Like, what is the difference between finding a subset of the data versus just approximating the function itself? I think it's just simpler. So, the problem is that the approximation, the approximation you're trying, it's kind of like the problem of variational inference. Like, you need to find something that approximates. First, like you need to find something that approximates well each model, but that changes according to the so particularly here because, for instance, the first task is just building a classifier. The second task is building a tree. So, like to be able to do these things, they're not adaptable to like you need to be, it's like three different papers. And instead, here's like one. So, it's more like, it's more adaptable to do it like this. Yeah, I just had a brief question, which was, and thank you also for your talk, which was: can you describe a little bit more the role of or the work that model misspecification is doing here? Because, I mean, these papers that you're referencing by Lidden and Holmes and co-authors, a lot of the motivation of this, like patient. And so like, are you I guess I had trouble following precisely what the distribution is that you're trying to target. Like what are we allowing p theta to be misspecified? Or are we assuming p theta is well specified or how this is clear? I mean that's kind of floating. I mean, that's kind of floating, like you can do both. So, the idea is that, so you have a model Pythera, and then because you can't really directly, well, you don't want to directly work with it, if you put the digital process prior on it, you kind of get a, if you put a digital process prior center at that thing, then you can get the posterior predictive, but sure, you can also basically, you get the model mispecification thing for free. Miss specification thing for free once you have that. But I mean, no, we're, I guess, we're assuming that the model is not mis-specified, but it's kind of like you do get this correction for free. So I don't have an answer, but I have a possible interesting open question, which is like when I was looking at this through my causal inference lens, I was immediately thinking of like, how can we use this for treatment of that estimation? And then I started thinking, like, is it possible? Thinking, like, is it possible? Like, suppose you have data from a randomized trial, and it's a large quantity. So, you have to enter data reduction in this way. Like, could you end up picking four sets that would actually end up inducing confounding? Like, maybe the treated units that end up getting non-zero weights are systematically different from the untreated units in terms of in some ways. And then you end up breaking the randomization. Under what conditions? Breaking the randomization. Under what conditions could you get a core set that wouldn't do that? I think would be super interesting. I don't know if that's... I think it's about this. Like you would have to define like a notion of discrepancy that does that. I don't know how to do that, but I guess if you do it like that, you can do something. Yeah, you'd have to add some constraint maybe that says select. Constraint, maybe, that says select them, but make sure they're balanced all the time. Yes, yeah, yeah, yeah. Yeah, anyway, yeah, it was super interesting. Uh, okay, thank you, thank you for great comments. The interest of uh lunch, which is anyway, 50 minutes later than what they want. Uh, let's uh break here for the break. So, thank you, Bernado. Let's make like 20-minute break and we reconvene. 