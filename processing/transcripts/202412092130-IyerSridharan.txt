Thank you so much, and thanks a lot for the invite and the opportunity to present at this conference. I'm sorry I couldn't make it in person. There was an overlapping workshop here at Zanadu. But today, in today's talk, I want to talk to you about a series of results that I was part of, and they fall under the umbrella of quantum error correction beyond the Pauli paradigm. Beyond the Pauli paradigm. So, before I start, I just want to say this is work done with David Pulan, Aditya, Joseph Emerson. Okay, so just to give a very gentle introduction to quantum error correction, I'm guessing this is the first talk in quantum error correction in this conference. So, just keeping that in mind and giving a very gentle introduction. When we want to build a quantum computer, we When we want to build a quantum computer, we basically imagine it to be some kind of an elaborate quantum circuit where the wires are essentially carrying qubits, memory devices, and the boxes here are the gates. And one of the popular issues with such a quantum computer is that the qubits are highly sensitive to noise. For instance, they could be some stray magnetic fields from like a microwave woven in the lab or Microwave woven in the lab, or someone just having a cell phone in their pocket. And we don't want this noise to basically affect the result of the computation. So what we do is we go back to the classical literature and take the good old solution of doing computation in the presence of errors. And the solution here is essentially. And the solution here is essentially to add redundancy. And by add redundancy, what we mean is that we basically now say a single qubit is not going to carry my unit of quantum information, but it's going to be spread across a collective state of many qubits. So the simplest solution here is me representing my zero and one of my information states, not just with one spin half. Not just with one spin-hof, but in a state of three spin-huff particles. And the way I would do it is I would basically say my zero state is all spins pointing up. And this means that if there was some kind of undesirable interaction that basically flipped one of the qubits, I basically get a state that is neither all spins pointing up or all spins pointing down. It's something that's out. Pointing down, it's something that's outside the subspace, and the idea here is to basically look at the state that I have after the error and statistically infer what I started out with. So, in this case, the statistical inference is simply a majority vote. Now, we can basically carry that idea forward to a little more complex situation where the state of many physical systems. Many physical systems that represents my qubit that basically encodes the information in many physical systems. It's not just a simple state of three spins pointing up or three spins pointing down, but it could be some complicated entangled state of some n physical qubits. And here is an example where you can actually have the zero state encoded in seven physical qubits, and the zero state is specifically. And the zero state is specifically this entangled state. And the one state is another entangled state that's orthogonal to this. So, this is the crux of quantum error correction. And the idea here is that once I encode my quantum information into many qubit states, then there is a specific classical algorithm that I can employ to look at the state after the errors. And then, And then infer what state I started out with. So essentially, you can think of now replacing all the little wires in my quantum computation with a cable that consists of many wires. And this many wires is the physical qubits in which we encode quantum information. And as long as the error in the physical wires or physical qubits is below some kind of a threshold. Is below some kind of a threshold, we can ensure that the error that persists after this quantum error correction algorithm is suppressed compared to the physical errors. Now, there's a special class of quantum error correcting codes that's by far the most widely studied. And here, there's an additional constraint on these entangled states that encode quantum information. And the additional constraint is that they have to be eigenstates of Have to be eigenstates of poly operators. In particular, if you look at this particular entangled state that I have at the top, it's an entangled state of seven physical qubits, but it turns out that it's actually a plus one eigenstate of all these six Pauli operators. And by just like a string of x, x, x, x, i, i, i, it's, it's essentially a tensor product of sigma x. sigma x, tensor sigma x, tensor sigma x, tensor sigma x, tensor identity, identity, and identity. So they, as you can see, there's seven qubit operators. There's six of them, which means that a common eigenstate of these six seven qubit operators, there will be two eigenstates. And these two eigenstates are exactly what we call our logical zero and our logical one. Okay, so what is the purpose of? So, what is the purpose of doing this? In other words, adding redundancy to the system. It turns out that if we add redundancy to the system and employ a quantum error correcting scheme, we can basically get a suppression on the error that persists in the logical information compared to the error that's happening in the physical information, or in other words, physical noise. Now, this is this basic. Now, this basically tells us how useful this quantum error cutting scheme has been. For instance, you can see for about 1% noise on the physical qubits, after the quantum error correction scheme, I've basically suppressed it to like 0.01% noise on the logical qubits. Now, it turns out that one, if one needs further suppression and noise, well, the idea roughly is to basically blow Blow up the size of the entangled states. In other words, use many more physical qubits to encode quantum information. And this can go on and on. And at some point, we basically have to do it to achieve a certain target in the logical noise strength. Now, there are some major questions, and this is what the talk is about. One of the major questions is, yes, we talked abstractly about physics. Talked abstractly about physical noise trends. Now, if you really walk into a lab and pick up a device, there are hundreds of parameters of this device. And therefore, it's actually crucial to know which of these parameters or what's the relevant measure of physical noise strength that is actually crucial to estimate what kind of logical noise suppression we can achieve with a quantum mechanical. Can achieve with a quantum error correcting scheme. The other question that's related to this is, of course, how do you really compute the y-axis parameter, which is we talked about the logical noise trend, which is essentially the noise that persists in the quantum error correcting scheme after quantum, that persists in the logical information after quantum error correction. So, we want to know how to compute this quantity. The third The third one that's really useful from the point of view of actually building a useful quantum computer is to essentially ask with limited resources, can I improve the suppression on the logical noise strength? Or is it just like a universal constant? It turns out we can improve it. And can I do that by selectively turning off and on some sources? Off and on some sources of physical noise in the device, or to actually improving the quantum error correcting scheme itself from some information I can get about the device. And these are the questions I want to walk you through in this talk. Okay, so let's look at the first parameter here, which is what's the physical noise strength? If we look, if we basically ask this question to a theorist, and for very good reasons, one of the popular models of noise on a physical qubit is given by this thing called the depolarizing channel, which essentially says that there are fundamentally three types of errors that you can have on a physical qubit. One is a bit flip error, the other is a phase. The other is a phase flip error. And the third one is just a combination of a bit and a phase flip error. And these things are given by the Pauli matrices sigma x, sigma z, and sigma y. And now we can basically say that a noisy qubit is nothing but a mixed state where I can have with some probability an sigma x acting on the state, or sigma y acting on the state, or sigma z acting on the state. Z acting on the state. And because this model is simple enough, just has Pauli operators acting on the state, it turns out that it's very attractive for theoretical reasons. And one can basically study how quantum error correction works against this kind of a noise model at large scale. And this model was also used to prove what's called the accuracy threshold theorem that says that if I That if I set all of them to be equal to some p over 3, and if this p is less than a particular constant threshold, then I can guarantee that I can achieve as much as I want in terms of suppression in the logical noise trend by just increasing the size of the system, kind of the plot that we saw earlier. But it turns out that if we kind of walk away from that theoretical description and From that theoretical description and see how well it fits something that's experimentally viable, it turns out that it's kind of a poor approximation. And here are some motivations. So for instance, if we look at, let's say, an ion trap device, and here is an ion trap device with Ether BMIM. If you look at this iron trap device and ask about like what are the kind of errors, so one of the more prominent errors is when we actually Errors is when we actually try to do a C naught gate with this device. And it's done using this kind of thing known as the Maldmer-Sorensen interaction. In essence, what we are doing is kind of doing a controlled rotation about the sigma x tensor, sigma x axis. And this rotation has to be done using a pulse that's precisely turned on for pi over two seconds. And as you know, pi is an irrational number. So we cannot. An irrational number. So we cannot have a pulse turned on for five seconds. That's there's bound to be some error in it. And it now it's kind of apparent that this error is just some kind of a rotation about this particular axis, which is clearly not a Pauli operation. Another example is when we try to engineer cat states in microwave cavities using superconducting circuits. And here you can kind of And here you can kind of see that it turns out that if there are temperature fluctuations in the superconducting circuit, that's away from this 10 milli Kelvin mark, then it turns out that the noise is some superposition of identity and the sigma x operator, which is again now not a Pauli transformation on the encoded on the physical qubits. And another operation is when we have. And another operation is when we have quantum computing using optical devices, it turns out that photon loss in optics on chip can also be viewed as some kind of effective transformation on the qubit space, which is again not a Pauli transformation. As you can see, here it's kind of a mixed state, but not really because the two operators on the left and right of row are not the same. So, this is motivation to say. So, this is motivation to say that if we actually look at realistic hardware, the type of noise that describes this realistic hardware is outside the Pauli paradigm. Now, there is error modeling just like the depolarizing channel that captures noise outside the Pauli paradigm. And for this, we have to go back to the good old master equation, which basically gives us some kind of a time evolution of the density matrix. And it turns out that there's an approximate solution to this master equation, which essentially assumes that the system doesn't carry memory over time. And this solution is given by what's called a completely positive trace-preserving map on the initial density matrix of the state. So a noisy state can now be described as the effect of this completely positive phrase preserving or CPTP map on. Or CPTP map on an ideal state. And this is the model we are going to use to study noise outside of the Pauli paradigm. And it turns out that a CPTP map can be expressed using a decomposition like this, where the k are called the Krauss operators. And it really follows from this physical idea that the quantum system interacts with its environment, and then once you lose the environment, And then, once you lose the environment, you are basically looking at the effective dynamics of the quantum system. And these CPTP maps now allow for a more accurate description of realistic hardware. And all the examples we saw in the previous slide are captured by CPTP maps. Okay, so when we have a noise, a model for noise, the immediate question is: okay, but what is the strength of noise? Because that's all we care about. Noise because that's all we care about in terms of knowing how good or bad the noisy qubit is. Now, it turns out that the strength of noise can be measured by various metrics. And these are essentially what you like a cartoon on the right, which is they're essentially a number that's in some renormalized scale between zero and one that tells us how much of a damaging effect the noise has. Damaging effect the noise has on the qubit. So if this number is zero, we imagine that the noise actually doesn't affect the qubit at all. And if this number is one, we just imagine that it maps, it destroys all the information, just maps any state to the maximally mixed state. Now, there are several notions, and because it's slightly tangential to today's talk, I won't really describe these metrics. But these metrics, for instance, one is These metrics, for instance, one is called the diamond distance, which is attractive for theoretical proofs like the threshold theorems. The other one is fidelity, which is popularly reported in many experimental results. And because now the CPTP map is a mathematical object, we can define L1, L2 norms, something called the entropy and the unitarity that describes how much of coherence a noise preserves. A noise preserves in the state. Now, in choosing a metric, there are a few criteria that we want to keep in mind. One is we want it to be useful and fault tolerance proofs. Like we want it to have the mathematical properties that if we look at the composition of two noisy operations, then the metric nicely adds up. Or if we look at tensor product, it again adds up. We want We want to be able to measure it in experiments. So it should have some kind of a physical interpretation. And it should, of course, be efficient to compute it numerically because we also want to, at some point, simulate how well a quantum computer works. So we want to see if we can compute these metrics. And the most important for today's talk, at least, is we want this trend to actually give us a good estimate. Give us a good estimate of the logical noise strain. Now it turns out that the diamond distance is the only one that has nice mathematical property. The fidelity is the only one that can be measured. Everything except the diamond distance can be computed efficiently. And by efficiently, I mean in the number of qubits. Now, in what follows, we're going to answer the last question. We're going to answer the last question, which is which one of these are good at predicting what the logical noise would be? Okay, so we basically saw what are physical noise processes and we saw what are measures of noise strength and what is it that we are looking for from these measures of noise strength. Now, let's look at what's the y-axis of that cartoon plot that we saw in the beginning of this talk. Beginning of this talk. And the y-axis is essentially estimating how well a quantum error correcting scheme is working and what's the suppression in the logical noise term. For this, let's go through quantum error correction from a very schematic point of view. So as we saw, individual quantum states that carry information are susceptible to noise. So what we do is we basically represent quantum states. Represent quantum information in collective states of many qubits, and that's formally done using what's called an encoder. So, an encoder basically takes a single qubit state and outputs an n qubit state. It's a unitary, so we just imagine that the n minus one qubits at the input are initialized to zero. Once we have encoded our quantum information, we imagine that the qubits, the physical qubits, undergo some kind of a noise process. And I won't describe And I won't describe this part in detail, but quantum error correction essentially involves doing certain measurements that are called syndrome detection. And conditioned on these measurements, we actually compute a recovery operation. That's the statistical inference part. And once we have a recovery operation, well, we have some n qubit state that actually maps to a meaningful single qubit information state. So we can imagine extracting. We can imagine extracting that single qubit state out. And now we have this nice formalism where in goes the single qubit state and out comes the single qubit state. It's pretty much like can be compared to the case where we didn't have all this machinery that's in the middle. And therefore, we can actually encapsulate this whole thing and call it some kind of an effective CPTP map or an effective channel. And that really describes. And that really describes the combined effect of noise plus quantum error criction on the logical qubit, on the logical information. And this is a nice tool because now any kind of metric we use to study the strength of noise on this effective channel is simply the amount of noise that persists in the logical information after quantum error function. So, what we have. So, what we have to answer our question is basically we have a set of physical noise processes, CPTP maps, and for each one, we can compute what the effective channel and therefore compute what's the strength of noise persisting on the logical information. And how we exactly do this, I'm going to skip this. We basically have an elaborate emulator that can be found in this GitHub link. Now, with these two quantities, what we really want to see is we want to basically see if I have a device and I know its physical noise trends, and if I compute its logical noise trend, now just by knowing a physical noise trend, can I predict what the logical noise trend would be? In other words, if I have many devices with the I have many devices with the same physical noise strength, but with different logical noise strength, that's bad because then that's telling me that the physical noise strength is not able to uniquely tell me what the logical noise strength is. And this is what we'll see in the following slide. So, here what we've done is we've looked at a lot of random quantum noise processes. And the idea here is And the idea here is essentially that when we really look at a faulty device, we know very little about the specific type of time evolution that's described, that's governing this device. So there is a lot of unknown factors and therefore, it makes sense to model it by a random CPTP map with some control on the noise trends. And here we're looking at. And here we are looking at a set, like all random CPTP maps at different noise trends given by the diamond distance. So, if you take a vertical cross-section, we are looking at all dots that are possibly noisy devices that have a diamond norm, a diamond distance noise strength of 10%. What its y-axis is telling us for each red dot is the logic. Is the logical noise trend after quantum error correction? In particular, it's the fidelity of the effective channel. And it turns out that if I look at all devices at 10% diamond distance, the logical noise trend can vary anywhere across eight orders of magnitude. And this means that if I don't know the type of device or the precise time evolution that's governing the device. That's governing the device, then if I just know the diamond distance, I can only get a very coarse estimate of what the logical point strength is. And this is just a note for the experts. You can actually see that if I just look at all devices at a fixed diamond distance and ask the question, what's the best noise I can have and what's the worst noise I can have, it turns out that incoherent noise is the worst and coherent noise is the best. And coherent noise is the best you can hope for. Now, it turns out that this is not only true with time and distance, but also fidelity, L1, L2 norms, and all the standard metrics that we come across in the literature. And this means that none of these metrics are actually good at predicting the logical noise thread. To basically come up with a constructive solution, what Constructive solution: What we asked was: if standard metrics don't work, is there still a way of accurately predicting the performance of a quantum aerocratic scheme efficiently with some experimental data that we can acquire from the device? I won't go into the details of this, but I'll give you a schematic overview of how we do this. Now, we started off by saying that really By saying that really the noise that describes realistic hardware is outside the poly paradigm. Now, it turns out that there is a way of starting with a CPTP map or a noisy device and kind of tampering with the device. In other words, tailoring noise back into the Pauli paradigm. So we basically apply certain transformations on the noisy device. The noisy device. And after these transformations, we can actually describe the noise accurately using just a Pauli channel or just as probabilistic action of Pauli matrices. And this process is called randomized compiling. So given that this process exists, we can actually assume that we can now force noise back into polyparthrite. The second step is now knowing that the noise. Is now knowing that the noise is Pauli, we want to basically learn as much as we can what the specific noise is. In other words, what are the specific probabilities of the different Pauli errors that occur in the system? And this can be achieved due to an experimental technique called noise reconstruction. Now, remember that we have tailored the noise back into the Pauli paradigm and we have The Pauli paradigm, and we have basically acquired all possible information we want to know about the noise process. The third step is to actually use all of this information to accurately predict what the logical noise strength would be, what the logical error rate would be. And this accurate prediction needs to be efficient because otherwise we're again going back to basically computing an exponentially hard quantity. Computing an exponentially hard quantity. And we basically have come up with an efficient solution for a specific class of quantum error cryptic codes called concatenated codes. And this efficient approximation is what we call the logical estimator. And this is a new measure of noise strength. And now we can basically see how well the logical estimator works as opposed to something. Works as opposed to something like a diamond distance. So it's similar to the earlier plot that we saw, where if we take a cross-section, a vertical cross-section, and look at the top x-axis, we're looking at all devices with a fixed diamond distance. So let's say 1% in this vertical cross-section. The logical error rate now varies across about six orders of magnitude. But now, if we look at a vertical cross-section in the bottom axis, Cross-section in the bottom axis. We're looking at all devices with a fixed logical estimator, and you can see how tight the spread is in the y-axis, which basically says that if I just know the logical estimator, I can quite accurately predict what the logical error would be. And here we've basically considered about 18,000 random CPTP. 18,000 random CPTP maps. And we, in terms of the actual dispersion, we get something like on average a thousand times improvement in the predictive power. So coming back to what is the relevant measure of moistene, it turns out that there is this measurable quantity called the logical estimator that possesses all the nice properties that we want to basically use it. Basically, use it to predict how well a quantum error creating scheme works. Now, what I want to say in the rest of the talk is to basically tell you that in the process of computing or measuring this logical estimator, we have basically employed a few techniques that we can actually use to not only predict but also enhance the logical error rate or the logical noise suppression. Or the logical noise suppression. The one, the first tool that we used is randomized compiling, which basically forces noise from a CPTP map into the Pauli paradigm. So the immediate question is, does randomized compiling not only help the prediction problem, but also does it actually improve the logical error rates itself? And to answer this question, we basically said, let's We basically said, let's look at this, investigate this numerically. So we can basically estimate the logical noise strength of a quantum error correcting scheme without applying any randomized compiling. And we can estimate the same quantity with applying randomized compiling. And just look at the ratio of the logical error rates. It turns out that if the ratio is greater than one, then the top quantity is actually larger than the bottom. Is actually larger than the bottom quantity, which means that after randomized compiling, we've managed to suppress the logical error rates. And to do this, we basically looked at a very popular model of errors in quantum information that are called coherent errors. And this occur due to imperfect control or calibration in a device. So, here specifically, we are looking at some kind of an unknown unitary acting on the Unknown unitry acting on the encoded state. And here the unitry is generally parametrized by some rotation about an unknown axis of the block sphere. So we can now look at the ratio of the logical error rates, but really look at the ratio averaged over the HAR measure, which is averaged over the angles of rotation that specify the rotation axis. Now, it turns out that if we look at this ratio, That if we look at this ratio, we basically get an improvement in the logical error rates compared when we use randomized compiling. In other words, this ratio is larger than one. And it turns out that you can actually arbitrarily increase this ratio or arbitrarily suppress the logical error rate by applying randomized compiling as long as the rotation strength is below some threshold. Strength is below some threshold. And in this case, we've looked at the steam code, and it turns out that this threshold is about rotation by 19 degrees. Okay, so this means that in the process of prediction, we've actually not only predicted accurately the logical error rate, but we've also shown that the tools that we can use to accurately predict can also be used to enhance the logical error rate itself. Enhance the logical error rate itself. Now, the second question is: remember that app. Yeah, so you're nearing the end of your time. Two, three minutes. Yeah. Sure, sure, sure, sure. So the final question we want to answer is after applying randomize compiling, we did extract all the Pauli error rates from the device. Can we use these Pauli error rates to actually make better statistical inference or better computer? Statistical inference or better computation of the recovery operation. And this is what we set up. This is what we investigated, and this is the last part of the talk. And here, we are basically asking if I have some kind of a CPTP map that has all these different information that encodes my CP that specifies my CPTP map. And particularly, if I look at the probabilities of poly errors, can I can? Errors. Can I basically extract just a few poly error probabilities and guess the remaining using some kind of an algorithm? And by doing this, can I basically improve the ability to do quantum error correction itself? And it turns out that we can do this. And again, we're going to essentially look at a ratio of logical error rates where we basically have. Error rates where we basically have the logical error rate computed using a recovery operation that has no information about the device versus the logical error rate with some information about the device, particularly k of the party error rates. And here we've chosen an experimentally relevant error model that describes noise on some of the IBM devices. So here we're just looking at some kind of a local Hamiltonian that specifies interactions. That specifies interactions between the physical qubits. And it turns out that by using just a few poly arrow rates, we can achieve this ratio to be about 10, which essentially says that you can suppress the logical error rate by a factor of 10 if you account for K-Pauli error rates coming from the device measurements. So, concluding my talk. So, concluding my talk, we basically asked four main questions: which is what's the relevant measure of physical noise strength? We saw that the logical estimator is a very useful measure of noise strength. And how do we compute this logical noise strength? We saw that we can use essentially the effective channel argument to basically specify the The action of noise plus quantum error friction on the logical information. And to suppress the logical error rate further, we saw that randomized compiling basically enhances the quantum error correction capability for coherent errors. And we can use some partial information from noise reconstruction to enhance the quantum error correcting capabilities. With that, I would like to conclude. Thank you. Like to conclude. Thank you.