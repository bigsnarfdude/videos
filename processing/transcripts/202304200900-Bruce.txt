I'm going to tell you about a somewhat older project that is joint with Daniel Uhrman that looks to use probabilistic methods in kind of a different direction. But I want to begin this talk with recalling three very classical and lovely theorems from algebraic geometry. So at the beginning part of this talk, k is always going to be an infinite field, whatever field you like. And throughout this talk, whenever I write x, I mean some projective variety of dimension n. So that's going to be kind of the general setup for the first part of the talk. And the first of the three theorems I want to recall is the following. So if x is smooth, Then x-intercept H is smooth of dimension n minus 1 for a generic hyperplane H inside of L. H inside of, well, let's say our variety is already puts in P to the R, or generic hyperplane inside of P to the R. So this is just the classical statement of Bertini's theorem that over any infinite field, a generic hyperplane section, like smooth variety, will itself be smooth and have the correct dimension. So this is Fertini. The next theorem that I want to recall is that Is that with the same setup for a generic choice of linear homogeneous polynomials say L zero through L n To Ln, the induced map going from X to Pn given by the L i's is finite. So it's rejected with finite fibers, say. This is of course just the statement, no the normalization. And finally, the last classical theorem is essentially a generalization. I like the thing that I can do with the normalization, which is to say that for any, I don't actually think this theorem has a name, unlike the other two, but it says for any generic choice F0, Fk of homogeneous polynomials of degree k, or sorry, degree D, the dimension of x intersect the vanishing of f0 through fk is exactly. is exactly n minus k plus 1. Differently, right, for a generic choice of polynomials, when I take the intersection with x, I get the expected dimension. I expect that each polynomial imposes an independent condition, and therefore my dimension drops by k plus 1. And so, of course, what do I mean by generic in all of these theorems? I mean that in the appropriate parameter space of things, so in the dual parameter. Of things, so in the dual projective space and the parameter space of linear polynomials, or in the affine space of tuples of degree D polynomials, there is some non-empty open Zariski subset of these things that satisfy these conditions. And I'm sure most people in the room are very familiar with these theorems. So why am I saying this? Well, I want to recall that these, a few things about this. So A, I like to think about these as being sort of probabilistic statements. As being sort of probabilistic statements for people who don't like probability, right? This is somehow saying: if you asked me, hey, I need a note for normalization and you gave me a variety, how would I do it? I would open up a call-2 and I would say, please generate linear forms randomly until I get them. And this theorem would say that, you know, I should expect that to finish very smoothly. But also, I'll point out that in these two theorems, that these are effective theorems in some sense, right? The fact that this is a hyperplane section and not just an arbitrary. Section and not just an arbitrary intersection of some other variety. The fact that these are linear polynomials gives me a lot of effective control on these theorems. So, you know, a very classical use of Bertini's theorem is to say if you want to construct some sort of curve or a different dimensional variety on x, you can use Bertini's theorem to effectively construct a curve often with controlled properties. And this is often very useful. But I'll note. But I'll note that the reason I put these three theorems on the board is that all of them fail once we get rid of the condition that k be an infinite field. So what I really want to talk about today is what happens when I, instead of working with an infinite field, I work with a finite field. So that's going to be the goal for the rest of the talk is to say, how does each one of these theorems These theorems, Bertini's theorem, for smoothness, no for normalization, and I guess I should give this a name, so let me introduce a little bit more terminology, right? So definition, I'm going to say that f0 through fk, which are homogeneous polynomials of degree d. Of degree D are a system of parameters, or maybe just parameters if I'm being sloppy, if condition star points in star points. In star or list. And so I would say they are a system of parameters on a given variety x if they satisfy this dimension in your call. But you're telling me the degree. Yeah, so I don't need the degree here, right? I'm just saying what they are. So you could, of course, ask that they mix degrees, but then the reason I want them all to be the same degree is because secretly, right, I do want to view this theorem as a generalization of no the normalization. In particular, given a system. In particular, given a system of parameters of the same degree, if they're a full system of parameters, so they cut down to the empty set, they will induce a finite morphism. And if I have different degrees, well, you could then try to say I'm going to re-homogenize in some delicate way, but I want to just mostly focus on the connection between these two theorems, so I'm going to restrict to only the same degree. But it's absolutely a great question. But it's absolutely a great question, and in general, it can be useful to allow degrees to vary. And I'll kind of point out where some work has been done in that. Well, in that case, this is like the generalization of the first theorem then with degrees theory. Yes, exactly. Other questions while we're here? So, if not, as I said, each of these theorems fails once I work over a finite field. So, from now on, instead of working So from now on, instead of working over k and infinite field, I'm going to work over, from now on, we're going to fix some finite field, FQ, just going to be a finite field with Q elements, where say Q is P to the E for some prime P. And so why do these theorems fail? Well, if you think about it, each of these conditions over a finite field is cut out by the same set of polynomials. Cut out by the same set of polynomials. The fact that the failure of Bertini's theorem is given by some vanishing of some polynomial system. So the set of hyperplanes that satisfy Bertini's theorem is a Zariski open set. However, over a finite field, it's no longer the case that an open Zariski set, even a non-empty open Zariski set, necessarily has an F. Necessarily has an FQ rational point. So, what's going on in these cases, right, is that what we prove is we say, oh, you know, the set of hyperplanes that satisfy this condition, or the set of linear polynomials that satisfy this theorem, is a non-empty open subset of the correct parameter space. And I want to find a k-point, but over an infinite field, any non-empty open Zariski set has a rational k-point. As a rational k-point. This fails over a finite field because you can, of course, write down polynomials that vanish at every FQ point, but not every FQ to the E points, or FQ to the K points. I guess that's because your parameter spaces have K points. Yes, the parameter points have K points, but not over all the sets of K points. Yeah, exactly. Okay, so over a finite field these fail, so for example, and I find this, you know, well, how bad do these things fail? How bad did these things fail? Let me give you a sense of that. So here's an example duty cats from 1999. So I want to look at some hyperplane inside of projective 2n plus 1 space. And the hyperplane is going to be given by, or the hyper surface, sorry, is going to be given by xi yi to the q minus. To the q minus xi to the q yi. It's the vanishing of this set from i, but I run from i equals 0 to n. So I'm thinking about the xi's and yi's as my homogeneous coordinates on this projective space. This is some nice, this is some nice hypersurface of degree q plus 1. And it turns out that it's not so hard to check that. It's not so hard to check that every hyperplane section of this is singular. In fact, not only does kind of the theoretical thing, but you can write down very simple varieties, a very nice hypersurface, in which Bertini's theorem just fails very complete quite completely. Failed very completely, quite completely. And I think, inspired by this failure, Katz kind of asked a very interesting question, which was to say, well, maybe hyperplanes don't work, but can we get away if I move away from working with hyperplanes to working with varieties or hypersurface? What happens if I take H here and instead of writing hyperplane, I write hypersurface? Maybe I can't always find a smooth hyperplane section. A smooth hyperplane section, but I could find the smooth hypersurface section of the crack degree. So Katz posed the question, and slightly after this, Putin proved the following theorem. From 2004, which says for every integer D greater than or equal to 1, there exists X inside of PR, say, a smooth projective grindy. H. So E is singular for every hypersurface. HE inside of P to the R of degree E less than D. So not only can you find varieties where every hyperplane section is, but for any fixed integer D, you can find some variety that is smooth and would fail the version of Martini's theorem where you place hyperplane or a hypersurface section of a, you know, of a bounded. Section of a, you know, of a bounded degree. So not only does Pertini's theorem over finite fields fail, if you want to try to generalize this to hypersurfaces, you have to be somewhat nuanced because you're going to end up with very bad counterexamples to this failure. This, I think, these two things highlight just how bad Bertini's theorem over finite fields can fail. How about these other two theorems? Well, These other two theorems? Well, I'll just mention briefly, right, that maybe you think, oh, no the normalization doesn't fail that bad over a finite field, right? We got it for 1962 that, well, if I replace the ln by polynomials of potentially arbitrarily high degree, then we can always find a finite search map from x to pn. His method, at least over a finite, Method over at least over a finite field. His method is kind of a very delicate induction. It's not at all clear if you could extract an effective bound from it. And it's also not clear if you wanted to be even more ambitious than just working from an infinite field to a finite field. But maybe I want to be very ambitious and say, what if I want to work over other base rings? What if I want to try to do nearthenormalization over the integers, over other one-dimensional things? I want to do noethonormalization. Dimensional thing, so I want to do no the normalization of families. Nagata's methods, not at all clear, would allow you to do that. So this result by Brunden. Is this X, is it somehow explicit? Oh, not at all. No. It's a pure existence statement. I think it would be fascinating to know how you construct such a thing. Cats, I find this example. This example is so simple. I think it would be much harder to write this explicitly. I will say it in a few minutes. Minutes, how Hoonen kind of gets approving this, and I'm being somewhat, I will admit right now that I'm being somewhat sly with my history of the retelling here. So I will tell you where I'm lying in the chronology here in a moment. Okay, so I've told you, well, this fails badly, this can be saved, at least in a non-effective way that I find somewhat unsatisfying, and this theorem also breaks quite badly. Also breaks quite badly in a similar way. So, may I ask a question? I was confused about one thing. So, in Bertini's theorem, you fix X. Yes. Right? In Pullen's theorem, you're not already fixing X. You're fixing the D and then you're finding a suitable X. So does that exclude that if you fix an X, maybe for a fixed X, you could still find the. This is a fabulous question. Let me write the next year. And I will point out right that, like, you are right. I want to point out that you are right that in Bertini's theorem I'm fixing x, right? But I also fixed the dimension of the hyperplane, right? So you could also think that Bertini's theorem is saying, you know, I fix the dimension of my hypersurface, then for any x, there exists such things. So I'm switching the order of quantifiers here, but you're absolutely right. So what happens if I do exactly as you say? So let me get to that punchline. So So, I need to fix a little notation, one more piece of notation. From now on, for all eternity in this talk, S is going to be the polynomial ring in X0 through XR over FQ. So this is just the homogeneous coordinate ring of the ambient projective space I'm always working in. Okay? And so what did Punin prove? Um in O4 again said if X side of P R Q is smooth and projective then the limit as d goes to infinity of Infinity of the probability that x intersects, the vanishing of f is smooth of dimension n minus 1, so it's smooth of the correct dimension, as f ranges all over all homogeneous polynomials of degree d. So when I write s of d, I just mean the vectors baked. D. I just mean the vector is based of degree d homogeneous polynomials map. So he says the limit as d goes to infinity of the probability that this inner x intersected with a smooth hypersurface of degree d is smooth. Well, he exactly computes this and he says that it's equal to a product over the closed points of x of 1 minus 1 minus q, the number of elements in my field, to the minus n plus 1, that's the dimension, times the degree of my point P. So this is some product over every closed point on my variety, every closed x. And what's the measure? Is it the? Great question. Yes. What do I mean by probability? I like to do probability as simply as possible, and so I will say there's a finite number of things that Say, there's a finite number of things in SD, and I take the uniform probability measure. So I'm literally just taking count how many things are in SD and count how many of these are smooth, and I get a rational number. That's what I should want to do here. I don't want to do anything clever. I want to be as naive as I could possibly be here. Okay? So this is basically something about conditions positive pointer independent. Yeah, so that's. Right, so exactly, I'll get to that in a minute in the proof, but let me just note. So what is this thing here? This goes by a different name. This thing here, this goes by a different name often. This is known in the number theory community as the Haseve zeta function of x evaluated at n plus 1, or at least the reciprocal of this. So if the product is running over a p in x, that cannot be. Oh, sorry, x closed. Closed points, n x. Not closed points. Right, so this is exactly the Haseve zeta function of x, which all Of x, which another way to think about this is that it's the generating function counting the number of point rational points on x that we range over different extensions, fq. And in particular, this number is always strictly greater than 0. So, right, this number being greater than 0 exactly answers your question, which is to say, if I fix x, then for some arbitrarily high degree, Arbitrarily high degree, we know that we can find a hypersurface section that has this property. How does Ponin prove this? Well, he leverages this probabilistic statement to say, you know, I can use this to cook up such examples via kind of a slightly not so hard probabilistic argument to say. I can bound error terms to get the correct thing I want. Great. I'll note that there's one other, I think, fascinating thing about this theorem, right? So, in this theorem statement, I've insisted that x be embedded in projective R space in a particular way. I fixed an embedding once and for all. And this left-hand side, a priori, right, the probability that this hypersurface section is smooth, depends on this embedding. I'm picking sections from I'm picking sections from the ambient projective space. The Hase-Bay zeta function, the right-hand side, does not depend on the embedding in any way whatsoever. And so this might ask, I think, or that there's a stronger statement lurking here, that somehow you can increase this, that I don't just need to work with a given embedding, that I can in fact work with any kind of sequence of embeddings I wrote sufficiently positively. Positively. And so, how does Poonin prove this? And I'll go through the proof quickly, and it's exactly as Frank suggests. Or at least I'll sketch the main idea of the proof quickly. Sketch. Right, and so the idea is: let's, instead of focusing that x intersect the hyper surface given by f. intersect the hyper surface given by f is smooth. Let's focus on what's the probability that it's smooth at a given point. So right say what's the probability that x intercept the vanishing of f is smooth at some point p in x, right? Well, being careful, write this one minus the probability that x intersect the vanishing of f. x intersect, the vanishing of f is singular at the point p in x, right, at fixed given point p. And now, what does it mean to be singular at this point? Well, if I fix local coordinates on x and I get a series of differentials appropriately defined, right, you know, that x intersect the vanishing of f is smooth, or sorry, is singular. Or sorry, is singular at t, if and only if, well, f of p is equal to the partial, say I fixed x1 through xn. So I take, fix my local coordinates, I get derivatives. I take the derivative of f with respect to these. Of f with respect to these. I evaluated at a point P. And I'm singular if and only if all of these conditions are equal to zero, right? This first one says that a point P is actually on this intersection, and the next four say that, right, my tangent space is failing to be the correct dimension. I think you can probably do that without choosing local coordinates. You probably can. Just like the, you know, just saying the differential is better. Yeah. I mean, you probably can, but yeah. I mean you probably can because you're you're sort of counting the number of points in a oh no that's a half minute I mean I think yeah I think you might be able to get away in the actual argument you have to be a little bit delicate because I'm sweeping some things under the rug but I think you could probably be a little more yeah I think there's a potential option I'm gonna sweep it under the rug for a minute so now what is this probability then well if you imagine that what that what could You imagine that what values could these take on? Well, it's however many elements are in the residue field of my point P. And so, right, it's, you know, I have to have each of these equal to zero, so there's one chance that I get the right number. And so this is something like, sorry, bad board work here, but this is something like one minus one, which is the number of ways I can get these to equal zero divided by the size of my field, which is. Size of my field, which is Q to the degree of P. And of course, there are exactly n plus 1 of these conditions, so I tack on an n plus 1 here. Aren't you assuming that these values are uniformly? Yes, I am assuming that. I'm assuming that all the elements of FQ, the residue field of P, are equal. The residue field of P are evenly hit by these values, and you have to check that that is going to be true essentially because you have a short exact sequence counting where these things vanish, and the map will be surjective. So everything gets fit evenly. So there's something to check, but under these assumptions. But somehow, when you're working over finite fields like this, the fact that everything has even chance of happening really just comes down to you will often have. You will often have some short, the vanishing, this vanishing condition can be phrased in a short, exact sequence, and that will exactly give you the uniform condition you want to account for. You don't just mean like if you're in chance, you also need independence of? Yeah, so I also need some independence here, which I'm sweeping under the rug. Here, independence, the independence of the partials is pretty easy to check. For the next step, what I want to then do is just to say, as Frank said, Right, is just to say, as Frank said, I can take a big independence check. So if this were independent, I could just say put it over here. Scratch my word work up too badly. All right. If x intersect f being singular at a given point, Given point P is independent of it being singular at a different point, then we'd exactly get the desired statement that we want. And then we win. But I guess I would just say then, you know, X probability X intercept. At this intersection, that's smooth. Exactly, the product we want, right? If this independence were true, you'd get this, and then this local calculation would modulo the details people are rightfully calling on, would show that we win. That said, this fails. But it doesn't always fail. So when does this fail? So you run into trouble. This independence fails, or begins to fail, when the degree of your point P is large relative to D. I don't have great intuition if I can derive you for why this is true, but you can check that it is. And so, what do you have to do? And so, what do you have to do? Well, you have to kind of use a delicate sieving argument. You somehow say, for any given D, let's consider the points only up to a given degree. And there, in that region, you check that things are essentially independent. And then you say, in the high-degree range, I'm going to get some non-independence that has some error term. And then the goal is you control that error term in terms of D. That error term in terms of D, and when you do this, you can say that asymptotically, that error term is going to be kind of not going to affect the actual answer you get. So asymptotically, you become into that. Yeah. So when you say you can check that it fails, or how do you check that? How can I show this? I mean, is it computational or is it an argument? There's an argument. There isn't like an actual example I think I can give you for where. I mean, I'm sure. I mean, I'm sure I can't think of an example of where this fails. I imagine you could write down, you know, you could write down probably a variety at two points of a high enough degree with some singular thing where you could see the failure. This is not always so hard to do in some cases, but in the argument means method you can just see it from and if there's an argument you make. What it's saying is you know the argument, but it's not so clear how to actually construct it. Actually, constructed. One thing you suggest to me about this is if you go to the geometric points, a point of very high degree has that many geometric points above it, requiring this thing to manage at all those. That's actually something you're trying to manage at more than just three points. And this may force it to be special and shit at all others. Exactly. That's a great way to do it. I think that's a great. You have to remember that these are points over a minor field. So a degree D point, if you go to the geometric algebra closure, whatever. The geometric elementary enclosure, but every R point has R points above in the electric closure. Think of just over R, you have one or two points above. So other questions quite. So this answer is, I think, right, is the best version of Bertini's theorem over finite fields. You might still argue it's not effective. I'll note that there are effective versions. Are effective versions in some ways. Enoi gave an effective version myself, and one Li proved a different effective version. And we needed those effective versions exactly as I said because we wanted to use it in a way that actually allowed us to control the resulting intersections we got. So what about the next theorem? What about, or sorry, let me skip down to the third theorem. What about systems of parameters? So what happens there? So let me just state the theorem. Just state the theorem. And the big idea, I think, at least, you know, this theorem of prudence spawned in number theorists in adjacent areas or people working over finite fields was that a lot of these failures of classical results can be recovered if we're willing to work kind of probabilistically and asymptotically at the same time. This theorem is due to myself. Myself and Daniel Luhrman was published in 2018. Right, so if x inside of PR over FQ is a projective variety, then the limit as d goes to infinity of the probability that the probability that f0 through fk these are of degree d are parameters on x. So the probability that if I pick k plus 1 random homogeneous polynomials of degree d, the probability that they intersect x in the expected dimension, right? So they intersect in the codimension k plus 1. Dimension k plus 1 is exactly equal to 1 if we're kind of submaxible, if k is strictly less than the dimension of my variety, and is equal to the, again, a zeta function plus 1 to the minus 1, if we're kind of in the maximal case here. So, asymptotically, right, this would tell us that just as in This would tell us that just as in the case of infinite fields, systems of parameters, at least sub-maximal ones, are ubiquitous, right? If you pick anything for a large enough degree, you should always just pick them randomly, and you always get a system of parameters. Are there a question somewhere? n is the dimension, sorry? The dimension of x is equal to n, sorry. For out from always, n is the dimension of x for me. n is the dimension of x for me. So I'll note that the case when k equals n can be extracted or follows from work of Akur and Kalaya from 2012. And in their work, exactly as Frank said, they kind of considered a version of Human's Bertini theorem where you're picking multiple forms and you're allowed to pick forms of different degrees at the same time. So they do something like that. So I think. So, I think this is interesting, but I think we're able to do this theorem. I want to explain with the rest of my time kind of some of the side consequences of this and what happens if we go into more detail. So I've told you what the asymptotic probability of systems are parameters, but can I tell you more? What's actually going on with this one and this zeta value? Can I explain something about them? What's actually going on here? So let me start by saying, let's mostly focus on this case scale. Let's focus on the sub-maximal case. I think when I see this theorem, show this theorem to most people, they say, oh, it's one, that's the less interesting case. I would like to know about the zeta function value. Tell me more about that. But let's hold that interest and focus on the seemingly uninteresting number one. So what can I say here, right? So if k is strictly less than n, then what we're actually Then what we're actually able to show is not only can we compute the probability, the limit of the probability, but we can actually show that the probability for d for parameters on x can actually do much better. We can actually say that this probability looks like 1 minus big O of Q to the minus. to the minus n minus k or sorry minus minus k plus 1 to the binomial coefficient n minus k plus d choose n minus k. So not only can we give you the probability, we can give you kind of the leading term in this, in the control of the error term. You might ask why Why in the world, like, what is this number here? Why is this appearing? This seems like a very, very random thing. And I think we come up with a pretty nice answer to that, okay, up to maybe this k plus 1 factor. I think we can give you a nice answer to why that appears. And it comes in the following idea. So I've given you a way to study these probabilistically, but let's return to our roots and try to study them geometrically by looking at the correct parameter space. By looking at the correct parameter space. What we're able to do is to say, right, we're going to construct some parameter space, script d kd of x, which is, you know, just going to be a parameter space of tuples, you know, f0 through fk of degree d that fail, so two d parameters. So many parameters. Ah, nice, right? So this is the sum parameter space. So if I take the k points of this moduli, I exactly get tuples of this four. And this lives inside of some big half-ine space that I don't really want to worry about as dimension, so let's just pretend it's some giant half-ine space. And then what we're able to show. And what we're able to show, we're able to show the following Jen, myself, and Daniel codimension of the script dkdx space. X space inside of this large affine space is equal to, well, okay, I'm writing equals to, and then I'm going to write greater than n minus k plus d, choose n minus k if k is less than n, and it's 1 if k equals n. So we're exactly able to bound the codimension of this space. We're able to bound the codimension. We're able to bound the codimension of this failure of things to be parameters, which is essentially what this term here is measuring. And in fact, the proof of saying what does this error term look like exactly kind of uses this codimension argument followed by a somewhat delicate induction. I'll note that this builds on a lot of other work, right? So the d equals one case of this is essentially due to Macaulay from 1916. The case 16. The K equals N case is a theorem about chow forms that you can find in PKL. And there's a few other known cases sporadically throughout the literature, but I think this is the most uniform result studying kind of the moduli of things that fail to be parameters. Okay? And that's exactly where this comes from. But let me do you one better. So now I've told you what is the actual limit, what is the error term. Limit, what is the error term? Can I tell you what the leading coefficient is? And the answer is we can. So let me state that. So I think this is probably one of my favorite parts, and that we'll see that this one here and this error term actually is carrying geometric information about our variety. So, what does it say? Theorem, myself, Daniel. Myself, Daniel. And it's gonna. Oh. How do you do that? Don't put one person's first name and then okay, great. So same setup as before. If X is some variety inside of PR defined over Q, and the dimension of X is D, N Q, sorry, is N. Sorry, is N. Then, if we take the limit as d goes to infinity of the same probability, right? Fk, 3d, parameters on x. And I divide this by, well, that thing over there, u minus k plus 1, big binomial coefficient. Big binomial coefficient. And so this is exactly going to compute the leading term. What do I get? Well, it turns out that this is exactly the number of n minus k planes L inside of P R F Q such that L is contained in X. So exactly the thing that's contributing most to the That's contributing most to the failure of things to be systems of parameters are kind of the number of planes of the crack dimension somehow. That's the main term. This is without assumption of k and n? Because we have this two k less than the n. Oh, sorry, sorry, sorry, yes. Yes, I need to assume k. That's that k. Yes. In this case. K less than n, sorry about that. Yes. And so we're able to say that somehow this, you know. Able to say that somehow this, you know, first the first error term is controlled by the number of planes on your variety. In fact, you can see this to some extent, right? You can take, and I can show you after the fact, that if you take a variety that has a lot of planes and one that has a lot of lines and one that has few lines, and you ask to generate systems of parameters, you'll find more on one than you will on the other. Just kind of experimental. And this is interesting. I think this is a fascinating thing. In fact, we can do you. In fact, we can do even a little bit better in that we could exactly describe this probability, if you would like, as being some sort of infinite generating function where the kind of term, the parameters all have like q to something in them, and the coefficients out front is exactly counting sub-varieties of a given dimension n minus k in a given degree. And as the degree goes up, the amount they contribute to that infinite sum goes down. To that infinite sum goes down. And the idea behind that exactly somehow is that, in fact, we never get an asymptote, we never get any sort of independent statement for just parameters. It fails very explicitly. And so instead, what we're able to do is do a very naive thing and just say, let's do inclusion, exclusion over every sub-variety that might fail that cause this to be a system of parameters, right? To be a system of parameters, right? Something would fail to be a system of parameters if the collection of them vanished on a sub-variety of too high dimension. And so we're kind of including, excluding over all the sub-varieties of failure. And it just so happens by a very delicate Hilbert function argument, you can show that the ones of higher degree are going to contribute less and less than this. So I think this gives a very beautiful picture connecting kind of Connecting kind of the previous work, kind of a very delicate explanation of kind of a geometric argument for what this probability is, and a very nice probabilistic argument to explain what the geometry controlling it looks like. And so what are some things that we can get out of this? Well, the first thing I'll note is that by using this theorem, we're able to prove an effective nother normalization, right? So same set as before x is r q is smooth. Q is smooth, or sorry, this is just a projective variety of dimension KN, right? Then this is true, and we take D, take some very, let me say this, we have some complicated inequalities. It's going to be the max of D and D divided by Q to the N. That's great. To the n has to be greater than the degree of x, and d has to be greater than or equal to log base q of the degree of x plus log base q of n plus log base q of t. So, two kind of nasty set of inequalities if this is true. So, if this is true, then there exists S0 to Fn of degree D to the N plus 1 that induce a finite morphism from X From x to pn. So, in particular, we get some effective bound here. I'll note that if we fix the degree of our variety, then as q goes to infinity, you can check, just help one style argument, that d equals 1 will work in the limit. So, just as we'd expect, kind of large finite fields would behave the same as infinite fields. We kind of see that here. We kind of see that here when we're able to take effect. So I'm confused about the first inequality. So when is DR reputed in not smaller than d? Max slow is D. Max. Okay, yeah, that seems right. I don't know why I wrote that in the notes. So let's just say D is larger than the degree. I forget. D, max of D in that was larger than the degree. That was already. I think, sorry, yeah. So, what happened? Okay, there's a. So, when I say variety here, I honestly mean an irreducible variety. And we have a slightly more general statement that gets a little more nuanced, I think, that we have to have one. But I put this remote. And so, with my two minutes remaining, let me not write anything else on the board, but say, why is this? Why are what else can you do? Why is this useful? Well, so I've told you. So I've told you that we've measured kind of probabilistically the systems of parameters over finite fields, and I've told you we've proven an effective no-the normalization over finite fields. It turns out that if you wanted to do no normalization over the integers, say, say something over z, you can work fiber by fiber and use these results in kind of an infinite Chinese remainder theorem and some other clever work to say. Other clever work to say you can also prove no thermonormalization over the integer, which, as far as I, which we originally thought was brand new, but we later found out that had also been proven one or two years before us by a few different authors, Faber and with a group including Faber. And so this somehow allows you to extend to working it over other base strings as well. Working it over other bass strings as well, and working hyper by hyperlink. And I'll end here, and thank you all for listening. I think you can stop with the closing theory. Then, does anyone have any questions?