Is full screen now? Yes, it is. That's perfect. Thank you. Okay, sorry about that. All right, so in this talk, I will talk about a new work on graph neural networks that use interacting particle systems. So all right, I mean, and now I mean, uh the probably the most uh puzzling word for for I mean computation is really I mean, computation is really so-called AI for science. So there has been lots of excitement, particularly, I mean, a couple of years ago, the deep mind in Google announced that they can use AI to really understand the structure of proteins. And that work was selected by Science Magazine as the Greeks Rule of the Year in 2021. And the Nature Magazine said this work. So this work is the, I mean, it changes everything, right? It's the biggest breakthrough in system biology. And more recently, I mean, AI was even used to prove mathematics theory. So there's certainly, I mean, there has been a tremendous amount of excitement in artificial intelligence that was used for science from. And well, so far, the AI has been most successful for image processing and natural language processing. Processing and natural language processing. So, typically, the problem can be defined in a Euclidean structure. For example, this picture on the right-hand side, where you have the pixels that's defined on a regular grid. But there are lots of problems where the data defined on so-called non-Euclidean structure. For example, you have this graph structure data, which you need to describe it by nodes. Described by nodes, edges, size varies, for example, a protein structure, molecular dynamics. And so this Eulerian structure is not enough for your AI or deep neural to really learn the geometry of the underlying structure of the data. So, for example, in the case of a node classification, very often these nodes are connected by graphs. So, therefore, this so-called So, therefore, the so-called graphic neural network has been introduced to deal with these kind of problems, where you have labeled nodes with edge information and use this information to infer unlabeled nodes. And there has been lots of applications that need this kind of technology, for example, in drug repurposing knowledge graph, recommended system for e-commerce, movie review, citation works, book review, public opinion analysis. Analysis. And so, this idea of a graph neural networks was introduced around 2005. And the basic idea is that suppose you have a graph, and here's G with nodes V and H is epsilon. Okay, here we assume it's undirected graph. So basically, when you introduce a neural network, you have to utilize the structure. You have to utilize the structure of the graph. So, therefore, here, suppose H is the feature you want to learn. All right, it's some kind of feature. So, here you have layers of neural network. Here, the L, subscript L denotes the L's layer. So, this is the iteration from L's layer. So, the feature is denoted by HL. And you want to move to the next layer, which is HL plus one, where sigma is the activation function, which is usually. Is the actuation function, which is usually a non-linear function. And here, this A, this matrix A basically defines these edges, this sort of connectivity between all these nodes. So this is one of the earliest graph neural networks, which now has been getting very popular in the machine learning industry. And very often, a graph neural network can be characterized. Neural network can be characterized by so-called neural message passing, where if you start with again, suppose you have this XI, XJ are the nodes, nodes I and nodes J. All right. So you start at the K minus ones layer. You want to move the K's layer. So here this phi K and gamma K are sort of an activation function. And this box are so-called is defined It's defined, it's kind of so-called differentiable node permutation invariant functions. For example, it can be like the sum and the mean or the max. All right. And so that's basically a neural network which sort of pass the message from the k minus ones layer to the case layer through this kind of connection, a relationship. All right, and you're going to put this relationship into a loss function and you train it. And there has been some representative graph neural network. So, this is all called a convolutional graph convolutional network where you basically have this A, which is an adjacency matrix that defines the connections or relations between our different nodes. And so here, you'll sort of normalize the adjacent matrix A by this D. A by this D, which are so-called degree matrix, which are summations of the rows of this adjacent matrix. Okay, so for the nodes i, xi, and you add all the j's which has connections with nodes i. All right. And here theta is a matrix which you will be trained by your neural network. And typically, a graph convolution operator, if you take, for example, a spectral discrimination. Spectral discretization, then it can be written. It's very much like you have a diffusion operator if you discretize it. It can be written as a matrix multiplied by a feature vector. So it can always be written in this form. Here, the only trainable parameters are this matrix theta. Okay, so this is one of this graph convolution network. Another one is called graph attention network, which actually also train the She actually also trains the adjacent matrices. So, here at Rij, the adjacent matrices will be trained by this activation function. Here, RELU is the rectified linear unit, which is actually a function of x. It equals x when x is positive and zero when x is negative. And here, the leak key basically instead of an active on the on the negative side, you also use another linear function. So, the piecewise. Another linear function. So it's a piecewise linear function. The reason people don't use regular rule is because on the right-hand side, the derivative is zero. So if you use, for example, a stochastic green descent method, you may get zero gradient. And if you have zero gradient, of course, the green descent method stops there. So people usually prefer the use of wiki remove functions. And so here you have to change this adjined matrices. This adjacent matrices, or in this so-called graph attention network. And very often, actually, you can write this kind of a neural network by some kind of a nonlinear PDEs, called a graph PDEs, where x is a feature vectors, and here you have a divergence, sort of like a nonlinear diffusion equation. So you have g function, which comes from these adjacent matrices, and they have this gradient x. So you can understand very often the Can understand very often the structural evolution and properties of this graph neural network by understanding this nonlinear differential parabolic type equations. There's a common problem in this graph neural network is so-called over-smoothing. And the problem is over-smoothing is that very often, if you, for example, if you want to do a node classification here, we have example of two different nodes, right? There's grow, I mean, there's a blue ones and I mean, there's blue ones and orange ones. So after just a few layers and they cluster into one feature. Okay. And then you cannot distinguish different features. You cannot classify different features. Basically, they only recognize one feature. And after some iterations. So therefore this because of this, the graph neural network can only be trained with very narrow layers, typically three layers. Typically, three layers. All right. But we know that deep neural network is very powerful. If you have deep layers, you can learn a lot, very complicated features. But unfortunately, because it's over-smoothing problem, graphene neural network only works for very shallow layers, just a few layers, like three layers. So, how do you define this over-smoothing problem mathematically? Suppose here we have a graph, all right, the G, and you have these nodes. The G, and you have V is the nodes, epsilon is H, and this N is the total number of nodes. So you can define the so-called division energy which is here. So the Aij is the adjacent matrix. All right, so this quantity is called division energy. So if you define a gradient flow, you take gradient of E and define this gradient flow, and then it looks like. And then it looks like it's basically a particle system, right, with AIJs at the JSON matrix. And for this particle system, I actually can learn its dynamics. You can analyze the dynamics to understand this over-smoothing problem. So over-smoothing, basically, this is definition, mass market definition over-smoothing. Essentially, it means that duration energy decays exponentially as you advance. Exponentially, as you advancing layers, you can view these layers as time, all right, as you're evolving time from this very familiar particle system to our community. In the long time, solution actually converts, you form some kind of consensus. So when this dilution energy decay exponent to zero, essentially Xi equals XJ, they all becomes basically, they become one cluster, one particle. One particle. So you can only learn one feature. And here is a plot of showing that indeed this green line shows that this energy indeed decays exponentially. And actually, of course, it's easy to prove that, for example, if you use this particle system, then you can prove that, suppose D is so-called degree matrix, but it's a diagonal component are basically the summation of each. Of each row of this detained matrix, then this dirtial energy would decay with a rate e to the minus lambda minimum square t, where lambda minimum is the minimum eigenvalue of this matrix D minus A. And here we introduce a new message passing particle system which avoids this over-smoothing problem. And the idea actually, you And the idea actually, you can view the reason you have this consensus is really because the force is attractive force, right? When you have attractive force, eventually all the particles are attracted into one particle, form a consensus of locking. So our idea is we add a repulsive force. So some particles will be pushed away from each other. So that's basically the idea. Okay? So both attractive and the repulsive force. And this actually was inspired by early work I did with the By early work I did with Dee Fang, who is a poster at Berkeley, Seoul Yeehai at the Seoul National University a couple of years ago. And of course, we know every big industry in this community in the last 20 years. There has been lots of work that has been denoted to study so-called self-organized agent-based models, swarming, flocking, consensus, synchronization, and so on. And actually, all these phenomena exactly correspond to this over-smoothing. Okay? And of course, there's a lot of And of course, there are lots of familiar names, that is certainly a non-exhausting list of people who have made important contributions to this field. And so a few years ago, I was curious why everybody is studying consensus. There's a lot of cases where there's no consensus. So in Chinese, that's what's called Wu Yilei Ji range. If people are divided by groups. For example, if you go at this election in the US, there's always the blue areas and the red areas for blue is for. Red areas for blue is for Democrats and red for Republicans. In the natural, and you see, I mean, these different brothers flock into different groups. Okay. And also in machine learning, people do this classification problems where you do have different clusters instead of one cluster. So with DFAM and the Song Yi, and so we introduce so-called by clustering cook SML model by introducing the so-called repair. Introducing the so-called repulsion, repulsive force. So, here, if you consider the second-order particle system, and basically Newton's second law, right? I mean, so if you are the same group, which are this group of X, okay, this X is the position V's velocity. And if you all belong to the X group, you have a repulsive force, that's the first term on the second equation. And if between X and Y, you change the sign of your this. Of your this communication function. Similarly, for the white group, which is within white group, is attractive, but PT next to white group is repulsive. And we also add this so-called rare friction term, which is also an added count term if you use terminology for each transition. Then, actually, what is the definition of bi clusterings? Well, first, right, the same group, the distance between different particles of same group should be bound. Particle of the same group should be bounded. And in the long time, the velocity difference will go to zero. They all move with the same velocity. Then that forms one cluster. However, the difference between the velocity of different clusters has a minimum which is strictly positive. Okay? So with these different velocities in two different groups, they will separate eventually, form so-called bi-clusterings. Called by clusterings. All right, so that's what we so basically just borrowed this idea and use it in this graph neural network. So the idea is attractive force will form the molar cluster, right? That corresponding to oversmoothing. And this repulsive force will push them away, all right? However, there's a problem with the repulsive force. Once I have repulsive force, then the energy becomes unbounded. Because all these particles will push away, the velocity will be sub. This particle will push away, the velocity will be separate. Eventually, I mean, they go away too far. I mean, you that energy becomes unbounded. But add this Allen count term of really friction and it allows us to bond the energy, then you can prove those theorems. Without this Allen count term, actually, you cannot prove those theorems of this by cluster. So the Allen count term will actually allow the particle to stay on these two equilibrium. We know this one and minus one are stable equilibriums. Stable equilibriums. The ones particles reach this neighborhood of this stable equilibrium, minus one and minus one, they will stay there. Those are attractors. And that allows you to bond the energy. And so this is what we can prove, right? Essentially, you can prove that you can prove this VC at the center, the average velocity of first cluster and W is the other cluster. So you see that the minimum distance. So you see that the minimum distance between these two velocities has a positive strict positive lower bound. So that's the proof of the by clustering. So now we extend it to this graph neural network. So again we modify this dirtial energy by first okay before we only have this Aij term now we minus the beta ij term and then we add this W function which is the potential for for the Potential for the Allen Khan potential. Okay, Jinxbog-Langda type of potential. Now, if you take gradient this phi, then you get this particle system which has this beta term, which will give you this repulsive force, and then you have this all and count corrections. So, of course, I mean, this equation is very familiar to us. And of course, when you apply, so this uncount massive passing, of course, this actually. Passing, uh, of course, this act of the feature is a vectors. So, basically, we do this operation component-wise or channel-wise in the language of a graph neural network. And here we can basically the beta we can fix to be a constant. Beta is one. And this R and delta will be trained. Those are trained parameters. So, we can actually build this island con massive passing into Car message passing into standard graph neural network package. For example, if you use graph convolution network, so what we do is before they have this, they don't have this beta term, they don't have this L and count term. So, okay, so we add is minus beta term and L and count term. And this is case where this adjacency is given, and the training parameters are both alpha and delta. Okay, and if you use put the alpha particle sum into this graph attach. A particle sum into this graph attention network, what you do is that again put this minus beta term into the adjacent matrix and count term, and use the same adjacent matrix, which will be trained still by this Ricky rule function. And thanks to this Allen Khan trapping force, you can prove that once these particles are near the neighborhood of one minus one, they will stay there. They cannot go to jump to the other. To the other well, or stay in the same well, and we can actually prove the following result. Okay, basically, we can prove by clustering, not surprisingly. Essentially, the particle that stays group one, well, the distribution distance will be bounded. Okay, they're always bounded. The particle position within the same group will be distance will be bounded, and the minimum distance between the position. Between the positions of group one and group two, well, there's some constants. After some time, okay, they will stay away. Group one and group two. The minimum distance between group one group two will be a part of a positive number. So it means that these two particles, two clusters are well separated. And so we can prove basically this is our particle systems. All right, we can prove first this duration energy. First, this duration energy is bounded of all time. And we can, under certain conditions on this matrix, all right, and we can prove that you form this by cluster by clusters, which means they will be well separated. Okay. And so this is in the last proposal, you see that group I, group one, a group two, the distance for every ij, the difference between the position of a group one and group two. Between the position of a group one and group two will be strictly positive. From this, of course, you get that the result energy will have a lower bound because the Russian energy is a summation of all ij's, this guy squared. Okay, so therefore, this energy will have strictly lower bound, will never go to zero for any time. So, this is the example. So, the first on the top is the standard graph. Is the standard graph convolution network? You see, it forms consensus very soon after a few iterations. But in our Alan Khan particle system, you see eventually form the different, you get these two different features, one the blue and one green. I mean, the blue, all the broadcasts form uh one or two clusters, while the orange one form their own clusters, but this they are sep the the orange cluster and the c uh blue cluster are separated. Group class are separated. And this actually shows these two graphs: two groups eventually form separation of two groups, even though initially it started mixed, they are mixed. And of course, we have this so-called neural ODE. You can use standard ODE solvers along Qatar. And here we use a fifth solar method, which people like in this community. And this graph shows it is truly energy actually. Durition energy, actually, the green curve are the original method. Okay, you see the decays exponentially while ours sort of stays bounded above a certain constant. And we do make comparisons for different data set. And it's a homophilic data set that's the right table and the heterophilic data set on the left. So these are standard tests. And here are the last lines: Alan Commission present with graph conversion network on the left. Graph conversion network on the left. Here, the color means the following. If it's red, it means you're number one. This is successful rate. Okay, successful rate. If red is number one, blue, number two, and the violet, number three. So, in these three typical tested textiles, Wisconsin, we're number one in two cases, number three in the Wisconsin case. Where for the homophilic data sets, and we tried both. And we tried both ACMP with the graph convolution network and with graph attention network. And in the graph convolution network, we get number one in two data set and number two in two data set. Number three in one. Why this graph attached network with this anonymous mass passing were number one in two sets and number two in one set, number three in one set. So overall, Stream one set. So, overall, I mean, average it outperforms all these previous network graph neural networks. And all right, so to conclude, basically we develop a new method passing method based on this Allen-Kahn particle system. And this new scheme treats graph learning as a particle evolution and try to explore flexible design of particle equations. So, the basic idea is that you use this. So, the basic idea is that you use this repulsive force to form separate groups and use Alan-Kahn term to bound the energy, bound kinetic energy. So, you can prove rigorously that you do get this separation of different clusters. And numeric experiments show excellent performance for all this standard test case compared to the previous neural network. So, with that, thank you very much. So, this is a reference if you are interested. So, thank you very much. Interested. So, thank you very much. Thank you very much for the nice talk. Do we have any questions from the audience in person or from Zoom? Okay, so I have a question. I can get started. So, yes, so you have this nice result. Yes. So you have this nice result of by clustering. Is it easy to extend this result to the clustering with like more than two clusters? Or is there any different? Sure, sure. Here, of course, I mean, we prove the byclassing, but of course, in reality, actually, they form. And this is 2D pictures. Basically, you see, we get four clusters. See, we get four clusters. If I have high dimension, I mean, you do it through this component-wise, you form different clusters. But of course, if it's all green here, we have four clusters, but I mean, you can really separate these two different colors. When you have more colors, you will naturally form more, more, more, more, more clusters. I see. So, yeah, for this vector, this vector system, I mean, you do this outcome component-wise, you will form each component sort of sort of. Form each component from different clusters. I see. Yeah, so I think the over-smoothing issue is like a really a big issue in GraphNeur network in general. It's a major network that people work very hard. This recent result by Sidmissher with some big shots from DeepMind. They use the waiver questions to solve. They use the inst uh wave equations to solve this because the wave is unstable, so the argument is if it's unstable, it will not s stay in one cluster for so that's another I mean the people a lot of recent works that focus on solving this over smoothing problem. I see, but we find I mean this is a part of the system that many people in the audience actually have have made important contributions. I mean actually can be used and for this uh machine learning problem. So it's uh for that I find So it's uh for that I find like it quite interesting. Okay, great. Thank you so much for your talk. Uh it's already pretty late from China. Thank you very much for like giving us this nice presentation.