So, let's go ahead and get started. So, today, what I'd like to do with you is introduce a map which I'll call Kappa that acts on a finite semi-distributive lattice L, and I'll define what that means. And then to invite you to study this map, but in a really different, in a different context from where it's defined in something called the Defined in something called the core label order of L. So, what we'll do is we'll kind of start by setting the stage. We'll define what a semi-distributive lattice is in terms of an edge labeling, and then describe this map kappa in terms of that edge labeling. And kind of everything will be built by labeling the edges of these special lattices. And then at the end, I want to connect this back to Crevora Complement. Cover a complement on non-crossing partitions. So that's roughly like kind of the map of where we're going. And as I talk, I'll try to have lots of examples that we're doing together. And please interrupt anytime you have questions. I also think I do a pretty good job of keeping track of the chat, but in case I miss something or feel free to interrupt. Okay, so what are our definitions here? Definitions here. We'll start with the basics. A lattice. So, a lattice poset is a postet where every pair of elements has a join or a smallest common upper bound, and every element has a meet or a greatest lower bound. So, I'll be focusing a lot of what I say on the join, but pretty much everything I'm going to say also could equally apply to. Could equally apply to the meat. I'm just kind of making a choice because there's going to be a symmetry in the lattices that I'm talking about. And so I'll talk a lot about join irreducible elements. So an element is join-irreducible if whenever you try to write it as a join of some set A, what you have is that J actually just ends up being inside of this set A. So you could imagine, like, how would that happen? Oh, yeah, sure. That's a great. How's that? Is that a little better? Give me a thumbs up if you can. Yeah. Yeah, no problem. Okay. So how would that happen? Like if I had, say, A and B below my J here, and I wanted to say, well, okay, J is joint irreducible. If I take the join of, say, A and B, If I take the join of, say, A and B, it should kind of get stopped somewhere before it hits J. And so what you end up having is that in a finite lattice, anyway, being join irreducible is the same as saying there's only one way to go down from your element J. There's only one lower cover. And you kind of have the analogous thing going on with what are called meat-irreducible elements. Okay. So let's go ahead and do a small example here just to make sure we're on the same. Small example here just to make sure we're on the same page and see who's paying attention. Which one do you think, if any, is a lattice? You can answer left or right or none or both. Take a minute, decide which one here is a lattice. So every pair of elements should have a join. Every pair of elements should have a meet. I'll give you a minute. You know, there's no rush. I have, my talk is a little short here. So you can take a minute and decide. Decide and type it in the chat. You can type it directly to me too. You can direct message me, and that way you're not sharing your answer with the rest of the group. All right, very good. Yeah. Okay, so I have some experts chiming in. Some experts chiming in, and some people who I think are maybe a little newer. All right, so good, good, good. I think we have consensus in the group. So, right, the left one is a lattice. Let's go ahead and compute a couple of like joins. The join of B and C. So I look at the common upper bounds and I need to find the smallest one, but there's only one common upper bound, and it's the top thing. So B join C is one hat. And also, if I were to do A join B, it's also one hat. And this thing is symmetric, like it's sorry, it's self-dual. So if I really only need to check that joins exist, meets will also exist. But on the other hand, this guy over here. But on the other hand, this guy over here, not a lattice. So, for example, A join B has two common upper bounds that are both minimal. And they're these guys, both C and D. All right, very good. So we're kind of comfortable with what it means to be a lattice. We're going to focus on semi-distributive lattices. So semi-distributive lattices satisfy a weakening of the distributive law. The distributive law, and that is these two implications. I don't want you to memorize this, but I do kind of want to give you the idea of where this is coming from. So let's just look at that first implication. If x join y is equal to x join z, then you can kind of factor the x out of the x join out of an equation with y and z. So that's what you've got here in the then part of the state. Here on the then part of the statement. If I put the x back into this equation, what it looks like is x join y meet x join z. And we know from the if part that those are both equal to each other. And so that's why we're saying that semi-distributive would mean that this kind of compound join is equal to x join y, or I could have written x join z. And the second implication is just where I've And the second implication is just where I've switched the roles of the meat and the joint. So there are many important examples of semi-distributive lattices, like some of my favorite lattices, the Tamari lattice and Cambrian lattices. Any distributive lattice is also semi-distributive. And then a bunch of examples that come from kind of more geometric things like hyperplane arrangements. And in the world of representation theory of quivers, lattices of torsion classes are also semi-distributive. Also semi-distributive. So, besides knowing that there are lots of interesting examples of posets which are semi-distributive, what I think makes them special is that they kind of have something like a prime factorization. So, every finite lattice is semi-distributive if every element can be written uniquely as a join of join irreducible elements, and this is called. And this is called, I'll call this the canonical join representation and shorten that in the slides later as just CJR. But it's like a prime factorization. And then dually, every element also has a kind of unique factorization in terms of the meat called the canonical meat representation. I'm going to sort of focus on this first one here. But like I said, I could be talking about the meat too. I'm just sort of Talking about the meat too. I'm just sort of choosing one of them to focus on. Okay. So I want to give you an idea of this and try to make an analogy with the prime factorization we're used to from number theory. So the idea of this canonical join representation for, and I'm going to do the prime factorization or the canonical join representation of an element W, is that this should be a kind of That this should be a kind of minimal way to write W as a joint of some set A and minimal in kind of two ways. First of all, we want the set A to be kind of as low as possible in our lattice. So if we imagine here's our lattice and W is kind of on top, we could look at lots of different subsets where if we take the join, we get W. And my canonical join representation should be as low down as possible. As low down as possible. And then the second way in which we want this to be sort of minimal is sort of more obvious. We want the set A to be kind of as small in size as possible. So if there's anything extra we could throw out of that set, we will do that. And sort of like the prime factorization, you know, prime numbers are their own prime factorization, a join irreducible element is its own canonical join representation. Canonical join representation. And every element that appears in one of these canonical join representations is join irreducible. The bottom element, we'll say, is kind of like an empty join, and that's its canonical join representation, just like the number one has sort of an empty prime factorization. And if you take any irredundant join of atoms, so what's an atom? An atom is an element which covers the body. An element which covers the bottom element of our lattice. Any irredundant join of atoms is a canonical join representation. You really can't get any lower than atoms. So one way that this canonical join representation is really different from the prime factorization we know from number theory is that not every subset of joint irreducible elements gives you a canonical join representation. Join representation, which is sort of annoying. It makes it more difficult to compute. Let's go ahead and do an example and try to see this together. Okay, so here's going to be my running example, the lattice we saw earlier. So let's do the canonical join representation for the top element. I can kind of do it quickly. I already told you that the join of atoms. That the join of atoms is automatically a canonical join representation. And we saw earlier that if I take the join of A and B, it's the top element, right? So the canonical join representation of my top element is just A and B, my atoms here. Now, earlier I said that not every join of join irreducible elements is a canonical join representation, and we can also see that in our example. Can also see that in our example. So, B and C are both join irreducible. If I take the join of B and C, it's also the top element, but that's not the canonical join representation. A join B is the canonical join representation. So this is all fine because it's a nice small example, but right, if you have a really large lattice, it gets difficult to compute. And so, this is a problem that I was running into when I was working on my thesis. Actually, computing the canonical join representation can be hard. So, I developed an edge labeling that would read off the canonical join representation. And here's how it works. If you have a semi-distributive lattice and you want to compute the canonical join representation for an element x, you look down by a cover. Down by a cover and find the smallest element, which is also below x, and whose join with y is again x. So the kind of picture I have in my head is I go down, I keep going down, and every time I'm going down from x, I'm checking that the join with y brings me back up to x. And I know I've hit the smallest thing if when I go down by one. thing if when I go down by one cover, when I take the join, I actually hit Y instead of X. Get this kind of rectangle here. And that's going to make my, this is what I will label my cover relation X covers Y by. So let's go ahead and do that in our small example. Ahead and do that in our small example. So, I'm going to remind you: we're going to label the cover relation by the smallest thing. Or, when I take the join with my cover, I get back to my element X. So, let's just look at our top dot. Let's just look at our top guy. If I go down by the cover relation, let's try B. I want to take, I want to check what is the smallest thing whose join with B is still one, and that is going to be A. And so I'll label this cover relation with A. And then similarly, if I go down by a cover relation to my element C and try to find the smallest. element c and try to find the smallest thing whose join with c is still one right if i go down to b that's fine and if i go down one more cover now i'm below c so i wouldn't get the join equal to one and you can see that doing this procedure is also recording my canonical join representation so i'll go ahead and fill in the rest of the labels here Okay, yeah, there we go. Those are our labels. And you can see, I'll write it here, the labels on my lower cover relations, which let me write that as like cove down. That's exactly my canonical join representation. Okay. So, a couple of observations. If L is not semi-distributive, this might not work. You might not be able to find kind of a smallest element whose join with this element Y I've been talking about. You may not find a smallest thing. You could find sort of multiple minimal things. But when the labeling is well defined, you're always labeling by joining irreducible elements. Always labeling by joining irreducible elements. And this theorem here is just exactly what we observed in the example. The labels on the lower covers of an element tell us our canonical join representation. What's kind of surprising is actually this labeling, even though it's in terms of join irreducible elements, it also tells us about the sort of dual labeling in terms of meet irreducibles. So you, if you So, if you focus instead of on lower covers, if you focus on upper covers, you can also read off your canonical meet representation too. Okay, so let's go ahead and play a game. So a little bit of notation, which I already sort of introduced. Cov down is the set of things covered by my element X, and then cove up is the set of things that cover my element. Things that cover my element x. And then when I put lambda in front of that, that just means I'm reading off the labels of those covers. Okay, so a couple of facts. Let's say I have a subset of possible labels. I don't know whether they really label some element, the cover relations of some element. Well, if they do, there's at most one element with lower cover relations. With lower covers labeled by my set A. And similarly, there's at most one element whose upper covers are labeled by this set A. And the existence of either X or Y is sort of equivalent. There exists an element whose lower covers are labeled by the set A, if and only if there is an element whose upper covers have those labels. So here's the game. And this is going to be the definition of our Kappa map. Given a semi-distributed lattice with the canonical labeling lambda, we're going to take an element X, look at the covers going down. We'll see, okay, what are the labels of those covers? Labels of those covers. And we're going to find the unique element of my lattice whose upper covers have precisely the same labels. Make that a little darker. There we go. So in my picture, kappa bar of X is Y. And the facts above sort of imply that this is a great question. The labeling implies that your lattice is join semi-distributive. And so if your lattice has this labeling and the dual, where you pick up your lattice and turn it upside down, Where you pick up your lattice and turn it upside down, if that lattice also has this labeling, then your lattice is semi-distributive. That's a really good question. When your lattice has, yes, there is. And I can answer that question. Yeah, sorry, that's a really good question. I can talk a little bit about that at the end of the talk. At the end of the talk. Sure, I can read out the questions. Yeah, so the first question here was whether the labeling implies that your lattice is semi-distributive. And the second question was asking if there is some kind of classification theorem for semi-distributive lattices in the sense that there is for distributive lattices. So, both really good questions. I'm going to save that second one until the end. Okay, so when your lattice is semi-distributive, this kappa bar map is a bijection, and so you can apply it over and over and kind of study its orbit structure, look for home AC, all the sorts of things we like to think about at this conference. And people have done that. So, I want to tell you a little bit about what we know, but let's maybe do an example first. I almost forgot that I had this example. All right, let's start with. All right, let's start with the top guy. If we apply kappa bar to the top element, maybe I'll let you guys answer. I'll wait like 30 seconds. What do you think kappa bar is when you apply it to the top element? Remember, what you look for is what are the labels going down? And then I want to find the unique element whose labels going up are the same thing. Yeah, good. Yeah, awesome. So, a bunch of experts, like some people I think are who are familiar and some people less familiar are answering. So, that's really nice. Very good. Yeah, it's the it's the bottom thing. And now, if I apply kappa bar to the bottom, right, very good, very good, Jean. Yeah, you're gonna go back. Very good, Jean. Yeah, you're going to go back up to the top. So, the kappa bar of the bottoms may be trickier. You have no labels going down, so you look for the unique element who has no labels going up. So, kappa bar has an orbit of size two here. And let's just do a little bit of sort of checking some statistic on this orbit. So, what if we add up the down degree of each element here and divide by the size of the orbit? So, my orbit has size two, and then the down degree. And then the down degree of the top is two, but the down degree of the bottom is zero. Okay, so I get one. And then there's another orbit here. All of the other elements are in an orbit together. So if you do kappa bar of A, so that's this guy. Look, okay, the down label is labeled by A, so I need to find a unique thing with up label A. This might be a little harder to see. That's B. Very good, exactly. Kappa bar of B. Okay, my down label is B. I need to find the unique thing with up label B. Very good. And now, I mean, I told you that these are all going to be in an orbit together, kappa bar of C. So I have seen my download. Kappa bar of C. So I see my down label is C, and I need the unique thing whose up label is also C. And that's just going to send me down this edge here. Yeah. So here my orbit has size three. So let's go ahead and do the same computation from before. And then the down degree of all of these elements is one. They're all joint irreducible. And there are three of them. So again, we get one. That's quite nice. One. That's quite nice. So, this would be, I guess, what we're observing here is a homomacy with respect to the down degree with the down degree statistic. And this is something that people have noticed in a bunch of different places. I actually talked about this in the context of torsion classes the last time we had this conference. When L is distributive, this cap. When L is distributive, this kappa bar map is row motion. So that's where the title of the talk comes from. And then there's way more interesting stuff that people have done. So Nathan Williams and Hugh Thomas have shown that when your lattice is trim, which is relatively easy to check for semi-distributive lattices, then kappa bar can be computed using toggles. And then Sam Hopkins, who I saw earlier, but may have Who I saw earlier, but may have, I'm not sure if he's here, has shown that, has used this toggling to study coincidental down-degree expectation, which was first observed by Tenner, Reiner, and Young. And then Hopkins proved it in a lot of other contexts, including minuscule lattices and certain intervals of the weak order. And, like I said, so I've been sort of interested in this stuff in the world of quiver representations. So, in the world of quiver representations, what you do is you take a subset or a subcategory of modules and you order them by containment. There's, okay, yeah, this is a good point, Tari. There's a kind of dual thing. It's really more convenient somehow to do actually like the inverse of count. Do actually like the inverse of kappa to see the row motion, but I can run a computation with you if you want. Sorry, Dari was asking about the row motion fact that I cited for you. It really does work, but I think perhaps it would have been better for me to state the kappa bar map as sending up labels to down labels. In the context of, oh, yeah, okay, thanks. Oh, yeah. Okay. Thanks, Sam. In the context of torsion classes. Emily, can I ask a quick question? Sure, sure, go ahead. About the remotion. I mean, Kappa is mapping elements to elements, not, I mean, are you thinking that the element is defining a downset? Yeah, so in a distributive lattice, every element is really an order ideal, right? Ideals, right? I see, in a sense, okay, yeah, okay, yeah, and so you think about the order ideals in terms of anti-chains, and um, and that sort of is helpful for seeing this. But I would be happy to run through an example with people if you'd like. And then there's a very, there's like a very active conversation in the chat about what I've said. And I think maybe the confusing point here is that. Confusing point here is that the example of my semi-distributive lattice is not distributive, if that is helping to clarify things. But I thank the other people in the group for chiming in. Can I try and speak for them? I think I sure. Yeah, go ahead. Like, take the full Boolean lattice. Let's say on three generators, A, B, and C. So it looks like the bottom element has no down degree, you know, and it has A, B, and C as the up labels, whereas the top C as the up labels, whereas the top element has A, B, and C as the down labels, and the empty set as the up labels. So it looks like Kappa would swap them, which would be an orbit of size two. But that seems confusing because row motion on the Boolean lattice of size three doesn't have an orbit of size two in the usual okay. And the Boolean lattice is the set of order ideals of an anti-chain, and that's correct then. The confusion is because you're doing Because you're doing row motion on order ideals of a Boolean lattice, but this is on the lattice itself. Right, like you're not doing row motion on order ideals of this lattice. Okay, yeah, maybe we can have questions and comments at the end, please. It's certainly an interesting map. It's an interesting map. Okay, yeah. It's it really. Yeah, it's it really, I think it really is row motion, but it is a little. I mean, when I was prepping for this, I decided, okay, we'll come back to an example together. Okay, so what was the last thing I was saying? That in the world of representation theory, if you apply kappa twice to a joint irreducible element, that's really the same as doing this Auslander translation. This Auslander translation and Auslander writing translation. And there's really a lot that I'm not telling you in this statement, but I guess I want to make the case that like this purely combinatorial thing has like just appears as kind of a map on modules. Oh, good. I'm glad. The last thing I saw in the chat is, okay, I get it. So I'm so glad that people are. I'm so glad that people are engaged. So, one of the reasons that I got excited about working with this is: well, because I really like this canonical join representation stuff. And it's sort of, this Kappa stuff kind of taps into that world. And in the Coxter-Catalan combinatorics, the way that canonical joint representations are really important. Join representations are really important is that they are a map from Tamari lattices and Cambrian lattices to the non-crossing partition lattice. And it really is take an element of this semi-distributive lattice, which the Tamari lattice is, take its canonical joint representation, and that is a non-crossing partition. And I have a really small, kind of blurry example here, and I'm sorry it didn't show up very well, of a Tamari lattice and a kind of GM. And a kind of geometric model of this canonical join representation, which just looks exactly like a non-crossing partition. Okay, and like we could give a whole talk just on that. And actually, in my work with these torsion classes, one of the key facts that we used is that there's this commutative diagram. In the top row, where you can do the kappa bar map that I've defined. Do the kappa bar map that I've defined for you in terms of up labels and down labels. And then in the bottom row, you've got non-crossing partitions and the crevora complement. And if you do this canonical join representation bijection, then this is a commutative diagram. So I would say that the kappa map is sort of equal to the crevoric complement. And we used this as when we were relating the kappa map to Auslander translation. Outlander translation. Okay, but what is the non-crossing partition lattice really? So, one way you can build the non-crossing partition lattice is by using the labeling that I've given for you. So, your input here is a finite semi-distributive lattice lambda. And then, what you're going to do to build the non-crossing partition lattice is you're going to You're going to build a bunch of subsets of labels, and the subsets of labels will be given by certain intervals of our lattice. All of the intervals look like when you take an element x, you look down by covers, take the meat of those things which are covered by x, and that's giving you this interval. And we're going to record all of the labels. And we're going to record all of the labels we see in this interval. Once we have all these subsets of labels, we order them by containment. So let's do that in our small example here, where we've got the labeling. So there's kind of only one non-trivial subset of labels, which we'll get by doing this procedure to the top element. So I look at the top element, I look at the things it covers, B and C. Things it covers, B and C, I'm going to take their meat, which sends me all the way to the bottom. So my interval, which I wrote here as the meet of cove down x and x, this is actually just all, my whole lattice for when I take the top thing. And so I'm going to write down all the labels that I see, A, B, and C. And now every other interval I'll get by doing this procedure is kind of a silly interval. By doing this procedure, it's kind of a silly interval. Like we could look at C here. C only covers one thing. So when I take the meat of the things it covers, I just get this one interval with two things in it. So I only get one label and it's the label C. And I'll get the same thing for A and for B. And then at the bottom, at the bottom, I have no labels going down. So I guess that would be the end. Going down. So I guess that would be the empty set. I know this isn't, the elements here don't look like non-crossing partitions, but this is isomorphic to the non-crossing partition lattice on like three things. Maybe I'll say non-crossing partition lattice on S3. Okay, so here's my Okay, so here's my take-home message for you. So, when L is the Tamari lattice, which it was in the previous example, the procedure gives you a non-crossing partition lattice. But actually, this procedure just works for any semi-distributive lattice. All I did was take my, like, it's all built from this labeling, right? So, really, every finite semi-distributive lattice comes with like an analog to the, of the non-crossing partition lattice, which I'm going to call. Crossing partition lattice, which I'm going to call, and this is what it's called in the literature, the core label order or sometimes the shard intersection order. And I really feel that this core label order is kind of an understudied object, especially in this dynamical algebraic combinatorics world. Come in a Torix world. So I want us, this is coming back to good teaching techniques. Let's notice and wonder. I noticed that there's a bite, there's still a bijection from the elements of my lattice L to its core label order. I also noticed sort of sadly that it's possible that when you do this core label order thing and take these subsets of labels and order them by containment, you might not get a lattice anymore. Might not get a lattice anymore, but maybe we're okay with that. And now I wonder if we just declare that our kappa map is equal to the Crevora complement as a way of sort of defining a Crevora complement for the core label order of any finite semi-distributive lattice, what could we say about it? If you have a nice semi-distributive lattice, like maybe trim lattices, is the kind of carever a complement? Is the kind of crevora complement of this core label order also nice? And I have some suggestions for ways in which it might be nice. And this is sort of my invitation to you. On the left-hand side, I have this things I've been talking about, the semi-distributive lattice kind of things. So our example is a So, our example is a Tamari lattice. That's a semi-distributive lattice. We know nice things about the Kappa map here. It's homomycic with respect to down degree. It can be computed using toggles. But now if you look at like the non-crossing partition side of the world, so I know non-crossing partitions have been studied a lot, and we know things about toggling there. But in general, for the core label order, we really don't know. I don't think. Don't know. I don't think we know very much at all about whether you can compute this crevora complement in terms of toggles or any kind of homocey or cyclic sieving. I think that there's a chance that this could be really nice. And so I want to invite people to maybe think about it. And I was thinking about maybe adding an additional column too. So, because I've been thinking a lot about representation. Because I've been thinking a lot about representation theory stuff. So, on this side, you've got torsion classes, which usually are kind of nice lattices and have been used in a classification theorem for semi-distributive lattices. On the other side of this table, you have wide subcategories. So, there's sort of this nice pairing happening too in the representation. Nice pairing happening too in the representation theory world. And okay, so I just want to sort of end with this. This is maybe like more of an open problems presentation, but I think there are some interesting things to think about here. And I'm way early, but that's okay because it sounds like there's things that people want to talk about. So I'll go ahead and say thank you. Thank you for listening. And yeah, questions are welcome. Questions are welcome. Let's thank Emily. I just saw Tom's chat. That's very cute. All right. So, yes, the floor is open for all your wonderful questions. Yeah, go ahead, Colin. So, yeah, I really like this talk. Thank you. Yeah, I really like this talk. Thank you very much. I'm wondering if anything is known about the core label order of the weak order on a symmetric group. Yes. So that's like virtually the only place we know things. But it's really cool. So Aaron Bancroft, who's a student of Nathan's, Nathan Reading, showed that it has an EL labeling. And then Kyle Peterson showed that actually. Kyle Peterson showed that actually the core label order for the symmetric group, type B and type D, all have an EL labeling, and that the symmetric group's core label order has something called a symmetric Boolean decomposition, which means that I think it means that the symmetric group sort of acts on that order poset in a really nice way, or the order complex in a nice way. Nice way. The symmetric Boolean decomposition is something introduced by Patricia Hirsch, I think. So then, and like this is something that I'm working on with the student is sort of diving more into the core label order of the symmetric group. There's a lot of nice combinatorics there. But I think also the core label orders for finite Coxstrick groups could be studied more. Yeah. The interval that you're looking at that where it That's where you look at the meat of the things covered by it has a nice combinatorial description in the case of the symmetric group in terms of something called the pop stack sorting map. Oh, okay. And you're going to be talking about that later. Yeah, I will be talking about that. Oh, great. Okay. I want to listen. I'm curious whether other core label orderings have as nice a geometrical presentation as non-crossing partitions. So the core label order was first defined for lattices that come from hyperplane arrangements. And it was, this is something I mentioned, the shard intersection order. So that's where you can realize the core label order as certain intersections of pieces of hyperplanes. And then it does have a lot of this nice properties that like the intersection lattice of hyperplane arrangements have. But it's, I don't think that it's a geometric lattice, so it's not as nice. I think Dari had a question. Yes, yes. Go ahead, Dari. Brendan? Yeah, that was a beautiful talk. So one question kind of related to cyclic sieving. This map Kappa, does it tend to have low order? Does it tend to have low order, right? I think it, I mean, it kind of depends on compared to the size of the lattice, say, or like in some cases, like if you have a cyclic sitting result, you typically want to have a map with relatively low order, like the GCD of these orbits, like for Krevaros complement, well, Orbits like for Crevoros complement will always be two times the number of things in your non-crossing partition. So if the order is really big, you look for home easy. But if it's, yeah. I don't know, actually. That's a really good question. I mean, and you know, this might not always have small order. It might just be for special latest you get this property, but okay, yeah. I think that. that so what is small small okay what is small relative like how do i well so for example for like non-crossing partitions it's two times the coxter number okay like so it's 2h um you know it's it's going to be something because cyclic sibbing remember you're plugging in these roots of unity right and if your root of unity in the complex plane is really really small In the complex plane is really, really small, but positive argument. It's going to be difficult to get like non-negative integer evaluations when you plug into sort of a reasonable polynomial. So, yeah, I mean, small is sort of an ill-defined term. But if you look at things like the sweep map, right, for relatively small parameters a and b coprime, you can get like orbits, they're like thousands of elements, or you know, just these real, these really big. These really big orbit sizes or really large map orders. So. So I don't know the answer to that, but it's something that I should look into. Okay, yeah. So Dari's question is asked in chat, and I guess I can state it. Can we try to see if you guys can hear us? It's kind of muddy. It's kind of muddy. I was able to hear you okay, but. Can you hear me now? I can hear you. I can hear you. Okay, so I don't know what this button does, but you can hear me, right? Yes, we can hear you. Sorry. So I was wondering if this cup of bar, now that I understand how it's defined, thanks a lot, Emily, if this has been lifted to PL and to birational realms. I don't believe. I don't believe so. I don't think so. My impression is at least on the BL level, the labelings will be join-preserving Wiki increasing maps to 0,1. I don't know, however, what Happer Bar will be doing to them. It will probably have to do with the labeling. This is a labeling on covers. I haven't thought about the PL and birational liftings of this, but I think other people have been working on that. That would be cool. If someone started a Slack thread on that, I'd be interested in joining. So, I mean, I can say a couple of things about it, but maybe on the Slack thread. But one nice thing is that But one nice thing is that these canonical general representations are independent sets in a certain Galag graph. And so that has a PL analog, which then might you. That's just, Emily, that's just your canonical joint complex. Yeah, yeah. The Galois. Yeah, so I don't definitely don't feel like I am an expert on the PL stuff, but the Galois graph, I understand. Other questions? I know somebody asked about a classification theorem for semi-distributive lattices. And Nathan Reading is here and has recently proved a really, really cool classification theorem for semi-distributive lattices, which is roughly kind of, I don't know, I would say it's something. I would say it's something like every semi-distributive lattice is sort of like a lattice of torsion classes. Oh, yeah, with David Speyer, who's also here, and maybe Hugh Thomas too. I didn't see Hugh, but sorry, I could just see Nathan on my screen. I'm not trying to leave the other people out. Thank you. So you can all ask them about that paper. Jean, did you have a Jean, did you have a follow-up question? Well, yeah, in regard to that classification theorem of is there a relationship between this core-ordered poset, I guess it will in general be a poset, and whatever is playing the role of the poset where you fish for ideals to get the semi-distributive lattice. Yeah, so Gene, what they do is they take like the joint irreducible elements of like, let's say you had a, I'm going to do the, I'm not going to explain this very well. If you have a semi, I just know you're familiar with curve representations. So I want to try to give you a sense of the connection. So if you have a semi-distributive lattice, then the joint irreducible elements kind of play a role like bricks. kind of play a role like bricks. And you can use the lattice structure to define like maps between your joint irreducible elements. You can sort of say like, well, there's an order relation between these joint irreducible elements. So I'm going to interpret that as saying there's something like a surjection from one of these joint irreducible elements to the other. But it's like formally defined. And, but they haven't, I don't think in that paper they really define like an analog of wide subcategory. Analog of wide subcategories, or I don't think that you guys really talked about the core label order in that paper. Okay, yeah. So somehow subsets in that object give you, give you, you recover your semi-dresserative lattice from some subsets in this thing. Yeah, you do basically you do, you do perpendicular, just like you would for torsion classes, you take like left perps and right perps to get the subsets of joint irreducible elements. Irreducible elements. But I guess my question was more. We have that construction, and I'm wondering how that relates to this other thing that you constructed, which is the core, the thing that's playing the role of the non-crossing partitions. I think that's kind of still open. Like this non-like it's just not studied yet. Yeah. So some people, so there is some work by Henry Muli on when the score label order. When this core label order actually gives you a lattice, but that's about it. And then, in some special cases in the representation theory world, people have also studied this core label order in the context of wide subcategories, but it's very like special cases.