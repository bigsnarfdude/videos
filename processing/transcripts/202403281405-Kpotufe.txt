            A particular prediction occur in New York. Is that clear? For say, is that clear? Sort of? So there are ties to these more practical questions that have to do with the economy of data. All right, any question here? So far? All right. So throughout here, there is a basic question underlying all these questions, which is, what do I even mean by information? And for me, here, information really means the distance between P and Q. What is that distance? P and Q? What is that distance between P and Q? And what is that notion of distance? Is it a metric? Is it a divergence? What is it? And then we have to ask that question really formally somehow. What do we really mean? But in particular, what do we really mean if it comes down to the problem of classification? Is that sort of clear to everyone? So obviously, every notion of Every notion of distance between distributions has been tried, and I'll talk about it in a minute. And one of the things we'll see quickly is that Wattsenstein distance doesn't work here. So this is just a quick vignette of past work. There's been tons of works on this subject in the non-parametric setting. The non-parametric setting is just one where I view my classification algorithm as belonging to a non-parametric class. Parametric class. But I'll fix attention here to just the parametric setting where I view the classifiers as just being picked out of a finite VC class. VC here standing for Tapnik-Cherbonenki's class. For those who are not familiar, it's just a notion of complexity of the class that allows empirical processes to converge fast enough. So just view it as a small enough class. Fixed parameters. Fixed number of parameters. Fixed number of parameters. All right, so we are in this setting. Now I'm going to assume that I have IID that are from my source P and IID that are also from my target queue. So as opposed to how people, this area has been looked at for a long time by machine learning theory people, but as opposed to how they've looked at it, they've always assumed that you only have data finite sample from the source distribution P and try to understand how well you do. P and try to understand how well you want to do on Q. One of the things we changed here when we started looking at this problem is: I'm going to assume that I have not only labeled that from P, but also labeled that are from Q. And I'm going to ask the following question. Is there an algorithm that, given just this amount of label data from P and Q, can do very well on Q? And why am I considering two data sets? Because then it starts telling me what's the relative value of data from P. Value of data from P versus data from Q. I hope that's clear. You know which is which. Sorry? You know which is which? Yeah, yeah, yeah. In this problem, you know which is which. At the end of the talk, I'll tell you about problems where you don't know. And based on notation, you're using, there's some common data they share in P and Q? Because you can say X I and Y I. No, no, it's just that the space on which P and Q are defined is the same. Okay, okay. Right? Yeah. Is that clear? But I have only NP samples from P, NQ samples from P. From P and Q samples from Q, and that's as much as I know the distributions P and Q. Otherwise, I don't know that. We'll see. Those are the questions we're asking. How far can P be from Q for me to still have information? Is that clear? I'm not going to make any assumption yet about P and Q. Right, I was thinking about size of the takeaway type point. You usually have much larger size? Oh, no, no. Let NP and NQ be anything. We go into. We're going to ask the question generally. You give me an NP greater or equal to 1, an MQ greater or equal to 0, and we'll try to decide. Alright. So the goal here, so I was asking earlier at lunch, how do we, whether if I say risk, everybody will understand what I mean by risk. Here's what I mean by risk. I'm just going to look at, it's an OEN classification. It's an OEN classification, the classification error measured on the queue, and that's my target classification error. We call it the classification risk. And so I'm just looking at the classification risk measured on the queue. My data is mixed from P and P. Classification error measured on the queue, and I want to quantify this object. The excess error over the best possible elect class. There? So that's the object, epsilon q, my excess error. Epsilon q, my excess error. I just want to know what is the smallest possible excess error that is achievable in terms of NP and NQ, and what I mean smallest possible achievable over any possible algorithm. I should have it estimated using both source data and target data? Yeah. The algorithm can use the source data and the target data however it wants. Is that clear? All right. Is that clear? Alright. And in particular, which notion of distance between source and target controls the hardness of this problem? Alright. So as I said, there is every notion of distance between distribution or divergence between distribution has been tried. So I'll give you quickly a sense of which notions have been. Sense of which notions have been applied to this problem. So often simple extensions of total variation where you look essentially at the difference in mass between P and Q over regions of space. I'll use a... Is there a... Yeah. So I'll use this simple cartoon. This is a source. It has this object. This is the target. It has this object, but they are distributed differently. Okay. Distributed differently. And the type of the restrictions of total variation just will restrict to the class H of classifiers I'm looking at. Why? Because as opposed to total variation, I don't need to look everywhere in the space. I just need to look at the parts of the space that are relevant to the classifiers that I'm looking at. So imagine, for instance, that I'm using linear classifiers. Then I just only need to look at difference of hybrids. At the difference of hybrids, and those are the parts of space that I can look at. So, this is old work. It's been understood that somehow the distance needs to be calibrated to the classifiers, to the class of classifiers I'm used. Fine? However, the type of result that is obtained in this area tends to be of the form something that goes down to zero with source sample size plus the distance between distributions. And so, in that sense, it gives the impression that we cannot get consistency. Impression that we cannot get consistency. That I'll rate asymptotes here. And we'll see that in a lot of situations, in fact, we can get consistency. The distance between distributions is not the hard part. Another approach is essentially looking at the CD ratios, the rate-only quote between Q and P. This unfortunately requires this dominance, which will Which we'll see that you actually don't need for a lot of problems. It turns out that P might have a lot of information about Q without them being related in this manner. And again, the type of result we get is something that goes quickly to zero plus density estimation error. And density estimation is an extremely hard problem. So it will give us the impression that the hardness of transferring, what I mean by Of transferring, when I mean by hardness of transferring, I mean bringing this object down. The hardness of transferring has to do with the hardness of density ratio estimation, which is not the case. And we'll see that through simple cartoons. Again, any notion of divergence, distance between distribution, Watson's time, Rainy, anything, this scale has been around for 15 years. And so people have tried everything, and the type of result they get. And the type of result they get tend to be this way. They all tend to be over-pessimistic for a given reason. The type of result says the following. If P and Q are close to each other, under any of these notions, then I can transfer. So that's at least what the basic thing we might hope for. The question we're asking is different. If this is the right notion, it should give me. Is the right notion, it should give me the other side. If P appears far from Q under my notion, it should be that transfer is R. That sort of clear? What I'm asking? And it also turned out that for most of these notions, we cannot PMP far from each other yet transfer is super easy. There is tons of information when it comes down to the problem of classification. So, here, just to illustrate quickly, suppose this is the Illustrate quickly. Suppose this is the source, this is the target. Okay? Then I can make notions of total variation, or the notions based on total variation, I can make them very high. Very high here in the case of classification, binary classification, is as high as one half. We're just looking at the probability of mistake. As high as one half, why? I can just look at this region. I can make them differ a lot in this region of space. Fine? However, consider the basic. However, consider the linear classification. Linear classification just consists of putting a line between the two classes. If I consider linear classification, then there is a lot of information here about this problem. I hope that's clear. Fine. From this final sample, I might be able to learn a good linear classifier that does well. And it's not just about linear classification. It could be polynomial classification, it could be the error's neighbor classification, and all that. Very neighbor classification and all that, this has information about the other problem. That's the key point. In fact, here we see quickly that metrics are not the right notion to measure the discrepancy between P and P. Why? Because the problem is asymmetric. I could have a discriminant here between the two classes that transfers well here. Well, here. From finite sample, I can learn this discriminant because P has good coverage of the discriminant, but Q doesn't have a good coverage of the whole discriminant. So if this was my source, I would not transfer here that easily. I would only be able to learn this part of the discriminant. So it's an asymmetric problem, and hence, a metric is not the right way to capture the distance between these two distributions. Next, Next, what about divergences? They are asymmetric, information theoretic divergences, they are asymmetric. But here is a simple case where I'm going to allow Q to be lower dimensional. So singular with respect to the other distribution. And yet, again, if I consider a linear classifier from this, it has information about this problem. So the key point being made here is simple: is that a lot of these notions of distance between That a lot of these notions of distance between distributions or divergence between distributions were designed to solve particular problems. And here, if I'm considering this problem of robustness in classification or whether it's in regression, I need to consider the risk measure that I'm looking at and understanding which way is the risk measure affected by my changing distribution. And that's the key point. I hope that's clear. How much time do I have? About 10 minutes. Yep. Okay, so how do we, what do we propose? We just propose essentially what I just said. I'll measure the distance between the two distribution P and Q by just asking, if I get a classifier that has low error under my source, does it mean that it's going to have low error under my target? And now that's something that I can parameterize. And now that's something that I can parametrize, I can quantify this. Effectively, it's just a notion of continuity between one error and the other error. So here is a simple first parameterization. I'm going to assume that the minimizer of the risk under the source and the minimizer of the risk under the target are the same. This is really just for the sake of presentation. The theory with the block doesn't need to have that. For the sake of presentation, this is the easiest case. Presentation: This is the easiest case. So let's assume this. Then all I'm asking is how fast does error, excess error under the source, drive down excess error under the target? And here I'm just going to parametrize it polynomially by this rho that I call it transverse point. Fine? And as rho grows, we'll see that P is farther from Q. From Q as rho is allowed to grow. So the continuum of rho will capture a continuum of hardness of the problem. That's the key point here. Alright, so let me go a bit faster. I hope at a high level this is clear. So just to illustrate quickly the transfers formats, let's consider a simple case here. This is some H indiscriminate. Some H star, a discriminant. I'm viewing this as my whole X space. This is some other H. And I'm just going to look at the difference in mass in this region. So imagine this classifier says class 1 on this side, class 2 on this side. And then this one disagree with this just in this region. So rho would be captured by just how the mass given. The masses given by P and by Q differ in this region. It will turn out that we will get rho equal to 1 if we look at it. We'll get rho equal to 1 and rho equal to 1 will turn out to mean to behave as though source and target are interchangeable. Source has just as much information on target as though I had target data. I hope that's clear. Same thing here. Same thing here. This is just to show Roek A equal to 1 again in the case where K already need divergence and all that go up. Here is a more interesting case. So again, this is H star. I'm letting rho be large, and how large rho is just has to do with how much mass the source assigned to the decision boundary. So the decision boundary is what I'm trying to find. This is the decision boundary between one class and another. I'm trying to find. One class and another. I'm trying to find this from finite sample. If the source assigns very little mass to the decision boundary, then it doesn't reveal that much information about the target problem. So how much mass is assigned to the decision boundary happens to control the value of this problem? I hope that's clear. So is a constant C dependent on the distance between P and Q, or what is C? The C is a constant based on P and Q. Based on another PMQ. Just an arbitrary constant? No, no, it's just a constant that will be given. It's C of PMQ. It's just a constant depending on PMQ, depending on how you set up the distributions. But it will turn out that what we really care about here is just the row. Right. Here it doesn't matter the kind of geometry of the it will also d it depend on ju the geometry of the edges. Fine. But I'm leaving it as C because it doesn't enter the uh the problem as uh as much. Problem as matching. Alright, so here is a case where we call supertress fer, where in fact rho is below 1. I just literally, I'm just using the asymmetry of the problem. So I switch. Here, the source gives a lot of information about the decision boundary. The target itself gives little information about the decision boundary. So in some cases, it's In some cases, it's better to use this different data than my own data. Okay. So let me quickly get to the type of results we actually get, formal results we actually get. So we want to understand the limits of transfer of domain adaptation and show that the limits of domain adaptation are well captured by this simple formulation, simple roadmap. So quickly, Quickly, this is how we set up the problem. Recall, I have NP that are from P, NQ that are from Q. And the main theorem is of this form. It just says that in a minimax, over all possible classifiers in my fixed class H, over all possible pairs of distributions that have transverse point rho, this is the best rates we can obtain. The minimaxity in machine learning or in statistics, all we really mean by this is that there exists an algorithm that can achieve this rate, but no algorithm can achieve a rate better than this, in the sense that for any given algorithm, there exists a distribution for which the rate is no better than this. So I'm getting the if and only if in a minimax. That if rho is large, the problem. The problem is indeed hard. The boundary doesn't enter rho, it enters the constant. Oh, no, no, it enters both. Yeah, it enters both. Rho is really just asking how well does a source reveal the decision boundary with respect to how the target reveals the decision boundary. How fast given a finite sample. I hope that's clear. What's the dimension of the dimension that I'm really in the right? Dimension is hidden. Dimension is hidden in the VC class. So you have your class H, the V C dimension of the class H tends to have to do with the V C dimension of your X. Sorry, of the actual Euclidean dimension of your X. Is that clear? So usually we're talking about rates of this form. Okay, where dimension goes. Okay, where dimension goes in here. It's usually Vc over, but Vc already absorbs dimension. Alright, so two, three minutes. Okay, so now this rates, to talk about this rates quickly, to explain it quickly, the beta here, you don't have to worry so much about it. All it is is we try to, it turns out that how hard. It turns out that how hard transferred learning is has to do with how hard the classification problem itself is. What is a hard classification problem? It's one, for instance, where the two classes don't have any margin between them. They cover each other. And this classification problem is one where the two classes are far from each other. And this parameter beta is a classical parameter we use to capture that. If beta is 1, we get a rate of the form 1 over n. If beta is 0, we get the usual 1 over square root n, that rate. And that print. So it just allows us to see how hard the problem is. Then we get an effective sample size from the source. And row captures an effective sample size from the source. So from there alone, we can already start answering some of the questions. What is the benefit of acquiring more data? You can look at a sharp threshold here, at what point the target data is better than the source data. I hope that's clear. I hope that's clear. This result also captures some of the questions I asked earlier about our label data. Will unlabel data be beneficial? It turns out that the algorithms, the inf is over all possible algorithms, including those that can have access to infinite amounts of unlabeled data. I hope that's clear. Any other question? So at this point I'll almost uh yeah, I think I have two minutes left, so I think I have two minutes left, so I'll just start wrapping up if this is clear. So, this is the main result. I'll skip how the lower bound is proved. It's through information theoretic arguments. The upper bound gets interesting because here we can start asking which type of algorithms actually can achieve this bound. In particular, there is a question of adaptivity. What we mean by adaptivity in statistics is just: can we achieve this bound? Can we achieve this bound without knowing these parameters? These are distributional parameters, the row, the beta, and all that. In particular, the row depends on the unknown decision boundary. So, can we achieve this? So, it turns out that it depends on the amount of noise. If there is very little noise, this is the case beta equal to 1, then you could actually pretend that the two data sets come from the same distribution. Just combine them and minimize the empirical receiver. And that would work well. That would work well. If the noise level is high, then you have to do some type of penalized risk minimization. For instance, you minimize your target risk using the target data, subject to the source risk being close enough to the empirical risk minimizer. How close depends on a particular quantity that is a notion of variance in the data. So, these types of algorithms are in fact heuristics that are used in applications, in machine learning. And so, what we're showing here is that these types of heuristics, in fact, are mini-max optimal. They achieve this particular type of rates without knowing the row or the betas and such things are pretty. So, that's that. I'll summarize quickly. Row is a Rho is a different notion of distance between distributions in some sense, but that is more targeted, more targeted to the actual classification problem that we're looking at. And it also allows us to get a sense of optimal heuristics that people have been trying and what these heuristics can be able to achieve. The general result does not use the The general result does not use the fact, I use here the fact that H stars under the source and the targets were the same, but the general result does not need that, and you just have an addition or some additional terms in there. All right, I had a lot more to talk about, but I'll stop here. There is tons of negative results also that we get in this area. The high level of the negative results, if I can say, do I have a minute? Yeah. The high level of the negative results. Yeah. The high level of the negative results is the following. This basic problem that I showed, I'm given a hypothesis plus, so the hypothesis plus is a model plus. I'm giving a model plus and I'm asking, I have data from a particular distribution, I want to do well under another distribution. That's sort of the idealized situation. In practice, people don't have a model class. They are dealing with multiple different models. These things are motivated, for instance, by neural networks. Neural networks have to choose the architecture, the depth. Networks, I have to choose the architecture, the depth, all these things. So I have really multiple models to decide from. Then the question we ask here is: we have multiple models to decide from. These are a notion of optimality. And I still don't have the right distribution. I have data from a different distribution and all that. What is the notion of optimality? What we show is that there is a particular notion of optimality from a Norwegian that knows a little bit about the distributions, but then there is this no algorithm that can achieve. Is this no algorithm that can achieve the order of ability? So, similar to this problem, I don't have a single source, I have multiple sources of data. Again, an Oracle might know which of the sources optimally transfers to the target. And then the question we ask, is there an algorithm that can achieve just from the data itself as well as the Oracle, and we show that there is no such algorithm. That there is no such argument. So, all these are based on decision information theoretic argument. So, I'll stop there. So, we'll move on to our last speaker for this present. I leave at least no questions. Thank you.