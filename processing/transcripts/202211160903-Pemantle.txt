Um several variables. Um oh we have to get that thing to go off. Let's see. All right. So it's a good time to give a review talk because we're in the middle of revising edition one of the book, Now that Commentarics and Several Variables. And so that book was by myself and Mark Wilson. And so they And Mark Wilson. And for the revision, Steve Meltzer has joined us. So we have his magic touch on making things readable, which is really needed. And so we have a great book which will probably come out during the next calendar year. And so I want to report on the state of analytic common choice in several variables. So what's it supposed to do? It's supposed to do. Let's see. So ACSD is, what's the goal, right? Extract coefficients, coefficient, usually asymptotics from generating functions. So that's the analytic combinatorics part. That's part of the Analytic combinatorics part. That's part of the title of the meeting, so I probably don't have to explain that. And then we work on multivariate generating functions. So what does the program hope to do by the end, right? So of course, any number of variables And asymptotics as the index, well, it's a multi-index. We have more than one index, a multi-dimensional array. So asymptotics as the multi-index goes to infinity in any way. Okay, so these are aspirational, by the way. I'll report how much progress we've made. I'll report how much progress we've made on each of them, but in most cases, a fair amount. Any accuracy as automatic as possible and any specifications? Specification of generating function. Okay, so let me quickly summarize the first four here, progress on the first four, and then the talk is really on the fifth. So let's see. For the first one, so For the first one, so this is a what style of talk is this casual. So blackboard, as you see, and oration. So sometimes I won't write stuff down. I've been, this is bad. I tell my TAs to write stuff down, but sometimes it's more like explaining over dinner, because this is a nice small workshop. So number one, as long as the number of variables doesn't go to infinity, the theory works. So that's our. So that's any number of variables. And as long as the number of variables doesn't go to seven, the implementations work. So one is pretty good progress here. Two, as the multi-indices go to infinity in any direction, it's a little complicated. Most of our theorems, our formulas work in certain regions, and then there And then there are other regions, and in the transitions between these, if the index is going to infinity, and that's a special region if it goes that way, and then in this region you're okay, and in this region you're okay, then maybe if the index approaches this direction sort of at some rate, you have to do a fair amount of work, and we don't have general results mostly. We don't have general results mostly on that. Anrola Jotzer, a student of mine from 10 years ago, had some results, and then we really never followed up. There's classic things from the analysis literature that tell you what to do with your saddle point integrals as you approach, but it's messy. And so that's kind of a gap in the theory. But otherwise, we handle all directions, including not just ordinary power series, but Laurent series. Power series, but Laurent series, because those turn out, you know, some of the applications turn out to go in sort of all directions, positive and negative. Okay, accuracy. Can you give an example of a multi-index going infinity? So for example, what's, you know, let's say a sub n comma n plus, you know, square root of n made into Square root of and made into an integer. How does that behave? If the diagonal behavior is one thing, and the behavior below the diagonal is another thing, and now we're approaching, or above the diagonal is another thing, so in this version we're approaching it from above, and we see some blend of the two behaviors. We try and get scaling limits for what happens at this window, right? N plus sort of order of something like that. That would be an example of something we don't know how to do. But other than that, you know, we're But other than that, you know, we're pretty good. Another question? Exactly what do you mean by specification? Yeah, well, we'll get there. I'll tell you. I've got to talk a lot about that. So in terms of accuracy, so one thing is most of our results yield asymptotic developments. So we have some formula that looks reasonably nice for the leading term, so maybe it's The leading term, so maybe it's you know, a sub vector r equals, I don't know, some exponential, you know, r dot something, you know, times maybe some polynomial correction times maybe some constant plus and then some other. Plus, and then some other asymptotics, you know, A1 R to the minus 1, A2, R to the minus 2. That would be a typical development. And these are asymptotic series. They're not necessarily convergent. I mean, sometimes they do, but they tell you if you cut off after a certain number of terms, then as our growth. As R goes to infinity, the error is big O of the next term. And so in order for these to be useful, the asymptotics, these big O's, have to be uniform as you vary R construct direction. Yeah. What does A1A B2 here? So these will be constants produced by the algorithm. So the exponential. So the exponential factor, the polynomial exponent, and then, oh, I probably shouldn't use A there. Oh, it's just a notational kind of thing. Yeah, C1, C2, something like that. Just notation. Just constants produced by the algorithm. Thanks for correcting the overuse of A. So, okay, so accuracy, you know, in this sense is good. We have implementations. They tend to work to lots of decimal places, so we're pretty happy with that. Happy with that. And then, as automatic as possible, this is really a long story. But to summarize, we have a lot of code. I didn't write any of it, but I work with people who are good at writing code. And so we have a lot of code, and it works. So we could do automatic asymptotics. And then we're constantly finding new things that didn't satisfy the hypotheses and making new code and trying to cover gaps. Cover gaps. It's just some sort of fractal covering of things that we can do automatically, and then a lot of things we can't. There's some topological problems that are really difficult. Often, what we cannot do automatically is a finite problem. So we can automatically tell you the asymptotics are one of five things. And which one is hard to say automatically, and actually often pretty easy to tell by looking at it, you know, if you know something about how the coefficients behave. If you know something about how the coefficients behave. So that's not a bad state of affairs. So now let me tell you about specification of generating functions. And at least in the first edition, and maybe in the more recent one of Stanley's book, Numerative Common Torix, there's, you know, he has dry width and he talks scathingly about Jim. And he talks scathingly about generating functions that are basically of the form sum over all the instances of some monomial. They don't really tell you anything. And so if you specify a generating function by saying whatever is the generating function for this, then we can't do very much, obviously. And so how can you specify a generating function better? So let's say we don't have any, you know, Infinite or unbounded sums or products, you know, because most of our techniques can't handle that, and that's not really playing it fair anyway. Well, okay, I take that back. Sometimes it's really useful to have an infinite product representation of a generating function. But for analytic combinatorics, it's a little bit of a mess. So. So, if Hardy and Ramonjin were able to make some hay out of that when they enumerated partitions 110 years ago, but we can't work very well with infinite products and sums. But then you say, okay, how am I going to specify a function as an analytic object? And so there's this hierarchy that's familiar to most of you. Rational functions, algebraic functions. Algebraic functions is a hierarchy because each one is contained in the next one. The finite functions, differentially algebraic functions, and one could make wider and wider classes, but this is not so relevant to us because, okay, in just a few words, Okay, in just a few words, we handle rational functions very well. And today I want to tell you that we handle algebraic functions equally well. So this actually eats up two big categories of generating functions. So I'll give you a few examples. And then d finite functions that's an epsilon. Sorry, it's not big enough. But you know, we actually don't have a lot of general techniques. A lot of general techniques, one could, I think. I mean, I think there's a lot of room for research there. But this is about where we stop. A little bit, I mean, even univariate definite functions are very hard to pin down. And when you get to multivariate, the notion makes sense of definite functions, but it's very, very hard to say stuff. And then we get past this, and we don't know much. And we don't know much. So, I'm going to tell you today about algebraic functions. So, let me just put up a few examples just to give you something to think about what kinds of functions we're talking about. Let me go I'll go back to the rational functions and just just say a few. And just say a few. So, rational functions, there's this paper called 20 Examples that Mark Wilson and I wrote 15 years ago, 20 examples of asymptotics of coefficients of rational generating functions or multivariate generating functions. And so I will I will take a random example out of here at the horizontally convex polyominos. There are these things that look like this, and people wanted to enumerate them by their total area. So these sit on some piece of graph paper. So they're, I didn't draw the graph paper, but they're integer number of Integer number of lines and each is height one. So there's a generating function, there's a univariate generating function that enumerates them by area. And in fact, I think the best or only known derivation of that is through a bivariate generating function that enumerates them by area and the, I think, the last, the size of the last. Of the last one on the pile. And, you know, it looks something like take actually a minute to write it down here. It looks like this. xy 1 minus x cubed over 1 minus x to 4 as minus xy 1 minus x minus squared. x squared plus x cubed plus x squared y. Okay, so you get the idea that it's just, you know, like a piece of minasperny soup in the rational function land. It's just got some features in it. And we don't know anything special about it. It's not symmetries. It's just, but it's not terribly complicated. And so, you know, you can do, so this is a typical thing that we handle very well with. Go very well with ACSV methods dating back 15, 20 years. And that's for the 20 examples paper. Quantum walks. These are fun because, so I won't say what a quantum walk is, it's some sort of thing prettier by quantum information theorists, again, 20-some years ago. 27 years ago. And the generating functions for these things look like determinants of certain matrices, identity matrix minus some Z variable times some diagonal monomial matrix, M. M has a bunch of other variables, X and Y, or whatever you want to call them, and then times some unit. Constants, some unitary matrix of constants. Constants. And we take this thing and we one over that. And what's interesting about these guys, so they tell you something about a particle moving in Z D what are the variables? What are the variables here? Like I'll tell you in a minute, but so let me tell you what the model is, and then I'll tell you that. So a particle is moving in Z D and it takes some finite number of, it chooses a finite number of possible steps. And so there's variables, let's say, x1 through xd. You're going to count You're going to count, let's see, you're going to say after, and then there's a z variable, and what you're going to do is you're going to count some over all paths in all points in Z D. So M in Z D, the, I'll call it probability, but it's really an amplitude, so let's call it amplitude. Amplitude of being at a point R in C D at time T and then we're going to multiply by X to the R and Z to the T. Does that answer your question, Jim, about? Well, so what, so the Z and the M are the. So Z is that Z. So Z is that Z. M is a monomial, actually, actually a monomial diagonal matrix. And these are on the diagonal is a bunch of entries of the term X to the S, where S represents a possible step of the walk. And then, you know, these are amplitudes, so everything takes place in quantum land. And they produce beautiful pictures. If you do two-dimensional quantum random walks, so. Walks. So, this is one limitation. I can't draw the picture in real time. But the picture of the intensity plot at time t of the quantum walk is pretty. And you can ask afterwards, and I'll tell you where to see them. But anyway, so quantum walks are a nice example of the power of rational ACSV. Rational ACSV actually works beyond the scope. Actually, it works beyond the scope of rational functions. It's really meromorphic ACSV. Rational functions, you can do a lot of your computation with computer algebra. And meromorphic functions, you know, for example, if I have f equals 1 over e to the minus x plus e to the minus y minus 1. So where did I steal this from? Where did I steal this from? This is a generating function, it's an exponential generating function for lone sum matrices. So this is an example. I don't remember all the authors. That's Bat. I should have written down all the authors. Steve, do you know? Jessica, Kira, me, and there's a few lone sub matrix. So there's a few lone sum matrices are 0, 1 matrices which have the property that once you know the row sums and the column sums, you know where all the. We know all the entries of the name. Eric Lumber. Eric Lumber. Okay, good. All right. So there's an exponential generating function. It looks like this. And from this, and it counts by Why are there only two variables? So maybe rho sum and m by n matrices. Yeah, I see it's the size of the matrix, right? Okay. So it counts by the two dimensions of the matrix. And yeah, so things like this are all handled really nicely by the rational theory. Okay, so now let's go over here and look at the algebraic theory. Right here. The quantum box type matrices are just for some nice types of we have the identity times variance multiplied and in that case this reciprocal determinant according to Magma, the coefficient can be extracted as equal to the coefficients of product of um for you know So you say there's a finite sum of products of polynomials representation. Yeah. Yeah, I'm not surprised. These are very nice forms of matrices. And in fact, without ACSV, if you just look at sort of transfer matrix method, you can learn a lot of things about the coefficients, such as in certain regions you get exponential k because of the eigen. An exponential of k because of the eigenvalues and so forth. So there's a very nice structure to those matrices. But I'll have to ask you after about the specifics you told them because they don't know that much. Are there any overhead lights under there that would make it easy? Not good. Well, it's more viewed. I think we have. So hopefully we can turn them off. I have all these gone already. AP? And those aren't the only ones, actually. Thank you. So let me say something before I give examples on the algebraic case. The algebraic case, it's been Case, it's been known to be reducible to the rational case in some sense for probably, well, it's implicit in some pretty old theorems, but nobody was looking at ACSV at the time. So the application of these in ACSV has been known for maybe 15 years. Mark Wilson and Alex Reitschev had a few papers on this sort of thing in the This sort of thing in the late 2000 odds. And here's a, let me write down just a few examples of what we're talking about. So trees with, so this is an example, trees with degree constraints. So for example, when I was So for example, just for fun, I wrote down what happens if you want to count degrees where every vertex has, these are rooted trees, so there's out degree for every vertex number of children. Each vertex has 0, 2, or 5 children. And we want to count by, it turns out you really only want a bivariate function. It looks like you might. It looks like you might want to count this, this, and this in the total, but there's actually a couple of different linear relations. So count by perhaps, you know, total vertices and type five vertices. And, you know, you when you And you know, when you imagine counting by that, you imagine a recursion. And this recursion is easy to write in terms of generating functions. And it leads you to something looking like f equals 1 plus, so that's if you're the empty, I think we allow the empty tree, if you're the empty tree, you get a 1 and all of this, plus some other terms that represent recursion, z. recursion, z times minus 1 squared plus z times y. So I guess z is counting total and y is counting these special type 5 ones, z times y times f minus 1 to the fifth. Why did I write down this particular one when I was messing around seeing if I could do stuff? It's because you can't solve the spiradicals. I wanted a quintic, so um and then I checked in in Maple that the I checked in Maple that the specification to one variable function had Galois group that was S5, so it definitely could be solved by radicals. So you only have the implicit definition of it as an algebraic function. So in general, algebraic function means that f satisfies some polynomial of f at z1 up to zd equals zero. Zero. So that's just that's when we're talking about spec any specification, this is the generic specification of an algebraic generating function. So constrained walks. So if you look at various, for example, random walks that are constrained to That are constrained to remain in the positive quadrant of C2. C2 plus. And you get a lot of different things. I mean, there's a kernel method, and sometimes you get something algebraic. Sometimes you don't, but you get something definite. Sometimes not even that. A lot of different things can happen. And if you look at one. Happen. And if you look at walks that are, you know, they take nearest neighbor steps only, say the walk that takes those four steps, then even those are a really difficult plasma to analyze. And there's a wonderful book, Brad Watson, Ford Plain, that does a great job of discussing how you analyze these, how you can generate these functions out. Anyway, some of them are algebraic. For example, Algebraic, for example, that one I think. I think I do the right thing, Gessel walks. And in some kind of tour de force, Helene Bostin and Adel Cowers eventually proved to people's surprise that this was that algebraic generating function. I think it was, was it not known either whether the univariate generating function or the bivariate generating function was algebraic? I'm not sure. Anyway, they have algebraic representations there. Representations there. And so that's a nice example of something where once you have the polynomial relation that satisfies, then we can go ahead and get asymptotics easily for that. Except this one's a little bit of an outlier because here, P of F C1 into Z D is pages long. So if you actually do this in computer algebra, Do this in computer algebra, you get something so big that you don't really want to go to the next step because you have to do group of basis computations on that. And when it's down long, sometimes these just don't work out. They don't stop. So I have not tried this one because of that. But it was produced by computer algebra, so you know, it's not saying it wouldn't work. Okay, so there's a Algebraic generating functions have their own 20 examples of paper. This doesn't have the word 20 in the title. But so Steve and Torin and Mark Wilson and a student. Tier Russa. Tier Russa. Yeah. That one actually had. Okay. Yeah. So they wrote a paper and it has 20 worked examples and there's a Sage worksheet online that goes along with it. And I'll just mention a few examples. And I'll just mention a few examples from that, and then we'll go on to some theory. So let's see. There's these things called bar graphs that look like, they look like compositions of an integer in picture form, except when you count them, you count them by, well, by the number of bars, so that's. Number of bars, so that's still pretty boring. And then by the total vertical height as well, which you divide by 2 because it always has to be even and you don't want periodicity. So it matters what order the pieces are in because you're counting absolute difference between pieces go up and down these vertical bars. The difference between the endpoints. So you count these things by these two statistics, and you end up with an algebraic function f satisfying sort of polynomial vanishing. What is that polynomial? I have it written here somewhere. Polynomial. I have it written here somewhere. I think it's f minus xy minus x plus y plus xy two f terms. So I don't know if I wrote this right. Minus xf squared equals zero. So yeah, I'm not. So yeah, I'm not sure if I should have put that in error if I wrote it wrong. But it's something like that. And I did some computations on this, which if I have time, I'll just sort of mention quickly how they go. Does it help at all, and in this case, it's quadratic and you could solve for F or not? Okay, so not so much. The theory looks exactly the same, whether you can solve it or not. Exactly the same, whether you can solve it or not. On the other hand, if you can solve it, you can make pictures in your head about how the function behaves and where it's, you know, if you solve this by radicals, you end up with something under a radical sign. And where that thing vanishes is a branching locus. So you have a little bit more of a direct route to the branching locus than you do without that. So it helps a little. So, um So, what I really want to do, I'm going to, at the end, do a couple of work examples, as many as I can. But that number might be pretty small, depending on how much time I eat up with theory. But what I really want to do is show you how it works. And I'm going to mention the old method and a slightly newer method. Newer method. So in analysis, you know, mathematicians are good at reducing to a previous case. So there's an analysis that goes, let's reduce to the rational case and then solve it. And the only reason this is a little problematic is you can't really go through the whole story in a lecture to a room. In a lecture to a room full of people in real time, because it pulls things in from different places. But let me just say how the outline would go. So P of F and some Z vector equals zero. And there's a theorem. And depending on whether P has certain ramifications or not, you either use a theorem of Hurstenberg. a theorem of Hurstenberg from 1967 or Safanoff from 90s, maybe 2000. And it says that there exists constructively another polynomial Um, I don't know, call it a rational function g over h in d plus 1 variables such that such that, okay, I'll just say it first. Let's see if I'll write down some ghost of what I said. Such that all the coefficients of this capital F appear as coefficients of g over h. As coefficients of g over h along some sort of generalized diagonal. The coefficient in the direction R1, R1, R2, so on to Rd of F is equal to, sorry, of G over H is equal to the R1 to Rd. The Rd coefficient of what you care about. So this is a reduction, yeah. And the G and H it depends on the direction that you're going? No, so this encodes all the coefficients simultaneously of F in G over H. And so you have to go, so then you want to analyze G over H in a diagonal direction with respect back to the first two, but an arbitrary direction in terms of R1 through. In terms of R1 through Rd. I'm sorry, why does R1 repeat? So, how do these coefficients sit in there? It's like, for example, if I have an algebraic univariate function, you know, summation a n x to the n. And then I somehow find a rational bivariate. Bivariate function where, you know, summation bij x to the i, y to the j, but b n equals a n. And so if I look at the diagonal coefficients and I can extract those, then I got my original problem solved, and then these other coefficients are just something that I don't even care about. Don't even care about. Okay, so do all of them repeat twice or just no. So from D variables, I go up to D plus one variables, so there's one repetition there. So that's step one, if you buy that theorem. And then step two is the usual ACSV, which is you write, so I'm going to use local variable names A, B, whatever. Variable names A, B, whatever I said A and B were before, they're not anymore, and now A and B's, right? So A sub R is 1 over 2 pi i to the d. D is also local. I don't know if it's really d or d plus 1 here. The integral over a small torus around the origin of z to the minus r minus 1. I reduce each index by an extra negative one. For an extra negative one power on H index times F of Z dz. So this is DZ1, DZ2 up to DZT. And AR are the coefficients of a power or Laurent series expansion of F. So I have F, I want to extract the coefficients. I use Cauchy's formula. That's what we do. And then somehow I So, in the smooth case, anyway, this is what I do. So, I have this variety where f is zero, and I have my torus here sitting near the origin, and I blast it out to infinity through the variety, and I get an intersection cycle. This is usually where most people tune out. I get an intersection cycle, and I take a generalized residue form. And I then integrate the residue form over the intersection cycle, and then I do some Morse theory to move the intersection cycle to a critical point. Okay, that's your second chance to tune out. And then I use classical saddle point analysis on the reposition cycle. So this version of the story takes, you know, hits you in a few places. Hits you in a few places, the algebra, the Morse theory, and then this residue stuff. And so, what I wanted to say is I just wanted to give a new perspective. It doesn't really do any more work than was done by Mark and Alex, and then the authors of this 20-example paper and so forth. They did all the coding and all the analysis, and Mark dug up these theorems originally, but Originally, but so it does what you end up doing more or less the same mathematics, but you don't need to delve into intersection cycles and residue forms and so forth. You just do the Cauchy correctly, and this is how it looks. So, this is, if you want to separate the review content from any possible new idea content, this is one new idea. Simplify this. This is one new idea. Simplify this in a way it can be presented to a room full of people with general knowledge, but not that level of differential topology and algebra, whatever you call it. Okay, so from this Cauchy integral, you So let's draw a picture here. What I'm going to draw a picture of, I have an algebraic function f satisfying some polynomial relation, p of f z1 zd equals 0. In fact, let me draw this a little differently, so it's obvious that there's z1 through zd like this. like this, and then there's zv plus 1, which is going to be my f coordinate. So I'm going to draw, I'm going to take this polynomial p and draw sort of this variety here b, where p equals zero. So it's it's a this variety is essentially a graph of A graph of f over z1 through zd, but it's really a graph sort of as a relation. f is multivalued, you know, and so there's a vertical line can intersect this multiple times, finitely many times. So I take that graph, and then I look at my torus here down in the plane. So I don't know, I drawing a screen. I don't know. Drawing a torus that's flattened into the plane is just artist rendering here. So I'll call that T. And so that's the same T. Since I have local variables, I better say when they're the same. So I have to do a Cauchy integral there. And when I do this integral, so let me now pi is some projection from C D plus 1 to C D onto the Plus 1 to Cp onto the onto this projects onto the first D coordinates. And so if I think of pi as a map from the variety V down here, in general, it's a many-to-one map. But fixed, whatever the degree is there, it's a K-to-1 map. Except at certain places where there's problems because you have mapping like this, and then here, somehow the map isn't very nice. And then here, somehow the map is very nice, not diffeomorphism. And if you were looking at what the different solutions get to this function, they collide at this point. These two solutions collide. So, but most places it's a nice map. And typically in the applications of the origin, it's a nice map. And the generating function f near the origin, there's several possibilities, but you have to know when you specify the generating function. You have to know when you specify the generating function that way, you're not done. Because you have to say something about which branch it is near the origin, like what's its value at zero. That's the constant term. You should know that. If you don't know the constant term of your Darien function, you might be out of your doubts. So the constant term might be there, and so we're talking about the branch of f, which as an analytic object, sits up there. So pi has a nice inverse. So pi has a nice inverse image. You can invert pi if you make this choice, at least locally. And so T lifts to some other torus T prime that's also a D torus and it sits in here. And this and then what are we supposed to integrate? F times the power of the first D of the of the Z's. D of the z's. And f is really the y coordinate. So we're integrating over d prime, z, well, y coordinate, z d plus 1 is really the name of that coordinate. Z D plus 1, that's it. That's the generating, that equals the value of the generating function. And then we have DZ1 up to DZD. So I'll, the answer we want is 1 over 2 pii because it's The answer we want is 1 over 2 pi i to the d times standard. I'll stop writing 1 over 2 pi i to the d every time. Okay? So we have to do this integral over that torus. But we no longer really have to care what the torus is if we know how to move it into saddle point position. And in fact, moving it into saddle point position essentially amounts to now that I'm on a smooth, so I've smoothed things out. You know, if I look down here, F has Look down here, f has this nasty behavior right at this point. So there's a vertical tangent, and there's my function f, and it's got a branch point there. But up in the cover, everything is smooth. So I just have some nice smooth torus. I move it around nice and smooth to this point. And this thing here, sorry, I neglected my z to the minus r term. It's a minus r or a minus r minus one, depending on whether I. Minus one, depending on whether I divide by each coordinate separately here. I could put over z1, z2, z3, or not. Depending on what code you're running, one or the other is easier. I'll put the one up here. So I have to take this, and it's a saddle point integral, because this thing is e to the minus magnitude of r times summation minus. Summation minus Rj over magnitude R log of CJ. So I separate out a sort of growing term as R is going to infinity. That's normalized coordinates in R dotted, you know, time as well. So the R vector dotted with the log coordinate vectors. And this is very standard. So if you haven't seen this much, it might kind of If you haven't seen this much, it might kind of not speak to you, but really it's standard. And so this is of the form a e to the lambda phi, where a is some, at least locally, smooth function, and lambda is going to infinity, and phi is the lambda is. But lambda is magnitude of r, and then phi is this thing. And you have to go to where the gradient is zero, but actually, we already did this, it's these branch points where, well, not all of the branch points, the branch points are where for a sum R, the gradient is zero. So now it's time to stop, to get a little concrete and look at an example. Look at an example. I want to get to a real example, but I have to do a toy example first. I don't think I could do it, jump to a real example and have you understand what's going on. There are kind of countries called branching points. So let me see if I understand what you mean. I mean, in general, there's a whole branching surface, and we have to choose one point. And the point that we're going to And the point that we're going to choose among all points where we have vertical tangents is the point that makes this function phi, which is that expression there, that makes the gradient of that vanish. So I can list out the integral. So that's the, I think, that's what I understand what you meant? Okay. Do you also have to worry about escape to infinity? Yeah, so you have to worry about the same thing. Yeah, so you have to worry about the same things. So you have to do the same math. You have to worry about whether you actually can move t to this point. In fact, there are finitely many such points once you do what I just said and you move it to a point where the gradient of that particular function is zero. There's finitely many. And if you look, sometimes you just tell which one. But sometimes you can't. And then that's a difficult topological problem. And sometimes even you can't get there because infinity gets in the way. The variety, you know, sort of going out to infinity, you're trying to get over here. out to infinity, you're trying to get over here and you get sidetracked over here. So all of the same worries exist, although you are working in somehow one fewer dimension and so the worries are that much easier to handle. But you can't really avoid doing the same, some of the same mathematics. And that's because the phenomena, the behavior of these generating functions is complicated and it's telling you that. And it's telling you that you're going to have to do something complicated to get an answer that results in this phenomenon. Okay, so let me do a toy example, which is, because I'm talking about ACSV for algebraic functions, but I'm going to do a univariate function so you can see how it works. So let me say before I embark on that, because I have to get in one or two seconds. Give in one or two sentences of philosophy. So for univariate algebraic functions, there are these, Baja Leon of Lisco have this lovely survey on how you extract asymptotics automatically. It's a singularity analysis. And for something like, so I'm going to use this example: Cadillac generating generating Generating function. And this has a branch point at x equals one quarter. And so it's a nice function if I get rid of array, you know, moving out to infinity from 0.1 quarter. And they show you how you can extract asymptotics using Cauchy's integral formula along a keyhole contour that looks like that. Integrate along that. And so, where does that come from? Come from? Just philosophically, I have to say, essentially, it comes from doing that, right? You have some lifting, you make a substitution, z equals one quarter minus u squared, and then when you write down the Cauchy integral, you get rational functions along some double cover of this, so the Riemann surface, but it's just that's still the complex plane q coordinate, it just double covers this. And so you have a nice smooth integral. Smooth integral, and it has this form, but just in one variable. And that's where this keyhole comes from. And you could decide to try to work in the keyhole contour instead of the lifting. And that's what they did. They found that easier. And in fact, you could do that in this case too. You could get a contour in many variables that sort of looked like a keyhole contour. So it sort of looked like a keyhole contour, but it was a lifting. And so Torin actually did this in his thesis a number of years ago. So there's one case in which you can do this just by getting the right contour. But this is kind of a method. If you do what I said here, it's a method for always using the right contour. You're just working up with a lift. So it kind of extends Torin's work that way. So, okay. I can get this done in time. So So let's look at how this works. So we look at the polynomial for this, which is xf squared minus f plus 1 equals 0. And we draw it, draw it to zero set, zero set. Zero set looks like. I'm going to draw the zero set, but only the real by real slice of the zero set. Sorry, I'm not into four-dimensional drawings today. But, so it looks like that. And I think this is, I didn't draw quite the proportion. This is the point zero, one, and this is the point quarter, two. Quarter on the two, and has some things that go off to infinity in various ways. Okay, so that's there. And then in the, so this is the x axis and this is the f axis. And each of them is really a complex plane. So we have some torus sitting there, torus, which is really a Or a which is really a circle around the origin and the x-axis. And we have to lift it somewhere. And in this case, there's two choices. We could lift it. So the right choice is to lift it to this branch because F of 0 is 1, but you could also lift it to this branch, which happens to have a hole right at the end. To have a pole right at the origin. And that's not the generating function you want. That would be if you chose the plus sign instead of the minus sign for the square root. And you'd get some contour that, you know, some circle that went around infinity somewhere. But it was, but it would be finite still because it would live finite level. Anyway, you choose the red one. And okay, so then you said that second, the one that leads to the circle around affinity, that's another round. That's another option? Yeah, that's another option. And in fact, that corresponds to choosing a positive root here. If you choose a positive root here, you'll notice that the function actually is infinite at zero. But there's some Laurent series that corresponds to that. You could decide that this relation was trying to encode that Laurent series, and you get a different answer. Asymptotically, just the negative 2101. Yeah. Okay, so you start like that. Okay, so you start like that, right? So, you know, 2 pi i of a sub n equals integral, well, then we lift to t prime here. So integral over t prime f z to the minus n minus 1 t. C, I guess this would be. Z, Z, I guess this would be, I guess my two coordinates are called, I'll call them X and F. So I'll call it Z. That's an X and Z. I'll just call this an X. All right, so you do this and you compute where all the vertical tangents are. And in this case, it's very easy because there's only one, right here, order, two. So you take this concept. So, you take this contour and you move it to here. And in this case, you can draw pictures, and it's pretty obvious. You don't have to do a lot of fancy stuff to see that this contour, if you just sort of widen this out, you can get it to go through this point in saddle point configuration. I'm not going to do the picture in real time. But you get an integral that looks like Let's see. What does it look like? It looks like the one thing that's tricky here is you have to reparametrize. So if I'm trying to describe a neighborhood of this point, I don't want to describe everything as a function of x. It doesn't behave very nicely. It folds over in the x direction. If I describe everything as a function of f, it's very nice. It's a graph, a nice smooth graph over f. For a nice smooth graph over half. So I reparametriz. So I have f times x of f to the minus n minus 1 times EF times a Jacobian for the change of coordinates. And that Jacobian is turns out to be f, let's see, two minus 2 minus F over F squared. So what's the important feature of that Jacobian? So we're near the point 1 quarter comma 2. So the F squared at the bottom doesn't really bother us. It's about a quarter. But this 2 minus F at the top is telling us that our saddle point integral is going to look like some one variable thing, a expansion a e to the lambda phi, where phi, where A, so phi is, has this critical point at the origin as we move L. So we parametrized by F, but really F equals 2 plus some parameter T. And here, A of the origin will be 0. So without doing any more calculation, since one doesn't do a certain level of detail of computation in public, without doing any In public. If you have to do any more computation, what can you say? Well, you know, the integral, so phi looks like, locally, like a quadratic, right? So integral of constant eta minus lambda x squared, minus infinity to infinity dx. That's like something like root 2 pi c root 2 pi. root 2 pi over lambda times c. So say root 2 pi c lambda to the minus 1 half. So it gives you a minus 1 half power of your large parameter. But if instead you have an integral of something that looks like 0 plus x or plus some constant times x, c 1 x plus c 2 x squared. That thing times e to the minus lambda x squared. Minus lambda x squared. Then, so the leading term drops out, and the next term is odd, so that drops out also. So, you get integral of quadratic times e to the minus lambda x squared. So with each successive power, you're knocking down, by twenty chain variables, by one half power and x, so you're knocking down by two halves powers, so you're going to get something like order of lambda to the lambda. Order of lambda to the minus 3 halves. Okay, so this is a long way to do this particular problem, but the nice thing is it scales up perfectly. So we can do any multivariate problem this way. We will have the same issues of finding which saddle point to move it to out of finitely many. Some things that work a little better, quicker are in the multivariate situation, finding all points. Finding all points where you have a vertical tangent is a very easy computer algebra computation. So it's not terrible to doing it the other way, but it's pretty easy this way. And so for a number of the examples that I would have gone into if there were time, and I'll tell you what they are because they're probably near and dear to some people in this room. So there's an example of something called assembly trees. Called assembly trees that Michelish worked on at some point. And it's very nice. All the varieties are very nice and easy to draw. And you can find the vertical tangents easily. In that example, you get a hyperboloid, and the band around the middle is where all the vertical tangents are. Yeah, I don't want to take up too much time. So that's one nice example. Too much episode. So that's one nice example. And then you have to find, you know, you've got finitely many points for any given direction, and you have to make that choice. And that's as difficult as always. But in the examples I chose, it was easy to find out. Okay, yeah, the bell rang a minute ago. I think I better stop and take questions. Why don't we take questions to the next one? Why don't we take questions to coffee break so that we can do that and have coffee break until turn home? Great. Okay, so I'll be out there if you have questions.