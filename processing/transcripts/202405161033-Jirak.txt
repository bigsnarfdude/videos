I thought we could. I didn't match it, but let's start. Welcome back to the second talk of the morning. Very pleased to have Maurice Jurak from University of Vienna. From the University of Vienna. He's a professor and he belongs to the Department of Statistics and Operations Research. Or maybe the department belongs to you because you're the head of the department now. So he's going to talk about weak dependence and optimal quantitative soft normalized central number theorems. Thanks a lot for this very nice introduction, and thanks a lot for inviting me. Thanks a lot for inviting me. I should add that being head of department in Austria is just means you haven't had that many good excuses as your colleagues. Happy to pass it on at some point or another. Okay, and I have to apologize in advance. I don't have any nice data sets. I don't have any nice pictures to show. I don't have any cartoons or whatsoever. Have any cartoons or whatsoever, I still have some motivation. So that's actually the story how I ended up working on this kind of program. So let us assume in a standard PCA setting, so we observe IID, Reinhard variables. We are in some Hilbert space, so dimension could be infinite, could be finite, we don't really care. And we have And we have usual covariance operator, the usual empirical covariance operator. We have the empirical eigenvalues, we have the eigenvectors and so on and so forth. And you should question how far are these away from the population. So here I'm particularly interested in the ancillary projections. And there's many ways how to measure this. To measure this. So, here's a particular line of research and results which I don't know, maybe go back to Anderson even, then Dog Schwarz. And then more recently, this was done by Kolczynski and Woodwood BC and then by Spokoin, Udenhoff, and all those people. And also, I also worked on this together with Martin Wall, and we got some results. And so, the aim is you look at the distance of the. The distance of the Hilbert-Schmidt norm of the empirical projections, you center scale, and you get obtrusion normal distribution. And J is the subspace? Yeah, J is the subspace, yeah. Yeah, yeah, the dimension increases, and depending on how fast the dimension increases and the eye whether it decay or whether it don't decay at all, you can get phase transitions and all that, and so on and so forth. And all that, and so on, and so forth. Plus, the dependent structure is important. The principal scores are they independent or not? And so on. Okay. The point is, so we got some results, we did some bootstrap, and this has all worked, but the question was: how do we actually estimate this variance in the end? So bootstrap, you estimate it implicitly. You estimate it implicitly. And I started thinking about how to actually estimate this thing, what's the optimal way to do this. And if you think about it in many even high-dimensional statistical problems, I would say we've seen this already this week a few times. At some point, usually at the end, you end up looking at the universe central limit. Summary statistics, you want to do some uncertainty quantification. You want to do some uncertainty quantifications in many cases in the end. You look at the CLT in some sense, or this case as well. And usually the long-run variants, so what this thing converges to is non-standard. You have some dependent structure which gets reflected in this variance. And then the question is: okay, how do I? The question is: okay, how do I estimate this? And if I want to look at it from a general perspective, so what's actually a reasonable general setup to look at this kind of problem? If I don't want to specifically talk about this, but maybe a little bit more general, what's actually the problem we're looking at. So if I'm thinking about the CLT and I want to have a CLT, how strong kind of dependency? How strong can the dependence be? In general, I need, I can only have a weak dependence. So, long memory in some cases works as well. I still get the normal distribution that I mean, but in general, it's weak dependent. So, we will be looking at a weakly dependent stationary sequence. Stationary is for convenience. We generalized, but we'll stick to that for convenience. Okay, and we have the CLT. CLT means we have a long invariance, which Roman variance, which like this. So, this is the typical normal variance. And then, if you have some consistent estimate, okay, we just normalize and then we use what some people call Schlutski's lemma or Slutsky's theorem or whatever, we get the standard normal distribution as the limit. Okay? So far so good. Just it's been done for decades. Stationary means strict stationary. Strict stationary. Yes, actually, weak stationary work as well in the end. Let's see. As I said, this is just the convenience for notation. Okay, then now the question: how do we estimate this this uh variance? Or does it even matter? I mean estimate. Nice to meet. And this has been studied for decades again. So people in time series analysis and dynamical systems have been looking at this really for decades, spectral density estimates and so on and so forth. So usually they look like this. You have the empirical auto-covariance function and then some kernel or weight function, whatever you want, and some bandwidth B. And some bandwidth B. This is what they usually look like. There are some other ones like Karlstein, estimators, robust versions, and so on and so forth. I'll be just looking at kind of estimators. Some of the results I will show you, the chemical transferred, you have to work a little bit, but I will stick to these kind of estimators. To these kinds of estimators. For a reason, because what is known, if you want to estimate this Laman variance, what is usually considered as a risk measure, mean squared error, and you have the usual bias variance decomposition. And then what have people been looking at? Two cases mainly exponential decay or algebraic decay. So exponential geometric decay means altogether function decays exponentially. function decays exponentially. And algebraic means, look, measure is a function is just algebraic decay. So if this prime at the a bigger than 1 and okay back this unknown. So what's the behavior? The key point is the variance does not depend on this decay, essentially, it just depends on the bandwidth, or large due to the bandwidth, and the decay only kicks in at the bias, right? At the bias. This is values in the bias, either exponentially or algebraically. And then, of course, what do we do? We balance these two. So this is what has been done for many, many years. So in geometric decay, what's the bandwidth? Of course, it's just the log. In case of algebraic decay, so I re I shifted a little bit. I shifted a little bit. Why? Because then we get the usual non-parametric weights, what we know from non-parametric statistics. And it can be shown these are the usual minimal and optimal weights. Okay, so why am I repeating all this? And I really want to emphasize here that in case of exponential decay, you can make the bias vanish. Vanish by staying minimax optimizer, right? Because if you make the constant in front of the log large enough, the bias vanishes at some point essentially. And in case of the algebraic decay, this is not possible. Constant has no influence. Just changes the constant here. You won't influence the rate. And this would happen. So, what's our aim? What I'm actually aiming for. So, I want to have a good normal approximation. So, I want to do some insurance. So, I want to know how close, at least do I have some guarantee that I'm close to a normal distribution. So, we can look at all kinds of probability metrics. Holmogoro, Wasserstein, has been pretty fancy lately. L P. Fancy lately, LP, ideal metrics, total variation would be best, but requires many assumptions usually. So from a statistical point of view, it has turned out, okay, Kronogorov metric is usually best or most convenient. Fairly strict, but we can still stay fairly general as well. So what I'm aiming for is we we need to find a bound here. We need to find a bound here, the optimal one. Will depend on the bandwidth, will depend on the sample size, of course. And once we have that, we will need to do some model selection. Because in general, we don't know the decay, and in case of algebraic decay, we don't know how fast our time series decays. So, we need to do some model selection that allows us to. That allows us to find an optimal procedure. Okay? So that's what I'm aiming for. First, find this optimal bound and then do the model selection to get the overall optimal bound. And this is of course not new. There's many, many, many results in the literature. I will get back to that. But first, a little digression. If you think about standardization or self-normalization, Studization or self-normalization, this has very interesting and nice effects. For example, if you look at the so-called Kama moderativation principle, there has been some deep research by results by Wang, Ki-Man Chao, and also Goetze before, and so on. We have actually, you only need three moments, and you still get a karma motor activation. A karma moderativation principle if you studentize. If you compare this with the non-studentized case, in this case you need that the moment generating function essentially exists. So just compare this three moments, moment generating function needs to exist. This is the effect of self-normalization. It gives really strong stability. And this is this is a And this is this is a proof is not easy, really deep result. And this somewhat been extended to weekly dependent sequences as well, but the results are not optimal. You need to throw away data and it only works for algeometric decay. Okay. Plus there's bad news. In case of algebraic decay you can show such a result is not possible. Is not possible. Lower bounds, Grammar motivation principle in case of algebraic decay is not possible. Okay, so we'll stick to a bare essay bound. We don't do Grammar motivation principle. Now what have people been doing in case of geometric decay? If you really look for rigorous results, it turns out, surprisingly, there's not that many. So, of course, So of course so the the usual name that pub that pops up is Goetze, Benko's maybe as well. So they've done um some actually Goetzek a lot of work in this area. They derived Varies and bounds, sub-optimal bounds, but in a fairly general setup. Then there's a famous work by Kunsch who introduced, also introduced maybe the blockbuster. Used maybe the block bootstrap, and they even derived address expansions in this case and showed that the block bootstrap works. And then this was continued by La Harie with some other works in multi-wheel extensions and so on and so forth. And there's also some work by Peter Hall. And if you look at the proof, I would say. I would say a typical Peter Hall proof like this, and if you buy to extend it, I think you would do all the details. You will end up with like 10 pages or something like that. I think it's correct, but it's really compressed. Yes, but anyway, so this is mainly about linear processes with exponential. With exponential decays. Nothing on an algebraic case, unfortunately. So as I said, in this case, people have even gone farther. They derive edgeworth expansions. Unfortunately, again, so the results of Gretsy require this conditional gamma condition, which is not easy to validate. Not so easy to come up with a general class of processes that fit it. And so I might quote Peter Hall here, he was not really fond of this condition. On the other hand, he acknowledged there's nothing, he cannot do anything better. On one hand, he doesn't like it, on the other hand, doesn't know how to improve upon this. Um anyway, um this is not what what This is not what we are aiming for. What about the algebraic case? Now, I forgot to mention before looking at this problem, of course I've been doing this for a long time, estimate longer variance. What I've been doing, I've also been using the Minimax optimal estimator and then just do a plug-in. And I didn't really think about this useful. Is this useful or not? Am I doing something incorrectly? Of course, consistent, so I know I'm in the long run maybe it should work, but is this the optimal way to go? We don't know. I didn't know. But I just did it. But I'm in very good company. So there's very famous works by Andrews and Liu and West. There's this very famous Liu and West. As this very famous UN West estimator. And this is exactly what they propose. So there is a model selection how to minimize optimal estimation for this Borman variance and then just plug it in, use it for normalization, and then let's use this for inference and so on and so forth. And this is being heavily, heavily used in the literature. Many, many, many thousands of citations, very interesting. It's very interesting. I don't know if the proofs are actually correct. I don't know. But I know for sure this has already been done by Michael Neumann and Wagner Kond. And these are correct. So these are definitely correct. These might be correct. I have to try. I didn't check them in the detail. Yeah, okay, but yeah. They are really, really popular. Questions: Is this the way to go? I don't know. At that time, I didn't know. There are many simulation studies, really, really many simulation studies. And depending on which side, which team you are, they say it actually works quite good. No, it doesn't really work at all. And they've come up with different solutions. Some call it Some call this the fixed B design. So the idea is again self-normalization, but a different one. So you come up with two different statistics. They both converge in distribution such that the variance cancels out. This is the idea here. So this has been around for a while with KSS statistics and these kind of things. But the idea is really you. Things, but the idea is really use two objects which have the same variance, and the variance just cancels. So, problem is this is not a normal distribution, this is significantly heavier tails. So, you will lose power for sure. On the other hand, these people claim that we have a better control. Their studies show this is more robust, has a better rate of convergence. Has a better rate of convergence somehow in finite sample, you have a better control. There's some more recent works here. Drawback is if you really want to do some theory, this is only in the Gaussian case. This means linear processes. So what about more general classes? So this is basically the literature as far as I know. So So, back to the original problem, just a reminder what I want. I want to find an optimal bound for this, and I want to do some model selection and hopefully get something meaningful. So now it's time to talk about the setup a little more. I said weak dependence, so what do I mean by weak dependence? Now here's the point. So, I always So I always assume we have the whole process, but effectively it's only a finite sample. And this means we can always represent the random variables in this way. You can always find a function, gk, and k independent random variables such that we can represent these random variables in this way. This is sometimes called the rules. This is sometimes called the Rosen plug, the Rosen tar, I'm not sure transform. It's just a quantile transformation, so we know what we know from statistics, conditional quantile transformation. We can always do that, so there's a very natural question. What happens if we replace one of these random variables with a different one and measure perturbation? Then a very natural one or Natural or one or the whole passed, right? And then a very natural measure of dependence is: okay, let's just take the difference and see how large the perturbation is. And if this is further back, we expect that the difference is smaller as well. And this is a measure of dependence, right? What we can do. And how does this look like in a linear case? For example, it's very easy if you look at a linear process. If you look at the linear process, it's just essentially the coefficient. Replace one, and if you replace the whole past, then you can use the Martingale argument. Basically, the square root of the sum of squares. The key point is you don't need to assume anything about these random variables. If you really do this construction, you can actually say. You can actually say, okay, these are uniformly distributed, but it turns from when you want to apply this, just some measure space and independence. This is really useful if you want to control this dependence measure. I didn't invent this, of course. This has been around for a very long time. You comment: if this function does not depend on k, then p K, then people sometimes call these Bernoulli Schieft sequences, or homogeneous Bernoulli Schiefft sequences. And these again have been studied for decades. There's Wallstein theory, this famous isomorphism theorem and so on. There are stationary systems, I didn't know that. They are stationary and you can show this function needs to depend on k, which is surprising. Which is surprising, but there are examples. So they are stationary, this function you really need needs to be. And I think the first one to think about this was Ibrahim Imov, a long, long time ago, who was actually interested in number theory and then invented this notion. Billingsley, of course, he used it. Gertz and Hib used it and then was sort of And then was sort of silence for a while. And then Weiberu reinvented it somehow, so he made some really deep connections to matting and approximations and realized this is really capable of deriving many sharp results and really popularized this. And you showed that actually this is Actually, this is a really, really general weak dependence measure. You can control many, many processes with this kind of weak dependence. Okay, I'm very good. So, why is this so useful? So, how do the typical weak dependence conditions look like? You put some condition on the decay. A needs to be something, and if this holds, you can prove I don't know that central limit theorem, for example. So A equals zero is sufficient for a central limit theorem, actually. So it's very weak. What's also convenient, it transfers to transformations. You can take any basically any kind of if I actually need a little smoothness, Berler is sufficient, for example, generalized Berla. And this essentially transfers. And now you can think about: okay, this is one kind of mixing. There are many, many others. You might have heard of alpha, beta, phi, c, tau, and so on and so forth. They've been around for a very long time. How do they relate to these kind of, you could also call it mixing conditions? Answer is unknown. Nobody knows. Answer is I don't know, nobody knows. Very hard, hard to say anything in this case. You cannot establish it by calculation, very difficult. On the other hand, who cares? I don't. If you want to apply it, turns out these are usually easier to verify. Verify beta mixing or alpha mixing, even in case of Markov processes. Of Markov process is not so easy in many cases. So I would like to give you a few examples. Some of them are really well known and I hope some of them are new or exotic. Classical example is Gauch process. This is very easy to verify in this case. So this is a univariate Gauge process. This can be extremely Process, this can be extended to what is called augmented gas garage process. It can be extended to Hilbert space Velo disguise process and so on and so forth. So this is sort of an ARMA process in a certain sense. Only the innovations are different. Why does this work? If you just iterate this recurrence equation, you get the formal expression. You get the formal expression which looks like this. And given some conditions, this is the key quantity here, this gamma. If this is smaller than 1, and if this can be generalized, this formal expression is actually a solution. And turns out the garage process is actually not so interesting. Why? Because it has exponential decay. But still, people have it. But still people haven't used it in practice and it's it's fairly easy to validate this. Okay, something completely different. Left random walk on a general regular group on a regular group. So what's this? People in physics, this is really important for people in physics. So what you do is you draw a sample from the regular. You draw a sample from the regular group, from matrix, and you keep doing that independently, and then you just multiply. You take the norm, and then you take the log. Random work on a regular group. And why is this important for doing mathematical physics or physics? Because the systems, they are recurrence equations, and when you solve them, you get. You get products of matrices similar like something like here. And then they started to study this product because they want to know about some properties of the solution of this recurrence equation. So people started studying these random works, and I was really surprised. So many, many books, literally. Many, many books, literally, many, many, many books have been written only about this process. Really, really important, appears to be really important in physics. And it turns out this is not so easy to do, but fortunately, somebody else did this work for me. So, what you do is you take the product, you take a norm, you take the log, and then you know, log is you can divide, multiply, divide, multiply. So, you can write this. Divide multiply. So you can write this down as a sum of certain random variables. And you can show, okay, this is actually a more interesting system. You have algebraic decay. And the decay interestingly depends on the number of moments. The more moments you have, the faster the decay. And what's interesting What's interesting to note, there are many results regarding the system, regarding central limit theme, strong law of large numbers, and so on. And they always require the existence of the moment generating function. And using this um Bernoulli coupling, you can show you only need um in many cases only eight moments or even less. Only eight moments or even less, which is a huge improvement from the existence of moment-generating function down to eight moments or even lower. They use very different arguments, but non-probabilistic arguments, but staying more on the probabilistic side you can actually improve massive learning conditions. Another example which is not so Which is not so well known, I think, is SDEs. Stochastic differential equations. It's also in the framework. So, this is a continuous time process. I put down like a standard stochastic differential equation. What do we need? So, we need this standard condition. If this holds, it's very well known. We have a pathwise unique solution, strong solution. Wrong solution, actually, if it's powerplasing it. Yeah, it's a very, very common SDE. And how does this fit into our framework? And this is where I would say one can really see the power of this notation. We know ground and motion increments are independent, so what do we do? We chop up the interval into smaller parts. Into a smaller part. On each interval, we take the increments and consider the process. So in this case, the innovations is really the process of the increments. Those are independent. And we get our representation. So, actually, very simple. And then you can think about: okay, what happens if I replace I replace one of the Berlin motions in the past, and turns out we again have exponential decay. It's not so surprising because special cases you might know about this is like the Ronstein-Rundeck process, for example. This is essentially an R1 process, has exponential decay. So it's not so surprising that stochastic difference equations, like E2 diffusions and so on, they have exponential decay. But it's maybe unusual or surprising to look at it from this perspective, right? From Bernoulli shift processes, it works as well. Important example is the Langevin diffusion, the overdamped version, which is important for Markov chain, watercolor and all those things. Plus people of look People often look actually these kinds of things. Instead of the sum, they look at the integral because it's a continuous time process. No big problem. We just don't process and then we can rewrite this as a sum, right? This is just linearity of the world. So this is now our framework. There are many, many more situations. There are many, many more systems, I think. I actually have lots of time, but I'll spare you this. So, there's many, many more systems, like iterative random systems. They are quite popular. These are sort of Markov processes works as well. Again, exponential decay in this case has been extended to symmetric spaces for examples. Examples. Again, a more exotic example. This is, for me, this was surprising. A so-called slowly mixing dynamical system. Now here's the viewpoint very different. You have a probability space, you sample one random variable, and you do it only once, one random variable, and then you apply this deterministic function again, again, and again, and again and again. Again, again and again and again and again. But on one end it's a very simple function, but this process gives you a fairly complicated, I would say, process. And the point is you insert randomness only once at the beginning and then you just apply this deterministic function. You can predict the future perfectly, in some sense. But still, this is. Some sense. But still, this gives you a random system, and you can again essentially validate, okay, in this case, you have algebraic decay. If again, a weakly dependent process, even though randomness is inserted only once at the beginning, you apply this deterministic um transformation, gives you a weekly dependent process. This is actually what people have been doing in dynamical systems all the time. Doing in the numerical systems all the time to look for systems which, in some sense, are deterministic but behave like random systems. Point of what they are doing here. And you have to work a little bit, this is not straightforward to get this kind of result. But again, unfortunately, somebody else did that. I d have to to think about that. Okay, then as Markov chains, I think I have to skip that now. I think I just skip that. If there's only one randomness at the beginning, how do you do this replacement of independent counting? Yeah, this is not easy. It's a set and always find this representation. And in this case, you have to really construct it. And this is not easy. And this is not easy. In some cases, actually, what in a linear case this is straightforward, for linear processes, for gauge processes, this is also straightforward. But if you have this perspective, this is not straightforward. You really have to work. For many questions from dynamical systems perspective, actually this representation is more convenient. But if you want to control these fixing conditions, this is actually not so convenient, and you have to work a lot. This is very recent work, a few years ago. Okay, yes. Barkov Chenzovinchen model and so on and so forth. Okay, I have a little time left to tell you what's what's actually happening. So I would now introduce some assumptions. Assumptions. As I said before, so we need a certain DK. What's my DK here? It's 13 over 6. That's just what turned out to be. I need 6 moments. It's a little more what you would expect maybe. So variance usually says 3 moments. I need 6. And I need some conditions on the bandwidth, which should grow to It should grow to infinity and it shouldn't do so too fast. So one uh so a quarter, which I would say is okay, it's fine. We will see it's sufficient of course. In fact, this can be improved. If you have more moments you can weaken these conditions. Um so here's the first result and uh I have to admit, for me this was surprising. I have to admit, for me, this was surprising. I didn't expect this. I'll tell you in a minute why. But what's the result? So, we do get an optimum variation bound, but not standard normal. So, the variance here is sigma squared over sigma squared p. So, it's not what we're aiming for. The whole point of standardization is actually we want to have a standard normal distribution which doesn't depend on any. Which doesn't depend on any parameters. So if we normalize, okay, we get the normal distribution, but still, this is by no parameters in here. Okay, so that's the variance. So how does our optimal bound actually look like? Roughly. So it's one over square root n plus different standard normal distribution and this different variant. But this is Variance. But this is very easy to compute. This boils down to the difference of variances. And what's that? Actually, it's just the bias, right? This is the bias. So we can, so to say, get this coronary if we can make the bias small enough. Scald n rate, we get optimal bound. We can improve 1 over scalar n, this is the optimal barrier 0 mod. Nothing to improve here. So, and these two are equivalent. Finally, so what's the sigma v? This? Just the. Ah, sigma b. Okay, this is this is the this is sigma b, right. Sigma B, right? You chop off at P. So the diff, so sigma squared and sigma P squared is exactly the bias. And now you need to think about it, what does it mean? So this is a non-parametric problem where there's no bias variance trade-off. Right? There's no variance. No variance, it completely disappeared. All we need to do is over smooth. So it's somewhat a degenerate non-parametric problem and we don't, there's no need of any model selection. Not necessary. All we need to do is just make the bias disappear, choose the bandwidth large enough, and that's it. And this is, yeah, for me, this was surprising. Yeah, for me this was surprising, but I would say this is a very natural statistical problem we need to studentize. So this is not a degenerate, well it turns out to be an information theoretic degenerate problem, but it's not like a pathological example that you would give a student, come up with an example where you have no barriers-variance trade-off. It's just, I would say, a very natural, non-pragmatic process. Non-pragmatic problem where the variance just disappears, no bias variance trade-off. And in particular, no model selection is necessary. Another point. Plus, now let's get back to the new and West and Anderson estimator. So what are you doing? Minimax optimal estimator, plug it in. Okay, we can do that now. And it turns out if you And it turns out, if you do that, you are sub-optimal. You don't reach the optimal rate. So, at least from that perspective, there's no reason at all to use New York and Rasiners. They give you sub-optimal rates. Not the optimal procedure. But exponential decay, this is all very different. If you choose a constant large enough, Choose constant large enough, everything is fine. You can use the minimax optimal estimator, you still get the optimal rate. That's sort of the moral of the story. Algebraic decay, minimax estimator, not good. Exponential decay, minimax optimal estimator. Okay, constant large enough works. I think in principle works. Okay. Okay, you can play the same game for Wasserschland metric, at least for V1. Same result essentially. Don't need to change much, conditions change a little bit, but control Vasov diametric. And once you have Vasov diametric, you can also control LP. Same result. Same result, doesn't change much. And now we can start thinking about: okay, how can I, what would be the next step? Does this also work in multi-word case? Not so easy. Not so easy, but I think there's a good entry point. Edgeworth expansions, that's possible. Gamma multiplication principle. A few minutes ago I told you. A few minutes ago, I told you not really possible. Turns out one can do something. You don't get the Karma moderation principle, but you still get an improvement. So, this is possible. General Wasserstein distance, not so easy. High-dimensional, surprisingly easier than a multi-word case. At least some certain aspects. So there's a high-dimensional blessing. Blessing which helps, which you don't have in the multi-word case. An infinite dimension, I have no clue. I don't know what to do. The question is always how do you normalize and with what do you normalize? In a multi-wheel case, what would you do? You would usually take the inverse of the coherence matrix. I-dimensional, you wouldn't do that. So you do something else. And this turns out, okay, this box. Works. Okay, then thank you for your attention. Finished a little bit earlier. Any questions? Comment a bit on your proofs? What the pardon? What? Can you comment a bit on on the tools that you have with the UP for security? The tools. The proof. The tools. The proof. Ah, the proof. So the proof, in principle, so what you do on a very general level, the problem is if you don't studentize, there's results how to prove various bounds for weekly dependent processes. And there are spectral proofs? There are, but the ones I do, there's not all spectral proofs. No. I don't use the pattern. No, I don't use the perturbation approach. But in a studentized case, the problem is how to do with the randomness. And it turns out, with some tricks, you can rewrite the whole thing as the sum of weekly dependent processes which are not studentized. So you get rid of the studentization, and then you end business. That's essentially the idea. So I see the final result is uh Universe version. Have you linked it with the original problem, the PC? Ah, well, how do you link it? Um the the the principal scores, as I said, depends on are they are they just correlated or independent. Or independent. Independence is easy, then you don't have any weak dependence. But if you assume they are only uncorrelated and have some form of dependence, for example weak dependence, then we get back to this setup. So what Kuczynski and Lumisi did, they actually considered the Gaussian case, in this case principal spaw-independent. With Martin Wall, we generalized that, but we still needed some conditions. And yeah, then. And yeah, then that's one way to go if you have SUP dependence. I have a very general question. So regarding the extension to the multivariate case, because for the universe you have this mass representation function of the epsilon. So for multivariate case, there seem to be different ways to introduce these dependency measures. Do you have any comments on what are my Yeah, multi-wide is still no problem because norms are equivalent up to constants. High dimension, it's different, so in this case I just do it coordinate wise. In infinite dimension, if in a Hilbert space, this is no problem. But if you in a general Euclidean space, then uh you have to do it calling twice. Otherwise cannot control it. Yeah, there's there's no uniform control. Uniform control. I don't, I mean, you can you can do count examples. This is just not a strong enough notion to control everything. No other questions? Let's thank Morris again. Um okay everyone. I I needed to make an announcement. Uh so the first uh speaker this afternoon speaker this afternoon, Yiling Chen, has to cancel her talk due to a health issue. So we will have another speaker and I will send out the detailed information about her talk. So she will be at 1 1 30. One o'clock.