that basically indicated that the number of reported um sexual assaults on campus had gone up substantially. And the person who charged the office of producing these figures was saying that they don't see these reports as anything other than positive, not positive that the violence occurred, positive that in the aftermath of us experiencing violence, our students coming to us. And my hope is that the report continues to increase if it's happening, then I'd want to know about it. If it's happening, then I want to know about it. So, this actually caused quite a bit of stir. The culture reported by procurement issues in New York State were much lower. And this was a significant spike from previous years. And so, as you can expect, the office is sort of interpreting this as a sign that people's willingness to report is going up. But in the news and in the modern discourse, it was perceived as very damning of what might be happening whenever it is happening elsewhere. Another thing that happened. Another thing that happened in New York State was in 2009, that's right, yep. There was an input in the mass wars on standardized tests, and politicians were actually displeased with this result. They viewed it as like standards being lowered rather than people doing better. So, again, this is sort of interpreted as, you know, by some audiences, as evidence that things were actually going poorly, even though it was presented by the Even though it being presented by the agency administrative test, it has been some improvement. And then the anecdote that actually really kind of started my interest in this general phenomenon was a description of what Rudy Giuliani ended up saying to the person who ran Comstat in New York City when, like in the 90s, the number of gun arrests in New York City ended up being lower than some other cities in the Some other cities in the US. And Julian was very upset about this because he thought that it meant that the police must be doing a bad job. And in response to this, this person who ran concept said, no, the reason this is happening is because crime is actually going down. And so this is, again, another indication where one audience is viewing this as a sign that there's improvement, and others see this sort of like a lack of efficacy and the practice that they're trying to monitor. Monogram. Okay, so this was like all the stuff I had to be thinking about before COVID. And then, of course, COVID happened, and then like this exact thing became like the standard point that everyone made about COVID statistics. And so here's a quote from Trump, where Trump really went out of his way at some point to try to assert that the only reason that there was an increase in important cases of COVID was because there was just more testing going on. So it wasn't that there was actually an underlying change in the rates that people were infected, it was just that. Infected. It was just that people are testing more. And of course, we know all the reasons why this debate kind of carries on. Okay, so what's going on here? I think what this is kind of all about is that when we formulate a problem, a policy problem, we often try to evaluate type in metrics that are inherently ambiguous. And this can even lead to confusion and contestation about what we even have as policy goals and how we know that we're trying to achieve them. And I think the reason. And I think the reason for this is relatively straightforward, and perhaps it's all very obvious to you as I explain it, but I'll try to be a little bit more precise here. So, the reason I call these equivocal measures is that they are particles of two terms, both of which are unobserved, and in particular, terms that can interact with one another. And what I mean by that is the following. So, if we just pick up the first example from the vignette, you can think of the reported rate of sexual assault as basically being a product. Is basically being a product of the true incidence of sexual assault on campus and the probability of it being reported. So basically, R is the product of T and P. Okay? And this is as formal as I'm going to get because I'm not a computer scientist or a statistician, so this is as much as I'm going to pretend to use any formal. Yeah, stop here. Okay. So a few things to observe. We don't know the true rate of sexual civility, and we also don't know the probability. And we also don't know the probability of it being reported. We may have no way of trying to form an exhaustive census of all the results that happen on campus. And we can also struggle to kind of figure out methods to allow us to figure out the rate that would be reported. There are, of course, statistical methods to try to deal with this. There's lots of different ways people have thought about this. I will not get into that because I'm actually sort of more interested in the sociological aspects on this. But I just wanted to, again, try to be precise about what I'm getting at. Precise about what I'm getting at. And when I say that these terms interact, I think this should be relatively obvious here, but it's especially obvious when it comes to policing. So the likelihood of sexual assault taking place on campus may depend on the likelihood of it being reported. So for example, if the probability goes up, I'm thinking is that perhaps the incidence goes down. You can again kind of restate this as like if measurement improves, it may deter the measured activity. Okay? So kind of performative estimate may be on this. performative aspect maybe on this. And in the absence of being able to decompose this into like its constituent parts, it can be really hard to interpret any changes in this figure. So I won't ask you to actually look at this table in any detail, but it basically tries to point out that without understanding the value of these constituent features, it's very possible many different things are going on under the hood when the rate goes up, down, or even stays the same. So, as I said, I'm kind of interested in the sociological aspects of this. So, unless we kind of understand what's going on within these component terms, we can't really interpret them. And further, developing interventions on the basis of these matters is fun because we also don't specify what happened to each of these terms. We want this number to change, but we don't specify which of the terms we're going to change in order to achieve that. And what changes we want to observe in these kinds of measures will depend on what we assume to be readily changeable. And so I'll try to say more about these two points. Sorry, I can move on to my screen. I think the order is fine. Okay, I need to split with this. I mean, the order, I messed up the order. Okay. So from a sociological perspective, So, from a sociological perspective, it's interesting because these equivalent measures can go from being perceived as a count of some activity. So, for example, the amount of crime or gun arrests or the incidence of sexual assault on campus, to being an indicator of how well someone is doing their job. So, like in many of the venues I mentioned, the people who were tasked with performing this thing that we were going to try to assess with this measure were often being evaluated as if they were not doing a good job as that number went up. Went up. So it allows for this kind of interesting, equivocal way of using the speaker. However, if we believe that the true rate of something is inelastic, then it might be reasonable to say that the recording rate is actually telling us something about how well someone is doing the job. But of course, if we think it's elastic, if it changes in response, then it can be a much more complicated square. Complicated square. Let me zoom in a little bit on primitive police, which I don't think I said some especially bizarre dynamics. And to my knowledge, I don't think anyone has put it this way, and I'd be interested to hear if I just missed something or if I actually get the argument wrong, but this is the real thing that kind of stood out to me when I was thinking through the issues. So in a lot of the setups, these kind of predictive leasing problems, it's kind of left unspecified why we're performing the prediction. So, you know, there's some things that perhaps we want to be a little bit more concrete about. So, do we assume that by informing the police that we're going to increase the probability of observing crime? Or do we also assume additionally that we're going to deter crime from happening? So you can think this is being like, I deployed a place somewhere. Is it simply that we're eating ECG, or are we additionally or independently lowering the true rate at which this is happening? And as I was saying, by recording this one, As I was saying, by recording this one measure, you have kind of concealment of these conflicting beliefs about the nature of the problem at hand. So, why, you know, generally, might you think that localities want to engage in predictive policing? And I'm not endorsement, I'm simply saying this to me is a more rational way of trying to characterize this. But of course, I have many objections to this practice as well. So, you can think of it as being, and it's kind of just. An attempt to simply increase the rate at which crime is being observed and therefore probably arrest more people. You could think of it as being an attempt to decrease the rate at which crime is happening at all, which perhaps you might think of as like deterrence in the moment. You show up, the crime otherwise would have happened because the police are there, it doesn't happen. Or is it to decrease the true rate by increasing the probability that you observe it? And here we could think of this as the church over time. So, like, you still arrest some people when the police show up, but there's awareness of. Show up, but there's awareness among other people who otherwise are going to give claims that they're likely going to be arrested has gone up. And so these are just different ways of thinking about what the task really is. Okay, and so if the goal really is to try to reduce the true instance of crime, which to me I think seems like the more rational way to solve this problem, then kind of weirdly, a good prediction would be one where the police are directed somewhere where they then ultimately fail to observe the crime. Because the goal is to. Because the goal is to deploy someone somewhere such that the crime does not happen. But then, if you think about this for a moment, you realize that the data that we're using to train the models actually doesn't have any of the information you want. So, we're training these models using data from historical, genetic historical data, which is basically just reported crime, right, where it has been observed. But that's cases where the crime happened positively despite. Where the crime happened, possibly despite the police being there. And so, for the sake of argument, I'm sort of setting aside the fact that, of course, there are other ways that the police might learn about this, but stick with me for a moment. And even more bizarre, and this is the thing that I really find kind of strange, is that if what we're trying to really predict is we're going to deploy police such that the crime would not occur, you have zero examples of that because you would not record that. Okay, so all of this might seem very silly if you think about the fact that, like, You need to think about the fact that, like, this is really just not a prediction problem, right? I have this kind of counterfactual in the title here, and so this really is a causal question, right? Like, what is the treatment that affects of having deployed the police in a particular way? But again, I don't think the discourse we have around this really orients around that way of thinking. And so it leads to these kind of equivocal measures, which can be interpreted in all different ways and allow people to then have very And a lot of people then have very fierce debates about what's actually going on. So, my main provocation ultimately is to say that, of course, there are interesting statistical things we do to try to address this problem, but I'm also interested in the sociological practice around this. How is it that people use the equivocal nature of these measures to be able to engage in different arguments about what's happening and to kind of possibly sidestep some difficult problems about what is even the policy problem? Uh, what is even the policy problem again? Okay, thank you, I'm done. Okay. Okay, thanks. Um, I guess we'll move to our panel part now, so I'm gonna go ahead and get it. First of all, here's the things you have to do quite a bit. Okay, so I guess just to start it off, like one very broad question. The second part of the title of this workshop is Bridging Prediction and Interventions in Social Systems. And you all have different backgrounds and trainings in studying these social settings. I was wondering if you could highlight how maybe in the process of your work, some conceptual distinctions you came across that maybe were pretty striking or were obvious to you based on what you read or what over your training that were perhaps not. Training that were perhaps not obvious or reflected in the existing cultural difference in learning or something. Do you have any examples of kind of like concepts or distinctions or ways of thinking that are perhaps not standard in the typical like call to inference statistical methodology that we overlap perhaps? Yeah, I think some of the stuff that me and Lily were mentioning earlier. Me and Lily were mentioning it earlier, thinking about like normative assumptions and a lot of these models. You know, one of the things I was trying to do with situating this discourse is like, you know, talking about like how philosophy of language is a big thing that comes up in a lot of debates about the nature of race and what it means and whether or not it refers to anything, etc. So for us, I definitely say, like, you know, deploying traditional methods and philosophy of like conceptual analysis, normative analysis, normative reasoning. Normative analysis, normative reasoning, philosophy of science stuff, you know, and I think something that was mentioned earlier, and I didn't get a chance to get to it in my talk, but Lily did mention earlier, the importance of like disaggregating categories, and not just race, but within racial categories like black. You know, I had some slides in there that like look at the difference between African immigrants versus descendants of Africans enslaved in the U.S. And how when you try to predict things like educational attainment, using black as a variable is a terrible choice. As a variable, it is a terrible choice of variable because, obviously, as a lot of immigration processes are fairly selective in the U.S., they come over here with different types of education. I'm four generations myself, away from slavery in the U.S. And it really makes a difference in even how we design policies and how we even, you know, make arguments to even suggest that certain policies are doing what they're supposed to be doing. Point to groups and say, look, we're doing better in terms of this, but then. Better in terms of this, but then native blacks are still bottom cast, you know. And that has to do with how variables are defined, what variables are chosen. And I just don't think people think about that at all in this type of literature. That seems to relate, actually, I think, to Solan's point about the measure that we're tracking may or may not be aligned with policy goals, where if you're tracking phrasing this aggregated. Phrase in this aggregated sense. Like, does it like what policy goal is that accomplishing if that's the way you're doing some kind of intervention? Like, maybe that is a link that I see maybe between. I also think there's like when people are using causal inference to specifically study. Inference to specifically study about discrimination, I think it's important to recognize: like, you're doing an empirical exercise on the one hand, you're trying to find out, like, causal effects, that's like an empirical exercise, but it's interesting, but you've equated it with a particular normative exercise. And like, therefore, your causal analysis has to, I don't know, be held accountable to that claim of normative conclusion. Conclusion. And I think that that's the deal that you make when you say that I'm going to study discrimination by studying causation, which I'm not even saying that one ought not to do that, but I think that that now puts a lot of burden on the causal inference exercise because there are ways of conceiving of discrimination that don't at all kind of think about the question about but for causation. They just think about, well, what is just, you know, when are people who are positioned in a certain way? Positioned in a certain way, when do they warrant similar treatment? When do they warrant differential treatment? And if you frame it that way, that's just a purely kind of ethical question. Now, if your way of answering that question is to do causation, right, like that's the kind of, that's the task that you've set yourself, then the way that you're doing your causal analysis has to be held accountable to this normative thing. Like, how is it the case that your causal analysis is inheriting or it's not inheriting certain views about substantive? Certain views about substantive similarity or substantive equality or similarly situated, like now you really have to make sure that your causal analysis is somehow incorporating this other stuff about substantive similarity. And I think that the problem oftentimes isn't necessarily that you've equated racial discrimination with race causation. Like, that's fine, that's just the way that you're saying it's a problem, but that actually bears on how you do the substantive causal analysis. To the causal analysis. Now, of course, there are other ways, which is that you could just say that visual discrimination is not this causal exercise, but that's a totally different thing. But once you're in the causal game, you do have to think about, like, am I doing inference in a way that's, again, held accountable to the normative enterprise that I'm supposedly investigating? I don't know if any of the channels are. Responses before. So can you hear us, by the way? Sorry, it's hard for me to hear someone close to the audience, I apologize. Yeah, it's okay. Okay, I'll try to repeat questions. I guess I can say really quick, like in response to Angela's question to begin with about sort of like discipline-specific things, ways of thinking. I think one thing, I have a very weird disciplinary background. I did my PhD in a philosophy department, but it was like a weird philosophy department. But it was like a weird philosophy department, and I was in computer science talk about biosets. But one thing that philosophers are used to, at least analytic philosophers are used to, are these really convoluted definitions of things where they say this thing has property P if and only if in context X it looks like Y relative to Z or whatever. It's kind of very convoluted sounding things. But one of the nice things about that way of thinking is that That way of thinking is that, like, defining things relative to some sort of context or relative to something is just normal and expected. And so, like, in particular, my introduction to thinking about causality was through the work of Peter Sperdes, Clark Leemore, and Richard Shinus, who wrote this book back in the 90s. And in that book, they, unlike Pearl and some other contemporaries, they weren't really trying to define causality exactly. They were trying to axiomatize causality. To axiomatize causality, they're trying to give you rules for reasoning causally rather than defining it. But they would have these kinds of definitions in their book that were like: A causes B relative to variables V in the graph if such and such holds. And so already building into the very basic definitions that appeared in the axioms is relative to the choice of variables. And so I think that fits quite nicely with some of the stuff that Millie was talking about because first. Like, first carving up the world in the right kind of way makes a difference to whether this counts as a cause or a potential cause at all. And I think that in statistics, at least, which is where I am now, that's less often the way people think. I think that's so my answer about this is coming from statistics, like my first training before getting into causal-related stuff was more about high-dimensional statistics. High-dimensional statistics. So what that did was that made it so that it was really obvious to me that notions of fairness based on not including a sensitive attribute in the model were definitely wrong. Because if you have many features, you could probably construct something that's highly correlated with sensitive variables anyway. And also, when it came into working on fairness, To working on fairness, I think about things very similar to the way you set up earlier, Lily, was the choice of variables is what matters when people are trying to define fairness. So an example is, say, the gender pay gap. And this is something that's been debated a lot in economics where the classical thing to do is to basically control away the gap, to just condition on all sorts of things which tend to be correlated with gender, to make the gap. Tend to be correlated with gender to make the gap smaller and smaller. And so I came to see debates over which definition of fairness is appropriate, which choice of variables are appropriate, as all having to do about choosing the variables that which variables you should condition on when you're checking for parity. And then I thought, like, the principal way to do this, choosing variables to condition on, I think there are some. I think there are some other ways of doing this, but like causality was the most natural way for me to think about it. Like relevance is another way of potentially thinking about it, but yeah. So that was just that background kind of made me prime to think about this seems like a causal question. Yeah, Solana? I don't know if you want to weigh in. Yeah. Yeah, we go too. I have two quick things. I have two quick things. So, one is that my general sense of that, and kind of exploring target choice and use selection, I feel like what Routine Mee came up in prior work for me was just that there is some kind of latent notion of causality that often goes into these decisions. So, even in what I was just presenting, right, like maybe it's not made over, maybe it's not even particularly explicit in the mind with people who are making these choices, but there's often some like idea of causality that is guiding the selection of your target. Guiding the selection of targets, what they expect that new target will allow the person acting on the prediction to do. And I think the same thing is even true for features, right? Like, features encode a huge amount of knowledge of the world that people have. And they're not just purely normative choices. I think they're purely, there surely is huge histories that explain why we have the features that we do. And I think a lot of it has to do with some implicit idea of all. I think there's some implicit idea of the false relationship that those movies have to a lot of things that we care about in the world. So it's not arbitrary. And yeah, and again, I think what could be interesting is to try to unpack or identify what those movies are when they're not often made explicit. Yeah, so if there are any other responses, I think we can open the floor for open discussion or questions. I know there are some questions. Questions earlier? Ben first? First of all, already this is like an amazing discussion. It's awesome to hear all of your thoughts on these topics. And I guess like this question is maybe slightly inspired by Solon's comment that he's not going to get any more formal than the multiplication equation that he showed on his slide. And I guess one thing that seems to unite a lot of your Seems to unite a lot of your perspectives, or like that, maybe you all are grappling with in some way, is to what extent formal approaches or like formal models are able to capture the like sort of conclusions that you're trying to make here. So like, Solon, in your case, like you had certain, like, you broke down certain things into probabilities and talked about how they interacted with each other. But then once you started talking about like what we don't know about Talking about like what we don't observe, when we, like, what police aren't able to observe, and that that might be what you're quote unquote predicting. This seems to like wade into territory that's pretty hard to, I don't know, like I'll let you answer, but literally saying, like, like, to some extent, you're talking about selecting features, but to other extent, you're talking about these sort of like similarities and differences and likeness across populations. Across populations, that maybe isn't like the actual choice of how to model this affects the construction of some of these concepts. So, yeah, in general, I'm just curious, like, how you situate formal modeling approaches in the work that you all are doing. Maybe I'll give an example. Like, in the 90s, when in the height of broken windows policing, there was this really Policing, there was this really stark graph that showed that actually it seemed as though judges were being much more punitive with people who were white who were arrested, if you looked at the graph. And people did a bunch of some cause, I actually have stuff on this site, causal effects that said actually being white made the gender more punitive towards you. And that was on a selection, a certain kind of conditioning set. And then people were. Certain kind of conditioning set. And then people were like, actually, it was during the height of broken windows. And so there were a ton of black people who were being arrested, just like willy-nilly a bit. And if we condition on this other set of variables, we actually saw that that gap went away, that first gap that showed that judge being much more punitive. And then there was this debate, like, what's up with that? Like, what was the correct conditioning set? And there's a way of framing that as just like a causal question. Like, what were the judges attending to? What were the judges attending to? What was leading them to make the decisions that they were? Were they actually trying to be more punitive with white defendants? But I think one of the risks of just purely framing that question causally is that that is an extremely normatively loaded question, like whether white defendants or black defendants are being effectively racially discriminated against during broken Windows policing at the particular stage of charging or something. And I think that people are really. And I think that people are really motivated, kind of like what Salon was saying, people are really motivated to think about what the normative implications of that causal analysis are. And I think that's okay. It's okay to be like, it's better to condition on this set of things because I want to paint a portrait of broken windows policing that expands our perspective from the charging. And I think that maybe this is naive, but I think we should be more explicit about the ways. Should be more explicit about the ways that normative thinking is figuring into the basic minutiae of methodology, where we're choosing certain kinds of variables. And that's a way that we can vindicate formal methods, but incorporate them within an ethical analysis, rather than saying we should stop doing formal stuff and we should just talk about what's fair or not fair. And then rather than saying we should not talk about what's fair or unfair and just do the formal stuff. There's just a clear way in which people just are already doing this. Just are already doing this, and that's already what is motivating various statistical moves that they're making. And that this would be much better served if this were just much more explicitly articulated and effective. Oh, maybe Solan first? Sorry, I can still hand. I can't see anyone else on the panel off the speaker. Feel free to go forward. Well, first, I just wanted to clarify the question. Like, was the question just to what extent To what extent do we believe formal methods are valuable when doing this sort of like? I don't know, I'm not even joking, I'm being so serious. I'm really just not a critical question. I think maybe that's part, but also like to what extent can they subsume like this sort of work that like can can they account, can they take into account some of the considerations that That you're talking about. I mean, yeah, I mean, I have work that's in just straight up machine learning that I think is doing something valuable and useful. I mean, but I mean, I kind of think about this, again, from a perspective and philosophy. The debate about like ordinary language versus artificial language is a very old debate. Like, human language is very expressive. We express all types of normative questions in them. And most people, I would imagine, are very silly if they think Martin Luther. Silly if they think Martin Luther King would have done a better job than his speeches if he put them in first-order logic and proved it first. And so, certainly, I don't think for every sort of normative question and sort of normative reasoning you're trying to do, it requires expressing it in an artificial language, whether that artificial language be first-order logic, second-order logic, probability theory, or whatever your choice of formalism is. I mean, that's what I was complaining about yesterday. But I But I certainly do think, insofar as you can express the sort of normative things you're trying to say without obfuscating them or just straight up excluding them altogether in formalism, do it. I would like to see it, you know. But insofar as it's obfuscating or misleading or is not, you know, characterizing the expressivity of natural human language, I mean. Language. I mean, you, I mean, again, this is like a whole debate of philosophy. Like, Wittgenstein was part of this debate that I was like, so I mean, like, I mean, I think, yeah, I mean, dualism is valuable and recognize the limitations of it. I would add that, like, I mean, I think that part of the value of formal tools in considering these questions is putting down on the table explicitly. Putting down on the table explicitly what you're committed to and making it easier to pinpoint what people disagree on. I think there's lots of examples like this, and causal inference in particular. Like, I think Lily was involved in this discussion about police stops and what's going on when people condition on being stopped and police stops. There's a lot of discussion back and forth. And I think what you hope could come out of those discussions is people writing down exactly what assumptions. People writing down exactly what assumptions they're making, and then we can have a more clearly defined debate about what we think is true. But there's another area in which sort of related to, I don't know, something Lily said about like another area in causality that maybe is less familiar to people in this room and which normative considerations I think are important. There's all this literature on actual causation. It's like attributing, not about Attributing, not about variables causing other variables, but events causing other events, and what you should attribute to the, like, some event happens, and you want to figure out what was the cause of that event. And this is one of these areas that is like, it's like mostly philosophers and computer scientists that have written about this. And so, like, Joe Halpern is a computer scientist, has written a lot about this. There's some philosophers who do it as well. And one of the things that, if you've never encountered these sort of Encountered these sort of thought experiments where people are like Susie and Billy both throw a rock at a window, and like Susie's rock hits first but then breaks the window. But if her rock hadn't hit, Billy's would have hit, so like Susie, the cause of the windows breaking. You can come up with like more complicated examples. But like part of what's at issue in a lot of these examples is like moral responsibility, or like at least responsibility or attribution, which has this very normative thing to it. And so one thing Joe Halbert will do is he'll draw these. One thing Joe Halbern will do is he'll draw these diagrams, he'll give you these formal models of what he thinks is going on and then try and answer the question based on like with reference to this formal model. But you might argue that this is like the wrong way around, but actually first you should settle what you think responsibility is about, and that will partly tell you what the model should be too. Like one of one, I think that the perspective that he had taken is like, first you say like what you think the s causal structure of the events in the world is like. Of the events in the world is like, and then I'll tell you who's responsible. But another thing you might think is: first, tell me what you think responsibility is about, and that actually will have an impact on what you, like, how you describe the causal structure of the world, which is, I think, another example of a place where normativity comes in. Solan, did you want to sing? Oh, actually, I was going to say something very similar to the first part of the video. Specifically, very similar to the first part of the previous comment. And maybe I'll just expand a ton of it, which is certainly like what was interesting is thinking about how difficult it is to pack in all the objectives you might have in choosing a target. And actually, I think Simone is there somewhere, maybe. And Simone has an excellent paper. Yeah, great. Simone has an excellent paper looking at the relative merits of developing a policy by choosing a target. You optimize for it. A target you optimize for it rather than kind of choosing the particular features you're going to use another rule that like aggregates those features and make a decision. And I just want to say that it's sort of a key, as I mentioned with the previous comment, where specifying the actual goals you want gives you an incredibly pleasuring experience because it's very hard to actually tap in all the things you want. But it can also be a very productive exercise because you have to make explicit what those objectives are. Whereas in a different process, maybe those can be left implicit and you're sort of allowed. Maybe those can be left implicit, and you're sort of allowed to choose some features and some rule that aggregates them, but you don't specify what it is that you're trying to achieve in doing that. And so I think this is not like an endorsement of formal methods exactly, so much as the formality of some of these techniques can be useful in making the terms of normative debate more overt. One last thing to this about formal using formal Using formal methods and language. So I also, one of the things that I like about formal methods is actually that you can be agnostic about their meaning, about their interpretation. So you can have, you know, I can write down a simple graph. I have variables, arrows. Writing an arrow here means like I'm assuming something about causality, but I don't actually have to say anything about. But I don't actually have to say anything about what in the world that arrow is doing mechanistically. Like, I don't actually have to know anything about the mechanism to write an arrow there in my model. So I can be agnostic about that sort of thing, in other words. So I think that that gives me some flexibility, but then there's also some danger built into that. So like sometimes once people do write down some symbols, then they can do the thing physicists like Then they can do the thing physicists like to say and shut up and calculate. But then there's a danger of never questioning the meaning of the symbols. So I think we have to always be open to re-evaluating them and going back and saying, there was a time when I was satisfied with writing this simple DAG down and drawing this arrow here, but now I'm actually really concerned about what that arrow means, and maybe I need to make a more complicated model to figure this out. Model to figure this out. So I had like two questions based off of the provocation. So one was sort of based off of like Lily and Alex or provocation. So you didn't quite get to it, but like right at the end, he sort of flashed this point of like race defined by racialization, which I thought was very interesting, which is sort of like, you know, rather than defining race according to Defining race according to, you know, maybe certain physical characteristical differences which might not manifest actually, which might be correlated perhaps with some of the ways in which people are discriminated. Like, there's been some studies on, like, you know, like black individuals in the States, like, despite their socioeconomic status, like, still face higher incarceration rates than white peers of the same socioeconomic status. And so it's like, there's things that are like, this is kind of. So it's like, there's things that are like, this is connected to like skin color, there's a history of skin color. But I appreciated where you were sort of getting to, which was like, you know, but like the reason we're even talking about skin color is because there was this sort of social stratification due to this. And, you know, what would happen if we defined, like, I also saw you flashed a slide. I wish you had had more time, but you had flashed a slide about like caste in India. And so I, yeah, it's sort of this idea. And so, yeah, sort of this idea of just like, can we define our category in a more sort of like pragmatic way? Where if what we're trying to get at is like, you know, the strata through which people are discriminated against, can we be more direct in terms of defining race by the way in which people get bucketed into that strata? And so I was just curious if you wanted to explain that, expand on that, talk on that a bit more, because I thought that was like an interesting direction. And then just to kind of connect to Lily's. Just to kind of connect to Lily's provocation, you kind of were also talking about these normative assumptions and how sometimes it's useful, sometimes it's not useful. So I'm curious if this racialization definition of race might be a more useful way of seeing why people might model this things. The reason we connect race to merit is because of the way in which we sort of stratify people. We sort of have this sort of We sort of have this sort of like, there's this like racialization around like, you know, different categories of people and merit. And it might not have, like, that the categories themselves are not like this like deterministic predefined thing, but like a byproduct of just like the way society treats certain people. So I'm just kind of curious to hear more about that from both of you. And then just a very quick point on like a quick question on Solan's provocation. Publication. Yeah, I really appreciated just like you kind of flashing that very quick equation as well. And I was kind of curious: do you feel like there's any aspects of that problem that's unknowable? So, you know, if you have increased reporting and then you have sort of like a true incident rate, like you kind of like declared the assumptions that you would need to sort of define the probability of reporting, but then you sort of presented this interesting world in which like, you know, that the report. Which, like, you know, that the reporting is kind of correlated with the true incident rate. And I'm kind of thinking of just like data sets out there in the world, and there are these crime data sets where they talk about, like, you know, these are the numbers, like, these are the arrests, these are the convicted. You know, people sort of combining prosecutor databases and police databases and sort of saying, like, you know, here's the racial stratification by arrest. And then here's sort of of those that are arrested, like, here's who's committed, but we don't. Committed, but we don't have anything for like you know the crimes that were not arrested. And so, like, you can never really get to a true incident rate that is sort of independent of the reports in a way that can inform any notion of that relationship between reporting and true incident rate with respect to probability of reporting. So, I was just kind of curious: like, given the data that's out there in the world, like the fact that we don't have it, you know, we can't We don't have a, you know, we can't like see incidents that are not reported. Yeah, how do you think about just like the knowability of like that entire space? Yeah, so I guess that's two big questions. Sorry. Yeah, thanks for that. Yeah, so the way the argument works in the paper, and like I said, it's in the journal Philosophy of Science right now, is racialization, that's going to be a sort of Haslangrian social races as Has Langrian social races as defined are basically racialized groups. Racialized groups, and this is the controversial point of her argument, is a race. But in order to be a racialized group, she has to specify what racialization is. Racialization is this process of social subordination, basically. And my argument was like, well, look, just by definition, that means positivity violation, basically. And also, this is other point, too. That's not just how you. That's not just how you define a race, that's how you maintain a race. Okay, so that means, as far as racism you maintain, it means positivity violations. So that means you can't know about race or racialization. Now that leads to a dilemma, and I put it, I articulated it in the paper. The dilemma is this. If you have racialization, you don't have causal inference. If you don't have causal inference, you don't know about racialization. Oh, that's a problem. Oh, that's a problem. The other horn is: okay, well, I give up on racialization as my view of social races to get social racist, to ease the causal inference tension. Okay, well, then the horn is an immediate contradiction because you don't get social racists, at least the type of social racist we're talking about, right? And so you're left in a dilemma there. And it's an interesting dilemma. And so that. And so that's kind of that story. And yeah, and so what I say is that's one reason why I'm sort of agnostic about the project. But the second one I think is actually more important for policy stuff, and it has to do with this thing Lily was talking about with okay, ambiguous variables is a very big problem in causal inference. The paradigmatic example to actually think Pyramatic example that I actually think was used in that prediction, cause, and search book. If it's not used in that book, they definitely have talked about it. No, it's actually not a book, it's in a paper on ambiguous manipulation. And it's like, look, total cholesterol, at least as I found out because my ass is getting old and I go to the doctor, consists of, I thought I had good cholesterol, but apparently I don't. It's composed of two different things. And each of those things have counteractive effects. And so if you do an intervention on TC to set it its own value, V. TC the center of some value V, that's ambiguous because you depending on how you did it, you could be pulling the levers in either one, right? Well, you know, the argument in the paper is like, well, okay, well, race, if it's like made up of something even like genetic ancestry or self-identification, if you intervene on self-identification where everybody has like homo, you know, like the same sort of genes, like you might metabolize a drug in a certain way to maybe make the drug look good or something like this. But then let's say I intervene on something else. I intervene on something else, like self-identification, and obviously, like the genes would be more heterogeneous, and then the drug might look poorly to me. So, basically, you get the same sort of total cholesterol issue. But it's even worse when you focus. So, that's like when you're talking about like race. But then within racial categories, it's even worse. Like I said, because you're talking about black, you know, one of the main arguments that the USB is arguing why they don't want to do reparations, it's like, oh, we don't know what that means. That's vague. Means as vague, you know. And when we use it, it's not a noisy predictor for things like educational attainment or income. Because obviously, like I said, like black immigrants are different from black folks who are descendants of slaves in the U.S. I mean, that's a real reason to bearable disaggregate. So you can do real policy intervention. So, like, we can know what groups we're talking about.