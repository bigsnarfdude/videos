My my today my talk is more about the applications and there there are maybe two things related to the theme of this workshop. First is uh privacy. So because of privacy concern we only have GWAS summary data. I will explain just in a few minutes. That's one. Secondly is that I'll talk about also causal inference which can be regarded as an approach more reliable and maybe even more fair compared to other approaches. I will actually first introduce the problem, especially those two applications. And then our main new method here is this, I call this as least square imputation, large-scale treat imputation. And then I will show you some application results using ukibiomac data. And I will end with a short study. So the first application, actually, this is Actually, this is the main application we have been interested in, we have been actually working on. So, we want to do causal inference. Most specifically, we want to IV regression. So, you can see that it's a two-stage model. So, you can see here, for example, X in our application, for example, could be a gene. And the Z here could be genetic variants, for example, SIMS. And Y here is some treatment, some outcome variables, for example, Alzheimer's disease. Ozember's disease. So, as we are interested in whether a gene is causal to Y. A key issue here, challenge, technical challenge here is we might have some hidden compounders. So, you here, you can see we don't observe you. There are many, many environmental and genetic factors that could influence both gene and outcome. So, the key thing here is that if we ignore you, because we don't observe you, so if we just fit this model y on x. Just fitted this model y on x, so that's what I say here. If we fitted this model directly, we will do that. It will cause bias inference, bias estimate, and bias inference, right? So, that's why here is that we want to take the actual IV approach. So, the IV approach is the most popular just two-stage work. It's very, very simple, right? Idea is that I fit in my first stage model. I ignore you because I don't know you. But then in the second stage, instead of using observed, I use the so-called impute. And use the so-called imputed X or predicted X, then theoretically you can guarantee at least asymptotically, you're going to get a valid statistical inference. So that's actually the main idea of this two-stage least square regression, or IB regression. One limitation here is that this is a linear model. And we're interested in hypothesized that maybe for some genes, maybe there are non-linear effects on some disease or some outcome. Disease or some outcome. So we first did this two years ago. We took a parametric approach. If you don't like this X, why don't we just add some higher water terms, X squared, and then so on. So parametric approach. So that's what we did. And technically, this is actually very, very straightforward. But to us, actually, the challenge is we were not really sure whether this would be actually useful in practice because everything here is driven by application of our mind. Our mind. So we apply this actually to the UK bioback data. We did find some actually additional genes beyond the if you just use a linear model. And that motivated us to consider a non-parametric approach because parametric approach, we may still miss, right? We look at our data, maybe sometimes it's not just quadratic, it could be something more complex. So that's why I decided to go with neural network. So as a non-parametric approach. A non-parametric approach. So you can say the second model here, instead of actually just the model x effects are y parametric, we say, okay, let's just put a neural network and the input is imputed expression, okay? And the theta here are all those whatever parameters in your neural network model. So our goal then is that we want to estimate the theta and the end we want to test is that actually whether the contribution of this H theta to outcome is zero or not. So in other words, genes actually associated with Words, genes actually are associated with all kinds. So, I also want to mention that the non-parametric IV regression is not very new. It has been started for many, many years. Even in the context of neural networks, there is this actually well-known paper called approach called the Deep IV. And when we first decided to work on this, we thought we could just simply just took this up called Deep IV. And then we applied our data and they just didn't. And then we applied our data and it just didn't work. And then finally, what we realized is that we read more papers and then we realized actually this actually pose a problem. So because of that, there are two issues. One is that if you do it non-parametric, it will not be stable. So this actually is well known. But we didn't realize that. Okay, that's one. Another is that because you pose a problem, you have to do actually something like an multi-carbon simulation. Like an multi-colour simulation to estimate this. This will be also very, very time-consuming. Of course, you know, when this was started, I don't know, 10 years ago or 20 years ago, the data actually much, much smaller. And for our problem, we just couldn't afford. So those two issues. So we have to come up with something else. So I'm not sure I have time to talk about that, but if I have time, I could come back to explain it. So anyway, so it was just either accepted or just published. Either accepted or just published in this journal about stats. So if you're interested, you can have a look at that. So I'll skip the technical details because that's not actually my main point. Yeah, so that's my first application. The second application is very similar to the first one, but it's much, much simpler. So what I want to do is that I have genetic variants. Then I want to predict some outcomes or some disease. So in human genetics, this is so-called polygenic risk or PRS. Or PIS. It's also called PTS. This actually is a very hot topic in human genetics. And then the typical approach is some variance of a linear model. So what is your outcome? And then you have many, many genetic variants. Three is a big, big vector. And then you want to predict after the outcome. The chemical approach, again, is just a linear model. There are various ways to do that. And then for our problem here, we thought is, well, for some trees, actually, some other people had also done this. Other people had also done this. Maybe a non-linear model is better. So that's what we plan to do here is whether we also fit up a non-linear model, like a neural network model, to see whether we can do better prediction. So those are the two applications. And then both applications, one common thing here is that our model here, for example, we want to use a neural network model. We do need individual level data. And I think maybe for most of us in statistics, we already For most of us in statistics, or even computer science and machine learning, we just take it for granted. We always have individual-level data. So, what is individual-level data? It's just a standard situation here, right? So, suppose I'm interested in predict why using some predictors. See here, in my application, it's just news, genetic variance. But generally, it could be any predictors. So, we usually have this actual data matrix, right? So here is the issue is an individual, is a participant, and then I have an outcome, and then I have many, many predictors, peak predictors. And then I have many, many predictors, peak predictors. And then I have individuals. So I have this big metrics. One case thing I want to also just point out is that here you already in human genetics and all this, our sample says in hundreds of thousands. And the predictor could be half million to several million. So actually, this table actually is both wide and long. So we're dealing with some big data issue here. So anyway, this is a standard situation we're dealing with. We always know that we. We always know that we might have individual level data. But the issue here is that because everyone's genome is unique. And because of that, no one wants to share their genome data with the public. So that's why in practice, actually, there's privacy concern. People don't want to publish their data like this here. So instead of individual-level data, what we have is something like this. So you do marginal regression. That is that you do marginal regression, marginal linear regression, use regressive outcome on each predictor. You get this corresponding marginal estimate, slope random for the beta. And of course, you can also get this variance or standard error or p-values. But the most important are those two things, just the point estimate and the variance. And again, the key thing here is the beta achieve here is just the marginal regression coefficient. And the law of this, I can tell you, probably 95% of human genetics, people just use summary data like this. Use summary data like this. Instead of this, they don't know. Most of the time, people don't want to share the data like this. So they want to share the data something like this. And as I said, in most applications, people just use this so-called GWAS summary data. And you may be wondering why. So the explanation actually is very simple. Because typically, the genetic variance actually, their effects on the outcome, actually are very, very small. Because it's small, linear model typically would work. And then you can imagine that if you want to. And then you can imagine that if you want to fit a linear model, Z here could be a subset of the predictors. If you fit a joint linear regression model, here is just ordinary square estimate. And then what you can see here is that there are two parts. First part is this Z prime y here. Z prime right here. This actually essentially is that beta, that coefficient. It will give you all the information there. And another part is this Z prime Z here. And if you standardize the Z actually, it measures actually the similarity among the individuals. Is a similarity among the individuals or correlation metrics among the individuals using your predictors. And usually, we don't know this thing. But what we know is that there are some reference data published. So here I use this easier. So here it's usually much smaller. And then you can use those published data actually to approximate this because we're talking about the same set of predictors. Of course, here I assume that my sample is from the same population as the reference. The same population as the reference sample. So the idea is that if you fit a linear model, it's not really a big deal. You can recover whatever the parameters you want if you want to fit a linear regression model. But the issue is that here we are dealing with linear models. And we probably cannot recover. So I can tell you, I switched actually quite sometimes. I feel that maybe it's possible to recover, but I feel. So at the end, I thought maybe. But I feel so at the end that maybe it's not possible, okay? Uh, so what I want to do now is I want to state by saying that I only have GWAS summary data, something like that. So, so, so, some treat, for example, HD of interest. And then, in addition, I also have access to, for example, UK biobank data. So, I also have another set of genotypes of those NLCs. So, my question is in the UK biobank data, I might not have whatever the outcome measurements. So, my question is that. Measurements. So, my question is whether I can recover the white or I can impute the white, I can predict the white. So, that's the main point of today's talk. Is that clear? So, in the bio-band data, you would have individual level C's, right? Yeah, I have individual C's. But I might not have Y. So, in my example, there is, we're funded actually by NI. So, we're interested in the Azimer species. And the issue with UKBalvact is that there are participants. Their participants are relatively young, as you know, right? Later onset AD typically happened after 65 or something. Most of the individuals and the baseline, 40 years to 60. So most of the marriages are too young. If you look at the prevalence of AD, it is way too low, that's the general population. So we know that we cannot use them. So we can cover the point as we recover the equipment. Yeah, actually, that's what I'll do. So that's what I'll do. Once I cover the why, I just pretend I know the why. Actually, this is the next page. Yeah, actually, that's what we did here. So let me go to a little bit more detail. So, okay, oh, this is also actually published just very recently, a few months ago. If you're interested, you can check the paper. So, again, let me just summarize my problem. I have a GWAS summary data with just those marginal association parameters, okay, for whatever the outcome of interest. I want to work. That I want to work with. But I don't know for the trial summary data, they only give me those betas, okay? But they don't give me actually the corresponding predictor Z stars or predictor Y stars. And on the other hand, I have a separate data set. Let's just imagine they're from SIM population. And I have the individual level Z, but I don't have Y. So my goal is other than I estimate Y. So here I just put it as white hat. And why I want to have this white hat here. And why I want to have this white hand here is once I have the white hand, I'll just treat my Z and white hand as if they were individual level there. Then I can input to my neural network model. And I can also do inference. I'll show you just very quickly one example of inference. But the main thing today I want to talk about is more likely prediction. Is that clear? So that's the problem. And idea actually is very, very simple. Idea is simple because if I have y, if I have y, because I already have z, then I can calculate the market. Have zik, there I can calculate the marginal association paramples from my new data, right? Just formula. If you standardize zik, this will give you the marginal association. And because I have GWAS summary data, another GWAS summary data, because I'm estimating the same parameter, and the typical GWAS sample size is actually huge. So I can probably assume the beta hat and then the beta in GWAS summary data should be close. Because they're estimating the same parameters. Same parameter of association between a sneak, a genetic variant, and the same output. Need a genetic variant and the same outcome. So this should be closer. And then what I can do, you know, just I can just create a negative regression model. Because I don't know why, I don't know beta hand, but it doesn't matter. If I assume this whole thing here is close to the beta hand star, I can just minimize the least square error. Then I can have actually a closed form solution. I'm just like here. One thing here is that you see that here is that I have this generalized inverse because I have to standardize my Z. I want to get rid of some. Standardize my Z, I want to get rid of intercepted terms because they don't give me the intercepted term. So I have to do standardization such that the mean of each trajectory is an zero. So in that situation, I don't need the intercepted term. And then because of doing that, this matrix actually is singular. So that's one complication. So I have used some generalizing inverse. And then my main application is I'm thinking about the fitting non-linear models. And then Right, and then the people will look at this thing. Here is the review process that people will say that you're recovering by using linear marginal association parameters. How could you recover non-linear information? So that's a very legitimate question. I don't think I have a good answer, but this is one attempt. So one attempt is that it's also a nice property of this method. If indeed my C, my individual level data is the same. My individual level data is the same as in from my GWAS summary data. It's very easy to prove they are exactly the same. My wife had worked exactly the same as the white star. So my argument is that because we're dealing with large data sets, so I can imagine my individuals with Z might be very similar to the Z star. So you can think about the interpolation or something. So maybe every white is somewhat close to one of the white stars somewhere. So in that way, I feel that. So, in that way, I feel that maybe there is still some non-linear information that can be retained. But, in fact, this is actually the main point I want to show you just in my second application. Let me also just give a quick comment about the two other approaches. As I mentioned, the random PIC is a very popular approach. So, one idea that people say is, why do I need this? Because I can use GWAS summary data. I can fit a linear regression model. Then I can predict the why, right? Then I can predict the why, right? Because you give me the z, you can, I fit a neural regression model, then I can do the prediction. Why bother to do this approach? Well, this would not work. The reason is because this is specified as a true model. Because my goal is that in the later on, for example, I want to find which things, which individual predictors are associated with my original white. I'm going to use white hand. Because if you use the first approach, your white hand is generated from specific links. From a specific linear regression model. For example, it will not contain any non-linear information. That's why. Second more important is that for many downstream genetic analysis, it will not be useful because you already specified what is a true model, which of course it cannot be exactly true. So that's the first issue here. And if you do inference, you're going to also encounter many, many other problems too. I'll show you one example. The second approach, this one, is also very mature, especially for people in much. Natural, especially for people in machine learning, is that well, I don't know why. Maybe I know some other predictors covariance, right? So, for example, if I want to predict this LDL, HDL, maybe I can use some other cardiovascular risk factors, whether this person is a smoker, you know, age, sex, and maybe some other predictor variables. Okay, I think generally you can do that, but I think for our purpose, we want to do downstream genetic analysis. If you do that, it will not work. Why? It's because, for example, if I if you Why? It's because, for example, if you use smoking status to predict some outcome, then what that means is that any genetic factor associated with the smoking status will be associated with your imputed white hat if your sample set is large enough. So in other words, I'm going to lose the specificity of genetic associations if I go with that approach. I use other variables to predict. So that's why here in my approach here, I have to stick with the genetic factors because I'm I'm mainly interested in the genetic associations. In the genetic associations. Is that any question? Okay. So I'll show you actually the application. Let's start with the second output. Second application. Because my second application is I have individual level predictors and then if I can impute the y, I want to say, well, maybe my white hand still contains some useful information, in particular some non-linear information. So what we're going to do is that we're going to use the You can. So, what we're going to do is that we're going to use the UK algorithm data. And in Ukraine algorithm data, there are about half a million individuals, but there are different ratio ethnic groups. So, here we focus on the European ancestry group. So, we use a large part of the data, so 150,000 individuals, to generate the GWAS for this specific way, lipo-pooking. Because some previous studies, this paper actually showed that there may be non-linear association between this treat and genetic variable. Between this treat and genetic variants. So that's what we picked out of this thing. And then I use the remaining data to build those models. You know, some part I'm going to. I know this is very complex. Maybe I should skip that. So idea is that I use some remaining data and some I use imputed treatment. Some I use observed treatment. The reason I'm using UPI operating data is indeed I do have one. But because I don't know whether my method works, right? I needed to validate it. Okay, so in the application, for some part of the Application for some part of the data, I'm going to pretend I don't know why. I'm going to use my white hat. And then I'm going to also, for some other part, I'm going to use observer. And then I'm going to compare and see how it works. So that's the idea. Let me actually show you this plot. That's the main thing. Maybe first look at this thing here. So this is R squared. So that's a predicted Y squared correlation between predicted Y and And observe the way. Here, actually, it's a little bit complicated because I'm using neural networks to predict it. So there are two predictive things. One is that I call it using GWAS summary data and the individual level predictors. I impute as a white hand. And then after that, I'm going to use my white hand or observe the Y, apply a neural network to predict whatever this Y up pay, okay? But I'm going to use a separate test data to evaluate. Test the data to evaluate. So it's a little bit complicated. But the main message is like this: so x-axis is that because I keep a certain part of my data, okay, and then this is the proportion. Let's say I have about, I think it's about 25,000 individuals. Among 25,000 individuals, then I'm going to say 10% of them, I'm going to use their white hand. I'm going to use their white hand, observe white. Other remaining individuals, I'm going to use imputable. So at the beginning, here is 10% of. So at the beginning, here is some 10% here. And then I'm going to feed actually a neural network model. So this is a specific neural network called the GLN here. So it takes account of the feature of some SNP data. So we can ignore that. You can just imagine it's maybe just a feed forward neural network. And then I fit several models. Then, as I said, this one here is I use the white hat. I use my imputed white hat. I use my imputed white hat. And then the blue one is I use my observed white. Okay? And then you can see performance is going down, right? Using my white hand. The reason is because the proportion, this is the proportion of using observed white because my total sample size is fixed. So if you use, let's say, 25% of observed white, I'm going to only 25% of observed. 25% of observed Y, then that means I only have 75% imputed Y. Okay, so that's why, as this goes up, I'm going to have fewer and fewer proportions of using imputed Y. And then, of course, if the proportion or the sample size for observed Y increases, performance will go out. And then what is this red line? This red line is, I combine them, I just linear combination of the predicted Y of those two approaches. One of those two approaches. So the main point, I think, the most important point, I think, is just this one. So this is if you have a smaller subset with observed y, this is what you're going to get. If your sample size increases, this is the performance you're going to get. And then the red line here is that in addition to the data with observed y, I also do imputation for some other parts of the data. And then I combine the prediction from those observed y. Prediction from both observed white and imputed white. So, that's why I can gain information. So, you can see that this red line definitely is above this blue line. So, of course, this is non-linear model. So, it seems to actually support my claim that maybe my imputable still contains some non-linear information because I'm fitting a linear model here. As a contrast, I also fit a linear PIS model. This is called PISS CS, which is one of the most popular methods. The most popular method, it's just a linear model. Okay, so you can say for failing the linear model, no matter what you use, performance is much, much, much worse. So that means for this data set, or at least for this outcome, there is indeed a non-linear association. So this is much worse. And we also use boosting. So I think the general conclusion is the same, but improvement is not that much. So for this one here. So any question? That's the main thing here. So I guess my main thing. That's the main thing here. So, I guess my main thing here is: I said, is in addition to use a subset of observed data, if I do imputation, I do data augmentation, I might acture again in terms of prediction. And then here's the y-axis is imputed, x-axis is observed, because for this one, I do have observed. So, I can see that the correlation here is about 0.3. So, this is using our LS imputation. This is if you use this linear model. This linear model, okay. So you can see there's a shrinkage effect. Everything is centered around the, I think it's centered around the zero. So center around zero here. So there is definitely a shrinkage effect because the PRSS is based on the PC approach. They use shrinkage prior. So I guess my point is if you look at this thing, I cannot really tell much difference. Now I want to get come back if I want to use imputed why to do inference, to look at association. To look at an association, then this is what happens. So, this one here, this is so-called Manhattan plot, because you have many, many genetic variants along the form. So, I just lining them up. And the y-axis is just minus log 10 G band. And then what this is, if I use observed Y, because my data actually I have observed Y, okay. If you use observed Y, this is what whatever the association you're going to get. You can see there is a big peak here. The main signal actually is here, okay. The main signal actually is here, okay? And the top one is that then we pretend we don't know why. We apply our LS imputation, we get the white hat, and then we do inference. We just do marginal association testing. Then we can do the log p-value, amount of log time p-value. You can see pretty much we produce the results in the bottom. Bottom is based on the true Y, observed Y. Top is based on imputation. So general pattern is very, very simple. Alright, and as a comparison. And as a comparison, this is what happened with PRSCS. Because you find many, many more associations. Why? Because PRS is that you already pick up some relatively significant SNPs. And then you just pretend that they are truly caused or truly associated. Then you generate a corresponding why. Of course, you're going to find many meaning of things. Because usually, PIS says it's based on thousands of SNPs to predict the outcome. Then you can say you can't. This is what I said. If you use PIS, you cannot really do the inference. You cannot really do the inference or the downstream genetic analysis because there are a lot of artificial signals here. So, again, this is based on observed data truth. You can see there are many, many actual false positives here, right? All right. So, that's my first application. And this is my second application. Oh, and I go back to my causing inference. So, my causal inference here is that I have two stage models. Stage one, I want to impute X. So, what we use is this also kind of famous data set, the GTA X here, and then. G types here, and then the sample size relatively small, only about 700. Okay, and then, of course, if you do this two-stage least square or two-stage inference, you need to make sure that the X-hand can be reasonably predicted from your SNPs, right? Otherwise, you cannot apply method. So, among many genes here, after about 4,000, pass to the first stage, okay? And then the main thing here is we're going to apply to the UK-bel metadata. We want to say that which of those genes are possibly. Of those genes are possibly causally associated with outcome. So, outcome here, I think I'm using LDL, HDL, LDL. Did I write that? Yeah, sorry, I think it's either HDL or LDL. And then again, is that I use half of my data to construct the GWAS summary data. So I pretend that I have that GWAS summary data here. And then for the remaining length, I just take a certain proportion. ones I just take a certain proportion and then I use the observed one and then the remaining ones I'm going to do imputation. I use my imputed one. So this is what they said is observed. I only take 10 to 30 percent because I want to mimic my application. I'll show you my application just in a few minutes. So among those remaining 170,000 observations, 10 to 30 percent I'm going to use observed white. And the remaining ones I'm going to apply my imputed white hand because I want to see whether I can gain something by using. Because I want to see whether I can gain something by using those additional Y times. Because typically, if you don't have Y, you're not going to do anything. You're going to use your observed data. So recall I have this using neural network. So I use neural network model. I apply it to my imputed data and then I test on the observed data. And as a comparison, I also say if you just use this 10 to 30% of observed data, what happens? And then this is more like a more optimal situation. More optimal situation is you use the whole data set. For the whole data set, you're not only using those observed, for the remaining ones, you also use the observed one. So complete the data is just like ideal, okay? Okay. And then this is the way it's 10%. Actually, let me show you maybe the last one, actually. This is about 30%. So observed the data. I use the observed, I use the 30%. The observed why uses 30% of 170k individuals. Okay, so that's about 60k individuals. I assume I know their why. I'm going to use their two white. So this is the result. How many significant genes you're going to detect? You're going to detect only two. And then if you use a complete data, if you use complete data, and then this is how many are going to data, maybe about 15, I don't know exactly. About 15? Alright? And then here's our result. And then here's our result. Delivery imputed results. Instead of using just a 30K observer, I also use another 110K with imputed one. And then I do the hypothetizing. Then this number I'm going to find. So you can say then the number is much bigger than if you just use observed data. So that means I'm gaining the power. And of course, if you use the complete data, you probably And if you use the completed data, you're probably going to do best. You're going to find those five additional ones. But if I use my imputed one, I can also find some additional two. You may say, well, those two probably are false positive, maybe. But on the other hand, they are reproduced by this parametric approach. So I think it's less likely to be false positive. So this is the common thing in the paper, last paper, we did is that if you compare parametric with non-parametric, usually they are complementary. Because if quadratic Okay, because if quadratic is reasonable, is reasonable after time ratio will be will be better. So, so that's that's how we do it. I think my time is almost up, right? Yeah, so I'm gonna say that right now we are considering this application. This is my target application. I mentioned it to you is that if we have individual level genotype data from UK BioBank, about 400K, but we don't have the AD status for most of it. The AD status for most of the individuals because they are too young to have late onset AD. But on the other hand, we have this so-called ADSP. This is a major initiative from NIA. And we do have individual data white. So this is only sample size, only 30k. So that's what in my papers I'm trying to mitigate this situation. We only have a small portion of data weight for observed white, but larger proportion we don't have. So we want to impute and then we want to combine it. Another reason, of course, is that for the Another, of course, is that for the imputation method, there are many other issues, theory, inference, and the computation. So, I want to thank AI for the support. So, we're part of this so-called ABSP, AI Machine Learning Consortium. The work was mainly done by several PhD students at the junior with Professor Shot also. Thank you. Yeah. Okay, so it and then it looks to me that when you compute the data, you switch it to use the LT structure. Yeah, so it looks to me that you can incorporate this structure by considering avoidance. Yeah, I mean that's possible. But the challenge to think about is that typically people don't do joint model. You're talking about joint model, right? People go with marginal models. There actually are reasons. Because if you go with joint action, you may gain, you may lose. You may gain, you may lose. Because LD structure, a lot of places are just too strong. Sometimes making the estimation much more difficult. Not a stable. Yeah, but I agree with you. Maybe you can. But on the other hand, another thing also is my GWAS summary data is also only marginal. So you have to convert that first. That's probably going to introduce errors. Right? But I think because your data has that, they are like correlated. Yeah. They are correlated because of those. Because of those, how you touch it? Oh, you're saying that it's like a channel and this work, right? Yeah, yeah, yeah. That's that's that's good question. The challenge is there is because now I'm dealing with this building metric. So maybe, you know, Bing Jin's work, you know, Hongo's work, you know, how to use that IoT. Because you cannot just use observed IoT. Because my sample is way small, right? So, so, but by the way, we also consider the weighted one. So, not because each SNP, the variance actually, beta's variance is different. So, we did it otherwise. It's more or less the same. So, having to consider, but it's correlation that way. Have a considerable, but it's correlation that makes more sense. Can I ask a question, my friend? Sure. Yeah, I'm sorry, I have another meeting to defend because I can assure another spot. Because I think I really like your work. I think generally, you know, I like the PR or T1 or Twitter. And uh what I I I I will really worry whether we we can really do any jump ones. Can really do any difference from my personal perspective? Are you people doing that? Do you agree with me? I think it's difficult. I agree with you. I mean, it's probably much harder than I thought. But on the other hand, I do think it's just a one-wheel approach. So it may still be informative. I don't think we can completely rely on the TWAS for MRC. I mean, they just offer one possible approach. I think they can definitely generate capacities. To ask us. So, yeah. But for the other connection purposes, if we think of where should we go for the field. Oh, that's a good question because as I said, I agree with you. I feel I'm a bit frustrated working with TWAS MR because the assumption there, I think, is generally just too strong. Way too strong. Yeah, way too strong. So that's actually, I think to talk about here, if you look at what we have been doing last few years, we try to relax those assumptions. So that's why we're Assumptions. So that's one way. But I don't know how far it can go. And even AI agency also invested a ton of resources to connect the different data types. Yeah. Some well-controlled subjects. Yeah, I mean, that would be the right direction to move forward. Sure, sure. That's another thing we are doing. So instead of looking at the gene expression data, you can look at the protein data, you can look at the metabolomic data. Yeah, that's what we are doing too. Loving data. Yeah, that's what we are doing too. So, yeah. So, we're just doing what you are thinking. Okay, thanks for the question. We are a little late. Thanks, everyone. So, our next speaker is Professor Esper Hansen from University of John Hong Kansen.