Yesterday, for those of you who have been here, let me repeat. So the up here with TSP is about computing a TSP to before you know what you need to visit. So you have certain customers, say, or locations that you want to visit, or that you may have to visit. And we want to conclude a tour a priori, like this CSP tour. And we don't know yet which customers we actually need to visit. Which customers we actually need to visit. This is revealed, say, on the morning, we say we need to visit only these two customers. And what we then do is we skip the other, the inactive customers. We just shortcut the tour. Next day, we work with the same tour. We have a different set of active customers, maybe these ones. And then we again shortcut our tour. This is still the same tour. Third day, it might look like this. We have these four customers. We have these four customers, so we again shortcut all given tours. You see, the tour will not be optimal, of course, in general. The tour on the active customers, but that cannot be avoided because we work with one tour for all the possible outcomes. So, in general, there is no TSP tour that is good for all subsets. There will be no constant factor regarding this. Factor regarding this. This is, by the way, called the universal TXP, optimizing the worst case, but I'm not considering this. Instead, we are minimizing the expected cost of the outcome. So we have a probability distribution over the outcomes, and as David mentioned, and we want to compute a tour such that after shortcutting to the active customers, Customers, the expected length of the resulting tour is many. And note that we do not compare against the a posterior optimal sample, right? Because that's not what we're not our task. So let me do this formally. And this has been introduced already in the 1980s by Joyer and Garth Simas. And here, in contrast to David's question yesterday, I'm considering a simpler special case, you can say, the independent activation model. And that's essentially the only model where we know anything about. So we're given a finite metric space and activation probabilities of the customer. So each customer has an independent probability that it is active. And we know that. And we know that data in advance. And then we have to compute one TSP to a T for all the customers such that we minimize the expected cost of the induced tool. So let me introduce this notation. This is an expectation where A, the set of active customers, is drawn from a probability distribution given by P. So an element V. So an element V is in A with probability P of V. And then once we have A and our tool T, we just take the shortcut, we just skip the inactive customers and we denote this by T induced A and we minimize the expected length of the tube. Probably clear? See several code people nodding if you are there. See several co people nodding. If you have any questions, please interrupt. So just the approximation ratios for this problem that were known at a glance. So there was a randomized constant factor approximation algorithm in a paper by Anopam, in a conference paper and co-authors. It was just a sketch and there was no proof, there was never a full version of this. Maybe Anupam can deliver it later this week. And simultaneously, there was. Uh and simultaneously, there was um I think more or less simultaneously there was a paper by uh David and Talva um who um give more details and gave a precise approximation ratio of four for this problem. And this easily proves to 3.5 as we'll show in the next slide. So, what was observed by Ian Suders and these were randomized algorithms and but David also provided a deterministic version. Also provided a deterministic version, but losing another factor 2. So that was an 8 approximation algorithm, and that was improved by Andre van Zoriven to 6.5, which was a state of the art until now. And we improved the approximation ratios, both the randomized and the deterministic. Maybe even more interestingly, we provide some lower bound and some further insights. Some further insights that we will see during the talk. So, all these algorithms that I mentioned, the old ones and the new ones, compute solutions with a very special structure. I call them master route solutions. So, in these master route solutions, you have a subset of the customers, here shown in green, and a TSP to adjust for this subset. Just for this subset, which I call the master tool. And then all the other customers are just connected to this master tool by a pair of parallel edges, in the cheapest possible way. This is a TSP tool for all the customers. I mean, it's not a Hamiltonian citer, but you can always shortcut main one if you prefer. And so this would be a solution to our problem, but the solution. Solution to our problem, but a solution with certainly a special structure. And in the analysis, that's important, otherwise this would not really be a restriction. We bound the cost of such a master out solution in the way that we always traverse the entire master tour, no matter whether the customers are active or not. But of course, we But of course we traverse the pair of parallel edges only if the customer at the other end is active. So look at this example. Here's our master route solution for this instance. And suppose the filled vertices are active. Then this would be the cost that I pay. I pay for the green tour, for the entire green tour. I don't have to visit it, but I visit it anyway. And I pay for these pairs of... And I pay for these pairs of parallel ashes to the other active vertices. And of course, I could again shortcut, but I will not gain from this. And in the worst case, actually, one might not be able to gain from this, but that's another story. So this is my way of analyzing mastery solutions. Okay. Now as a warm-up, let's review David's algorithm. With you, David's algorithm. It's very simple. And in particular, it's very simple if we assume a depot. And by depot, I just mean a customer that is always active. Assume that that's our starting point of our vehicle, for example. In many practical, you would probably have a depot, but okay, let's assume this for now. This makes the particularly simple and also the analysis. So we sample L. So um we sample S randomly. So S is the set of customers that we would include in our master tool according to these activation probabilities. So this customer will eventually be active with probability one half. I will also include it in my sample with probability one half. This one is active with probability 0.2, I will include in my sample with probability 0.2. Exactly the same probabilities. And this might result in this green set, and we connect it by TSP to just by calling a black box TSP algorithm, say an alpha approximation algorithm. Think of alpha 1.5, for example. And this gives, of course, the cost alpha times the optimum TSP tour on S. And then we connect all the remaining customers to S. So, what we do, we have to pay. Customers to add. So, what we do, we have to pay for this. Well, we have a pair of parallels, that's the factor two. Then we sample eventually the active customers according to the given probabilities. And then, for every customer that is active, except for the depot, we connect it to S in the cheapest possible way. And here I do actually a little trick. I don't connect it to S, I connect it to S minus V. To S minus V, which would not be necessary. So, customers that are active and already in S, I don't have to pay anything. But here I pay something in addition. Okay, it's an alphabet. I can do that, right? And this little trick helps me to analyze it, or helps David to analyze it, in a much simpler way. Okay, what is the analysis? It fits here on these three lines. So we have this size. So we have this sum of the costs. So alpha times the optimum TSP2. And we have to take the expectation because we sample S randomly. So we take the expectation over all possible samples. And then the second term, and we here sample according to the activation probabilities, but they're exactly the same as the sampling probabilities. So it doesn't matter whether I write here A matter whether I write here A or S, it is just the same. And then I have this sum. I can do that because it's now independent, because I removed V from set. And now this here, whether I write S or A, it doesn't matter because it's the same. Now the sum of the distance to the nearest other element in S, well, that's a lower point. Well, that's a lower bound on the cost of a TSP term. So I can bound that this term by the cost, by the expected cost of a TSP term. So instead of alpha times, I have alpha plus 2 times the expected cost of the a posteriori optimal TSP term, which is clearly a lower bound on the optimum a priori. And that gives alpha plus 2 for any other possible channels, which means 3.5. Which means three point five or slightly less with a curly climate over scale. Okay. So assuming a depot makes this analysis significantly simpler. I think the difficult part of David's paper was actually dealing with a case where there's no depot. So let's do a little pre-processing also for the following for the rest of the talk. The following for the rest of the talk, we will restrict attention to what I call epsilon normalized instances. And these are instances that not only have a deep pool, but also are somehow uniform because all other customers have the same activation probability. And this is a tiny number, absolutely. And I can assume this. So the outline is: first of all, in the case where the expected number of active customers is very small, we can We can essentially guess an optimal master tour with just a constant number of customers, and we can do complete enumeration about this set and about the TSP tools for this. And one of them will be good, so we don't have to deal with this case. And if the total activation probability is large, we can just sample one random customer and declare a depot, just raise. Then declare a GDPO, just raise its activation probability to one, and this doesn't harm much if the total activation probability is large. Just lose an SPO. And okay, and then to get this uniform activation probability, I can just replace every customer by sufficiently many tiny copies. And the details are a bit involved to make this happen, but uh essentially that's what's happening. Okay, a bit more precisely Okay, a bit more precisely, so for any sequence of epsilon i's that go to zero, it is sufficient to consider epsilon i normalized instances. But I need to do this for all i. So I need to really do let epsilon i go to zero. Not sufficient to work with one fixed. Okay. And so the motivatory questions of our work are the following. Questions of our work are the following. So, is it really optimal to sample S with exactly the activation probabilities? It made the proof very easy that we saw. But that's not our primary goal to make the proof easy. So, we want to get the best possible. Is it really best possible? It's natural, but the other question is: restricting to master our solution, how much do we lose by this? So, there is an inherent loss. So, there is an inherent loss involved here, but how big is this loss? And also, what is the limit of random sampling? By this random sampling, I get a master route solution. Can I ever get the best master route solution, even assuming that I can compute optimal GSP into it? And here are the answers, just as a spoiler. So it's not optimal. You should simple less. And the master operation. And the master rod ratio is actually slightly more than 2.54. And with random sampling, you cannot get to the best muscle rot ratio. You might be able to get close, but no better than 2.65, which is strictly large. And we will see how we get to the answers in the rest of the talk. Any questions up to this point? Any questions up to this point? Okay, so here's a slide. The question is, what kind of bird is that? Here's a slightly generalized sampling algorithm. And it's essentially the same, except that sampling a customer V into our putting it on. V into our, putting into our sample with probability p of v, I just have some function f of p of v. And then I call the approximation for TSP on the sample and I connect to every other customer in the cheapest possible way. So that's the same, except that I do instead of P of V, here some function. And instead, indeed, I will use instead of the identity. Indeed, I will use instead of the identity function, I will use this blue function, which I showed here. So a sample slightly less. And then we can prove that this gives actually 3.1 instead of 3.5. With, oh, I forgot, it's written here, with a 1.5 approximation for T squared. And the other one does not achieve. And the other one does not achieve this. And we can also give a lower bound, no matter how you choose F, you will never get better than 3.049. We actually assume that this lower bound is tight, and this is just, I mean, there are some numerical issues here. I will get to that. The 3.1 is not tight. Sorry, you mean in this approach a different? approach a different master transform no other sampling no other sampling no other function f can do better I mean up to this tiny gap here between 3.1 and 3.05 but sigmas are optimized for this choice of alpha yes very good point for a different alpha you get a different sigma we did also We did also the math for alpha equals one, for example. Just yeah, yeah, we I could get to it. So how do we compute the upper bound? That's really the most interesting one. The lower bound is just an example which we have to analyze, which is not completely trivial, but a bit boring. So the first goal is: let's fix some epsilon. Fix some epsilon, it's considered epsilon normalized instances, and we will actually set up in the end your linear program to determine the approximation. And to this end, let's assume we have an optimum RPO tool. So this is our optimum. Of course, the algorithm will not know it. And let's call V0. And let's call V0 the repo and number the customers accordingly, otherwise. And then I define, I somehow aggregate the distances. I don't want a variable for every distance between two customers, but I aggregate the distances. So any two customers at distance three on this T-star tour will be summed up, any such pair. Any such pair, and I sum up all these distances. So here you see all the pairs at distance 3. Well, near the depot, I cheat a bit. You can consider replacing the depot by infinitely many copies. And so this would be distance 3. This would be also distance 3. I would also stop at the depot in one more. So that's for technical reasons. This is happening at the depot. And for another reason, I will multiply this by epsilon squared. This sum. This sum, and this defines me as CK. Think of it just aggregated distances of pairs at distance k in the t-star. And these will be my variables, or the first attempt of my variables in my optimization code. And then I will write what is the value, what is object, since I have just written out this optimum. This optimum up here into a T star, I can easily compute up now. Because for every edge, let's say here this edge, what is the probability that it appears in the shortcut tool? Well, the probability is that this one is active, which is probability epsilon. This one is also active, another probability epsilon. This gives these epsilon squared here. And all the ones in And all the ones in between are not active. So this is the 1 minus epsilon to the i minus 1 if we are at distance i. And we should just sum it up. And so this is the cost of this TSP to a T stuff. So that's easy to write down. The next thing we would like to write down is the cost of the tour we get by sampling. That's not so easy. But we make some assumptions. But we make some assumptions here and we get an upper bound. And these assumptions are probably, as far as we know, no weakening. But we cannot strictly prove it. They are up to some numerical, so we would say. But we are close to proving it. Okay, so how do we do this? So we sample with probability sigma epsilon. So then we assume this, I mean, for if we go back to this. for if we go back to this function and this blue function for small and we're considering small epsilon here this is equivalent to sigma times p. So I just have this sigma as my sampling parameter so instead of with probability epsilon I sample with sigma epsilon and then I have two customers sampled at distance k in this At distance k in this t-star term with the probability, well, epsilon, sigma epsilon times sigma epsilon times 1 sigma epsilon to the k minus 1. And here is the sigma epsilon, and here's another sigma epsilon. So, and this gives the first term. And we have to multiply by alpha because we will have an alpha approximation. I don't certainly say that. Now, we also have to I mean this might not be the optimal GSP tour, the short the T star shortcut tour, there might be a better one, but I assume this. So to get an upper bound, that's good enough. And now I have to connect all the other customers. And suppose this is one edge of this shortcut tour, so these are two sampled customers and the ones in between are not sampled. Are not sampled, then I have to connect them to the nearest sampled customer, and I will only consider two possibilities: namely, the two neighbors here in this sense on the T transfer. These might not be the nearest. None of them might be the nearest, but I get an upper bound just restricting to those two. It's not sufficient to consider just the nearest, by the way. We need to consider the two neighbors. That's an interesting point. An interesting point. The value will rise if I restrict only to one. Okay. And that's essentially what's happening here. Maybe in the meantime, some of you could follow. So here we have, say, for an edge of distance k is 3 in this, my example here. I sum up over all the customers in between and I take the minimum of the Ci and C chain. So this gives all This gives opt and alk, and now I want to find an instance where the ratio of alk over opt is maximum. I can also say by just scaling, alk is one and I want to minimize opt. That's the same thing. That gives me the reciprocal of the region. And that gives just this optimization problem here. So I minimize opt subject to Subject to argues at least one. That's what's written here. And the variables are the Ci, and I have the triangle inequality. The triangle inequality of the original instance transfers to this Ci variables as can be seen very easily. Okay, so that's an optimization problem that gives an upper bound, or the reciprocal of the value gives an upper bound on. The value gives an upper bound on the quality of the sampling value. There's a little problem here, or there are actually two problems. One is it still depends on epsilon, and we should let epsilon go to zero. So we should actually have an infinite sequence of epsilon and an optimization problem for each of those. And the second one, this problem still has infinitely many variables. Because we don't know the instance size, we cannot bound the number of customers or so. We have to work with Of customers, or so we have to work with infinitely many CI, so that's not so nice. We don't know how to solve this, and we do several tricks to get to a single linear program. So, the first trick is we aggregate CI's, consecutive CI's where the I is close to each other, into buckets, and these buckets will have size approximately one of epsilon, or proportional to one of epsilon. And then for the buckets with very large instances, we can just ignore them because they are extremely unlikely to happen, and we just use the tiny epsilon in the guarantee. And then you have to express all these, the opt, and in particular, which is more difficult, the oak, with respect to the bucket variables. It can be done, it's a bit technical. It can be done, it's a bit technical, but we get then a single linear problem, which I'm not even writing down here, which looks a bit similar to this, but now with bug experiments. And we solved this by numeric buzz, by Gourobe, and we got a private solution that gave us a hint about the worst case example, how the worst case example looked like, which we can then analyze on paper. We can then just analyze on paper to get a lower bound. And also, any feasible dual solution gives us a lower bound on the value of the optimal and therefore an upper bound on the quality of the second value. The dual solution doesn't look very nice. But then we can just check it independently, the inequalities one by one, and prove that it is a feasible dual solution, and that gives us then. And that gives us then an upper bound of less than 3.1. And then, if you do the variables larger and improve your constants, you could probably get this down with more computer power. And the lower bound, which we also, which looks like this, by the way, where the distance, the cost of the i and vi plus k Depend only on k and in the following way. So, first it is constant and then it grows reliably. This is the worst case that's happening. And of course, the constant depends on the other constants, alpha, and this gives a lower bound. So, unless you compute better TF2 than this, which nobody can do, today Today you will not get better than 3.049, which we believe is probably the right answer. And this is just an American difficulties. So a few questions on this master route ratio. So again, this is how we analyze the master route solutions. We call this the master route solution, the worst case ratio of. Case ratio of this, how we analyze the Masveron solution over opt over any instance, say with a depot. And this ratio is at most three, even without depot by the Schmois-Talber analysis. And Anke von Zeuen showed that if you have an upper bound on the Master Road ratio, you immediately get a deterministic approximation algorithm with this quality: 2 plus alpha times rho, rho is the Master Roth ratio. Times rho is the master rot ratio, alpha is the TSP approximation ratio, which got the 6.5 in the past, and now we get a better upper bound on the master load ratio by a very similar approach to founding the sampling algorithm, which gave 2.6 as an upper bound, which immediately improves the decimalistic approximation ratio. And here we also get a lower bound of 2.54, roughly, or 1 over 1 minus. Roughly, or one over one minus one over square root of e. And the example is actually very simple. We have many customers at the same spot, and we have distance one in between these clusters, and the total activation probability in one cluster is just one half, and then this is very easy to analyze that gives you this number, which is probably the worst case. Okay, so I Um okay, so as a summary, um the sampling algorithm gives uh we can now analyze it very well. Uh we get this quality. We can do this analysis for any value of alpha. So we did it for example for alpha graph equal to one. We also get the number up to some precision and it shows you this is even this lower bound is worse than the upper bound that we have on the master rotation. So there's an additional loss, inherent loss at this point. Inherent loss at this approach. And with this, I'd like to close. Thank you all. Any questions? Yeah? What if you allow the truck driver a little bit of food? For example, every morning he can decide to adjust the position of tools. Adjust the route? We haven't considered this. Yeah, that might be interesting how to do Be interesting how to do adaptive calculus. One could consider a variant of British solution. What about coming up with a small portfolio of possible solutions? Yeah, that would be interesting that you have oh yeah, I have no idea. What about the arbitrary? What about the arbitrary given distribution? Yeah, I mean that was David's question yesterday. If you have a set of scenarios, let me say a polynomial number of scenarios, and you know one of them will happen, we don't know anything about it as far as I know. So there is no cancer factor approximation. About the e-signature question? Yeah, it's a very good question. That is also completely open. The approach fails as far as I know and As far as I know, and uh there's no constant spectrum that's challenging open question. Okay, thanks, yeah.