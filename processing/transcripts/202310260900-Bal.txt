Chicago. He's gonna talk about uh asymmetric and for its avocado for the rest of the future. Okay, thank you very much for the uh invitation. Great pleasure to uh be back in Branf. It's a fantastic workshop. So let me show you some pictures of what we're interested in. Topological insulators in multiple applications, multiple wave phenomena at different scales. Historically, the integer control effects. I hope you have time to I hope to have time to talk a little bit about a bilayer graphic model, a symmetric transport, along an edge that separates LB and BA stackings that we have seen in some of these modern structures. There are also applications in photonics. These are metamaterials constructed by Ken Kaif at CUNY and they are in different topologies. And at the interface separating these two topologies, you see very strong transport. You see very strong transport. Very strong transport in spite of a very jagged interface, which is the surprising aspect. Not only strong transport, which is asymmetry, but also very robust structure reactions. And the topic I really want to insist on, or the application that I want to focus on for concreteness, is one of the largest scale for which this phenomenon is known, which is the Earth, where the The Earth, where the northern and the southern hemispheres are insulators. There is a Corias force that's positive in the upper hemisphere, that's negative in the northern hemisphere, and which acts as an energy barrier. So in some energy range, fluctuations, this is mass transport in the atmosphere. Fluctuations cannot move away from the equator, but they can move along the equator. It's metallic. And it turns out that this movement is itself asymmetric. Itself asymmetric. There are two modes that move these words, which are well known: the Yani wave and the Kelvin waves. And they themselves are pretty immune to all kinds of impurities. So that's the application again I want to focus on that. There's a lot of different physical phenomena in which the mathematics is essentially invested. So, how do we try to understand heuristically what this asymmetric transport is on the end? Asymmetric transport is along the edge, and why is this asymmetric transport immune to a lot of filtrations? Well, so there is some heuristic interpretation, which is that the northern and southern hemisphere are in different topological phases. So priori, you can assign topological environments in the north and in the south. And physics is somewhat frustrated because those two numbers are not the same. And that frustration is restored. Frustration is restored, sanity is restored physically by having another asymmetry along the edge, which is also not natural on its own. And a principle, the bulk edge correspondence, which is a generic principle, says that this restoration of sanity has to be related to this difference of integers being compensated exactly by a number, a quantized number of modes that propagate along the object. That propagate around the edge. So the bucket edge correspondence is used a lot in all kinds of applications, but there is very there are very few derivations. And there are no easy derivations. There's nothing simple that explains beyond this imbalance that has to be compensated. There's really no easy explanation for this phenomenon. So that's what I'm going to try to show you, what type of models we can come up with. Alright, so, and I put some question marks. Okay, there are I put some question marks. Okay, they are also estimates, but you can see I put some question marks around topological invariance, because these are actually not topological invariants. The difference is a topological invariance, as we would see. But the northern and southern phases are actually not topological invariants. And that's something that's also reasonably well known in physics, which indicates, and I'll mention it again, that it's easier to look at phase differences than it is to look at absolute phase differences. So the important topic here is a difference. The important topic here is a difference of ages between Northeast. Alright, so that's one aspect. There's another aspect that I want to insist on, which is a bit different, which is related to scattering. Look at this Earth and assume that we're interested in waves that move along the equator. And the equator sees a lot of jagged fluctuations, a lot of randomness. And you want to create Of randomness, and you want to create a scattering theory for such phenomena. So, scattering theory is typically you send some plane waves onto this jagged, complicated system, and you try to see what emerges out of this slab or this area where you have remnants. And so, what you see here, because nothing can go up and down, everything is close to the boundary, what we see is that some signal transmits across that randomness and some signal gets reflected. So, there's a scattering matrix that you can introduce. A scattering matrix that you can introduce. And I want to insist on two parts of the scattering matrix, which are these objects here, so these are non-negative numbers, which correspond to the transmission from right to left. Okay, so the plus here, the blue thing, is transmission from right to left. You send a signal from the right, it transmits to the left through this randomness. And there's another one which goes in the opposite direction, and we put a minus sign so that we can distinguish it. Okay, so those are transmissions across. So, those are transmissions across a slab of randomness. And then you have a phenomenon which is arguably also extremely stable and extremely robust, and one of the surprising effects of mathematical physics and physics, which is Anderson localization, saying that for classical materials, topologically trivial classical materials, this transmission across a slab of randomness experiences an exponential decay in the strength of the randomness. The strength of the randomness. So, this axis here is how much randomness you cram into your system when you look at how signals move from one side of that random system to the other. And what you expect is an exponential decay corresponding to some localization length, which comes from under some localization. They are very difficult theories, physically and mathematically, but there's a lot of models. All of them are, again, extremely complicated. But what you expect is. Complicated, but what you expect is this: that as the randomness increases, transmission decreases exponentially through a slab of randomness, which means that everything gets backscattered. Nothing transmits, and hence everything backscatters. It's a Hamiltonian system, everything is constructed. So that's what we have obtained numerically. So topology is all about qualitative aspects. We also want to look at quantitative transport, and so we've done. Transport, and so we've developed some numerical tools to do that with Jeremy Hoskins and Song Jian Wong and with Solomon Quinn here. So using those tools, which I'm not going to present in more detail, we can understand these transmissions, we can compute these scattering objects extremely accurately. And what we observe is that in the trivial system, we have understanded localization, as we expect. In the systems that correspond to this asymmetric transform that corresponds This asymmetric transform that corresponds to these topologically non-trivial materials, Anderson localization is broken. You see that the transmission in one direction indeed goes to zero, as you expect from Anderson localization. However, the transmission in the opposite direction is guaranteed by the non-trivial topology, independently of how much randomness you find. That's very surprising, the fact that there's this obstruction to understand localization. And not only that, Some localization. And not only that, but it's contact. If you look at the difference of transmissions from left to right versus right to left, you get your environments. The topological environment that I want to show you is exactly that one that gives you the difference of transmissions. Okay, so there is very little theory on why these transmissions converge to these numbers. There's some attempt in in that paper, it's a model which generalizes techniques that Patreon Polo had developed to understand understanding of polarization. To understand of colorization. It's a rich topic. So, just to say that there are really very striking and bizarre phenomena that we have here, and we want to explain it. So, I'm not going to explain why these objects converge to 0 and 2. What I want to explain is that the sum of these guys is always 2 here. Always 0 here because it's trivial. And the last thing I want to say, since I'm here, is that I'm going to look at Z invariants, chair numbers, churn topological integrators. There's also the wonderful Z2. There's also the wonderful Z2 topological invariance, in which, in the setting of the transmission, when the material is, for instance, fermionic time reversible, you expect the transmissions to be preserved in non-trivial materials because you cannot gap edges and that's reflected by these numbers. So that's what it is. We're working on this with this, but that's not what I want to talk about. So I want So, I want to talk about PD models or PDs. PDs, I don't necessarily care that much, but macroscopic models. I assume that you started from a microscopic description, tight binding, or maybe a microscopic Schr√∂dinger equation with periodic coefficients, and you've done your homework to extract macroscopic physics out of this. So, we have something that looks like a Dirac equation. The simplest operator that is topologically non-trivial is the Topologically non-trivial is the Dirac operator. The Laplacian is trivial, so you go to the next level of simplicity, which is really given by this Dirac Hamiltonian. And I assume that there is a master, or there is an object somewhere that is mere transition between the northern and the southern hemispheres. So here for the Dirac operator, it's a master. There are all kinds of interpretations of exchanges of orbitals that allow us to understand why we choose. And that allows us to understand why we change topology when M goes from something positive to something negative. But I want to have a mass term that explains this transition from one topology to the other. And I want to assign an invariant, and I want to make sure that I can add perturbations which will be localized. I don't want to touch what happens at infinity, but I want to see the robustness of that transport with respect to perturbation. And that will be, if I add any localized perturbation, Add any localized perturbation, I want my invariance to be stable with respect to the localized perturbation. So I want to look at Hamiltonians that are sufficiently rich to include macroscopic perturbations and make sure that invariant again is immune to those perturbations. Something you cannot do on the brilliant zone, you're stuck to something that's periodic forever. You cannot add any randomness that is not periodic. And so we had to kill the brilliant zone. We really have to go to an open space problem and that creates. Open space problem, and that creates some difficulties. There are other Hamiltonians of interest, so Dirac, there are several applications in superconductors, so you can look at Volovic's book or Bernovic and Hughes, and there are some various models that represent Poco-Dosian Hamiltonians for various waves. So these are Hamiltonians, and you can have, again, parameters like chemical potentials, so on, that can vary and that diff. Potentials and so on that can vary and that give you a transition from one insulator to another one. There are other applications to floccate topological insulators, something we worked on quite a bit with Danielle, and the bilayer graphene model is something that looks very similar. Come back to that. And that's the Earth model. The Earth model is a Hamiltonian spin one, actually. It's a spin one model, which creates a lot of difficulties compared to this non-spin one half. To this nice spin one half. So the spin one model is much more complicated theoretically, and you recognize that colour is force here that also moves from negative to positive as you cross the equation. Okay, so yes, that was enough. So let me spend a little bit of time on domain walls. As I mentioned, the Cori force moves from negative to positive. You change topology, same thing for the master. I want to introduce those domain walls that will be positive in or non-zero. In or non-zero in a certain hemisphere, non-zero with the other sign in the other hemisphere, and roughly zero close to the edge. And a priori, the theory should be relatively immune to what the domain world looks like. How you transition from one topology to the other should be relevant for the topological classification. So you can have in mind a domain world like this. Now, I want to make a statement, also an additional statement about these domain walls, which is that I want to use. Domain walls, which is that I want to use them as classifiers. Those domain walls actually help me to generate a classification of Hamiltonians. I want to, you give me a Hamiltonian, I want to know what topology to associate with that Hamiltonian, and I will show you a very, very simple way to classify all these Hamiltonians, so long as they satisfy some touch properties. Very simple way of classifying all of them. And that classifier will be based on domain rules. So several of you might be more familiar with linear. More familiar with linear response, how do we compute a whole conductance where you put your system a little bit out of equilibrium at adding a certain difference of potential in a certain direction and you try to look at the whole effect in the transfer direction. That difference of potential and the whole effect in the transfer direction is one way to understand the topology of the material. Well, alternatively, you can have the domain wall in one direction and see what transmits in the transverse direction. It's a very different physics, but a very Different physics, but a very similar system nonetheless. And that classification is extraordinarily simple to see. Okay, so before I go to the classification, there's another ingredient that I need to introduce. Then we're almost done with what are the tools that we need. And that's to understand really what asymmetric transport is. So I mentioned to you, I have modes that move along that edge, Yanai wave and so on and so forth. The an eye wave and so on and so forth, but that means that you have diagonalized your system already, there's no perturbation, you need a lot of information to identify those modes. And I don't want to have a theory that relies on us knowing those modes. We need something that's much more systematic. And that much more systematic object is something that's dubbed an interface conductivity for the language coming from electronics. And that interface conductivity is constructed. Is constructed like this. You need to construct a current operator. You need to assume that you have a certain density of state in your system, that is a density of states of excitations at energies that stay close to the boundary, because you don't want to introduce in your system excitations that can move into the bulk. So you have to restrict yourself to a certain energy range. And you construct an observable of that current against that density. And that's an object that knows a real number, a very anywhere. That now it's a real number, and you want to see what happens to that point now. Okay, so let me be a little more precise here. What is p? P is a function which is morally 1 on the right of an imaginary line and 0 on the left of that same imaginary line. And sometimes you have to smooth it a little bit out so that you do some theory. But morally, it's a function that's one on the left of, again, that line, and zero on the right, and vice versa. And so, if you want to look at an observable, which is how much signal. An observable, which is how much signal sits on the right of this line, while you just compute the expectation of the observable. Take that guy against whatever density you have, and that will tell you how much signal sits on the right. You want to know transport, you want to know how much signal goes from here to here. So you have to take the derivative of that observable, and the derivative of observable gives you, in Heisenberg picture, gives you a commutator with the Hamilton. So that guy is a current operator. Is a current operator which tells you the rate of signal goes from here to here per unit time. So it's an object that hopefully we can associate to interesting physics. And again, as I said, in our system, there are some energy ranges which are stuck close to the boundary. There are other excitations, higher energies that could propagate into the bulk. And so mathematically, we have to deal with this, and that's the pain. We have to make sure that we only select those excitations. only select those excitations that have a chance of setting along this right. So you have an operator here, so you have a trace, which is a standard expectation of a current operator against a set of density. So everything is reasonably well posed, at least physically, and that's the object that I need. So you see here, I don't need any diagonalization of operators. I don't need to know how many modes do what and how they are perturbed and so on. It's an object that's much more simple. Much also. P has nothing to do with momentum or value. P is just a projector, right? So there are lots of projectors and lots of unitaries in those theories. And P is a projector. It projects onto the right of that line. So that's the name. That's the reason for P. And yeah, no, it's not a momentum. But here it's not a position, it's not a classical current operator where you would replace P by X. That's too unstable. That thing is too unstable for me. So I have to really localize it in the vicinity of localize it in the vicinity of what's phi prime so phi prime is a derivative of phi it's a density so you will see why phi prime is called phi prime but it's a density so it's an object that's non-negative and that integrates to one and phi will be useful at some point so that's why it's useful but it's just a function that's localized again what's important is that that function I didn't write it here but that's the function that's supported in the energy range that is stuck to the Energy range that is stuck to the boundary. So phi prime is zero away from an interval of energies, which are the ones that propagate along the input. So the fact that you need to smoother the boundary doesn't cause a problem with it being a projector. Sure. Right, so there will be theories in which P will really have to be a projector, and there will be theories in which I differentiate P, and I don't want to have delta functions, and you have to fight against that. But these are these are detachments. But you're right, but more, you want. But you're right. But Moral, you want to think of P really as a projection. So, if I understand correctly, you're saying that what's on the right-hand side of your bottom equation is something that measures how many electrons per unit time go across some line, right? That's right. Normally we call conductivity, you know, the ratio of that to a driving term. Okay, so yeah, so there there's got to be a driving term that you have to To be a driving term that you have to, yeah. So, okay, so this is terminology that I generated from. Okay, but yeah, no, it has an interpretation as a conductivity, it's an edge conductivity as the bulk conductivity for the integer control effect can be defined as a conductivity. So there are similar parts. So it it can be given an interpretation as a conductivity. But it's not a linear response. It looks more like a current, that's all I'm saying. An inch current. That's all it's like. Yeah. The way it's defined here, it's more like an actual. Yes, that's right. I will come back to that in a second, but absolutely. Okay, so now I'm ready to mention, okay, so I'll show you some classes of Hamiltonians for which the theory applies, with relatively broad class, whether we uh like the class or not, and it it can certainly be improved, but there's a relatively large class, which again involves all kinds of perturbations. Involves all kinds of perturbations. And then I will show you that we can introduce two Frederam operators. So this F of H, which we vary on the classification by domain rules, which is very easy to construct, so that the index is very easy to compute. And the conductivity is actually much more easily related to another index, which is an index which is also extremely classical in the field of non-commutative geometry, which has been used a lot to classify. Been used a lot to classify systems. And so that operator is the form PUP, as I mentioned. So the U will involve the P. And so that once we have obtained these objects, the big theorem, if you wish, is that this conductivity multiplied by 2 pi that I've introduced earlier is actually quantized and equal to those two indices. One which is an index which is nice but hard to compute, and one which is an index which is easy to compute. To compute. But then we can use some techniques to compute those invariants explicitly. I won't mention that much. But here is the class of Hamiltonian. These are pseudo-differential operators or differential operators. And so we've seen them yesterday already. So let me add a few things on what Eric mentioned yesterday. This is a very natural and useful representation of operators. It's not that I'm really looking at It's not that I'm really looking at classes of operators which come really out of the blue or only reside on the moon and so on. Any operator has a Schwarzkernel. Any Schwarzkernel has a Fourier transform unless you really go to very exotic objects. And so an A is always there. There's always an A associated to every operator. So it's a viol quantization which goes from symbols, kernels, to operators. But it's always true that there is a correspondence between operators and kernels. Now, what's useful here is to say instead of Now, what's useful here is to say instead of fighting hard against the properties of the operator, I'm going to relate operators precisely to their symbols. And that's easier to manipulate symbols. And at the end of the day, I want an index. An index theory, if you know, at a single type index theory, the indices that you get are related to the symbols of the operator. So that's the integrals that we get have to integrate functions, not operators that come out of the. And so now what is important is. And so, now what is important is what type of assumptions you make on those objects. And here, I'm just going to assume that these are objects which are in certain classes, and I'm not going to say anything else than this. We assume that we have objects which are in certain classes, and all my differential operators belong to a class like that. Dirac, M is equal to one. When you have the bogus of the Dogen operators I mentioned, M was two because there was a Laplace. So it's how many different shards you have. You see that. Different charts you have. You see that all you do here, if this object does not depend on x, all you do is you take a function, you Fourier transform it, you multiply it by a Fourier multiplier, and Fourier transform back. So it's really a Fourier multiplier, which is any operator that's invariant by translation has a Fourier multiplier. So you recognize very easily all the natural operators that we are dealing with, and that allows for specially varying coefficients. And what's important and nice in this representation is that when A is a self-identified Is that when A is a self-adjunct, is a Hermitian matrix, its Hermitian value, the Dirac operator of this A is a 2 by 2 matrix. For any A that's a Hermitian, you very quickly prove that this operator is self-hydrogen, which is important because as you saw in the conductivity, we take functionals of Hamiltonians. Okay, so the second thing that I want to, the hypothesis two and three are related to what Eric just said. 2 and 3 are related to what Eric just said. For y-large, which means very deep into the northern hemisphere, very deep into the southern hemisphere, you want your Hamiltonian to be gapped. There's a mass term, there's an energy barrier that prevents things from moving up or down in a certain energy range, and you want your V prime precisely to be supporting exactly inside that gap so that you focus on stuff that stays close to the inductor. Alright, so that's the model. And as soon as you have this, everything follows from what. Folllows from what. These are the only hypotheses that we everything follows from it. You can do that for higher dimensions, more complicated, but that generalizes to arbitrary dimensions. And now here's the classification. We see that dimension was very simple and is new in some ways and also new in many other ways. I want to start from one of these Hamiltonians that is confined in k dimensions out of D, or D dimensions. Of D. You're in D dimensions and you are confined in K dimensions. If you're in two dimensions, all the models that I mentioned are already confined in the y variable. So you start actually from the Hamiltonian that I was. Now, if k is not d minus 1, you're in higher dimensions, or if you start from k is equal to 0, if k is not d minus 1, what you do is you confine, you add domain walls. And you need a three-ford algebra structure, which everybody uses, and we have to use it as well for. And we have to use it as well for reasons of orthogonality somewhere. So you add domain walls. So these are objects which are nothing but x1, x2, x3, multiplied by appropriate matrices that are the appropriate representations of your appropriate key form matrix, which depends on the dimension D. Okay, so you have an operator here now which is confined in every direction but one, which means that there will be, if you assume that we're in three dimensions, we have two domain walls, the remaining The remaining Z direction is not confined. So you will observe some H transport along that direction. So that object is really the one that models H transport or transport along a one-dimensional interface such as D. And the last thing is, okay, once you have confined in every dimension but one, just confined in the last one, once you have confined in the last one, you have killed all kinds of essential spectrum in the vicinity of zero and you are in uh business now. You're when you're in business now to have a Fredom operator. Fredom operators are like that. They don't have any essential spectrum. The spectrum that corresponds to transport is absolutely continuous spectrum. That's the one that we are after to understand these transport properties. You've killed it. You've removed any kind of essential or absolutely continuous spectrum. There's only point spectrum left and you can have a chance of having a federal moderator. And then you apply her, you look at her model at the right page, which is a theory that's really very A theory that's really based on a work of Fedozov that was during the IT Asinger theory developments. They had to come up with calculation of indices of operators that live in the Euclidean space. And you have a formula. So this A here, which is the symbol, like the bias symbol that I mentioned earlier, this A, which always exists, again, this A associated to F, that's a matrix-valued object. You construct a D2D minus one form. D 2D minus 1 form, you're in phase space. Phase space is 2D dimensional. 2D minus 1 is always odd. So there's always a winding number associated to odd stuff. You compute that guy, and that gives you. So that's the classification. So I gave you a classification for all possible operators that belong to these classes. It's very simple. And there's an expression for it. And the index is you saw it earlier. That's the object that's topologically stable. object that's topologically stable, the dimension of a kernel is not topologically stable. Know that you can have eigenvalues that screw up the dimension of kernels. But that difference is okay, so briefly let me give you some examples. If you start from DIRAC with no domain walls, this system allows for propagation in every direction, a microscopic model of graphene. So the first thing you do is to confine it in one direction. Confine it in one direction. The second thing to do is to complete the confinement in the second direction, and then you have a Fredon operator whose index you can compute and boom, you get points. And you see that the symbol of f is actually something that's linear, the way it's constructed, and C, zeta, y, and x. And you see that this guy is equal to zero only at the center where the topological charge resides. And how do you compute a computer charge? Well, you draw some sphere around it and you integrate whatever. And you integrate whatever object you need to integrate around the sphere, and you give it. So, we're maybe used to churn numbers. I will go very quickly through that because I don't have the time to be more detailed. But you can realize, and those are things that really come from things that you see in Volovich and the Singurari, which are a very nice way to meet you to understand topological insulators, which are related to Green's functions. So, my A, the symbol of F, A, the symbol of f, is actually very much related to one of these Green's functions with imaginary frequency. And so once you realize that these objects exist, and you do a little bit of change of contour, you see that instead of integrating over a sphere, you dilate your sphere so that it integrates over hyperplanes in Y. So you fix Y to be either plus R or minus R, which means you fix Y to be either in the northern hemisphere where you have bulk information or the southern hemisphere. Information or the southern hemisphere, where you have another bulk information. And what you see after some calculations is that the object that I mentioned earlier is nothing but a churn number coming from the southern hemisphere minus a churn number coming from the northern hemisphere. And you're looking at the difference. Now, those churn numbers are not churn numbers. There is no phase that's defined in the north and in the south, because my brain zone, in order to accept a pair of In order to accept perturbations, my Bruin zone was opened up to an infinite domain. The A inverse is the symbol of the inverse. That's the inverse of the symbol. That's the inverse? Yeah, yeah, it's inverse. The symbol of the inverse. Alright. No, no, no, no, no. That's the inverse of the symbol. Alright, so you have operators. You have operators, but you also have phase space. Here we are at the level of phase space. We have completely the operators have disappeared. Everything is classical algebra. So I have a matrix A, which is invertible, matrix A, which is invertible away from the invert object, and I construct a phase-space Green's function, right? So it's a fake Green's function. It's an object which is the inverse of the symbol of the operator. It's not the Green's function of the operator. Function of the operator is just the inverse of the C. Okay, well, that's how. Yeah, okay, so but that's not the green function, it's not the kernel of a green function. Okay, it's not a kernel of a resolvent, it's an inverse of a single. So, this is something that okay is explicit, and again, if you integrate it well, you do the standard projectors, maybe you recognize the integral of a very curvature. So, these are really integrals of very curvatures. And what I was saying was that. Curvatures. And what I was saying was that these breeding zones, quote unquote, were opened up. And when you integrate a berry curvature over an opened up, something that's not compact, there's no reason why it should be one. And it's never one, actually. It's one half for the RAC. And for the bilayer model, which I'm not sure how much I will have time to talk about, we get actually a continuum of values. It's not even one half, it's actually a continuum of values that depends on your coefficient. Now, what you can do is take north and south and glue them together by Glue them together by circle compactification along a sphere. Now a sphere is good, it's a good object. And you can compute a bona fide churn number on the real object. So, okay, so mathematically this is nice and everything is working relatively well, but you don't have churn bulk invariant, but you have a bulk difference invariant, which again physically is related to what I was saying, that it's much easier to do relative phase. Where is G used? Where is G used? Whereas G used? G is A. So it's related to a Grange function. So, when you have a projector, you can exponentiate a projector. You go to Bernavique and use at the right pace the page again, and you see that you can exponentiate a projector, and any churn number can be rewritten as the winding number of that grains function, which is an exponentiation with an additional variable, which is an expansion. So, maybe at that form of a definition of HK, is it formula? Yeah, so the formal definition of H hat is my operator here. Once I looked at this operator here and I put myself at y fixed deep into the northern hemisphere or deep into the southern hemisphere, I've assumed that my Hamiltonian were constant coefficients. So I take the Fourier transform of that object and voila, that's this guy. So that's the Fourier transform of the Hamiltonian deep into the northern or the southern. Hamiltonian deep into the northern or the southern hemispheres, where you assume that everything is constant coefficient. And so this is the symbol of the Hamiltonian deep into the northern or southern hemispheres. So then G is the Green's function, right? No, well, okay, so not really, right? Well, it's a Green's function for the northern and southern hemispheres, yes. Yeah, so I get, yeah, yes, it would be, but it's a Green's function of Fiche equation when the coefficients are constant. Okay, so. Are constant. Okay, so it's better to look at this in phase space because these objects are still defined when the coefficients depend on. It's better to see it again, that's the inverse of the Green's function, the inverse of the symbol of the operator, but when the coefficients are in Barnaby, you're right, when the coefficients are constants, those two values. Okay, so I want to mention that in 3D you can do the same thing. You can start from a Vile Hamiltonian, which is a 2x2 system, add a domain. A two by two system. Add a domain wall, that's not enough. Add another domain wall, add a third domain wall, and boom, this is my answer. It's very easy to do. So, this has some applications in high-order topological insulators where you add as many domain walls as you need to classify something which is a higher topological insulator. You need two confinements in order to get something interesting. Now, you could, instead of having x1 and x2, you could have more complicated expressions, more complicated polynomials, and those. Polynomials and domain walls that together gang up together to make sure that you are confined in the x1 and x2 variables, but not confined in the x3 variables. So if you start with objects like this, so I'm starting with h2. Instead of confining it to an x0, I'm given an h2 which already has two domain walls, but which are much more interesting in some ways. And then you can try to classify that. Okay, so you construct the federal operator which finalizes the Which finalizes the confinement. And then that's allowing you to have a coaxial cable with P-protected modes. The topology of that guy is P. I don't know if there are realizations of that, I've not seen them. But you see that there's a lot of versatile technique to topologize this, which is very difficult for higher-order topological insulation to topologize this. Okay, so here are the two results, and I, yeah. Yeah, so in the 3D case chart, the third dimension is H. The third dimension is M1, M2 are domain walls, but I can think of X3 as H. Yeah, so it's a hinge. That's right. So you call it... So it's typically called a hinge in the framework of high-order topological insulators. But yes, you're confined in that direction and in that direction by this more complicated domain wall, which has a more interesting phase uh more interesting phase. Phase more industrial phase. And yes, you will have P modes that are topologically protected going from top to bottom or bottom to top, depending on your orientation. So I want to be more arbitrary, but not M3. Okay, I'm cheating a bit. If I want a theory, I need actually this guy to be, I need in the theory that I have, I need to homogenize them, right? So that object I will take a square root, a pth, I will take power one over P of that whole thing, right? So repeat of that whole thing. So we can go morally you morally this this is a these are two confinements and here they're confined. And if you want to do to go through the theory all these objects need to be homogenized. First order that has to be homogenized to first order and first order. But those are technical details again. The idea is that if you can confine in every direction you will have a relatively easy object to compute which is easy. Alright so the main results are this and I won't have the time to mention And I won't have the time to mention them to you, but for this sodio-differential calculus, these techniques, these assumptions on the Hamiltonians, as I've mentioned, they allow us to construct conductivities to prove that this operator is stress class, which is by no means trivial. It's painful to prove that an operator is a stress class. If you've tried, there are no easy, well, there are some criteria, but there are no easy criteria. So you can prove easily a stress class. You can prove relatively easily that 2œÄi by modification, once you have. By modification, once you have the trace and you know that an object like this is Fredom and the index can be written by a trace itself, you can relate the traces, the two traces, and that's where the phi is. So this phi here is the integral of that phi prime I mentioned earlier, something that goes from 0 to 1. And out of this, once we know it's an index, we know it's stable respect to all kinds of things. So it's certainly stable respect to fluctuations which are compactly supported, which was Compactly supported, which was worth on the origins. It's also stable respect to rescaling to work the semi-classical regime. And that's something that is nice because that allows us to prove that last result. But that's something that is also a difficulty with adequating physics and mathematics. Topology has all kinds of continuous deformations, including many that are extremely non-physical. And you have to have a theory that includes all these non-physical objects. Includes all these non-physical objects in your theory because otherwise it's not a theory. It's not at least a mathematical theory. So this fact that we are invariant with respect to death scaling is actually very useful because then you can use semi-classical analysis like what Eric showed us yesterday and you can prove this result. This result says what? This result says that this index that's very easy to compute is exactly the same index as this guy that's very hard to compute. So that's the And this is joint work with Solomon. All right, so there's some work in the literature. I'm going to skip it. Just want to say that K-theory is very important in this system. The analytic techniques that I'm using borrow a lot from this very, very nice work in the 80s and 90s of Barry Simon and collaborator on the integer control. And from the physics literature, this is really what was the most inspiring. Okay, so here are some results. The theory applies to all these results, these Hamiltonians that I mentioned earlier. I'm going to skip that. I'm going to say that the ellipticity condition that I mentioned, ellipticity conditions means that the branches go to infinity at infinity. So here are some pictures where you have a, okay, so maybe also look at that picture here. Maybe also look at that picture here. A Dirac operator. Every branch goes from to infinity and to infinity. That's a signature of ellipticity. For the bilayer model, that's also true. I also have branches that I can identify as going from infinity to infinity. That's not true for the geophysical problem, the one that I chose as my example. It turns out that this object is a spin one operator, and so it has two nice branches that go to infinity. Are branches that go to infinity and that awful flat band. So I know you guys are all after flat bands. In that view, flat bands is the ultimate enemy because when you start having some fluctuations, the fluctuations talk to this damn reservoir of infinite dimensional spectrum that is the flat band. And so I can't get rid of it. And I cannot get rid of it. And here is an example where if you replace the Cori's force to be linear, where you get everything I said with a churn number which is true. A chart number which is true, so the bulk boundary corresponds works. In the case where the Correlist force is a sine function, so minus one in the southern hemisphere, plus one in the northern hemisphere, everything collapses. There's only one branch that crosses this energy interval. Any interval, let's say, between 0.2 and 0.8. The bulk boundary corresponds fails. So maybe you don't want objects that jump, but if you accept objects that jump, then the bulk boundary corresponds. Object that jumped, then the bulk barrier is transverse, and the elliptic condition is not satisfied at this formula. So you can fix it because that bound is flat, but it's actually topologically trivial, so you can move it a bit around the theory. I wanted to say that that ellipticity condition is actually... Okay, so one second on twisted biography, and I'm almost done. Where we're interested in a setting where you have A, B, and D A stacking, and after relaxation, as we have seen, these As we have seen, the Hamiltonian is much more stable and constant along that edge. And so you can try to understand what happens in the presence of a gate. So the top layer is at a different potential from the bottom layer. So you have a two omega difference of potential. And if you have these models, so the BM model where you have removed the Moire structure, so the interesting aspect, where you focus only on that region one here. Only on that region one here, so that there is only variations in space that are orthogonal to that interface. Then you can compute those objects. You see that you have shifted Dirac cones at the level of the bulk objects. This is awful. Instead of having a Dirac point in which the two bands touch, you have a Dirac circle in which the two bands touch, alright? So just shift two Dirac cones with respect to one another. They intersect at the circle. It's a nightmare. It's a nightmare. And if this is what happens to the edge modes, which are computed numerically, but we have no idea on how to compute that theoretically or have a more theoretical understanding of what happens, but the bug edge correspondence applies. So you can fight against what happens in the AB and what happens in the BA regions. You can fight against those Hamiltonians and get a burke shared number per difference in variant, which is what you expect. So there's two modes. Which is what you expect. So, these two modes that propagate on this edge. And you see that there's two regions, one and two. So, region two is actually what happens at the intersection of, well, six. So, the AA is shrunk to a point, and you have the AB and the A regions here that generate a more interesting pattern of domic volifuge. And so, we've also had a topological interpretation of that. Don't have the time to queue it, I won't mention it, but that's work that we did. Mention it, so that's work that we did with Danielle and Solomon and Paul, who might be watching us on. Okay, so here's the summary. The conductivity is given by an index which is hard to compute, but surprisingly given by an index that is extremely easy to compute and extremely versatile. This but differential number. And to come back to the slide two or three, I mentioned to you at some point that we could also relate this asymmetric tree. Also, relate this asymmetric transport to what goes from left to right minus what goes from right to left. And so, spectral theory that we are starting to develop, we have some examples for that one. In some cases, we can prove that indeed this asymmetric transport is what you expect, which is the difference of transport from one side to the other. Okay, so I'm late, right? So, sorry. Uh maybe we'll have a pick. So maybe regarding the the assumptions in your theory with this semi-classical analysis do you need the key models or can you also because you start initially by talking about like disorder and things like that so right so the the techniques are um typical techniques in in in this whole business which is that you start uh so let me look at um So let me look at that result here. You start by defining indices that are defined for all classes of operators. So that index and that index are defined for Hamiltonians in the presence of perturbation. Now you want to compute that index. And you're not stupid enough to cram in some very complicated formula when you can cram in a much simpler one. So you prove that this class is immune to perturbations. To perturbations. And so you can remove the perturbations, you can put yourself at one point in space, identify one point you like, and every other coefficient you shrink to that, the value of that coefficient at that point. So you can homotopy a complicated Hamiltonian to a Hamiltonian that's a lot simpler, and then you can do that computation. So very similar to the fact that the coefficients can be constant. Coefficients that have to be non-constant, they stay non-constant, but every coefficient Constant, they stay non-constant, but every coefficient that can become constant, you make sure it becomes constant so that you can do the so you can do the calculation. So, every calculation of every index is based on something like this. What's hard is to prove that these indices are also defined in the presence of perturbations, and they are in that. So, for example, when you have some kind of, let's say, just a mobility gap, right, so I don't know how to do mobility gaps. I don't know how to do mobility gaps because mobility gaps are perturbations V here, which are much more painful than what I can handle. But okay, so mobility gap can be a continuum of values that's complicated. I can have a perturbation that generates a compact perturbation, and so in my gap, I can have as many eigenvalues as I want, but they will stay finite, right? So that the theory applies. Now, does this generalize to more complicated these? I don't. Complicated these. I don't know, but certainly in the theory where you have a certain slab of randomness and you don't have randomness outside of that slab, that corresponds to a compact perturbation. And so that at least answers the question of when you have a very jagged interface, is the transport going to be stable? The answer is yes, and it's enough to have a compact perturbation to to answer that that question. Now mobility gaps, uh, when the spectrum generated by the fluctuation is uh more complicated by talk. It's uh more complicated by Lynn, Lynn, would you turn off recording? Lynn, could you turn off recording and it's pre-started? 