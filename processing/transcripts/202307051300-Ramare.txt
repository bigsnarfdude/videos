I'm here to present the beginning of density estimate and how it goes roughly. And tomorrow, Azif Zaman will present some more evolved stuff. So history, history. The history starts in. Well, just a bit before 1918 by Long Daro and Bohr. They were, of course, trying to prove the Riemann hypothesis, and they couldn't. And they decided to prove that at least the Riemann hypothesis was not completely wrong. So, the first drawing I have to do is the same drawing that you see everywhere. So, let me try to if I So let me try to fancy. I guess you recognize what I'm doing. Yes. That's real part of S equal to one. That's real part of S equal to one half. And so the Riemann data function is believed to have all its zeros on the line real part of S equal to one half. And we know the number of zeros. Rows in a rectangle. Let's see imaginary part of s equal to t and here imaginary part of s equal to one. So in this rectangle, we know the full number of zeros. And we know that we have about t upon log t times log t upon log t zeros in this region. And we believe that. And we believe that they are all on the line real part of s equal to one-half. And what Bo and Londao proved is that the number of zeros in this small part here is little O of T open log T. A density estimate is to estimate the number of zeros on the others on the right-hand side of the line one-half. Yes. Yes, I'm not going to say more than that. Okay, so how do they? Oh, this one is gone. Oh, God. Thank you. Yeah, I need just some notation. N of t is a. Oh, no, no, I'm not going to use this one. I can skip it. I can skip it and of sigma t it's what it's the number of zeros rho so that zeta of rho equal to zero real part of rho is larger than sigma and imaginary part of rho is between zero and t yeah so that's the quantity we want to bound n of sigma t and we want sigma And we want sigma strictly larger than one half. The question is bound n of sigma d and yes, so we would like estimate like n of sigma t less than, I don't have enough space, sorry. Have enough space. Sorry, let me take some more space. N of sigma t less than some constant, which is called usually c t to the a times one minus sigma and some power of log, log t to the b. So we are aiming at this kind of result. Theoretically, we want A to be as small as possible. Practically, and from an effective viewpoint, we want A, C, and A, all of them small. So that the total number is small. And we want that or sigma larger than something. Let's say, let's say, strictly larger than one-half. Have but essentially what we're interested in in what we are doing is to keep sigma close to one. Typically, sigma equal four fifths, something like that. Not one, one minus something, 19 by 20 is very close to one, but still, yeah. Okay, let me check. Yes. So I have to tell you about, I'm going to try to change the page. So what do we believe is true that the density hypothesis and what it tells you that n of sigma t, I mean, it will tell you, is less than a constant. Than a constant t2 plus epsilon one minus sigma times some power of t. So you see, I had a constant, I had t to the a times one minus sigma. We believe that a should be less than two for every sigma strictly larger than one half. That's the age. Um, that's the aim. In fact, we can do better in some range of sigma, right? Um, but that's uh, that's the goal we've put on the horizon. Uh, now I want you to have a look at the t to the one minus sigma. Uh, you see, when sigma goes to one, this one goes to one. Okay, when uh When sigma equals one half, two times one minus sigma is one, so it's t, meaning that the total number of zeros is t log t to some power, which is correct. And especially you see when sigma equals on the Girl Free region, one minus one upon log t to the one minus sigma when sigma is on this line. Sigma, when sigma is on this line, is exactly a constant. So this t to the one minus sigma would be a constant on the Dorphy line. That's why we have this power of log. And as if tomorrow will tell you that we can remove this power by some more evolved techniques. And in a smaller part. So how did Bo and Landau Did Bo and Landau proceed? And we use, in fact, the same kind of pattern of proof for all the proof. The first thing we do is that zeta is one function, but it's a nasty function. So we try to modify it so that it becomes better. So the idea is to find something that we control, a function g of s, so that g of s. G of s so that g of s times zeta of s is a good guy. So what would you do? Well, if you take g of s equal one upon zeta of s, that's cool. But you need the Riemann hypothesis to do that, which is a bit complicated. But what you can do is that you can take the approximate the truncated Euler products P P the g of s your product for p less than some parameter p of one minus one minus p s and then you believe that maybe g of s times zeta of s looks like one if p is large enough with respect to s maybe g of s is an appropriate of s is an approximate inverse of zeta and that should bring a zeta of s close to one that's what uh Londao and Bohr and Londao did I'm telling you that because so that you won't try to do it because it it uh a calcium Because Carlson just noticed in 20, I think, 21, 20, 20, that it's better to take better g of s is a sum for n less than x, that's traditional, mu of n upon n to the s. So don't take the earlier product, take the partial sum. Product takes a partial sum, and Carlson got a result that was much better than the result of Londau just by changing this candidate. Okay, let me see how I want to do the sequel. Okay, once you start doing that, you may want to do some other trick. More tricks. Fix for instance, you can look at one minus g of s zeta of s minus one to the two k. If okay, if g of s is an inverse of zeta, g of s times zeta is one minus one is zero, so this function takes the value one, okay. Okay, but if zeta of s is zero, the case that we believe won't happen, then this function takes the value zero. So this function has zeros where zeta has zeros. And I'm going to use k equal to 1 because you'll see in the proof that it's good to have an L2 mean. If I had more information, I would use k equal to 4, equal to 2, exponent 4. I'm just telling you that you may try to do this kind of thing. And I'm keeping a GFS a bit general, because in fact, in several proof, we take a bit something, something a bit different than this kernel. Different than this kernel. We still believe that it should be an approximate inverse, but we have to adapt to the proof. And I think we have another. Oh no, the Calconi already. So the second one is a Calcon. Okay. And I'm going to stop here for the tricks. Yes. I have to say something else. Something else. Okay. Now we have a function which has not the same zeros at zeta. It may have more zeros, but where zeta is zero, this function is zero. We believe that it is fairly small. And then we need something very important, which is a way to detect a zero. So I want you to remember. So, I want you to remember this term, which is zero detection method. It's a very important concept. And let me see, 18. Okay. So, I'm going to spend the next 30 minutes, if all goes well, on several zero detection methods. And the end of the talk is going to be on how to use. Talk is going to be on how to use moment estimates to control the number of zeros. Okay, but zero detection method is a fundamental part. Often when we write papers, we just write lemmas and lemmas and lemmas. But you should see, okay, in this paper, that's how this person detects the zero. That's in this paper, that's how it is done. Just identify this part and put it out properly. It out properly. So you know, at least my group knows, how to count the number of zeros. You take zeta prime upon zeta and you integrate it along the rectangle that takes care of the zeros between real part imaginary part equal to zero and imaginary part equal to t. So zeta prime points. So, zeta prime upon zeta, you go on the line real path equal to two, yeah, you cross and you go to real path equal to minus one half, that's fair, and you take all the zeros in this rectangle. And why do you succeed? The horizontal segment, so complex analysis tells you that you count the number of zeros by doing this integration. That's okay. That's okay. On the two horizontal segments, the two horizontal segments are very small. They are of finite length. You can control what happens here. You don't have any difficulty. On the line rel par data is equal to two, zeta prime upon zeta. It's a DRF series. We know everything about it. So that we can control easily. And then on the line minus one half, what do you do? You use a functional equation. You use a functional equation and you bring the minus one half to three half again. You have a Dioshi series and you have it. Okay, so this logarithmic manner of analysis zeta prime upon zeta way of doing is working because of that. And now if you look at what we have usually, and now in our case, sorry, let me do it from CD again. Uh no one has one That's the rectangle we had, yeah. So we have to integrate on the line two, same problem as before, no difficulty. We have two horizontal segments. We have two horizontal segments as before, it's a negligible contribution, but then we have this line here, which is on the other side of real part of S equal to one half. The functional equation will bring s to one minus s, which is farther away, and you're still in a field. So zeta prime upon zeta does not work. I mean, for it to work, you will need information on the zeros, but what you're You will need information on the zeros, but what you're trying to get is information on the zeros. So, the idea of Bohr and Lambda O and then the idea of Little Wood has been theta prime upon zeta is a bad guy, but the integral of theta prime upon zeta is log of zeta. And log of theta is almost log of absolute of modulus of zeta. And then we have to bound above. have to bound above theta we have to find some upper bound for theta or for the function you want to whose zero you want to detect and uh that's uh that's the first way that was uh that was presented and uh do i have the i thought i would write the statement fully on the board but uh you have to be patient because Patient because it is long. It's a classical CRM of little wood that you'll find in Titch March. Phi of S in the domain alpha, real part of S beta, and keynote imaginary part of S T. T holomorphic. Okay. What do I want else? A little fire. Yeah. Okay. Yeah. I wanted to write this one because we are going to define capital Phi. Yeah. We want to be log of little phi. Little phi. And but of course, this little phi may have zeros. So defining the logarithm is going to be difficult. So we are going to assume that we assume that pi of s is not zero on real part of s equal to beta. For us, it's going to be real part of s equal. For us, it's going to be real part of S equal to two. Here it's not, it's not zero. And that's all I'm going to take. Yes, okay. And in fact, also by a first non-zero of any part of the rectangle. So unreal. So, on real s equal to alpha, also, and on imaginary part of s equal to t or t naught. So, on my rectangle, I assume that it's non-zero. How am I going to define the logarithm? I'm going to take a pass, go from T node, go up the segment real part of S equal to beta. Real part of S equal to beta somewhere. And then I'm going to go horizontally to my point S. And that's how I'm going to get the log of S. It's precisely this path that I'm taking. If you take another path, you'll get another logarithm. So we have to fix something. It is this path. This path works, except if on this horizontal line, you cross a zero. But the zeros do not accumulate. Um, do not accumulate, yeah. So, in this case, you go a bit up, epsilon up, you find a value, and you take the limit going down, the limit will exist. So, define πFS by integrating In a beta plus it node top beta plus it up s equals sigma plus it you integrate on that and that's what you you call uh you so you integrate uh uh phi prime upon phi and that what you call uh your lock. Your lock. That's all. Of course, I need nu of sigma. You can guess the number of zeros of phi on real part of s larger than sigma. And then the statement I'm going to. The statement I'm going to write it here tells you that the integral upon my rectangle, so everything. So you go from beta plus it naught, you go from to beta plus it, then you go from beta plus it, you go from alpha plus it, then you go from alpha plus it to alpha plus To alpha plus I t naught, then you go from alpha plus it naught to beta plus it naught of phi of S ds. It is equal to minus 2 pi, if I remember well, 2i pi, sorry. Integral from alpha to beta of nu of sigma d sigma. Okay, that's uh That's the theorem we have. So, an integral of my log gives me an integral of the number of zeros. So, I've explained you why I need an integral. I know, I need an integral because I want to compute everything. Psi is a good guy because it's a log. But what you get at the end is you get at the at the end is only the integral of mu and not nu because in fact phi was already had already an integration inside uh proving this lemma is easy i'm not going to do it because i'm a bit late um but it's a it's uh it's it's easy but now i want to show you that because of this formula we are going to lose automatically a log at the end The end so that as if tomorrow is not going to start by this formula, but it's because you have to integrate zeta prime upon zeta that you integrate new, that you lose the log. But if you don't integrate zeta prime upon theta, you have something that you don't control. So how do we do we get to Get to how do we get to n sigma? We write, assume you want n beta zero, n beta naught t. Well, the number of zeros with real part of s larger than beta naught is a decreasing function. The bigger beta node, the less zeros. So you can say that it is less than the say that it is less than the integral from beta naught minus one upon log t to beta naught of n of beta t dt and since you want an upper bound it goes in the good direction uh uh except that uh time is locked t yeah you want small t instead of Small team instead of uh yeah, that would be better with small team. Uh no, wait. No, no, no, no, no, no, no, no, no, no, it's it's uh uh uh sorry, sorry, you won't debate us, yeah, yeah, it's debate, yeah, yes, this way it makes sense. I hope, yeah, okay, and. Okay. And so this one is less than integral from beta naught one minus one upon log t to one of n of theta t d beta log t. And now this integral of n is given by the lemma of little wood, where we have an integration of log of log of my auxiliary function, not a d. Auxiliary function, not a zeta, but a bit more complicated than zeta. But you see that this log t is required here, it's it's uh it's part of the method. If you don't integrate n of t, you don't integrate zeta prime upon zeta, and you cannot reduce to upper bound of something. Okay, I'm uh late enough, so I'm going to conclude. That's what I call That's what I call. Did I put it? So this one was, it was what I call the logarithmic way. It's the way we've used, we've done density estimates from 1918 till 1970. So for 50, 40. So for 50, 40 years, 50 years, that's this lemma that has been done. We've used this part and we've repeatedly used that. Chen, Jingrun, and also myself, we proved an explicit density estimate, and I used this way of detecting the zero. And Abiba proved an explicit density estimate, but she proved something numerically very good, theoretically. Very good, theoretically weak, and she manages exactly also with this one. Now, I'm going to present you some other ways that are still extremely connected and so that if you see it in a single installment, you'll see the link between them. So, one, two, the large value approach. The large values approach. The large values approach. And there's going to be three and four. There's going to be the weighted large value and the arithmetically weighted large value approach. So in the large value approach, you start in the same manner. You start with zeta of s in the sum of one upon n to the s n less than with a key. Let's say T is big O of T to the minus beta plus for so S equals sigma plus I t okay and I assume that T is between capital T and T by two and I assume that beta is larger than one half Then sigma is larger than beta is larger than that's my approximation of zeta. And I'm going to look at zeta times mx, which is my this one is a partial mu n by n s less than x. It is some An X. It is something like some from okay, and we're going to say like that. Some for d divides n of mu d d less than x one by n to the s whose big O of x t is. Exit big to be completely correct, even if it's a sketch, to be completely correct, it's better if you put the log 2x here to take care of what happens on the line real particle to one. Anyway, Anyway, it's an error term. And you see this coefficient here: the sum for d divided n d less than x. Of course, it takes the value 1 when n equal to 1. It takes the value zeros if n is between 2 and x because the convolution of 1 times mu is 0 on this part. At on this on this part. And then after this part, it is something. So this one is one plus the sum for n between x and xt of coefficient 1 upon n s close error. Okay, so what happens if you get a zero? If you get a zero, If you get a zero, zeta of s time mx is zero. It means we the one is equal to one. The error term, forget, you'll find your parameters that it's a true error term. It disappears. So the sum in the middle is equal to minus one. And it means that we have detected something which we believe is a too large value of this one. I'm going to show you why it's a too large value. I'm going to show you why it's a too large value of this one. So if you split in the headic interval, x, 2x, 4x, 8x, 16, till xt, you get about log t by log26. And this log t by log2, one of and the sum of all of them, so let me put it j from zero to log t by log two, the sum from x times two to the j and x two to the j plus one of coefficient. Of coefficient divided by n2ns is equal to minus one plus eroton. I mean minus error term in fact. So one of them, one sum for one j, for one j we We have some from x to the j and x to the j plus one of one other. My coefficient divided by n to n s is larger than minus one upon log t about one minus One minus bit of something. In absolute value, larger, and I'm going to remove this one because otherwise it's not understanding. One of them has size at least constant. Constant divided by log t. But if you look at this n to the s in modulus, it's about n to the beta. So you deduce that by summation by parts, you deduce that exist n and n prime less than 2n, so that this sum of micro. Of my coefficient n to the minus i t beta is larger than is larger than some constant n to the beta upon log t uh no this was uh it was the rho this one gamma Okay. Okay. I should put when s equal beta to psi gamma is a zero of zeta. Okay, you follow till here. Now, what do you believe about the sum of a coefficient which we are? A coefficient which we assume is not very big, like a divisor function, divided by something which is oscillating, a sum of length n. You believe that you'll get square root cancellation. So you believe that this sum should be of size square root n. But because you believe, you think you assume that beta is strictly larger than one half, even very close to one, this sum is of size n to the one minus. n to the one minus a bit of something divided by log t, which we believe is abnormal. That's what we call the large value approach. We find a Dirichlet polynomial which takes a large value and then we are going to say it cannot happen or at least it cannot happen too often. The number of points gamma on which all these coefficients n to the i gamma align to make a large value is To make a large value, it's not possible that there should be many. That's the large values approach. And my computer is down. Okay. That's the way that it is presented in Ivanette's college. The third way. Way so the third way is what is weighted large value large values approach in fact historically it appeared before the second one but the second the lat value approach that I that I presented is a simplification of Is a simplification of what was done before. So the weighted at-value approach, essentially, it comes from Montgomery in 1969. In Montgomery in 1969, wrote a paper in Invention S about, I don't remember the title. Dirty polynomial, averages of Diracle polynomial, something like that. 1969. And look at this sum. Some for n less than y of sum for d divides n, d less than x of mu of d 1 upon n to the s. Why I say weighted is that because now we have n less than y. n less than y which is as if you put a weight one for n from one to y and then zero from y to infinity this one it doesn't factor out so how can you relate this one to zeros of zeta that's very easy i guess uh michela can tell me how to do that explicitly you use Basically, you use a pair of formula and an approximate truncated pair formula. Okay. Let's say two minus it to two plus it. The complete sum. Complete sum is here. E divides an e less than x of mu of d one upon n to the One upon n to the uh let me call this one sigma uh sigma node s naught because otherwise I'm going to get uh confused s naught plus s y s plus s d s plus error now it's just approximate peron i i i recognize the condition n less than y by uh um the lean integral i i have the y to the s I have the y to the s, the s, but now I have a complete sum. It's d, yeah. And this complete summer, you know what it is? I mean, I could put it for the assignment for tomorrow, but then my talk will stop here. Will stop here, which is not good. Well, this one is simply mx of s times zeta of s. That's the function we had before. And once you have that, you push the line of integration. And because in s equal to, sorry. Equal to uh sorry, no, it's it's s is not s. Sorry, I should step s plus s naught, it has s plus s naught. When I push a line of integration in s equal to zero, I have a pole coming from ds by s, but which is killed by the zero of zeta. That's where you use the That's where you use the fact that you have a zero of deduction. In this manner, you show that this sum is very small. The full sum is very small. But yet again, in n equal to one, the sum has value one. So the other guys are small. That's a tricky. Push the line. Push the light. I have to go this faster. Okay, in fact, I present it in this manner, but what Montgomery was doing is just a bit different. And this one is a And this one is a true exercise, n less than y. It's not n less than y, precisely. Yep. N larger than 1 of mu of d exponential minus n by x upon n to the s. But you see this kernel here, exponential minus n by x. n by x you can use the Melin transform uh Marcela is not happy with that you can use a Melin transform to recognize exponential minus n by x you do the same as before things are even simpler because you don't have any problem with convergence in the uh um other part yes yeah which is oh yes sorry yeah yeah otherwise it will Otherwise, it would converge rather fast. Yes. Okay. You want that to be white then? Yeah, Cisco is white. Sorry. Okay, it's a very, very weak musim, but. Very weak smoothing, but it's it's good for what fencing. And four arithmetically weighted it's a room large value large values approach. That's for essentially also for people who are going to do the project with NATO. The project with Naton. Once you see that you have this kernel sum of d divides n d less than x of mu of d if you if you saw what happened when we had the complete sum, this one divides n d less than x of mu of d one upon n to the s. We set it's mx s theta of s. And why does it work? Because this one work because this one this sum you put the summation of d in front yes and you get some for d divided n of one upon n to the s which is zeta of s times one by d to the s so the only thing you are interested in was that it was a condition d divides n the fact that it was mu of d you don't care so you can you can for the same prime For the same price, you say, okay, now I have some space. I can put another one from L divides N of theta L and some for N larger than one. And then the result is M sharp, which will depend on your theta. You get a very complicated formula, but You get a very complicated formulae, but you're going to take t-tile which is compactly supported, so it's only a small modification. That's what Graham does in 1978. It's very related to work of Moto Ashi in 1978 and recently the And recently there are some papers by Moto Ashi that prove some density estimates for L-functions of modular form, which uses this kind of stuff. Anyway, so it has some history. It continues. So you see, at the beginning, we have the logarithmic approach, zeta prime upon zeta, we integrate, we get log. It's still the idea that we modify and we... That we modify and we modify, it's one, it's the function should be essentially one. The same approach, you can say, Well, let us look exactly at the kernel before looking at the logarithm. Let us look at the function and we detect a large value. Then, because only taking a large value may be a bit tricky, you prefer to introduce some parameters, so you put some smoothing, so you wait. And because arithmetic. And because arithmetically you say, maybe I may save some more things, you put, you add an arithmetical weight. But all of that is of the same of the same table. And my computer has gone. Okay, so when you have a paper to read, or if you want to write a paper on density estimate, the first question you ask yourself, how am I? The first question you ask yourself: How am I going to detect the zeros? What am I going to do? Where is my detector? Once you have your detector, the second information is how am I going to input information on zeta to use my detector. I didn't tell about the size x. It's clear that if x is extremely large, I have an approximate inverse and I'm An approximate inverse, and I'm having so. In the course of the proof, after that, I'll be forced to take x a bit small because otherwise I won't be able to handle the integral that I get. And that's in this trade-off that you get your estimate. I have to no, I have to, but I won't. Abiba? You know how to I will add a page? Yes, okay. Can you see that? I can do that. Okay. So about some general things about density estimate. Just now the theory of density estimate had worked very well from 19. Had worked very well from 1918 till 1978, let's say, something like that. And then we got stuck. I mean, when I say we, it's not me, because, I mean, I tried, but since I never produced anything, it's not that I got stuck, but I never moved. Bourguin managed to move it by a tiny bit. Tiny bit. Breisca, I think, can push Burger a bit further. So that's the last bit. And it seems that we are blocked and we need some energy to continue. Now it's about 20 years that we are blocked. We need some energy to continue. And I think that by using effective, explicit angle, now we can get some new energy to look again at this problem. Um, an incredible problem is a three-quarter problem. We have an estimate n of three-quarter t less than t to the 512 times log square t. So there's an exponent of t, t to the, I think it's 512. This exponent has not moved in 70 years. We are stuck. So Bourguin saw we are stuck. He said, let me go and beat the three quarters. Go and beat the three-quarter, he just managed to work on the right-hand side of three-quarter three-quarters. We are stuck. If you assume Lindola hypothesis, you assume that zeta of sigma plus it on real part of s, real part of s larger than one half, is less than t to the epsilon. You know on that n of sigma t for sigma larger than three quarter is less than. larger than three quarters is less than t to the epsilon but between one half and three quarter we don't know how to do i mean this part is due to turan long long long time ago uh very often i look at this problem i say okay to do to do something just three quarter minus something that's it so it's it's really a tantalizing problem that is fully open Is fully open. Of course, if you assume Riemann hypothesis, you can do it. It's an assignment for people who want to be sure to have proved something. Let me okay. What time is it? Okay. Okay, I'm not going to talk about arguments of you know since I had a log I had some arguments to control that's rather easy it's very interesting to do you have to be precise if you look in the teach mark you'll see that the statements are a bit vague it's interesting but essentially the contribution is extremely small so I'm going to skip this part now if Now, if you remember, okay, skip, skip, skip, skip, skip. Okay. I'm going to continue with the logarithmic approach. I had the integral on my rectangle of log of my weight function. So that's the second part of the usage of moment. At some point of time, we thought we would give a 50-minute lecture followed by activities. So it was supposed to be two talks, but then we realized that there would be too much shifting and going and so it's better to put only one talk. So that's the second. So essentially, what we had, so this integral, you have this integral on the real part equals to two. Real part equals to two, real part of is equal to two, you don't care to the horizontal segment, you don't care. What you're left with is integral from, let's say, sigma, sigma, and sigma plus it of log of one minus zeta of s times, I am going to put mx minus one to the two k y. 2K DS. That's the guy that you have to face, and you want to find an upper bound for this guy. And you're going to use that log of you're going to put absolute value and say the argument you don't care. And you're going to say that it is less than theta of s mx of s minus one to the two k. To the 2k. And I'm going to use k equal to 1. So, what you need to bound is this integral of theta of s mx of s minus one square v yes, not exactly, but So now you see, we wanted to count the number of zeros, which is somehow to get a lower bound for zeta, to show that it's non-zero. And now we converted this problem to an upper bound of something. And upper bound is good. That we know how to, I mean, we know. At least we know better than the one. How are we? How are we not? Are we sure that the zeros of MXLS will not be? No, you may get more zeros. Yes, yes. I don't know. I don't know. Since Mx is supposed to go to one upon zeta, it's not supposed to have zero. No, it could. But it could. Yes, yes, no, no. Especially when you start putting. Especially when you start putting your one minus zeta m x minus one. No, no, you may have more. We are just trying things. We have no fine control about what we are doing. So this integral, how to evaluate it, what we are going to do What we are going to do is that we are going to decide that I write too big and I'm going to use another shift here. No, no, no, no, no. So, to handle this integral, and that's the point why I wanted to come. We're going to use convexity. Convexity, you know, this integral is on the line sigma or beta, but sigma is between one half and one. Okay? And we are going to say that this integral, this L2 moment, is a convex function. So it's going to be the convex. So, it's going to be the context average between what happens in one half and what happens in one. We are not going to go to one because in one it's not a good idea. We are going to go to one with epsilon with epsilon being one upon log t. And numerically, it's going to be constant upon log t with an optimal constant that you might just a bit off the line one so that you can expand zeta. So, that you can expand zeta and m in a DRC service. And the complexity, and the result we use is a theorem of Gabriel. That also you'll find in TitchMarch, except that in Titchmarch, that the statement which I find vague, because there's a function that depends on some parameters. Functions that depend on some parameters, the rectangle on which it is. And then there's a less less. But this less less may depend on the rectangle, like it depends on the coefficient. But this rectangle, you have T, capital T, no? And we don't want things to depend on capital T. And also because I don't know how to prove explicitly the result that is announced in Tichman. Basically, the result that is announced in Titch March. What is Gabriel? I'm going to state the theorem of Gabriel. So the theorem of Gabriel is let F holomorphic, let's say holomorphic. Also, because I'm going to tell you about a place. I'm going to tell you about a place where my proof is not optimal and you may improve it. So, there's something that I don't know how to do, and it's fine. In alpha, real part of S beta. Okay. Okay. And with that the condition, f of s equals little of one. Little of one when imaginary part of s, absolute value, goes to infinity. You know what we are doing is that we're using, in fact, Cauchy's theorem on a rectangle. Now, and I'm looking at the line in the middle, that essentially is Cauchy's theorem with the line on top and the line here. And we are sending this line to infinity and to my plus infinity, my loose infinity. My plus infinity, minus infinity, like Freyman in derivative. So we need this condition, f of s is little of one. But if you see the candidate we have, zeta of s time mx minus one square is not going to go to zero. That I can gain. So you need to multiply by some canon. I have to finish the statement. Okay, maybe I'm going to give you the full statement of the To give you the full statement of Gabrielle because it's beautiful. There's more parameters than that. No, this one is not the one I want. This is one I want. So put J sigma lambda. I mean, I've done everything with L2 estimate, but you can put L P estimate. That's what I'm going to give you, because anywhere the CRM of Gabriel works. Anyway, the theorem of Gabriel works for everything, and if you get a good kernel, it will work for everything. So, this one is going to be to the power of one of lambda. Yes, the integral of your guy f of sigma plus it to the lambda dt to the one by lambda. We have, we have all of I know that I'm not English speaking, but still. Not English speaking, but still. We have J of sigma ah, okay P lambda plus Q mu less than G of alpha lambda J of beta mu where T are my parameter of so it should be So it should be beta minus sigma beta minus sigma upon beta minus alpha and q is sigma minus alpha upon beta minus alpha. Convexity, this time the coefficient is one, so we know that it doesn't depend on anything. And from an explicit viewpoint, it's very good. We control the Point is very good. We control the coefficient, and you see you can use it with alpha equal mu equal to so unit lambda mu positive and sigma between alpha and beta. Sigma it's um yeah, okay, um student. Stream time. So, I'm going to tell you the kernels that I've used in the paper above. Phi of S is equal to S minus 1 upon S. Don't worry about that. It's only cos s to the 1 by tau. Two for one half, less than real part of S less. Part of S less than one subside one input. Let's say, let's say let's say 11, 10 and tau is big is going going to be T. It's going to be T, the capital T at the end. So you put cos s to the very, very small power, one by tau. It's extremely small. It's better to take tau and not t at the end because the t is going to vary and the tau is not going to vary. So anyway, so it's closest to something that is extremely small. Okay. Small okay, and I put s minus one upon s. This one is not a bad guy, it's because I'm looking at zeta of s, and zeta has a pole in s equal to one. So I need to kill the pole by multiplying by zeta of s, but I correct by dividing by s. So this one, don't you cannot do uh you cannot do better than no use doing better than that. So it's not a place where you where you can do things. What you want. Can do things. What you want, you want your kernel phi of s are going to multiply your auxiliary function theta of s times lx time phi x. So you want this function to be one between one and t and zero afterwards. Okay, not exactly, but you want it to be close to that. This one is not at all close to that. I mean, it's not ugly because finally I got a result, but That but what we have is that we have why did I write it like that? One minus one upon k to the one upon two taus, if we don't care about this one, one upon one plus sine h square t to the one upon two tau less than five fs. Less than 5FS, less than, yeah, this one will tell you about what happens. Fine two sigma upon one upon two tau. One tau, it turned out, it's not two, it's one, and exponential minus t by tau, in fact. Okay, that's what we have. Okay, that's what we have. So you see, in fact, the function phi behaves like exponential minus t by tau. It's a smooth thing like that. So when tau equal t, where I want, and t equal t, it's one by e, but then it decreases really steadily, and there's a loss in between. Even if you take Even if you take between t and t by 2, you restrict your integral to that. So that's one of the assignments. Find a kernel that approximates this function. One and then put it in one and then zero better. This one is exponential minus x between zero and one. Things what we are trying to save are really constant. Constant, um, it's just something to do at this event. Okay, I'm just on time. Um, okay, I just have one more result to show you because I told as if that I would do it. Um, yeah, something amazing happened some years back. It's not because Habiba is my student. It's not because Habiba is my student, but it turns out that density estimates used to be very theoretical stuff with lots of t to the epsilon powers of t and very complicated proof and constant and so on. And I did not imagine long time ago that we would ever be able to use that for explicit purpose. Absolutely not. I'm just sure. And now we have a way, I mean, A way, I mean, Hariba is managed, so it means that we are at this level of using density estimate to improve our explicit information. So we are the level, we are just at this level. I mean, we are not very comfortable. So if we save a constant two, we are happy. I mean, really, we are at the level where it is improving, but it is just, we are just doing that. So we just So we there's a lot of room to improve, which is amazing that we're able to do it. So get the best kernel, improve on that. I think it's a very good thing to do. Okay, now we have our Gabriel stuff that puts convexity. And we put between the line one half and one to the epsilon. We still need to compute this L2 estimates, this L2 moments. That I haven't told you how. That I haven't told you how to do. I'm going to give you a very simple lemma. So to bound the new partner, what do you have to do? Turn from zero to t of zeta of s m x of s. Mx of S square dt. You want an upper bound for that. And for s equal one half plus I t now you have the estimate of first name is sorry. The first name is Hairy August. Thanks. What we are going to do with this integral, we are first going to take out the maximum of zeta of one half plus it, and then we are going to rely on estimate of very complicated names to. Very complicated name to say for me. Hi, Rig, yes. No, guess Harry. I'm sorry. I used conductiv estimates that were less efficient. Now we have better estimate. If you use this estimate, you will improve my work almost automatically. So, exponent pairs. Yes. Pairs, yes, yeah, so yeah, one of the conclusions is zeta of one half plus i t is equal to uh 6.5 t to the 16 log t. No? Something like that? Yeah. And so you get t to the one six and convexity gives you t to the one quarter. You have the same log t on both sides. So it will be better. So it will be better. Okay. T is not so big. T is about 10 to the 12. So t to the 1,6, I mean, the smallest t is t to the 12. So t to the 1, 6 is 100.6 is 100. So it's, we're really at the level of constant, but still it's going to kick in. Okay, so we had, I mean, that's a way to do. Mean that's the way to do. If we have more information, we can use other things. We can use force power moment, we can use 12 power moments, we can use a lot of things. But get the maximum of theta of s for real part of s equal to one half and imaginary part that key out. And then you're left with a moment of a finite Dirichlet polynomial on a finite integral. On a finite integral, and it's a square, it's an L2 step. Okay, and yes, I got it. And to do that, you go back a bit and you discover that I have not written the results. Okay. You have a theorem of Montgomery and Bohn. The theorem of Montgomery and Bone and 73, which tells you that the integral from 0 to t of this, some for n less than n of a n n to the it square t dt which is the guy we want equal to term for n less than n of a n square times t cos big o in fact with a constant one n cus one three half and that's all And that's true. What is the start? Sorry, I should multiply by two pipe. The map minus 35. This is infrared or equal to that. So it's two ways to concentrate. With constant equal to one. No, the old Russian blocks use the most theta. Yes, yes, yes. Yeah, but uh but uh theta is not theta with uh is a say is there is a number. I mean I prefer my my big o star because it's My big o style because it's okay. So, you see, it's a very clean statement and not exactly optimal, but very good. You can see that for it to be good, you need n plus 1, I mean n, the size to be somewhat less than t, because otherwise the arrow term is going to dominate. Is going to dominate and the addition of Montgomery and Bone is to get little n and not capital N. When you get a n, which is divisor function divided by square root of n, it gives you an improvement because then you get a n square is divisor function divided by is divisor function divided by n if you multiply by capital n you get a log an additional log if you have a little n you don't have this additional log there's a fight on this three half which i put in the assignment so we believe that this three halves should be one uh prisman a student of desuyer managed to get Of desouet managed to get the constant which is close to four-thirds instead of three-half. Um, we Selberg has told someone who has told Bonvieri, who has told Montgomery, or something like that, that Selberg was able to do much better. When I asked Montgomery, do you have any idea of how he did it? You answered, why don't you ask Selby Alex? Why don't you ask Sylvia? Which is a good answer? Which is a good answer. So, yeah, I know. There's some difficulty. Anyway, there's some fights on this three-half, and it has not moved. I mean, Freisman wrote a paper in the 90s, I think. And anyway, no, no, that's why he did not publish it because he wanted to prove the final result. And he said he had something much better than the three-half. You can trust that it's most probably true. What was the number? I think it was 1.1 or something like that. Yes. Yes, yes. But anyway, so with that, you can compute L2 moments. And that's going to be about the end of the talk. I'm just on time. Yes, cool. I'm just going to finish with some remarks. So there's lots of difficulties. You see that we've introduced some from D. some from d divides n d less than x of mu of d. Yes. If you don't use the mu of d, it's almost a divisor function. And the divisor function is going to, even if you handle it properly, it's going to give you some logarithm. And since you have this divisor function to the square, it's going to give you even more logarithm. That's Uh, that's that's a killer. From an explicit viewpoint, you kill everything you save on your power of t because you have a log t to the five and you don't even manage to show that you have less zeros and on the full rectangle divided by two. That's my test. Um, so this divisor function is very complicated, is a is a pain, and we Is a pain, and we need to use the fact that it's mobius or to get a better weight. Okay, there are some candidates with Barbara and Vov, but we need better to understand them better from an explicit viewpoint and so on. And maybe we can do better than that. But at the level of this weight here, I told you, you know, we have lots of room for modification. That's what we. That's what we know. I mean, I showed you the philosophy why we introduced them. But now that you see the proof, you see, well, I can choose them as I want, provided I keep the first coefficient. So at the beginning, it should be mu of d. And after some time, that's the first time, the first steps, choose an. The second thing is that we use convexity, and of course, we use convexity, so we go to one half and we go to one. That's what we do. But in fact, if you know exponent pairs, precisely, the exponent for zeta on one half and one, the best. One the best exponent that we are going able to do on three-quarters is smaller than the one that we do by convexity. Ivich wrote quite a bit on that. He put lots of exponential pair kappa lambda. He gets results with exponents that are one page long, which are a bit difficult to understand. But so convexity, you can use, go to three quarter. Numerically, it's not clear that this is. Numerically, it's not clear that there's going to be a saver. But you can go to sweetwater and because what we're going to do, Abhipa, I don't remember you use the line 910 for your density estimate. Something like 4-5 or 9-10, anyway, on the other side of skip. Yeah, 9-10. Yeah, nine tech. So we could go to the lines reporter and see what happens there, and maybe things will get better. I'm absolutely not sure of that. Of course, we want to get estimates that you can reuse and reuse. So we use a standard point, which is one half. Okay, but three quarter is a good point, especially for density estimates. I mean, if we get an estimate on one half, an estimate on three quarter and estimate on one, I think it's good. Yeah. So line real part of S equal to three quarter. Okay. And I think that's again. I think that's this one I didn't do. This one I didn't do. Cool. Yes, so that's that's all for what I have to do. And so there'll be activities tomorrow morning. And then you go and ask as if what we do in. What we do in modern times. Thank you.