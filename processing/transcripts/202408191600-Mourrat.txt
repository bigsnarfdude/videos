Very good. Awesome. So, yeah, as Weijun said, so first of all, I apologize for not being able to be here in person. And I'm very grateful for this opportunity. It's a great honor to be part of the conference. So, yeah, thank you very much for this. As Weijun said, this is the first lecture of a series of three. Of a series of three on the homogenization of interacting particle systems, with the second lecture being given by Chen Lingu tomorrow morning and the third by Tadaisa Funaki on Thursday morning. So the general object that we want to discuss is a model where there are lots of particles that evolve in Rd or in Zd. And you have to imagine. And you have to imagine that each of the particles wants to do a sort of Boone motion, sort of trajectory. But at the same time, the way it's moving around is influenced by the position of the particles around it. So, and you have to imagine that there's some sort of a density of particles everywhere on. Of particles everywhere on Rd, or if you like, in a big box of Rd or a big box of Zd. And perhaps if you look at this cloud of particles from a distance, then there is a sort of density of particles that you can make sense of, maybe in some limits. And the classical result, which we could call a homogeneous result, is that as you let the time pass, As you let the time pass, this density of particles is going to evolve according to a partial differential equation. And should I said also, I'm not going to study every possible model that sounds like this. I'm going to focus on a specific class, which in the topic of particle systems is called non-gradient models. For people who like partial differential equations, I would call it equations that have an operator in divergence. Have an operator in divergence form, and in particular, there will be a nice measure that is reversible for the dynamics. Okay, this will be important. But when we go and prove this homogeneous result, so this is a known result, it's been known for, I don't remember now, but maybe 40 years or something. And when we prove it, one of the aspects of the result is that the coefficients that describe the limit equation. coefficients that describe the limit equation are not obvious a priori. You have to define them in some implicit way. And this will be part of the difficulty in dealing with this result. The talk, like the emphasis of the talks, of the series of talks, is to go beyond the statement that in the limit of very, very large size, you have this PD description and try to get precise bounds on how far. On how far away from the limit we are when we are in a large but finite volume. So perhaps the just to, yeah, no, maybe I want to say this. Does this make sense roughly? Or if there are questions, I hope you can interrupt me or maybe show to Weijun that you would like me to be interrupted. Is it okay so far? I think so. Yes, so thanks a lot. So, in order to do that, for me, and I think for also other people who worked on this question, it was important that we had some previous experience on other models to rely upon. So, for me, the inspiration comes from previous work we had done on elliptic equations with random. On elliptic equations with random coefficients. So, if you like to think in terms of the probability system, it's like a random work in a random environment or a diffusion in a random environment. So, I'm going to speak also about this class of models, even though this is not really the end goal. This is just where my inspiration comes from. And I still think that it's the place where it's easiest to explain the intuition for how we're going to derive these quantitative estimates. Quantitative estimates. So, because there will be lots of different models and different settings in this talk and also later in the second and third talks. I want to spend a bit of time to discuss all of these possibilities. So let me try to give you a sort of a chart of the set of possible models that are going to appear. So, first, there is this choice between Choice between these old things we understood before. I'm going to call it random work in random environments, or if you like, elliptic equations with random coefficients. And the real goal of the series of lectures, which is interacting particle systems. So these are, so there's a choice between these two things to make. There's also a choice when you set up your model between Set up your model between oops sorry between doing it in the continuum in Rd or doing it in the in a discrete set in Zd okay and then I want to also stress there will be a third sort of choice if you like or maybe choice is not the best way to put it but there will be a model which I consider the most basic possible uniformly Basic possible, uniformly elliptic, if you like. And then there will be more difficult models, which I'm going to here denote as with hard constraints. So just to give examples of models like this. So if you do random work in random environments in the discrete setting, people also call it the random work. Setting people also call it the random conductance model. So, if it's basic, I would say it's basic if the conductances are uniformly away from zero and infinity. If you want to do a random work on the percolation cluster, then I would call this a model with a hard constraint. Okay, and this also Chenlin has worked on this also with Paul Dario. So, for me, the setting where I'm going to The setting where I'm going to try to first explain the intuition, the one where I think it's easiest, is going to be elliptic equations in random environments or random box in random environments in the continuum. And with these basic assumptions, let's say uniformly elliptic. Okay, and next, hopefully near the end of today's lecture, I'm going to move to an interacting particle system, and then I'm not going to change anything else. Not going to change anything else. Okay, it's still going to be in the continuum and still a uniformly elliptic in some sense. I want to spend time to explain this because next in the second lecture of Chen Lin, then he will continue with interacting particle systems. He's going to move to a discrete model. And you may think this is the important point to handle, but I want to stress that, in my opinion, this is not the important point. The important point is that it's going to be a model which The important point is that it's going to be a model with a hard constraint. Okay, the problem is that in the model it's going to consider, it's called generalized exclusion process, there is an exclusion rule, but it's a discrete model, it's on Z D, you have particles on the sides of Z D, but there is a hard constraint on the maximal number of particles possible on a given site. And this is a difficulty. You could imagine a discrete model analogous to the one I'm going to discuss today, which would be basic, and this is not. Which would be basic, and this is not the model that he's going to discuss. Okay, this will be a sort of you have to think about a sort of random conductance model, but where the environment is a function of the particles around them. If you build an environment like this, so a model like this, which is uniformly elliptic, then you will have to be to allow for an arbitrarily large number of particles on a given side, possibly arbitrarily large. Okay. Okay. All right. So that's it for this. And then in the third lecture of Tanda Isab is going to be a continuation of the model that Shenlin is going to discuss, this generalized exclusion process. But there will be yet another complexification which does not really belong nicely to this kind of partition I described to you. So it's still going to be a Pascal system discrete with. A particle system discrete without constraint of exclusion. On top of this, there will be also a global dynamic. So you will also be able to make some particles disappear or appear at a given site. So for the model I'm going to discuss of particles towards the second half of today's talk, the number of particles will be preserved. The particles, they do not disappear or they do not appear. But this will be something that happens. Something that happens in the model that Adaisa is going to discuss. All right. So, and perhaps just to continue on the two possible models, one model which we yet at the moment have not studied and which maybe is even harder is perhaps the model that would be easiest for you to imagine at first. So, I want to say it. You could imagine. Say it. You could imagine that the particles are just hard spheres and they move in Rd, that they want to do the above motions, but they don't want to overlap because they are spheres. So whenever they touch, they just bounce on each other. So this is also a model with a hard constraint. There's really a motion that is not allowed at some point. And this is a model that we have not studied yet. Are there questions or comments on this little tour of possible models? Models you're good. I don't hear complaints, so I assume everything is fine. So if everything is fine, let me start by explaining. So I want to explain now the intuition for this homogenization and for getting quantitative. And for getting quantitative rates on this, what I consider the simplest example: random hooks in random environments or elliptic equations with random coefficients. So I'm going to do it in the continuum. So you have to set this model, you have to first give yourself a random coefficient field that is going to describe how our random work. How our random work diffuses when it's at a given position. So it's a function from Rg to the space of matrices. If you don't like matrices, you can think of it as being a scalar and you just, you know, slight Boolean motion, but it can change its variance depending on where it's sitting. But I'm going to take it to be a matrix. And the associated differential operator is going to look like this. The divergence of A. The divergence of A of X, the gradient of whichever function you're calculating. This is the operator that is associated with dynamics I want to study. The important part is I see the important part is that this is in divergence form. And so that makes it so that the Lebesgue measure makes this operator symmetric. If you want to think in terms of stochastic processes, the Lebesgue measure is a reversible measure for this. Reversible measure for this dynamics. So, if you run the dynamics in a finand box, the flat measure is going to be invariant for this dynamics. Okay, and more importantly, you really have reversibility. So you can do integration by parts if you want to be, let's say, on the PD side. So, what do we have on this coefficient field A? So, it's a random coefficient field, but I want to assume. Random coefficient field, but I want to assume that it's the way we pick it at random is invariant on the translations. So let me call this a stationary random field. I hope you can interrupt me if this doesn't make sense, but I hope it does. And because I want to get rates of convergence, I'm going to also assume it has a good mixing properties. If I look at the coefficients. At the coefficients in this region of the space, and then in that region of the space, if the two regions are, let's say, a distance at least one from one another, then the coefficients I see in the two regions are independent variables. So this I'm going to call finite range dependence. Okay, you can weaken this assumption, but you need to assume something, otherwise it's not possible to get the rate of. It's not possible to get the rate of convergence, even for, you know, if you want to do a law of large numbers, if you have no missing assumption, it's not possible to get a rate. And as I said, I also want uniform ellipticity. Okay, this is my basic assumption, uniformly elliptic. This means here that these coefficients, they are They are uniformly bounded away from zero and infinity by some constant, which here I call the capital lambda. So, if you think in terms of probability, this defines for you a diffusion in this, for each choice of the coefficients A, it defines for you a diffusion in these coefficients. And the central limit theorem is that if you look at this diffusion in random environment from a sufficiently Random environment from sufficiently far away, then essentially what you see is a Boolean motion trajectory. Okay, so this is homogenization from the point of view of probability. I'm going to state something similar from the point of view of partial differential equations, which is about this operator that is here in the corner. So, how can we say it? Well, there are different ways, but let's say we are. Ways, but let's say we are trying to solve a problem like this in some domain capital U, and think of capital U as being a very large domain. So the coefficients are random, they vary, but they vary on a very small scale compared with the big domain capital U. And I solve this with some boundary condition. Okay, I give myself a boundary condition G and I solve for this problem where A here depends on X. Where a here it depends on x, okay? It depends on the position I'm already, but this is a of x. Maybe I'll do it already. And then I want to compare it with something which has a constant coefficient field. So now here in this equation, A bar is constant. U, u bar was on the boundary. And so the claim is that if the set U is very large, then U is very large, then indeed you will have that. You can find some constant matrix A bar such that U and U bar are going to be close, let's say in L2 sense. So let's say if you large, then U minus U bar is small. For a bar, let's say in L to the L. For a bar, let's say in L2 node, for a bar, a constant matrix. So, when I say constant, I mean it does not depend on space, but also it is no longer random. Okay, it's deterministic. And also, it does not depend on the domain, it does not depend on this boundary condition G. In fact, you can think of it as. In fact, you can think of it as one half the covariance of the Bohr motion that you see when you scale things out for the diffusion. Okay. So one thing to notice here is that A bar is not the average of the coefficient field A of X, because that's the difficulty. That's why it's called homogenization. Otherwise, maybe we should call it averaging rather. It's averaging rather. But here it's not just you take the a of x and you average it. So, in particular, earlier I told you if you like, you can think of this coefficient a to just take scalar values, like multiples of the identity, if you like. But even if you do this, it's not necessarily going to be the case that the homogenized matrix is going to be a multiple of the identity. So, after you homogenize, in any case, You know, after you homogenize, in any case, a bar will may end up being a matrix, even though each of x might be scalar value. Okay, so I don't have too much time to explain why a bar is not the average of a, but this can be explained without too much difficulty. So, this is well known, has been known since the 80s, or maybe even a bit earlier. Um, that you know, but this is an asymptotic result, it's like in the limit of extremely large scales, u and u bar get close to one another, and we want to get rates of convergence for this. And the first step I want to make before I try to get rates of convergence is to simplify the problem. So, here there's a domain, there's a boundary condition, they are arbitrary, or maybe they are smooth, but they are very general. I want to simplify this away. And I claim that it's possible. And I claim that it's possible if you understand this sort of behavior on simple domains, then you can do a sort of patching together of these solutions and reconstruct the full result on general domains with general boundary conditions. So it suffices to understand the problem when u is, let's say, a box and the boundary condition is just an affine function. And then you can sort of do a Taylor expansion around Expansion around marginal functions, and it kind of works. Okay, so I won't simplify the problem and just look at this instead. I'm going to write the domain like a cube like this because I think of it as being a cube literally now. It's just a big cube. And at the boundary, let's put an affine boundary condition, p.x. condition p.x where p is some vector in order okay and I want to understand the the behavior of this object as the cube becomes very large where p is fixed so okay notice that if a was constant then the affine function would be a solution so the homogenized solution in this case is just p dot x okay so I'm So I'm hoping that u of x, in the video I'm going to write like this, I'm hoping that u of x basically is like p dot x up to some error. I want to say that this u of x looks like p dot x plus something that is not to be. Okay, so the goal is to show that phi p of x is a small part of this affine part. Okay, maybe. Okay, maybe p dot x is not the best, but maybe I should say the length scale of the cube, you know, the cube to the power of one over t. Okay, this is what I would like to say. I'd like to say it's a very small comparison, not just asymptotically, but in some precise way. So I'm going to rewrite the equation above and instead write it as the equation for this correct correlation. For this correct correction part, it's called the corrector of this guy, this fine, because it corrects is the correction you have to add to the affine function in order to get the true solution with the varying coefficients. Okay, so let me write the equation for phi p. It's going to look like this. A, so when I take the gradient, you know, I substitute, I take u, now I write it like this, and I substitute in this equation. substitute in this equation so t plus quite five equals zero and then the boundary condition absurd the boundary condition is zero okay so now I face this and I'm going to go ahead and Okay, so now I face this and I'm trying to find ways to say that phi p is actually small. Now, why is this not an easy problem? So recall that our assumption, the ingredient that should help us to say something about cancellation, like that there is homogeneation happening, is that the coefficient field A of X has a finite range of dependence. Now, if I was studying If I was studying a local function of a over x that I sum in a big region of space, I would have a central limit theorem. It would be simple. But here, phi p is not a simple function of the coefficients of a of x. It depends in a very complicated way. Even if I want to compute phi at the origin, let's say, I may have to think carefully about what A of X is doing very far. A of x is doing very far from the origin. And this is why the problem is difficult to address if we directly try to focus on this quantity phi p. Okay, in fact, after a lot of work, you can show that phi p essentially resembles a Gaussian free field. So it really has long-range correlations. And this is difficult to study. So instead, we're going to take a detour and study another quantity which behaves much better, which is much more. Behaves much better, which is much more local in the way it depends on the coefficients. And so then we're going to be able to exploit the independence of the coefficients. And this quantity is related to, in fact, even how you solve for this problem. If I show you, if I ask you to prove well-posedness of the equation that is on display, well, I don't know how you would do it, but personally, the way I would do it is to say, let's find, let's Let's define this through a minimization problem. There is an energy function, and if we minimize it, the minimizer is the solution to this thing. So let me write this. So a new of QP. I mean, this is very classical. I'm sorry that I spent time explaining this. So, if you look at this minimization problem, the minimiser is the solution to the equation of Isabel. This is very well understood, but the claim which is not trivial is that I claim this could be much better to study for us. Be much better to study for us because the way it's going to depend on the coefficients is going to be nicer. And one first glimpse at the fact that this is nicer, more nicely behaved, is that you can see right away that this quantity is subadditive as I change the domain. So let's go over this argument. Imagine I take my big cube and I split it into subcubes. Maybe I split it like this. Maybe I split it like this. What I can do is I can look at this energy problem, this mediation problem, on each of the small cubes. And it finds for me a minimizer in each of the small cubes. And these minimizers have zero boundary condition. Okay, so all on these boundaries, here, here, here, everywhere, they are all zero. Everywhere, they are all zero, so I can glue them together, they match, and when I glue them together, I get a valid h1 function in the big queue. Okay, and it still has zero boundary condition on the outer box. In fact, it's even zero on the full grid, including the outer box. And so, it's a valid candidate for the minimization problem in the bigger box. And so, in particular, the quantity nu of the big box is always. Of the big box is always smaller than the average of the new in the small boxes. Does that make sense? And this is what I call nu is subadditive. Okay, and just like this, just because it's subadditive, you already know that it's going to converge to something as the cube is sent to RD, just by applying the subadditive algorithm. The suballitative ergodic theorem. So I'm going to write this, but before I write this, I want to make also a simple observation. What I want to observe is that when you look at this equation that is on top, you see that it's linear in P. I'm not going to explain this. It's clear that phi P depends linearly on P. And so when you write the energy function, it's going to be essentially a square. Essentially a square of this phi p or p plus Ï† p and so it's going to be a quadratic form. So because it's a quadratic form, I can always represent it in the following way. You can write it one half of p dot some matrix. I'm going to call it a of the cube times p. And then by Lagoudic theorem, by the sub-sethouse. By the subbasive Lagoon theorem, we know that it converges as the cube is sent to infinity, and the limit I'm going to call p dot a bar p. Okay, so the limit is by definition p dot a bar p and this is your definition of a bar. Okay, so before I had not given you a definition of a bar, I told you the exists of a bar, now this is a definition. Okay. Okay. All right, so we applied the suballitative ergody theorem. It's nice. But I claim that, okay, so I'm not sure what to say first. Perhaps the first claim I want to make is that ultimately we can show that, in fact, this sub additivity becomes really essentially sharp as the cube becomes very large. Becomes very large, and so you see that essentially is a sum of these nu's in the smallest scale, and this is what I mean by it depends more nicely on the coefficients. You can see it as essentially a sum of local functions of the coefficient field, at least over a large, sufficiently large scale. This is very good. Yeah, perhaps one thing I want to say also is: so, in the end of the day, maybe what we really But in the end of the day, maybe what we really want to say is that the character of this phi p is small. So, how do we go from information about new to information about phi? Well, imagine I tell you that in this suballitability argument that we have just done, in fact, you make essentially no error. Essentially, when you glue the minimizers in the small scales together, essentially, what you compute and you What you compute, and you compute the quantities, essentially you get the same as the quantity on the big box. If you know that this is true, then in fact, because this is a uniformly convex minimation problem, you know that the glue together small-scale minimizers must be close to the minimizer in the big box. Okay? But then, if the minimizer in the big box is close to these small scale minimizers glued together, it's not possible that the large guy is large because already you see it takes value zero essentially on this subgrid. So I'm not doing the argument very properly, but it's not difficult to show that if you know that the true minimizer is close to the glued together. Glued together small-scale minimizers, then the true minimizer is actually very small. Okay, so what I try to argue here is that if in this convergence here you have a good rate of convergence, then you can deduce that phi must be not too large. Okay, so it's just I want to shift emphasis and replace our goal of showing this, oops, sorry, of showing this. Sorry, of showing this with the new goal of getting a rate of convergence for this. Okay, I claim that if you get a rate of convergence for this, you can conclude about the size of the correct. Okay, so far we have applied the subadditive ergodic theorem, and this does not give you any rate of convergence. So we have to have some idea, like some new ingredients in order to get a rate of convergence. In order to get a rate of convergence, and it's difficult a priori to compare to understand the rate of convergence because our only definition of a bar is as the limit of this thing. And so it's very hard to, you know, when you, if I just give you a big cube and I tell you even what is a bar, you cannot know it for sure. There is uncertainty. You cannot, you know, it's not a local quantity. You cannot, you know, it's not a local quantity, so it's very hard to argue about this. And so, instead, we're going to introduce another finite volume quantity, which is going to be approaching a bar from the other side. Now, here we have a sub-additive sequence, so it's decreasing to a bar. And I want to find another thing which is going to increase to a bar and also define infinite volume so that I can sum two this. So, that I can sum two these two things, I can play with them. So, let me write this. So, the other quantity I want to introduce is like this. Nu star of UQ is the supremum over U in H1 of one over the volume of the cube, integral of minus one half. gradu dot a gradu plus q dot graduate okay so again this is for q in already i define this this is uh i mean i guess most of you know this but if you set up this problem the so here i wrote a supremum so you maximize this if you if you maximize this thing the maximum Maximize this thing, the maximizer is the solution to the equation where you put a Neumann boundary condition. Okay, and like you put a flux at the boundary. Before we were putting an affine boundary condition, now you put a flux Q at the boundary of the box, and then it's harmonic inside the domain. Why is this quantity interesting? Well, perhaps. Um, well, perhaps two observations. First, I claim it's also subaditive, but for a reason which is kind of opposite to the previous one. In this case, notice that the definition here is with H1, not H10. So I can do something which is a bit like the other way around from what I did before. If I look at the maximizer in the big box, then in particular, the restriction on the Then, in particular, the restriction on the small boxes, when I restrict an H1 function, of course, I get an H1 function in the restriction. So I get a candidate for the small boxes. Okay, so the values in the small boxes must be larger than what I get when I just take the restriction on the big I. Okay, and so again, this is subadditive. Okay, I made the argument which is kind of opposite, but also the virational problem there is a maximization problem. That's why I still get sub additivities. Like I have two minus signs in something. So JC? Yes. So in your new star, is the capsule U also supposed to be the box to be the Be the ah, no, no, no, you is the function. Ah, so you had the box. Yeah, you're right. I'm sorry. Yeah, thanks a lot. Did I make the mistake also, Bob? No, I did it right. Yeah, yeah. Yeah, thanks a lot. Other comments or mistakes that you would like to point out? All right, so far is good. What I can do is is take the if i take the optimizer for for this problem in fact i'm thinking of the the whole thing like p.x plus 5 p of x and if i plug it as a candidate you know it's an h1 function i can plug it inside here what i'm going to get as the first time is the energy so it's the is our nu of p and then this part um so i get q dot p for the linear part and then the h1 linear part and then the h10 part has a zero average gradient so i immediately get that nu star of the cube and q is greater than p dot q minus nu of the cube and p. Okay, and this is valid for every p and every q. So, in particular, if you like, you can take the supremum over p on the right-hand side and the supremum over Hand side and the supreme over p is the convex dual of the function loom. So here I'm saying that nu star is always above the convex dual of the function loop. And in fact, if we believe again that this corrector ultimately is going to be small, that ultimately over large scales, these optimizers want to look like affine functions, then essentially, you know, so if you suppose, if let's say. Let's say u optimal, the maximizer in u star is approximately like p dot x. If you imagine it's a bit like an affine function plus some coefficient for some p. Then essentially, the argument we just did to get an inequality is going to be essentially sharp. You have to fix a little bit the boundary, but you will not get. But you will not make a lot of error by doing so, and this is going to be essentially short. Then, this, instead of the inequality like this, you're going to get approximate equality. Okay, so over a large scales, we should expect that nu star wants to look like the convex dual of the function nu. And so, in particular, you know, in the limit, the function nu is like this. It's this quadratic form. So, in the limit, nu star is going to. So in the limit, new star is going to want to look like one half to a new star of 2 over each is going to converge to one half. The convex dual of the quadratic form is a bar inverse q. Okay, so if I, and again, it's a quadrature form, so I can define it as one half q dot, and I'm going to call it a star cube. call it a star cubed inverse now he wants to look in the limit he wants to look like a bar inverse so before i pass the limit the matrix here i'm going to call it a star of the cube inverse okay so it's subadditive it's is it's approaching from above its limit but now in terms of a star a star is going to approach approach a bar from below then i take the inverse From below, then I think the inverse. So you should think of the situation as being like that. Let's say this is a drawing where I put, this is the value of a bar. And on this scale, this is the length scale, the size of the cube. You increase the size of the cube. And you have A of the cube, which approaches A bar from above. Which approaches A bar from above, and you have A star, which is going to approach it from below. Okay, because the inverse approaches from above. And so instead of comparing A with A bar, instead of trying to measure this distance as you increase the scales, we're going to try to control this distance. It's an upper bound. But this is much nicer because if I show you the If I show you a finite box, a finite piece of the coefficients, you can compute both quantities. And I want to try to find a way to say that if some nice things happen in this box, then they must be close. Okay, and this is how we're going to get a rate of convergence. So, to be more specific, what we're going to try to show is to control this difference. Okay, so perhaps yet another way to think Okay, so perhaps yet another way to think about these matrices is if I give you a just a finite box where you can see the coefficients, and I ask you what is a bar, you cannot know for sure because you only see this finite box, but you can give me a sort of confidence interval. You can give me upper and lower bounds for where A bar should be. And the upper and lower bounds are these matrices, A star and A. They give you this bracketing. Okay, they give you this bracketing of what the true a-bar is, and to get the rate of convergence, we try to estimate this distance in terms of the flatness of the function I'm drawing. So if you imagine you, let's say I'm indexing the, let's say the cube of size n is 2 to the minus 2 to the n, 2 to the n to some power. Okay, let's let's. Okay, let's say we take it like this, or maybe in the paper we take three instead of two, but it's the same. So you want to try to control this by essentially whether these things do not vary too much between different scales. Maybe it's the same thing with the A star. Okay, because when you compare between two scales, you know, I try to gesture at this argument a little bit. If you know that when you compare, it's like when I was saying if the suballiativity is almost correct, then you know the corrector is small. You can learn things if you know that when you. When you go from one scale to the next, the coefficients have not changed very much. You know, this A of the cubes don't change very much. It tells you that when you glue the minimizers together or when you look at the big scale minimizer, they are almost the same. So it's not crazy to believe that something like this is possible. I mean, I don't fully explain, but I gave you some of the ideas. So in truth, we don't exactly prove the inequality. Exactly, prove the inequality I wrote down, but this is the idea. Maybe we're going to have more scales involved, maybe, okay, you know, it's a little bit more complicated, but this is really the gist of the idea. And if this is possible, then as a consequence, you get an algebraic rate of convergence. And the exponent depends on the constant C, which at this stage you don't really understand very well. So then, as a consequence of all this, you get, let's say, a theorem. You get, let's say, a theorem, which is that A of the cube minus A bar, and also you can compare the other one if you like. Oops. That all these things, they are smaller than some negative power of the size of the cube. Of a cube for some alpha positive. Okay, and this is how we get a first rate of convergence for these quantities. And as I tried to explain, once you know this, you can infer the smallness of the characters with some algebraic rates, and then you can get quantitative homogenization running. Okay, so at this stage, the exponent is not explicit. You just have a tiny positive exponent. A tiny positive exponent, but once you know that this is true, you can use it to prove good properties of the solutions. You can show regularity properties of solutions, which I'm not going to explain. And once you also have this at your disposal, you can revisit this sort of argument and do a sort of iterative argument that progressively improves the exponent until you reach the correct sharp exponent. Okay, so this is also very important. So, this is also very involved, and we made it work for elliptic equations, but this we have not made it work yet for particular systems. For particular systems, the analog result that we have is the one that is on display here. We have a small rate of convergence, a small positive exponent. Okay, so I spent a bit more time than I was anticipating, but now it's time to move to Pascal systems. Maybe it was time earlier, but okay. If there are questions, I'm still happy to take. We'll try to speak about particles a little bit. Okay, so what is the model in this case? So you have to assess how much I say. So you have to imagine that you have a cloud of particles. Maybe I'm not going to define it very well. Let's see if I can do it a bit informally. So you have to imagine that you give yourself a cloud of particles. Maybe you start from them being distributed according to a Poisson point process on E, maybe with a constant density. Maybe with a constant density, you want to study this, and each particle wants to do a Bohr motion, like a Laplacian of something, but it's going to be, you know, each particle wants to do a sort of divergence of a of x quad a bit like before, but instead of a of x is going to be something which is just a function of the configuration around it. Yeah, so maybe I'm going to set some notations. So let's So let's say maybe initially you can think of these particles are being indexed by some index set, I, and then T is my type. And I don't really want to record the exact identity of each of the particles, so instead I prefer to keep track of the cloud itself, which we can encode as a measure. I look at the sum of the dirac. Look at the sum of the Diracs. Okay, so if you look at our papers, you'll see that we never really use this sort of notation, we only use this measure notation because then you really see that the particles are not distinguishable and they are really just a cloud of things. And when the particle is sitting at X, you have a diffusion matrix for this particle. For this particle at x. And so it has to be a function of the cloud of particles and the function of x. Okay, and then my operator is going to be, you know, for each particle, there will be a divergence of a of mu, x grad of the function. And this thing has to satisfy. So this is a fixed function. For instance, For instance, let's say I could take a of mu x to be one plus the indicator function. That's maybe I don't want to have too many particles run, so I'm going to say and then this multiplied by the identity, let's say. Okay, so if I take the choice that is written here, what I'm doing is I'm saying each particle is doing a diffusion with diffusion matrix two times the identity. If there are no more than three particles around me, or maybe two because I count two plus me around me, but if there are more than this. Me. But if there are more than this number of particles around me, then I just do a diffusion with one time the identity. Okay, so in general, the assumptions I want to be in place for this thing is it has a sort of translation invariance. If I move the cloud of particles mu and I move the X together, then A or mu and X changes accordingly, like in the example, for instance. I want that it's a I want that it's a finite range of dependence condition. When you look at A of mu x, you look at a neighborhood of x. You look at mu in the neighborhood of x, but not a crazy form. And I wanted it uniformly elliptic. I don't want it to be close to zero or close to plus infinity. And so my time is running short. So just to wrap up, under this assumption that I just said, Assumption that I just said, basically, the same program that I explained to you for elliptic equations can be enforced. Okay, we can find analogs of nu of cube and p and nu star of the cube and cube. And so define similar things, A of the cube, A star of the cube, and show, you know, and get the sort of same, by the same sort of argument. The sort of same by the same sort of argument, obtain that these things converge to a bar with some rate of convergence. Okay, and the a bar is the coefficient that appears when you show the hydrodynamic limit for the particle system. Okay, so I'm a bit rushing, but maybe just one word on the difficulties. The difficulties are that, well, first it was not very clear to us how to even define the coins, these coin d's that are here. You have to, okay, you have to. You have to okay, you have to think what it means. But in general, for instance, what does it mean to be a function in h1 zero? You don't want to say that whenever there's a particle near the boundary of your box, you set your function to be zero or be good because there are so many particles that basically there's always a particle that is almost at the boundary. So if you do that, it's nonsensical. We have to do something else. And in general, I think this also is going to feature in the talk. This also is going to feature in the talk of Chen Lin. He called it the curse of dimension, I think, in his abstract. This is something we have to fight against, and it creates special difficulties, but we found ways around it and we managed to get a small rate of convergence. So sorry for this tightly over time and for rushing a bit too much on particles, but I hope you could get something out of it. Thank you. This is my talk. I'm happy to take questions. 