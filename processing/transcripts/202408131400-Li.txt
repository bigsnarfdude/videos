Happy to have Shuang. She's going to tell us about some discrepancy algorithms with my next presenter. Hi, everybody, and hi, everybody, on Zoom. So, my name is Shuang Pyung-li. I'm a stein founder in the Department of Statistics at Stanford. I'm on drug market this year, so if you know any opportunities, please let me know. So, this is a work on the binary free sectoral model with wonderful collaborators Suliu Shram and Khan Die Trou. It's a fairly recent, so it's uh It's a fairly recent, so it appears in archive in August, but it's actually submitted a while before. Okay, so it's very nice because there are many sort of relevant talks in this workshop about binary perceptron. So we see yesterday, you know, Dylan's lightning talk, which is very relevant. And then towards the end of Mark's talks, there's also, you know, he's also mentioning the perceptual model, even if he didn't defy it, so I'm going to define it today. Identify it, so I'm going to define it today. And on Friday, there's going to be Bryce work, which is also, you know, Bryce talk, which is also very relevant. So make sure to stay all the way until Friday to hear about Bryce's work on the percept level. Okay, so let's start it. So I'm not tall enough to reach the top of the slide, so I have plenty of it. Okay, so what is the binary perception problem? The model is very simple to define. Very simple to define. You start with a m by n matrix x, where each entry is just the id normal 01, for example, the one you see here. And then you look for a vector SETA, which has entries plus minus 1 to the power n, so a lens n vector where each entry is plus minus 1, such that the following set of inequalities holds. So you want the matrix times this vector zeta normalized by 1 over root n, which is the typical size. Which is the typical size of the entry, is larger than or equal to kappa for any fixed kappa rib. Okay, so this is the picture, your matrix, and you want it times the zeta normalized by one normal square root of the bid. This is a vector of this m, and you want each entry of it to be larger than when you go to kappa. And for a matrix x, we define the set S of X, we name it the solution set, to be the set of all. Get the solution set to be the set of all zeta such that it satisfies this set of inequalities. Okay, so you know what are the questions that people would be interested in? Okay, so this is called the binary perceptron, but sometimes people also call it interchangeably with the easing perceptron because you are having the easing spins here. Okay, so I forgot to change the title. But so, what are some questions that are relevant? So, first the question is a satisfiability. The first question is a satisfiability threshold, in which people ask for which values of M and N are there solutions. In particular, people are interested in the regime when M and N goes to infinity with a fixed ratio alpha. And you ask the question that for what value of alpha are there solutions with high probability. It's quite natural that the higher the value of the alpha, the more constraints you have. Therefore, it's harder to solve the problem. So in other words, you are asking what is the largest value of alpha. What is the largest value of alpha such that there are still solutions which have both? So, ideally, you would like to say something like this. You know, a priori, you don't know transitions like these exist, but we hope something like this exists. So, there is some alpha set set for satisfiability, such that if your alpha is above alpha set, then there is no solution with hyperbolability, and if your alpha is below alpha set, then there is a solution with hyperbolability. And if such Is hyperlogic. And if such thing exists, then this transition, this R Z, is often referred to as sort of where the transition happens, where it's also called the limiting capacity. Sometimes you see in literature people write R C and C for C for capacity. Okay, a priori, you don't really know whether you have phenomenons like this, but it will be clear later on that it is indeed the case. And we are actually going to be more interested in the related algorithmic question in the Related algorithmic question in this talk: that when are there efficient algorithms? This is just the existence of solutions, and this question asks whether there are efficient polynomial time algorithms that can find the theta such that it satisfies all the inequalities. Okay, right, we're going to focus on this question in our talk, and the spoiler alert is something interesting is gonna happen. Okay, so before I go. Before I go to my results, there are a very interesting related model called the symmetric binary perceptron that I would like to mention and it's also related to Bryce's talk on Friday. So the same matrix, you just copy things, and the same space that you're looking for, but it's just a set of constraints are symmetrized. Here we want to ask the matrix times the vector zeta absolute value normalized by square root of the. Value normalized by square root of n to be less than or equal to kappa. So, in other words, you want the matrix times the vector up taking absolute value. Okay, so this should be capital N, without the vector to be less than or equal to kappa. So, in other words, you want x times theta to line in the interval minus kappa to kappa. It's a symmetric interval, so this is a symmetric constraint. That's why it's about the symmetric value, etc. And so, same set of questions could also be asked. Questions could also be asked for the symmetric binary perceptron, right? So, when are there solutions with high probability and when are there efficient helps? And the reason that people consider this symmetric binary perceptual model is that actually similar phenomenon qualitatively are expected to hold for the two models. What do I mean by similar phenomenon? So, there are some phenomenon that we're going to see later. For example, the sharp threshold results were some. Threshold results were some strong freezing properties. So, if you look at how the solution space looks like quantitatively, it could be the same for the two models. But there's also symmetry is nice in the sense that it makes this model more tractable compared to the original binary perceptual model. Therefore, the hope is to sort of understand more on the symmetry case and then shed light on what actually happens for the binary perceptral. The binary perceptor. So that's like the original motivation for considering such symmetric binary perceptual problems. Okay, so now I will talk a while about literatures on what have been done for this perceptual model. So for the original perceptual model, there's of course a problem about satisfiability. So all of the computation starts with Cross and Moudart in 89, where they had this, this is a statistical. Where they had this, this is a statistical mechanics conference, right? So they have some statistical physics type of computation. So they did their fancy physics work and write out that they think the transition should be at a specific value alpha star, whereas alpha star is a fixed point of some complicated equations that you can solve numerically. And in particular, when kappa equals to zero, alpha star should be around 0.833, which is a number that's below. Which is a number that's below 1. It's not triggering to show it's strictly below 1, but you need to do something. But R star should be around 0.823. And then I actually skipped some literature about Tyler Grant and other people, but there's this very nice paper by attending at Nike, Nike Zoo in 2019, where they showed a nice lower bound. They showed that if alpha is below alpha star, then there exists a solution with positive probability, with probability larger than one. Probability with probability larger than or equal to delta. It's not yet a high probability result, but it's still very nice because it matches all the way to what the physics prediction was. And then later on, Chang Ji Xu and Naka Jima and Sun had a sharp threshold sequence result, which allows you to boost this positive probability result to the high probability result. So combining this, you're able to say that if alpha is below alpha star, then there exists a solution with high probability. Then there exists a solution with high probability. And then earlier this year, Bryce Wong had a very nice paper that had a matching algorithm. So if alpha is larger than alpha star, then there exists no solution with high probability. So I emphasize there's many probabilities with the same last name. So it's nice to spell out the full name. This is the Bright Spawn. And so D1, right? So this is. And so, like, this is the phase picture, phase diagram for what happens when kappa equals zero. Above, around 0.803, there's no solution with hypermobility, and below this, there should be a solution with hyperbolic. So, I put this little sort of star around the result to mean that there's actually more complications in the result in the sense that you need to verify certain family of special functions are below zero in certain. Below zero in certain regions. Which, you know, if you believe that's indeed the case, which I do, then these are humble results. But I mean, so if you are very picky, you might say, ah, you didn't verify these and these conditions. But these are just special functions that it wants to verify that is below certain things. Okay, so this is satisfiability. What about some algorithmic questions? So Tim and Rush in 1998. Tim and Rush in 1998 they had this nice multi-score majority algorithm, which are proven to be true for alpha less than 0.005 when kappa equals to 0. So surprisingly, there aren't many available, mathematically rigorous algorithms available for this perceptual model, but there are these nice heuristic arguments that actually achieve quite high capacity. Quite high capacity. So there are ones that sort of go between 0.89 and 0.75 each. And they're based on belief propagation and some very stereo temperature per belief propagation. But these aren't, they are physicists and computer scientists and they don't prove that this works. They just run the simulation and they actually find solutions with hyperbolic. Okay, so and then there's another paper by Baldessian Kohl. By Baldaci and co-authors, you know, they are also sort of doing physics-type computations, and had this concept of some vocal entropy computation and tries to describe what are the different phase diagrams for what is going on for the protectoral model. And in particular, one of the transition is this value alpha u, which numerically roughly equals to 0.77, which they say signals a change in the structure of the space of the solution. In the structure of the space of the solution in some complicated way. But they sort of indicate that this might have been the algorithmic threshold for the perceptible model. So sort of here is the picture. 0.833 is the satisfiable threshold. And the actual known algorithm, it only works up to 0.005 here, which is marked as easy. And the layers from this 0.005 to 0.75 each are easy with a Each are easy with a question mark, meaning that they should have been easy, but we don't know how to prove them. And there's these three question marks, which means that we really don't know what is going on, probably some hardness today. So looking at the diagram, there are two messages that we make out of it. First, there's really a big difference between 0.005 and this. So there's a lack of efficient algorithm with rigorous performance guarantee and works at red. Confirmance guarantee and works at relative high density of alpha. So, meaning that's how to sort of close the gap for this easy question mark place. And then there's the second is what actually happens at this three question mark. Maybe there's some hardness. Maybe there's a statistical computation gap that you can say from there. So, and our results are sort of partially answering these questions. So, proposing an efficient algorithm that sort of pushes this 0.0. That sort of pushes the 0.005 up and also has some hardness results down. Okay, so that's like the very vague overview from what we have been doing. Okay, and then I have a slide for the results for the symmetric binary perceptron. I just want to say that much more is understood for the symmetric binary perceptron. For example, people know the limiting capacity. You know, people in the audience, people in the audience like Dylan has some nice work on the critical window. There are just a On the critical window, there are just many more structures that could be said for the symmetric final set form. Okay, and I won't really say why, but it's just that symmetry makes the model more attractive. Okay, so before I go to our algorithm and our results, I want to share a very interesting phenomenon that's going on in the perceptual model, which is the strong freezing phenomenon. I need to go through this step by step. So the main message is this perceptual model has a strong freezing property for any alpha between zero and alpha set. What does it mean by strong freezing? It says that typical solutions are isolated and they're actually theta n, so order n you're having distance away from each other. What do I mean? So you look at your solution set and you pick a solution uniform at random and with high probability. form at random and with high probability the solution is isolated meaning that all of its neighbors are not solutions and you look for the closest the neighbor is actually very very far away it's ordered n away from this point so this point is like genially truly isolated it's just like it's not genially separated so this might be a picture for what is going on so if you plot all of your solutions you know most of them are isolated Of them are isolated and they're of order C to n away from each other, very scattered in the space. And this sort of strong freezing phenomenon is conjectured to be the case for the original model and proven to be the case for the symmetric model. And this is again a phase diagram for any alpha above alpha set, no solution. But as soon as you are below this, you have solution, and at the same time, you have strong freedom. So this is a different picture for. So this is a different picture for what Mark have shown in this lecture yesterday, because for a lot of the random constraint satisfaction problem, when alpha is around alpha set, you sort of see these phenomenons going on. But when alpha gets lower and lower, it means that you have less constraints and more solutions. There are periods here where you see solutions are well connected. But this is not the case for what is going on for the perceptron model. So it's quite difficult. It's quite different from the phase diagram for what is going on for other random constraint satisfaction problems. Okay, but then I have this slide that says, so typically, when you see strong freedom phenomenons like these, you would expect that this would imply algorithmic hardness, because solutions are really scattered and far away. How are algorithms even able to find, even able to? Not able to find them. So, this is like, sounds like a very difficult task. But if I pause here for a second and let you think just for a while, then you actually realize that a few slides ago, I said that there are these, you know, even if this alpha is small, there are these algorithms that are proven to be true for the binary perceptual model. So, how do we explain that even if you see the strong freedom? That even if you see the strong freedom phenomenon, there are still efficient algorithms that find solutions in the space. And I sort of tell you the answer because this typical is in red. So this lies in, and the reason is because the statement is only for typical solutions. So in the sense, it is true that 99.99% of the solutions are isolated. But there are these, if you filter all of these away, there are these rare, but what Away, they are these rare but well-connected clusters. And algorithms actually succeed by finding these, right? And this is like an intuition where an explanation given in this physics-ish paper by Baldaci and Causes, where they answer, were sort of explain this seemingly contradictory fact by saying that this is indeed the case, that efficient algorithms succeed by finding exponentially rare clusters of large size. Of large size. Which also means that in order to study the algorithmic tractability of the problem, you couldn't really just look at strong freezing because it doesn't tell you much. Looking at typical solutions doesn't tell you much. You should be finding a different way to study such problems. So one way out of this, a popular way, is the overlap gap property, which through some work will typically imply failures of the stable algorithm. The stable algorithm. So, for example, we will and closers have looked at the barriers in the symmetric model, in the symmetric binary perceptron, through the multi-overlap gap property, which Mark have sort of talked about this yesterday. So, this could be a good alternative way to study the hardness of the model. And so, what we did in our work is that we proposed an algorithm for the model when alpha is small but not too small. But not too small. And the algorithm is actually inspired by discrepancy minimization algorithms, which I will explain in a second. And we show that the model has some multi-overlap gap property when alpha is large, which implies failures of certain stable algorithms. So sort of pushing things up this side and then pushing things down also on the other side. Okay, so what is discrepancy minimization? So you can start with, I mean, there's different definitions for sets, but for the matrix, if you start with an mf by n matrix x, then the discrepancy function of this x is just defined as the minimum over all the choices theta inside plus minus 1 to the n, and then the pseudo norm of the matrix times this theta. Why this is called discrepancy minimization? Because it's really like the discrepancy between the plus ones and the minus ones. And it wants to minimize the discrepancy. Minimize the discrepancy minimization. And a central problem in discrepancy theory is to compute, approximate, or bound this value. And in particular, there's this proportional regime in the discrepancy minimization, which means that you have a fixed aspect ratio m over n, exactly our setup. So the goal is usually to find a solution zeta of this very small x z with very small super With very small soup load. And if you think about this, this is exactly the numerous of the symmetric perceptual model. In the symmetric perceptual model, you are given a kappa, and it wants to find a theta such that essentially this is less than or equal to kappa. In this permitting minimization, you sort of look for a theta that's with a small, with a small, that's exactly the same problem if the matrix X is ID level. So in other words, anything that you can say about the symmetric You can say about the symmetric perceptual model, you can also sort of translate it to the result of the discrepancy minimization in the proportional random routine. And on the other hand, any discrepancy minimization algorithm should also be an algorithm for the symmetric binary perceptron. And then we thought, you know, there are algorithms for the symmetric binary perceptron. Why not we try it and see what happens if we apply it to the perceptron model? And that's sort of part of our Sort of part of our work. And here is our algorithm. So we use these very classical discrepancy minimization algorithms, like the Ross Was and Elden thing, and the Loan and Becker ones, which I will talk in detail what are these in the next slides. But intrinsically, our algorithm is the first step of linear programming, followed by T steps of iterative bounding. The linear programming step is really sort of the Roswell and Alden Singh sort of The Rosswas and Alden Singh sort of linear programming algorithm. And the iterative rounding is like the random Clovet Mecca algorithm. Okay, so what does the first step do? Recall that our goal, so to solve the binary perceptual model, you look for a CEDA with plus minus one spins such that it satisfies all the inequalities. So our first step guarantees part of the work. So we make sure that all of these inequalities are satisfied, but we don't make sure that But we don't make sure that all of the CDIs are plus or minus 1. But it's guaranteed to lie in this box minus 1, 1 to the power n, and with part of it, for some of the CDIs to be plus or minus 1. So you're like halfway towards your goal. And then the next iterative rounding of wet mecha, every time you round it, you sort of get still holds, still make sure that the inequalities hold, but you are getting more and more C that I is to be plus minus 1. And in the end, everybody. Plus, minus one. And in the end, everybody's plus minus one, then you find a solution. So that's sort of the rough idea for what the algorithm is doing. And compared to the previous algorithms, like the one, the only available one by Kim and Rush, this is simpler to analyze and also performs better both theoretically and numerically. Okay, so now I'll tell you exactly what is this median programming algorithm. Algorithm. It's very simple. So you start with this matrix X, essentially the only thing you know. And I want an output zeta hat. What do I do? I just first draw a vector V, just in the most natural way that you can think of, just following a normal zero identity, which is independent of your data matrix X. And then I want to do this linear programming. I maximize over the inner product of B and C naught. The inner product of B and C naught, subject to the constraints that all of my required in quantities hold. And this C naught also lies in the box minus 1, 1 to the N. So here is a picture. So the purple box is just like our minus 1, 1 to the n, n is 2. And the red box is a fully top sort of presenting the set of the inequalities. And then you just uniformly have an E. Uniformly happen V. And how to look for the zeta? It's just like I want a vector with the most overlap with this V, but also stays in the intersection of the box with the probito. And for example, this is the one for the specific choice of V. And you can see immediately why this could have been a very good algorithm to have, is because if you are on the boundary of this box, that already means that some of you means that some of you see the eyes are plus or minus ones. And the maximum sort of guarantees that you really want to be pushed to the boundary. And it happens that portions of this, you know, it could be pushed to the boundary on the polypole, but it could also be pushed to the boundary of the box. If it pushed to the boundary of the box, then you really liked it. Yes? Confused, are we the binary perceptron or symmetric binary perceptron? Just a binary perceptron, not symmetric. Right? So you hold it. Symmetric, right? So you pull it all symmetric. Yeah. Yeah, thanks for the question. Yeah. And compared to, right, so compared to what, so the original paper that sort of proposed this type of linear programming, the L DN thing, and the Rossbos paper, they focus mostly on the symmetric complex body. And their analysis, a lot of the times, needs to be symmetric. Needs to be symmetric. For example, they use the Gaussian correlation. It has to be some symmetric complex body. But for us, we are able to analyze the behavior of this progeneral complex bodies. And this largely relies on the Gordon's inequality, which I will talk about this later in the proof. So, okay, that's the first step. And then what happens for the next wrong? What happens for the next rounding step? This is this random work rounding from Lobetta Mecha. So what we do is we just start from the output on the previous round, start from the output of the linear programming. So you are guaranteed that you sort of start here, which is inside the intersection of this polytope and the box. And then what you do is you do a, I mean there's random walk, so you do random work. You just walk, walk, walk until you hit. You just walk, walk, walk until you hit some boundary of the some boundary, right? What do you mean that you hit the boundary? It means that either some of you c d i becomes plus minus one or some of your inequalities becomes tight, right? So after you walk to this, you can still sort of walk in this like hyperplane, such that these specific tight constraints aren't really changing. So you could still, but you just get a degree of freedom reduced. A degree of freedom reduced by one because you need to work sort of orthogonal to this, right? And then you continue. I mean, this is 2D. I should really draw a straight line, but I want to say it's a random walk, so it sort of zig-dagged a little bit. It actually should be a straight line all the way to there. So there's got to be some point where you sort of hit the boundary and you walk on this space, and then you hit another boundary and you walk further orthogonal to somebody, and then you walk. Orthogonal to somebody, and then you walk until you couldn't do it anymore because you don't have any degree of freedom left. But if that happens, then you sort of reduce your kappa a little bit, just to give it a little bit more room to play with. And then you do the edge work once again. It starts to do the random work. And when you hit boundary, you work orthogonal to the previous constraints, and you do such things. And you can sort of show that each time you iterate this F-walk. You iterate this F-walk, the number of CDAIs, so there will be more CDAIs, becomes plus and minus ones. And eventually, everybody, or almost everybody, becomes plus and minus one, and you have a final random rounding step to make everybody plus minus ones. And this is nice because it just always guarantees that the inequality that you want it to satisfy are always satisfied. And you also make sure that the more and more C the I's are getting plus and minus ones. Therefore, that's. And minus ones. Therefore, that's sort of the moral story of why this succeeds. So, you know, again, compared to Lovan and Mecca, so we refine the partial cuddle in Lemma because we want really detailed understanding for what is going on with things. So we sort of refine their estimation and get a better estimate of the performance of the IFL for specifically this case. Okay, so now let me. Okay, so now let me. I'm finally ready to present the results. So there are three different cases that we consider. First, it's the most classical case when kappa equals to zero. So our algorithm finds a solution. This is a proof of holding the case. At alpha is approximately 0.1. And compared to this is like 20 times the key rational we know, which is 0.005. But actually numerically, our algorithm could go to around 0.5. To around 0.5, but we just couldn't really show it because the random walk thing could go back and forth, and we were not able to make use of this fact. So that's why there's a gap between the actual performance of the algorithm and what we can prove. And for the Kampa goes to infinity case, when Kampa is very big, which means the constraints are very difficult to satisfy. So then the RFA. Satisfied. So then the alpha set needs to be very small because you can't really put many constraints, otherwise, there aren't many solutions. And it can be shown that alpha set is of order 2 over pi and some constant times of order 1 over kappa squared or kappa 2 minus 2, you know, up to some multiplicative factor. So and our algorithm finds a solution all the way up to up to this. So it finds a solution when alpha is 2 or 1 solution when alpha is 2 over pi kappa to the minus t. Up to a multiplicative 1 plus okay oh kappa of 1, which means that you sort of are having a very small gap even if there is gap for the kappa goes to positive infinity case. And more interesting things happens when kappa goes to negative infinity. So you can show that the alpha set in this situation is of order log 2 over. Is of order log 2 over phi of kappa, which again is a multiplicative factor. So, sort of one could also use a brice form, which gives exact formula for what alpha set is. But it's just right, you need to verify this numeric condition. But if you are good with this one plus small one, there's some second moment model you could do to get, to show that alpha set is indeed of this order. And our algorithm finds a solution when our. algorithm find the solution when alpha is of order one over phi of kappa phi is the CDF for one and kappa squared. So there's like a kappa squared multiplicative difference between what is the satisfiability place and where the solution, where there are efficient algorithms. And you know, to complement the results, we also have a MOGP computation which allows us to rule out stable algorithms and show that this is the case when alpha the case when alpha so there is hardness when alpha is of omega log squared kappa over phi kappa kappa squared so it's sort of tied up to a poly log factor up to this log squared factor right which you know brass on friday might tell you more more about how to how to do this log squared uh how to work with the log squared thing how to get rid of some of the logs um okay great and and then i also thought a few Right, and and then I also pulled a few pictures, just putting the words in the picture for the kappa to positive infinity case, the onset and the easy all sort of happens in the 2 over pi times 1 over kappa squared case. And so, you know, it's not clear, you know, there should be just a little bit hardness, but even if there is hardness space, it's gonna be very small. It's gonna be very small compared to the actual order of the transition, where the transition happens. Transition happens. But for the negative case, there's a big gap. The unset is roughly of order log 2 over phi of kappa. So I should really put a big anomina there, but the parenthesis will overlap a bit. So I just delete them. So it should all actually be on order. But so, yeah, there are some, it should be easy below 1 over 5, kappa squared. Should be hard above low. squared should be hard above log squared over phi kappa kappa squared and there's this big hard face where you know stable algorithms aren't going to find solutions and there's a again a kappa squared multiplicative factor away from each other. And I don't have a very good intuition for why you know kappa goes to negative infinity has such a big gap, but kappa goes to positive infinity have a very narrow gap. But you know potential Narrowed up. But you know, potentially, one explanation is: so the perceptron has this spherical version, where instead of looking for c that has plus minus one spins, you look for c that that's on the sphere. And for the spherical model, you know that when kappa is positive, this is a convex problem. So there is no hardness phase. If there's a solution, there is diffusion algorithms. But then when kappa is negative for the spherical situation, it's just more complicated what's going on. More complicated, what's going on, there might be harmless. So, I mean, even if this is not on the sphere and it's discrete, there's no way to talk about convexity, still, it might be the case that when kappa goes to positive infinity, the problem maybe starts to look convex-ish. So then the hardness space or the hardness transition is going to be very small compared to what happens when kappa goes to negative infinity. Okay, so then I'll talk. So then I'll talk about the proof. Okay, so a key, right? So this proof, I only give the sort of proof sketch for how to analyze the algorithm. And the algorithm have two phases, the linear programming phase and the iterative running phase. The linear programming phase is exactly sort of the optimum value is exactly this program, is exactly this problem, right? So you want to maximize. Problem, right? So you want to maximize V is just some random Gaussian vector, V, you know, product of V and C d subtract so that all of your inequalities are satisfied and your C d are in the box. So it's important to understand the optimal value of this linear programming. And the reason is as follows. We sort of need very accurate information for what's going on after the first step of the linear programming. There are two things that we care about. There are two things that we care about. First, after running this, how many zeta i's becomes plus or minus one? And the second thing we want to know is sort of what is like the empirical distribution. This is a vector, right? What is the empirical distribution of x times theta? Or in other words, how far away are there to do the actual constraint to tape? These are the information we are going to use for the second iterative boundary step. And to understand these two, you can just, for example, you say, You can just, for example, you say this portion of my theta are plus minus ones. You can just add these actual constraints to this optimization problem and see how this optimal, how the casino changes. And if it changes a lot, then you sort of know what is going on with the number of CDAs that are plus or minus ones. And it also happens to be this case. You can put in sort of, you dream property for the integral distribution of identity. For the integral distribution of x center to these constraints and study how the optimum value changes, and then you can you can sort of have an understanding of this. So, the key then is to understand how to sort of understand the behavior of this optimal value. And for this, we use the Bordens inequality. So, for those who don't know about Bordens inequality, it's just sort of for two Gaussian random vectors, there's a way to compare them. Way to compare them. And so if they satisfy this set of inequalities, you have this union intersection with a little bit like a minimum, maximum on the previous slide. In particular, there's a useful corollary for the Gordon's inequality, which I think I'm going a little bit faster than I thought, but okay, so essentially, we can forget about what the core is saying, but essentially, you have some random. Essentially, you have some random Gaussian matrix of size m by n, and you are able to compare this to just two one-dimensional vectors, one of length m and the other of length n. So for us, there's this m by n Gaussian matrix available. And you can compare this to two one-dimensional vector whose max and mean are much easier. I mean it's not easy, but it's sort of like easier compared to the matrix. Sort of easier compared to the metric to analyze. And therefore, you can reduce the understanding, which is to your understanding of the one-dimensional vectors. There you can compute the optimizer quite explicitly. So that's how we're going to use the Bordens inequality. Essentially, the one x is just like a fault of matrix, and the other y's are like the one-on-one factors. So, right, so in particular, for example, Right, so in particular, for example, when kappa goes to negative infinity, and when your alpha is in this regime, one over five of kappa kappa squared, which is like how where we show our algorithm could work, we can show that after the first step of the linear programming, so a 1 minus small or 1 over 1 over big or 1 over kappa squared portion of the coordinates of z becomes plus minus 1. So actually, because kappa have very big absolute values, this is Have very big absolute values. This is just almost all coordinates already becomes plus minus one after the first step of linear forwarding. And then you can just, you know, start to analyze what's actually going on for the random work, for the random work algorithm. Right, so yeah, so for example, when kappa is like minus 100, then this is like super close to 1. Maybe you don't want to. Maybe you don't want to take kappa too small. Kappa is minus 4. So you can say that at least a big proportion of the coordinates becomes plus 1. Yeah, so these are all the proof ideas. And I have just the last slides on directions for future research. So whether one can have weapons analysis or algorithms at kappa equals to zero. Kappa equals to zero. So I really want to understand, you know, the kappa equals to zero is the most classical question. So we really want to know whether this 0.77 is the correct transition. What happens, whether you have a nice algorithm that goes all the way to 0.77, where you will have some hardness down to this. And for the asymptotic regime, when kappa goes to negative infinity, whether you can close the log squared kappa gap, which rise might have. That which might have some answer on Friday. And so these are of order, right? So these are like order something, whether you can get the actual constant where you know the next level of expansion, the constant before the order, and what is exactly this alpha og in terms of the function of kappa. And again, so whether you can understand more about the geometry of the solution set and how is it Of the solution set, and how is this related exactly to the alpha, to the alpha out? So, that's a little bit early, but that's all for my talk. Thank you. Questions? Yeah, thanks for the great talk. Can you go to the Kappa Ghost Puzzle thing we saw? So, here at the Office chat. Here, the office status result is for the sphere. So I guess it also follows from your result that the sphere and cube have the battery. No, we also have the binary. Yeah. Well, okay, I guess the sphere upper bounds the binary one anyway. So does it follow that the sphere and binary have basically the same separator? Uh I think Dina had a note on this, right? So how how the severe and the how how the severe and the binary case compare? Sephir and the binary case compare to each other. Yeah. But this is the one for the Sephir as well. But I don't quite... I'm not sure. Does he have an automatic? Yeah, so this is another of his paper, not the Sephiroth. He has a lot of paper on that sort of. None of them are public. Yeah, but it's nice. So yeah, but it's also not a very difficult computation. But it's also convenient to just decide him. Do you think your algorithm is stable? That's a great question. Maybe. We didn't try to do this because there's a lot of work, but potentially. Or even in practice, I guess you need to sort of check if you sounds like you actually can implement it. Oh, that's true. Yeah, yeah, but I didn't track the stability part. Yeah, I should, yeah. But maybe, because you could, you know, if you just think about how to prove it, you could just decouple so so the linear programming part, you could just a couple of things and the random work is it again, you could just the couple couple of the different the different stuffs. Yeah, but we didn't choose to pursue it. But we didn't choose to pursue it because it probably analyzing the algorithm is quite the model where then the stability may be might be important. That's a great question. Physics papers have equals zero with 0.77. Do they also have some experimental evidence like setting out that we just 0.77? That was purely just no, so the no, it sort of um Here, right? No, so all of the relevant ones are sort of, like I said, between 0.69 and 0.75. Yeah. And 0.77 is like something like a local entropy. And you want this local entropy computation to this thing to be monotone. So there's a yeah. I mean p potentially, you know, you Potentially, you know, you could come up with, you know, this might again relate to what Mark and Bryce and Nike are doing, but sort of intrinsically this could be, if something's monotone, then you can design an algorithm that's related to the monotonicity of this local entropy, then it could work. But yeah, they don't have it to the algorithm. Does your analysis of the algorithm show that it goes to these like connected Connected clusters rather than isolated ones? No, we didn't show it. It would be related to the stability question. So if it's stable, then you maybe can sort of change your matrix in some way and cook up this structure and show that electro algorithm actually converge to these clusters. But yeah, we didn't we didn't pursue it. But I think one could do it. But I think one clue to it. Yes. I guess, so when you said this fact about these clusters that exist, I guess one way those clusters can exist is that they're secretly solutions to kappa being slightly positive or something like that. Do you have any sense of what these clusters often actually look like? Is it the case that they're approximate solutions for kappa being 0.001 or anything like this? Is it the case that many of the constraints are actually quite a bit? Many of the constraints are actually quite a bit larger than zero? Great question. So, okay, so we have a work before for the symmetric perception, where you could say a little bit about this. So, in particular, for these, because when Artifact is very small, right, so you could you have a lot of you don't have many constraints, so you could come up with solutions that have a big margin away from Kappa. Big margin away from kappa. But then, sort of, around this point, there's clusters of linear size, linear unit size, diameter that exists. Right, but the whole picture should be more related to their, you know, maybe three, you know, automatic three-like structures, but yeah, it's complicated, related to level z of Hamiltonians. But yeah, so Yeah, so it it's actually, I think it would be very nice to sort of say, be able to say a lot about the actual structure of these connected clusters and how exactly this relates to them. Thanks, Greg. Just a question, comment related to Reza's question. Maybe you haven't thought about it. A conjecture might be that it's actually algorithmically hard to find an isolated solution. You think that could be? That could be true or not. Makes sense. Yeah, it makes sense to me. Right? How are you going to find the isolated? Yeah, I mean, David, Ramanek, and Achman have a movement for saying it's hard to sample. Yeah, I saw that. But much stronger would be it's hard to even just find one. Yeah, that's true. Good question. Thanks. And the results don't chever ones? Results to whichever ones? I mean, they really rely on saying it's hard, like an algorithm, a stable algorithm, can't replicate the gives an intro. But it's much, much stronger to say that you can't find one really wacky isolated. Any other questions? Thank you very much. 