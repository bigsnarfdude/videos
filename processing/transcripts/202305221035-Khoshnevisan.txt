I do have a reasonable excuse work-wise, but I'm happy to be able to remotely join and enjoy the conference from afar. I'd like to speak about my joint work with Kanu Kym and Carl Mueller. I believe Carl might be here. And before I do, I'd also like to thank NSF for their support over the years. This is Over the years. This is work on the stochastic heat equation, which is a topic that I've been interested in for a few years now. So the object of interest is the following parabolic PD, which I put up here for your convenience. Pardon me. The solution is a function of two variables, u of t. Variables u of tx, t is the time variable, x is the space variable, and it solves the following partial differential equation, which is a heat equation, which I've highlighted here, plus a stochastic term, hence the stochastic heat equation, which has a feedback term sigma of u multiplied by noise. The noise is sort of the most fundamental noise that we have in this topic, which is just space-time white noise. So it doesn't have any modeling assumptions whatsoever. It's just the centered Gaussian distribution. It's a generalized Gaussian process. Its covariance is delta function in time and delta function in space. So if you built a discrete model of this, Discrete model of this, so instead of time being continuous and space being continuous, if t and x and s and y were discrete, delta function would be an honest function on discrete spaces, and this would just be iid normal random variables. This is a continuous model of that, which is distribution value. So, this is an initial value problem. So, let's set up the initial state of the system. State of the system. At time zero, we start with some function u zero, which I assume is non-negative and it's bounded or essentially bounded. And let's just say it's non-random so we don't have to worry about technical issues. I do want to assume that it's not zero. So even though it's non-negative, I do want to assume that its L1 norm is strictly positive. So on some, let's say, interval or something like that, it is. Or something like that, it is strictly positive. Elsewhere, it's bigger than or equal to zero. So that's the initial value. That's the initial state of the system. If you want to think about the solution as a density of a particle system, so this is consistent with that. At time zero, you have a certain particle density, then you watch it evolve according to these dynamics in time. The feedback mechanism is a non-random function which I'm going Random function, which I'm going to assume is Lipschitz continuous. So now you have really no technical issues with existence, uniqueness, and so forth. And I do want to assume that sigma maps the point zero to zero. So when the solution goes to zero, then sigma kills the noise. And a consequence of this condition, one of the many consequences of this condition, The many consequences of this condition is that the solution is positive. So I'll come back to Markov property in a moment. But a very important consequence of sigma of zero being zero is that the solution is positive. And because at time zero, it wasn't strictly zero, it was strictly positive somewhere, it actually is strictly positive at all times t bigger than zero, everywhere. So this would be a natural consequence of a Natural consequence of a maximum principle. If we had one, we don't really have one. The noise is too rough to yield a maximum principle, but there is a comparison theorem which does that. And that's due to Carl Mueller. The process is an infinite dimensional Markov process. So if you view this as a path-valued process in time, so at any given time t, what you see is a u of t is a function of x, so that moves in the space of functions. Moves in the space of functions, that is a Markov process on the space of continuous functions. So you really should fix a time t, plot the path as a function of x, and watch that object evolve using these Markovian dynamics. That's the stochastic gate. Sigma is Lipschitz, so it has a finite Lipschitz constant, which I'm denoting by Lip. That's quite common. I am going to assume, pardon me. I am going to assume, pardon me, I am going to assume that near zero, sigma behaves like a line, and therefore actually everywhere it behaves like a line from below. So another way to say that is sigma of x over x is bounded below by a constant, which I'm calling L sigma. That's a technical assumption, which says that the graph of the function sigma must The graph of the function sigma must lie in a cone that goes through the origin. So, sigma of zero is zero. Moreover, the graph has to grow within a cone. It really has to lie within two lines. Those are the technical conditions for this equation. And then you simulate this. By the way, to get simulations that look like this, To get simulations that look like this, you really need that cone condition. But then you simulate this over large x intervals. So here is a time, this is the solution to this stochastic PD for sigma of u equals some very large constant times u. So it's actually linear. I'm plotting it at time one over a very large span of x's. So here x goes from zero to some number of 4,500. I haven't told you what the scale is, so it doesn't. I haven't told you what the scale is, so it doesn't really matter, the number 4,500. But what I would like to point your attention to is this phenomenon that you will observe these very, very large peaks distributed over narrow islands. Please remember, I mentioned quickly that the solution is strictly positive. It is not zero in these valleys between these islands. It's just relatively smaller as compared with the peaks. As compared with the peaks. And this phenomenon is called intermittency. It's sort of akin to gusts of wind. You're standing outside, enjoying being outside. And every now and then, you get these gusts of wind. So the density goes very high, very quickly, and then goes back down. And then there are periods of calm. There is a growing, and in fact, now a large literature on the peaks. The peaks have On the peaks. The peaks have attracted a lot of attention. How many peaks do you have? How do they distribute themselves in large scales? How tall are the peaks when they're tall? And so forth. This is a talk about the valleys about which much, much less is known. So what happens in the voids between the peaks? And actually, we really don't know very much at all. What I'm about to tell you is all that we know. All what I'm about to tell you is all that we know at the moment. A special case of this is the so-called parabolic Anderson model. That's sigma of u equals u, so it doesn't lie in a cone, it's actually a line. And the simulation I showed you is basically this multiplied by a constant. So it's a rescaled version of this. And the term parabolic Anderson model, so you might know Anderson is, of course, a very distinguished physicist, Nobel Law. Nobel laureate, amongst other things, material science. And this is called the parabolic Anderson model in honor of Anderson's work. Anderson studied static equations. Well, he studied actually the hydrogen atom, but they are akin to static equations, parabolic versions of which are kind of like these. Harmono Molshadov, in a very influential paper, introduced these types of equations. To avoid technical issues, they Technical issues, they considered the case where x is discrete. So now the heat equation is, you still have the Laplace, but the Laplace operator is the discrete Laplacian on the graph Laplacian on Zd. And you can do this for every D. Here, I've done it for D equals one only. There is no solution. Otherwise, here there's always a solution. And C could be any Gaussian noise, pretty much, any interesting Gaussian noise. Any interesting Gaussian noise, specifically any noise with this correlation structure. And these could be measures as long as they're tempered, everybody's well defined. And if you specialize this to gamma being a delta function, that's a measure, and capital gamma being a delta function, that's a function because x is discrete. Then you get space-time white noise. Space-time white noise when time is continuous, space is discrete. And in any case, they called all of these models the parabolic Anderson model. And so, what we're seeing here is the continuous version of that, which we're sometimes calling it the parabolic Anderson model as well. One of the original interests in these models came when D is bigger than one in the work of Zeldovich and his collaborators in the study of large-scale distribution. In the study of large-scale distribution of galaxies, but now it has lots of applications to KPZ fixed points, KPZ equation itself, and so forth with really spectacular recent progress by some of the many people that are listed actually here. These are, they can be itself, this object can itself be a limit of particle systems, but it's also related to a larger story of particles. Larger story of particle systems for Markov processes and infant dimensions. So that's the object that we're studying here. Here's the simulation that I showed you of this PD. One thing that I do want to point your attention to, and this is something that physicists noted a long time ago in the 90s, I think, or maybe 80s, is that this is a linear equation, so you have lots of scaling properties. And so what you should expect is if you And so, what you should expect is if you take this equation, here's the simulation. Take this simulation and just take a portion of it, which I'm going to try to outline and obviously fail. So take from 500 to 1500, take this portion and scale it up so that fills your screen. What you should see statistically is a small version of what you're seeing now. I hope that makes sense. That's because if you That's because if you take a linear equation, multiply the solution by two, that still solves the equation. You just solve the same equation with two times the initial data. Okay, so if you take a small portion of this picture and blow it up, you will see a statistical analog of what you're seeing now. And what that tells you is what you should have here are sort of fractal type objects within fractal type objects. So you're seeing big fractals. So you're seeing big fractals that are constructed by small fractals that are constructed by smaller fractals and so forth. And so they predicted that this object should have multifractality behavior. And of course, this isn't really, I'm not stating it using mathematics language, and neither did they. So part of the challenge to resolving this relied on giving meaning to what that means. What does it mean that this is a multifrack? So, and that's what we did with. Right, so and that's what we did with Kong Wu Kim and Iman Shao a few years ago by appealing to an old idea of Barlow and Taylor using macroscopic Hausdorff dimension. So maybe I'll say something about that as well. In any case, so I mentioned earlier that a lot is known about the peaks, so let me just quickly say something about what is known about the peaks. What is known about the peaks, and then I'll show you a theorem. So, most of the work comes through calculations of moments. As mathematicians, you have only a few things you can hang your coat on in this business. One of them is you can try in some cases to calculate moments. And in any event, in a really influential paper, Cardar Parzian Zang, when they studied this, they sort of implied that this can be done. And Cardar a few years ago actually calculated. Years ago, actually calculated it's really a prediction, and he wrote an equality here, which, of course, isn't true. But something like this is true: that the moments behave like the following. If you take a solution at a fixed time t, at a fixed space x, then its mth moment, when n is an integer or possibly not, has this particular form, e to the m times m squared minus 1 t over 12. This should be true if t is large or if m is large or there's Is large, or if m is large, or there's lots of different asymptotic settings in which this is true. And I believe the first person to actually do all of this was Sha Chen, although earlier Bertini and Kian Karini also introduced ideas. So moments are known. So we know things now about the moments. There is an ergodic theory, sort of heuristic, which suggests that those peaks that you saw in the simulations should arise because moments grow like this. You notice the nth moment grows like e to the. You notice the nth moment grows like e to the m cubed. That's far, far faster than what you see in classical moment problem conditions like a Carloman condition. So we don't even know for sure if the moments determine the distribution. But still, if they did, then this heuristic maybe would give you some information. On discrete space, Carmona and Molchano have made similar arguments. Arguments and for non-linear sigmas, there's also some work by some of these folks. So, this is some of the works that are known about the peaks, not the valleys. And I'll just quickly show you, since I don't have that much time, quickly show you what we mean by multifractal. So, it turns out that the peaks form a multifractal in different senses. Here's maybe one sense. You look at the set of all You look at the set of all points, space-time points. You have to scale the time differently, but let's just put t here. You look at the set of all points tx, where the solution is bigger than e to the beta t for different betas. Okay, so these are infinite sets. T goes from zero to infinity, and x is over all of r. There's a notion of Hausdorff dimension for large spaces, just like there's a notion of Hausdorff. Just like there's a notion of Hausdorff, which and which parallels the notion of Hausdorff dimension for small sets. So most people, when they think about Hausdorff dimension, they think about taking a set, looking more and more closely microscopically, blowing it up and looking more and more closely locally at the set. Well, if you blow it out and look at the set more closely globally, farther and farther from the origin, but otherwise similarly as you did before. Otherwise, similarly, as you did before, you get a macroscopic Hausdorff dimension. And that's a bona fide dimension as well. And that macroscopic Hausdorff dimension is basically two minus constant times beta to the power three over two. What's beta? Beta is the height of the peak. So different peaks of different heights have different Hausdorff dimensions. That's one notion of multifractality. Can I ask you? Can I ask you a question? Sorry. Of course. Can I ask you a question? Of course, yes. So, sorry, I lost you from the beginning. I thought I understood. So, the peak, you define the peak as a beta. So, how do you define the peak again? Because the peak is the value of the solution or the magnitude, right, or the solution. Well, so we're looking at the points where the solution, utx, is bigger than a big number. Is bigger than a big number. And it turns out that a very natural thing to do is to set that big number to be either the beta t or t is the time variable. But why does it depend on time? This is my question. That's the right thing to do. It's because the moments grow exponentially in time. Okay. So there's sort of a natural exponential scale in the time variable. But what this tells you is that there's no canonical exponential scale. Canonical exponential scale in the time variable. For different betas, you see different dimensions. Okay. Thank you. Sure. There's also a notion of multifractality in just space. It says space-time, but there's just space. So you fix a t and you look at x that's very far out. You also have a notion of fractals within fractals, but macroscopic fractals. And if you And if you fix x and let t get big, there's also a notion, and that's recent work by Das and Gosal and Gosal and Yi, which I want to invite you to look at. So this particular theorem right here, if you actually dig into the definition of Hausdorff dimension, which I won't be able to do today, but if you actually dig into that, what this implies is the following. If you look at the valley that straddles the origin. Origin. So let me show you what I mean by that on our simulation. Here's the origin. If there is a valley, so this is a void, the area where there aren't peaks, the solution sort of behaves typically. If there is one that includes the origin, call the length of that L of t at time t. Now here you might think, oh, there's no valley, but actually this is the heat equation. Everything goes to zero. So eventually there will be a value. Okay. That's not obvious, but it is true. So if you let L of T be the length of the valley, if there's no valley, call it zero. So that's well defined. Then L of T has the following property. There is a random sequence Tn that goes to zero. This is a consequence of this fact. Of this fact, such that log of L of Tn is at most Tn times a constant, or equivalently, it has sub-exponential behavior along some sub-sequence. Which might not surprise you, the solution when in a typical valley, the valley is less than some huge number. Okay, that's all it says, really. I mean, this says a lot more, but from this, that's all you can deduce. All you can deduce, uh, but it might be interesting to know if actually this valley does get large, for example, at all. And so, what we what we prove, and that's the basic theorem of this talk, is that yes, in fact, it does grow. And in fact, the length of the valley, which I haven't formally defined yet, but that's where the solution is not big near the origin. The length of the valley is at least e to the constant times. At least e to the constant times t to the power third, it's quite big, so the valley is big as well. And of course, to define what a valley is, you have to say something about the height of the solution on the valley so you know that you don't have a peak. So you also get some side information about the supremum of the solution over a valley. So let me just show you a theorem, and I think this would be a natural place to end. To end on this page. So, here's one way to make that into a mathematics statement. Let's say we start with a uniform initial distribution of density. So that means u0 is just one on all of r. So at time zero, you put density one particles everywhere. And then you watch them evolve according to the stochastic Git equation as time goes by. And what this says is there are two non-random constants, lambda one and lambda two, such that if you look at Such that if you look at an interval of length e to a constant times t to a third, so that's a very large interval when t is large, near the origin. The soup is very small on that interval. How small was less than e to the minus constant t to the third? As t goes to infinity, this is an almost sure statement. So in particular, what this says is eventually the solution. Eventually, the solution is less than any constant because e to the minus t to the third goes to zero. And so there's a valley of length at least twice e to a lambda one t to a one third that contains the origin over which we don't have very tall peaks. No peaks above, say, one. In fact, no peaks above this. And that's that's the main theorem here: if you start with initial data, that's one. Data that's one, the value grows like a stretched exponential at least eta t to the third. And so maybe I'll say something about how the proof, at least part of the proof goes. So the first step of the proof is to change the problem. So this is the dissipation problem. You really are interested in how the solution goes to zero. Interested in how the solution goes to zero. On the real line, that's not so easy to do. So, but it's much, much simpler to show that if you had the same equation on a compact space, the solution really would go to zero exponentially fast. So, what we do is we try to relate the problem to something like a problem on a compact space. So, one way to do that is you start with initial data that That let's say has compact support or has Gaussian decay at the most. But let's say compact support. So if the initial data has compact support, then for a long time, the solution in a range like this doesn't know that the equation actually is an equation on R. It might be an equation on a very large but compact set. So I'm emulating the compact support case. And what you can prove actually is that the global supremum of the solution. Is that the global supremum of the solution goes to zero, like e to the minus t to the third? Okay, so if there are valleys and peaks here, we're not, we don't know what they mean because everything goes to zero and everything goes to zero globally at a similar, I'm not saying that these constants are the same, but the rates are the same: e to a constant times minus constant times t to a third. And then there's a superposition argument that I don't have time to get into, but you basically say, well, okay, if the solution has compact support at time zero and everything goes to zero, then maybe I can juxtapose the initial data and do a comparison argument. If you've seen PD arguments like this, kind of like those. Of course, because of the non-linearity here, you have some work to do because. Have some work to do because you can't just superimpose initial data and add the solutions. And that's part of the innovation of the work. But nonetheless, you can think of the linear case. And so that basically reduces the problem to compact support initial data in that case. And then the remaining problem is how do you prove this? So I think I'll end on this one note. This one note, you prove this using a Martinial argument. That the integral, instead of soup, if you integrate utx dx, in this case, that integral is finite because the initial data is compact support. That integral is a Marti L in T, and you can actually estimate a square function. And this is a result of that estimate. Thank you for your attention. I'd be happy to chat more, but maybe afterward. Okay, thank you very much. So, are there any questions for Dava from the audience or from Zoom? Yes. Hi, Davar. So, could you tell me where you're using exactly the fact that you have a space-time white noise, or is it something which is just Is just for convenience or simplicity? So, a lot of the work can be done for other noises. So, those who might not have thought about the topic, these equations are of some interest when the X variables in higher dimensions. And I think what Sammy is asking is very important. The way people give meaning to those equations in higher dimensions is by not having spacetime. Is by not having spacetime white noise, rather, a noise which has some correlations in the space variable. Otherwise, there's no solution. So the dissipation problem is indeed interesting in those cases. Part of what happens there is this Martingale estimate. So maybe I'll show you what the Martingale estimate should be. So if I call mt the integral that I mentioned, which is just integral of utx dx, okay? Okay, you look at low moments of that interval. So look at the half moment of that interval, call it f of t. What you can prove is that if you have space-time white noise, f of t solves a certain differential inequality, which you can solve. And that differential inequality says any function, including yours, which is highlighted here, that solves that differential inequality must go to zero, like e to the minus constant times t to the third. If you have colored If you have colored noise, you get a differential inequality of a type, but it's extremely complicated. To get something explicit that you can solve, well, okay, I guess if you have certain types of correlation functions for the space variable, maybe you could do it. But that's actually where it all gets hung up. Thank you. Thank you, Davar. Just one quick question. So, this information about the peaks and the valleys, is it so? People from the KPC community, do they use this kind of information, this kind of formulas, something? I'm not sure I understand your question. I'm sorry. I mean, some of the people that Some of the people that I mentioned consider themselves part of the KPZ community, but I don't know exactly what you're asking me. Like for the KPC equation specifically? Or the KPC fixed points? Okay, so the fixed point is a different object. The KPZ equation, so the solution is positive, so you can take its log. The KPZ equation is basically log of U. It solves the KPZ equation. So peaks of U are So peaks of u are, up to the logarithmic transformation, the same as the peaks of log of u. So these are the peaks of the KPZ equation. The KPZ fixed point is what happens after you normalize this solution and let t go to infinity. And there, I don't think these peaks persist. So the KPZ fixed point is actually a relatively well-behaved stochastic. Relatively well-behaved stochastic process, which does not have intermittency in the sense that I showed you in the simulations. Okay, thank you.