Talking about diffusions on the graph, my motivation of the study comes from realized variance. So if you look at from 2012 to 2021, daily realized variance of indices, SPX, DJI, RUT from financial market, we can see some dependence. This is a time series data set. We have some spikes as almost the same time. Almost the same time, and even under the lower fluctuation part, we can see dependence. And if we look at dependence from minimum spanning tree, we can see some structure. And if we want to model such system, we want to propose modeling a directly tree structure. So if we want to see x of t, so process times this theta depends on some. Data depends on some factors x1 tilde, x2 tilde, x3 tilde. It's like a children which affects x, and each x2 tilde, x1 tilde, the children also are affected by grandchildren, xw tilde, xw tilde, xw tilde, and so on. So this is a picture of when d, the graph has four vertices, d equals 4, and we want to see. We want to see because it becomes complicated. We want to see some simplification such that the joint distribution of x1 tilde to xd tilde is the same as that joint distribution of x2 tilde, x double tilde, grandchildren, and so on. So we want to see some structure among stochastic processes, which can be indexed by graph. If n d equals one, which is like a directed graph, linear, i depends on i plus one, i plus one depends on i plus two. So the simple idea maybe is a Gaussian process, Onstein-Uhlen-Beck system. And if we look at only n vertices, n points x ti, i equals 1 to n minus 1, depend on i plus 1. And then the last one, let's say n spot. One, let's say nth particle, nthlicas process depends on the first one, for example. Also, for a comparison, we can think about dependence through the mean, mean field or mean and X T I in this again the ownsteim Limbeck type interaction, X T I depends on or try to be attracted towards the average. Attracted towards the average 1 by n of the summation of XTJ. Then, if we compare these two systems, directed chain or Gaussian cascade and mean field interaction, several questions can come up. And first question, what is the essential difference between two types? Can we filter the observations? Third question, optimization problems. Fourth question, that's what I want to. Question: That's what I want to explain, introduce today. What is the relationship between the input noise and the resulting process and these interactions? My motivation comes from three structure in volatility in the financial market, but probably we can see some graph structure in the real life, real life. So, let us introduce or consider interpolating. Consider interpolated system between two systems. First one is directed chain, second one is mean field system. So I put u. u indicates how much rely on the neighborhood local interaction. So u times xti plus one for xi. xi depends on i plus one with this much u and one minus u proportion which affects by the average as an interpolation between two systems. Two systems. Then, question: first question: Fat is the limit as n goes to infinity. So, here w is an independent Branier motion, but my goal of this talk is to explain W can be, because of additivity, W can be coming from the space of continuous functions. So, from that limit, as n vertices goes to infinity, infinite graph, then Infinite graph, then we can define xt indexed by u b a drift coefficient depending on time t xt itself and f is coming from the mixture distribution or mixture law coming from Dirac measure of x tilde in that neighborhood and a law of x t. And important constraint here is that constraint here is that identical marginal law such that x has same distribution as x tilde let me call law of x u x and x tilde have the same distribution this is a little bit different from the marking brasov equation because we have a pair x and x trader and we assume x and x truda marginal distribution over the time are the same and here we want to interest it in the dependence between Too interested in the dependence between x and x to the because of the graph. Two extreme cases, find u equals zero or one reduces to the two systems that I mentioned in the beginning. So the first probation, if we look at a special case, b is a drift, Lipsis continuous in the sense that it's a time version. B can be written as the integral b tilde t x y mu d y b tilde is Lipsis continuous. P tilde is represent continuous with at most linear growth. Then we can construct a weak solution xx tilde pair given Branya motion B such that dxt is drifted by B T X T and F T, where F T comes from the mixture of Dirac measure at X tilde. At x tilde, the neighborhood, and 1 minus u proportion to the law itself. And assuming that distribution of x, distribution of x tilde are the same, and this b driving noise is independent of the neighborhood guy, driving noise for x is independent of the neighborhood guy and its initial distribution. Proof is based on the contraction. Contraction construct contraction argument for picard iteration and the one fastest time distance metric. So maybe skip. We can from that perspective iteration fix u we can go to the next proposition. The common marginal law L of x L X tilde is a unique fixed point of the solution map that depends only on the That depends only on the common distribution of initial value distribution and Branian motion B with the following inequality, contraption inequality such that passes time distance between the joint distribution of x and x delta for one input and for the second input, which is another input, I1 delta two. The order 2B2 comes resulting xx tilde. If we look at the distance between these two joint marginal row is bounded by some constant, the exist CT constant times W1. Here B is the inputs, can be a random element, non-necessary Branier motion. And for the solution map of this equation with such a general B, we take unique solution. We take a unique solution to the ordinary differential equations. This comes to the pathwise part of this talk, and there are several related talks even for this conference workshop. It's very close to Remy Kataria talks this morning. And let me explain briefly about example. For example, Btx mu can be written as a linear version. It is a linear version, then in that case, we can relate, simply calculate this drift component. It looks like this. For simplicity, if we fix starting point as a zero, then we can make a lot of simplification. Expectation becomes a zero, so we relate this original little bit more general equation to the simpler one so that we can think the solution constructed with casibury. Constructed the Casiburi iteration scheme, and in particular, find u equals one for all simplicity. Let me write the solution. So starting at zero, u equals one, u x two dagger is suppose that x two dagger is known, then we can define x one dagger because this is Onstein Lebeck type. Let me come back. So b t x mu is minus x minus u, so which give us Onstein on Leb. Give us Onstein under the drift. So given x2, we can write an x1 dagger. And given x3, x2 is written in terms of x3 and with additional noise, p1, p2, branier motion, for example. And then substitute this x2 into the fast equation, a fast representation for x1, then we get this. For x1, then we get this. So we have integral of noise for the first integral and the second double integral. But we can use Fubini's theorem, stochastic Fubini's theorem really write. So we can continue for the product rule for senior matching error in Brownian case. Then we can write it down this integral with respect to Branian motion first, and then next the big integral that we can write down. That we can write down in exponential form or Poisson type kernel. So if we continue this iteration, we get x1 dagger can be written as a lot of noises, brand new motion is compounded by this Poisson probability. So we can write the dandel solution x1 dagger as this, and then we can compute. As this and then we can compute because of the Gaussianity, we can compute covariance, etc. So, let me go to the next topic. So, there are several examples we can discuss, but if we look at the strength or password approach, I like to say particle approximation when d equals one again. And we want to approximate the joint distribution of x and x tilde by patching. Exterior by particle approximation and approximate approximation system. Let's say we have n particle and we start with initial IID random variables and then it is natural to write the n plus one particle is x1. So we have a like tolerance and then if we do so then we have Then we have it's not IID, it's not same it's not IID, so we cannot it's not an IID situation and we cannot expect propagation of chaos type results, but still we can make the following. So, distribution of xi is same as the distribution x1, joint distribution on the next. Joint distribution on the next to each other is the same, and so on. So, if we use this idea, and if we look at the empirical distribution of the one and its neighborhood, so capital M is empirical distribution of the one and the neighbor and the marginal distribution. And assuming that the initial value, initial distribution converges to the initial distribution converges to the Dirac measure at M0. So this is a case of, for example, IID case. Then distribution of MT empirical measure combines with a point mass Dirac measure at MT, where M T can be described by integral equation. M is here, capital M is a joint distribution and little M is a marginal distribution can be characterized by the following integral. Characterized by the following integral equation. When u equals one, in that case, this integral description characterization is reduced to Markin-Brasov theory. So in particular case, we are interested in pathwise approach. So Wasser-Stein metric between empirical measure of Empirical measure of n particles and the limiting measure is bounded by again Ct times W1 of initial random variable and the noise component and if we look at I cannot I don't have time to explain but we can also consider the fluctuation and around the mean so what I mentioned mean so what i mentioned here is the row of large numbers results so we we are interested in in fluctuation around the mean so i'm interested in crt and large deviation principal results for particular approximation thank you very much for your attention Are there questions? I think, yeah. So at the beginning, you had this realized productivity model. Or how do you calibrate these SDs that you have for your data? That's a good question. I haven't touched the data itself to calibrate, but what I can mention is that we can consider this is a continuous time version. We can also consider time series data version with the same idea. series data version with the same idea saying that maybe those time series data set depends on some those time series data set depends on the graph and here we only look at the very simple directed graph but we can make time series version which is also written in one of the papers I was invoked and from that perspective we can use some times' data analysis methods to Data analysis methods to implement to estimate some of the parameters. That's what I can say. But I think it's very important because of the data science machine keep learning. So maybe not necessarily linear structure. We might be able to look at more non-linear or more complicated functions dependence, functional dependence. Thank you. Thank you very much. Thank you very much. Time for one more quick question. So, hi, Tommy. You mentioned market vision principles work here. So, is this something you start to work on or is it in the future? In the future, so I have only put in minadi understanding, and I assume maybe it's very close to radio debation principle for maquimbras of type. For McIntras of type theory, and I'm quite interested in that direction. Thank you. Thank you. All right. Zoom questions if someone wants to point. All right, so if not, let's thank the speaker. Thank you. Switch to the next first four.