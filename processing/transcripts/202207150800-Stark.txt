Some work here. So, this is the paper that I wrote and that I'll be talking about. And yeah, in our paper, Aquibind, we're tackling a crucial task in drug discovery, which is related to small molecules and proteins. And here we have such a protein. It's swimming around in our bodies right now and fulfilling an important function, which is to control cell growth. So, telling whether or not. Cell growth. So telling whether or not cells should replicate. However, sometimes this protein is mutated and it always tells cells to grow. And this uncontrolled growth is what we know as cancer, which we would like to prevent. So biologists have figured out that if we have a that in particular this specific region is important for this abnormal behavior of the For this abnormal behavior of the protein, of the mutated version of the protein, and furthermore, that if we have a small molecule that if it's also swimming around in our body that attaches to this particular region, if it comes close to the protein, then the normal behavior of the protein can be stopped. And this attaching of the small molecule to the protein is what we call binding. So we see that We see that predicting this binding or predicting the binding structure, the location where the small molecule binds, would be very valuable for drug discovery. And this is exactly the task that we tackle here in Equibind, where as the input, we have a protein with its 3D structure and a small molecule without its 3D structure. So only the molecule. So, only the molecular graph. And what we're trying to predict is the conformational change of the small molecule, the 3D structure that the small molecule will take on during binding and its location with respect to the protein. So, these are multiple things, right? So, we want the 3D structure that the protein at the small molecule will take on during binding. Take on during binding and where it will go with respect to the protein. And when tackling this task, we first make an initial guess of the 3D structure. But this really is only an initial guess and will not be our final prediction for the 3D structure. But then we use this initial guess and to use it, we And to use it, we need to respect some symmetries because no matter where it is initially placed in space, here, here, or here, we always want to end up making the same prediction. And so we want to be invariant with respect to the translation and rotation of our initial placement. And this can be done with Equidoc, which is a method. Which is a method that Octavian introduced last year. Octavian Garnier, who also worked with us in this paper. And in this method, we first process both structures, so the protein and the small molecule with GNNs. So we have messages passed inside of both structures and between. Of both structures and between both structures. Then we end up with some embeddings. And then we use an attention mechanism over the coordinates of the small molecule and of the coordinates of the protein. We use an attention mechanism over the coordinates to predict a set of key points which summarize the small molecule and where the model thinks that the small molecule should go into the protein. Molecule should go into the protein. And then we have the same number of key points on the molecule side and on the protein side. And we also have a one-to-one assignment between those key points. This means that we can use an algorithm called the CAPSH algorithm to calculate the optimal translation and rotation to optimally match. Optimally match these key points as closely as possible, like with as low RMSD as possible. Then, if we've calculated this rotation and translation with the Kaufsh algorithm, then we can also go ahead and apply it to our small molecule, right? And this way, no matter where the small molecule is initially placed, we will always end up making the same prediction. End up making the same prediction for its final location. Because if it were initially placed somewhere else, then we would also have obtained another rotation and translation in the capture algorithm. And this whole process is differentiable. And yeah, we can back propagate through it and optimize our methods with our usual techniques. And we do that with the RMC or with the Or, with the MSE of our predicted ligand structure and the ground truth ligand structure. A ligand is just another word for small molecule. Yeah, but if this was all that we were to do, then we would always treat the small molecule as a rigid body. And this does not correspond to reality at all because. Because a given molecule could take on many, many different plausible 3D structures, and molecules are actually very flexible objects, especially in their torsion angles. So the angles around the angles of a molecule or the angles around bonds, if we rotate a part of the molecule around a bond. Molecule around a bond, especially those are flexible. So we want to be able to model molecular or the conformational change of the small molecule, its flexibility during binding as well. And for that, we allow a GNN to change or GNNs to change the coordinates during our binding process or during the calculation. Process or during the calculation of the embeddings. But we also know we have some prior knowledge that we want to build into our method, and we all only want to model chemically plausible conformation changes of the small molecule. Because right, if we just take an arbitrary atom of the molecule, then move it arbitrarily far away from the rest of the molecule, then this wouldn't be a realistic change. This wouldn't be a realistic change. So there's a huge space of changes that does not make sense. And it would be nice if we can ignore that space or a large space of a large part of that space and only focus on the part or on the changes that do make sense and have our model being restricted to that so that we can use its modeling capacity only for real. Yeah, for realistic and sensible changes to the coordinates. So we introduced a couple of techniques to build in this prior knowledge about how molecules can change into our model. And first, I want to talk about this side of how do we actually model our molecule and our protein, and how do we actually change its coordinates. So, for that, we use equivariate graph neural networks where we, well, first of all, we see our small molecule as a graph, where the nodes are given by the atoms, right? And the edges are given by the connections to other atoms in a certain radius around an atom. And then we end up with these node features and. Features and distances between nodes that we can use in a graph neural network architecture that's called equivalent graph neural networks. And in our when processing these molecules, well, no, first let me maybe also say our protein we similarly encode as a graph, but we coarse-grain it, so to say, we because we only consider. Say we because we only consider the locations of specific atoms and we don't look at all atoms. Proteins are made up out of these building blocks of smaller molecules called amino acids. And we can all of them have a specific carbon atom. And we only consider the location of the specific carbon atom. And we summarize the whole amino acid into this single position. Position. So we basically coarse grain of protein and don't look at what's called the side chains of the, or we don't have an explicit encoding of the locations of the side chains of those amino acids. And this cost graining maybe can make sense because we already know from many other chemistry and Chemistry and also biology applications of graph neural networks that sometimes modeling hydrogens, for example, explicitly in their locations does not make that much sense because a hydrogen doesn't really have a particular fixed position, right? It's just a single proton attached to our molecule, and maybe it does not make so much sense to assign a Much sense to assign a certain location to this single proton. And similarly, we are not looking at the or not explicitly specifying the positions of the protein side chains. So those parts of the amino acid acids that we summarized into the single location, because these are the really flexible parts that can change during bio. Parts that can change during binding, and yeah, maybe it does not make so much sense to assign specific positions to all of the atoms in those side chains. Anyway, we summarize them into these single locations, the amino acids, and a single direction into which the side chain of the amino acid is pointing. And then we have also these nodes for all of our amino acids. For all of our amino acids, we do our message passing between the amino acids using the distances between them. We do our message passing between our small molecule atoms. And then we also have layers where we use an attention mechanism or a cross-attention mechanism where we pass messages between the two structures. And these messages that we pass between the two structures, right, they don't have distances associated. They don't have distances associated with them. Because otherwise, if we would have distances, then we wouldn't be invariant with respect to the initial placement of the ligand, right? The distance here would be different if the small molecule was initially placed here instead of over here. Yes. Okay. But this is just how we encode the small, our, how we encode our structure. How we encode our structures, and in this whole process, while we have multiple of these GNN layers or e-GNN layers, we also have coordinate updates happening to the small molecule. And this way, right, we model the or we change the coordinates of the small molecule. But we also said that not every coordinate change is chemically plausible. And we introduced some ways to make or to constrain us to more realistic coordinate changes to save some modeling capacity and only use it for the updates that actually make physical sense. So let's look at this part of the techniques. And this is by inspired by Inspired by the fact that biologists or from chemistry, we know that during binding, not all distances actually really change between atoms in a graph, in a molecule. From chemistry, we know that during binding, the distances between nodes that are only one edge or two edges apart in a molecular graph, they almost take. Molecular graph, they almost stay completely rigid and don't change at all during binding. It is only the distances between nodes that are three edges or more apart that are flexible during binding. And to build that into our EGNN, right, we have all of these EGNN layers that change our coordinates. To build that into our model, we say that after each EGNN layer, after each coordinate up After each coordinate update, we run a correction layer on the coordinates where we say that the transformed coordinates, the distances between those, like the one edge and two edge distances between the transformed coordinates after one EGNN layer, they should be the same as the one edge and two edge distances before this EGNN layer. And then we have Layer. And then we have this summarized in this term over here or in this objective. And we minimize or we optimize, we change the transformed coordinates with gradient descent to meet this objective better. So after each EGNN layer, we run a few steps of gradient descent on the coordinates. So this gradient descent. Coordinates. So, this gradient descent only changes the coordinates to more closely meet this objective and to minimize. This is not a loss function. Well, this is not a loss function that we're minimizing to optimize some weights in our neural network, for example. This is a loss function that we're optimizing with gradient descent to change our transformed coordinates to be more real. Coordinates to be more realistic and to stay within a chemically plausible space. So you can imagine it sort of like a batch normalization layer on top of coordinates with which we change the or normalize the coordinates back to stay to lie in a chemically plausible space. And here is now a short visualization. A short visualization of what these corrections do. Where on the left we have the original molecule, and we see that it has the realistic bond lengths and bond angles. And on the right, we have the molecule, or yeah, the molecule, how it looks like after one GNN layer. And we can see that these bond lengths really aren't realistic and they. Realistic, and they just aren't physically plausible at all. So now let's look at an animation of what it looks like if we run our correction steps after this GNN layer. And this is what it looks like. Let me run it once more. And we can see that after running these correction steps, After running these correction steps, which we do after each GNN layer, we end up with much more realistic point lengths and point angles. Of course, this isn't a completely realistic molecule, right? But we end up with much more realistic and physically plausible atom configurations and ensure that throughout the network the model reasons about more physically plausible atom configurations. Possible atom configurations. Yeah. And another step that we do to get our right, this is only a soft update where we or a soft constraint on the coordinate changes, where we run these gradient descent steps after each EGNN layer to move towards a more realistic. Towards a more realistic confirmation, but to ensure that in the end our final prediction really is 100% chemically plausible and has completely realistic bond lengths and bond angles. We introduce a new method that after our EGN analysis, we end up with this point cloud, which might not necessarily have a realistic configuration. Realistic configuration of bond lengths and bond angles. So, what we do is we take the initial molecule, our initial guess of the structure, and we only change its torsion angles here to match the we only change its torsion angles to match this point cloud on the right as closely as possible. Is possible, and we come up with a new method to calculate this change in torsion angles to fit the point cloud as closely as possible very efficiently and very quickly. Okay, and all these methods come together. Ah, well, never mind then. I have a quick visualization of distortion angle change here as well. And there we can. And there we can see, right, this is one of those point clouds here that we produce after our EGNN layers. Oops. This is one of those point clouds. And we see that the bond lengths and bond angles here also aren't completely realistic. So what we do is to change these. Change these torsion angles of the initial conformer to fit this point cloud as closely as possible. And then we end up with these conformers over here with these atom configurations, which are the final outputs, right? And these are guaranteed to have realistic bond lengths and realistic rings that are planar, if they should be planar, because we obtain. Because we obtained them from the initial guess that we made of our configuration, which is done with our D kit. Okay, but then let's summarize all this, right? We have our molecule and our protein being encoded. We end up with these point clouds of the small molecule. We use the We use the equidoc mechanism to generate key points and calculate the optimal rotation and translation to put it into our pocket, to put it into our protein and the location where we then put it. This is called the binding pocket. And then we have this additional step where we restrict the We restrict the or we normal always normalize our coordinates to be more realistic and have physically plausible atom configurations while reasoning about how the structure should change during binding. And we have this final projection step where we make sure where we guarantee that the final conformer that we put into the pocket really Put into the pocket really has realistic bond lengths and bond angles, and only the torsion angles are changed during binding, which corresponds to what we know from chemistry to be where the flexibility lies in the small molecule during binding. Okay. Let's test our method. Let's evaluate it and look at some results. So, Results. So, what we use for that is the standard data set PDB bind, which has many experiment, many of these experimentally determined structures of small molecules bound to proteins, 19,000 of those. And we compare against four baselines, which are popular open source tools or also commercial tools. And then, And then we can find, first of all, when looking at the runtimes, that, for example, in this comparison over here, we almost can't even see the bar that is there for equibind, because it actually is 1200 times faster than the fastest baseline that we compared against. And this is the case, or why I'm pointing this out is because. Um, is because this is really important for drug discovery, right? In drug discovery applications, we sometimes want to search through millions of molecules and being fast is crucial. And this or this speed up is possible because we introduce a completely new paradigm. We basically show that it is also possible to predict good 3D structures in a single shot. A single shot because of what all the previous methods did is that they would always sample many, many different positions of the small molecule where it could be and score all of these positions. And then the best scoring position would be the final prediction. And all of the sampling takes quite a lot of time. And the speed up is possible while mainly maintaining accuracy. Accuracy. As we can see in this diagram, where we show the fraction of test examples for which we make a good prediction defined by having an RMSD less than 5 angstrom. And we can see that equibind is very similar to the baselines in this metric as well. And of course, we can also look at more metrics later on if you're curious about those. But for now, I want to get into some I want to get into some visual examples because this molecule that I showed you, or this protein that I showed to you in the very beginning, this is actually not a random protein. It was actually suggested to us from Pat Waters, who's an expert and a leader in drug discovery, the CTO of Relay Therapeutics, I believe. Therapeutics, I believe. And he pointed this protein out to us because it actually is a really hard case where all of the previous methods failed. So he then went ahead and ran the, used our code on GitHub to run Equibind on this protein, only to find out that Equibind was able to almost perfectly predict the binding. Predict the binding structures of two important drugs for this for this protein. So let's maybe look at some of these visualizations. What you can see here is the ground truth structure in which this small molecule, this drug binds to the protein and with which this drug binds to the protein. So these are two different molecules, right? And now I'm And now I'm always going to show images of this ground truth image and of the predictions with the molecule colored in the same way that some method made. So first of all, let's look at that for glide. Here, right, you can still see the blue ground truth here. But you also see here in red. See here in red is the prediction of one of the molecules that glide made, like of the red molecule that glide made, right? And it is in the pocket where the blue molecule should be. And over here, you see that it puts the blue molecule into the pocket where the red molecule should be. So GLIDE switched the binding pockets around. And meanwhile, And meanwhile, the method Smyner it put both molecules into the same pocket right here. So one prediction is very good, but the prediction for the other drug was completely wrong. The red drug should be over here. Then, meanwhile, with Equibind, we were almost able to perfectly predict. To perfectly predict the location of where the small molecule or where the drug should bind and in which structure it should bind. Okay, and I want to remember that this example was not in our training data, this protein and the small molecules. And this is not a cherry-picked example, actually, right? Because it was pointed out to us by Pat Walter. From by Pat Waters. And then, after Pat Waters ran it, he also noticed that actually Aquibind is able to predict the structure almost perfectly. All right, but then I want to finish or to come towards an end by saying that this project, which was really important to me, would not have been possible without Octavian. Tavian, the co-first author on this paper. He recently died this May. And he was a fantastic researcher and also mentor. And I want to highlight that his advice really is a big part of who I am now. And one of the pieces of advice that he always gave is to work on hard. To work on hard and impactful problems. So let's honor these words and maybe do that and work on hard and impactful problems. Also, to honor his name, we're actually hosting a conference in his name in case any of you are around MIT on October the 21st. We're having a short small molecular machine learning conference, MOMEL. Yeah, taking. Taking place at MIT on this year on October the 21st. All right, with that, I would be happy to get into any questions or show any additional results or also to mention to you that you are very welcome to reach out to me if you're at ICML next week, where I will be also presenting. Where I will be also presenting this work and at ICML on Monday and at 4 p.m. Baltimore time, we are having a meetup regarding learning on graphs and geometries. So if you're interested in that, please also reach out. But I would be very happy to hear any questions or to have any discussions about. Have any discussions about the material presented here?