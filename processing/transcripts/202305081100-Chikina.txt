Sari, so if you're doing tumor profiling and you really just want to kind of isolate, look at like the tumor-specific signal and now the TME signal, probably you don't need to go single-cell in every single sample. And you can just bridge the gap computationally. And another thing I'd like to throw out there is for commercial applications, generally bulk assays are preferred for commercial development. Okay, so what is the best Okay, so what is the best deconvolution method? Well, we probably all have our favorite one, but there's really probably a thousand different methods out there. And they can basically be broken down into kind of a couple of choices that you can make. So one question that you have is, what is the prior information, if any? There's some that don't use any priors. So the prior information can come in basically two types. You can have quantitative prior information where I have just these vectors of expression states for our pure cells. Like I could just have like what For our pure cells. Like, I could just have like, what is a green cell supposed to look like if I had isolated it? I can also have qualitative information where I just have marker genes, these biomarkers, which are just, you know, hopefully uniquely expressed in the green cell, or at least much, much more highly in the green cell than the other cells. So you have to pick your priors, and then you have to pick the algorithm. And the algorithm is basically like you have some function that takes in your data and your prior and outputs deconvolution results. And typically, these will be tailored to the kind of priors or there's some. Will be tailored to the kind of priors, or there's some algorithms that can work with both. Okay, so if we have just qualitative priors, these you can use what are called score-based methods, and these basically proceed as follows. So you pick, you know, you have a cell type of interest. You have some gene sets that represent that cell type. Like you can take T cells and take the T cell receptor. That's pretty specific. So that would be a good marker. So you have those gene set. So, you have this gene set. You can go across your samples and compute scores according to some metric. Now, there's lots of, this is where the methods really diverge. What metric are you going to use? Like, average rank, that's a metric that's going to give you some score for every sample, average z-score. And so now, this doesn't give you proportions, but it gives you something that's like hopefully monotonically related to the proportions that you're interested in. And then there's methods that can try to correct for various issues, like the fact that you don't have. For various issues, like the fact that you don't have proportion units and actually get things on a proportion scale. So, I think one of the more prominently used methods there in this class is a method called Excel. Anyone's used that ever for anything? People are laughing, so I'm going to, that's a yes. And so it actually has all the steps, but one of the big kind of achievements, I think, of Excel is actually the first step. They did a lot of work to curate gene sets for lots of different cell types. So, Excel comes not with just the method, but it also comes with the Not with just the method, but it also comes with the prior. Okay, then we have this whole class of methods which takes quantitative information, and I'm going to call these regression-based methods. And basically, these work by realizing that what's, if we have a mixture sample, so here this is just like a toy mixture, it's B cells, which come in two different types, not even memory. And so basically, any mixture sample should, we should be able to describe it as this. So we have some pure expression states for our two cell types. Pure expression states for our two cell types, and then we have some random proportions that add up to one because these are the only two cells that we have on our sample. And our bulk gene expression just looks like a product of these two matrices. And so this is just, you know, linear combination. So all we need to do really is to just reverse this, right? So, like, given this and the output, we should be able to infer this. This is just a regression problem. Okay. So, there's also. Okay, so there's also lots and lots of regression-based methods. How are they all different? And okay, so these are, you can also break down the regression problem into steps. So step one, you have to form a reference matrix. You have to, you have to get your prior from somewhere. So you have to pick a data source, but apparently that's not enough. You also have to pick a set of genes that you're actually going to fit. You're not going to use all the genes. This is very important. It's interesting to think about why you don't want to use all the genes. Genes. So, um, so you're going to form this reference, which comes from some source, and you're going to perform some feature selection on it. And after that, okay, I said it was a regression problem. We're going to pick your regression method. So least squares, non-negative least squares, weighted least squares, robust regression, hinge loss, or epsilon-sensitive loss, I guess, technically. All right, cyber short is really popular. Really, it's just a slightly different. And really, it's just a slightly different regression formulation from the more common ones. And another thing I'd like to point out: the many, when we think about like cybersort, it actually does both of these things. And the method is like the combination of these two things, but there's really no reason why you like can't break it up and take the first part from one method and second part from another method. Okay, and finally, I come to the methods that use no priors. And these are called reference frame. And these are called reference-free methods. So, we don't have any knowledge of what our pure cell states are. We don't even know what cells are in our sample. We have no idea what the markets are. Just like we don't know anything. How can we possibly hope to solve this problem? Well, the only assumption you can really make is that your data lies on a simplex because you're trying to infer proportion variables. So, you make this assumption that your data lies on a simplex. Here's like a nice triangle that you can actually visualize, but you can imagine a higher dimensional object. Dimensional object. And so, yeah, basically, you just have methods for finding simplex-like structures in your data. So, here's one I put up here: Linseed. So, this is their some of their benchmarking results. They get no priors, amazing results, just like perfect correlation with everything. Seems like magic, right? This seems kind of how do they do that? Okay. So, we have a lot of methods, and how do we know which ones actually work? Well, Which ones actually work? Well, there's a thousand methods, and there's maybe like 10 different extremely comprehensive benchmarking studies, some of which are listed here. And I can share the slides afterwards if you're interested. And with the advent of single cell, it's actually become kind of easy to do benchmarking because, given a single cell data set, I can just now construct bulk data sets in controlled proportions. And so, seemingly, this gives me like some sort of perfect benchmarking pipeline. Some sort of perfect benchmarking pipeline where I can benchmark methods. So I can start with a bunch of single-cell data sets, form like an infinitely many combinations of things in different proportions by just adding cells up together. And I can use the same data to build markers, references, and I can put it through all of the different kinds of choices regarding like how do I transform the data or which regression method do I use. Or which regression method did I use? And so, some of the observations that came out of these studies is that one thing is the choice of the regression method actually matters very little. And almost everything else does. So, here's they looked a couple of the there's a normalization, which is on the on the left there. And you can see like the different regression methods are. See, like the different regression methods are really when you have the linear normalization, which is best in grade, they're all performing kind of pretty similarly. The penalized ones are maybe not as good. I actually don't know how they got a result because it seems like OLS is just like, you know, a special case of OLS defense. I don't know why they're different. But and then something like the choices you make for building your reference, which are represented here, like how many genes you're going to use. Again, that matters. Again, that matters a lot more than the actual regression method. So I guess this is just another kind of older result pointing this out. Okay, so it turns out actually, yeah, so this is another older result that's showing that, yeah, again, just the choice of reference. And now this is just not like how many differentially expressed genes, but like really like all of the steps of reference choice. So they have. Of reference choice. So they have, these are kind of like three built-in references that you can just use off the shelf for immune cell types. And they show that that really matters a lot more than about anything else that you can change in these pipelines. Okay, so it turns out that one of the most important things really is this like feature selection, like which genes am I actually going to fit in my regression? And it turns out regression methods really perform poorly when you use either a random set of genes. When you use either a random set of genes or all of the genes, I think this is kind of puzzling. Like, why don't we just use all the data that we have? So, typical brochures, you do some kind of differential expression. So, you really try to emphasize the genes that are distinct across the cell types that you want to deconvolve. But, you know, as your tissue becomes more and more complex and the cell type distinctions become more and more subtle, this becomes a difficult problem to solve. You might not have genes that perfectly differentiate your cell types. Differentiate your cell types. And I actually think, like, all of the work in this area came down to this very interesting method that purported to just solve all of these problems. This autogena has anybody use this for anything ever? No? Okay. But this is something that will just build you a reference. It will do the feature selection. That's all it does. It's a recently published paper. It was in a nice journal. All it does is do feature selection for this problem. Basically, they have a very complicated, like multi-objective optimization. It does actually do what it says. It builds a good reference. It takes a very long time to get there. I think this is a really interesting thing to think about. What is going on here? Why do we need so much work to just kind of select a subset of genes to do regression on? Sorry. So they want to, so basically, they want like things that distinguish cell types, but also avoid collinearity. I mean, go read the paper. I don't feel qualified to give a talk about it. But the point is, I just want to emphasize that this method was published recently because this is like such a difficult problem. This was considered a tremendous advance in this area. Okay. So. Okay, so what I want to propose, even though there's an old problem, so many methods, so many benchmarking studies, it's actually not solved, and we don't completely understand what's going on. So, for example, we don't really have a good theoretical description of what this like feature selection does and why it matters so much to the point that we actually need like a method that takes an hour to select genes. What is it even doing, right? So, another interesting observation is we have all these benchmarking studies that maybe even start with single-cell data. That maybe even start with single-cell data. They often produce a correlation of near one. So, this is like real proportions, estimated proportions, perfect fit. When you work with real data, this is rarely the case. And another, the third point I like to bring up is that for something like a highly optimized and very popular method like Cypersort, when you use it, the results are often really counterintuitive. It will give you Intuitive. Like it will give you an estimate for memory B cells, which is something I work a lot on, that's not correlated with the genes that are actually expressed in memory cells. So I kind of, I don't have, I can't prove that it's wrong, but I definitely don't trust it. Okay. So we decided to really get to the bottom of this and understand why there's like this kind of divide between what we get out of benchmarking and simulation studies and what we experience as users. And what we experience as users of these methods. So, and what we've come to realize is one thing there's a big problem with all of the benchmarking that's been done is that it really just, so here's an example. This is a medioblastoma data set. And so, like, this is like a single cell data set that you could potentially use to create your like fake bulk samples and do benchmarking. And one thing is you can color it. One thing is you can color it by measurablastoma type. This is one of these cancers where subtyping is like really well established. And you can see kind of the subtypes that look very different. And you can even color them by patients within the subtype. You can see that even all the patients condition on the subtype are actually quite unique. But when you go to build your simulation bulk data, these are all malignant cells. They're all one thing, and you just sample them randomly. So for your like normal simulation, this would be a sample. This would be a sample for your malignant fraction in your data. This represents no patient that can ever exist, right? Because you've mixed everything up together. And in particular, like you create this like fake malignant cell type that doesn't represent any subtype or any patient. It's just like an average of everything. So basically, the fundamental problem with this is that when you do these simulations, you're basically forcing them to the only systematic. Forcing them to the only systematic variance that they have is proportion variance. But in practice, that's not true, right? Like the percentage of malignant cells is not the only thing that's different between samples. Like the malignant cells themselves are actually also different. And that's just completely kind of averaged out. So we've decided to just fix this. And it's a really tiny little fix. And we think this is a more realistic simulation strategy. And basically, when Simulation strategy. And basically, whenever you pick like your malignant cells or your T cells or whatever, you're only allowed to pick them from a single biological sample. Now, you can mix samples together, and this way you can create more data than you have samples, right? So somehow, like in the simulation setting, what you want is create a lot more data than you actually have. So you can mix patients together. You can like pick malignant cells from one patient and immune cells from some other patient. So you actually create more combinations that way. But your particular But your particular cell is always restricted to that patient. Okay, and so we define these kind of classes of simulations. So we have the homogeneous simulation, which is kind of what everybody's been doing, like pick cells randomly. We have the heterogeneous simulation where we focus really, because malignant cells are really the most distinct because they have different genotypes. And so we only do the heterogeneous sampling in the malignant cells, and then we do a full heterogeneous sampling where it's We do a full heterogeneous sampling where it's always sample-specific for any cell type, even if it's of normal genotype. And so, one thing you can very quickly check that if I just look at the coefficient variation in the genes in the simulation versus what I get in pseudo-bulk, which is by pseudo-bulk, I mean really putting the samples back together into bulk. So, no, no simulation, no mixing up, just putting exact samples back together. So, these are all have kind of low. So, these are all have kind of low variance relative to what you would want. And this R simulation is kind of close. So, this is another view, same point, but another view. So, we're stepping through kind of more and more realistic simulations. Then we have the pseudobulb. And then, finally, just for comparison, we have like a completely different data set. It's the same disease, but this is the TCGA. So, we actually have a lot of samples. And this is just. Of samples, and this is just the variance now across some kind of pathways of interest in cancer. And you can see that the heterogeneous simulation is almost all the way there to real data, not all the way, but it's very, very close. Okay. So now that we have this new simulation in place, we've decided to go back and kind of revisit some of these things, like which of these methods work better. I mean, for example, I mean, for example, the reference-free methods seem really like magic. Do they even work in this in the heterogeneous setting? Regression-based methods, obviously, everybody thinks Cyposword is great. Is it actually... So we've decided, we've put these methods into their respective classes. So we have reference-free, regression-based, marker-based, and we have this extra class, base prism. Anyone heard about base prism? It was very recently, just recently came out. Came out. Okay, so lots and lots of work later. This is like a tiny summary figure. And basically, all I want to, so over here we have our simulations. And, you know, as you move from left to right, they get more and more realistic. And we tested all of these methods, but in the bottom, I'm just here showing best in class. And the base person is its own thing because. Base person is its own thing because there's only one thing in its class. And as you can see, so for reference free, which is red, these drop off real fast. Once you create a realistic simulation, the simplex structure becomes almost impossible to find. So these really don't work. The regression-based methods, which is the green, also drop off once you're in like the realistic space. And actually, Marker methods are quite robust to the realism introduction, interestingly, even though they use less information. Even though they use less information, but base prism turns out to be very, very good. So, what are we telling the discussion about this correlation? This is a correlation. We also do like RMSE and it's the same. Yeah, the trend is the same. I actually don't have a lot of time. Like, I don't have a lot of time. So, one thing, just right about when we posted a preprint about this, this other preprint was posted, which came exactly the same conclusion: that this new method, Bayes Prism, which is Bayesian, is really best. And in this paper, they had actually some kind of like, it was a very realistic setup, but they're doing a large cohort study, but they just did some single cell in parallel. So they could really just do benchmarking. And so, this is different real to estimated. And base prism is yellow. Base prism is yellow. I don't know if you can see, but it's better. And okay, so what is this thing? It's this new magical thing. It's in its own class. What does it do? This is a paper. This is cell cancer paper, 2022. And so this is what it does. It's really a prism, right? You have your single sample comes in. It has a bunch of read counts for all of your genes. And it's going to split this into like 20 different vectors of cell type-specific gene expression. Of cell type-specific gene expression. Okay. And kind of you get proportions as a side effect, and the proportions need to be like way more accurate than anything anybody can get with any other method, which we, they, you know, they postulate, and then two separate independent studies can confirm that this is true. So, um, what does it do? This is, it's MCSC inference, it's very slow. Is it Bayesian magic? So, um. So turns out it's not Bayesian magic at all. And in fact, you can totally derandomize this algorithm, which we did. And you can, these are the convergence traces from their method, which is based on sampling, our method, which is completely just has closed form updates. You can see that they're actually exactly the same. And the final results are also exactly the same. Our method is about 400 times faster than their method. We're using exactly the same results. Exactly the same results. That's the timing there. The actual algorithm turns out is really simple. And really, all we did was just like for every single step, we just derandomize it. Like instead of taking a sample, just take the mean. And that was it. And really, it's just a bunch of multiplications and divisions. There's like almost nothing to do. And I guess what very short on time here, but one thing I like to point out is that when you have like, I like to point out is that when you have, like, it is a really great method. We can talk about what the magic sauce really is, and it's not the sampling, and it's not the Bayesian part. So, I'll give you a little teaser. But the fact that we now have a fast one really allows us to do lots of things. Like we can do deconvolution across like 50 different cell states for all of TCGA just in a laptop in a few minutes. And so, we looked at some like kind of metabolic profiling as part of a hackathon. On, um, and um, turns out you know, the metabolism of your TME cells actually is kind of uh predictive of various clinical outcomes. And because I'm short on time, I'll just go straight to acknowledgements. So, all this work was done by Meng Yi Hu, who's a second-year graduate student in the joint CMU and University of Pittsburgh Computation Biology Program. And I'd like to thank Casey Green and his graduate student, Ariel, for discussions because we had. Student Ariel for discussions because we had these two preprints that were very similar. So we kind of strategized after that. And I'm happy to take questions. Yes. Sorry? Are they using the reference reference space? Yeah, they're reference-based, and generally, both methods assume that your study is done in this way where you have some parallel single-cell sequencing. Like you're doing a large cohort study with some, and so it will build the reference. You give it a single cell data and it's going to build the reference from that. But yes, we're working on compiling some built-in references for cancer that are cancer specific. So that's. Cancer specific. So that's going to become part of the Instaprison package. Use base prism or instapism. Use Instaprism. Okay, I agree that we see. But in our experience, they are very good at predicting the proportions and concentration for Yeah, I don't I think the reference-free problem is kind of unsolvable in in the face of other sources of heterogeneity, right? Because when uh if you simulate data and you do a PCA, it actually you can see simplex structure in it, but if you take real data, try to find the dimension in which it looks like a simplex, it's like it's impossible to find. It's like it's impossible to find. And so, yeah, I don't think these methods can work in general, I guess. All right. Yeah. I feel like I started late. So maybe I should hit an extra. Yeah, that's a really interesting talk. I don't really know about this area. The first question I had was: so when you have a slow Gibb sampler for something like LDA, I feel like a lot of Bayesian-minded people, their first choice to kind of, if you don't really care about Choice to kind of, if you don't really care about that whole posterior, you just want a quick map, like point estimate. One might try something like variational inference, right? I mean, a lot of the LDA models, so it's like you still have, it's not really de-randomized, but you're kind of changing it to a fast optimization problem to just get one estimate. And in some sense, it at least has some guarantees that it's the closest thing to your original posterior. So, do you have an idea of like how, because like you derandomized it one way, you chose the choice of mean, but not everything is linear. So, do you know? But not everything is linear, so do you know how that might compare or like how well you're doing randomization? Because if you want, um, well, so as far as we know, like the results are equivalent across lots and lots of the point estimate results are actually equivalent across anything that we've tried. And once you're at the kind of final convergence point, you can do whatever you want at that point to you can like start doing sampling again if you want a distribution, right? It's just like you just get there much faster. It's just like you just get there much faster, basically. But actually, base prison is an LDA type thing. So good guess there. No, I mean, you said, yeah, I just ask you a couple more after. And then the other quick question was just on the slide of saying that the regression doesn't matter, like statisticians kind of get scared when you say the choice of regression doesn't matter. No, okay, let's go back to that. There's some subtle things. I just mean to ask, like, what is the, is the setting here? Is P greater than N, for instance? Is p greater than n, for instance, like is the features greater than the number of samples? Oh, okay. Um, yeah, and then the other thing is like, you know, when you have hinge loss, I think that operates on like classification, you know, it's like you take different inputs, why, for some of these different losses. It's like the regression equivalent of hinge loss. It's actually, I've got intensive loss, yeah, I had to correct it. Sounds like I just get to find you after your talk. So, okay. Yeah, it's actually, it's not like it really, really doesn't matter. So, uh, side by sword, which is the Uh, cyber sort, which is the like um kind of the L1 type regression, um, it's a little better, it's a it's supposed to be more robust, and it is, but it's it's just it pales in comparison to all of the other choices that you can make. That's but yeah, it's actually on this plot, it's kind of clear, yeah. Okay, um, yeah, um, one of the reasons why. One of the reasons why using all of the features turns out to be bad is that typically people like to standardize them so that all of the features get an equal voice. But power is really about expression levels. And so when you standardize features, right, you have 100 out of the 10,000 features that are informative. You now take the other 9,000. The other 9900 and make them look just like the informative ones, they're indistinguishable, and then people get surprised that they can't actually cluster because you've just taken all of your information out of your experiment. So, but this is not clustering. We're assuming this regression problem. And the fact that it doesn't work on all the features just tells you that it's the problem is misspecified. I disagree. Because if the regression problem is correctly specified, adding more data should help. Wait, statisticians, back me up. Statisticians back me up. Come on. If I simulate data exactly according to the model that I'm going to fit, would you like to have less data or more data from my simulation? It's expensive. But no, it's not. It doesn't matter. It makes it worse. It's worse. You have features that are really informative, and you have features that really aren't. And if you take a step in your analysis, And if you take a step in your analysis where you take the ones that really aren't and make them distinguishable from the ones that really are, you're going to pay a penalty for that. It's not a you didn't give me more data. You gave me more things, but those things aren't informative. Well, but I'm this is like a completely like toy example. We could actually do this. I simulate data according to a particular regression model. Now, you're going to fit exactly according to my model. Would you rather have less data or more data from my simulation in order? Or more data from my simulation in order to fit the parameters. No, they're not covariates. Oh, no, no, no. There's no covariates. No, genes are, in this case, we're fitting the genes. The genes are just points. They're not covariates. Okay. Well, anyway, this is my take on why it doesn't work. It's right here. I kind of glossed over the slide. Work, it's right here. I kind of glossed over the slide, but it's the regression model is actually not well, like it's an incorrect specification for the problem, and everything we do is kind of just trying to fix that, basically, that we have like a kind of a wrong model for the problem. Okay. Yeah, I think we can continue later. We are going to have more time for discussion. We have two discussion sessions. Yeah, I also have questions about that because we talk about features in the lecture. About that, because you talk about features collection, so now I got confused. But we can talk later about that. No, no, you were great. Yeah, no, you were great, and we will have two discussion sessions that I