So this talk originally started as a hodgepodge talk but has been gradually been shortened for two reasons. First, to let Khan give his presentation, but also because I've spoken about the other two halves of this talk during discussion sessions, so I don't want to subject everybody to death again. So what I'd like to talk about today is actually work with what I originally did in this area and what got me excited about this sort of like Hodge theory, which was applications of the discrete Hummel's Hodge decomposition to non-equilibrium steady states. This was work that I did as a graduate. Was work that I did as a graduate student and was sort of a cornerstone of my doctoral work. So these are sort of collaborators who've been involved in this process. So Peter was my doctoral advisor. Karen, who's here, was involved through all of this. There's a decent amount of physics in this. That was sort of all inspired by Mike Kamchemski at Case Western. And then this whole approach was originally inspired by the paper that many of us have seen from Lekhang Lim, who I've worked with now, and things like the randomized sort of podge approach. That's something I'm doing with him now. That's something I'm doing with him now in Chicago. So, with that introduction, let's sort of jump in. Big picture, I think a lot of the work that I've done is in some ways trying to discuss some interplay of structure and networks in different systems. And this is sort of an example of that. What I'll be doing, I'll be considering applications of a tool borrowed from topology to try to better understand random walks on networks. So, as an example of trying to use something structural, in this case, this tool borrowed from topology. In this case, this toolbar of topology to do something about dynamics, about the random walk. And the tool here, nobody's surprised, is going to be talking about sort of the discrete helmet-logged decomposition. And the application I'm particularly interested in here are non-equilibrium processes. So these are Markov processes whose steady states retain some non-zero circulation once at steady state. So let's start by trying to pose a Markov chain. Markov chain in a way that we can both talk about its thermodynamics and then try to tie its parameterization back to some sort of like Hodge type story. So we'll start here with sort of something not too surprising, just a discrete space continuous time Markov process, say capital X of T, walking on a network, say we're in continuous time, so we'll think that this is something that transitions with exponentially distributed waiting times. And there's many different reasons to look at models of this kind. You can think there's all sorts Of this kind, you can think there's all sorts of example models that come as the form of a discrete space continuous night Markov chain or approximations to other models. So maybe we're thinking systems of chemical reactions with small particle counts, or thinking some signaling process with signaling channels. If we're thinking maybe ecology, we might think some birth-death processes, or maybe we're thinking epidemiology, where you're transitioning between infected and recovered in these different states. So there are all these different models that are some sort of continuous time discrete state Markov chain. State Markov chain. Now, what I'm interested in in particular will be for this talk is the steady state distribution and the steady state probability fluxes of this chain. Now, from the outset, you may say, well, what is there new to do here? You define transition operator and you find is null space, and you're done. And that's certainly true. Now, in one way, something that you can argue that's maybe new to do here or is a standard challenge is that there are many cases where you have Where you have a discrete space continuous time marker process where the state space is huge. The state space grows maybe combinatorially in some number of inputs, and you're cursed by the cursor dimensionality. So you may be in a situation where it's true, you could look at the null space of a matrix, but the matrix is ginormous and gets so big that you can't handle it. So that would be one challenge. That's not the challenge I'm interested in today. The challenge I'm interested in today, it's maybe also not a surprise from the discussions we've had this week, is that I am generally not satisfied by linear algebra. Satisfied by linear algebra from an interpretation perspective, if you are forced to turn a crank that you can't interpret. And I think that one of the interesting challenges when thinking about a discrete space continuous time Markov process and asking about its steady state distribution or the fluxes which control the rates of observables is can we say anything about these distributions without solving for the null space by building a matrix and maybe doing Gaussian elimination? Can we say anything about it by tying into some physics of the problem? Into some physics of the problem. And there's a good reason, at least naively, to hope that this might be true, which comes from classical thermodynamics. That the steady state of systems who are energetically closed is ultimately Boltzmannian and is described just by defining an energy of the state. And so there's this really beautiful tie between energy and steady state probability. And that's something that breaks down in non-equilibrium processes. And so we sort of are aiming for the same thing. We're aiming to find a way of talking. We're aiming to find a way of talking about steady-state distributions and fluxes that's driven by some interpretable physics. So let's parameterize some things. So we'll let pi of t be the probability that the system x takes on the state i at time t, and then we'll define our transition rates. So we'll let l sub j i be the transition rate from state i to state j, so I don't need to transupose my transition operator. We will make one assumption, so this is why I sort of start my initial slide for Markov processes. My initial slide for Markov processes, which we'll make a reversibility assumption. We'll assume that if Lji is greater than zero, then Lij is also greater than zero, which says sort of that for every forward reaction, there is a backward reaction. Now, these rates may be totally different from each other. In fact, that's where everything interesting happens. But we will assume that if there's a directed arrow in one direction, there is a directed error coming back. That's motivated in the physical system by microscopic reversibility. That physical laws, in some sense, are time-reversible at At some level, which means that all processes that can happen forward can also happen backwards, maybe with very different probabilities. Next, we can talk about fluxes. So this was something that came up in Dane's talk. So a flux is just looking at the difference in the rate of probability flow across an edge going forwards and backwards. So j sub i j, capital J here, will be the flux from i to j. And so that's the flow Lji times Pi minus Lij times Pj. Minus Lij times Pj. And you can think that the overall rate of change in P will be the divergence of these fluxes. So immediately you think that, okay, like a steady-state condition would be to say that the divergence of the flux is zero. So you know right off the bat that steady-state fluxes have to be divergence-free. So they have to be circulating in some sense. More strongly, if we're talking about a continuous time discrete state markup chain with exponentially distributed waiting times, then the rate of change in this probability distribution is just given by this sort of simple. Is just given by this sort of simple linear equation, right? The master equation, the rate of change in P, is my transition operator, all times P. Is the option of reversible and the flux is zero? No. No, this is a key point. So having this flux be zero is tied to what's called detail balance, right? And this has to be being energetically closed. So what you need to be. Oh, it's not the steady state, right? Right. Steady state, right? Right, so steady state says that the divergence of the fluxes is zero, right? Not that the fluxes are themselves zero. So the net flux into or out of any node is zero, not that the flux across any individual edge is zero. Yes, and that's a reasonable confusion because detail balance. Fusion because detail balance comes with time reversibility, which is why this is sometimes called reversible. This is reversible in the microscopic reversibility sense. Do you want more clarification here? If you ran the process backwards in time, would it be the same or it could be different. It could be different. Yeah, and that's and that's so the interesting case here will be the case where it's different. So this is not time reversibility. Time reversibility. This is reversibility in the sense that for every forward edge, there is a backwards edge. Flux is a function of time. And yes, the flux evolves over time, as P evolves over time. So then, if we're thinking about steady state, under the assumption of microscopic reversibility and the assumption that your graph is connected, then this process is ergodic. And so this is sort of classical, this is a linear system. System, it will evolve to a steady state, which is a particular eigenvector, a null vector L, the chrome for Benius eigenvector, and it will converge at rates controlled by the eigenvalues of this operator, right? So we talk about mixing times. And so we can guarantee that P of T will converge to a unique steady state distribution Q from any initial condition. Now, this whole conference is about doing things with Hodge theory. So we'd like some sort of alternating function on the edges. We'd like some sort of flow. Alternating function on the edges. We'd like some sort of flow. And I'm going to pick a very particular edge flow here. I'm going to define my flow, f sub i j, to be the log ratio of forward and backward rates. This is part of the motivation for requiring that things are reversible, right? If we don't satisfy microscopic reversibility and one of these rates is zero and the other is non-zero, then the logarithm blows up. Period is not like that. We're in continuous time. Yeah, so in continuous time periods, yes. Yeah, so in continuous time periods. Yeah. So here, then you can think: all right, what does this edge flow tell us? This edge flow is essentially telling us sort of the asymmetry of these rates, right? It's the geometric difference in these rates. So the sign of the edge flow tells us whether the forward rate is faster than the backwards rate, and its magnitude tells us the degree of this difference. In particular, one of the other nice things to notice here is that edge flow is unitless. So we're talking about a unitless edge flow. Now, the reason for introducing this is Reason for introducing this is that in physical processes, it's often true that this log ratio is tied directly to the work required to transition across an edge. So what is true, not for all processes, but for many physical processes, it's true that the ratio of two forward and backward rates is exponential in the work required to make that transition scaled by some temperature. Which means that if we're sort of thinking about a generic Markov process model that has microscopic reversibility, Model that has microscopic reversibility, we're going to be able to construct a phenomenological thermodynamics by studying this log ratio. Because if we study this log ratio, we can think that's analogous to work in some physical system whose edge flow is tied to this, the work in this fashion. So this doesn't require that there is necessarily some specific physical system and you're recovering exactly the work in this fashion. Just that if you have some Markov chain, it satisfies microscopic reversibility by looking. Microscopic reversibility, by looking at the log ratio of forward and backward rates, you can construct an analogous thermodynamics that runs along with that process. If we've done this, then in some sense we've sort of reparametrized our Markov process. Because now we're starting from the asymmetric component of a pair of numbers. We're starting from that work. Of course, now to recover the actual transition rates, you're also going to need a symmetric component. So let's think about parametrizing our transition rates in terms of both the geometric. Transition rates in terms of both the geometric difference and the geometric average. We could define something that we might call sort of conductances, which would be the square root of the product of the two transition rates. And the nice thing here, these conductances, these keep the original units. So, right, the edge flow is unitless. These conductances keep the units of one over time. And then this is sort of a symmetry, asymmetry decomposition of my transition rates. Now that I have an edge flow, so you can think I've started with, in some sense, I've started with transition rates, and I've broken them into sort of time constants. I've broken them into sort of time constants on the edge and something measuring the asymmetry of the rates. I could further break down this flow using the discrete humbles of edge decomposition. So you could think that we can further decompose F into something that's associated with a gradient subspace. So you can imagine defining some potential, something that's analogous to internal energy, and take its gradient. Here, this is maybe abusive notation for this conference. I usually use G for the gradient because it's often something I speak about. Because it's often something I speak about to physicists. The idea of using B for the gradient seems very strange. And then there'll be some sort of circulating component left over in the null space of the divergence, which I could express in terms of, again, this is often I present this to people with a physical background and you think like how much decomposition I want at vector potential. So you could express this as some sort of curl transcript applied to something like a vector potential or a set of vorticities. And so now really I have a new way of parametrizing my Markov chain. I start with Chain. I start with some notion of internal energy and some sort of circulating component, some vorticities. Those combine to define a flow, which represents work across edges, and then each edge has some sort of time constant you can think like conductance or resistance associated with it, and then that determines the transition phase. Now, yeah, let's get to this, the reversibility language and all these paradoxes of sort of similar language. So, lemma, a discrete space continuous time marker of process on a finite Continuous time Markov process on a finite network satisfying microscopic reversibility also satisfies detailed balance if and only if one of these four things is true. So first, at steady state, so P of t is equal to steady state distribution of Q, the flux over each edge is zero. And so this is where the name detail balance comes from, is because you're balanced in a detailed sense, you've balanced the flux on every edge. This is equivalent to, and this is the problem in the language, it's equivalent to time reversibility. So if the process satisfies detailed balance, then you can't tell. Satisfies detail balance, then you can't tell forward trajectories from reverse trajectories. In terms of the Hamilton language, this is equivalent to requiring the edge flow F is conservative. And this is sort of the beautiful connection here, is that detail balance, which is sort of a deep physical property of a system, in the Hamilton language, is exactly restricting f to the conservative subspace. It's requiring that f is a gradient of some potential function. And this is especially And this is especially nice from sort of a computational perspective if you want to do averages, because this potential uniquely specifies your steady state distribution. So if you're in detailed balance, and actually only if you're in detailed balance for the most part, then your steady state will satisfy Boltzmannian relationship with this potential function. And you can think this is great for ergonomic processes, right? Because time averages are the same thing as space averages. So if you take the long-term. Averages. So if you take the long-term average of some process, you can express what that'll look like entirely in terms of energy here, because if it's both one. Now, where this gets interesting is these processes, these equilibrium processes, in some ways these are fully explainable, right? And this is entirely interpretable. Your steady state doesn't require pushing through some linear algebra black box. If you know the energies, you know the steady state immediately. You don't need to actually do anything with the transition matrix. With the transition matrix. None of that is true once we break equilibrium. And so this is what in physics is called a non-equilibrium system. Again, this is some sort of confusing language. Non-equilibrium is not the same thing, or equilibrium is not the same thing as steady state. So all of these processes have some steady state distribution. They evolve to. Equilibrium is your steady state Boltzmann thing. So energetically closed systems will fall into this category, but say you're motivated by biophysics. Say you're motivated by biophysics and you're interested in the physics of like living matter, then often you're interested in processes that are not energetically closed. And almost by definition, if you're interested in the physics of living matter, you're interested in matter that does something, it uses energy to perform a function. And in that case, if you're looking at active matter, you almost always want to be out of the setting. You want to be in the non-equilibrium setting. And the non-equilibrium setting is just much harder. First, okay, in the non-equilibrium setting, my edge flow is not restricted to living in the conserved. Flow is not restricted to live in the conservative subspace, so there's some rotational component, which means the work to cross an edge is not just a change in potential. It's not just a change in internal energy. There's some external driving that can generate a rotational component of the edge flow. And that rotational component, you can think of sort of forcing probability currents that will drive probability currents even at steady state. And so there's some coupling between these driving sources of rotation and realized currents, realized currents at steady state. Currents, realized currents is steady state. And a reasonable question is: how? What is that coupling? Moreover, because steady state probability fluxes don't vanish, and this process is ergodic, that means that the long-term expected production rate of some observable will be controlled by these steady-state fluxes, and that observables can be produced at some rate over time, like entropy production. And really, I mean, why this is often really interesting in a biophysics setting is that all of this has to couple to an external energy. Is that all of this has to couple to an external energy source? Because in order to maintain this non-equilibrium steady state, you need to be continually using energy. As far as the math problem goes, let's think about trying to find our steady state and steady state currents. We have this beautiful equation in the detailed balance setting where your steady state is just a Boltzmann distribution. But all of that falls apart in the non-equilibrium setting. If I have a non-equilibrium process, then the steady state distribution is not. Then the steady state distribution is not just e to the negative potential energy squared, or potential energy. And your steady state currents are non-zero. Now, in general, you can solve these with some sum over spanning trees. There's a variety of constructions to do this that are sort of each beautiful in their own right. But there's nothing that approaches Boltzmann, right? There's nothing that approaches the beautiful physical interpretability there. Now, if you're familiar with this area, you might say, like, this is hopeless, don't try it. This is something that's sort of a classical problem. Classical problem. But I think this is actually an area where the Helmontology composition does quite well. I think this is a particular problem where these tools, and this is almost the simplest application of these tools, we're only looking at the gradient and the null space of the divergence, are the right language for talking about this problem. So the second half of this talk is going to be devoted to trying to show that. In some sense, there's a way in which this decomposition does something fundamental in this setting. So, okay, we can't do our steady state generally, but we can do it at equilibrium. So, maybe we can solve for our steady state or approximate our steady state perturbatively about equilibrium. So, we'll look at processes that are close to equilibrium. This is what's often studied in terms of what's called linear thermodynamics. That's a regime. You're thinking, you do have some driving rotation, but the driving rotation is weak. Well, it's necessarily true that steady state, the rate of change in my probability distribution is zero. So, stationarity requires that my So stationarity requires that my transition matrix times q is 0. So q is the unique normalized non-negative null vector of the transition matrix. Now in detailed balance, we know this is proportional to something that's Boltzmannian. Outside of detailed balance, we don't know what Q is, but we'd like to figure out how Q depends on the conductances in the work, the edge flow, or more generally the conductances in these internal and external energy sources. Again, okay, we can't do this generically outside of detailed balance. Can't do this generically outside of detailed balance. Let's do this when theta is small. Let's do this when f is close to conservative. Now, to get there, it's helpful to simplify the problem. And one of the ways in which we can simplify the problem is here, generically, we're starting with something where f is neither entirely in the gradient subspace or entirely in the signal subspace. But it turns out that you can always trivially get rid of the gradient component, dependence on internal energy, and you can do Dependence on internal energy, and you can do this just by essentially changing coordinates. Instead of talking about probabilities, like probability that you're at a particular state, let's talk about probability you're at a particular state relative to what the equilibrium distribution would be at that state if there was no rotational component. So if there was no rotational component, everything would be governed by your internal energy spi. And so I could measure, you think, in some sense, probability relative to the corresponding equilibrium distribution. To the corresponding equilibrium distribution. And if I do things in this way, well, you can easily transform your transition rate and get a new matrix called L hat and have new sort of, these aren't exactly probabilities, these are sort of probabilities relative to equilibrium p hat. And what you're left with is still a transition operator, and still a stochastic matrix. But it's a stochastic matrix where the work component is only the rotational component of the original matrix. So by scaling by these equilibrium problems, So by scaling by these equilibrium probabilities, you drop the whole conservative component of the problem. Which means that if we want to generically solve for a non-equilibrium steady state, it would be equivalent to be able to solve for non-equilibrium steady states when the process is entirely rotational. Because if it has some non-rotational conservative component, you can always scale that away or bring that back in by scaling. And so here, what we'll do given a generic process is we'll start by doing this scaling, which replaces F, the flow, with just the rotational piece. The flow with just the rotational piece, and updates the conductances by sort of multiplying by this combined factor of the equilibrium distribution on either end. Here, this is actually a nice notion, this conductance. If you take one over conductance, that's like a resistance, right? This resistance, r sub ij, is one over the rate at which probability flows from i to j at equilibrium. So once I've made this sort of change in coordinates, you can think of resistances are really, they're just looking at what is the rate of exchange of probability at equilibrium. Take one open. Of probability of equilibrium, take one over that, that's your resistance. So now let's start expanding. So, okay, we said we're imagining a situation where rotation is small, and we want to do a perturbation analysis in weak rotation, which means we're going to introduce some parameter in front of the rotational component, and we're going to let that parameter go to zero, and we're going to try to expand around that particular parameter. Here, I've used beta. This is often the language used for inverse temperature, the motivation for beta. If we perform this expansion, we have three things to expand. We need to expand. Three things to expand. We need to expand our transition operator. Well, the transition operator is defined in terms of like exponentials here, so that's totally nice. You can expand it with a Taylor series. We also will try to expand our steady state as a Taylor series here, and if you want to talk afterwards about how we know that it's valid to try to Taylor expand this, we can. And at the same time, we'd like to expand our steady state fluxes. We'll try to simultaneously solve for both the perturbations of the steady state and the steady state fluxes driven by the perturbations to the transition operator. To the transition operator. Well, it's necessarily true that at equilibrium, or at steady state, stationarity applies, so L of beta times Q of beta is zero, which if you then write out in terms of expansion in group terms, you end up with a recursive sequence of linear equations, which says that your zeroth order steady state, here, this is just the uniform distribution, because I've scaled away the energy component. So what is the equilibrium distribution for a set of states with equal distribution? A set of states with equal energy, plus, like, right, it's uniform across that state for a default balance. And then we're left with a series of recursive linear equations that produce the corrections. So here, grouping terms, we get a sequence of linear systems where on the left-hand side, we have the transition operator when beta is zero times my nth order correction to steady state is equal to some sum over lower order perturbations. Lower order perturbations to the transition operator and lower order perturbations to the steady state. And the key thing to notice here, outside of the recursive structure, is that this operator stays the same at every stage. Even if you wanted to solve for these perturbations at every single step, you'd always be solving a linear system with respect to the same transition operator. It's the transition operator at equilibrium. And that's actually sort of a fundamental. Is there a minus missing there or? Minus missing on. Yes, there is. Yeah, you're absolutely right. Yeah, there should be a minus outside of the sum. Okay. So it turns out, let's look at the first order term here. Let's look at the first term in this expansion. It turns out that first term, if I wanted to solve for Q, could be posed exactly as a weighted Helmet Hazd decomposition, coupling the perturbation of the steady state to perturbations of the steady state currents. And the reason that's true is this L0, this transition operator is. L0, this transition operator 0, this is just a gradient times a diagonal matrix of 1 over the resistances times a gradient transpose. And so you can think that this is like a Laplacian, it's like a weighted Laplacian weighted by the resistances. And any time you have sort of a discrete Poisson equation with respect to Laplacian, you can view that as the normal equation corresponding to Helmholtz-Fashi-Kalpovich. At first order, the way this reads, it says that the negative gradient of the first order The negative gradient of the first order correction of the steady state plus the resistances times here, this is the first order correction to the fluxes, is twice the driving rotation. And so there's a number of sort of like qualitative and physical stories to emphasize here. One is that there's a trade-off, right? The more of the rotational component I can absorb into the steady state fluxes, the less my steady state is actually perturbed. And the more my steady state is perturbed by these driving rotations, the less efficiently I drive flux. By flux. This is actually something we can do more specifically. Let's define some coefficient. Alright, so these are our first-order corrections. There's some exchange between perturbing steady state and driving flux. Let's introduce a measure of efficiency with which we drive flux. So the thermal efficiency you could define as the ratio of entropy production, that's realized current, to what you might hope you could drive for this rotational model. This quantity eta squared, this is nice, this is dimensionless, it's between Nice, this is dimensionless, it's between 0 and 1, and it equals 1 if the resistances are uniform. So, if the resistance is the same across the whole graph, when I drive current with some rotational component, I'll just get back essentially exactly that same current scaled by the shared resistance. And so this is sort of a nice notion of efficiency. All right, let's come up with a coefficient that describes the other half of this, the perturbations to steady state. Well, why is steady state perturbed? To first order, steady state is perturbed because I'm trying. First order, steady state is perturbed because I'm trying to drive rotation, which so you can think of driving some little initial probability currents. But if the resistances are not uniform, it's not equally easy to drive current through every edge, which can lead to bottlenecking, where the divergence of those realized currents is non-zero. So that's going to try and either increase or decrease the probability at some location, and that will have to be balanced by diffusion. So what I could do, I could look at the other half of this equation. Half of this equation, which here this is L0, this is gradient transpose or divergence of resistances times gradient of correction of the steady state, has to equal this sort of flux that I'm trying to generate with the rotational concomitant. These are, essentially, this is what I'm trying to perturb away from. This is the diffusion that has to balance it. So what I might define is some coefficient that measures how much this bottle acts, right? Measures how much this thought lacks. The larger the divergence of this term, the more those currents I'm trying to drive pile up or leave from one node. And so the more they're going to try and push me away from my equilibrium distribution. And the degree in which that has to sort of balance will be with respect to this transition operator, right, this sort of diffusion operator. So I could define the bottleneck and coefficient, which will be the ratio of this divergence, the divergence of the rotational fluxes. Of the rotational fluxes divided, rotational work divided by resistances. And I'll measure that with respect to the pseudo-inverse of this transition operator. And just like the coefficient I defined before, this is dimensionless in between 0 and 1. And there's situations where you can push this close to 1 or push this close to 0. Now what's really beautiful is that these two things are complementary. The thermal efficiency squared plus this bottleneck and coefficient squared is equal to 1. is equal to one. And if you measure the degree to which I perturb my steady state, so this is sort of measured with respect to like the energy norm in G. If I measure the degree to which I perturb my steady state, that will be bounded by this bottlenecking coefficient times the intensity with which I drive rotation. And so this shows us in a very quantitative fashion that there's this exact exchange between how efficiently I drive current and how much I perturb my steady state as controlled by these two coefficients. Coefficients. So that's pretty beautiful. That says there's some sort of real physics here. And you can think the reason I get a Pythagorean theorem has to do with the orthogonality of these subspaces. Let's see what else we can see in first order. Well, maybe we're interested in long-term production rates of observables. So that's you have some, you can watch transitions for each transition. It changes some number. And so you track over the process. You've got some number that's changing driven by the transitions. You take its expectation over a long time. You could look at the rate at which that changes. Over a long time. You could look at the rate at which that changes. The long-term production rate of an observable is going to be controlled by the curl of my steady-state flux. And actually, this is something that if you look at the cycle flux theorem, this is from Hill, this says that the counting rate of cycle completions will converge to these cycle fluxes. There's a sense in which this will predict the rate at which any observable is produced. These cycle fluxes are directly related to the rotational potential at first order by some matrix M. There's some matrix that transfers from the Matrix that transfers from the driving rotation theta to the realized rotation around any loop, the curl of J. And if you've sort of studied your linear thermodynamics, you might recognize that this is the type of setting where we see Ansager reciprocity. This is an idea that Ansager actually received the Nobel Prize in part four. And this idea is essentially that there's going to be a symmetry between the ways in which we drive rotation and the realized rotation of fluxes. In particular, M is a symmetric matrix. This is the reciprocity here. This is the reciprocity here. And this is, in some ways, a surprising thing. Because this says that the way in which driving rotation is coupled to flux is symmetric under exchanges of the two loops. So if I think about like the ij entry of m, I could look at the i-th loop and drive current there and then measure the observed flux on loop j. Or I could do the opposite experiment. I could drive current, I could drive flux with some rotation on loop j and look at the realized flux on loop i. And the astonishing thing Astonishing thing, at least to first order, those couplings are symmetric. So it doesn't matter how those loops are related to each other, it doesn't matter their sizes, it doesn't matter where they are in the graph. The amount to which driving rotation on loop I generates current on loop J is exactly the same as the amount to which driving rotation on loop J generates current on loop I. And so these coefficients, these are sort of famous. You can prove some standard things about them, like there's inequalities between the Inequalities between the diagonal entries of this matrix and the off-diagonal. And you should sort of think in some sense that if I can effectively drive rotation on one loop, I can effectively drive rotation on another loop, that there should be some ways in which those two numbers bound the degree to which I can drive rotation by driving on one and observing on another. So you can get some nice balance here. And the other thing is you can establish this is independent no matter how you define your cycle basis. So if you build a curl operator explicitly, the values on any off-agam entry or something. On any off-diagonal entry are so physical of the problem, they're not tied to some choice of cyclopacis. Okay, so we could stop here, and this would just be the first-order expansion. And if this was all, then in some ways, we wouldn't have too much new to say. What I think is actually astonishing and is worth stating goes back to this statement about the structure of this recursion, which is that this matrix never changes. Because this matrix never changes, it means that every single time I have a linear system of this kind, Every single time I have a linear system of this kind, I can think about it as a discrete Poisson equation. Which means every single time I have this equation, I can think about the q's as a solution or one component of a helmetology composition. Which means that actually, if I go for the entire expansion, every single order of this expansion is governed by a weighted helmetology composition. So it's not just first order, it's every order. The negative gradient of the nth order correction of the steady state plus the resistance. Steady state plus the resistances times the nth order correction to the currents is going to be equal to some term that comes from combining the driving rotation and the previous corrections. And if you like, afterwards, I can go through this as sort of the messy side. But the key takeaway is that this side stays the same. This story about perturbing stasi and perturbing flux and their coupling is exactly the same every time. You just recursively update the right-hand side. Now, you might say, okay, that's great. You can write down the Taylor series. Does it converge? Down the Taylor series, does it converge? Is this a valid thing to do? And this was maybe the trickiest thing to prove. The answer is yes. So, moreover, this power series expansion defined recursively in this fashion always has a finite non-zero radius of convergence. You can come up with bounds for this radius of convergence in terms of the intensity of driving rotation, things like the maximum degree of any node in the network, the condition number of the inverse resistances, and say smallest non-zero singular value of the unweighted graph possible. That's possible. Now, to help convince you of this, I'll just show you an example. So, this is an example for a three-state network. Here we're looking at blue is the path of the true steady-state distribution as I change beta, and magenta is like first-order approximation, second-order approximation, and so on. And here we can see this Taylor series generated by recursively performing HHD converging down onto the blue curve. The other thing you can see here is that it does not converge everywhere. This Taylor series, we're This Taylor series works out to here where I've marked with blue, that's beta equals one, and we do very, very well up to one, and then at one or past one, the power series fails to converge. This is something I think there's good physical reasons that you can argue why we actually shouldn't expect this Taylor series to have a, to converge everywhere. It has to do with whether your process is diffusion driven or sorry, diffusion dominated or drift-dominated. Dominated or drift-dominated. And that's a discussion I'd be happy to have afterwards. Now, a question for this conference is: well, okay, we see this thing, we see that it seems like this Taylor series converges provided beta is less than one. That doesn't seem like it should be a coincidence, and we can test that in some numerical examples. But we haven't been able to prove it. If I go back to the lower bounds, I was taking these lower bounds for large matrices are very small. So that bound of what you expect. So that bound on which you expect your convergence, or you can guarantee your convergence to happen, is much, much smaller than what we're usually observing in practice. So this is sort of a conjecture or maybe motivation for some research work is can you push this radius of convergence larger? Can we get tighter bounds? And that's sort of a fun problem in this recursive series of HHTs. Okay, so I'll skip this last piece here just for the sake of time. You can ask other questions like from this setup. Like you could ask, are there This setup, like you could ask, are there ways of driving rotation which don't perturb the steady state to a certain order? And you can do some sort of beautiful analysis with Van der Baan matrices. I'll leave that for future questions. Instead, what I'd like to do here with the end of this talk is to just go through some sort of discussion points and then open the floor for questions. So this was sort of the first thing that I did using helmetological decomposition. I thought it was really exciting because in some ways you're seeing the helmetology composition fall out of the innate structure problem. In some ways, it's like fundamental to the way that Taylor's. In some ways, it's like fundamental to the way this Taylor's usability is. What I work on now, sort of building from this, so we've talked before about these sort of random functions on graphs. So, this is a discussion point I'd be interested in continuing. I also am interested in thinking about complexes beyond simplicities and cells, so this is something that I've been doing with Lekhang. So, thinking maybe about trying to do it, like a complex built out of polytopes in some sense. I'm also interested in these embedding problems, and this is another thing that we've discussed. So, trying to embed Discuss. So, trying to embed data via the flow and use that to visualize circulation, maybe identify a good cycle basis or summarize this topology. And then, specific empirical applications I've been interested in recently are examples drawn from neuroscience, this is thinking about information flow, or from empirical being theory, and this is work with David. So, with that, thank you for your attention. And any questions? Doesn't have a simple example where you can verify just by how to do that. Yeah, so you can do arbitrary loop graphs by hand. So if you have like a loop on end nodes, you can solve explicitly for the steady state. And so there's a nice solution in terms of convolutions against those distances. And you can show that that matches this expansion. Yeah, actually, and I do have the appendix slides for that. I have any slides for that, but just show you. Yeah, so if you have a loop, you can build your solutions using some convolution. I'll show you, this is like an animation showing what the black line is. These are, this is, you can think like, what would perturb steady state in the loop? If I try to drive current around it, basically, if I'm driving current, say, clockwise, and I have an edge with large resistance that I'm approaching, you're going to pile up probability on the lead side, and then you're going to end. Probability on the least side, and then you're going to empty out probability on the other side of that edge. And so your distribution will end up mimicking the distribution of resistances on the edge that comes next in cycle. And so this you can do explicitly, and this matches that Taylor series. Other questions? Yeah. So it's simple to see it's that's the talk about How complex you can put any policy? Yes, so the question there came from actually said like this way of doing nameless decomposition, I'm using a curl operator that's defined with respect to cycle basis. And so it's not tied to some simplicial complex, right? Because it's not saying that the curl only acts on triangles, it's acting on some fundamental cell loops. You could think about partitioning your curl and just saying, okay, I'm going to cut that curl operator to be just triangles. We're going to be just triangles. And then there'll be a piece of it that's okay. Well, now we're going to do squares, right? And then we'll do pentagons and so on. But in some ways, initially you think, okay, that sounds analogous to doing simplicities, but it's not really, because it's talking about perimeter or click size rather than a notion of dimensionality. So if you were sort of trying to make a diagram, you could think about the normal like supplicial complex idea, right? Is you have operators that are mapping you up and down dimensions. So that sort of would go down like a diagonal. So it would go down like a diagonal. And this cutting up a curl to find my cycle basis, my perimeter, would be going like horizontally in click size. And so then you would end up with a grid where you're interested in subsets of a graph that are indexed by both their click size and by some sort of notion of the dimensionality of that set. If you have a subgraph, you can define a polytope associated. For some subgraphs, you can define a polytope associated with it. So you could ask, can you build a complex that does these two axes? Can you build a complex that does these two axes at once? And have transitions instead of just doing transitions on the diagonal or transitions sort of horizontally, can you do all the transitions in that grid? Do you know the web IP? No, I never heard about this. Okay, I mean it's like it's very very similar. They do none equilibrium thermodynamics trying to generalize Thermodynamics tracks generalized tracking version itself. Okay. So PRL. Yeah, that'll be great. Yeah. Yeah, thank you. Okay. Yeah, so then that'll be this talk. Christopher, I guess you're up now. So let's move you down. 