So structured industry, single documentation for large neural networks. And this is a journal with many collaborators. Let's start with the motivation. Okay, so let's start with the motivation. So we know that neural net models are powerful because we can use it to generate impressive pictures and also videos. And somehow that neural net models can understand natural language. For example, us ChatGPT, which is a chat GTT. Ask ChatGPT, which is a chatbot to create a picture of an empty room with no elephant in the room. And here is the output. And we can see that a neural net model somehow can understand image. So in order to achieve such amazing results, we usually need to have a large neural network which can contain billions of parameters. Moreover, that we often have to use Have to be using modern architectures such as a tension mechanism or sinoid transformers to achieve such results. And classical methods such as screen descent doesn't work well in this case. So therefore it is challenging to train a large neural network in a modern setup. So in this talk, I'm going to talk about how we can design second-order optimizers for a Optimizers for large-scale neural networks with minions of parameters. We mainly focus on gradient-based methods, and we're going to explore geomic structures hidden in a survey function. We're going to talk about challenges in neural network training from a theoretical aspect, computational aspect, and also numerical aspect. And here is an example to show that we can use a second-order method to train. We can use a second-order method to train an image, to train a visual training program, which contains millions of parameters. So, before we talk about these challenges, let's start with the definition so that you can familiar with my notation. So, what is a neural network? So, mathematically speaking, it's a function that takes the input, in this case, going to pixel of image and output vector. And what's the neural network training? Basically, it's that we want to start. Network training basically is that we want to solve a minimize internal problem by define a loss function and so that we can try to measure the difference between a neural network output and also a label. And in this case, it only considers just one data point, but in projects, we have training set with n data points. So we would like to minimize the sum of all these individual logs. And in this case, we're going to omit the features and labels. And here they are using meme to And here that I use mu to denote the neural network parameters. And I mean in neural network settings, this is also known as our weight matrices. So now let's talk about the computational challenges. So as we know that the loss function is a sum of individual loss, and in quadratics, in order to achieve amazing results, we only have to use a very large training set, which n can be large, can be millions of data points. This itself can create challenges. Itself can create a challenge if you're only computing a gradient. So, one idea is to use a subset of data points to approximate a gradient, and this way introducing so-called city. Another challenge is that we're often using a very large neural network, a very deep neural network, which contains billions millions of billions of parameters. In this case, D also very large. So, this create itself create another problem, is that we can even load this into computer memory. load this into computer memory to be precise profit card memory. So one way to address these issues nowadays people are using low precision data points which store them in a low precision format. But this can create another issue, numerical issue. We're going to address that later. Now we talk about theoretical challenges. So we know that a neural net loss function is non-convex. So unlike in the convex settings, So, unlike in the convex settings, usually the loss landscape are relatively simple, so we can relatively easy to find the optimal solution. But in the non-convex settings, the neural network length, the loss landscape can be very complicated. So, they even have a lot of set of points, local minimums, and also that may not have unique optimum solutions. And also, that we mentioned that because we have to use a subset of data points to compute the gradient, so we also. Data points to compute the gradient, so we're also introducing randomness. So, ideally, we want to have reduce the, we want to monetically reduce the loss. It's like just like the blue curve. But in practice that, if we're using stochastic methods, the trajectory is just like the purple curve. Last problem is that this also has a statistical problem: is that in projects, we only focus on generalization, just Generalization instead of just minimizing the loss, training loss. This is because that in a real world machine learning system, we focus more on training of generalization. We want our machine learning system to make an accurate prediction instead and make an accurate prediction and instead just reach the optimal solution. So, which means that give an unseen data point, so which means. Point. Which means that we don't need to actually reach the optimal solution as we can make achieve a good test accuracy. Now I'm going to talk about numerical challenges. As we mentioned before, that we often store them in a hard precision format. So this itself can create a challenge that classical matrix operations, such as matrix inversion, matrix decomposition, are usually Matrix decomposition are usually not available in low-precision format. This is because these methods are usually not numerical stable in low-precision settings. And also that even a standard gradient computation sometimes can be unstable. For example, it can get a gradient explosion or maybe get a number error. So here is an example that you can train on the training loss. So you can see that. Training loss. So you can see that at the beginning, we can decrease the loss, and then suddenly it goes up, and then after that, it also jumps, it also decreases, and then suddenly it goes up, and then got an end error. So this makes that a lot of numerical issues. And surprisingly, so categorical gradient descent work surprisingly well, even in these settings, given where we face so many challenges. So we must first review gradient descent. So let's start with gradient descent first. So this is the definition of gradient descent. We're just taking a gradient step by following the negative gradient direction. So when we perform our gradient descent, we're essentially doing a local linear surrogate approximation according to a first-to-approximation. In surgery settings, we can see the mini-vex gradient, and this introducing the stochastic. So in this case, Stochastic. So, in this case, we're only using a subset data point to approximate the gradient. So, we're going to argue that basically we can use a second-order method to track a complex neural network such as transformers. And in fact, we're going to show that adaptive gradient method can be considered as approximately second-order method with a diagonal preconditioning matrix. But first of all, let's review the classical second-order method, Newton's method. So as you can see, in Newton's method, Newton's method is a precondition gradient descent, where we're using the second derivative, which is known as the Haysian matrix, as a precondition matrix. And we have performed a newness update. We're essentially doing a local quadratic approximation. And this is due to a TELUS second-order proximation. Tell us a second-order approximation. In the mini-based case, it is natural to incorporate the curvature from previous iterations. And we call this is a mini-batch newton. And in fact, you can observe that many data gradients follow these update patterns. So we're also going to mainly focus on these update patterns. So it seems that so-called queen design works well in neural network training. So it is tempting to say that how about using this standard second-order measure and using it to train a large neural network. Unfortunately, because the preconditioning matrix, we're introducing additional challenges. This is because that web we have many challenges that we have to address. For example, We have the address. For example, we cannot use a greedy line search and because this is due to the surfaces and also non-convexity. And also, that we cannot exploit the negative curvature because it's unclear, the negative curvature came from runomist or came from the non-convexity. So we often cannot explore the negative curvatures. But in the classical settings, we often can explore negative curvatures, at least in the determined settings. And also that classical. And also, that classical general purpose second-order measure can be computation-intensive or memory-inefficient in the neonail settings. And also in neonail trainings, we focus on a low work clock climb instead of a high-clock time. And also that in neonet settings, because we're using both card, we also focus on parallelizable algorithms. And many classical methods usually can be sequential. And last one is that we also have. And last one is that we also have these numerical issues because nowadays we're using low person data types. So we also have to address numerical issues. So this is our approach. So in our case that we're going to consider no line search, which means that we have to tilt the learning rate. But I should know that stochastic line search is an active research area. Maybe in the future we don't need to tilt the learning rate. And we're also going to consider a cost definite. Going to consider positive definite curvatures, so which means that we're not going to consider the negative curvatures. And also, that we're going to explore sparsity in our pre-condition matrix to make that, so to make our update scheme is computational and memory efficient. And we're also going to divide our update scheme is based only on matrix modifications, so that we don't need to be using a matrix decomposition and matrix inversion. And we can show that our metric, in fact, is parallelizable, so that you can use. In fact, it's parallelizable. So that you can use it in a graphical card. And also, for now, we just assume that we can compute the Haitian of a neural network. But in projects, we can use any efficient Haitian or curvature approximation for neural networks. So the main idea is that we're going to derive an update scheme to solve relaxed problems and which allows us to exploit hidden geometric structures. And then when Geometric structures. And then we're going to modify the scheme to solve the original problem, and also we're going to propose a new august. So now we're going to talk about how can we make a relaxation. So the key idea is that we observe that the original problem can be written as expectations. And here, Q is a Gaussian distribution. And in this case, with infinite inverse covariance or zero covariance. Inverse covariance or zero covariance. So we can see that in this case, the Gaussian is not degenerated, it just becomes a zero-gel function. But we're going to consider probabilistic relaxation. So instead, we're going to consider a finite inverse covariance. So we're going to consider solving these problems with the mean and also the inverse covariance. So this is what a justification. So let's start with the case that this is known as an evolution. This is known as an ego strategy they frequently use in within free settings. So now consider in the settings that we are not allowed to compute the gradient, we're only allowed to query the loss function. So basically the simple idea is that we can use the finite difference. However, in high-dimensional case, it's unclear how we can efficiently compute the finite difference gradient. The key idea is that we're using the Gaussian. Is that we're using the Gaussian in the class setting, we're using the Gaussian as an adaptive perturbation. So that we hope that we can use Gaussian to find a search ejection so we can efficiently find the perturbation. So here is an example that at the beginning, at the initial step, we just guess the initial point and initialize the inverse coverage matrix as identity matrix, and then we're going to generate monocal. Matrix and then we're going to generate Monte Carlo samples and to approximate the expectations and then we're using these samples we're going to query the loss function and then we can get a function value and then we're going to use these function values to weight these samples and then we use these weighted samples to update the mean and also inverse covariance and then in the next step we're just using the updated Gaussian with generic samples and of course we can also we can also reuse some We can also reuse some samples from previous iterations. We just keep doing that until we converge. And in terms of that, this idea is also a tool, is a key component in ChatGPT to incorporate human feedback. And this is also known as a reinforcement learning from human feedback. In this case, we consider human feedback is a black box of function. So that's why we're using a gradient. And this idea, in fact, And this idea, in fact, is not new. It was invented in the 1970s, and nowadays it is still very useful. Okay, so far we talked about in the grid and free settings. But why it's useful in our settings? Because we're working on considering gradient-based case. We can argue that even in this gradient-based case, it's still useful. So the idea is that consider the negative log likelihood of a Gaussian. In this case, you can consider the In this case, you can consider this the convex quadratic approximation. And let me remind you that in the Newton's case, we just do a quadratic approximation, and there's no guarantee the quadratic is convex. But if we're using a Gaussian, this is a convex quadratic approximation. And essentially, this equivalent is doing in a quadratic space. And this means that we're going to minimizing the distribution, the Gaussian also. Distribution, the Gaussian, also defined using the loss function to define unnormalized distributions. And now we're really trying to minimize the distance between the Gaussian approximation and also between this unnormalized distribution. And this we're going to be using the KL divergence. And if we simplify this expression, we can get this update, uh get these objective functions. And this is known as a variation inference problem. Variation inference problem. And you can see that the main difference in the variation evolution strategy and variation infant problem is that we have an additional term, entropy of the Gaussian. And this works well in the machine learning settings because we focus on generalization instead of optimization. So the optimal solution in the evolution is that the equals co-arrangement is going to be infinity. So this means that the distribution is going to become a delta. Going to become a delta function. But in the variety infinite settings, because the entropy term, the distribution is not going to be degenerated. It's always going to be finite. In this case, the inverse covariance is always going to be signal. And if you are familiar with the optimal transport, this is also known as entropy regularization. And usually, you can find that the entropy regulation can be used to improve the performance for optimal transport. And this idea is also not new. And this idea is also not new. It's originally considered in 1980s. Now we're going to say that how we're going to explore, given this new part, these relaxed problems, we're going to explore the geometry of the Gaussian surrogate. So basically we're going to explore the geometry of the surrogate because in this case it's at the Gaussian, so it's relatively simple. We're going to define, we call it the Fischer-Raw matrix. You can see that it's the K-divergence. You can see that it's the KL divergent, it's the Haitian of the KL divergent between a two Gaussian. So, if we're using this feature information matrix or made a feature raw matrix as a precondition matrix, we essentially perform a gridded update, we essentially can take a gridded step and while locally keeping update close to the previous step, current step in terms of care divergence. And in fact, this is a valid Li Mania metric, and it is also not. Many metric, and this is also not equilibrium. And also, that you can see that the feature information matrix is a coordinate-dependent using a coordinate-dependent representation. And this is essential because by using this coordinate-dependent representation, we can show that natural graph descent, as we will see soon, by using it as a preconditional matrix, is a parametrization invariance because the feature information makes it quality dependent. So we're now going to introduce the natural grain descent by using the feature information matrix as a preconditioner. And you can see that this is a preconditional grain descent where we're using the feature information matrix as a preconditioner. And you can understand this is a steady descent in terms of the KL divergence. And in fact that this is also important that by exploiting the geometry, in this case the future information matrix. Future information matrix, this in fact better than just doing simple gradient descent. And moreover, because this is a coding-dependent representation, we can show that natural gradient descent is reparametrization invariance, at least up to first-order accuracy. And in fact, natural gradient descent can be understood as a trash-region method, proximal gradient descent, mill descent, and Liemannian gradient descent. There are some related works in the This up related works in the optimizing context. So in the machine learning context. Now I'm going to show that Newton's method can be understood as a Gaussian approximation. So let me write you that this is the natural gradient descent, where mu, where tau is just a mu is the mean of the Gaussian and also the inverse covariance matrix. And because this is a future information matrix of the Gaussian, we can explore the structures and we can simplify as The structures and we can simplify as follows. And now we're going to be using an important gradient identity known as a star identities. And the key idea for star identities is that we're going to move the partial digit with respect to the Gaussian parameters into the gradient and Haitian in the waste space. And this allows us to simplify this update as follows. And you can see that this is like a second-order method. And let me review that. Method. And let me remind you that way, this update is that we solve these relaxed problems. But now, if we want to solve the original problem, this original problem, we need to make modifications. So the modification is relatively simple. It's just get your expectations and evaluate the Hayesian ingredient at the means. And you can see that if I set the left side, beta equal to one, this is exactly Newton's method. And we need to observe. And we observed that basically in this formulation, the Haitian can be understood as a partial derivative. And this is essential when we have structures in the preconditioning matrix. And also that if the preconditional is polydefinite, this automatically gets a convex quasi-aximation. And by the way, that we also have a zero order in terms of the function value. In terms of the function value, and also the second order identity for the curvature approximation. So it seems that we end up just recovering Newton's method. But why should we do that? So now we'll talk about the advantage of our approach. So this, let me write, this is the Newton's update. In the mini-batch case, you can just incorporate the curvature information from previous iterations. But because we, our formulation. But because we our formation, we can view the curved approximation, sorry, the preconditioner, as an inverse covariance matrix. Therefore, this allows to reparametrize the covariance matrix so that we can guarantee the precondition is always quite definite. Even the curvature approximation, the HF is not quite definite. Moreover, we can decouple the precondition from the curvature approximation. For example, if I assume the curvature, for example, assume the precondition. Curvature, for example, assume the precondition has a sparse structures, and the curvature approximation have another sparse structures. And if they are uncompatible, then usually you cannot make such updates. You need to make additional approximation. But if we allow to, if this structure can be achieved by recommendation, we can project the curve approximation as a partial data by the chain rule. So that we don't need to solve an additional optimizing problem. Optimizing problems. And this idea can also be injected into a mix of Gaussian case. That here, that we consider the distribution going to be a mix of Gaussian, and the type distribution is a mix of student T. And we can also absorb it through doing a fast approximation. We also get so that it can work well even in a 300-dimensional case. And here is a key important message. Even we're using a KL divergence, we need to. Using a KL divergence, we need to use a proper optimizer. If we're using gradient descent and proper update in the delta space, it usually doesn't work well even in 20 dimensional case. This is because, for example, you are doing add on, you're just doing a single Gaussian with a diagonal covariance approximation. But in our case, we perform natural descent by using explore the geometry or a mix of Gaussian with full covariance matrix. Okay, this is a side. Okay, this is a side, but the point is that the formulation that allows us to exploit the parametrization invariance of the natural gradients. So this is already mentioned that if I only perform update in the pre-conditioner, this can violate the polytechnic constraints. And also the other issue is that in the Newton's update, we have domatic invergence. But we can directly reparameterize it to update the inverse. To update the inverse of the precondition directly. And more work, we want to break the cubic complexity by using a sparse structures. And unfortunately, that if a shield using a sparse, for example, sparse trolley, this update is untrackable because the feature information matrix for sparse track factors do not have special structures. Special structures. To achieve that, we're going to explore adding structures. We're going to assume that we have a leap of structures, as we discussed. So now we're going to talk about a leap of structured inverse free conditionals. So the key idea is that in that performance update, in the choice vectors, which is square matrix and non-singular, we're going to consider the tender space of these factors. And we're going to perform the network descent in the tender space. In the tennis space, which is the matrix logo weak space, and this basically is the Lie algebra. So, this is the map we're going to consider. And this is useful because we can turn matrix inversion into matrix subtraction in the logo-weak space. And then we're using a truncated exponential map, so that we don't need to do matrix decomposition. And in fact, we can show that this is a simple share the semi-definite carton's mass of moving frames. And also, we can show that basically the tennis space is a Basically, the tenure space is a generalized Lebanon normal coordinate. So, here is an example. For example, at iteration k, we're going to have this decomposition, b k and high speaker transpose. And then we're going to construct the tennis, we're going to use this tennis space, and we're going to using the origin to represent the calm point. And we basically show that in these local coding systems, the metrics, the The matrix, the feature information matrix, evaluate the origin, it becomes an identity matrix. So, which means that, and also in the next situation, we're going to create another local coordinate systems. And then we're going to perform natural grade descent in this step. But because now, in this case, this becomes identity matrix. So, natural grade descent becomes grade descent locally. And basically, we're going to show that this is a generalized normal coordinator. Basically, the idea is that we're doing a local, we're going to dynamically also normalize the metrics. Also, normalize the matrix. This is similar to a local dipomorphism. And also, we can locally convert the SPD manifold to unconstrained problems in your Lie algebra. Yeah, and then another update in the tenant space, we're going to use this map to map it back into the lego space and then go back into the manifold. And then later we'll see that, in fact, this map can preserve legal structures, and which allows to exploit the sparsity. We're going to see in the next slide. Excuse me. What does it mean that you dynamically autonormalize them at three? We're going to have a technical slide at the end, so we're going to have a look. So basically, the idea is that to incorporate a legal structure is that you can observe that basically this is the update, and we're considering the dense case. This update, and we're considering the dense case. And now it's also B is also dense. And here, H is basically the truncated exponential map. And you can see that by choosing a proper truncated map, we can guarantee this is non-singular, and also this by design is non-singular. So the product is also non-singular. And by exploiting the group structures, the group structure is that the structure is closed under matrix multiplication and matrix addition. So we can replace it with structured legal. So you replace a matrix inversion with structure. So you replace a matrix inversion with matrix exponential. But we do function. But then it's not really the exponential. Yeah, but you will see that this in fact is very useful. And in fact, although we're showing that, in fact, you just need a second-order truncation and you want to satisfy all our conditions. It's just that now it's not a normal coordinate system, it's a generalized normal coordinate system. So which means that the metric is true. So, which means that the metric is trivialized, but the crystalline symbol is not vanished. You just need to do a second-order truncation. Then, in that case, the crystal symbol is non-vanishing, but the matrix is also normal. So, that's what we call the generalized normal coordinate. But this allows us to encode structures either in in the inverse covariance uh in the inverse precision matrix, sorry, in the inverse preconditional or upper conditional. Additional application doing cost structures. And this allows us to break the qubit complexity. Yeah, and so far we assume that Gaussian was a zero mean. The effect you can consider as an SPD manifold. And we can show that in fact, if doing a performance exact Liemannian grade descent, can be considered doing a local grade descent in a Liemannian normal coordinate. Normal coordinate by using the F invariant metric. And we show that the gray descent is essentially doing inexact Liamian grain descent in a generalized normal coordinates. And this idea can be generalized into SPD sum manifolds. And we're going to give an example of what I mean by SPD sum manifolds. And moreover, this idea can allow us to using the Euclidean momentum to approximate the Lehmanni momentum. Approximate the Lehmanni moment term, which share the same speed of Cartan's mass of moving frames. So here is an example of that. Now I can see the Gaussian, which is non-constant mean. So in this case, we can, in fact, this paper, they show that, a very old paper, show that in fact in the K dimensional Gaussian can be embedded into a high dimensional SPD manifold. And this is a sub-manifold because the last entry must equal to one. Equal to 1. And by using our approach, we can always stay on a manifold for every iteration. And another advantage of our approach is that our method can work a class of structures. So unlike standard legal method, you have divided a retraction, and the retraction map can be varied from one sub-manifold to another sub-manifold. So we can apply for a class of structures by using the same update scheme. Another case that considers another sub-manifold that considers another inverse covariator have a sparse pattern. Here we just assume have dense patterns. So far, we just assume the Hayesian approximation or curvature function is known, but in predicts it's unknown. So we need to have an efficient curvature approximation for neural networks. And so basically, there are some curves. And so basically, there are some clever ways of doing it. Here, that way we focus, we can consider, in fact, we can show that the green outer product can be considered as an approximated Haitian, although it's not exact, but it's also not good. But it can be considered as a Haitian approximation, which means that it's have a, in fact, later we can show that it's can have a scalar invariance and also FI invariance. So we have a paper there. There and we also show that this idea worked well on vision transformers with a billion parameters on several data sets. And recently, our diagonal approach also have been used to train a TBT2, which have a billion parameters by other researchers. Of course, there are some limitations of our approach. Because we need adaptive coordinate systems, we can't get no convergent analysis. And also, our focus is mainly on computational. Our focus is mainly on computational and practical aspects. Our goal is to reduce the workload of time. So this means that we're using a simple discretization, explicit discretization, so that we avoid inner loop. Because we don't want to be using a double loop. And also that our approach so far only focuses on XPD sub-mining folks. So to conclude, and we're going to talk about the bonus slides, basically all the technical stuff in bonus slides. So to conclude, basically, we can reparametrize the preconditional matrix. That put conditional matrix so they become inverse-free and structured. And also, we can support several arbitrary coverage approximations by considering their possible duties. And then we can incorporate many chip coverage approximations for large neural networks. Let's go to the bonus slides for the technical detail for many for optimization. So let me remind you that we consider this map. Here, that we consider the money or the F5 ingress. The Lehmanniac or the Fi ingress Lehmanniac metric. In fact, you can see the two times the featurable matrix. And this is the Lehmanniac exponential map or the geodesic. So here is the Lehmannian normal coordinate. The main difference is that we have additional terms. This is because that way, by the definition of a Li Manner normal coordinate, this requires the matrix in that coordinate should be also normal. That's why you should use an additional term. And if you carry out this update, you can simplify as follows. simplify as follows. And if you plug in the math we used in here into this form, you can see that we replaced the symmetric square decomposition with trassi, for example. This is because that symmetric matrix do not have group structures. Because symmetric matrix doesn't I mean the symmetric matrix doesn't come that present under dimensional multiplications. But if I consider it's a square non-singular matrix, these have a group structures. These have a group structures. And we can consider, in this case, you can see the general linear groups. Of course, you can see that in this case, it's singular, right? But because we only perform update in the attendance, and you can see that basically the M, basically in that case, is a symmetric matrices. So automatically, we do the dimensionalities. So basically, we can, in fact, we're doing a performance naturally set in the subspace of the algebra. And this guarantees always non-single. And this guarantee is always non-singular. And moreover, this is going to be trivialized at the origin. And this we call the generalized normal coordinate. So this is the technical slide for manual optimization. We have time. So I have more slides about how can we do pace approximation for neural networks. So basically, so far we're going to assume that we can do a probability reformulation in the waste space by weight perturbation. waste space via weight perturbation and this is the definition of the feature metrics in the waste space. Now we're going to change to do a topic reformulation in a label space. This is the reader that we want to do a curvage approach, we want to do a curvice approximation. So in this case, we don't need to do the weight perturbations. So basically the main idea is that to identify, in fact, the loss function either is the cost entropy loss or the square loss. Laws or the square loss can be written as a Parvi distribution in terms of label space. And we're going to show that, in fact, this is a Hayes approximation. So the idea is that we're going to write the distributions of the labels in terms of these distributions. I use the negative loss likelihood. So this basically is the loss functions we use. And then we're going to. And then we're going to define the joint distributions. So, in this case, I'm using y to denote the labels and x to denote the feature vectors. And then, just remind you that this is the loss function that we consider in this talk, we can rewrite in terms negative log p, the joint distributions. And then we can define the feature information matrix of these joint distributions. So, so far, I give you one definition, this is another equivalent definition. Definition, this is another equivalent definition. We also have another equivalent definition for the feature information matrix. And in this case, we'll assume the labels are sample from these joint distributions. But if we are using observed labels, approximate expectations, we just get back to this. So this is from the first line, and this is from the second line. If I'm using the observed labels, approximate the expectations, of course, this is not true. If it goes to the mini value, This is not. If you go to the minimal setting, it's not unbiased. But if we allow using sample, we can show this effect unbiased if we're doing the mean effect settings. Can you explain something? I mean, this approximation or this approach means that the Hessian has to be symmetrical as it doesn't. And by the Hessian of the original problem, it's equal. Yeah. That's the right thing. So basically, the idea is that you need a That's why so basically the idea is that you need to take expectations. In if you take expectations, this is the example. But if you do just do uh just do this approximation, this just always just approximation. If you're using expectations, this guarantee the same. But if you just observe labels, approximate expectations. I'm adding symmetric post and definite matrices. I'm adding infinite of that. Sorry? I'm adding infinite number of symmetric random matrices, symmetric postal definite matrices. Yeah, so basically this identity is holds. Identities hold if we take expectations because this is a definition of visual information matrix. And this one is not. This is why it's just using samples. I listened something with others, but it doesn't make sense. And also that you might notice that they have one additional assumption. It should be normalized. And if we only look at optimizing perspectives, it's required to be normalized. For example, if I scale loss function, I mean the scale are matched, right? But because if we scale it, But because if we scale it the loss function, now this becomes unnormalized. So this is the heat assumption you must be normalized. But you can always make it normalized by moving the constant term outside. Why if we move this inside, this is not, it's not, the scaling is not matched. But if we can move the scaling outside, it can match the scaling. So that's why I saw that if if we d if we're using this formulation, it can define the future information matrix called. Information matrix. Hardware. That's it. So I'm not convinced your method is second order at all, because if you go to the curves that you show, you go back to the curves, which might convergence curve, any convergence curve you showed so far. For example, asymptotically they all look the same. Newton's method does not look like this. Yes, I know. So that does not look like second order. Because, yeah, this is because we're using chi approximations. But basically just using a green outer product. So everything is first order. You can consider it like a Newton method. We only preserve. It really depends how we define second order method. In our definition, is that if we preserve fine invariance and also the I mean scale. Invariance and also the scale invariance, we consider the approximating Newton's message. Approximating second-order method. But if we only assume that we have to use that patient, then it's not. It depends on the. And the second order method requires second-order convergence, and this is not what you're showing. Yeah, yes, exactly. But if you look at the optimal literature, they even have this method that claim they are considered second-order methods. So this will barely just follow the same definition. So this way will barely just follow the same definition. So let me you might so let me show that. Basically all these all these measures they in fact is not second order method. But they but they always consider this a second order approach. So basically they take the same spirit of second order methods. So it really depends how you define. So BFGS according to you the second order method? You can consider it's BFGS. Okay, that's U to B. So maybe the point, the initial part of the iterations is supposed to be the SM targets right now. Is supposed to be the S targets, right? The initial part of the iteration is almost always first order, anyway, until you get to the closer to the where it starts to be interesting closer to the solution. But so there's not really a point. I mean, in this machine learning context, is it the right notion, really asymptotic convergence model, right, to use? Because most of the time, I mean you have many, many parameters, most of the time you're very, very far away for your to your actual To your actual optimum, right? So most of the time you're not seeing the asymptotic. Let's fix all the second-order method. Yeah, yeah. That's a small statement. It's clearly not. Like if you look at a straw-barring, it's actually not the asymptotic rate. It's got nothing to do with the asymptotic rate. It's about the initial difference of the order of the dimension of the problem, right? So there's no actual variant here. I just remind you that this is the approximation just using false order information and to approximate the Haitian. Just to remind you, this is basically this is the true Haitian because we consider neural networks, so these are real-world neural network settings. We're using four layers, and basically this is the approximation using very rules. And this is the Haitian approximation, just using false order information. So if we consider by this by looking at the pictures, we can say this is a Haitian approximation. Of course, you can always argue that this is not the true Haitian approximation. Troy Haitian approximation. But this is in the machine learning context, we consider this a second approximation. Do you have an idea whether it's order 1.2? It depends on assumptions. If you're doing minibus, it can't even be order 1. It could be probably 1 over k or something like that. Yeah, so that's, I don't know. Yeah. Probably not even order 1 in the middle. Yeah. So maybe these are. Yeah, so basically this is the point bigly that we're using it to train our very large neural networks. Okay, so I suggest we continue the discussion during our coffee break. So let's start with the speaker again. And we have a certain minute algebra break.