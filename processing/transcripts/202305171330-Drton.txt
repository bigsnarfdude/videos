I'm assistant professor in Melbourne and previously was in Seattle at UW. So this will be a bit more of a statistical piece, but maybe it does have some elements of interest, right, to you guys. So what I want to talk about with this slide is just Talk about with this slide is just so what are we doing in algebraic statistics very often? We sort of implicitize, right? And so I'm just put this up as a slide to talk about where there's a nice little curve, right? You could imagine that if you're a statistician, somehow this somehow corresponds to a statistical model where there's distributions indexed by two vectors that are then modeled. Um, model to be on some curve like this, and I think in most situations of statistics, the model might be given to you in some parametric form, like indicated with this parametrization in some parameter t on the bottom left of this slide. And most sort of the default vanilla statistical calculations are often then always in terms of this parametrization, and we can. And we can, however, implicitize and derive also some constraints that describe the model in terms of equations on the object here, some vector xy that corresponds to the distribution in our model, vector of means or covariance matrix or something like that. And then, so why do we do this? Or what motivated this? Then there's, of course, this beautiful, beautiful. Then there's of course this beautiful, beautiful story that got most things started in Benz and Percy Diaconis' paper, where this amazing thing happens that you want to solve a sampling problem. And at the very heart of it is in a way it's figuring out Grubner bases and relations. And that helps you solve the problem. And then there's many other instances, I think, where working out algebraic relations between. Algebraic relations between objects that index or that correspond to distributions in models are very, very helpful. For instance, thinking about conditional independence, like many people have talked about already in this workshop, also is behind these beautiful results in graphical modeling by Judea Pearl and others, and Stefan Mauvitson and others, about Markov equivalence theory and all these things. And all these things. But then there's also, I think, always a bit this idea that we can also use constraints to do statistics and solve things like testing problems via constraints. So I put on this picture here next to this green curve that's supposed to be a statistical model. I also put a little red dot. Sorry. A little red. Sorry, a little red dot. And then there, I want you to think about this picture as so the things on this green curve they correspond to probability distributions. And well, they can generate points in the plane. And so then the question is when you see such a via some stochastic process. And when you see a red point and you think about this may have been produced by some probability distribution, you want to decide whether this a red point. Whether this red point is at a sort of a typical distance to this green curve or not. So you want to decide and solve a testing problem. Could this point have been generated by a distribution that's on the curve? And there, I think, so if you're in many statistical problems, people work merely with a parametrization, right? And then you would be led to some procedure that looks like you compute the distance. That looks like you compute the distance between the red point and the green curve, and maybe you optimize over this parameter t, and then you derive a statistic based on this distance, and you calibrate it in a way, and you decide whether the point is too far or not. But in this talk today, I want to talk a bit about how about using the constraints. So, you can, of course, also think about inserting this red point in the constraint that's displayed on the bottom right here. Right here, and you can ask: Is this somehow too different from zero for the point to have been generated by a distribution from the green curve and with a little bit of noise there involved, of course. And okay, so I hope that the takeaway will be that one can do interesting things with constraints. And one of these aspects will be irregularity, that in a way should make you think about, okay. That, in a way, should make you think about: okay, so this curve is a singular curve. It has some self-intersection point of self-intersection there. And then the other thing in the title was many constraints. And this is another issue that's a bit that's distinct from issues due to irregularity or singularity in this picture here. Just that in many problems that we look at, there are actually many, many polynomial constraints that we derive in some maybe a groupner basis or some generator. A Kruppenabasis or some generating set for some idea, and then we have to think about this carefully when we do the statistics of inserting a data point into all these polynomials and somehow decide whether they are violated enough or not to reject the hypothesis. So, this is, of course, just some bogus picture here. But so, the types of examples that I'm thinking about is this, or one instance would be this good old factor now. Good old factor analysis model that I looked at when I was a postdoc with Bernd. And so, in this model, this is an example where instead of a point in the plane indexing distributions, it's a matrix, a covariance matrix here that indexes maybe Gaussian distributions. For simplicity, everything has zero mean, like Epioto's talk. And then this factor analysis model. And then, this factor analysis model is one where you would say, I have a bunch of random variables here, let's say five. They are depicted in this graph as the non-filled node here, and they may be interestingly correlated. So, for instance, it could be a student takes five math tests and it's his results. And then the model just says that, okay, you expect that there's a lot of dependence, and this is. And this is in this model primarily induced by one latent variable age. That may you may think about some sort of latent math ability of a person. And if somebody has a strong math ability, he will tend to do well in all of these tests. And that creates interesting correlations. From a point of view of a parameterization, so in this model, if you motivate this model from an application point of view, From an application point of view, then you will end up with a parametrization like this. So, this will be a model in which you have a covariance matrix as an object that the distributions you work with correspond to. And under this model, this covariance matrix is parametrized by a mapping that takes a vector gamma that you think about is these are the sort of strengths of the relationship between math ability and specific subjects in which you take tests. Um, subjects in which you take tests, then, and so you have a vector describing that, and this goes into outer product, so it makes a symmetric matrix of rank one. But then there's noise on top, and this noise on top means that this rank one matrix will have its diagonal disturbed by some positive numbers added on. Okay, so this is an interesting model, and this is a model that one can understand very well in terms. understand very well in terms of implicit an implicit description. So when I was then in Bern's group, then at some point Bern told us that he already had figured out this problem 10 years earlier and that there is a nice Gerdner basis of little two by two determinants. So these are equality constraints that hold over the set of Constraints that hold over the set of covariance matrices. That is the image of this mapping that maps diagonal matrix omega and this one vector gamma to this factor analysis covariance matrix. And these two by two determinants, they vanish because they are based on distinct sets of indices for rows and columns, so they don't touch the diagonal. And so when you evaluate them over this matrix here, it's like evaluating it over the rank one part, and then all these two by two. And then all these two by two determinants vanish. If you want to, and you can, I think this was in a paper by Piotre also, and then we also have this in ours again. So you also can give inequality constraints. So you can give a full semi-algebraic description of precisely this, the image of this parametrization if we so want. And so now you have this. This is an example where you have this two. Example where you have these two options. You could work with a parametrization or you can work via constraints. And it's an interesting one. And so if you so the topic of the talk in a way is the following statistical problem. I want to test goodness of fit. So I want to say, is this one factor analysis model able to describe the dependent structure that I see in data? Or is that not the case? Or is that not the case? Is it too simplistic? And so I want to test this one-factor model against a richer alternative of all normal distributions, say. And the data available is to say this explicitly. So this is, I would have in this example, n vectors that are all independent. And each one is of the same length here. For this example, it would be five. And they would have an interesting five by five covariance matrix. And the structure of that is what it's about. Okay, so this is what the problem is. So, this is one example that exhibits all the features one could have to worry about. So, this is a model in which you have interesting singular points that correspond to matrices that are too sparse in a way. So, you can have interesting singularities. In my mental picture, this looks like this. There would be sort of rays with little cusps. With little cusps. These are matrices that have one of diagonal entry, non-zero only. And then there's also this point here that's like a zero that corresponds to diagonal matrices. But so this picture is just to say that there are interesting singular points in this problem and they're connected to having matrices that are too sparse or they're almost diagonal or actually diagonal. Those are the singular points. And this is one example where now we have. One example where now we have an implicit description and we could choose to work with the parametrization or the implicit description to solve this goodness of fit problem. We know other examples where we have interesting implicit descriptions. So Piotre actually has a nice paper in Biometrica with others where they looked at a slightly more general case where you have Gaussian latent tree models. So you could think about this pretty. So, you could think about this previous model as a star tree, and then here there would be a more general tree where you observe variables that are on the leaves of the nodes, Gaussian variables, and then you have a bunch of independent, a dependent structure that's interesting that comes from the fact that one thinks about a graphical model in which there's then a tree where the inner nodes are all latent variables. And so, this paper that Piotr did there is sort of in the spirit a bit of what I'm going to talk about. A bit of what I want to talk about. And you see, for instance, in this, the title says things like model selection without fitting. So, this means that they're going to work with constraints rather than optimizing over parameters in the parametrization mapping when trying to build statistics to solve these goodness of fit type problems for trees here. If you look at this model, at this paper, maybe puret it correctly, but this has a number of nice applications. This has a number of nice applications, but they're more focused on problems with a small number of variables. And the constraints are also of this type that one sees for factor analysis. So there's also always then these tetrads, these two by two determinants, and similar inequalities than what I showed on the previous slide. And the reason why I put this paper up is, on one hand, it's sort of a nice Paper up is on one hand, it's sort of a nice instance where people work with constraints. It's also an instance where, by the nature of the approach, I think you're constrained to working with relatively small examples. This is a case where one uses sort of sums of squares of polynomials suitably weighted to form statistics evaluating the polynomials over estimates of covariances. One interesting thing is in this, this was an example where there's also cases where you may not be perfectly at singularities that correspond to zero correlations, but you can be interestingly close to singularities. Because when you think about variables that are very far apart in a tree, they can have very small covariances in correlations, and that can put you sort of close to singularities in terms of these tetra. In terms of these tetrad constraints, but so that will be more evident in a moment. But then, so these sort of interesting to use polynomials for actually doing the goodness of fit testing. This is also in other cases. We have another project currently running where we're looking at these linear non-Gaussian structure equation models, and where we try and test rank constraints on objects like a third-moment tensor and maybe a matrix that combines. Matrix that combines a flattening of a covariance matrix and flattenings of tensors, and then there's a rain constraint on that. And so we're playing with these. So this testing constraints is an interesting business. But so then how to do it is not so easy, actually. In terms of what we do in the paper that this talk is based on, then well, so we're going to dress this up not in an extremely specific setting like coherence matrices, but Covariance matrices, but we wrote this up in a bit more general fashion. So, in a general setup, we might talk about some statistical model that, for simplicity, I'm going to take to be parametric. So, it's a family of probability distributions here that are indexed by a parameter vector theta. Ah, there's a typo, so this should be a subset, where this theta is an element of RD. And for simplicity, we could just say it ranges through all. Could just say it ranges through all of our D here. So, this is a d-dimensional parameter vector. And so, this is sort of my background model. So, if you want a specific example, you can think about the probability simplex for discrete data or a cone of positive definite matrices in all normal distributions. And then I can think about some subset of interest, like say the one-factor model. And now we know that we have some. Know that we have some implicit description of this model as well. And so I can say this is given here as a set where I have a bunch of constraints that functions fj evaluated at the parameter vector have to be here not positive the way it's written. Okay, and there can be a number p of constraints I might be thinking about. So p is the number of constraints. D here is the dimension of the object. The object that I'm constraining. That's a parameter that corresponds to the distributions I'm thinking about. And then again, I want to do this problem of goodness of fit. So based on data, an IID sample from one such distribution, P theta, I want to decide whether theta is in this set that is defined by the constraints or not. And the main interest for me is the case, of course, where these constraints are polynomial, but some aspects of what I'm going to talk about are. Some aspects of what I'm going to talk about are not specifically restricted to that. It's just that the things I want to do, you can always do when you do things like polynomial constraints on covariances. Okay, so if you actually solve a testing problem such as goodness or fit test, you have the classical approaches. So one is doing likelihood ratio tests. So likeylid ratio tests are about determining how likely are the data. How likely are the data under when you grab any distribution from the small model versus the big model? So the constrained and the unconstrained model, and you compare how to what extent are the data more likely under the larger model. And you then have a theory that says in regular models, you know how to calibrate such a statistic that looks at this ratio. That looks at this ratio on a log scale and times two for usual standards, then you know how to calibrate the statistic. So you know you can understand in a regular setting what is a typical size for this type of statistic. And this has to do with so-called chi-square distributions. And you can set a threshold and make calibrated statistical decisions about whether you believe it's this smaller model or the larger model. Okay, and then when I said singular points can cause trouble, what I mean by that is then, or I illustrate this just here with a little simulation. So if you take data from, so now this is about a problem with 15 observed variables. I'm talking about a problem with one factor analysis. I have a covariance matrix of size 15 by 15. I, in the unrestricted In the unrestricted alternative. And it's one of these special matrices of diagonal plus rank one form when I talk about the hypothesis of a smaller one factor model. And I can do calibrated tests or put this statistic on a p-value scale with the help of chi-square distribution theory that is valid for regular points. And here's an illustration where I simulate data sets of a size a thousand from Data sets of a size a thousand from distributions that correspond to geometrically regular points, and this works out beautifully. These p-values, if it's correctly calibrated, should be uniformly distributed. If, on the other hand, I move to a singular point, so I take a point that is close to one of these points where I have what are the axes, what are the x and the y axis in these diagrams? And the y-axis in these diagrams. Ah, here? In all three diagrams. What are the axes? What do they mean? So the x-axis is from zero to one. So p-values are the probabilities of observing a certain value of this statistic. So you give me a data set, you compute the statistic, and then essentially I compute what's the chance of seeing something as big as this one in. In if the distribution is actually drawn from a, if I'm generating data from a distribution that belongs to the small model. So it's a p-value in the sense of, yeah, let's say, as we also do with Markov basis. I don't know if this is an answer. And so then I can, I have data sets of a size a thousand. And so now I don't remember how many times. And so now I don't remember how many times we repeated this, but today we repeatedly generated data sets of a size 1,000. And every time I have a data set, I can compute one of these p-values. So I compute the statistics one time for one data set. And then I can ask what's the chance of seeing such a value that I computed under a certain distribution. And this probability is then giving me one, this gives me one probability. And then I do this many times over. And this gives me many simulated values of a probability. And if the Simulated values of a probability. And if the test is correctly calibrated, then this should be uniform. And so this is here relative proportions, I guess, of these p-values that are generated falling into small intervals. Does that end enough? Yes, thank you. So I guess I could have shown you instead, I could have just calculated this number here. Have just calculated this number here in every data set. And I could have shown you as over this. So, this is a non-negative number here. Always, I could have shown you over the non-negative numbers and grouped into intervals, how often do I see this number fall in certain intervals? And then I would have wanted to see a curve that looks like roughly what a chi-square density, a density of a chi-square distribution would say. So, this calculation to this transformation. This calculation to this transformation to PVLs is just making it a uniform distribution instead of some other given distribution. Okay. And so this is what it should look like in terms of then the actual test then says compare the p-value to a threshold. And if your threshold is 0.1, then you want that sort of in 10% of times you reject erroneously if the distribution is actually in your model, like it is here. And if I move to a singular point, If I move to a singular point or even just close to one, then you get that this is very non-uniform. And this would mean that when you simulate data sets from a distribution in my null hypothesis, in my small model, but a distribution that is close to a singular point, not even right at a singular point, you get this behavior with data sets of a size 1000 that you see many of these small p-values. That means that you saw means that you saw a statistic here that is so large that it would be unlikely to see. So small p-value says these are data that are unlikely to be seen under the null hypothesis. And so this would then say, even though the model is perfectly correct, you seem to be getting numbers here that look far larger than what you would think in a, they should look like in a regular setting. And so it's so if you calibrate your statistic method, So, if you calibrate your statistic by thinking about regular points, you're going to make bad mistakes at all the singularities. And you get a very similar issue just from dimension. So, I can take a beautiful regular point in a problem where instead of talking about 15 by 15 covariance matrices, I talk about 200 by 200 covariance matrices. And if you know what that is, I can even make chi-square distribution approximations as good as one can with so-called Bartlett corrections. But even if I do that in high-dimensional problems, does this just fail? In high-dimensional problems, this just fails. So, even though I'm generating data from a perfect one-factor distribution, just by the fact that this dimension is so large, I'm dealing with so many covariances, the underlying sort of applications of the central limit theorem that drive this chi-square distribution theory for this statistic is just not accurate. It doesn't kick in the normal approximation. And you see here also that you would. And you see here also that you would constantly call the null model incorrect, even though it is correct, if you use this type of like good ratio test. And if you think about the first slide, I want you to think like good ratio tests is calculating the distance between the red point and the green curve. And what a typical distance looks like is very different at singular and regular points. But practically, you're calibrating based on the hope that it's a regular point. Now, I said I can also use constraints. So, if I want to use constraints, I can do that. And let me just talk about one constraint for simplicity. Let's say I take one tetrad and I just want to test the hypothesis whether this tetrad is zero over the true covariance matrix. Then, I think everybody would come to the conclusion that one thing I can do is I can try and estimate my covariance matrix. Estimate my covariance matrix covariances with the same type of object that Piottio used, a sample covariance matrix, at some point at least. And so I have a sample covariance matrix. This is an estimate of variances and covariances. And I can insert that into the polynomial. And this will give me something by random variation that is not zero. In order to judge whether it's zero, I might square that thing so I get something non-negative. Thing so I get something non-negative, and then I want to say, is this small? Now, as you think about whether this is small, it's evident that you have to account for the scale, right? I could just scale all my numbers here, and then this would change. So, I somehow have to account for scale. And the way you can account for scale is you're going to try and estimate the variance of this F1 of sigma hat that's being squared there. And you can do that. So, there's a canonical approach to doing that that looks at the gradient vector. At the gradient vector of this polynomial, you insert the estimate in there, then you take a matrix that accounts for what is the variability in this estimate of the covariance matrix. And so you form a quadratic form on the bottom. And so this is a canonical estimator of this variance down here, strictly speaking with a 1 over n that I then moved in the numerator. But so in this way, you can have a nice calibrated statistic that in the regular case, it will. case it will behave for a large n like a chi-square one. It will just behave like a squared normal zero one random variable. However, if you're at a singular point you have issues again. So a singular point now for a single constraint is just these are points where the gradient vanishes. So if the gradient vanishes, it turns out that you see this in this in this gradient formula here. So your variance estimate is based on a gradient vector that actually will be Vector that actually will be zero when you insert the true value of the covariance matrix that this sigma hat estimates, if that's a singular point. And so you're having a statistic that stochastically behaves like something that estimates zero. And the numerator will, at a regular point, this does not estimate zero, but at a singular point, this numerator is still converging to zero. And so at these, at these So, at singular points where the gradient vanishes, this means that this quantity here is very, very close to zero then. And even close to zero, then when multiplied with n, the square of this f1 of sigma hat is still zero. So if you want to understand the distribution of this, you sort of have two random variables that converge to zero in the numerator and the denominator. So you have to look very carefully at what this ratio actually does. And so we did this in one previous. Actually, it does. And so, we did this in one previous paper. I shouldn't talk too much about this, but the gist of it is that this classical approach that's called the wall test, this is also causing problems when you're at singular points. And moreover, and this is also, there's an extension for doing more than one constraint, where instead of squaring one constraint, you form a quadratic form in a vector of constraints that you've estimated and you suitably account for the dependence among the coordinates. Dependence among the coordinates, these different polynomials we estimate. And one can do that. But this only really works if the number of constraints p is no larger than the dimension of the object that you're basing inference on here, covariance matrix. So if you think about this one factor model, going back to this one, if you think about this one factor model, right, I have a matrix, I have say k variables. A covariance matrix is sort of an object with k variables. Covariance matrix is sort of an object with k squared order entries. But the number of equations that I have here, this is k to the four in order many, because this is about picking four indices out of k to form equality. So you have many, many, many more constraints than you have even covariances in your covariance matrix, and you cannot form classical wall statistics at all if the dimension is a bit bigger. Okay, so what can you do? So this is maybe. So, what can you do? So, this is maybe the most important slide of the talk. So, what I'm going to say is that if you have implicit descriptions in terms of polynomials, and these are in these nice quantities like covariances, which are moments of things related to the data, then you can circumvent all issues due to singularities by giving up a little bit of estimation accuracy or a good amount of estimation accuracy. Good amount of estimation accuracy, but then you can circumvent issues due to singularities. And you can also circumvent issues due to dimensionality. And so let me try and explain this a little bit. So if I have a single tetrad here, I don't know where this. Oops. Sorry. So if I have a single tetrad here, then this single tetrad is a difference of two terms. Two terms, and each term is a monomial, just a product of two covariances. So if I ask you, give me a very, very, you can give me any an estimate that's arbitrarily inaccurate, but it has to be unbiased. Then maybe after a little while, you get the following idea. You know that this covariance here. This covariance here is an expectation of, in the mean zero case, of a product of two variables, namely the first and the third, right? So the expectation of the product of the first and the third variable in any one of my observations vectors is this sigma one three. And similarly, sigma two four is an expectation of a product of the second and fourth variable. And the fourth variable. Now, there's the fact that now you have to estimate a product of two covariances, but there's something very simple you can do. Namely, you can look at an expression like this, where I pick two observation vectors, namely, say the ith and the j-th one. They are different ones. And when I, and then I form a product of four random variables, two of them I grab from the ith observation vector and two of them from the jth. And from the i. Jth. And from the ith one, I take one coordinate one and three, and from the jth one, I take two and four. So if I ask you what is the expectation of this guy, then you're going to tell me, well, okay, by independence of xi and xj, the expectation will be the product of the expectation of this guy and the expectation of that guy, because j and i index independent vectors. So therefore, the expectation of this product altogether is exactly this product up here. Here. Okay. So you can do this essentially with any polynomial. If you can estimate the individual variables unbiasedly by some function of the data, you're just a product of two coordinates of a random vector. Then you can account for this product structure by just grabbing independent data points. Okay, so you take different data vectors, different rows in your data matrix, and then you extract whatever you need to estimate the individual variables that enter a monomial. Variables that enter a monomial. So, this is an unbiased estimator here for this tetrad. Okay, so we did a little bit of symmetrization. So, when I take the first and the third from the i vector, then I later do it for the j vector, and then two, four from the j, then from the i. So, I did a bit of symmetrization. So, it's symmetric in the choice xi and xj. But if I just highlight this first part here, you would all agree. This first part here, you would all agree that this is an unbiased estimator of this petrot here. Now, if I can give you an unbiased estimator by taking two data vectors, then you can already immediately give me a better unbiased estimator. Namely, you can say, well, if you can estimate this unbiasedly by taking two data vectors, why don't I just take all possible pairs of data vectors, evaluate this super easy, stupid estimator, and then average over. Estimator and then average over all pairwise evaluations. Okay. And such a statistic is called a U statistic in the statistical literature. So it's a U for unbiased, and it's unbiased, and it has a kernel. And the kernel is that simple, stupid, unbiased estimator that you just take based on a very small number of data points. Here in the Hedgehog case, we can just take two. But then you get a good estimator out by visiting every pair of data vectors. Every pair of data vectors and doing this, evaluating this kernel. Okay, now it turns out that if you, in this Tetrud case, take this dumb little estimator based on two data vectors and you average over all pairs, then you get back the estimator that you had originally, namely, you plug in sample covariance here. And well, you have to do a little adjustment with n times n, n over n minus one. times n n over n minus one to get exactly this but you effectively back to the same estimate i had before okay so this is uh in a way just a very complicated view of what i i told you talk to you about earlier already however this use statistic view is is going to be useful now so these use statistics you can uh there's a very well developed uh theory for studying use Developed theory for studying new statistics. So, when you look at this summation here, you see that clearly there are terms that are dependent. They're not independent. It's not a sum of independent terms you can apply a central limit theorem to directly. But whenever there's indices shared here, ij, ik, for instance, there would be dependence. But if the indices are not shared, they're independence. But anyway, so there's a very nice theory called, this based was introduced by a guy called Hodjek that is about, you can prove a Gaussian approximation. Gaussian approximation results for, let's say, one such use statistic by approximating this sum that has dependent terms with a sum of IRD independent terms. So there's this notion of a projection of a random variable onto the space of random variables that are sums of IID terms. And this is called Hajik projection. And when you do that, for instance, for this tetrad, you would realize that this statistic can be. Statistic can be up to a term that is negligible, the same as a sum over the independent data vectors xi. And then each time you evaluate a function of xi, and this would be a conditional expectation of this kernel. But in this case, here it just ends up being you replace the j term by the expectation. So this would be a sigma, sigma, sigma, sigma. Okay. And then you see this result here that says there's a Here, that says there's a central limit theorem that applies. This estimator here will become asymptotically normal, and there's a variance. And then you see this effect of singularity now again. It just looks, it's dressed up in a different way. At a singular point, the gradient vanishes. This means that all four covariances are zero. If all four covariances are zero, this function g that is supposed to help approximate this statistic with a sum of independent evaluations of this same function. Independent evaluations of the same function over the data points xi, this actually becomes the zero function. So this says in this central limit theorem, the asymptotic distribution you're approximating with is just a point mass at zero. Okay, and so this is the same thing. So irregularity is here connected in this u-statistic world to this projection being the zero function. And these are points where then you don't have a non-trivial. You have a you don't have a non-trivial Gaussian approximation that you can do statistics, okay. Um, so now, in general, when you think about a more complicated problem than just one tetrad, you could think about now our setup will be I have many polynomial constraints. Each one of them I can estimate with something like this here, unbiasedly. And depending on the degree of the polynomial, I may have here more than two data independent data vectors involved. Vectors involved, but that then depends on the precise problem we're looking at. But then, once you have such an unbiased estimator, you can do this averaging here and you can form new statistics. And maybe now I have in our problem like a one-factor problem. Maybe I have now, I don't know, 10,000 constraints or a million constraints. P is a million. And then I have a million use statistics. And if I want to, I can take their kernels. I want to, I can take their kernels, I can stack them all into one big vector-valued kernel. And M is now my, it's this is connected to how many independent data points do I need to touch to form these small little unbiased estimators. So you could think about this as this M has to do with the degree of my polynomial in the covariances. Okay. And so if you have this setup now, more generally, you could think about: okay, I form a use statistic, this is a good estimator. Statistic: This is a good estimator of my constraints. If these kernels here are unbiased estimators of this constraint vector f that calls these constraints f1 up to fp that I'm interested in terms of defining my model. And now then I'm going to make a first proposal that is important or first point that's important. So if you do combine the values of different constraints, if you have many, many, many constraints. Many, many, many constraints, then it is it tends to be more convenient to look at the maximal value of the constraint than some other way of combining it. Okay, and so more on that in a second. So then the proposal would be that you're going to try and form an overall statistic by looking at what this becomes now this used statistic that estimates all the constraints. And you look with some standardization, so taking in account how variable this is. Into account how variable this is, you estimate that in some fashion, and you take into account what is the largest violation of any constraint. Okay, so you take a maximum of all constraints and look at that. And if you can tell that this is too big, you can reject. Now, you can try and argue what is too big with the help of a central limit theorem. And if you have a use statistic, you will have a central limit. I use statistic, you will have a central limit theorem and you will have an asymptotic covariance matrix. And this asymptotic covariance matrix is the covariance matrix of this function g, which is this Hajik projection. So it's a conditional expectation of this kernel function, just like on this previous slide here. And you can talk about its covariance matrix. Now, this is fine. So you can do something like that. And there is actually, there is in the last 10 years, there was interesting theory. In the last 10 years, there was interesting theory developed that would allow you to actually calibrate this maximum to come to a decision of when is this maximum or what's typical values of this maximum. Unfortunately, this covariance matrix here at irregular points, this will degenerate and become a matrix that has zeros on the diagonal and is then zero in entire parts. And this will mean that then you don't have a non-trivial approximation and then you cannot really judge. You have cannot really judge very well what this maximum, how this behaves. So, this is now where our stuff starts. So, what can you do if you have a situation of degeneracy where this covariance matrix is singular? Well, one thing you can do is you can try and instead of average so aggressively over all, in the Tetrad case, all pairs of data points, you could be a little bit less ambitious. You could be a little bit less ambitious and just split the data set into blocks. So you have split the data set if you're looking at, say, estimating a polynomial of degree M, you could split your data set into M blocks. And you use the different blocks to estimate the different variables that go into these monomials of degree M. So suppose they're square free, right? You have M monomials in a term, and you're going to estimate. In a term, and you're going to estimate the variables that appear in the monomial using independent parts of your data set. And if you do that, that avoids issues of singularity because this becomes then an ordinary average over independent parts of the data set. If you have an ordinary average, you can rely on what has been called a high-dimensional Gaussian approximation in the statistical literature. So this works for maximum. Literature. So this works for maxima. So there's a theory that says, okay, so I should say, in high dimension, so they have the central limit theorem that works. In any fixed dimension, if you have an average of many, more and more and more IID terms, this will, under the usual variance, finite assumption, this will be going to a normal distribution. However, if you are in high dimensions, this will fail and you cannot approximate the distribution. Approximate the distribution of a very high-dimensional sum of IID random vectors by a Gaussian. However, over rectangular sets, the approximation is still good. And this is a theorem of Viktor Chenozukov and Kengo Karto and some co-authors. And so this means that if you look at maxima of high-dimensional random vectors, you can still approximate their distribution with the distribution of maxima of Gaussian. Of maxima of Gaussian random vectors. So you can work with these maxima when you have many, many, many constraints, and you can calibrate statistics with the help of central limit themes. Now, you will also then maybe come to the conclusion that somehow this is a bit silly. So, if I have a higher degree polynomial, then I have to split the data into smaller, smaller chunks. So, if I have a thousand data points. A thousand data points and a degree ten polynomials, then I would have to split things up into 10 pieces of size 100. And I use them separately to estimate each covariance or whatever I want. And so the question is whether you can do better. And so the question is whether you can, instead of blocking so aggressively and estimating variables in independent parts of the data, can you do that better? You do that better, and so from a point of use statistics, there's a nice tool that's called randomized incomplete use statistics, and this goes like this: so, in a use statistic, you have an easy, simple estimator that tells you take a certain number of the independent data points in your data set, so say m many, and compute some statistic. This statistic estimates unbiasedly what you want, but it's a terrible estimator because it uses very little information from the data set. Said in a standard use statistic, you average over all puples of size m that you can grab from your data set. In an incomplete use statistic, you do not average over all m tuples, but just a sub-selection of that. And in a randomized incomplete use statistic, you randomly pick tuples on which you evaluate. And then there is a computational budget parameter, n, I call it here. Parameter n, I call it here, that is the number or the expected number of m tuples that you would evaluate here in such a statistic. Okay, and this is now a parameter that you can play with. So if I pick n very large, I form a very good estimator, but it will be an estimator that is then close to this full use statistic, which is exhibiting exactly the type of single. Exhibiting exactly the type of singularity issues we don't want to succumb to. If I were to go with n very small, I could essentially do this blocking different groups and just evaluate the kernel across different groups. But you can go in between. And so this is what Niels came up with. And so there is a way of picking the computational batchet parameter on the same order if you pick it. Parameter on the same order, if you pick it on the same order as the sample size, then this is our conclusion in the paper: that this will provide you with a method where you can, you build the kernel based on the fact that you've worked out algebraic relations. Once you've built this kernel and you know the degree of the relations, you can then form these randomized incomplete use statistics. But then as you do it, we would recommend that you pick the computational budget on the same order as the sample size. On the same order as the sample size, maybe twice the sample size or something like that. And if you do that, then you can form a statistic. This is now triply indexed because this talks about the jth constraint, estimating it with a randomized incomplete use statistic where you have a data set of size little n and a computational budget of capital N, meaning how many simple estimators of a monomial do you use in a way. In a way. Okay, and if you do that, then you can get a statistic out that you can always approximate with a maximum over a Gaussian random vector, the coordinates of a Gaussian random vector, where the covariance matrix is non-degenerate. So it is a regular matrix, an invertible matrix. And in practice, you wouldn't know what this What is this asymptotic covariance matrix, but you can build an estimate of it with the help of what's called Gaussian multiplier bootstrap. So it's a resampling method. And so with this resampling method, you sort of are able to work with an estimate of this covariance matrix. And based on that, you get an approximating Gaussian random vector out that you can work with in practice. Everything you need to know can be computed from data. And for that, you can compute maxima. And then these, And then, these distribution of these maxima serves for a calibration of what the behavior of this is. It tells you what are typical sizes of this type of statistic that checks to what extent do you violate constraints that should hold, that describe your model implicitly. Okay, and then this is Niels's sort of statistical contribution. So, he worked on this very carefully, examining what was in the literature and proposing a framework. And proposing a framework of mixed degeneracy where he can show that when you pick your computational budget parameter on the order of the sample size, then we can really have a methodology that is valid irrespective of whether the point is singular or regular. I resolve this singularity issues by estimating the variables in the product from different parts of a data set. Different parts of a data set. And it also works in high dimensions. And that is true because for maxima, you can approximate the distribution of maxima in this setting here with the distribution of maxima over coordinates of Gaussian random vectors even when the dimension is high. So this is an advance of the last decade in the stats literature. Okay, and so these are the references for that part. So there's this work by Cheno Zukov. This is about Gaussian approximation. This is about Gaussian approximation for high-dimensional random vectors that are sums of independent vectors, and how to do this Gaussian approximation, how to prove that it's valid, and how to do it in practice with the bootstrap method. There's papers that then go and move this from sums of independent random vectors to use statistics where you have some dependence among terms. And in this world, there's then always an assumption that these u statistics are non-degenerate, so they have. Statistics are non-degenerate, so they have asymptotic variances that are actually strictly positive and bounded away from zero. And then Niels's main thing is that he showed that you can go in between these existing pieces of work and deal with cases where you're sort of some polynomials can be a wave, can be regular, or they can also be singular, and it all still works out okay. And so then you get results like this. And so then you get results like this. So, think about this: it is a statistic that estimates to what extent are your polynomial constraints violated. So, what is the largest deviation from zero for any of the estimated polynomials that are supposed to vanish over your model? If you calibrate, standardize this a bit, blow it up the estimation error it has in terms of the constraints by a square root of n factor, you can approximate probabilities. You can approximate probabilities over rectangles by Gaussian probabilities, and this works in a way that the dimension, the dependence on the number of constraints, is logarithmic. So, in principle, you can work with a million constraints on covariances. When the sample size is a thousand, that is no problem. So, just like you want it when you think about this Koebner basis for the fact analysis model. Okay, maybe one word about this bootstrap. Um, maybe one word about this bootstrap approximation. So, this bootstrap approximation is um, you're going to have a covariance matrix that you can express as a sum of independent terms, or it takes a bit of a form of a u statistic. And when you ever use a Gaussian multiplier, it takes a sum over tuples of data points, and it instead of leaving it And instead of leaving it as is, it's going to insert a normal zero, one random variable in here. And so this is a term that is re-centered to have zero mean. And then with this Gaussian multiplier, you get some additional variation. So it's like a resampling method that produces vectors that are like vectors that you would have seen if all the constraints were true. Constraints were true. Okay, but this is maybe a bit much to talk about right now. But so, long story short, so we are able to cope with singularities. So, these are these p-value plots. This is supposed to be uniform, it's not fully, fully uniform. We're slightly conservative here in terms of the p-values have a little bit of a tendency of being. Have a little bit of a tendency of being too large, but this is a big improvement over what you've seen for the pictures for the likelihood ratio test, where I have a problem about covariance matrices of size 15 pi 15. And we're looking at a lot of tetrads at this point. And we're taking a computational budget parameter of 2n. So we're using, we are evaluating 2n estimators of the monomials that appear in our tetrads, unbiased estimators. Unbiased estimators. And n is a thousand, so data sets of a size a thousand. And so then we have plots here where we play a little bit with problems with singular points in this 15-dimensional space. So if you explore the computational budget parameter, this is difficult to prove something about it, but it seems like you can even take it higher than the one we recommend. And for this example, it still is fine in terms of keeping an error. Of keeping an erroneous rejection probability low. If you're below the solid diagonal, that means you're doing you have a well-calibrated test. And this line up there just says the like-hood ratio test does not work. Yeah, and then we have various simulations that examine whether we actually have any power. So we sacrifice explicitly, we sacrifice estimation accuracy by estimating. Accuracy by estimating variables in the monomial from smaller parts, from different independent data points. And we do not use that many estimators, combine that many estimators of monomials as we could, but we do this on purpose in order to guard against singularities and you also can guard against high dimension. Now, this I didn't have it up plot right now, but this modulo or the fact that you just at some point Modulo, the fact that you just at some point have too many constraints, so you have to do something about the computational aspect of the problem in terms of the distribution theory, the probabilistic aspects, they would be fine in high dimensions. So that's it, gonna stop. So this is the paper, it's still on archive, so Niels is gonna work on a new revision. But we have one NIPS paper also on this topic where this is doing it without these use statistic. This use statistic perspective more in this blocking fashion. But I think this is a methodology you should keep in mind whenever you want to actually use algebraic relations that you've found for practical statistical testing, especially when you have many constraints in some generating set for an ideal, and you want to do these goodness of fit problems. Of fit problems. So that's it. Thank you. Thank you. Yes, many thanks, Matthias, for the nice talk. So we can move to questions. So Sonia. Hi. Great. Thanks for the talk. Hopefully, yeah. So I don't know if I misunderstood. So correct. Misunderstood. So, correct me if I'm wrong somewhere when I state this. The model goodness of fit test has a null hypothesis that my true distribution came from the model versus the general alternative. And so your goodness of fit test truly doesn't have the alternative big model, which is better than likelihood ratio test, which tests a small model against a big model. So I mean, the results are great, but I don't think it's. The results are great, but I don't think it's fair to compare to likelihood ratio test. Although, I don't know if you have something else that you could even compare this to in this scenario. I'm just saying that the alternative hypotheses are different in the LRT and in your test. Yeah, yeah. But I mean, so I'm going to be a little bit maybe controversial, but I don't actually care about the alternative, right? The only reason why you shouldn't, yeah. The only reason why you care about an alternative is to. About an alternative is to formulate a test statistic, right? So, from uh, if you know, if I'm Fisher, I'm gonna say, Yeah, the only thing I care about is that I'm going to give you a statistical test, meaning a function that says, yes, no, the null hypothesis is true or false. And that function has to have the property that if you feed me data when the null hypothesis is true, it should say one, uh, reject the null, only about, let's say, five. The null only about, let's say, 5% of the time, right? And so, from that perspective, the likelihood ratio test is not satisfying this criterion then at all points under the null, right? I know, but how do you know that, I mean, I'm with you, and I believe this is actually better, but how do you know the improvement is not due to the fact that you have dropped the alternative and made it general? This is a stupid question, perhaps. This is a stupid question, perhaps. This is a philosophical question, but I've turned into a statistician, I think. May maybe maybe it doesn't matter. I'm sorry. Yeah, and so in a way, yeah, okay. So in a certain way, I would say it doesn't matter because I'm going to only my only evaluation criteria will be behavior under the null. Now, this is, of course, also nonsensible, right? Because we all know how to make a test that is always valid. Know how to make a test that is always valid. It's just the one that flips a coin, right? And doesn't look at the data. But so I need to look at power as well. So I think that maybe this is answering your question a little bit, this picture on the right here. So here I'm showing you what is the power of rejecting the null when it's actually false. And you can see that in these, well, it's not huge dimensional, but. It's not huge dimensional, but somewhat a bigger dimensional problem with 15 by 15 covariance matrices. So, this is a good number of parameters you have to deal with. And we purposefully decreased the sample size a little bit here. So, it's not true that the likelihood ratio test that is very good at detecting normal alternatives is far better than our test in terms of rejecting the null, right? So, our tests are these types of curves. So, from that perspective, From that perspective, this is, I think, what you're saying, right? You're saying that, yeah, I think this, this, this sort of supposed to make me happy now. So, yes, thank you. Yeah, so that's, yeah, so you're right, absolutely right in terms of that. Yeah. Okay. Any other question here? Yeah, Piotr. Thanks, Matthias. Great talk. So the way you set up the whole The way you set up the whole talk, there are two issues: right, there's the closeness to singularity and this high dimension. I guess this is just a remark. I guess that in higher dimension, you are probably the geometry so that at least in your example, you are bound to be close to the singularity. So these things are kind of tied up together. But I just, I really wanted to understand the first point, the one where you The one where you fix the problem of being close to the singularity. Can you just repeat the argument on slide 10 when you now independently estimate each for each monomial take independent set of variables? Yeah, so maybe so if you so suppose you have two parameters, right? And you you can you you you you have to estimate their product, right? Estimate their product. So you have to estimate the product of two parameters. So then one thing you can do is you can say, I have a very good estimator of the parameters, and I estimate the product of the parameters by the product of the estimates. Yeah, so this we can do. If you do that, if you think about what is the distribution of the product of the estimates, right? If you have both estimates estimated. Have both estimates estimate a parameter that truly is zero, then you can see that if every estimator has sort of a one over square root of n estimation error, the product will have a one over n estimation error. And so if you do a square root of n calibration, then all of a sudden when you're at the point where both parameters are zero, where the two lines are intersecting geometrically, then you have all of a sudden a smaller estimation error. It's one over n rate. Error, it's one over n rate, and so your statistic will behave funny because you're going to have an estimator that is converging to zero, and you will have maybe standardized this in a wall test with another quantity that's estimating the variance, which also goes to zero, and then things go wrong. Does that make sense? Yeah, so what I'm saying, what I'm saying instead is let's not estimate both parameters accurately and multiply them, but instead, let's, I would say, take one half of the data set. I would say take one half of the data set to estimate the first parameter, take the second half of the data set to estimate the second parameter, and multiply these things. And if you do that, then this issue doesn't arise. Okay, okay, nice. Thanks. Okay, I have sort of a math question, right? So years ago, there was a resolution of singularities, Watanaba, log likelihood, you know, log canonical threshold. Is this completely gone now, or is this somewhere hidden in your talk? This is somewhere hidden in your talk. Beautiful math of resolution of singularities. So, yeah, so this would help you understand the behavior at singular points. And if you can understand that very well, you could be in situations where maybe you can understand all the singular cases and all the regular cases. And all the regular cases, and maybe arrive at some result that says on my original statistic, like a ratio-ratio statistic, I could find a worst-case typical distance. So something like that I could potentially do. So this may be feasible in some problems. And I would say this, in a way, is a perspective that is used when people test inequalities that are so nice inequalities, let's say linear inequalities. Let's say linear inequalities. People take this perspective a bit. But then here I would say that at the end of the day, this is so complicated to study these singularities very carefully. And you can also be, it is very difficult to say what is the worst case typical distance to a set when you have singularities of many different types. So then what I'm here proposing is that we circumvent this issue by Issue by sort of, yeah, it's trading off this sort of geometric difficulty with using the data in a different way. So using the independence and different parts of data to estimate different parts of parameter vectors that then enter polynomials to resolve, to avoid having to talk about this. Yeah. Okay. Is it an answer? Okay. Is it an answer? Yeah, so the short answer is it's gone. It's gone away. But it comes at a price, right? It's gone with a cost. So I should say. Sorry, Matthias, it's Piota again. So one of the following up on what Bernd said, one of the issues with the resolution of singularities approach was that the asymptotics depended on where you were on the singularity. On where you were on the singularity. And here, in your theory, the asymptotics will not depend on that, which is, I think, beautiful. But I guess the finite sample behavior will still depend. Can you somehow, is this real log canonical threshold still going to play some role in describing how quickly you converge to the asymptotic? Asymptotic? Conceivably. That I don't know. But maybe I should say one thing that I then didn't explain so well. So the asymptotic distribution, it will still depend on the unknown parameter. However, it is always of a regular kind. So it's always, so after, so with these maxima, they don't, they are calibrated coordinate-wise for variability, but not. Coordinate-wise for variability, but not like in your biometric paper, for instance, you would calibrate also with respect to dependence across different coordinates here, right? But this is not doing that. So because it's not doing that, there remains some dependence on what the underlying parameter vector really is. But you avoid this with this bootstrap resampling scheme. You estimate that dependence again. And because everything is within the realm of normal distributions that are non-degenerate with in That are non-degenerate with invertible covariance matrices, these resampling techniques work. If you had singular points somewhere and you do resampling methods, this doesn't necessarily work. Okay. Thanks. So are any last question online, perhaps? Anyone online wants to ask a question to Matthias? Okay, I don't see anything. I don't see anything. So, in that case, I think we'll go for lunch. Well, I'm really hungry. So, thank you, Matthias. Sorry about that. Thanks for the patient listening. Okay. Yeah. So after lunch.