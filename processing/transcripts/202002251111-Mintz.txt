I'm going to go pretty fast. I'm going to give you kind of like the best hits of sort of applying different kinds of reinforcement learning methods to try and closely loop in come up with intervention. So it's going to be kind of very similar to the vein of talk that Susan gave earlier this morning. So before I delve too much into the nitty-gritty, no. No. Oh, okay, it's the down, right. Sort of, uh, broadly, the kind of research problems I'm interested in are ones where we try to leverage individual data to help people make better decisions. And in particular, in the context of healthcare, how can we help individuals leverage their personal data from, for instance, wearable devices to make healthier choices and meaningful behavioral change? So, really, this kind of regime over here where we have a bunch of individuals. Where we have a bunch of individuals that have some kind of sensor and we're collecting this longitudinal data and we want to influence them with some kind of intervention. So, the main motivation for the work that I'm going to be presenting here has to do with activity and obesity. So, in the United States, obesity is a really big problem. In fact, over 70% of adults in the United States are either overweight or obese. And healthcare costs and complications are extremely expensive. And unfortunately, you know. And unfortunately, you know, the best, or rather the least side effect way we have of treating obesity is through diet and exercise. And something that I don't think I need to convince this particular room of is that the use of mobile and wearable technology can really help us potentially with leveraging all this patient-level data to come up with recommendations for individuals to help them make healthier diet and exercise decisions. So, the main sort of design challenge that I'm going to be talking about in this. That I'm going to be talking about in this presentation is how can we design these personalized interventions in an efficient, effective, and cost-effective manner. And I'm going to be sort of talking about two broad problems that fit into this design dynamic, two-ish problems, and different methodologies and results that we use to solve them. So, first scenario that I'm going to sort of describe is very similar to what we saw earlier. Is very similar to what we saw earlier with sending messages to individuals to try and encourage them to do some activity over the course of a trial. So, suppose we have our co-order patient and we have an array of messages that we can send to them each day. Depending on what kind of message, we want to potentially increase the total amount of steps that they take over the course of the interventions, right? So, this would be our reward sigma. So, one way to model this is we've heard talk of like multi-armed banded models, and the idea is to think. Models. And the idea is to think of each message as sort of the arm of a slot machine, right? And depending on which arm I pull, I get some kind of random reward. The issue with sending messages is if we send too much of a particular message, patients are going to become habituated to that message. They're not going to respond as much. On the other hand, if we throw in a mix of messages and potentially a message that they haven't seen as much before, there's some kind of recovery effect where less recency might increase. Less recency might increase the effectiveness of a particular message. And so we developed this model that we called reducing or gaining unknown efficacy bandits, or rogue bandits for short. And essentially, the idea is that we assume that the reward here being the step count, we can think of it as a sample from a random distribution that depends on some time and variation parameters of the patient. So, you know, potentially like their demographics. Like their demographics, genetics, stuff like that, and also time-varying parameters. So, here we can think of the recency of a particular message, the nudge factor. And the way the time-varying parameters vary is according to the set of dynamics that are a function of the policy that we've implemented so far and the current state that those time-varying parameters are in. And our goal is to maximize the expected reward over the course of the intervention. Or alternatively, this is phrased in the RL literature, we can think of an equivalent objective. We can think of an equivalent objective of minimizing regret, where here we're comparing how many steps we were able to achieve from the patients using our policy that we're calculating versus some oracle that knows the magical messages to send to the individual each particular day of the intervention. And so in the paper, we kind of analyze this model. We come up with two different algorithms to tackle this. One of them is an upper confidence bound method, where essentially we're Where essentially we're trying to leverage this knowledge of the dynamics to do some maximum likelihood estimation and optimistically give the message that we think has the highest probability of giving us the reward. And we also present an epsilon greedy method. So we're going to play the message to the individual that we think has the highest maximum likelihood reward, and sometimes we're going to throw in a random message in there to encourage some exploration. Random message in there to encourage some exploration. The reason is, unlike the scenario that we had before, here we're not necessarily interested in causal inference later on. We're really just interested in figuring out what the most effective message is that we can send the individual. So to validate these results, oh, I'll hit it down. Okay, here we go. We simulated a clinical trial based on some data that we had previously. And we modeled patient decisions. We modeled them as these utility maximizing. We model them as these utility-maximizing agents that, based off of the message they receive, sort of nudge them towards wanting to do more exercise or less exercise. And we call this rogue agent model or rogue-stackelberg model, kind of like rogue agent, kind of like Jason Bourne here. Where here the agent is to imply that the patients have their own agency and how they choose to do the exercise. We compare it to six existing algorithms. So, several algorithms that have to do with stationary bandits, like That have to do with stationary bandits like UCB1 XP3, for those of you familiar, as well as algorithms that are traditionally used for non-stationary bandits, like the setting we have here, which include discounted upper confidence bounds and setting window upper confidence bounds, which are really designed for settings where, oh, and also restarting EXP3, which are really designed for settings where we have sort of like setting changes in the reward of a banded arm, but not necessarily for scenarios where the reward is actually dependent on the policy that we're applying. On the policy that we're playing. And also pure exploration strategy, because we really hope that we can at least beat completely random messages in the algorithm that we're proposing. So we ran this comparison for 1,000 polls where a year, each poll we can think of as a single day where we send the daily message to the patients and 10 replicates per patient. So we compared cumulative regret as well as the average reward. And the reason we do this dual comparison in this sort of non-stationary This dual comparison in this sort of non-stationary setting is we can end up in a setting where we have very bad, not necessarily terrible regret, but we can still be pretty close to what the optimal reward is. Because you can be slightly off and still come close in these non-stationary cases. So first let me show you the regret comparison. So here on the left is all the algorithms. So as you can see, you can't even see the cumulative regret lines of the algorithms that we're proposing. Regret lines of the algorithms that we're proposing. Basically, all the traditional methods are sort of obtaining linear regret, which is kind of the worst case scenario. Moreover, only, I believe, discounted uppercomfidence bounds of the traditional algorithms actually beats pure exploration. So you're better off doing something completely random in these non-stationary settings as opposed to doing a classical kind of banded algorithm that you might want to use. And when we zoom down into the level of And when we zoom down into the level of the algorithms that we propose, here in the blue is the upper confidence bound method, and in the orange is the epsilon greedy, we can see that they get this sublinear regret, which we have some theoretical guarantees that this kind of meets. And moreover, rogue UCB will have less cumulative regret just because it explores less than epsilon greedy. So depending on sort of what the goals are of what you're doing this randomization for, you might want to opt for one or the other, depending on what. For one or the other, depending on what kind of exploration you want to do. Doing the cumulative, the average reward comparison, we see a similar thing where both epsilon Green and Rogue UCB get very similar rewards. Difference here being that, again, Rogue UCB, because it's based off of it's doing less of these random, completely random exploration steps, has a bit higher average reward in the last step amount. Something like sliding window, upper confidence amounts tend to do really bad because. Confidence balance it to do really bad because it's not capturing the whole trend of the individual it's seeing. It's really throwing out a ton of data and not actually seeing what we want them to capture. Okay, so moving on. The other problem that I'm going to describe has more to do with designing the actual interventions. We were interested given data of how many steps and how much food a patient eats, can we? Food a patient eats, can we figure out what the right step count to give to a particular patient should be to encourage them to do more exercise, and whether we should schedule a clinical appointment for them to encourage them to adhere more closely to the intervention. So, here we kind of have two main pulls of how we can make decisions. So, the first one is, again, setting this exercise goal. The second is kind of this binary decision where each day of the trial I can decide zero one. I'm scheduling the person in to meet with a clinician to sort of encourage them to stick to their diet and exercise. Encourage them to stick to their diet and exercise regimen. And for the model of the patient, we assume that there's sort of like three states that we really want to track. First, the physical system states, so relevant for us in the weight loss setting, this would be like the actual weight of the patient in question. The motivational states. So think of this as sort of like a cognitive aspect, right? Like how motivated is a patient by an exercise goal? How motivated is a patient by having a visit with a doctor? And the final set are the actual. And the final set are the actual patient decisions, which for us is how much food and how much exercise a person actually decides to do each day of the trial, given all these things. So we form this kind of dynamical model where we have the dynamics of the physical states of the patient dependent on their decisions and the physical states of the previous time step, their motivational steps depending on the policy that's implemented by the clinician, as well as the other decisions. Their decisions, physical states, and motivational states. And the patient decision model is actually, again, a utility maximizing agent. So we assume that they have some kind of utility function that has to do with their goals for their weight in the future. So in particular, they might want to reduce their weight in the future. They might have some ideal amount of exercise, an ideal amount of food they want to consume in a particular day that relates to their cognitive levels and different influence from the clinician possibility. And different influence from the clinician policy, and they want to find the amount of steps and the amount of food that sort of meets this kind of requirement. And we assume that we observe only the partial noisy observations of both the physical states and the decisions that the patient gets, because we're assuming we're getting this all from accelerometer and mobile data. So, moreover, something to note, another feature of this model is the clinician policy only interacts with the motivational states of the patient, doesn't interact with the The motivational states of the patient doesn't interact with the physical. This is again to capture that we're only really motivating people to do more exercise and eat healthier. We're not, you know, physically going into their house in the middle of the night to perform liquid suctions to reduce their weight. So we want to focus there. In the paper, we sort of talk about how we can use different kinds of optimization algorithms to actually train this model and learn all these unknown parameters. Parameters, and we can convert it into sort of a full Bayesian method where we can compute a visterior over what we think the weight of the patient would be by the end of the intervention and use that for prediction. So, let me show you some of those results. Are you using a full Bayesian? So, you're using like normal errors, you know, you're doing a complete multivariate distribution, not a complete multivariate distribution, Laplace errors, not normal errors. Yes. So that's morely a computational issue. So that's morely a computational issue. So this ends up, the algorithm to train this ends up being a mixed integer program. Yeah, so when you use normal errors, quadratic mixed integer is just computationally a lot harder to solve than Laplace. So that's mainly why we do that. Yes, sir. Are the thetas participant specific? Yes. Everything's participant specific. And is there any hierarchical model to try to borrow strength across patients? Yes. To try to borrow strength across patients? Yes, okay. So, what we did for the prediction part, when we do the training, and I'll actually show MAP estimation next, is we did cross-validation for this where we took all the data from other participants, except for the one we're using right now, and we formed data-driven prior. So essentially, we computed histograms for all these different parameters on the thetas and used that to do our training. Oh, okay. Yeah. Okay. Yeah. The training is within a person. Within a person. That's right. So the training and the prediction is within a particular person. But the priors come to the data. Are from other individuals, right? Exactly. So the idea would be, you know, let's say I already have data from prior participants. I want to incorporate that into biasing my predictions now to try and, at least in the short term, get better predictions. Okay. So, yeah, so this is kind of what this will look like. So, here on the left, we have an example of the posterior that we compute using this method. And you can actually see that we get some really nice descriptive power. Like, these things don't really look, you know, like Laplace or normal distributions. You get these really funky shapes. And moreover, we can use this method to do map estimation, as I sort of described. And here we see an example with 30 days of patient data, predict. Of patient data predicting the remainder of the particular intervention. The red line is sort of the predicted trajectory, and you can see the model does a decent job at trying to predict these things. So we did do also some simulation validation of this, but another thing we did is we actually implemented a version of this algorithm in a randomized control trial that we performed on UC Berkeley's campus. And the main focus here was on calculating exercise goals. We didn't really do the We didn't really do the actual counseling sessions due to budgetary constraints, so we focused on that aspect. We ran this 10-week trial with 64 participants that were UC Berkeley staff. Reason staff were chosen is usually, you know, they go into the university, they sit at their desk, they're fairly sedentary for most of the week. With students, especially in Northern California, where the weather's nice, they walk around too much, so we wouldn't expect to see as much of an effect. Although we did run a pilot study with students later. Run a pilot study with students later. 34 were randomized into the intervention group, 30 were in a control group, and they both received an iOS application that would give them goals during the week to sort of step goals during the week. The intervention group received goals that were personalized using the algorithm, similar algorithm to what I described before. And the control group got the, essentially, like a Fitbit intervention of a constant amount of step goals per week. Amount of step goals per week. So, this is sort of what the iOS app looked like. So, they would log in, would show you how many steps you did today, and then they would give you a summary also of how you've done so far in the week. You know, green means you met your goal, red means you didn't meet your goal. And sort of our collaborators mentioned that we should include this kind of freeform contact us, 'cause turns out that when you do sort of like NLP analysis on this, it gives you a lot of interesting insights about what motivates particular individuals to. What motivates particular individuals to actually join exercise programs? So, with this actual study, how did you get your prior? How did you develop your prior for your parameters? For the actual study? So, for the actual study, we used the prior from the clinical trial that we used for validation on the other model, for the simulations that we did for the algorithm. Yes. So, this is sort of how this reinforcement learning algorithm. Reinforcement learning algorithm worked. So we get the data from the individual patients, it then gets stored in the central server. We did our inverse reinforcement learning, which is the parameter estimation for the model that I described before, and then used sort of like an optimization algorithm, or like this classical reinforcement learning algorithm at this point, to compute the intervention for the next week. So the step goals for each day for the next week, and then we distribute it to the patients every single week. So, these are some of the results from the trial. And this is protocol. And here you see the intervention group in blue, the control group in red. And what you can see is that overall, the intervention group ended up doing a lot more activity than the control group, approximately 1,000 additional steps a day. So pretty meaningful. Not only that, there is a run-in effect, right? So both groups, by the end of the study, ended up walking less than what they did in the beginning. Up walking less than what they did in the beginning, mainly because when people sign up for you know a high-tech physical study, physical activity study, they're really excited to do like a lot more exercise in the first few days. But the intervention group decreased less. Something else that I didn't include these numbers here, but I believe you can, these should be included in the actual publication of the study. There was also higher adherence from people that received personalizable than the ones that received sort of the generic Fitbit. Sort of the generic Fitbit intervention. So more of them kept up with using the app by the end of it. So that said. So did you, when you did the classical statistical analysis for your two-arm group, what kind of, so it was like a thousand, average, a thousand was the difference between the means? Yes. That's right. And was that, what's the standard deviation? I wonder what kind of confidence you have. I'd have to look at the actual paper to. To look at the actual paper to tell you. But it was significant. The effect was significant, I remember that much. But we could look it up in detail. It should be in the paper. All right, so I guess I'll conclude here. So for my kind of collaboration and challenge opportunities, basically from the kind of research I displayed, I'm really interested in learning more and collaborating about problems with. We're collaborating about problems with safe reinforcement learning and applications with our mobile technologies, as well as mobile intervention design. Two kind of areas that I don't know a lot about, but I'd love to learn more because I know that we have some experts in the room about this are using different data fusion methods, especially with unstructured data. For instance, like let's say we get these text responses and trying to come up with intervention design and also how we can use this sort of methods for tracking chronic diseases and neurodegenerative diseases like Parkinson's disease. Diseases like Parkinson's disease, for example. So I'll end it there. Thanks, and any other questions you may have, you can ask me. I have another question. Sure. So I just, one of my colleagues ran a study, a really interesting study, in which they did step goals. I'm just wondering how you would mathematize this. You would mathematize this, what I'm about to talk to you. So they ran two arms, so nothing fancy or anything. One arm adds a lot more variability to the step goals. So like you could get anywhere from 10% to 120% of your last week's performance as your step goal for the next week. And it was randomly decided. So some, some, I can't remember if it was the day at the day level or the week level would be. The day at the day level or the week level, to be honest. But you know, some days it was like you got a total vacation, other days were hard days. Gotcha. And they compared that against a standard approach of slowly increasing step goals, which is what the behavioral scientists know, not what Fitbit would do. And they found that, and the average step goal would be about the same for these two groups. It's just one had higher variance, the other had less. And they found that the arm that got the higher variability. That got the higher variability did much better. And they feel that this is all about novelty, excitement. And I don't know how you would match, like when you think of the kinds of system dynamics models you're building, I don't know how you pick that kind of stuff up. How do you build that? The fact that variability all of a sudden is therapeutic. So variability we didn't necessarily capture, but we sort of based the But we sort of base the model off of, like, obviously consulting with our collaborators at UCSF and also from social cognitive theory. So, something that we built in, for instance, is that if a person meets a step goal in one day, they're more likely to want to meet the step goal in the following day. Right? So, this is like a self-efficacy input into it. Or we want to avoid over-parameterization, so the way the goals impact the participant. So, the way the goals impact the participant is they're more upset if they don't meet the goal than if they exceed it. But the variability is definitely an interesting thing. I wonder also, something that we learned kind of anecdotally from talking to participants is that there's a lot of like sort of like convenience and scheduling effects, right? Like, basically, people want to walk more on weekends, they want to go outside more on weekends than they do during the work week. So, it'd be interesting to see where also like the So it'd be interesting to see where also the randomization sort of lined up with how people were doing during the week. But yeah, but I think that'd be interesting to talk more about. I need to think more about including that as well. Yeah, it's very interesting. Variability is a therapeutic evaluation. Very interesting. So if you sorry the decision points So the decision points there were very frequent, right? So there's like a daily decision point. Yeah, that's right. What if, so there are scenarios in health where decision, it's very rare, you will have all the decision points happening at daily level, some might be weekly, some monthly, and sometimes it might be user-initiated. So could this technique work in that sparse? Technique works in that sparse decision? So you'd have to do something more similar to the second method that I was describing. So, right, so bandits sort of rely on frequencying decisions in order to get you close to what it is you're trying to optimize in a fairly fast way. So it depends, right? I guess it would have to be very domain-specific. Also, there's risk considerations about how the decision is initiated. Because sort of with classical bandits, risk is sort of like kind of a band-end, right? You're just trying to explore as much as you can and get eventually to the right answer. Whereas you'd have to probably do conservative reinforcement learning or conservative bandit methods if there's more risk considerations. All right, good thanks, thank you. Do you want to have any more discussion?