I didn't realize we would destroy the schedule before I talk. This talk may be a train wreck as well, because I'm worried that some of you don't know what a dynamical system is, and I'm not going to get through to you in 20 minutes. And the rest of you, some of you know lots more than I would be able to say, so you might not get much out of this. But let's try and explain what a dynamical system is by starting with some very trivial, well, what to me, trivial examples. It's me two examples. So here's a scalar linear differential equation. I'm always interested in differential equations, dynamical systems. That's not true. I do some maps as well, but mainly for this talk we concentrate on differential equations. So dx by dt, the rate of change of x with time, is equal to lambda times x. That's what we call a linear equation. There are a couple of interesting things there. We're looking for a solution x of t defined for time greater than or equal to zero. Defined for time greater than or equal to zero. We have an initial condition, so we have to specify what the value of the solution is at the initial time, which is time zero. And there's also this parameter lambda in there. And we're not interested really. So when I started off life as a numeric analyst, you were interested in solving this problem as accurately as you could, or as over an infinitely long time interval, and the emphasis was always on the initial value problem as set. Value problem as set. But in dynamical systems, we're not interested in solving one problem, right? As you know, from the applications that we're all interested in, you don't know the parameters exactly. You don't know the differential equation exactly. You certainly don't know the initial condition. And if you're interested in some physiological system, often you don't want to go to the initial condition because it's a single cell in the womb developing into a fetus. And it's very hard to model hematopoiesis. To model hematopoiesis starting from developmental biology. So we're really interested in what's going on with this differential equation for a whole class of initial conditions and for a whole class of parameters. So how does the behavior change with the initial condition? How does the behavior change with the parameter? And this differential equation, anybody who's done the first course on differential equations knows how to solve it. Equations know how to solve it, this is the solution. And most of my pure mathematician students stop there and think they're finished. Look, because they've written down a function. But that's not what we're interested in, dynamical systems, okay? I'll do a harder example in a minute and I won't even write down the solution because for the interesting problems we can't and also it's really not incredibly useful. So the things we're interested in here are really the properties of the solution. Are really the properties of the solution. So you see here, from looking at the solution, you can see if the lambda is negative, then as time goes to infinity, the solution goes to zero. And if you run backwards in time, the solution gets larger. And if lambda is greater than zero, it's the other way around. The value of x grows as time goes forwards, or decays if you grow backwards in time. If you broke backwards in time. And there's a couple of other interesting things going on. The sign of the solution x of t is always the same as the sign of the initial condition. The solutions can't cross x equal to 0 here. And x equals 0 is really interesting. If I start at x0 equal to 0, then x of t equals 0 for all time is a solution. That's called a steady state. And then we say the steady state is stable if lambda is less than 0 because the neighboring solutions are approaching it. And it's unstable if. And it's unstable if lambda is greater than zero because everything's going off to infinity. Now, interestingly, you don't need this to say any of those things, right? If I just look at the differential equation at the top, dx by dt is the rate of change of x. Assuming my parameter lambda is always positive, so if my parameter lambda is negative, then you can see that the rate of change of x is going to have the opposite sign of x. Of x is going to have the opposite sign of x. So if x is positive, x is decreasing, and if x is negative, x is increasing, so everything gets squeezed to zero. You don't need this formula to get that. So if we go on to a slightly harder example, another scalar equation, the logistic equation, this time I'll multiply my lambda x by 1 minus x. Again, there's an exact formula. I don't need it though. What we do is I define this. is I let I define this function f of x lambda to be lambda x1 minus x. So the x is my state variable, the lambda is my parameter, and I draw this graph of f of x lambda, two different cases depending on whether lambda is positive or negative. If it's positive, it's a divert parabola, lambda is negative, that's like an ordinary parameter. And then you just see the rate of change of x has the same sign as f. So where f is positive, So where f is positive, x is increasing, and where f is negative, x is decreasing, and so on this real line here, you can just draw the arrows. So here where f is negative, x is coming this way, here it's going that way, there it's going that way, and you immediately see that in the case where lambda is positive, if you start with any initial condition where x is positive, you'll end up at 1. And in the case where lambda is negative, any initial condition with x less than 1, you'll end up. Condition with x less than 1, you'll end up at 0. This is stable in this case, this one is stable in this case, the other one is unstable. So there's a change in stability as lambda crosses 0. And we also have some solutions which escape to infinity, which, if that's some engineering system, that's really bad. But that's because this model doesn't capture everything. Capture everything. Right, now let's go to higher dimensions. So these are the famous Lorentz equations in R3. I now have three variables. I have three parameters as well. I've chosen some particular values here, following what Lorentz did. I need an initial condition. Now it's a set of three numbers, one for each for x, y, and z in half three. Z in R3. And then the solution also can be thought of as a vector in R3. And if I plot the different components of the solution here, so I think the red is X of T, the blue is Y of T, which is always quite close to X of T, and the black is Z of T, you get these strange growing oscillations and then switches, and it just looks like a real mess. Now, for many years, this was a standard way of displaying the results of differential. Displaying the results of differential equations and its horror. Now, this is where the power of dynamical systems comes in. Because what I do instead is I'm going to plot this exactly this solution here. I'm going to plot it as a parameterized curve in R3. And forget your advanced calculus stuff if you ever did that, right? Everybody hates parametrized curves because you do integrals over them. Here I actually, I've already got the solution of the differential equation. I'm just thinking of it as a parameterized curve to plot. Thinking it was a parameterized curve to plot it, and that's very easy to do. And when I plot it, you get this beautiful attract. So, this and this are exactly the same thing, just displayed different. Okay, it's a bit small down there, so let's see it again. Here it is again. Okay, so why is this curve so elegant? Why does it look so good? Okay, well, there's a bunch of things going on here. We've plotted the solution as a parameterized curve in R3. Solution is a parameterized curve in R3, but the initial condition also lives in R3. And that initial condition is enough to specify a unique solution of the ODE. And because the solution of the ODE is unique, the solutions can't cross each other in this space. So if the key thing is in the definition of the equations for the Lorentz equation, once you know what x, y, and z are, that determines the f on your right-hand side, and that On your right-hand side, and that then determines the rate of change of each variable. So that's unique. So if you ever come back to the same point in three-dimensional space, you go off again in exactly the same direction. Okay, and we also have the right-hand side of the equations. I don't have them written up here, right, but let's go back to the slide. These are all nice continuous differentiable polynomial functions. In fact, they're very close to being linear. There's only two nonlinear terms in here. And so, what that means is if I move slightly in R3, the direction of the vector dx dt dy dt dz dt only moves slightly. And so if I start at one point here, I go in some direction. If I start nearby, I go in a very similar direction. So not only do they not cross, they kind of channel each other around. And then there's all the chaotic attractor stuff going on here as well. So there's So, there's contraction in certain directions, and that's very common in differential equations. What makes the strange attractors so interesting is they have expansion as well. And once you have contraction and expansion, it's like kneading dough. Everything gets well mixed. Okay, so my message to you is: you think of phase space is the space that the initial conditions. Is the space that the initial conditions belong to. The crucial feature here is that the dynamics depends only on your position in this space, not on the value of time when you're there. So in particular, systems with delay, noise, forcing are excluded from this setup for now. You're not going to be able to model those as a dynamical system in R. System, you know, I'm going to do that as part of my time, but that's going to be tomorrow. Unless the schedule gets even more blown up. Okay, once I have a phase space, which remember is just my space of initial conditions, so let's call it Rn for now. Well, you can think of it as Rn, we can define an evolution operator. And the evolution operator is actually a monoid, for those of you who are pure mathematicians here, or we talk about having Or we talk about having, we always talk about having a semi-group property, but it's also commutative. But really, the easiest way to think about it for anybody else is it's just a convenient piece of notation. So I forgot to actually write it down here, right? But it's just the map. You just define s of t to be the map that maps you from the initial condition you know to the solution at time t. To the solution at time t. So this is, so I'm thinking Rn, it's R3 if it's Lorentz, and it maps you to a different point in Rn as well. And this operator is what lets you get into the math to studying these things. So once I have an evolution operator, I can start talking about invariant sets. So a set A is forward invariant under this evolution. Is forward invariant under this evolution operator if for every point in this set A if I do S of T on that point I'm still in A. And it's backward invariant if you can do the same thing backwards in time. This is slightly tricky because we usually define the evolution operator only forwards in time for a semi-dynamical system. But you can once this exists, you can make this work. And then it's invariant. This work. And then it's invariant if it's both forward and backward invariant. Invariant sets include steady states, periodic orbits, and more exotic things, one of which I've already shown you, which was the Lorenz attractor, right? So there's an invariant set there with some weird dynamics on it. Invariant tori is another one that comes up a lot. So here's the periodic orbit, right? So the thing is, in phase space, if I start at some point, if this is my initial condition, if I come back here, If I come back here, then because of the way the system is defined, because I'm here again, I have to do exactly the same thing as I did last time I was here. So I go around the orbit, and I keep going around forever and ever. Now, you have a periodic orbit, or if you have a steady state, and we saw steady states before, the Lorentz attractor also has some steady states, we're going to be interested in the stability of them. So I'll just do this for a steady state. So if I have a steady state u star in my Rn, then I look, I let V be a small perturbation from the steady state, and then I linearize the equations, and I end up with an equation that looks a lot like the very first example I showed you, remember? The dx by dt equals lambda times x, except now this is a system, not a scalar. So A here is a matrix. So A here is a matrix. But just as in the very first example in R1, when lambda was negative, I had stability of 0, and when lambda was positive, I had instability. We get the same thing gets generalized to this n by n matrix. If the eigenvalues of this matrix all have negative real paths, then you stay stable. And if it has one real positive real path, that gives you an unstable direction. And then flock A theory can be used to generalize these ideas to empiric orbits. I don't have time to do that. Okay, so once I have some invariant sets and I'm interested in their stability, I was talking about parameters before, so let's go back to the parameters again. So if I have some differential equation du/dt is some function of the state and the parameter. The state and the parameter. I want to know what happens if I vary the parameters. And the implicit function theorem is one bit of math I use. So I've come over the years of talking to physiologists and pharma people, I've come to a couple of really incredible revelations. One is that pharmacists and physiologists have as much trouble talking to each other as they do to mathematicians, because they use completely different units. It did never occur to me that other scientists had. It never occurred to me that other scientists had difficulty talking to each other. I don't know why. And then the other thing is any problem you do, if you wanted to work in the real world, if you want to think in a bank space or some really abstract mathematics, fine. But don't mention that to your collaborators. If you want solutions that work, you have to use the lowest level map possible to get to the solution. And that way, you're The solution. And that way it'll be understood by the most people. So, anyway, the implicit function theorem, though, is one that's really useful. All it basically tells me is if the eigenvalues of that matrix, so those are some quantities that you can get by solving an equation, don't worry about it, if the real parts are non-zero, so just a condition on some numbers, then as the parameter is varied, the steady state moves continuously and its number of eigenvalues with positive and negative real posts doesn't. Values with positive and negative real purse doesn't change, so that means the stability doesn't change. So, this gives me immediately a condition on my ordinary differential equation such that I can vary my parameter and nothing goes badly wrong. Things just move slightly. And then a bifurcation is a qualitative change that occurs in the dynamics as a parameter is varied. And so, these must occur when the real part of lambda is equal to zero, and they occur in a couple of different ways. And they occur in a couple of different, there are lots of different bifurcations, but the simplest ones are what we call steady-state bifurcations, where real eigenvalue crosses zero and the number of stability to steady states changes, or you get a Hoff bifurcation, and these occur a lot in delayed differential equations where complex conjugate pair of eigenvalues cross the axis and you get a periodic orbit as well. And there are lots of more complicated bifurcations. And there are lots of more complicated biifications as well, but you need to take a full course to get those, not a 20-minute talk. Okay, my time's already up. Well, let's just introduce delay differential equations. So, tomorrow I'm going to carry on this with delays. And delays arise in physics and engineering due to transport, communication, processing. There are probably more, but those are the main ones, right? It takes time for a signal to go somewhere. For a signal to go somewhere, it takes time for some product to be transported or it takes some time to be processed. This is really bad in physiology because these are not separate things in physiology, right? So usually all three of these are mixed up together. So if I want to signal to produce more of a certain type of blood cell, I have to first produce the hormone, then I have to transport it to the receptor. Then I have to transport it to the receptor, which then has to process it. So you could think of those as separate delays. And then the biggest ones: if I start making new red blood cells, there's time. There's a maturation delay involved in that. And then the other thing is there's not really any delays in physiology at all, right? It's always something going on. You just might not be seeing it. So it's going to be a modeling choice to incorporate a delay rather than model the entire process. Rather than model the entire process. And I'll show you tomorrow why, if I wanted to model blood cell production properly, I would need at least 10 to the 11 equations. And I'm not going to do that. Okay, so I think, can I stop there? Yeah, sure. And then can you queue up my slides and I'll keep the recording going and just in other words, you're going to delay? In other words, we are delaying the delay to tomorrow. Yeah. There are going to be other things going on in between. But they're not important. We don't need a model that we're going to be able to do. No, no, it may even have to advance the slide. I think I just said it. Alright. Alright, so to pick up from Tony, so I think what I'll do today is I'll tell you a little I think what I'll do today is I'll tell you a little bit about a general dynamical paradigm to, I hope, illustrate the power of dynamical thinking. So, I'm going to talk about hyperbolicity in general terms, what it is, and why we think it's relevant in physiology, and why we think it's a big challenge in physiology, and why it's challenging for inference also. The point of this is to illustrate that I think dynamical thinking. I think dynamical thinking derives its power from being geometric and general. So, I'm going to show you an effect that arises in a deceptively simple model. It's linear before forced. But I'm going to then claim that, in fact, this dynamical effect that arises even in this simple linear model is relevant for highly non-linear physiological models. So, yeah, so in dynamical systems, one of the most successful paradigms that people have studied for the last 50 years at least is the notion of hyperbolicity. Right? So, and Tony touched on this. The idea of hyperbolicity is simply that in phase space, you have directions that are contracting and you have some directions that are expanding. And it's very interesting when both expansion and contraction are happening at the same time. At the same time. So the simplest example of such is just in 2D. So if I write down something like, let's say dx dt is, say, lambda x and dy dt is, say, minus beta y, where both parameters are positive, then the origin The origin doesn't move, right? So that's a stationary state, a fixed point. And what you observe here is that lambda being positive causes exponential growth in this direction along an invariant line here, right? And beta being positive causes exponential contraction in this direction, right? So this is an invariant set in the sense that Tony defined, very simple one, right? Just a point. And you can see that in a neighborhood. Point. And you can see that in a neighborhood of this point, the trajectories will be hyperbolic, right? Hence the name. Sorry, lambda and minus beta are the two eigenvalues which are. That's correct, yeah. Yeah, exactly. So this is the picture of hyperbolicity in the fixed point case. The dynamical innovation behind hyperbolic theory was to think about hyperbolicity not at a point, but along entire orbits. So now think in phase space. So now think in phase space as you apply the evolution operator, you move around, and so you imagine tracking orbits. And then in the case of hyperbolicity, what you want to think about is having contracting directions and expanding directions that follow you along orbits. So in other words, along any given orbit in phase space, there will be some directions wherein nearby orbits will be exponentially attracted to your base orbit, and other directions where you will be exponentially repelled. You will be exponentially repelled from your base orbit. So, this is a very interesting kind of contraction and expansion picture along base orbits. And the question that dynamicists have been looking at for a long time now is, well, what is the implication of having this type of contraction and expansion along orbits? What does it mean for the global dynamical picture of the system? Right? Right, and what it turns out that very deep things are true. So, what happens is this: so the expanding directions amount to giving you some orbital instability, right? And that turns out to be linked to sort of very deep statistical regularity of the systems on a global level. So, it's one of the most highly successful dynamical paradigms that Smooth or Gaudic theorists have studied. That Smooth or Gaudic theorists have studied. But there are some things that are left to be done, to put it mildly. And I would say that one of them is we need to find, I would say, or we need to have a better understanding of how hyperbolicity actually gets generated by models that represent physical systems. We have some examples of hyperbolicity being generated by geometry. For example, in remodeling, For example, in Riemannian geometry, there are geodesic flows on negatively curved manifolds. But what I have in mind here is speaking to people who model physical systems, what mechanisms can generate this hyperbolicity? In particular, does this form of hyperbolicity I described actually arise in mathematical physiology? We think yes, and if yes, what are the implications for clinicians, for influencers? For clinicians, for inference, for understanding what clinicians are seeing. Okay, so now let me turn to this simple system. Okay, so here's a recipe that we call delay-induced uncertainty, though today I will not talk about delay because I'll stay synced with Tony. So let's just call this a So let's just call this a recipe for the inducing of uncertainty. I want to come up with a way to create hyperbolicity, but in such a way that the recipe is relevant for physical models. Okay, so this is the idea, and this won't totally make sense until I show some pictures, but let me plant the seeds anyway. So the first idea is that I want to start out with an invariant set. An invariant set. So, for example, a fixed point or a limit cycle. So, I want to start out with something invariant, a periodic orbit or a fixed point, and I want it to be stable. So, I want to start out with something that's stable. So, that means that nearby orbits will be attracted to the stable one. And think periodic orbit for this time. So, I have a periodic orbit, and I just want nearby orbits to... And I just want nearby orbits to be attracted to the base one. That's stable. Okay, but interesting things happen in biology when things are not too stable. So I want my invariant structure to be only weakly stable, meaning that if I start kicking the system, something can happen. The second ingredient I want is something we call a shear. I usually think of shear in terms of just In terms of just kind of velocity gradients and airflow or fluid flow. So just think of shear as, yeah, just a mathematization of the idea of a velocity gradient, a fluid flow, or an airflow. So I have this system now with one and two. I've got like a weakly stable limit cycle. And near it, there's this shear. So these things are like they sort of... They sort of make the system excitable. So I'm setting the stage to have something like an excitable system. And then what I'm going to do is I'm going to kick it. I'm going to externally force the system, but I'm going to do it in a gentle way. I'm just going to provide sort of gentle kicks to this 1 and 2 setup. And then what I want to see is, well, what happens when this gentle external forcing interacts with the geometry that I've configured from 1 and 2? And what can happen? One and two. And what can happen is that this interaction can produce hyperbolicity. And in particular, one can get chaos out of it. And you can have a situation where what before was this weakly stable limit cycle is turned into now, once you force, a complex invariant set, something like the Lorenz picture that Tony showed. Okay? And there are Okay, and there are some mathematical characterizations of what the dynamics are like, which I'll speak about tomorrow. But overall, I see this recipe as a way to produce hyperbolicity and a way to do it, I think, that's hopefully translational. Okay, so let me show you something to make this more concrete. All right, so here is, actually, let me. So here is, actually, let me show the equations first and then I'll go back. Okay, so I'll bounce back and forth. So here is, I think, the simplest way to create hyperbolicity according to the recipe that I just showed. So I want to start out with a two-dimensional system on a cylinder. So the phase space up there, the cylinder direction is called theta, and the z direction will be the cylinder. The cylinder direction like this. So I have theta like this, and the cylinder direction up and down. And now I wanted to find the following system of two linear differential equations on the cylinder. Very simple. So in the z direction, dz dt is just minus lambda z. So in the z direction there, I just have this, right? So same idea. So in the z direction, I just contract towards z equals zero. In the theta direction, In the theta direction, I'm going to spin. I'm going to spin, but crucially with a speed that is z dependent. So if you look up here, when z is 0, I'll be spinning at angular velocity 1. And then assuming sigma and lambda are positive, in particular assuming sigma is positive, as I move up, I start to spin faster. And as I move down below this equal zero level, I start to spin more. This equals zero level, I start to spin more slowly, right? So you think higher means faster rotational speed, lower means slower rotational speed, but in the z direction, which doesn't see the theta behavior at all, I just am contracting towards the z equals zero level, right? Okay, so back to the picture from the previous slide. All right, so this is what this is what I envision this system doing. All right, so. Alright, so I'm, and my son will tell you this, not the best artist, so I didn't even try to draw 3D things. So I drew 2D things and I'll make 3D things like this. So the picture is, the picture is the left column and the right column are glued together to make a circle, right? So this column and this column are glued to make a circle, and then the cylinder direction is up, down, right? Okay, so. Right? Okay, so I've drawn the z equals zero in gray here, light gray. So that's the limit cycle, z equals zero. So a point that starts on that gray line, which is actually a circle, will just do this forever, right? It'll move across, glue back, move across, and just do that forever, right? Okay, so that's the setup. And now, going back to And now, going back to that recipe that I showed you, I've got a weakly stable thing here, right? This limit cycle. When is it weakly stable? I would want lambda to be small in magnitude, right? That would make the contraction weak. Okay, so take a weakly stable thing, and now I want to kick the system, all right? So I'm thinking of a kick here as an instantaneous deformation. Instantaneous deformation of phase space. So just apply a deformation of phase space instantaneously, and let's say that that deformation transforms the limit cycle into this sinusoidal shape. So the kick is represented in blue. Of course, the kick is going to kick all of phase space in general, but that's what the kick would do to the limit cycle, say. All right. Then. Interesting question. Yes. What do you mean that a kick is a deformation of phase space? What do you mean that a kick is a deformation of phase space? Like, is this different than a forcing phase space? Oh, so I mean, I mean, I'm not, so I mean specifically, I'm not adding a forcing term that's continuous in time to the differential equation. Instead, I'm literally kind of stopping the dynamics defined by the differential equation and just applying a map to phase space in an instant. So I'm literally transforming phase space by an instant. Transforming phase space by an instantaneous map. Then I'll restart the dynamics. A very fast eater. Yeah, so yeah, these kicks ultimately will come to represent glucose kicks in a glucose insulin system. So think somebody's eating something, but super fast. Right? Chump them. Yeah, so the way I eat Kit Kats, say, very fast. Very fast. These kicks don't have to be instantaneous at all. This is just, I think, the simplest way to talk about it. Okay, so I kick, I deform the phase space, and in particular, my limit cycle gets deformed into the sinusoid in my example, right? Now what I do is I ask what happens after this kick. So I've kicked the system with just a jolt, and now With just a jolt, and now I just let the system relax. So I just let that original dynamical system, the linear one, relax. And what happens? So in the z direction, we know we have to contract back, right? So this thing becomes squashed. In the theta direction, though, something interesting happens, right? Remember that the angular velocity is higher above, lower below. So this wave front here, So, this wave front here gets stretched and folded. This one, kind of the reverse happens, and so you have this stretch and fold geometry that kicks in while the system is relaxing. So, that's the idea. This is a simple geometric picture, right? It's a kind of kick-relaxation cycle, right? Then, this is what happens if you do it once, right? A dynamicist would ask: well, okay, what happens if I continue doing this? Actually, in a way, you've kind of done it lots of times here already, right? Because you've got an image circuit, you have a phase. So the actual kick you're experiencing depends where you are. Each of those blue arrows is a different. That's true, yes. Yes. Yes. And then I can ask: okay, what happens if I repeat this picture? Right? So now I want to think. Right, so now I want to think as a dynamicist would about kind of long-time behavior. So I have this, so let me go back to the differential equations now. Okay, so I have this very simple linear system that has those two ingredients that I listed, right? A weakly stable structure, which is the limit cycle, and some shear. For us, that's the angular velocity gradient. And now I force it. Gradient. And now I force it. How do I force it? Well, I force it with this object right here. This is just a mathematical way of writing down kick the system every so often. So at every multiple of some time capital T, I'm just going to kick. And then I have some kick profile in blue. That's the blue arrow object that Tony was referencing. And I kick with some amplitude. Right? Okay, so that's the game, right? So I have a differential equation that I just run, but every time capital T comes around, I apply a kick. Then I go back to the differential equation. So it's sort of like kick, relax, kick, relax, and so on. Okay, and the question is what can happen when you do this? And the answer is, depending on how these parameters Depending on how these parameters are tuned, you can get hyperbolicity out of this. Okay, and in the last, I'll take just a couple more minutes to explain when you would expect to see hyperbolicity and when you wouldn't. Okay, so what would we expect? Well, let's think about the players. I want to think about two things. How could we get hyperbolicity and how might we avoid it? Okay, so the players in this game for this simple model. The players in this game for this simple model are, well, there's this angular velocity gradient parameter, sigma, which we can think of as a shear parameter, right? That tells us how much the angular speed varies with the z position. Then there's a contraction rate, lambda, which tells us how strongly we collapse to the z equals zero level. Then there's the strength of the kicks. And finally, a fourth parameter. This tells us how long we go between each. How long we go between each kick. So, those are the players. So, you can see, even for a simple system like this, there are a number of parameters and their interactions are important. But here's what's true in the end. If I want to create hyperbolicity, right, so if I want to see chaos, what would I do? Well, the naive thing you might do is think about this parameter here. Think about this parameter here where I put sigma on top, alpha on top, and lambda on the bottom. Right, so in other words, I want to think about creating hyperbolicity by having a lot of shear. So let's make, think of sigma as large. I want to kick with some strength, so the system is feeling something, so I want the kick amplitude to be non-trivial. And then, and this is important, even if I kick the system strongly, Even if I kick the system strongly, it won't matter if the relaxation phase just squashes everything immediately, anyway, right? So I want kind of weak contraction. So to get hyperbolicity, in summary, I want this factor to be large, right? Strong shear, strong kicks, weak contraction, right? Last ingredient is, with this diagnostic being large, I've set the stage for the I've set the stage for the emergence of hyperbolicity, but I also need to allow a long time between the kicks, right? Because if you go back to this picture, by making that key diagnostic large, I'm setting the stage for this to occur, this stretching and folding, which will produce hyperbolicity. But to make it actually happen, I need to allow this relaxation phase where you... This relaxation phase where the shear has its effect, I need this to happen for a while before I kick again. So to summarize today's story, if I make this key diagnostic large, I've set my stage for hyperbolicity emergence. If I then allow relaxation time to be sufficient between kicks for this. To be sufficient between kicks for this stretching and folding to happen. So to summarize, we started with a geometric idea, how to make hyperbolicity emerge, and then we thought about it for a concrete, simple system. This model is known as linear shear flow. It was studied numerically by Zozlovsky and then again, both numerically and analytically by... Both numerically and analytically, by Kevin Lin and Lyseng Young. For our purposes, this model serves as kind of the simplest way I can think of to generate this hyperbolicity. Uh-oh. I've been warned. Okay, 30 seconds. I'm here about hyperbolicity. Yeah. Oh, oh, oh, sorry. You wanted to ask me that, and I probably should have answered. You wanted to ask me that, and I probably should have answered. Oh, no, that was Day's question. That would be great. Why do you care about Archibald? Well, so I think I think we just talked about this last night, right? Did I have a good answer then? No, so, so, um, so. So, one way to answer this question is that hyperbolicity will lead to problems with prediction, right? If you have these persistent orbital instabilities, right? One way to think of hyperbolicity is it's kind of like a sustained form of temporal chaos. So, you have this situation where divergence of nearby Divergence of nearby trajectories will be exponential in time, and that's persistent. It's not a temporary thing. So the issue is, if I have a hyperbolic system, so I have this persistent temporal instability or persistent temporal chaos, if you will, the questions I think that are relevant translationally are: what does that mean for inference? And then also, So well, what would it mean? I guess one thing to assess is what would it mean Yeah, okay. I'll respond with a question what would it mean to a clinician to know that this is present in a given physiological system? I mean I think in some sense hyperbolicity gives you stability. gives you stability because it gives you a geometric frame geometric geometric defining features that are persistent. You have, effectively you have chaos. Yes. It matters. So what do clinicians care about? They want to know if they can predict something or if they're going to know what it will be in the future. So if the thing is always hyperbolic, that's okay. Because that Because that effectively means that you have nice statistics and things converge. And if you're not hyperbolic, then you don't, your statistics, your measurements won't converge to something or may not converge to something. You might measure something, and depending on how you measure it or when you're measuring it, you would get a completely different answer. So it depends. Yeah, so it defines. Yeah, so it defines a few different things, but we, I mean. There seem to be a couple of different things being mixed in together here, right? Because when I think of hyperbolicity, that isn't an expanding direction necessarily. Right, yeah, when I say that, when I'm using the term hyperbolicity here, I'm kind of, I guess, implicitly thinking of having both contracting and expanding directions. Right. But yeah, so right, so I'm thinking of saddle orbits basically. Yeah. Yeah. Yeah yeah but I think I think I think the I think the presence of hyperbolicity in physiological systems remains to be assessed. And I think tomorrow Tony and I will talk about the delay angle in all this. Yeah there's one really important thing I didn't mention earlier, right? So you have this thing, when you lose hyperbolicity, so when these eigenvalues cross the axis, you have these bifurcations. The axis, you can have these bifurcations and dynamics change. But the other thing is that these eigenvalues, they're real parts, are giving you the rate of contraction of the nearby orbits onto your stable orbit. So as you approach the bifurcation, the real part rounds go to zero, so the contraction speed gets slower and slower. As you get to the bifurcation, it becomes infinitely lower. That's why every single numerical simulation you ever try to do of one of these all bit maps with the bifurcation. One of these orbit maps with the bifurcations, it always goes around at the bifurcations. Because whatever you set the transient time to in your computation, it's never long enough somewhere. And so I think that's a really crucial thing because the longer the transient time is, so if you have one of these things where you're periodically forcing it, once you get close enough to the bifurcation, your time to converge back to the original orbit is longer than the interval between the kicks, and so you never get. The interval between the cakes, and so you never get back. Another way to say it in a measurement setting with the clinician would be when you're close to non-hyperbolic points, your statistics may not converge to anything. So you try to calculate the mean, but it's just not possible. Those are the points at which all of the dynamics that you didn't model come into the body were what we call. Or what we call noise. But everything else dominates the dynamics around those points. And we'll see tomorrow how this plays in with nutrition and meal choices. Is there a physical interpretation of this? So, I mean, one way to think about it is you want to be able to model some kind of chaos, and this model gives some kind of chaos. But if I look at But if I look at the process that's giving it, it's this like discrete time forcing, essentially. Right. I don't know. Do you think of that as being important to be able to model it that way? Oh, it was to get chaos, right? Oh, totally, yeah. And this was meant to be my effort to give maybe the simplest possible way to generate this type of hyperbolicity that I'm thinking about, but is not in any way essential. Any way essential. The kick almost doesn't matter. Yeah, in fact, the shape of the kick, the exact nature of the kick. So that's the, I mean, for example, I could definitely replace that with a continuous time thing. So for example, when we say that mathematics has some power of generalizability, you take the system and then they abstract it, and you can show that almost any kick will reproduce this problem. So when you look at a physiologic system, When you look at a physiologic system.