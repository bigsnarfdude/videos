Intentional. That's intentional. All right, I think it over. We don't see your image on the on the right. So the beer's organizers can put the PC video stream on the right. All right, take it over. Thank you so much. It's good to see you all. I really wish I was there. Unfortunately, I'm in visa exile, so I'm in Berlin right now for visa reasons. And I couldn't make it. I wish I could. Couldn't make it. I wish I could. It's such an amazing workshop, and I've seen some of the talks, and they were just absolutely stunning. So I really wish I could be there and talk with all of you, but it has to be remote. I want to start my talk with a very by showing a very like mundane video. Okay, so in particular, I want you to look at this frame, which is a situation that I'm sure all of you have been in in one way or another. And I want you to predict what will happen next. So I can tell you right now, this is a video. So maybe you just look at the scene a little bit, right? Like take it all in. Bit right, like take it all in. It's a very mundane everyday situation. What will happen next? Okay, let's look at the video. Okay, so placing the plate. Oh, unfortunate. So basically, right, like what happened here is the person who places the plate, they like fumble and drop the plate and sits after. So why is this interesting? Let me stop that video again. Why is this something worthwhile thinking about? So I think that to me, like these mundane videos. To me, like these mundane videos actually capture best what I'm excited about in computer vision, right? So, when I ask you to look at this frame and predict what will happen next, what goes on in your head, right? What goes on is that you look at this single video frame, and then you understand that you take in the 3D structure of the scene, you understand there's a table, you understand there's a plate, you know, the plates are made out of ceramic, you know how approximately heavy they are, you know that if you place a plate and the center of mass is not above the table, it'll drop and it'll shatter. And what is required for this understanding. Is required for this understanding is really the core of computer vision to me, which is basically state estimation. What you're doing constantly as you're walking through the world is you take in video frames and you constantly estimate the state of the world in an incredible amount of detail with all of these material properties and weights. And intuitive physics is crazy. So that's what I do research on. And that's what fascinates me about computer vision. I want to build computer vision models that learn to infer these kind of general 3D representations and dynamic 3D representations from. 3D representations from images and from video. And of course, that would be useful for all kinds of applications. And some of them I'm also investigating. Cool. So to me, then the big challenge really is self-supervised training and self-supervised learning of these kinds of representations from a natural video. And I actually really like this EpiKitchens data set. It's actually the only video that I could find which has a mosaic of videos like this. So the EpiKitchens data set is a good example. This kind of very mundane footage is incredibly hard. Which is incredibly hard for us today in multi-view geometry, which is funny to me. But this really, I think, is the gold standard for building self-supervised models that learn 3D representations, basically being able to predict what will happen next in each of these videos. Okay, and of course, as many of you know, my core approach to this has been this like a line of like generalizable 3D reconstruction. So basically, Reconstruction. So basically, the approach that I have been taking for a long time is to take a set of images as input, build some kind of neural network encoder, infer some kind of 3D representation, build a differentiable renderer, re-render the frames that you had as input, or re-render different frames, train the whole thing end-to-end. And since then, there has been a lot of fantastic work, right? Like PixarNerf, generalized patch-based neural rendering, IBRNet. So there's lots of work on like prior-based 3D reconstruction and rendering. There's all these light field papers, which I'm particularly Field papers, which I'm particularly excited about, right? Some of Andrea's work, Scene Representation Transformer 0123, great stuff. And we also had some work at CBPR this year where we really tried to push how far can you get with these like epipolar line based renderers. And we were able to achieve quite good results. But really, the question is: is this the thing that we need to be able to scale this up to self-supervised learning on the kinds of data that you just saw? Like, what is You just saw? Like, what is missing, right? So, like, what do we need to be able to scale these kind of self-supervised learners up to just Epic Kitchens? Why can't we train a model like that on the Epic Kitchens data set? And there's a few problems. And one problem I want to talk about now. The first problem is the fact that we are all using camera poses. All of the generalizable renders and generalizable neural scene representations that you just saw on this slide, they all require camera poses at training time. And camera poses using camera poses. And camera poses using camera poses is a huge disaster. And why is that? If you think about how we get these camera poses today, it's usually done with structured promotion, which requires like five GPU years for just bundle adjusting Code 3D, which is insane. It's like an insane amount of time. It often breaks for dynamic scenes. So like bundle adjusting something like Epic Kitchens is very hard, although it can be done. But even if you do it, it doesn't capture the dynamic parts of the scene. So clearly, this is also a problem. And it often Also, a problem. And it often breaks even for other reasons. It just sometimes just doesn't work. And it takes forever. It's like an offline optimization process, optimization process. And just as an overview, right, there's like these structure for motion and SLAM-based approaches, Colmap, ORPSLAM, Void SLAM. There's recently these joint nerve and pulse optimization approaches like iNerf, BARF, Mellon, and so on. And then there's actually very excitingly, one of the papers that got me into computer vision in the first place, one of Noah's papers, in fact, is concerned with joint estimation of ego. Concerned with joint estimation of ego motion and depth. And this is done in a feed-forward manner. So, this is kind of the landscape of like camera pose estimation. And what are the problems with these existing approaches? Well, like structure for motion in SLAM is always per-scene optimization. It doesn't use any 3D learned priors. And to me, that's crazy, because if you think about how progress in the past 10 years in computer vision has been accomplished, it's like unarguably deep learning. And multi-view geometry is the one branch. Geometry is the one branch of computer vision where we are basically still not using any learned priors. In like structure for motion, of course, you can maybe learn some correspondences and stuff. But basically, the pipeline we've been using looks exactly the same, like 30 years ago. And I'm a huge fan of structure for motion. It's great, but it really holds us back in terms of achieving our goal of self-supervised training on large data sets. And I think Noah remarked the same in his talk as well. And then the thing that I like about this, like, That I like about this joint depth and camera pose estimation is that it's really online. So it works online. It learns priors over both poses and over depth. And it's really self-supervised. The problem is it doesn't actually work. So it works for like self-driving scenes, but it doesn't actually work for like unconstrained camera trajectories. And here's where our most recent work comes in. It's called FlowCam, Generalizable New Scene Representations Without Camera Poses. And I'm very excited about this. It's just the first step in what I hope will be a very long line. In what I hope will be a very long line of research. But what you're seeing here is a video from the Co3D hydrants data set. And on this video, what you can see on the right is poses that are estimated in a feed-forward manner by a neural network without any scene-specific optimization. And further, what you can see here is at the same time as inferring poses, our method also learns to infer pixel nerf representations. So basically, piecewise pixel nerf representations along the trajectory. So these are re-rendering. So, these are re-renderings. And note, these are also feed-forward. So, this is not, of course, this is not as good as single-scene optimization, but that's really not the goal here. The goal is that this is all done feed-forward, and the inference is done in real time. And let me briefly capture what I think is the interesting research direction, and like what is the key difference in getting this to work. The key difference, I think, in how I would think about post-estimation today. And how I would think about pulse estimation today is that what I think we should be doing is we should view the, we should basically take the perspective of a camera-centric coordinate system and say that pulse estimation, the first step is to reconstruct the motion of 3D points relative to the camera. I think that's step number one. Let me show you how that might work and why that is beneficial. So let's say you have like a video stream like here on the top left corner. Now what you do, we build a differentiable pole. We built a differentiable pose estimation pipeline where we always take two consecutive frames as input. We take these two consecutive frames, we compute optical flow. This is off the shelf, pre-trained. And optical flow basically gives you 2D correspondences, right? So then we use this optical flow. Sorry, then we use single image pixel nerve to reconstruct two single image nerves. So basically, what you see here is one nerve for the top frame, one nerve for the second frame. And what does that give you? It gives you a pixel line 3D representation, right? Pixel line 3D representation, right? So now for each of the two frames, you have one pixel nerve and you have 2D correspondences between the two frames. And what that actually gives you is a scene flow field. It tells you for every pixel, for every 3D point, in fact, that belongs to a pixel, how has that 3D point moved from frame one to frame two? And so you can reconstruct the scene flow field, which basically tells you how every surface point that you have observed has moved relative to the camera. And now, of course, the pose is, you can very easily back out the pose from that scene flow field. Very easily back out the pose from that scene flow field, for instance, via least squares fit of an SC3 pose. And so that's exactly what we do. And after getting the poses for each pair of consecutive frames, you can pick a subset of the frames and use the subset to reconstruct another generalizable nerve and supervise it by just re-rendering all the video frames. And you can get another source of supervision, which is for the pose that you have inferred, that pose basically gives you a pose-induced flow field. So given a depth map, for instance, and if I tell you how Depth map, for instance, and if I tell you how the camera moves, you know how each of the 3D points have moved, and that induces an optical flow in image space, and you can supervise that optical flow as well. So, why is this interesting? Or like, how is this different from the way that people have done prior-based pose estimation in the past? In the past, people have usually done a pose estimation that is based on pose estimation that is based on black box inference. So, you basically take two frames, you concatenate them, and then you just predict the camera pulse. Just predict the camera pulse. And that doesn't work because the camera pulse is a global property. Like you have to look at all of the pixels at the same time. So you lose all the benefits of like shift equivariance and so on. While estimating the motion of each pixel from frame one to frame two, that is a local property. And you can exploit the shift equivariance of that local operator by estimating the scene flow field. So going to the scene flow field first and then estimating the pulse from the scene flow field is a much, much, much simpler inference problem. Much, much, much simpler inference problem than trying to predict the poles directly from two consecutive frames. And this is what we're exploiting here. And we train this on Code3D. On Code3D, it works quite well. And I can tell you in a moment what I mean by that. But here you can see two trajectories. Note that these are really hard trajectories for classic online SLAM because they have lots of rotation. So that's very difficult. And here's another interesting thing. So you can see a sequence from the tanks and temples data set here. You can see the sequence. Here. You can see the sequence on the top left. On the bottom left, is the poses that you get by BARF by running BARF on this. In the center, you can see our model doing feed-forward pose inference. And the model is only trained on real estate 10K. So we trained a model on real estate 10K, fully self-supervised, never has seen a single camera pose, no bundle adjustment necessary. And even that model, zero shot, generalizes pretty well to predicting the online poses on this very challenging video. And then with just fine-tuning a little bit, by which we With just fine-tuning a little bit, by which we mean fine-tuning the weights of the model, so not fine-tuning the camera poses, but fine-tuning the weights of the model, gives you poses that are almost perfect. And so here you can see the reconstructed pixel nerf from time to time. Note that they're not time consistent because basically you get one pixel nerve per, like it's like applied in a sliding window manner. So you always get a set of poses for like 15 frames at a time. Okay. And this works. And this works much better than baselines. But what is most interesting to me is that this on a challenging Coal3D data set, on that data set, it actually works better than state-of-the-art SLAM algorithms. So we benchmark with ORP SLAM V3 and with Deutschlam. And ORP SLAM V3 and DeutSlam perform worse in terms of camera pose estimation than our method. And of course, you know, this is just a first shot that we took. And the camera poses we get for general scenes are still not amazing. Still not amazing. It's much worse than what you get by running ComAP generally. But I think this first stab that we just took at this already outperforming SLAM is quite significant. So I'm very excited about that. So what's the limitation? So right now we don't predict intrinsics. We have a simple way of predicting intrinsics, but it's not very good. There's no loop closures. There's no refinement of the camera pulse happening. This is odometry. So it's frame-to-frame pulse prediction. It's not photorealistic, obviously. So, and oftentimes the geometry that we get with our And oftentimes, the geometry that we get with our method is faulty for complex scenes because PixelNerf tends to jeat for these scenes. And the training is expensive. It's very, very expensive. But I think this is a direction that I'm excited about. And I really think if I took 10 steps back, what's the big vision to me? The big vision is just let's build generalizable models that learn to infer intrinsics and extrinsics. I think everything else is not a great idea. I think offline camera poses. I think offline camera pose estimation is not the way to go. Cool. So, this is the paper. And the thing that is amazing, this was done by Cameron Smith, who's a visiting researcher in my group, in collaboration with Gilen and Ayush at MIT. Ayush, of course, also being in the audience at this workshop. And Cameron has knocked it out of the park. This guy is amazing. So shout out to Cameron. Okay, so now let's get to the second part. I have eight more minutes. The five more minutes. Oh, yeah, okay. Five more minutes. Oh, yeah. Okay. Then we're going to go fast. So, camera poses gone amazing. So, can we now use this for learning downstream representations? The answer is kind of. So, if you train the self-supervised, what do you get? What are the features that are learned by this model? Short answer, the features that are learned by this model are going to look something like the features learned by a monocular depth estimator plus plus. Why is that? If we want to learn good representations, by which I mean representations that are correlated with semantic classes as humans interpret them. Semantic classes as humans interpret them. Like, for instance, the thing that you get from Dyno, the thing that you really need to train for is completion. And we know this from Dyno, we know this from all the work on generative models that we've seen recently. And so that's the path we have to take. So here's this model that I'm really excited about. I used to led this effort. So as you know, if you take a single input image, you stick it into a generalizable scene representation, you do novel view synthesis, this is what you get. You're going to get a blurry mess because it's not a probabilistic conditional generative model. Conditional generative model. We just built a method which is called diffusion with four models that gives you the thing on the right. So, this thing that you see on the right is a nerve that is directly sampled from a distribution of nerves in a single diffusion denoising diffusion pass, reconstructed from the single input image. So there is no score distillation here. This model has never been trained on nerves either. So, we've never reconstructed a single nerve to train this model. This is really an end-to-end train diffusion model that learns. end-to-end trained diffusion model that learns to sample 3D nerves trained only on images and just from a single image. And so here you can see the depth map that is inferred for this nerve. Let me tell you, yeah, so Ayush led this project. Ayush will be on the job market. Sorry, this year, not next year. This year, he'll be on the job market. And yeah, I think everyone should hire Ayush, basically. Short story. Okay, so what's the problem with diffusion model? What's the challenge? The problem with diffusion models is generally you need access to the distribution. Is generally, you need access to the distribution that you want to sample from. So, for instance, if you want to train a generative model on images, well, you need access to images. Unfortunately, we don't have that. We have 3D scenes, we don't have access to 3D scenes. So, the situation we're in is the thing that you can see here. So basically, we have some kind of signal in our case that's 3D scenes. We have a forward model, which is rendering in our case, and that forward model generates images from the 3D scenes. And the problem is that these images are impoverished. They don't observe every aspect of the 3D scene. But what we really really seen. But what we really want is we want to train a model only from observations that learns to sample signals. And this is a very general problem. It's called stochastic inverse problems sometimes. And it turns out this is a very general framework and we also show other applications in the paper. But let me briefly tell you how the 3D application works before I run out of time. So it will only suffice to give you the high-level idea. Please refer to the paper for details. But the high-level idea is basically this. We're going to make the diffusion operator The diffusion operator, sorry, the forward operator, the renderer, we're going to make the renderer part of the denoising operator. Okay, so let me walk you through this diagram. Imagine I give you a single image. What you're going to do is from that single image, you're going to pick a target pose. So you're going to pick a target pose. At training time, that target pose has to be observed. So I give you a context image, you pick a target image. And now what you're going to do is you're going to denoise that target image conditional on the context image. But the way you're going to denoise it. You're going to denoise it by adding noise to it, then predicting the 3D nerve conditional on the context and the target image jointly, like in a multi-view pixel nerve setting, and then rendering out the target image and calling that the denoised estimate of your target image. So, again, I give you a context image, you pick a target pose, you add noise, you reconstruct the nerf from the noisy target pose and the context image. Post and the context image, and then you render it from the target post, and that's your denoised target image. And you can supervise that end-to-end and train it end-to-end. So that's the higher-level idea. The problem here is that you need a way of conditioning the diffusion model for the target post. And what you're going to do here is you need to build a 3D structured conditioning mechanism, because what doesn't suffice is just concatenating the context image to the target view, because, of course, it's like not local, so that doesn't work. So, what we're doing basically. What we're doing basically is we take the context image, we render out a deterministic estimate of the target post first, just with single image pixel nerf, basically. And then we use that as conditioning input to our model. There's like more steps here, but like I'm going to skip them. There's some nuance, but basically that's it. Let me show you some results. And then we are basically done. So this is a single image reconstruction on Code 3D. So from a single image, you reconstruct the SNURF. That's cool. This is a single image. That's cool. This is a single reconstruction on more hydrants, but maybe more excitingly, it works for actual 3D scenes. So, this is trained on real estate NK. All of these four scenes are samples from the diffusion model, and here you can see the diversity. So, for parts of the 3D scene that weren't observed in the context image on the left, you can really see that you get a lot of diversity in the 3D scene for these things. And note, each of these samples here is actually a nerf. So, each of these is not an image. Each of these is a nerf. You can render it from arbitrary novel views. Very novel views. Okay, to wrap up, because I think Andrea is looking at me like I have to wrap up. Again, so Ayush led this project, so this is fantastic. Hire him. I think the big vision is vision foundation models. I don't think scale is the answer. Everyone's like scaling it, and I'm excited because light fields work at scale, but I don't think that's the answer at all. I think that even with LMM, scale is not the answer. It kind of works, but it isn't the right way of doing it. I don't claim that 3D structure. Way of doing it. I don't claim that 3D structure is the right way to do it either. But I do think we'll learn much more about the structure of the problem by trying to solve the structured problem. And I think what will in the end win is models that actually have a data efficient way to discover patterns and data and discover symmetry. And I don't think we have that neural network architecture yet. Transformers isn't it and confidence isn't it either. And I think we'll have to go back to the drawing board and build some more fundamental deep learning architectures. That's it. And this is my research group. And this is my research group. They're amazing. And visit us at MIT. I would be very happy to host you. Thank you. Awesome talk, Vincent. I like how you actually answered the question that I had for Noah like three days ago, basically in the talk. I completely missed that paper. Do you have questions in the audience? Because I'm not interested in somebody how this is. Thank you so much for this talk. You're gonna have to speak up. Oh, sorry. Thank you so much for this talk. This was like a super useful overdue to the paper. I had a quick question about the diffusion forum models paper. I just want to know a little more how like the inference process works. How exactly does ancestral family have in here? Because my account is like when you start on the birth iteration, kind of penalized the sort of the target and the context in it, they sort of pass through the pixel or Pass through the pistol. Do you sample a distinct RV for each generation of that sample, or is it just the original parting view that just kind of think voice out important? So yeah, I have a really hard time understanding you, but let me try to repeat the question that I made up. I think your question is: if you only sample one target view, how do you denoise a whole scene if you only sample one target view? Andrea, can you not if that's the question, basically? I cannot answer the question. Okay, sorry, is that one aspect at least of the question you asked? I tried to understand it, but I can't understand the question very well. Maybe you can go to Andreas' microphone and ask it again. I'm very sorry. I didn't hear it. Yeah, basically, that's it. I mean, the question is, are there multiple closes or not during inference? Like, are we yeah, great question. So, yes, so for the code 3D scenes, so it is indeed the case. Code 3D scenes. So it is indeed the case. When you pick the target pose, you are denoising, you're reconstructing only the part of the scene that is observed by jointly the context pose and the target pose. So that's the part of the scene that you're reconstructing. Note that every other part of the scene that doesn't have any uncertainty conditioned on those two views will also be sharp. But any part of the scene that has uncertainty conditioned on those two views will continue to be blurry. And so what we do for the code. And so, what we do for the code-free DNC scenes actually is we do it auto-aggressively. So, I give you the context view, and then you denoise one view, and then you pick the next one, and then you denoise the third one, conditional on the first two, and so on and so forth. And then that way you can sample the whole scene. There are other ways of doing it as well, but that's what we do, and that works. Yep, that is awesome.