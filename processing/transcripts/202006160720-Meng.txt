This term contains many computational methods and special cases, for example, canonical correlation analysis and the multiple co-inertia analysis. So I won't focus on the details, but more focus on the concept and how we can use it in different cases, or like at least as a computational prototype can be used for biological questions. So let's start. Questions. So let's start with a very simple example here when we have overlapping samples. And we have, for example, this colorful box here, a present metrics where the rows are the features we measured could be genes, proteins, and columns are the samples here. And exploratory analysis, often when we have one data, we use, for example, PCA, and we want to see which samples are similar to each other, which features are similar. Similar to each other, which features are similar to each other by looking at the sample space or feature space. And when you have multiple data sets, the question is also about how samples or features from different data sets are similar to each other. You can use methods like multiple factorial analysis, correspondence analysis, or multi-block PCA method. And all these methods can have a similar view and give you this information. And when you have And when you have like phenotype and you want to make predictions using the features you measured in the multiple omics data, and this method could be, I think, viewed as a generalization of the partial least square. In partial least square, you have two predictive metrics, you have one or dependent or independent matrices, and use one predicted another one. But here you have multiple independent matrices, and so you can use. And so you can use, for example, the correspondence analysis, some people call it, or some people call it multi-block PLS. I think I'm not sure if MixOMIX implemented this method, but it's a good chance to ask if you're interested. I mean, ask not me, but ask the organizer, Kim, and Likao. And so when I look into this SC single cell. This SC single-cell targeted proteomics. And one of the questions inspired me is: how should we approach integrating partial overlapping protein data collected on different patients with similar phenotype? So the question I define is showing on the left side the figure here. We have, in this case, example data, we have four omics data, there are four micro-arays. Data, there are four microarray platforms measured the genes of 60 cell lines. So, in this example analysis, I only use 200 genes. But I think the number here is not critical, you will also see later. And when we have complete measurement for three platforms and only one platform, we only partially measured the cell lines. So, could we predict the expression of this? Predict the expression of these cell lines in that platform, which haven't measured this expression of these cell lines. So I use the least square regression as a baseline, training the linear model using the other three platform, Agilent, HGU 133, and 133 plus 2, and training three models, and apply the model in HGU95 and make the prediction. Make the prediction. So, the final prediction is the average of the three platforms, the result of three platforms. And in this case, I found PCA is approached like you do in PCA. You can project and reconstruct the missing samples. And in this case, I found here, you can see there are eight bar plots. Every block here represents one. Block here represents one data and one method here. So the same data always plays together. So you can see the MBPCA method is always better than the ordinary least square or least square regression method. And so actually this result is encouraging. So I tried something from the data we uploaded in the workshop for the workshop. It's a mass tag data. And in this data, we have 39%. And in this data, we have 39 proteins measured many patients, and the model six are the triple negative patients. And the question now I ask is, I select those six triple negative patients, and every patient have like a thousand to three thousand single cells measured. And the question I ask is: if we use, for example, 25 of proteins, could we predict the expression of other types? Expression of other 10, other 14 proteins in the patients. So still compare MPPC and least square. But here in this case, the result is shown on the bottom right panel here. Ordinary square is almost the same with MBPCA. Although here we have like a thousand of cells in the least square regression, we have much more sample size that means. More sample size, that means, but actually doesn't really improve our outcome, also. And PPCA doesn't better than the ordinary least square. So I think it's not really matter how many samples we, what's the sample size, what's the gene number, what the sample size, or how many samples you use in the training or testing set. So what's the critical factor I think is the R V coefficient. So R V coefficient is So, Re coefficient is a generalization of the squared spearman correlation to a multivariate comparison. In simple words, or intuitively, we can understand it in that way. We have two matrices. If the pairs of variables in one matrix have a high correlation and they are also highly correlated in the other matrix, then the R V coefficient should be high. And we can see in the mass tag data, the R V coefficient is around. Data, the RV coefficient is around 0.6, so 0.8, but in the RV coefficient, 1460 data is around 0.9. So this is very high. I think the RV coefficient should be a single statistic to indicate how well our MBPCA method can work, at least compared to the ordinary least square method. So I think the most important thing is how we can improve. Is which how we can improve the RB coefficient of different matrices by normalization? Of course, we cannot massage the data too much. Of course, we need to make sure the data is still representing the biology. And we need to do the row-wise normalization, for example, centering and scaling, column-wise normalization, for example, the variant stabilization, normalization, stabilizing normalization, and also very important the data-wise normalization, for example, multiple factorial analysis. For example, multiple factorial analysis, weighting the matrix using the eigenvalues, the first eigenvalue of different metrics. Or status, giving the metrics have a high similarity to the compromise matrix, a high weight, and to the matrix have very different structure, a low weight. Or because here we are making prediction, I think it also makes sense to weight matrices according to their similarity to the matrix. According to their similarity to the metrics to be predicted, I never thought this, but I think it's something that makes sense to do and was to try if we want to use this method or with some of predictions. Okay, so now the last question is more general. And here we don't have overlapping cell lines, we don't have overlapping features. In this example, we can imagine we have two ohms. Imagine we have two omics data sets: one is a proteomics, and the other one is metabolomics. And the different individual cells, the various metabolizes and proteins over different individual cells, and nothing overlapping here. Also, you cannot map proteins to metabolize directly. And what we have all is a phenotype. We know that those individual cells have similar phenotypes. For example, the cell types we have. Yesterday, we see, I think, the Yesterday, we see the individual cells can be classified into eight different cell types. That could be the dummy, we can dummy code the cell type assignment to a matrix. Or in a fuzzy clustering, we can make the matrix as probability of one cell belong to one of the clusters. And so, what we have, if we want Have at the if we want to use a phenotype to link the two data sets, what we can do is to multiply the phenotype matrix. Of course, here you need to transpose one of them and to get the phenotype as a linking matrix to link the two matrices, proteomics and metabolomics. And so, here we have three metrics already. So, what we are interested is to know something about the link between metabolites and proteins. And we have three matrices, and this is the first. And we have three matrices, and this is the false corner. So, I think this method also has been known as the false corner problem or the LQ problem. But most of these methods are documented well in French literatures. And this method could be applied into a prediction problem, also could be used for the feature selection. For example, if we can impose them lasso penalty onto the protein and metabolite space, then we can. Metabolite space, then we can return maybe 10% or 5% of metabolites and proteins. So we can make a prediction and link, or for the feature selection, and then predict the phenotype. Some final remarks. Okay, time almost due. So I think metrics decomposition or MBPCA still have a great potential for integrating multi-omics data. And the normalization is very important. Important. And also, we cannot find a one-fit-all method for all the biological questions. We need to adapt or extend our current methods to fit the need of specific biological questions. And the last thing I didn't talk about, but I think it's really important, actually most important thing deciding whether people will use it or not is the visualization and integrating with prior knowledge. And the prior knowledge is about that. knowledge is not is about like for example gene ontology or pathways all this will give give users a quick a quick a quick sorry a quick way to explore different hypothesized and this I think how people will use it that's my talk and thanks thanks organizing the meeting and our audience and I'm happy to answer your question I'm happy to answer your questions. Thank you. Thank you for tackling a very difficult, the very difficult challenge of partially overlapping or non-overlapping data sets. Genevieve Steiner-Brown asked the question, what's the difference between the RV coefficient and the ICC? I'm not sure what is ICC. What do you mean by ICC? Integrative correlation coefficient. Correlation coefficient. The way you described it, they sounded like they were the same thing, where it's basically looking at the correlation of the correlations. I don't know how exactly it is calculated, sorry. Yeah, no, that's what I was wondering. There was differences in terms of... In my understanding, they're actually quite different. That's something we can explore maybe later. Themologist. I have maybe one quick question. So, Susan, oh, actually, Susan answers the question on the RV coefficient as a generalized inner product that does not need the data points to be in groups. I think different backgrounds. Amrit Singh asks, which libraries implement MBPCA for high-dimensional data? I believe ADE4 is for N greater than P? Yeah, I think not only, but the problem is. not only but the problem is um yeah so they are so 84 is for the for the for the ecological study I think it doesn't show the data in a way we can really use for the multi-omix data I have a package M-O-G-S-A and also I think mix omics have something like this for CCA I th I'm sure it has I think I'm sure it has. And also, there are some packages for the generalization of partially square because it's also very broad term. But I mean, it might be implemented for some other field already. You can look up, for example, the term multi-plug, partial liter square. I think many packages have implemented it. But for biological data, you can look at MOGSA and mix or mix. And mix all mix. Yeah. I think RGCCA from Tenant House might have it. Yeah, yeah. Okay. Yeah, RGCCA. There's a question from Mike Love on the last remark: integration of prior knowledge going pathways. Are you imagining both after model fitting or also informing model fitting? So integration the going. I think it means integration the going. Yeah, I think both. I think both. It can be done both. But as long as it can be integrated after we get the result, it's helpful already. Sometimes I implemented a few methods, but sometimes I even don't want to use it myself because I know how very long time I will need to deliver. You need to deliver the result to the people in a way that other people can understand. I think it's very hard. But I think integration, the knowledge, I think it can be done on both ways, during the calculation and after the calculation. I think that's a great thing that we can discuss during brainstorming as well. Let's move on to the next talk, Pradeepa. 