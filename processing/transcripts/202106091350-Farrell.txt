Millions or billions of degrees of freedom for very, very finely resolved simulations. Okay, so first the Newtonian case. So everyone knows this problem. This problem is trivial for all of you, but I just want to show some interesting insights into these equations or some interesting structure of these equations. So first, let's consider the stationary incompressible Navier-Stokes equations. So for a given viscosity, that's a fixed real number, I'm finding a velocity. Fixed real number, I'm finding a velocity and a pressure that satisfy these equations. So I hope you can see my mouse. You've got the viscous term, the advection term, the pressure gradient, and the incompressibility constraint. And the point of the work that led up to this project on non-Newtonian fluids was a new preconditioner, a new multi-grid method with the property that its convergence is robust with respect to the viscosity. So most other, basically all other previous methods for this problem break down in the This problem breaks down in the high Reynolds number limit. So the error estimates blow up, and the number of iterations required to converge in your Krallov method in GMRes also blows up. But by exploiting the structure of these equations, you can achieve performance that's robust with respect to viscosity. And so this combines lots of different ideas, and I'll just touch on a few of them in this talk. The main ones being an augmented Lagrangian term for controlling the sure complement. I'll explain what that is. Complement: I'll explain what that is, and a kernel capturing relaxation. So, it turns out that in order to solve these equations of incompressible flow, the central character on the stage is the kernel of the divergence operator. And if you can understand that, you can solve the equations. Okay, so how do I discretize these? So, I'm a finite element guy, so I'm going to use Scott Begulius elements. So, that's continuous polynomials for the velocity of degree k and discontinuous polynomials for degree. And discontinuous polynomials, so degree k minus one for the pressure. And that's a really, really nice element because the divergence of the velocity space is mapping onto the pressure space. But the downside is that they're not stable on general grids. So in order to make sure that they're stable, we introduce a special mesh structure. We take in whatever mesh of whatever domain that you want, and then we barycentrically refine it. And that makes sure that it's stable, but it causes some headaches in other areas. And why is this a good idea? And why is this a good idea? Well, it's because this discretization preserves a fundamental structure of the underlying incompressible equations. So, the fundamental structure of these equations is that the divergence of the Sobolev space H1 maps onto the Sobolev space L2. And with this choice of discretization, we preserve that surjectivity, that structure in the equations. We build a so-called sub-complex of this Stokes complex. And the consequence of that is really, really nice. Consequence of that is really, really nice that in the discretization, we really enforce the divergence of our approximate velocity u is equal to zero exactly, or at least up to solver tolerances pointwise. It's not just enforced in some kind of weak sense, in some kind of integral sense. It's really enforced exactly. We really have something in the kernel of the divergence. And a consequence of that is that that gives us viscosity robust, Reynolds number robust error estimates. So that's really nice. Now, if I consider the stationary equations, or if I'm considering Stationary equations, or if I'm considering a fully implicit time discretization, so rather than advancing velocity first and then computing a pressure and then advancing velocity again, if I try to treat the problem monolithically, which you want to do for lots of different reasons, maybe you're doing bifurcation analysis, or maybe you just want to consider a fully implicit time discretization so that you have complete flexibility in the choice of time step. Then, or maximal flexibility in the choice of time step. So, then you have a non-linear problem to solve. And so let's Problem to solve, and so let's apply a Newton iteration to that. And then, when you discretize, at the heart of it, at the heart of the code, is the solution of a linear system that I write like this. So, a saddle point problem for the velocity and pressure, where A is the viscous terms and the avection terms, B transpose is the pressure gradient, and B is the divergence operator. Now, this linear algebraic structure is ubiquitous across numerical PDEs, and there's been an awful, awful lot of research on how to solve these. And the main way How to solve these, and the main way, or one of the main ways to solve these is to take a so-called block factorization approach. So, you try to build some kind of splitting into the scheme to solve a problem on the velocity space and on the pressure space separately. And what these block factorization methods do is they break down the solution of this big coupled block matrix into the solution of two smaller matrices, one on the velocity space and one on the pressure space. And the one on the velocity space is just the momentum up. velocity space is just the momentum operator that we all know and love but the sure that but the operator on the pressure space is this so-called sure complement and that's really the the crux of how do you solve these block structured problems can you get a good handle a good functional analytic understanding of what this sure complement is so that's the game that you play for each different pd that you consider that you want to find fast solvers for that top left block and for that sure complement and normally it's the sure complement Complement and normally it's the sure complement that's the really hard one. Okay, so what's the key difficulty or was the key difficulty with solving the stationary Navier-Stokes equations? Well, it's that at large Reynolds number, no good approximation of the short complement was known. So as you would increase the Reynolds number, your handle on the Short complement would get slippier and slippier, and eventually you would completely lose control over it. You're not able to solve the system. And so the number of iterations of solver iterations that you have to do to solve this. That you have to do to solve this block system blows up. How are we going to fix this? So, the basic idea, or the first point in this sort of wrestling match to pin down, is to get control of that short complement, is to get control of that operator on the pressure space. And it turns out that you can do that by adding a term to the momentum equation that doesn't change the solution, but does control the Sugar complement. But does control the SHUR complement. So, this is called the augmented Lagrangian term, the augmented Lagrangian method. And you can think of this as somehow combining a penalty method for enforcing the divergence constraint with the Lagrange multiplier. And by adding this penalty term, you're reducing the weight of the work that the Lagrange multiplier has to do, if you want to think about it in this very loose way. But adding this term, this minus the gradient of the divergence of u times some penalty parameter gamma, and gamma might be really large. And gamma might be really large, that controls the sure complement. So that's fantastic. But why have we really made anything better? Because we've shifted the difficulty from the sure complement into the momentum operator. And the easiest way to see why that problem is really hard, first of all, just for simplicity, let's throw away the invection terms because they don't matter for this discussion. So we just now want to solve this momentum operator. So we've got the viscous term and we've got this augmented region. Viscous term, and we've got this augmented Lagrangian term. So we've got the divergence of the symmetric gradient of the velocity times the viscosity plus or minus this gradient of the divergence of u. And now my viscosity is very small, this nu is very small, and this gamma is really large. But if you think about this operator, it has this really split personality. So on the kernel of the divergence, and the kernel of the divergence is really big, then this second term disappears. Term disappears. So all that remains is the viscous term. And that's nice because that's symmetric and coercive. We like that. But on the kernel of the divergence, on the rather on the orthogonal complement of that, the augmented term dominates. This one is the important one. So on one part of the velocity space, it's vanished. And on another part of the velocity space, on the rest of the velocity space, it's huge. So in order to solve these equations quickly, you really need to have a specialized solver that reflects. Specialized solver that reflects this structure, that knows something about the kernel of the divergence to deal with this personality. Okay, so this is the one most technical bit of the talk, but I'll try to explain it in a clear way. So let's just consider a generic abstract variational problem. So I've got real-valued coefficients alpha and beta that are greater than zero. I want to find a solution u. I want to find a solution u in some function space, some discrete function space, that satisfies this variational statement that alpha times a of u, b plus beta times b of u, b is equal to some given data for all test functions. So to be concrete, in our context, we've got a is the term arising from the viscous term. So it mixes the symmetric gradient of u with the symmetric gradient of v. And then this b is the term from the augmented Lagrangian term. So that's the Augmented Lagrangian. So that's the grad div u, and when you take that to weak form, it becomes div u div. So A is symmetric and coercive, so that's great. We really like that, but B is only positive definite. It has a huge kernel. Okay, and the way that we think about solvers in this functional analytic viewpoint is that you think about breaking up the space in which you're looking for the solution into a bunch of smaller vector spaces. So this is a vector space. Vector spaces. So this is a vector space sum. And so I consider, so I want to solve the problem in this big V, but that's too big for me to solve. So I'm going to write V as the sum, maybe not a direct sum, usually not a direct sum, of a bunch of smaller spaces in which I can solve the problem. I'll solve for a correction in each one of these subspaces in turn, and then I'll stitch the updates together somehow in a way that doesn't matter so much for our purposes. So just to be concrete, if you know the So, just to be concrete, if you know the Jacobi method for solving linear systems, what does Jacobi do? Well, when you made it in a linear algebra class, what you were told was to you might approximate the operator by the diagonal, or you might think of it as looping the matrix row by row. And when you get to each row, you say, okay, I want to solve for the entry on the diagonal, assuming that all of the other guys are correct. And then I'm going to figure out what's the correction to the entry here for this. The entry here for this degree of freedom. And it turns out that that linear algebraic viewpoint can also be understood from a functional analysis point of view, that it's exactly this subspace correction method with a particular choice of this space decomposition. So if v is the span of n basis functions, you can say, okay, I'm going to take n subspaces on the right-hand side, one for each n, and each small space vi is the span of a single basis function. Is the span of a single basis function. So that's why in the Jacobi method, what you have to do is you have to solve a one-by-one equation for each row. Okay, so now there's a really, really beautiful and fundamental theorem that I want to mention. So this is really the heart of the matter. So if you consider the kernel of this awkward term of this B guy, so these are all the u's in my function space. u's in my function space such that b of u comma v is equal to zero for all possible test functions that you give me okay so this is in our case is the kernel of the divergence all solenoidal functions and that's what we're looking for the solution of our equation in remember and so there's this beautiful property that if the space decomposition that you chose captures the kernel in the following way then in a stable way then the convergence will be robust with respect to the parameters so With respect to the parameters. So, if you can achieve this property, and I'll explain what this property is in a second, then your convergence will be robust with respect to the viscosity and with respect to the augmented Lagrangian term, and it turns out with respect to all the parameters that you'll have in a non-Newtonian problem in a moment. So what does this magical property say? So, what does a sum of vector spaces mean? What this means, if I have a space decomposition that satisfies this, we can think of it as a game. Can think of it as a game. You will challenge me with a function that's incompressible, that's divergence-free. So you've taken an element of n. And what I have to do is I have to be able to write that as the sum of functions in each of the small spaces that I've chosen, but there's a constraint. In each of the small spaces, I can only take divergence-free functions. So if you challenge me with a divergence-free function and I can't With a divergence-free function, and I can't write it in this way, then you win, I lose, and my convergence isn't robust. But if no matter what function you give me, I could always write it, I could always decompose it in this way, then I win, and the convergence is robust. So that's the way I think about it. Okay, so where do you come up with this space decomposition? So I won't go into the technical details. I only want to give a flavor of sort of how the analysis proceeds. It really hinges upon this. Really hinges upon this Stokes complex, really hinges upon this set of Hilbert spaces that underpins the structure of incompressible flows. And if you study this Hilbert complex for long enough, then what you realize is that you need a certain space decomposition, which we call the macro star space decomposition. So what you do is you loop over all of the vertices in the mesh before you did the barycentric refinement, and then you take all of the degrees of freedom in the Of the degrees of freedom in the cells of that mesh before barosynchronifyment, the macro mesh, but not including the ones on the boundary. And so, this is the patch that you take for one vertex and you do the same for all the vertices in the mesh. So, instead of having a one by one problem to solve for every degree of freedom, you've got a little n by n problem to solve for every vertex. You solve the equations patchwise in those problems, in those patches, independently in parallel. Independently in parallel, and then you combine the updates. Okay, and so if you do that, maybe I probably should have cut this slide, but if you do this, then this is sort of how the solver goes. You're doing continuation in the viscosity. You're solving a nonlinear problem at every continuation step. So you're using a Newton method for that. Then, when you do the Newton linearization, you want to use a Kralov method because we want this to scale up to supercomputers. So we're using, in this case, flexible. Computers. So we're using in this case, flexible GMRES. We have the block preconditioner that I introduced before. And then with this augmented Lagrangian term, we know exactly what the Sugar complement is. And it's a really, really nice matrix. So we can solve that really quickly. And then we've shunted all of the work into this momentum solve, into this momentum problem. And really the heart of it is this relaxation with this kernel capturing space decomposition that I described. That I described. Okay, and so you can run this on a big computer. You can solve big problems in 3D. We've run this with other discretizations up to a billion degrees of freedom for quite challenging problems. So you can really go to scale if you have the computer to do it. And so I'll present a few of these graphs. So just the first time I do, I will explain what this means. So again, here we're just considering the Newtonian case for the moment, the non-Newtonian case. For the moment, the non-Newtonian case comes next. But so, here, what am I presenting? So, as I go, so for a fixed problem, for a fixed Reynolds number, as I go down a column, I'm refining the mesh. And so, what I want to see is that the number of crawl of iterations that I have to take to solve every Newton step doesn't increase as I refine the mesh. If I have that, I say it's mesh independent. So that means that I can solve really big problems. That means that I can solve really big problems if it's constant as I go down the columns. As I go along the rows, I'm making the problem harder. I'm increasing the Reynolds number. And so if I have robustness as I go along the row, then that's parameter robustness. That's what allows me to get to very difficult values of parameters. And for stationary problems, achieving a Reynolds number of 5000 is really hard. This wasn't possible before this work. Of course, achieving a Reynolds number of 5,000 for a transition. Achieving a Reynolds number of 5000 for a transient problem is straightforward, but for a stationary problem is very, very difficult. And so, as we see, indeed, as we go down the columns, we have mesh independence, and as we go along the rows, then the number of crawl of iterations per Newton step barely changes. So we do indeed have good parameter robustness, and we can use this to attack problems that we couldn't attack before. Okay, so now let me go to the non-Newtonian case where I'm going to first go. Newtonian case where I'm going to first going to consider the isothermal situation where I'm not also solving for temperature. So I've been influenced a great deal by this implicit constitutive relation community that comes out at, well, lots of places, but particularly from a school in Prague, which says the following. So in the Newtonian case, you have an explicit formula for what the stress is in terms of the symmetric gradient of the velocity. Symmetric gradient of the velocity, and then you substitute that into the viscous term in your momentum equation where you have the divergence of the stress tensor. Well, if we're going to consider an implicit constitutive relation, instead of saying that s equals something, I'm going to say that some implicit relationship between the stress and the symmetric gradient must be satisfied. So, some equation G of S and the symmetric gradient must equal zero. And so, it isn't the case. Zero. And so it isn't the case that I have the symmetric gradient is equal to some function of the stress, or that the stress is equal to some function of the symmetric gradient, but just that there's some general implicit constitutive relationship between them. And the advantage of taking this formulation is that it allows you to change the constitutive relationship that you use very, very easily. When you've got the code, all you have to do is literally change one line into state what this G is, and then the solver proceeds exactly as it did before. Proceeds exactly as it did before, but with very, very different physics. So, some examples of constitutive relations that fit into this framework, so the generalized Karo-Yasuda model, for example, for blood, or regularized Bingham fluids. You have to regularize in order to get it into this particular form. But once you've regularized a little, then you can indeed also consider Bingham fluids. Now, the disadvantage of the work so far is that. Disadvantage of the work so far is that this is really only for viscous constitutive relations. We don't yet consider viscoelastic, although we would love to. Okay, so now once you've got this general expression g of s in the symmetric gradient is equal to zero, what that means is that in general, you're not going to be able to solve that for the stress. So when we take a finite element formulation, a variational statement of these equations, when we write down the PDEs, we have to also solve for the stress. To also solve for the stress explicitly as a variable, but that's useful because often when we're considering these problems, we want to know what the stress is. So, here's now the system of PDEs that I'm considering. So now I'm just taking the divergence of the stress. I still have the eviction term, the same as before, still it's the pressure gradient. And now, since I've introduced this new variable s, I have to add an equation to pin that down. And that's exactly this g of s and the symmetric gradient of u. And so there's been a lot of. View. And so there's been a lot of PDE analysis on this system. And so under very weak conditions on this G, it's known that solutions exist. So this is a really, really general framework for viscous constituent regulations, for viscous fluids. Okay, so now that we're solving for the stress, we also have to think about how do we discretize that variable in our finite element formulation. And so here I'm going to use the same element for the pressure, but in matrix. The pressure, but in matrix form, so a d by d matrix of the same element for the pressure. And so there's a lot of good reasons for choosing that because remember that what we wanted for velocity pressure was that the divergence mapped from the velocity space to the pressure space. Well, now what we want for this is the symmetric gradient operator maps from the velocity space onto the stress space. And this choice also achieves that. So it's IMSUP stable for both the velocity pressure pair and also. Velocity-pressure pair, and also for the stress-velocity. And because this element achieves exact incompressibility, we can also enforce that the stress is exactly traceless. And that means we have fewer degrees of freedom to solve. Okay, so once you've considered this extension where you're also solving for the stress, now when you get down to the heart of it, when you've got the linear algebra that your code actually has to solve, you've got a three by three block matrix for the update. Block matrix for the update to the stress, the update to the velocity, and the update to the pressure. And now, how are we going to solve this? Well, so we have to block the variables somehow, and here's how we do it. So, the idea is to take it the short complement with respect to the pressure again in the same way that we did for the Newtonian case, but we're going to treat the stress velocity block monolithically. We're not going to try to split that up. And that turns out to be quite important. That turns out to be quite important. There's this quite tight coupling, of course, between the stress and the velocity that we want to honor in the solvent. So that's why we tweak the stress and velocity monolithically. And so it turns out that if you do the maths, that the macrostar space decomposition that I showed before captures the kernels of all the operators that you've got showing up here. So that's good. So we still expect good convergence. But there's a difficulty here that now we have to transfer in our multi-grid scheme in a multi-grid. In our multi-grid scheme, in a multi-grid scheme, you've got different levels of a mesh hierarchy, and you need to transfer information between them. In the Newtonian case, you're linear with respect to pressure, so you never need to transfer any pressure updates. So we're not doing multi-grid on the pressure block, so we don't need to transfer any pressure updates. But now we are doing multi-grid on the stress velocity block, so we need to somehow figure out how to transfer this discontinuous stress approximation between non-nested mesh. Between non-nested meshes. And I won't go into the details, but I'll just mention that it turns out that this hinges upon the solution of a problem in computational geometry, where you take two meshes and then you construct the so-called supermesh between them. So this is a mesh that's a common refinement. So if this is my coarse cell that's been barycentrically refined once, and then this is a cell in my fine mesh that's been uniformly refined once to give you these sort of four triangles. See these sort of four triangles with the tick black edges, and then barycentrically refined once for stability. So these elements are not nested within each other, right? These don't give nested mesh refinements. And so in order to do the transfer, I have to construct some intermediate mesh that is in fact nested within both. But that's just to give a flavor of what's involved in extending to the non-Newtonian case. And if you do that, then now you can solve much more general problems. Much more general problems than the Newtonian one. So, for example, the solver results that I'll present are from a lead-driven cavity with a Karogia pseudofluid. And again, this is the same kind of table as what we had before. As I'm going down the columns, I'm looking for a mesh independence, which is really a statement that I got my functional analysis right, that I'm not making any functional analytic crimes. And we are indeed achieving good mesh independence, perfect mesh independence. Mesh and dependence. And then, as we go along in the parameters, we're making the problem harder. And if I have, if these numbers aren't growing or aren't blowing up as I vary the parameters, then I've got a solver that will let me get to difficult parameter regions. And indeed, for this problem, we see superb robustness of the solver, even for really, really challenging problems. And so the number of degrees of freedom here for this, this was only run on the workstation, but this solver really will scale. Solver really will scale up to simulating things on supercomputers with billions of degrees of freedom. Okay, and just two minutes for the anisothermal case. So, what's new here? Well, first of all, we consider a constitutive relation that allows for yielding, both for yield stress and for yield strain. So, these variables here, the sigma hat and the tau hat, they now show up in our constitutive relation. And everything almost can depend on the Everything almost can depend on the temperature, theta. So the viscosity can depend on the temperature, the yield stress and the yield strain can depend on the temperature. And the thermal diffusivity can also depend on the temperature. So now we've got, here I decided to write it in non-dimensional form with the Plant and Rayleigh numbers. We introduce one extra equation for the temperature, including viscous dissipation and adiabatic heating, and then allow all And then allow all the parameters in the constitutive relation to vary with temperature. So, in principle, you could go from something like an activated Euler fluid through to a Bingham fluid as a function of temperature. So, this is a really, really general framework. And here I'll present some results for a simpler case where I've got some kind of temperature-dependent power law behavior and a viscosity that depends, negative exponentially, on the temperature. Density on the temperature. Okay, so here are some pictures of solutions at certain parameter regimes. So a Prandtl number of one, a Rayleigh number of 30,000, a dissipation number of two. And as you vary the power law behavior, you can see that the flow changes quite dramatically, of course, as you want it. And so here the results are still pretty good, I would say. They're surprisingly effective. So as you go across, okay. So, as you go across, okay, so now as you go from Rayleigh number one up to 30,000, the amount of work you're doing at every nonlinear solve is indeed increasing from about three to about 15, but this isn't blowing up dramatically. This is still keeping well within the bounds of computational feasibility. So that's really nice. Okay, and so let me conclude. So, what was the toolkit that allowed us to attack the fast iterative solution? The fast iterative solution of really, really big scale problems. So, the toolkit was the study of these Hilbert complexes, the function spaces with the structure that underpins the incompressible equations. Then we had block preconditioners to break up the solution of these couple problems into smaller pieces. Then we used augmented Lagrangians to control the Shure complement. And then we had subspace correction methods, but with space decomposition. But with space decompositions that exploited the structure of the kernel of the divergence. And we can use these to build preconditioners ultimately for anisothermal, implicitly constituted viscous flows. And with future work, what we'd like to study next is to broaden these to viscoelastic constitutive relations, which basically everybody else is talking about. So I realize after this conference just how important they are. Okay, that's what I wanted to say. Thanks very much for your time. Wanted to say, thanks very much for your time. Thank you, Patrick. You got the prize of the longest talk. We have, well, since this is the last one, we have some time for two questions. We have one from Ian and one from Mike Ren. So, Ian, you want to ask the question? Thanks, Patrick. I enjoyed that. Yeah, I've seen the implicit formulations, but I guess I haven't fully understood them, and maybe I still don't fully understand them. Fully understand them. But I see at least where you're coming from. So the question was just whether there's, is there a unique way of defining G, you know, for a given model? I mean, you're putting it in as a constraint. So presumably you... No, there's not a unique way of defining a specific model. And in fact, you can find different Gs that represent the same constitutive relations. So you have a modeling choice of what's the best G or what's the right G, and model a particular. To model a particular behavior. Which I guess is the question: yeah, what is the best G, you know, how do you not really one within my personal expertise. So I would say that probably the way that I would approach this question is to go back to the analysis and look at the properties of the G that affect the method that, you know, we. The method that, you know, what are the assumptions that you make about the G? And then you want to find the G that describes the physics that has sort of the best constants for those assumptions. That would be where I would start. But I think in practice, it doesn't really seem to matter too much. So I know that Alexei has done, so Alexei did all of the simulations, I should say. And I remember discussing with him that he had implemented several different formulations for the same constitutive relation. For the same constitutive relation, and I think that they basically worked. So I'm not sure the success of the method is usually sensitive to this. Mike, do you want to ask your question? Sure, I was just curious whether there was anything specially needed to do with the advective terms in this case. Maybe not because you're looking at steady states, but I was impressed with the Reynolds numbers that you could get convergence to steady states in. Yeah, so that's the main mystery of this work, I would say. So the theory for preconditioners and solvers in the symmetric and coercive worlds is really well developed. That as soon as you go to a non-symmetric problem, most of the theorems that you build upon just don't apply. And so, you know, effectively, what I did was I built a solver for this. What I did was I built a solver for the Stokes equations and then applied it to the Navigator-Stokes equations and observed that it worked. And that's not true for most solvers that you can build for the Stokes equations. So something magical is happening here that whatever it is that's important, or whatever it is about this macrostar space decomposition, is that it's also working for the eviction terms, even in regimes that are eviction-dominated. And I don't understand that, and that's sort of my current. Sort of my current big question in my mind as an analyst of these things. But it's an amazing experimental result that they do indeed work, even in these infection-down-lated results. Great, thank you. Okay, thank you very much. With this, we end this session. I'd like to thank all the presenters. It was quite interesting talk. Sorry for being slightly late, 10 minutes or so. So, with this, we end up, and now we have the discussion. So, with this, we end up, and now we have the discussion, wrap-up, and closing. So, I give the floor to Ian, maybe. Okay, thanks, Mano, for chairing, and thanks to all the speakers.