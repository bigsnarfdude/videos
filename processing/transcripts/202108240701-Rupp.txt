The university is actually quite easy to pronounce for a German. So the La Paranta Lachti University of Technology. Thanks a lot for everybody who's joining. And I specifically want to thank the organizers of the conference who did a great job in organizing everything. Everything's running really smoothly. And especially in the times of the pandemic, I think that's particularly hard. And I also want to thank them for giving me. Want to thank them for giving me the opportunity to do this presentation here. And starting with having to thank somebody, I also would like to acknowledge my co-authors or collaborators. And by doing so, I would also like to give some sort of a small sort of a history of EG methods, which were kind of originally proposed by an article by Becker et al. in 2004, but back then were called like the P1. Like the P1, a reduced P1 discontinuous Gaiokin method. And then they were sort of reinvented and further developed in the group of Mary Wheeler, especially by Mary Wheeler and Stan Yun Lee, who's also one of my collaborators. And she then gave, I think in 2018 at the Interpoor in New Orleans, she gave a great presentation where she introduced this new method and she told everybody about the nice and also the not so nice properties of it. And she basically invented, yeah, she. She basically invented, yeah, she invited everybody to join the community of EG. And since it's such a new method, everybody can contribute and find new things and interesting properties of this method. And therefore, she also proposed some open problems. And basically, Sangyon and I started to tackle one of these open problems. And then Wadim Eisinger came to the idea that one could also have a look at adaption equations for IG. And we quite quickly recognized. And we quite quickly recognized that for these types of equations, you need some sort of add-infinity stabilization. And when you're in Germany and you have some sort of issues with this kind of topic, then, or at least if you are me, there's a natural address to go to if you have these problems, and that's Gortmund. So we asked Dimitri and also Hennes for help, and that's how they joined this sort of community. And Dimitri actually came up with actually two ways of doing L-infinity. Two ways of doing L-infinity stabilization, which work out pretty well. And also had the idea to do some sort of additional enrichment to this enriched Gajorkin scheme, the sub-cell enrichment, which will be the third part of my talk, basically. And this was then conducted by Wadi Eisinger, Florian Frank and Mortitz Hawk. Wadi Eisinger, Florian Frank, Wadi Eisinger is in Bayreuth, Florian is in Erlangen, Morditz in Augsburg and Sangjun. I forgot to mention that is at the Florida State University. Is at the Florida State University. So let's maybe jump right into the topic and start with something maybe all of you are quite familiar with. So, what we're basically looking at is the linear attraction equation, which is at least the simple most looking equation I think one can imagine. Although it's quite hard for the solutions to be approximated numerically. What we have is we have an unknown U, we have some velocity field or whatever at action. Velocity field or whatever at action A, and the whole equation that lives in this time space domain. Of course, we also have some inflow boundary, gamma minus, and some inflow data, and some initial data. And then quite an artificial condition, which will not hold for this limiting and stabilization issues. The more general is that the unknown is some sort of in H1 with respect to the spatial domain. As already mentioned, this will be kind of skipped or disregarded in the following. But it's important for the analysis at the end. The analysis at the end. We have some sort of a solenoidal velocity field, which is supposed to be Lipschitz continuous. Then, of course, the domain is supposed to be kind of nice, so that is founded on Lipschitz. Yeah, already mentioned we have this inflow boundary and initial datum. And then basically, if you want to discretize this equation, what you proceed as always in the standard way, what you do is you want to obtain a weak formulation that is basically you test the equation with a smooth test function with respect to some subset of our respect to some subset of our domain omega and the subset is already noted e indicating that this will later be on later be an element of a mesh and then you multiply the equation by the test function you integrate and you integrate by parts and then it's quite a familiar thing what you what you what you get out of that now basically we want to turn to the discontinuous guykin method um as sort of a of a suit of a source or a root method so what we do with this weak formulation is okay we assume it to hold basically on Assume it to hold basically on all cells of a mesh. And then we have to replace the test and trial functions by some discrete approximations of these. And for DG, you just assume that these discrete approximations, they are element-wise polynomials, but you don't assume any kind of continuity condition between two elements. Therefore, what changes basically is that the gradient doesn't exist with respect to the whole domain anymore. So you sort of break it or split it up into the gradient over the different elements. The different elements, and you also have to replace the boundary integrals by pseudo-nomerical fluxes, which will, in the following, always be the standard upwind fluxes. And you also receive some sort of a linear form, which is quite standard. And now, if you sort of want to get to IG at some point of time, what you need to think of is a kind of a weird, let's say, interpretation of DG as sort of the source and the root method for continuous Gajorkin and finite volumes. Continuous Gaia and final volumes. So basically, the idea you could follow is: if you now do not assume these test and trial functions to be polynomials, which are allowed to be discontinuous across certain interfaces, but you force them to be overall continuous, basically, then what you come up with is the continuous Gajawkin method, the standard finite element method. And what changes basically is that this guy here, it vanishes at interfaces because you have the jump in the test. Because you have the jump in the test function v here, basically. And if the solution is overall, the test function is overall continuous, then of course the jump will vanish and you come up with the standard finite volume, a finite element formulation. As opposed to this, you could also say that you want polynomials of degree k is zero only, and then you sort of come up with the standard or one standard finite volume scheme. Basically, in this case, this guy here, it cancels out because if v is a constant, then the gradient is zero. And this guy here, Gradient is zero, and this guy here will go away. And here, there's only constants you test with one. We receive the standard final volume scheme. And bearing that in mind, we have a short look at the advantages and drawbacks of the different methods. Well, the first thing one could argue is basically the local conservation property. Saying that local conservation basically is given by testing with piecewise constants, then of course the final volume scheme and the DG scheme, they are both. Scheme and the DG scheme, they are both locally conservative. While for the linear CG or the linear finite or the finite element schemes in properties in general, this is not the case because we cannot test with element-wise constants. But in the next presentation by Remy At Gral, I believe that we will see that they also satisfy local conservation. For the rest of this comparison, we will restrict to the lowest order schemes of the CG and the DG method. So that is. The DG method, so that is piecewise linear. And if we compare these basically, then we have a small amount of degrees of freedom here because each of the node for linear CG has one degree of freedom. While for this freedom scalar triangulation, at least all the interior nodes have six degrees of freedom in the DG sense, trying to count them somehow. While for finite volumes, we clearly see that each of the cells carries one degree of freedom. Cells carry one degree of freedom. So, this is a very small amount. And now, if we think of convergence order and stability, and we have the famous papers of Johnson and Peter in mind, there's a certain relation between the convergence order and stability. And they basically, one of the results is that if you have a certain sort of stability, you get an additional order of convergence one half, which you see for the linear Digi and the finite volume scheme, but you do not see it for the linear. But you do not see it for the linear CG scheme. And this condition basically is that you can test with some sort of rescaled gradients. So basically, what you need is that if you take a gradient of some element of the DG space, then it is an element of the same DG space again. You have the same for the finite volumes, basically, but you do not have this for linear CG or for all the CG spines. That is basically what makes this sort of What makes this sort of stability and gives the additional order of one-half, and then we also have the order k, which comes basically from the polynomial approximation property, which is sort of intuitively speaking. We will see this in more detail later on. And the question one could now rise is, okay, we have the beneficial properties from finite volumes, which is basically the stability and the additional order of one half, and we have the small degrees of freedom, and we have the beneficial. Degrees of freedom, and we have the beneficial properties of the linear CG method. And the question is: can we merge them in a suitable way such that we get basically all the additional properties of the DG methods, which are the combined additional properties of finite volumes and CG, but without this enormous amount of degrees of freedom. And this is one way to look at EG, basically. So this is done to obtain EG. And what we do basically is we take the test and trial spaces for continuous. Test and trial spaces for continuous Gaiorkin, so the final elements and the finite volumes. So it's a kind of a weird space that is, we have a degree of freedom per node and we have an additional degree of freedom per element, loosely speaking. However, we have to observe that in both these spaces, the global constants are contained. So if we want to come up with a basis, it's not just adding the both basis because we have one dependent degrees of freedom. We would have to get rid of one at some point. Point. And since we know that the DG bilinear form is, so to speak, the root bilinear form for both the schemes, CG and final volumes, we just stick with this bilinear form and we still are sure that it's like consistent and working out. And the linear form is the same. So now we hope that we get all these beneficial properties somehow. So this optimal convergence order of k plus a half, we want it to be stable. We want local conservation to still hold, which is kind of obvious. Obvious. Basically, what's nice about this is that all these things can be obtained or all these properties can be shown by just using the DG proofs in a kind of different matter. The only additional idea one has to have is some sort of constructing a suitable interpolation operator. And basically, this suitable interpolation operator, for example, can be given by just using the standard interpolation to polynomials, to polynomial spaces. That is a finite element interpolation operator. Plus, then we project the error between the original. Project the error between the original function and this interpolation to the piecewise constants. So, that is one possibility. There are others, and they have all their pros and cons. But this is maybe the most intuitive and where you can get the furthest, at least from my perspective. And having described this, now we can come to the issue of locus doublization and some properties of EG. And now we start with a comparison to the DG. So if With a comparison to the DG. So, if DG has some sort of spurious oscillations or violates some bounds or whatever, basically, when you think of this correcting these things, you have a big advantage. And that is that you can have a local orphanomal basis representation per element. So, what you do basically is you do a two-step approach. In a first step, you apply some flux limiter. That is, you correct the local means. And the local means, they carry all the mass in this approach, basically. So, you maybe increase the mass in one element, but of course. Maybe increase the mass in one element, but of course, to be over conservative, therefore you decrease the mass in other elements, neighboring elements. And if this is done, basically, the local means are correct, then you can take care of the higher order parts by just applying a slope limiter. And then basically you're done. So the limiting is a two-step and it's very sort of intuitive in some sort of sense. Now, if we look at the EG scheme, things get kind of more complicated because we have this issue already. Because we have this issue already that there's no, let's say, unique, not a good unique basis representation because the piecewise, the element, the global constants, they are in both approximation spaces, loosely speaking. And of course, there's no local orthogonal basis representation. This, of course, raises the question: what is a good choice for the degrees of freedom? Most intuitively, one would split up in the solution UH by just defining the disconnect. By just defining the discontinuous degrees of freedom, basically, those are the ones that are the final volumes degrees of freedom. They are taking care of the piecewise constants, and then the continuous degrees of freedom, basically describing the final element representation of the method. This would maybe be the most intuitive choice. However, it's not a real physical choice. It's kind of, from the real world perspective, it's kind of made up. A more, let's say, physical choice would be to split up the solution into... Be to split up the solution into the local mean values and the continuous part of the solution. This, from a certain perspective, which we'll see in two slides or so, makes more sense. And still, we can reconstruct our solution from the local means and the continuous part by additionally evaluating the local means of the continuous part. So, this is still well posed, or it's still easy to represent all the functions by these types of unknowns. And if we use Unknowns. And if we use these types of unknowns, what we see is that we have a sort of a good structure of the problem. So still we have the well-known bilinear form, but now if we have a look at the structure of the problem, one can rewrite the HE scheme in the following sense. So the first line basically here is taking care of the local means. That is to be tasked with the piecewise constants. And therefore, this intermediate guy here, it cancels out again, basically, like with the finite volume. Again, basically, like with the finite volumes. And if we do so, we immediately see that we have a diagonal mass matrix. And then, yeah, we have the local, the discontinuous unknowns, and we have a right-hand side. And this right-hand side basically only consists of this guy here. So we already see this is the upwind flux. So basically, this is sort of a finite volume scheme. But of course, we also have to take this continuous part into account. While if we consider While if we consider the continuous degrees of freedom, then we would like to do some sort of mass lamping, which is always a good idea if you think of stabilization of methods. And additionally, it gives you this, or that's also the purpose, it gives you this diagonal mass matrix. Then, of course, we test with overall continuous functions. That is, this guy here cancels out this time. So we end up with a standard bilinear form on the right-hand side for finite elements. And of course, we've introduced some error by the mass lamping. We've introduced some error by the mass lumping, and this error is corrected by this term here, which is just correcting the lumping error, and that's it, basically. So if we have such a method, we can now have a look at some properties. Of course, we have this diagonal mass matrix here. What's important to note is that basically what's driving the equation, so to speak, or what's the approximating part of the solution we want to see is actually the continuous part of the solution. Is actually the continuous part of the solution. That is, the continuous part of the solution is almost the discrete approximate to the solution u. One can easily understand this if we consider the kind of DG setting and we maybe have a smooth solution, then we know from the DG analysis that basically chance they vanish so quickly that they do not disturb the order of convergence. Similar thing happens here, too. And then we have that. And then we have that we can write this correction error between the lump mass matrix and the consistent mass matrix as a discrete diffusion operator. It's basically a graph Laplacian. And of course, we can rewrite this right-hand side here as just a single term to simplify notation. Then this line here basically becomes this line here. And we have that this continuous part of the solution. So if you remember these unknowns with respect to the Unknowns with respect to the finite volume part, they do not change mass overall in the simulation. That is, we test with the global constants and then we see that the mass of the continuous part is basically equal to the mass of the mean values. That is, the difference between those does not change over time. And knowing that, basically, we can come up with an algebraic splitting, which one always does. Algebraic splitting, which one always does if you want to correct the solution. So basically, the idea is we come up with one low-order scheme, which is provably, which satisfies some local maximum principles in a provable sense. And afterwards, we correct this low-order scheme to be basically as close to the real approximate, which would have the optimal convergence order. However, it does not violate this local maximum principles, is L-infinity stable, or has also additional properties, which one would like to. Properties, which one would like to have. So, what we have to do now is we have to split up these right-hand sides to come up with a low-order scheme and this high-order correction. And basically, if one does this for the local mean values, for example, the beneficial property is that the first equation, which is for the local mean values, here the low order schemes, it also only depends on the local. Only it also only depends on the local means, and the same holds for the overall continuous part. The low order scheme for the overall continuous part only depends on the overall continuous part, while these corrections might depend on the other part as well. So if we do this, for example, for the local means, the low order scheme can be obtained quite intuitively. We know that the finite volume method, basically, it satisfies all the constraints we want to impose on it. So it has this local maximum principle. So basically, we just write down the finite volume. We just write down the finite volume method for the local means as a low-order scheme, and the correction is then just the difference between the local means and the overall solution we would like to have. So basically, if you remember the bilinear form, it's just the difference which we have to add. And for the continuous part, we use a discrete upwending as a low-order scheme. And we will more focus on the discrete part in the following slides. What we could do then is do some. Yeah, do some flux-corrected transport and monolithic limiting, connected limiting to then, if we know that the low-order scheme is satisfies some maximum principles, to correct it as much as possible. And of course, we know that it satisfies some sort of maximum principle since we know this from the finite volume world. And therefore, we can easily show that if the time step satisfies some CFL-like condition, which is given here, then of course the local discrete maximum principle in the usual sense holds. In the usual sense, holds for this first-order upwind approximation as defined before. So, basically, this local maximum principle says that our solution will be in between the smallest neighbor or the local mean value will be between in the smallest neighbor of local means and the largest neighbor, the largest local mean of a neighbor. So, this is quite intuitive and also it fits very well into what, or it is exactly what Manuel told yesterday. What Manuel told yesterday. And now we would have to correct this. And basically, the question is: what is our FCT solution? So, this is our corrected solution. Basically, as already described by Manuel, maybe much better than I can today. The key observation is that you can write this FCT solution as low-order solution plus some fluxes. And if we now apply the whole fluxes, which we left out before. Which we left out before, we would just come up with the standard solution again, which exhibits these spurious oscillations, for example. So we limit these fluxes by some sort of introduced parameters alpha, which then will be chosen as large as possible to be as close to our real or to the solution the real method would, so to speak, obtain. But we shake it as small as needed such that these various oscillations do not occur. And if we want to do so, And if we want to do so, we have to define sort of a maximum value, these local means might attain in an element, and a minimum value. And these values are now the critical point, which are sort of different from BG or whatever, because now we have to take into account the local means of neighboring cells. But if we want to have like some sort of an no spiritualization within the whole basically thing, we also should take into account the We also should take into account the continuous part in the vertices, which are neighboring or which are belonging to our cell, and the low-order solution of the neighbors. So, basically, the maximum bound would be the maximum of all the mean values of neighboring cells, the mean values of the low-order solution of neighboring cells, and the continuous part of neighboring cells. The same for the minimum, and what we then try to control with these bounds is the flux between two elements. And you might already have recognized this is not a very good picture. Have recognized this is not a very good picture, so this is so to speak. Check if you're already sleeping. Since we are using EG, of course, we have to have continuous finite elements somewhere. And it's hard to obtain a continuous finite element formulation for this kind of a structure. So we would like to have like only triangles or squares or quadrilaterals in this picture. And with these bounds defined, basically, we would like to come up with these correction factors. And what one could do. And what one could do is to apply some worst-case assumption. Basically, we correct it by the minimum of one. So we should never apply more than the flux, which is basically actually wants to enter an element. And then we basically multiply it or we multiply it by the thing that or the amount of, say, concentration that is allowed to enter an element over the worst case amount of concentration that wants to enter the element. And if we multiply this. Element, and if we multiply this by the flux itself, then we are sure that we do not exhibit this worst case. Of course, if we want to go for the lower bound and check this, we also have to do that in an analogous fashion. And if we correct the flux between two elements, then one element will have a problem with the upper bound, the other one with the lower bound, and that's how we construct our alphas. And if we did this correctly, and also we did the analogous part for UC correctly, then For UC correctly, then we will have that both the local means and the overall continuous part, they will be local maximum principle, where they satisfy a local maximum principle. However, the issue now is that we know that the local means and the continuous part, they both independently satisfy a local maximum principle. This does not impose that the overall solution satisfies the local maximum principle. However, I also have a sort of a small simulation here. So if this is based Simulation here. So, if this is basically the low-order approximation, and there's a non-conservative weight, just because to make the picture look nicer, to which the method would be would like to correct itself, then we show or we check basically how much can we really fill into the next box. And if the next box can be filled by something, then we are just apply this. This is basically what this limiter does. Okay, the other idea is to obtain or to do this with monolithic convex limiting. I will not go into too far. I will not go into too far too much of a detail with respect to this formulation, since it will be detailed in much greater detail and also much better, I suppose, by Einnes Heidegger in the talk after the next talk, basically. However, we can take home as a message that monolithic convex limiting consists of two words, or three words actually, limiting is clear. Monolithic basically says that we put every constraint we want to have into the equation itself. Into the equation itself, which has some sort of benefits. Basically, one benefit is that we do not rely on a certain time discretization, also allowing us to go to steady-state problems before we sort of relied on the explicit Euler or SSP methods. And the word convex limiting basically relies on the fact that basically we obtain that it's an admissible solution since we always take convex combinations of other admissible solutions or approximations, and therefore everything will be fine. So the main Be fine. So, the main idea is to put everything into these equations in a certain formulation. And what we need to have some sort of well post-ness of these formulations is always a kind of a Lipschitz representation of our overall equations. So, what we will heavily employ is the maximum and the minimum functions. And of course, we have to define upper bounds for the local means again, which are kind of the same as before. Here, it's the maximum of the inflow boundary, the maximum of all. Inflow boundary, the maximum of all local means which are basically neighboring, and the maximum of the continuous part of all neighbors where the support is given. And then, of course, by this, we continue with correcting the fluxes. Again, basically, if something flows from one element to another element, we have to take care of the maximum of one element and the minimum of the other element. And basically, that's why these triple minimums are applied here. Minimums are applied here, or this triple maximum is applied here. So, yeah, all of this will be discussed later on, I suppose. And also, again, if this is conducted in a reasonable fashion, then the overall method is balance-preserving or local extremum diminishing under the assumptions we already proposed. So, that's basically nice, but as already mentioned, we have the problem that this. As already mentioned, we have the problem that this only holds for both the parts, but not for the overall solution. Now, if one remembers that the basically the convergence property, it already is sort of hidden in the continuous part of the approximation, one could, for example, just disregard the local means or the discontinuous part, sorry, and just plot the continuous part and would be kind of fine. However, there's also an even better way to represent the solution, and this is by a And this is by a constraint projection. What we do basically is we now interpret our EG solution, the whole EG solution which we obtained, to be not an element of the EG space, but to be an element of the DG space, which is sort of covering the EG space. So what we now allow for this projection is discontinuities between the elements. And we do not sort of restrict how this discontinuities look like, because for the EG scheme, this continuity was always just by a constant. The method was. Just by a constant. The method was shifting the same solution sort of on different elements by a constant. And now we do not have this restriction anymore. And therefore, we can just reconstruct our solution and then use the DG slope limiters, maybe the ones from Cusmi and these vertex-based limiters, and then we will have an admissible solution again. And this is sort of the best, at least, I know you can get. Now to some numerical. Now, to some numerical results. As you see here, we used the Slavec solid body rotation scenario. Basically, there are three initial objects. They are rotated once. And if we do this with CG, what we see is these oscillations here indicating this lack of stability. While if we do it with EG, everything is looking much nicer, although we have not yet applied any limiters. And therefore, we still see slight oscillations. We still see slight oscillations or smaller oscillations. And if we continue now using the limiters, we see that the quality of the solution largely improves. So the FCT and the MCL version, they're both from the, I would say, I-norm, they are indistinguishable here, but usually we experience that MCL is a little better. Then I would like to highlight that all these facts with simply says or with triangles and squares, quadrilaterals, they were from a Quadrilaterals, they were from a purely analytical point of view. So there's not a lot of difference if you use a mesh of triangles or of quadrilaterals here. But now this difference will kind of become more prominent when we do like the analysis section, which is to come. And this starts with local subsoil enrichment. And the basic idea you could have here is, well, what if we have the addiction equation on some sort of domain? And now, let's say by some magic, one would... Let's say by some magic, one would like to do this sort of adaptively, but for let's say the convenience of representation, we say that by some magic, we know that there's some place in the domain where something is bad. We have some troubled cells where we would like to have either additional, let's say, stabilization properties, or we would like to have sort of finer mass conservation properties or whatever. Or we just want to refine the mesh for some other reasons. So, what one does then basically, or in our case, what we do, is we start with a very fine mesh and we give ourselves a finest version of a mesh, and then we consider the troubled cells, and then we just refine these cells by combinations of cells of the finest mesh. So, that's kind of an artificial assumption, but basically, it's always true. So, what we do now is we refer. So, what we do now is we maybe there's some trouble in this cell, so we refine it once. Maybe there's a little bit more trouble in this cell, so this contains a finer cells, and so on and so forth. And what we now assume, and this is sort of important now, is that all the meshes that are these sub-meshes of a single cell, they are supposed to be quasi-uniform. So, but this mesh is supposed to be quasi-uniform, this mesh is supposed to be quasi-uniform, and this mesh is supposed to be quasi-uniform. They don't see one another. Another. And then we would like to do EG on this type of a mesh series or this type of a domain. And again, we would like to make it even more, let's say, general. So we use the DG bilinear form again, but we use this very fancy test and trial spaces here. So basically, what this here says is we have the space of finite elements, basically in this first part here, finite elements of degree at most k. So it's piecewise. Most k, so it's piecewise polynomials, but they are overall continuous. They are enriched by a discontinuous function of degree L, which live on the coarse mesh, and further enriched by a discontinuous function of degree M, which live on these finer cells. This gives sort of a natural hierarchy that M should be smaller than or equal to L, and this should be smaller than equal to K, where K is supposed to be larger than zero because the linear finite elements is the smallest. Because the linear finite elements is the smallest, so to speak, you can get. And if you don't want any enrichment, you set these numbers to zero, to minus one. And this basically, the polynomial degree minus one basically then corresponds to just adding nothing. So no enrichment at all. So what we get out from this basically is if we choose m and l equal to minus one, then there has no enrichment by discontinuous functions. And there's only the test and trial functions from the CG method left. So we have CG on the CG method left. So we have CG on this coarse mesh. If then we set L equal to K and M still is minus one, then we have the CG method of degree K, and we enrich it by discontinuous polynomials of degree K2. So basically what we get out is just dg on this mesh TH, this capital H. If we are additionally fully enriched with polynomials of degree K again, basically, so all this are the same. Basically, so all this are the same orders of degree, orders of polynomial spaces now. Then, what we get out basically is kind of intuitive now is dg on this finer mesh. And if we are thinking of recovering the method we have just investigated before, is basically we set m to minus one, so no finer enrichment. The polynomials here are chosen to be the linear, so we have a linear finite element method combined with a zero of order dg. So, L is zero, k is one. This gives us the is zero, k is one. This gives us the classical EG method from before. Again, I would like to highlight that m is minus one basically always removes the enrichment and gets to the more simple method in some sense. So if we do this basically and try to do some analysis for this, one could assume that it becomes much more complicated. But actually that's not true. If we follow that steps, actually what we know is, what we should know is just what happens to DG. What happens to DG, and then what happens to EG basically is the same. The only thing that changes is this interpolation operator or this projection operator. But we have to know that if we look, if we consider this pi here, it's not a projection anymore. So because we don't, it goes from L0 and the functions should be L2 and C0. So they should be overall continuous. But if we apply the operator, they will not be overall continuous again because we have this L2 projection to. This L2 projection to some discontinuous space in it. So, what we really do here again is we choose the interpolation to polynomials of degree at most k. And then we project the error basically we did to this to the space of polynomials which are discontinuous. So, it's the kind of a weird structure. It's this polynomial space of polynomials of degree L on the on the coarse mesh and M on this finer mesh, but it's Mesh and M on this finite mesh, but it's basically just the L2 projection to all the kinds of polynomials that are allowed to be discontinuous. So it's not a projection because it does not, the range of this operator is not within the original space. However, one could also use other projections for analysis and basically those projections used for all these other interpolations for analysis, which are which need to be those interpolations you need to put the initial data into the correct space. For example, Space. For example, you could just use the final element interpolation operator, IK, which is sufficient as long as you do not want to have the best properties that come from this local refinement. You could use the L2 orthogonal projection to the EG space, which is great, but actually I didn't manage to find, or we didn't manage to find proper proofs if you apply this, but we are sure that it's actually possible. And then basically, you can do just the DG analysis. You can do just the EG analysis. And just to show you that it's really that simple, basically, is I show two small results. One is that the EG solution is stable and I denoted the EG solution just by a capital U here. The reason for that is that now I would like to have to have UHH and then LKM or something like that. So it's a really complicated expression. So I'll just abbreviate it by U. And if you want to show that, basically what you do is you test your bilinear form with U and you use form with u. You use this identity you basically always use if you want to have some sort of statement with respect to parabolic or hyperbolic equations. You integrate by parts and then you receive the first equality here. The only thing you should know from DG analysis is that these two guys, they simplify to a positive term and then you just use usual Cauchy-Schwarz and Young's inequalities and Groenwall at the very end to obtain the stability. At the very end, to obtain the stability you want to have. So it's basically the proofs don't get more complicated, although the scheme gets much more complicated. Basically, the last part is then the convergence. And this is where I would like to also show that this sort of strange property of the gradient, re-re-scale gradients, is sort of important. Here, basically, we have a slightly more complicated. We have a slightly more complicated setting. So, we have this mesh, which is with all the cells being quasi-uniform. That's why we call it sort of just because we quasi-uniform double sequence of meshes. And we have the artificial sort of precondition assumption that U is an H1HK plus one, which is quite a lot. And it comes from this interpolation, which is in the projection. If we used the pure L2 projection, then this year would be L2HK plus 1, but I'm not able to show that it's really. But I'm not able to show that it's really true, but it's kind of intuitive. Then the H approximation U converges in the L infinity L2 sense, that is just in this sense, with this kind of an approximation rate. And what we see here basically is that we see the age of the coarse mesh, and we see everywhere where the mesh has been refined, we also see the age of this specific cell. And what we see also is that it's basically at the end. That it's basically at the end, it will be k plus, it will be just 2k, and at best, it will be this plus one half, which we obtain sometimes. So, from this m tilde. And basically, it's one half if the assumptions with this with this rescue gradient holds. So, basically, if the mesh is simplistical and we enrich on the coarse mesh by a polynomial degree of k minus one at least, because that's where this property. Because that's where this property holds, or if we just use the dg method, because then we already know that it is true, and otherwise, we just obtain m. So we will just then obtain an overall order of convergence of k basically. And to show that this is true, I will just go through it extremely smoothly because proofs are not like that much, I think. We split up our error into a discrete part and an interpolation part. Then we use consistency of the method and test with the discrete part. And then after some algebraic. Part. And then, after some algebraic manipulations, we come up with this equation here. And we can already basically see from the DGE analysis, if we know it, that this year will just enter Grunwald's inequality. Then, if the interpolation is sort of well-suited, this year will converge with an order of k plus one half. This year will converge with an order of k plus one. Oh, sorry. This year will converge with an order of k because of the gradient here, which will be drawn to the left-hand side. This year will be. From to the left-hand side. This year will be or converge with an order of one-half if everything is well-suited. And this year is the bad guy because it only converges with an order of k. And now this bad guy will totally vanish if the gradient is in the space to which to this interpolation is orthogonal to. And that's the dg space, the space of all the discontinuous functions we had in this projection. So that's the reason why this here, in this kind of analysis, simplified from Johnson and Wittgenstein. Is this simplified from Johnson and Bit Granda because we have much stronger assumptions why this works out in our case? So, the only fun thing one has to prove with respect to EG is that this interpolation is sensible and converges in the optimal sense. And this is kind of easy in some sense because you just write out what this interpolation is. You can easily rewrite it as the L2 projection of the interpolation. Then you, because you have this double mesh sequence, you do some analysis. You have this double mesh sequence, you do some analysis tricks, but then basically you have the L2 projection error of the interpolation error. You just do Bramble Hilbert once, then you get the interpolation error, you do Bramble Hilbert a second time, and then you are done. So basically, the only thing you do is you do the standard analysis twice. So with that said, I can show some numerical results. Well, what we see here is we have this coarse grid of an of a An of a refinement stage four for the first four pictures, and we refine the fine grid, but only around these, since we know whether magic will happen, we refine it only around the domain where the three objects will move. And then we already see that refinement of the small mesh gives you something, so it slightly becomes better if you refine the small mesh. But of course, if you refine the whole mesh, you also see that the solution quality improves. So that's exactly what we would like to see. That's exactly what we would like to see as a low order of convergence when refining the small mesh, because we have a lower enrichment there, a lower order enrichment there, and a higher order of convergence when defining the coarse mesh, because we have the full order of convergence there. And that's basically what this analysis also told us. And with that, basically, I just have some general notes on EG left. Basically, it's some sort of an advertisement. I would like to highlight that EG has been applied. Has been applied to various vast family of problems, including elliptic and parabolic problems, also with dynamic mesh adaptivity. There's something on multi-phase fluid flow and porous media. There's something on non-linear poroasticity. So really also harder problems that have been tackled with EG two and three field formulations. Also, they used it to describe phase field models describing fractures and stuff like that. It has been applied to Shalo. That it has been applied to Shilla-Water equations too. And the last slide then is about what you should take home. So, the most important message from my point of view is that if most people talk about EG or some people talk about EG, it's not to be mixed up with extended finite elements, which is something commonly done. And it also is sort of, there's a reason for that. First of all, the names are quite similar. And also this enrichment strategy that which you have and employ is also in a sort of sort of In a sort and sort of sort and kind of sense, the same, but it's also significantly different. So one should not mix it up. The second thing is EG can be seen as a generalization between CG and DG. But one also has to be careful here because it's not in this Crosseravia sense that you sort of relax the continuity constraints by saying, okay, we just have it up to, let's say, certain moments. And you get from CG to DG by using, yeah, this rabia stepping. Closely ravia stepping, but it's a different way. The third thing I already told you: EG is a relatively new scheme, and I think that the community is quite rapidly growing as much as I can see it at least. So there's still a lot of things to be done. Even one can contribute quite, let's say, fundamental results to it. So from that point of view, it's very interesting and very nice. And it's also computationally very attractive, I think. For analysis and also for WCC. For analysis and also for stabilization, the crucial trick was basically doing everything twice. And for stabilization, especially choosing the, let's say, degrees of freedom in a correct way, and then doing everything twice. And with that, I would like to thank you for your attention. Thank you, Andres, for that very interesting talk. Okay, it looks like we have some hands raised. Let's see. People put their hands up and then put them already down. So if you