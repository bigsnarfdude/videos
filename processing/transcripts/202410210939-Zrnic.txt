Thank you. Awesome. Thank you. And thanks so much to the organizers for putting this together. This is really an awesome group of people. Okay, so today I'll tell you about a line of work that I've been working on over the past couple of years or so, and it's called prediction power dippiness. And like Jamie said, please feel free to interrupt at any point. I'm happy to take questions. Worst case, we're not going to get through all of it, but I want to make sure that at least like the first part, we're all on the same page. We're all on the same page, but okay, so this is based on joint works with a bunch of different collaborators. So, with these collaborators on the top, we were mostly working on core methodological work, and then at the end, I'll also tell you about some applications of these methods to social science problems that we've been working on with people in the bottom. Okay, so the motivation for what I'll be talking about comes from this observation that we're seeing. Observation that we're seeing an increasing number of settings where machine learning predictions are replacing real data. And to really convince you that this phenomenon is very pervasive, I want to give different examples from two different problem during this. So this is something that we're seeing in policy where predictions from satellite imagery are replacing survey data and ground-based measurements. So what people are doing now is they're taking satellite images from readily available platforms such as Readily available platforms such as the Google Earth Engine. And they're training computer vision algorithms to predict all kinds of socio-economic or environmental indicators from these images. So they're training algorithms to predict things like poverty, deforestation, population density, and so on. And so these computer vision predictions are then being used in downstream statistical analysis. This is definitely something that we're seeing in science, and I think especially after the In science, and I think especially after these Nobel announcements, we're going to see even more of this. So, we all know that the release of AlphaFold in 2020 created a whole explosion of new biology research because prior to AlphaFold, we had on the order of about 200,000 experimentally derived protein structures. But then AlphaFold came out, and now all of a sudden we have protein structures, which is all proteins that are known to science. And so, of course, this whole new wave of data generated a lot of exciting. Of data generated a lot of excitement about the fundamental biology. And finally, especially now with LLMs, we're seeing this all over the place in text analysis. So we're seeing annotations from large language models being used as a replacement for human judgment. And so we see this in contexts like model evaluation. Okay, and so if you think about it, I mean, there is a pretty good reason for using. A pretty good reason for using machine learning as a substitute for real data. So, machine learning predictions are abundant, they are very cheap to collect, they are very fast to compute. Real data is scarce, it's expensive, it's slow to collect, but of course there's a flip side, which is that with real data, we're getting something that's generally accurate. We're getting just the gold standard measurement of whatever we care about measuring, whereas with machine learning predictions, we generally don't know what we're getting, especially with these. Generally, it doesn't always get anything, especially with these modern black box models like the ones that I showed in the previous slide. And so, of course, any kind of downstream inference that just naively trusts these machine learning predictions has to be invalid. Okay, and so that brings me to the topic of my talk today. So I'll tell you about methods for inference assisted by machine learning predictions. These methods can be applied with any black box machine learning model. They come with provable validity guarantees. They come with provable validity guarantees, which are the same. And the theory applies to many different inferential tasks. Okay, and so in particular, my talk will roughly have two parts. So in the first part of the talk, I'll talk about prediction power difference, which is a setup where we have a small amount of good data, so we have a small amount of labeled data, and then we also have an additional large amount of unlabeled data. And we're wondering how we can use machine learning to impute all of these labels that we don't have. To impute all of these labels that we don't have. Okay, and then in the second part, I'll talk about a related setup. It's called active inference. And there, the setup will be that all of our data will be unloadable. And we want to use machine learning to somehow help us decide how we should collect the data in the first place. Okay, so I'll jump into the first part. Okay, so this is the basic setup that we'll be working with. With. So we have capital N unlabel data points and we have little n label data points drawn from the same distribution. So here in this picture we're observing x tilde, x and y, but we're not observing these outcomes, y tilde. They are missing. And really the setting that you should keep in mind is that this capital N is much, much larger than little n. So usually we can collect a lot of unlabeled data, but we can only collect a much smaller amount of labels. Okay, and if you want to keep sort of a running example in mind, you can think of these x's as being images, for example, satellite images. So we can collect a lot of satellite images. But Y would be some measurement, like let's say the deforestation rate. So to accurately measure deforestation rate, I actually need to say send the truck out into the Amazon to take accurate measurements, and that is very expensive. So I only have this expensive label for a much smaller set of images. So far, it's good. Setup images. So far, it's classical semi-supervised image. Yes, this is the semi-supervised head setup. Yeah. But the task will be different, which we'll see in a moment. Okay, and so in addition to this data, we have a black box machine learning model. So you can really think of this as just some black box function f that can predict y from x. So we're not going to make any assumptions about what this model is doing, how accurate it is, or anything like that. Okay, and so of course, intuitively, what we So, of course, intuitively, what we kind of hope to do in this setting is we would hope to take this machine learning model and just impute all of these missing labels with our machine learning predictions. But of course, if we do this naively, that's not going to work. And so what I'm going to tell you about is basically a principled way of implementing that kind of strategy. Okay, and so our goal in this setup is to estimate some quantity of interest data star. And so because this is a machine learning theory audience, I just want to emphasize that this I just want to emphasize that this theta star, you should think of it as some low-dimensional statistic that we are trying to estimate. So it is not, we're not trying to train a better machine learning model. We're not trying to get a predictor of y from x. We're just trying to estimate something like, let's say, the expectation of y, or maybe we are trying to estimate model performance. We are trying to estimate the accuracy of our model. So it's a low-dimensional estimation. Okay, and so. Okay, and so note that we are assuming some amount of complete data. So, if we are doing something non-trivial with what I'm going to tell you about, then our estimate should be more accurate than if we just completely ignore this unlabeled part of the data and if we just stuck to this small amount of labeled data. So that's going to be really the main baseline we'll try to use. We'll try to see how much we can squeeze out of this extra data that is incomplete. Okay, and so now I want to show. Okay, and so now I want to show you one very special case of all of this methodology. And really, everything that we do is just a lot of bells and whistles on top of this. But this is really the core idea. Okay, so let's say that my goal is just to estimate the mean of y. Okay, very simple. And then to explain what prediction-powered inference does, I think it's easiest to compare it to just classical inference. So the classical approach, which is this approach that just completely ignores machine learning, Approach that just completely ignores machine learning would just say, Okay, take the sample average of the labels that you have. Okay, and we can say a lot about this estimator, of course. We know that it has the right mean, so it's unbiased for what we are trying to estimate. And its variance goes down as 1 over n times the variance of y, because all of these are just y and this is just an average of y and deep examples. Okay, now what prediction power difference does is Now, this average might be biased because we don't know anything about these machine learning predictions. And then what it will do is it will estimate the prediction bias on the small label data set. Okay? And it will subtract off this bias from the initial estimate. So now, if we look at the mean of this estimate, If we look at the mean of this estimator, it's still unbiased because we're saying everything is IID, so these f of x tildes have the same means as the f of x's, so those two cancel out. So we really are still targeting the right mean. But now the interesting thing happens with the variance. So if we think about the variance of this estimator, we basically have two averages. We have an average over a lot of things and an average over a smaller number of things. When we average a lot of things, that basically doesn't contribute to the variance. So you can really think Contribute to the variance. So you can really think about the variance of the first term as being basically zero. So really, most of the variance should be coming from this second term, which is again an average over n i terms. But now the variance is 1 over n times the variance of the prediction error, rather than the variance of y. And so the whole point of why these methods work well is that these modern machine learning models can explain away some of our outcome variance. Some of our outcome variance. If we use these modern machine learning models, the variance of the error will be smaller than the variance of the outcome itself. So we see that the prediction power and estimator will be better when predictions explain away some are variance. And because we get smaller variants, that basically immediately implies that we can construct smaller confidence intervals. So it's pretty trivial to put confidence intervals on top of these estimators because they're just averages of IID things. So we can just use classical statistical. Facts of statistical theory for this. Okay, and so yeah, I just want to note that this trick that we're seeing here is really not a new trick. That's not something that we're coming up with for the first time. It's something that appears all over, not just in statistics, but in optimization. And it's really a very classical idea that you might have seen in Hopefully this works now. So it's something that we might have seen in the I did something different now, so hopefully now it should work. So it's something that you might have seen in doubly robust estimation, in survey sampling, control variants. There's some keywords that basically make use of. Words that basically make use of a synchronic trick. Okay, and so I want to show you one example application of exactly this algorithm that we saw in the previous slide. So the backstory for this was that astrophysicists had all these telescopes that were taking images of galaxies. But to do science with these images, they needed these images to basically be annotated in a particular way. So, for example, they want to know what's the frequency of so-called Of so-called spiral galaxies, because spiral galaxies are a particular kind of galaxy that correlates with star formation. So, for example, our galaxy is a spiral galaxy. And so, at the beginning, what they were doing is these scientists were just hand-labeling these images, and they would go through many thousands every week. And then they realized that that wasn't sustainable, and they created what is probably the largest citizen science initiative in the world, which is called the Galaxy Zoo. And so, they tried basically educating the general population on how. The general population on how to annotate and have fun with these images. And so then we saw this and we're thinking, okay, this sounds like a great use case for prediction power difference because what we can do is we can take a smaller set of images where humans have provided these gold standard annotations and we can just train a computer vision algorithm. We can just download, we can basically download a pre-trained resonate and just fine-tune on a small number of examples and we can just impute predictions and everything else. Predictions and everything else. And so, this is what happened when we applied the algorithm. So, the green confidence intervals are these prediction-powered confidence intervals. This vertical line is the ground truth we're trying to cover. The gray intervals are just classical confidence intervals. They just ignore machine learning, just use the set of label data that we have. So, we can see that the prediction-powered intervals are smaller than the classical confidence intervals, which is what we would expect. Intervals, which is what we would expect. But now, the one other interesting thing is this imputed interval. So, by imputed, I mean what would happen if we just used our machine learning predictions and pretended that they were just real intervals. We just used them evenly in our downstream analysis. And so, the point is that we have a lot of machine learning predictions, so that's why we are very, very confident. And so, if we don't do anything smart on top, we will be super confident, but we'll miss the browsers. Miss the branches. Now we have three sources. One of them is the system that they know what you're doing. One of them is the crowdsourcing, and one of them is the machine learning. Where does the difference in quality between crowdsourcing and the machine learning come? Yeah, that's a great question. So I do think an interesting extension of all of this would be to look at multiple different Be to look at multiple different labeling mechanisms and to try to take into account what the quality of different labelers is. Here, we're not taking that into account. I actually am not even sure if in the concrete data set that we used, if there is a distinction between the physicist label and the human proxy label. But yeah, I agree. In practice, we should definitely expect these to be more accurate. It's not the same label function. Yeah. Do you know in this example whether the analytics? Example: Whether the ML predictor is actually biased, or like how biased it is? Yeah, so it definitely has to be biased because this is what we're seeing. If it wasn't biased, it would be covering the grand truth. So is it that's like a bootstrapsta overgrown standard? This is just a standard CLT interval, but with the bootstrap, it would look the same. But it is definitely yeah, it is biased. Otherwise, it would it would be right here. Yeah. So assuming like the randomness and the sampling does you do the kind of the bias? Kind of the randomness of this? What do you mean by the randomness in the. So we are, just to be clear, here we are kind of, because we have to evaluate if we're covering the ground truth, we have to restrict our attention to actually the set of data points where we do have all the information. And so we are, like, because everything is simulated, you know, we're pretending that part of our data is labeled, part of our data is unlabeled. So we are, like, making sure that the sampling is completely uniform with the grant. Is completely uniform with the grant. That's what you mean. So, kind of like we have control over that at the simulation. Okay. Yeah, I was wondering about like you could get something that doesn't cover the ground tooth because of the theta sample. Yes. Like it could be on the other side with a flesh sample, but this probably is not happening here. Yes, that's a great point. I totally agree. So that's not the case here because here the sampling is within our control because it's a simulation. But yeah, I totally agree. Like, seeing that you're Agree. Like, seeing that you're missing the ground truth could be also because somehow this IID structure that we're assuming at the beginning is actually not satisfied. Yeah, it could be due to the same. Okay, so I want to show you now the general principle. So this was all about just mean estimation. More generally, we consider estimates that can be written as solving a convex optimization problem. And this class of problems really includes most things. Really includes most things that we care about reporting statistically. So, this includes things like means, quantiles, linear regression coefficients, logistic regression coefficients. So, these are really most of the statistics that we care about in practice. And now what we'll do in this more general setup is we'll basically apply the same kind of trick, but we'll apply it in law space rather than on the raw outcomes themselves. So, what we'll do is we'll again take our We'll again take our data set that's imputed with machine learning predictions. We will average the loss in that data set. Now, again, this loss might be biased. And then what we'll do is we'll subtract off the bias and the loss as estimated on the small data set where we have all the information. Okay, and so by the same logic as before, the expectation of this loss is just the actual loss that we're trying to minimize. So this objective is unbiased. Minimized, so this objective is unbiased. And again, if the predictions are good, then most of these terms in the second average will be basically zero, and so we will have a lower variance than if we just did standard empirical risk position. Okay, so now this maybe seems a little bit abstract, so I just want to give like one concrete example. So if you instantiate this for a linear regression, you will just get a closed-form solution. A closed-form solution. So, this estimator here looks a lot like just a standard OLS estimator where we use our predictions as the outcomes. But now we see that we have this extra term that comes from the de-biasing. So, we have this extra term that tries to basically de-bias this part with. That depends on why. Okay, and so building on just classical. Building on just classical M-estimation theory, what we can do is we can show that this estimator is asymptotically normal, and that allows us to form confidence intervals based on this estimator. Okay, so now I want to show you another application in this more general setup. So, we were really inspired by this paper right here because this paper tried taking just all alpha-fold predictions, and they tried. Predictions and they tried estimating a particular relationship between a structural and a regulatory property of a protein. So, what these things mean exactly is not too important. I'll just say that the structural property is something that we can only know if we have the protein structure. So, we need alpha-fold to know the protein structure. And this regulatory property of the protein is something that we do have for all the proteins. And so they used all alpha-fold predictions to basically estimate this relationship. Predictions to basically estimate this relationship, and then they had a way of kind of sort of justifying, like, yeah, the predictions seem to be good enough. You know, they were trying to be careful about not using actual measurements, but alpha for predictions, and they had kind of like a hand-weavy way of justifying that. And so then we were thinking, okay, can we apply prediction power difference in this setting and what would happen if we did so? And so, again, we had to restrict our attention to the smaller data set where we do actually have all the information for the purpose of evaluating. Information for the purpose of evaluating the method. Okay, and again, this is what we saw. So these green intervals are prediction-powered confidence intervals. So we see that they're smaller than classical confidence intervals that can sometimes even blow up. And again, I don't know if you can see it, but there is this imputed interval that, again, misses the ground truth. So again, it is very, very confident because we have a lot of alpha flow predictions. We have millions of alpha flow predictions. Have millions of alpha fulperitions. So these confidence intervals will be very, very small, but they're going to miss what we're actually trying to learn. Do you know the ground truth in this case? Yeah, so the ground truth we always take as the value of the statistic on the whole large data set where we do know actually the y. And then what we're going to do is we're going to, so we're using that as a proxy. And basically, so the actual ground shift has to be within 1 over root n, like where n is like root n, like where n is like millions. So they can't really change much. And then the way we're simulating is we're pretending that like, you know, we're only observing like a handful of labels in this large data set. So basic diagram, the part of your novel labels is actually divided into two validation. So the this so in this example, this So, in this example, this vertical line is the value that we would get on the whole, like in, if we had the complete data set, the complete missing data set. Okay, so that concludes the first part. And so then, in the remaining few minutes, I want to talk a little bit about active infrastructure. I just have more questions about this, but you promised guarantees at the beginning, and so far we haven't seen any guarantees. I didn't. So, what I said, okay, yeah, I didn't say it explicitly on the slides, but everywhere we can form confidence intervals that are provably valid. So, because we can show asymptotic normality, say in this case, we can say that our confidence intervals probably covered the target with rate 1 minus alpha for any alpha. Yeah. Because we have, like, so the guarantee is that we have a distributional characterization of this estimator, and so then we can say all kinds of things demonstrate. Follow up on that. Follow up on that, is there any sense under which this is like an optimal strategy? Or like is there something like there could be something better to do to get even smaller continuous? Yeah, there are things. So we did also have improvements on this strategy. For example, one thing that you could do better compared to just this baseline that I showed is if your predictions are really bad, imagine that your predictions are just pure noise. That this can actually hurt you, right? If you are just leveraging You are just leveraging right here. So, what we can show is that if you add basically like one tuning parameter on top, in particular you multiply all the terms that depend on the predictions by a hyperparameter lambda, then you can tune that lambda so that, you know, it goes between 0 and 1. 0 means I'm back to standard DRM. 1 means I'm back to this. And I can basically optimally tune that lambda so that I never get hurt by using machine learning predictions. By using machine learning predictions. So that's one sense in which this is optimal. But there are other notions of optimality in semi-parametric statistics that this doesn't satisfy, because I can maybe talk more about that offline. But yeah, there is a notion of optimality that we could shoot for in the setup. Okay, so active inference. Okay, so the setup in active inference is pretty similar to what we had before, only all of our data is unlabeled. Only all of our data is unlabeled at the beginning, and we are still considering the same family of targets. So, solutions to complex problems. Okay, and so, of course, if we don't collect any true labels, I mean, under no assumptions, there is nothing against it, right? We need to have some amount of new data. So, the setup that we're working with here is we're going to assume that we have a budget, and we're going to denote that by MB, and we're going to say what's the best that we can do with this budget and how many labels we can collect. Okay, and so the basic idea is the following. So imagine that someone told you for which data points your model makes a mistake. Then, of course, what you would do is you would just collect labels for those points where your model made a mistake, and then you would know that you have a perfect data set. So that's basically the key principle that we're trying to implement. So we will ask the model for which data points it is most uncertain. Most uncertain, we will collect those labels with a higher probability, and for everything else, we will just use the machine learning predictions. And just at a very high level, to give you a sense of what we mean by model certainty, so imagine that we're in a binary classification setting. Then, if my model predicts something very close to 0.5, then that indicates that my model is very uncertain about a particular data point. If my prediction is very close to 0, very close to 1, my model is very certain, and usually in this Very certain, and usually in those cases, it is also correct. Okay, and so a little bit more formally, we design a sampling rule pi, and we collect each label with probability pi of xi. If the model is uncertain about a data point, this probability will be close to one, otherwise it will be close to zero. Of course, we said that we have a budget on how many labels it can collect, so we're just gonna basically rescale this rules. To basically rescale this rule so that we actually meet the budget constraint. This rescaling just depends on X, so I don't need any information that I don't already have to do this rescaling. And so then this is our estimator. So it's pretty similar to what we saw before, only now we see that we have these kinds of inverse probability weights to take into account that our sampling strategy is now biased. And so now again, if we take the expectation of this loss, we'll get the actual loss that we're. This loss, we'll get the actual loss that we're trying to optimize. So, this loss is again optimized. Yeah, in some order. Are you assuming that if the model is very certain about its prediction, then it's correct? Yes. Yeah, we're not gonna, yes, we're not gonna assume it in the sense that the theoretical guarantees don't rely on that, but our theoretical guarantee. No, no, no, the confidence intervals would. It's not, no, no, no. The confidence intervals will be valid regardless of whether that is true, but we are assuming that in the sense that our results will be more accurate. We're still going to accurately estimate the fluctuation of the estimator, but our results will be more accurate and will have a smaller MSC if this is true. So we are assuming that that's going to be true because that gives us a smaller MSC. And in practice, we... So what happens if it's not true? If it's not true, yeah, if it's not true, you could have a bad sampling rule. A bad sampling rule, and if you have a bad sampling rule, then you will have an estimator that just has a very large MSE. We'll still correctly quantify this MSE in the sense that we will still give you a correct confidence interval, but it might just be like a high variance estimator if this is not satisfying. Accuracy needs it to be correct, or does it just need to be calibrated on like some yeah like in other streams get good accuracy? Is it enough for the sampling rule to be calibrated for example? To be calibrated, for example, like without actually being correct? Yeah, okay, so that's a good question. So maybe we can talk more about that offline because I have a lot of thoughts on this. So we were doing some experiments with LLMs that I'll try to tell you a little about. But basically in those experiments, we observed that these probabilities are not actually calibrated, but somehow they preserve their ranking. And as long as you kind of have the right ranking, you know, if your uncertainty is kind of You know, if your uncertainties kind of encode in terms of ranking, like how accurate your model is, that's enough to somehow post-process those. Yeah, but it's not an assumption, again. It's like, you know, the method is valid even if this is not satisfied, but in practice, we observe that it's satisfied. And so things work well in practice because of this. Yeah. Few minutes. Okay. Okay, so yeah, I'll just say that we also have a sequential version of this algorithm where we Sequential version of this algorithm where we are actually able to fine-tune our model and fine-tune this sampling rule. And so this is just natural because as we collect more data, we should be able to make better decisions in the future. But now the statistical analysis of this becomes a lot more complicated because now everything is dependent. We are fine-tuning this rule based on the data that we've previously seen. So these terms in the sum are not actually IIT anymore. But what's key about this kind of What's key about this kind of structure is that this law still has a Martingale structure, and so this Martingale structure is a very kind of weak form of dependence that still allows us to give pretty precise characterizations of the distribution of the saskinam here. And again, we can still get confidence over this. Okay, so I just want to show you a few examples. So these are some applications that we've been working on in collaboration with computational social scientists. So in this first example, the goal is to estimate the progress. For example, the goal is to estimate the prevalence of a particular political bias in a large corpus of media articles. And the hope is that by doing this, we can just use LLMs to basically augment human annotations and we can get better results. And so this is what we observe in this example. So again, by doing this active strategy, we can get something that's better than PPI, because PPI just doesn't assume that the labels are strategically collected, but everything is just kind of INT. But everything is just kind of IoT. And the PPI baseline is better than the human-only baseline. So, here I can maybe describe offline in a bit more detail what this means, but this is just the effective sample size. It's like if I give you 500 human labels, my results look like I have, let's say, 600 human labels. So it's like the, we're basically like matching the MSC of our estimator to the MSC that we would have gotten. That we would have gotten if we had more human labels. So, this is for left-leaning political bias, for right-leaning. We're getting much less gain compared to PPI, but again, compared to just ignoring any kind of LLMs, we're observing actually the massive gain. And I'm going to try to be short, another application where we try to again estimate the relationship between these certain linguistic devices. Linguistic devices, the perceived stance of global warming, again, in media articles. And so I didn't say these red intervals are the intervals that we would get if we just naively trusted our LLMs. So again, those are very invalid. But then if we use the strategies that I described, we can get something that's valid. And again, if we actively sample data, we can gain quite a bit. Okay, so that's all I wanted to talk about. I'll just say we do have a package that we've Just say, we do have a package that we've been kind of developing actively, and it's very well documented, I think. So, if any of this sounds interesting, please do take a look. So, with that, thank you. Sorry to take away from the coffee book. Yeah, we'll come back at let's come back at 10:30. 