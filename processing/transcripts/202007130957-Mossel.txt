So we're back for the third, well, for the nth week of the OOP School, and for the third week that was organized that the speakers had been originally planned for the Seminar de Mathématiques Supérieure. This week we're going to have two sets of lectures. The first set of lectures will be by Archenan Mosel. will be by Algernon Mosel. We're going to start with that today. We're going to have a lecture by him today, tomorrow, and on Wednesday at this time. And there's going to be two exercise sessions. One is tomorrow after the lecture, half an hour after the first lecture. And then the second one is on Thursday, half an hour after the first lecture. And then the second course of lectures is going to be by Shuishendoganguly. Shendo Ganguly. And those lectures will start on Wednesday after the first half an hour after the first lecture and then continue on Thursday and Friday at this time. So to remind you, Jess, the lectures are going to be recorded. So if you do not wish to be recorded, please keep your video and audio off. The abstract, the slides, and other The slides and other materials for the lectures are going to be available on the OOPS website. You are welcome to ask questions during the talk in the chat. We will try to moderate these questions and pose them to the speaker. And also, you will have the opportunity at the end of the talk when we stop the recording to freely ask questions in person or over the chat as you wish, because we will allow the Because we will allow the participants to unmute themselves. Today we're going to start with the first lecture by Elhanan Mosel. Elhanan got his PhD from Hebrew University in Jerusalem. He then went on to work at Microsoft Research for a couple of years in the theory group. Then he was a professor at Berkeley. Berkeley followed by a couple of years at Weizmann, then a few years at the Wharton School at UPenn, and is currently a professor jointly with the statistics and the data science center at the at MIT. Alhanan, please. Yeah, thank you very much for the introduction. Thank you very much for inviting me. In these crazy times, it's In these crazy times, I guess it brings a little bit of normal life when you talk to people about mass and not just about the pandemic, which we just did until a minute ago. So I'm going to talk about this topic, simplicity and complexity of belief propagation. It's a topic that I worked on from essentially the beginning of my PhD. And I think I'm going to talk about I think I'm going to talk about older stuff today. I think most of the stuff today is stuff that some of you might have seen in some of my talks up to like maybe five years ago. And maybe newer stuff in the next talk, and maybe newer, newer stuff in the last talk. So I don't have a very specific objective of getting to any particular point at any lecture. So and I'm pretty flexible also in terms of the partition of the lectures. So please feel free to ask questions. Please feel free to ask questions. I'm not sure I'll be able to see your questions, so I'd appreciate help from one of the organizers. If you see that somebody asks a question that's urgent, or you know, if I'm asking, are there any questions? You can tell me, yes, there are 25 questions. Please look at the chat box. Okay, so I'm going to start. I hope everybody sees my slide. I'm going to talk about belief propagation. And I'm actually going to start from sort of not a probability perspective, but more of a machine learning perspective. But more of a machine learning perspective, but it's just going to be for a few minutes. So please hang on and stay with me. Okay, so we are going to consider a very simple and classical generalization of Markov chains. It's called the Markov chains on trees or Markov random fields on trees. And I'm going to look at the very, very special case that I'm going to describe right now. Some some at some later points I may talk about generalization of this even in this lecture, but Generalization of this, even in this lecture, but for concreteness, let's just consider this a very simple process. So I'm drawing my trees, the trees are drawing from top down. So this is the root here. And in this small tree, these are the leaves. And maybe I should write final. So these are the leaves, and we are going to consider the following process. So here's what we're going to do. We're going to call out the root randomly. So randomly. Randomly, so randomly, maybe in these slides, I mean uniformly at random in one of Q-passable colors. And then we're going to repeat the following very simple Markov chain process. So for each child independently, we are going to copy the color of the parent. So this is going to be copied from here with probability theta. Theta is some number between zero and one. And otherwise, if we haven't copied, we are going to choose a color uniformly accountant. So we are going to do it for this guy, then for this guy, then for this guy, then for this guy. This guy, then for this guy, then for this guy, then we're going to repeat. Maybe in the next level, sometimes we toss a coin, maybe we toss the coin here, but the coin turns out to be red, so it's still red. And we definitely toss the coin here and here. And in these two guys, we got the blue. Okay. So I think in this lecture, I'm actually going to talk about full DRE tree. So let me just write what is a full DRE tree. So full DRE tree. So we are going to have a root here. So, we are going to have a root here. The root is going to have D children. Then, each of the children is going to have D children. And whatever I'm going to talk about, I mean, in probability, we often like infinite systems. So we are going to take an asymptotic approach, but the trees are all going to be finite. So this is going to go down for some fixed number of levels, H levels. Okay, so that's the basic model that I'm looking at. Any questions so far? Okay, good. Okay, good. So let's be, you know, undergrads in machine in the data science and machine learning program at Iron University, and then let's ask a question that probabilists will find ridiculous. You know, we'll ask, you know, can we infer the color of the wood from the color of the leaves? So let's just ask these questions very naively. So the first very naive answer is: you know, if theta is less than one, then we You know, if theta is less than one, then no, right? Even if I have this very simple tree here, and I tell you that this guy is blue, you cannot know for sure if this is blue or red. So, infer in the sense of saying, making statement is probability one, obviously we cannot make. So, maybe a more refined question is how much can we infer about the root from the color of the leaves? You know, what can we say about the colour of the root from the color of the leaves? And of course, you know, now it's a more interesting question. Still, for the question. Still from the machine learning perspective, and this is where I'm going to, this is the last slide I'm going to talk about machine learning, is that the answer is trivial. The answer is trivial because it's very easy to compute the posterior exactly. So if I give you any tree, so let me draw a cartoon of this tree, it's a very big tree, and I have blue, red, red, blue, red. You know, I have a bunch of color here. I want to compute the posterior probability that the root is either blue or red. Again, I'm only seeing the color at the leaves. I'm not seeing any other colors. Any other colors, there's an algorithm that's very simple, algorithms called belief propagation. You know, we would call it a recursion. There's a very simple algorithm or recursion called belief propagation that will tell you exactly that, you know, given this configuration of the, given this configuration of the leaves, which we are going to call later maybe x of h, it will tell us what's the probability distribution of the root given x of h. Oops, sorry. The wonders of the iPad, they did something wrong. They did something wrong. Okay, so that's the machine learning answer. So it's a very, algorithmically, this is not a very interesting question. And for trees, people have studied beef propagation in other settings for other graphs. And these simple recursions have been studied before in biology and in statistical physics. The earliest references I know are from the 1970s. References I know are from the 1970s. Any questions so far? Okay, good. Okay, so today I want to talk just about, you know, I'm going to sort of divide the lectures into three parts. So maybe I should tell you what the three parts are. So part one is going to be the linear theory. Part two is going to be the non-linear theory, obviously. Obviously, so this will be tomorrow, and non-linear theory would correspond to q, say, greater or equal than five or greater than four. So, somehow the question if the theory of this process is linear or not somehow depends on the number of colors. So, if q is equal to two, then it's going to be linear theory. And today we're only going to talk about linear theory, and tomorrow we'll talk about non-linear theory. So, we'll talk about q greater or equal than five and other situations, and then and then. Other situations, and then and then the last lecture will maybe I'll say what the last lecture. So about three, yeah, there is a question in the chat about graphs that are not trees. Okay, so I'll say just what, so what is the question? Can you tell me what the question is? For non-tree graphs, how to choose in what order the colours propagate? Very good. So I'll answer this question in a second. Thank you very much. So part three is about complexity. Complexity and simplicity. So, this is the, you know, after we will understand this recursion, you know, better, we will try to understand if they're complex or simple, and we'll do it in some rigorous sense. So, of course, in general, when people apply belief propagation, what they do, there is a different procedure for sampling the colors, right? This would be some Gibbs measure or Markov random field of the graph. And the hack, if you want, or the approximation that people in machine learning. The approximation that people in machine learning are performing is to say, well, we'll just forget that the graph is not a tree and we'll assume it's a tree. So I didn't exactly say what it means, but if you formally write the set of recursions that you have to carry out to do belief propagation, it actually gives you an algorithm that says, well, right now I think that your color is blah, blah, blah with this probability and color blah blah blue with this probability. And you know, you process this information from all of your neighbors and you send information to the other neighbors. So this you can carry up. So, this you can carry out, these algorithms you can carry out on graphs that are not trees, even though probabilistically these updates are correct only if the graphs are trees. And people actually do this. And people have done it for a very long time, at least from the 80s. And there is very interesting work in our community, which shows in what situation, which shows at least some situation in which this is rigorously sensible to do that. Sensible to do that, right? So I don't think we'll talk about that at all, but I mean, this relates to decay-of-correlation on Gibbs measures or Markov van dom fields. You know, usually this belief propagation updates make sense in our setup, in the probabilistic setup, in situations where the graphs have high girls, right? So that's the situation that we could look at. Good. Other questions? Yeah, so Linden is asking about Q equals 3 or 4. Right, so we can. three or four right so we when when i talk about i mean when i talk about lecture i mean so yeah so we'll talk about this so q equals three and four is actually the linear theory uh at least q is equals three is the linear theory q equals four is a little more delicate it's sort of the phase transition if you want but we don't know everything that we want to know about q equals three and for simplicity i'll just going to talk about q equals two right because i want to tell you what the main results are so q equals two is definitely linear theory q equals three everybody believes is linear theory q equals four is Linear theory, Q equal 4 is tricky. It depends if it's ferromagnetic or anti-ferromagnetic. You know, there's some results that we know, there's a lot that we do not know. And Q equals greater or equal than 5 is definitely non-linear theory. But we will get, when I tell you what we know, you'll see what I mean by that. Okay, good. So let's start. Okay, so now we want to do mass. Okay, so let's start from the very easy mass. How do we make it a mass? So in this lecture today, unless I say otherwise, which You know, unless I say otherwise, which I don't think I will, I'm going to fix the number of colors to be two. There are only two colors. We're going to fix the DI3, so every node is D children, and they're going to be H levels. And for mathematical simplicity, you know, it's harder to sum the colors blue and green or blue and red. I'm going to call the two colors plus one and minus one. Okay, and let's give some random variables name. So we're going to call xv is going to be the color of node v, and x0 is going to be the color of the root, right? Not is going to be the color of the root, right? So, you know, here's a picture of my tree. This node is going to be called not or zero, and this is going to be x0, it's going to be plus or minus one, it's a random variable. And then this is level h. Here I have a bunch of colors. This vector I'm going to call xh. Okay, this is the vector of colors at level h. Okay, so these are the random variables that we're looking at. Okay, so we want to ask the following question, how much can we infer uh on the root color from the leaf colors? On the root color from the leaf colors, and let's look at two variants of this question. Both of them we will address, at least in the linear case. And the first question is: can we analyze the optimal estimate or can we analyze BP? So BP will actually give me what is the posterior. There is an algorithm that computes the actual posterior. And we can ask, can we just analyze, you know, how does the posterior behave? And then we can say how much information do we have on the rules. So notice that BP takes random variables. So maybe I should write it. Variables, so maybe I should write it BP is a random variable that depends on level H, right? So BP of X, it's a random variable that takes this vector random variable X of H, and it's a random variable, you know, BP doesn't have any other randomness except applying some deterministic function for X of H. So deterministic function of this vector X of H, we want to understand this deterministic function. How does it behave? In particular, we want to understand, is it very close to the constant one half, one half? If it's very close to the constant one half, one half. Very close to the constant one-half, one-half, it means that we cannot say much about the root, or does it have variance? If it has variance, it means that sometimes we can say something about the value of the root, right? So, again, x of h is a random vector, bp of x of h is a deterministic function of this vector, which is by the fact that it's a function of a random vector is a random variable. Want to understand is this random variable asymptotically essentially constant or does it have variance? So, that's one way we can ask this question. Another way we can ask this question, which is maybe the naive way. Ask this question, which is maybe the naive way that you would think about. I mean, maybe this talk is recorded, I shouldn't joke about it. But suppose sometimes break into your house at night and say, I'm going to steal your favorite toy unless you're going to tell me what is the color of the root, given that these are the color of the leaves, right? So, you know, that's, and you know, you're waking in the middle of the night, you haven't heard about this problem before, said, okay, okay, what are the color of the leaves? What will you do? You know, you will probably say, okay, how many reds are there in the leaves? Say okay, how many reds are there in the leaves? How many blue are there in the leaves? Okay, if there are more blue than red, I will say blue. There are more red than blue, I'm going to say red. So, this is the majority estimate. Also, let's see what it is. Formally, I sum so sorry, so all the plus a question in the chat about the about the BP output space. So, so does BP output a plus or minus, or does it output the probability? Very good. So, BP outputs random. Outputs a random variable, and the random variable takes a random vector in our case with two coordinates, the sum to one. It's the probability, so let me write it down. Output of BP is the probability that x0 is equal to plus given xh. Okay, if you want, and the probability that x0 is equal to minus, but of course, the sum of these probabilities is one, so that's that's what we get, right? So that's the output of BP of xh. It's just the conditional probability. Of Bp of X H, just the conditional probability that the root is plus V when X of H. Excellent question. Okay, so maybe this is something that we want to do if we are sophisticated globally, but maybe if somebody wakes you up in the middle of the night and you forget your base rule and you don't want your favorite toy to be stolen, you say, well, what is the configuration of the leaves? I'm just going to take the majority. Is this a good estimate? So why is this a reasonable question? Why is this a reasonable question to ask? So, by Bayes law, we know that the best estimator is BP. I mean, this minimize the base risk or whatever you want to call it in statistical terms. So, that's what we should use. But maybe there's something that's easier to analyze and easier to understand, which is just to take a majority vote on the leaves. And that's what already gives you a good performance. Maybe this is easier to analyze. And historically, I should note that question two was addressed before question one to some extent. So, that's another reason we will study it in this. We will start with this. Other questions? Okay, good. So there's a lot of background here, and I'm going to do a very bad job in giving all the references and all the connections. I mean, this question emerged from statistical physics. We'll see some connection to computational evolutionary biology today. Evolutionary biology today, there's some connection to graph partitioning. There are a lot of connections here. I'm not going to give all the references or even a good sample of the references, but just historically, this question definitely, you know, the first time it was asked, the question, you know, in very different words or very different terminology, was in terms of externality of the free measure of the Ising or Potts model on the infinite beta lattice. And the connection is, you know, shouldn't be completely obvious to you if you haven't seen this problem before, but it's one. You, if you haven't seen this problem before, but it's one of these questions that was asked in statistical physics, and then you know, and actually was the first answer in statistical physics. But then people in other communities started to take a look at it too. And I'm giving you a modern view of this question. And I don't really want to tell you what's the infinite Gibbs measure and what's the specification and what's the free measure. I mean, you can read about all of that, but you know, I think it's not crucial for the main ideas I'm going to present. Okay, so let's start to do a little bit of math. So I was asked to give you exercises, so I decided that everything that I need to compute, you will do as an exercise. And even things that are a little more complicated than computing, I'll ask you to do an exercises. And let's try to understand the majority estimate, right? So again, I'm going to draw this picture again. So we have our trees. These are x naught, and here is xh. And I want to know if this is a good xh, if sh, which is the sum of. If SH, which is the sum of all the indices of XH, the vector of XH, is a good estimator of X naught only. Okay, so let's start to do it in the undergraduate level of sophistication. So what do I do if I'm an undergrad and I ask you, is SH a good estimator of X naught? Here's what you're going to do. You're going to say, well, if I'm an undergrad, the first thing I'm going to do is I'm going to compute the expected value of SH condition on X naught. Okay, and your exercise is to show that the Okay, and your exercise is to show that the expected value of SH given X0 is in fact d theta to the H times X naught. Okay, so this part of the exercise I'll even do for you because it's so easy. I can even do it online. But I also, you know, if you're a little more sophisticated, I'll also ask you to compute the variance. So there's a formula to the variance that I didn't write down. So you can also compute the variance of this. Okay, so I'll ask you to compute the expected value and the variance. Value in the variance. And before maybe I give you a hint on how to do the computation of the expected value, let's see why it is interesting, right? Because what we know from basic statistic or engineering is what we're interested in the ratio between the signal. So the signal is d theta to the h, because this is what multiplied the original random variable that we're after. That's the signal. So here we have the signal square, d theta to the 2h, divided by the variance, right? So if the signal is strong. Variance, right? So, if the signal is stronger, is as strong as the noise, we are happy. And if the signal is much weaker than the noise, then we are sort of unhappy. And what you will figure out if you look at this calculation in the exercise is that the signal is stronger than the noise. So this is C theta that is strictly bigger than zero if d theta square is greater than one, and it's zero if d theta square is less than one. Okay, so just for a sanity check, we would not. Just for a sanity check, we would not expect somehow intuitively the ratio of the signal to the noise to be infinity because we know that even after one level there's some randomness. We cannot really recover the color of the root. So it's not the situation that the signal is much, much stronger than the noise, that the ratio between these two is going to infinity. We expect that even in the good case where we can say something, all these limits are h goes to infinity, the best that we can offer is the ratio is a positive number. Okay, so maybe let me give you Okay, so maybe let me give you a quick hint about how you do the first expectation. So, you know, here is my root, and I want to compute the expected value of SH given the root. So, you know, obviously I'm going to use linearity of expectation. So I just have to look at one particular guy. And I'm going to ask, what's the probability that this particular, what's the expected value of this particular guy? So we have to see, if we think about it in terms of the process, there are H times where I'm doing my copying process, and I copy with probability theta, and otherwise I randomize. probability theta and otherwise I randomize. So if I copied all the time, which will happen with probability theta a to the h, I'm going to be identical to x naught. And if I haven't copied even one time, I'm going to be independent of x naught. And my expectation is going to be zero. So the expected value of this particular xv at level h given x naught is going to be theta to the h okay, so that's that's the reason that when we have d to the h lives, so the expected value total condition. The total condition expectation is going to be detailed to the h-time, x0. I mean, if it was a little too fast for you, you can try to do this logic on your own. And similar computations let you do the second moment, right? Because if you do the second moment, you're going to expand this. You're going to look at this square. It's going to be this square, and then you're going to ask yourself, oh, okay, what is xhi, xh, j? We say about xhi, xhj, it's not always going to be the same thing because if i and j are here, then the correlation between them is going to be strong, but if they're farther away, To be strong, but if they're farther away, the correlation between them is going to be weak. But with the same kind of logic, you can compute that. Okay, this is probably easy for some of you and too fast for others, but you know, that's one exercise you can try to sort of get a phenomenal problem. So I want to actually carry, I mean, so with a little bit more work, this actually implies with a little bit more work. So let me add some work here, which I'm not going to do. I'm not going to do with a little bit of work this for both of these implications. It implies that actually, in the situation where d theta squared is bigger than one, the total variation distance between the law of sh given that the root is plus and the law of sh given that the root is minus is bounded away from zero as h goes to infinity. So, you know, the two laws actually behave differently. And if you know a little bit about statistics or inference, this means that if you are given SH, you can say something normal. Are given sh you can say something non-trivial about xh about x0. Your posterior is not going to be always very close to one-half, one-half, and in fact, you know, with the same amount of work or slightly more work, you can actually show that there is actually correlation between just the majority estimate or the sign of SH and X0. So, if somebody would wake you up in the middle of the night, and if you're lucky and detailed square is greater than one, and your guess is to say that the color of the root is the sun. That the color of the root is the sine of SH, you are not doing the optimal thing, but no matter how big H is, you are doing something that's non-trivially better than random, right? So you're doing something that has really better chance than 50-50. Okay? Okay, so this is a calculations that people can could have done a hundred years ago. And of course, people have done much more. So I'll tell you a little bit of the much more that that they did. So a very influential paper. So a very influential paper in the 60s that was not influential about like maybe the 2000s is that this paper by Kesten and Stigman from 66. And this paper of Kesten and Stiegon did much more, much more in a much more general setup than the setup that I'm talking about right here. You know, there are two beautiful paper, very important paper of Keston and Stigman from the 60s. These are hard papers, as you'd expect from Keston. But what they did is they looked at SS. But what they did is they looked at SH and they actually analyzed the Fourier transform of SH, and they derived much more than what we said before. They derived the following phase transition, if you want. If d theta squared is less or equal than one, in the situation where our heuristic indicated that there's no information about the root, they show that in fact, if you normalize sh in the right way, which I won't write right now, then what you get is you get a normal law, and this normal law is independent of. And this normal law is independent of x norm. Okay, so you look at sh, this is some random variable, it's going to be very big, you normalize it. If you look at the right normalization, you get a non-degenerate normal random variable. This non-degenerate normal random variable is just going to be normal 0, 1, and it has nothing to do with the value of the rule. And moreover, they prove that if d theta squared is bigger than 1, then after you normalize xh in the right way, you get a non-normal law and the distribution. Normal law and the distribution that you get, you get one distribution if the root is plus one and one distribution, a different distribution or the middle of this distribution, which is different when x0 is plus one. Okay, so why do I have a little star here? So this little star is annoyance for people who work in this area. You would want to deduce from this detailed square less or equal than one that you cannot say anything about the root when you're About the root when you're given the H level, but let me just explain to you why this is not the case. So, there are two reasons why this is not the case. The first reason is that all that it tells you is that SH after normalization doesn't tell you anything about the root, but it could in principle be that if you look at SH mod 3, it tells you something about the root. The Kesten-Stigun theorem does not exclude that. And more importantly, what SH does, it symmetries the location of all of the variables. The location of all of the variables, and it's not clear that you're allowed to do the simulations. And we talk about when we talk about the non-linear theory, we will see that, in fact, you cannot, right? You cannot assume that SH contains all of the information that you have in XH. So, SH is the sum of the guys in XH. You cannot assume that it contains all of this information. And surprisingly, 30 or 40 years after Kesten and Stigmat, when people realized this. But when people realize this session is very important in a bunch of areas, including what I'm going to talk about now, but also in constraint satisfaction problems and a lot of other problems, you know, the name, this threshold, d theta square is equal to one, is referred to as the Kestenstigon threshold. This is threshold in terms of theta, in terms of d and theta, of where you have some sort of information decay between the root and the sum of the guys at the loop. Okay, so one more exercise. So Kestan and Stigman did not do it this way. I think at the time there was no version of the Martingal CLT. But in fact, it's not hard to prove at least this part of the theorem. It's not hard to prove just using the Martingal central limit theorem. So I'll just give you a hint about how you do it with the Martingale limit theorem. So what is the Martingale? So, what is the mounting? So, there's x naught here. This is going to be plus or minus one t a half. So, this you know is going to be a random variable that I like. Now, when I look at the first child xv, what I'm going to look at is I'm not going to look at xv, I'm going to look at xv minus the conditional expectation of xv given the parent x0. Okay, so this means that I'm going to get a mean zero random variable. Then, you know, there's something to check, but you you sum all of this mean zero random variable. sum all of this mean zero random variable you check that you can apply the so you know so the the the increments are all all have covariance zero to each other so that makes you happy and then you have to check that the variances do not explode and you check again that the variances do not explode exactly when d theta square is less than one actually d theta square applied and you know it's a little hard it's slightly harder to do it in the case that of the inequality so let's Of the inequality, so let's just do it in the case that detailed score is less than master score. Questions? Okay, good. So we got to the 1960s. Let's get to the 1990s. Okay, so in the 1990s, what was proved in this linear case, in the case that Q is equal to 2, is, and for me, this is very exciting because I got this very. Exciting because I got to this area in the late 90s. So we get to the result that if data squared is less or equal than one, it's not only the case that our naive calculation told us maybe there's nothing we can do. It's not only that the cast and sigma tell us that if we look at the sum of everything, it's a normal, that whose behavior is independent of the root. It's actually the case that if you look at the optimal estimator BP, so this is again this random variable, it Is again this random variable, it converges. Okay, I have to tell you exactly in what sense it converges. So, but the probability, you know, for example, you can say that in many senses, but the probability that BPXH minus the vector one-half, one-half in whatever norm you want. So, this is some norm, is greater than epsilon. This goes to zero as h goes to infinity. Okay, so this random variable BPXH, which tells you. Which tells you the posterior of the root given the leaf. This random variable is going to be very close to one-half, one-half. Okay, so this means, in some sense, there's no information in the root about the leaves. Questions about this? Okay. So, I mean, obviously, the conclusion. So, I mean, obviously, the conclusion from that given that we've seen is that, you know, the person who woke you up in the middle of the night, maybe you use the majority estimate, but, you know, you did non-trivially if and only if I did non-trivially. Well, I computed the BP, you know, I did these recussions in my head and got my numbers. I mean, I actually did better than you. The probability that I would be correct is bigger than the probability that you you would be correct. But if you if you did better than random as h goes to infinity, I did better than random and vice versa. Random and vice versa. Okay, so I mean, here I just decided to give you a bunch of proofs because I'm going to actually try to give a proof of this fact in the next maybe 10 minutes. So the original proof is again for statistical physics is this paper by Blecher Zach Ebnov. Then there was a nice proof by Diema Yaffe. There was a proof by Edmunds, Kenyon, Perseus, and Schulman. And I'm going to give you. And I'm going to give you a to follow one of the older proof that was actually in the case of string glasses on trees, which I didn't define, but it's closely related to what we're talking about. This proof by Chase, Chase, Setna, and Soles, which actually we did to somewhat more general setup in this paper by Boggs, Chase, and Sebastian Wolf. But there was a bunch of beautiful proofs of this, and I'll give you one. Of this, and I'll give you one proof. One of the things that this proof does, it actually writes the recursion for belief propagation. So, I didn't tell you what is the recursion, and I will see this recursion of random variables, and you'll see why it's a little bit complex, maybe. Questions? Okay, so this is going to be the most technical proof of today. Sorry, before you go on, there was a question about whether they predict equally well the B. They predict equally well the BP and majority in the limit, or is there a gap? There is, yeah, so I said it, I said it a few times, I'll say it again. There is a for every, whenever detail score is bigger than one, the probability in the limit that the BP is accurate is strictly bigger than the probability the majority is accurate. Okay, so BP is always better than the majority, and that's, I mean, that's actually an easy exercise, so I'll add it as an exercise. I think it's a pretty I think it's a pretty easy exercise. Exercise. Show that if t theta squared is greater than one, the limit as h go to infinity, the probability. Okay, so now I like something. I hope that you won't be mad for me. The sine of BP, okay, right? You have to round BP. BP doesn't actually just give you posterior, so you have to round it to the closest number. So the sine of BP is equal to x0 is strictly bigger than the proof. Is strictly bigger than the probability than the same limit, the probability that majority is equal to x0. Okay, so BP always does strictly better. Okay, that's a very good question. And in fact, in used in relatedly, you know, this fact, you know, for people who ask more refined questions, which I guess we won't go to in this class, this is an important fact. Excellent. Excellent question. Other questions? Okay? So, all of, yeah, so I mean, maybe some features of other proof, this proof by Evans, Kennedy, Paris, and Schumann and a very nice feature that it works for generalities in terms of something that's called the branching number. And all of this proof uses some concavity of some functionalism, but we will do it in the most direct form in terms of the actual curve. Okay, so let's let's show let's see the proof. Okay, so here's the proof. I mean, so I mean, everything is pretty simple. I'll give you some exercise. All of the exercises are either underguard probability or underguard probability where you have to apply basal. Why do I put them in separate categories? Because basal is confusing sometimes, right? But you know, it's all something that you can do with your underguard probability. So let me tell you how the proof goes. Okay, so. Okay, so we're going to write pt plus for the probability measure. We're also going to write this as an operator of xh. Remember, xh are the leaves at level h when the root is plus. We're going to write pt for the measure that is actually of interest where we take this probability half the root to be plus and the root to be minus. So pt minus is the same as a minus. Okay, now the quantity that we're going to be interested, I'm calling, going to call m m stands for call M stands for M stands for magnetization this is again comes from the fact that there's an easing model here and the magnetization is going to be the following is going to be the probability given this x h if you want to think about the mass boundary conditions that the root is plus minus the probability that the root is minus given this this boundary conditions right so that's the if you want the magnetization of the root that's the random variable that if it's very close to zero then you That if it's very close to zero, then it means that we don't really learn anything about the root, and if it's bigger than zero, then we in absolute value, then we're happy. If it's far away from zero, then we're happy. Okay, so here's the base rule that I won't do for you. Maybe depending, yeah, I don't know how much rule I do. I want to cover a bunch of other stuff. We'll see. But what's nice is that M actually tells us about the Radoni-Coudim derivative, which is very simple in this case. Everything is discrete between this measure Pt plus or minus two. measure Pt plus or minus to the average measure Pt. And in fact, the derivative, the Nicodemian derivative is exact one plus m if you're looking at Pt plus and one minus m if you're looking at Pt minus. And a nice consequence of that, which is pretty amazing, is this following equality, which if you see for the first time you don't believe, but E t according to the plus measure of M is actually equal to E t of M square. Okay? And it's actually also equal to E T plus. And it's actually also equal to et plus of m square. I mean, this is less, you know, this equality is less surprising, it has to do with the symmetry of the plus and the minus. But this equality is very useful, and of course, it comes very directly from this Nicodemic derivative, right? So I write ET plus. I'm only going to do easy calculation, right? So what is this? So this is exactly ET or E. Yeah, I don't know why I call it T instead of P, you can think about this T of P. You can think about this T of P. So, this is E t of m times 1 plus M, and the expected value of M is zero because it's symmetric, so we just get E t of m squared. Okay, so this is very trivial from what we have, but it's very useful the fact that you can compute second moment by taking first moment, right? That's a very useful feature of this model. Okay, so now we have to understand. I mean, and this is where we'll actually do something like the analysis of we will. Like the analysis, we get the first thing of what recursion do you write when you do believe propagation. We are going to look at the following situation, which I wrote syntactically in this terrible name, but let me see what I mean by that. So, here is my root zero, and my tree is going to consist of the tree S connected by a single edge to zero. Okay, so I have an here I have an edge tree of some H levels. This is a big tree. Here I have zero and here I have a single edge. Here I have zero, and here I have a single edge. Now, I want to understand the connection between the Banjo-Randall variable. I want to understand M. That's going to be the magnetization at this vertex, and how it's relate to N, which is the magnetization of this little root over here. Okay, I want to see how they relate. Is the picture clear? I guess the picture will disappear in the next slide where I tell you what I'm actually claiming. So, Linden is. So uh Lindel is asking to clarify the connection between the radonic derivative and base. So so right, so uh this is your exercise, right? So you have, I mean, when I'm telling you P T plus, I'm giving you some additional conditioning. I'm telling you the root is actually plus, right? When I'm telling you it's minus, I'm giving you some conditioning, right? So you have to apply base, you somehow have to, and in M, I'm actually asking an opposite question. I'm asking you, what's the probability that the root is plus versus? What's the probability that the root is plus versus minus? So, in one place, I'm conditioning in this way, and the other one I want to condition in the other way. So, it sounds like a base volume exercise is to figure it out. It's a good question. This is why I put it as an exercise. Other questions? Okay, so this is the configuration that we have here. We have the root of this tree. This tree is very simple, it's not just connected by a single vertex to this tree. The root of this tree has the random variable n as the magnetization of this tree. As the magnetization of these three. Now, let's see what is the relationship between M and M. So there's a bunch of relationships. Maybe I won't talk so much about the last one, but M is actually, maybe I'll draw the picture again. This is the picture. We have M here and I have N here. M is actually equal to theta n. So this is very nice in terms of computing recursion. Suppose I'm a computer programmer and I have something that computes the random variable n for here. I want to compute the variable variable n for here. It's just theta time the other random variable. And this sort of makes sense because whatever you tell me here, I'm going to believe the same thing here, but this probability and theta are going to be equal, the root and the guy below it. Why minus theta are going to be independent? So the strength of how much I'm going to believe in what I believed before is going to be theta times what I believed before. There's another identity which also has a theta, and it's a different theta, and that's a little bit confusing in this setup. If I compute the expected value where the root is plus hill, Is plus here, and I compute the expected value of n of this guy here, then it's theta times the expected value as if I computed it here. And this also makes sense. Why does it make sense? Because if I have a plus zero, it's probability theta is going to be copied and otherwise it's going to be one half, one half, and then the expected value is going to be zero, right? But these two thetas are different. And in fact, it's the product of the first theta times the second theta that corresponds to this detail. theta that corresponds to this d theta squared that we see in this formula for the casting-stigma because we do both we do compute something in terms of we compute m in terms of m and we compute m in terms of n. m is equal in theta n but we also compute with respect to a different measure we compute with respect to the measure at the root of the big tree and not with the respect to the root of the small tree so this gives us another theta and this is what's going to give us the theta square okay so i want to prove this claim i mean this Okay, so I want to prove this claim. I mean, this claim is between 30, and similarly, a very easy formula for et plus of n squared. Okay, either you copy this probability theta, if you haven't copied, it's random, and you just get the expected value according to n squared. The more interesting claim, which obviously I do without enough space to do it, is what happens when you look at two different trees, T1, and I'm going to draw T2 in the opposite direction, and I'm going to combine the information from what I hear. Bind information from what I have here and what I have here. So there are two trees that had separate roots, and then I'm going to glue the roots together and I'm going to ask what's the new random variable of magnetization going to be. And this is again another application of Bayes' rule. I meant this to be an exercise. So this is an exercise. And this is also an exercise. Because all these are exercises. This is another application of basal, and the baseline tells you the following. It tells you, well, It tells you: well, the magnetization at the big tree is the sum of the original magnetization divided by one plus the product of the magnetization. So, you have to check that that's what Bayes rule gives you, but that's what it gives. And then finally, if you combine the two things that we've seen, so you know, what you really do in practice is I have two trees on maybe n minus one leaves, and I have one edge coming from each of them that's going to connect to a root. Then that's going to connect to a root. So if I look at the belief propagation recursion when d is equal to 2 for binary trees, not just q is equal to 2, d is equal to 2, then what I'm going to see is I'm going to see this recursion over here, when instead of n1, I'm going to have theta times the previous guy, instead of n2, I'm going to have theta times the previous guy, and I'm going to get this recursion. The random variable at level n plus one is theta times the random variable coming from one subtree plus the random variable coming from the Some tree plus the random variable coming from the other subtree divided by one plus theta squared, the product of the two random variables coming from these two trees, mn and m n prime are not independent, right? There's some correlation between them because they're all coming from the same big tree. And what it means, like if you're a computer programmer who's programming belief propagation, said, oh, here is for this particular realization of mn, you know, a particular realization of the colors, I got the root of this up to the value of the magnetization. Is this value, the value of the magnetization? Is this value? The value of the magnetization is this value. What is the belief propagation value of this? It's going to be this non-linear function of the tornado variables that are computed for each other. Questions? Okay. As expected, I'm covering about one half of the slide I was planning to cover. But I promised you before that it doesn't bother me, so I'm trying to remember that. Trying to remember that. Questions? Good. Okay, so why is this helpful for us to do this now that we understand this recursion? So from now on, it's the kind of things that we all like to do, maybe when we're studying probability, maybe also all the way that when we do probability, to say, oh, now I have a recursion of one of the variable. Let's see that I want to analyze it. So actually, what do I want to do when I analyze it? What I want to do is what's written over here in the title. What I want to do is say, to show that if two title squared is less than one, then the Is less than one, then the magnetization random variable is going to zero. So, one way of showing it's going to zero is going to show that its second moment is going to zero. This is definitely going to say that with high probability, it's less than epsilon. So, that's what we want to show. So, now this is just an analysis of one-to-variable. I tell you how do you do this analysis. So, we have this recursion m is equal to theta n1 plus n2 da da da da da. Then I'm going to use this fascinating algebraic relation. 1 over 1 plus r is equal to 1 minus r plus r square over 1 plus r. is r plus r squared over one plus r i'm going to apply this when r is this beautiful expression written over here theta squared n1 n2 okay so i'm going to apply this recursion and what am i going to get i'm going to get the following recursion where somehow the ratio disappeared why did the ratio disappear because when i look at this expression r square over one plus r part of this expression is going to be m okay so i'm going to get the following regression you can check it algebraically m is equal to theta and Algebraically, m is equal to theta n data minus theta cube. The main term is theta n1 plus n2. Then there's higher order terms, and then there's a term that contains m on the other side. And then you just very naively apply the bound that m in absolute value is less than one. It's a magnetization, it's a conditional, it's a difference between two conditional probabilities. And you get that m is less or equal than theta, but n1 plus n2. This expression is n1 and two. Again, these are the expressions that are coming from these two different trees. Expressions that are coming from these two different trees. I have M here, I have two edges here, I have N1 here, and I have N2 here. Okay, so I get this recursion here, and now I have to analyze it, I have to compute moments. Remember, what I want to do is I want to compute the expected value of the square of another variable, then I'm going to use this magical property that the expected value of the square is actually just the expected value, and therefore I can apply linearity when I condition on the root being plus. root being plus. So I'm going to take look at this expectation when the root is equal to plus. So I'm going to look at expected value with the plus on the root of m. And then I'm going to claim, I'm going to get theta times expected value when the root is plus of n1. Remember in the previous slide I said that if the root, when I go from the root of n to the root of n, there's another factor of theta. So I will get theta square, the expected value according to the subtree of n one and then of n two. And these two expectations are One and then of n2, and these two expectations are going to be the same. So, this is going to give me this term. And you can also control all the other terms. So, why can you control the other terms? It seems like I'm running out of time, so I'll just tell you. So, the reason you can compute all the other terms is that once I condition on the root being plus, what I have on the right sub-tree and what I have on the left sub-tree are independent. So, when I compute the expected value of this guy, it's the expected value of the thing that comes from the n, and the expect n1 and the expected value of what comes from n. n1 and the expected value of what comes from n2. So this this term times that term. What's the other nice fact that comes into this? The other nice fact is that I know that the expected value, right? Remember this amazing fact that expected value s plus of n1 is in fact equal to the expected value of s plus of n1 squared. And here since I have n1 squared here and in one of the terms I just have a term that's linear in n1. These two terms are actually going to be the same. One will have a teta q. One will have a theta cubed, one will have a theta to the fourth. I do this computation a little bit fast, but what you get is that the expected value of m squared is less or equal than the expected value at the level before, minus a positive quantity, minus theta to the four, the expected value of m plus. Okay, so you do this recursion, you do this computation, and then you're done. Why are you done? Because when you apply it recursively for the binary tree, you get that for the nth level tree, the expected value of mn squared is less or equal than what you got by the coffee. Than what you got by the coefficient, the main coefficient of the function, 2 theta squared to the n. And this goes to zero exponentially when n goes to infinity. Okay, so maybe we haven't followed all the algebra, but just to give you a test of what happened here, we derived this beautiful recursion, which was the belief propagation recursion. So this is the recursion that's written over here. This recursion is ugly. This recursion is ugly. So, you know, for many years, people looked at it and were scared. But in fact, physicists are tough, or people doing statistical physics are tough, and they said, okay, we will be able to handle it. You do the linearization in the right way, which is maybe not the trivial way, right? You don't linearize it in the most trivial way. You linearize it in a slightly less trivial way. When you linearize it in a less trivial way, you get this following approximate or bound approximate. Bound approximate bound recursively, you analyze this bound recursively, and it tells you that this magnetization, the random variable that tells you how much you know about the wood. You know, this random variable, if you look at the expectations query, it decays exponentially with questions about this proof. Good. Okay, no question. Everybody thought it's either trivial or they sleep or they left and watching some other things on YouTube. So that's the wonderful situation with online learning. I cannot even see if you're sleeping. So that makes it very difficult. For those of you who are sleeping, keep enjoying it. So I think what I will do. Yeah, I was planning to do one of two applications in the linear. To do one of two applications in the linear theory, and I was actually planning to do this phylogenetic inference problem. So, maybe I will start with discarding this problem and then we'll see the connection to the KS-bound or to the linear theory next time. So, I'll just tell you about this problem for a few minutes. I mean, it's so I sort of gave you the statistical physics interpretation to some extent of this model. We were talking about magnetization, we were talking about the boundary conditions, we were talking about the Boundary conditions, we were talking about the language of statistical physics. Let me just give you a taste of other places where people have looked at this model. So, the picture on the left is the picture from Darwin's book, The Origin of Species. So, he liked to draw trees. People these days also like to draw trees, but they don't like to draw by hand. They don't like to travel to islands. In fact, they do like to travel to islands, but they don't travel to islands anymore. So, they do travel to islands, and what do they do in these islands? Actually, a biologist. Doing these islands actually have biologists when we did it. So they take DNA sequences of species, it could be floral or fauna or whatever, and then they press a button on the computer, and they press a button on the computer, and this gives them this. So let's try to understand what's the mathematical model here. So I'll tell you a little bit about the mathematical model. This is a a picture that I've shamelessly stolen from a costas dascalakis, who was one of my collaborators in some of some of the phylogenetic work. Of some of the phylogenetic work. So, basically, what happened in the phylogenetic model is exactly the same model that we talked about before. We do again, but before we just did it for one plus-minus random variable. Now, we are going to do it for many, many, many plus-minus random variables. So, this picture of the dinosaur is supposed to represent the DNA of the dinosaur. So, the DNA of the dinosaur is not going to have ACG. It's going to be some sequence in plus minus one to some space L. I have to choose a L. I have to choose the name L. And what will happen is, you know, there's going to be some speciation event, and this dinosaur is going to become two different dinosaurs. There's going to be some mutation. If you look at the DNA sequence, it's not going to be exactly the same DNA sequence. We are going to have the same process that we had before. And the model is very simple. Independently for each pixel, you do the process that you did before. It's probably data you copy. Otherwise, you choose something uniformly accountable. And we do it over and over again. And we do it over and over again. Now, there are multiple questions that we can ask. So, the question that we were asking right now is the question of recovering the root from the leaves. And what we, for example, saw is that if you apply the majority function, maybe if I apply the majority on this picture, I get this picture, which is not identical to this picture, but it's correlated with this picture. In phylogenetic problem, what I'm given is I'm not given any of, sorry, I'm not given Altitus to tell you what I'm not given. So, all of this is not given to me. Is not given to me. All I'm giving him is this vector, and this vector is actually permuted. So, this is DNA of one dinosaur, this is the DNA of another dinosaur, I have DNA of eight different dinosaurs. And then my goal is to recover this tree. What does it mean to recover this tree? It's to say, wait, is this dino the sibling of this dinosaur or of this dino? Is this a sibling species of this dinosaur or of this dinosaur? And the question is, how do you do it? Okay, so obviously, you see that the broadcast process that we talked about before is still. Process that we talked about before is still here, but the difference is in what we want to do. One thing that we want to do, and these people also in computational or evolutionary biology want to do, say, oh, I know the tree of the species because maybe Darwin was doing it for me, or maybe my advisor was doing it for me. And now what I want to do is I want to learn about feature of the species. And then this is this question about applying BP or majority and so on and so forth. But the more basic question in biology and the question that Darwin was asking is: I'm not given any of this information in the top layer. Information in the top layer, not just the pixel. I'm not even given the structure of the tree. What I'm given is this DNA sequences in some random permutation, and my goal is to build it. Okay, so maybe I'll stop here for today. Next class, I think I'll talk a little bit more about this linear theory applied to this problem of phylogenetics. And then I'll talk a little bit about the non-linear theory. I wanted to say that if I'll have time, I'll also talk about block models. I'll have time. I'll also talk about block models, but I won't have time, so I won't talk about block models. But if I happen to have time, I will talk about block models again in the context of linear theory. So, all of this discussion I want to do first in the context of linear theory. And maybe I'll try to go to, so I'll just say, say what's happened next time, next time. I'll tell you mathematical results for phylogeny, mathematical results in model for phylogeny. Phylogeny. And of course, the main reason I'm going to tell you is the threshold deter squared equal to one is going to play a very important role there. And then I'm going to start to talk about the non-linear theory. Okay, so let me stop here and I'm happy to take more questions. Okay, so I'm going to unmute everyone so we can think of them collectively. Don't worry. No, not at all. Of course. It's from just that we just