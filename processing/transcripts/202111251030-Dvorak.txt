This is actually not a talk on my research. David asked me to give a presentation on a result of Kraus-Gammer and Lee on graphs with polynomial growth. So this is what I will do. But I would of course like to add some personal notes to that. So I will draw some connection to things like a simple. Some connection to things like asymptotic dimension, which wasn't formulated that way in the paper of Kravitz Kamerenti, but the connection is fairly natural. Okay, so what are we going to prove? So what is polynomial growth? So what is the growth of a graph? So by this, I will denote the set of vertices at distance at most r from the Gizzon vertex. Given vertex. Now, growth of graph G at distance R is at most the maximum of these sizes of the R neighborhoods. And there is this product structure theorem of Kraus-Gambar and Lee, which kind of predates the product structure by 10 years. That if the growth of your graph is polynomial at every distance, Every distance, then your graph is a subgraph of the strong product of bounded number of paths. Where if the degree of this polynomial is C, then the number of paths is roughly on the order of C times log C. So this is kind of an amazing theorem because it's actually like an approximate characterization of graphs with polynomials. Of graphs with polynomial growth, because of course, any subgraph of such a product has grows at most r to the number of these mass. What is also very nice is that the result is tight in the sense that you cannot decrease here the number of masks. So there are some. So, there are some constructions derived from expanders which show that you cannot do better than c times log c. And so in this talk, I would like to give some basic ideas from the proof of this theorem, maybe some application for replications of these ideas, but one full replication of these ideas. And okay, so. Okay, so how do you go about proving this? Well, so you have your graph and you want to map injectively the vertices of your graph into this grid. So each point for this grid has these C log C coordinates. So we want to find these C log C coordinate functions, which have the following. Have the following properties. Whenever U and V are adjacent, they must be mapped to the adjacent vertices of this grid. So in every coordinate, they must differ by at most one. So let's call this, let's say, one Liksit's property or contractive property. And secondly, this mapping here must be injective. Here must be injective. So, for any pair of distinct vertices, there must exist at least one coordinate in which they differ. So, maybe at the first side, this first condition may seem more complicated than the second one, but actually, it's not quite the case. We will be getting the first one essentially in just one way and kind of for free. And the whole fight is to get this second condition B to all. So, how do we get? To all. So, how do we get this condition A? Well, this will not be precisely the way that we will do it in the proof, but there is kind of a standard way how you can get functions with this property. If your function is just the distance from some subset of vertices, then any two vertices, the distances differ by at most one. So, that's precisely what we want. And so now you can. And so now you could imagine that maybe we will try to find some system of such subsets, C log C subsets, in such a way that we would get this second property B, so which I will say that this system of coordinate functions distinguishes Parova issue and we will not do it quite this way, although I mean, you could actually formulate it by Could actually formulate it, you could formulate the result in this way if you were a little bit more careful about what you are actually saying. But okay, so let me get, let me say like slightly refined version of this idea, which is closer to what we should be using. So imagine that we have some subset of vertices. We have some subset of vertices of our graph. Removal of this splits the graph into a couple of components. Now, in each component, we want to make an independent choice. And so for each component, we will uniformly independently at random choose some offset from some bounded range, where B is something. And then I will define this function f to be, well, basically the distance from the set, except that I decrease the distance by the offset of the component in which the vertex is contained. I mean, this is still fine. And what I have What I have achieved is that if I have a look at the vertices in different components, I will have some probability that their distances from the set after adjusting by these independently chosen offsets will differ. So, in particular, if I take a vertex, which is at a distance at least b for At least B for from the set S, then with taking these offsets, I will have like B possibilities where it will be mapped, uniformly chosen. And so for any vertex which is not contained in the same component, I will have probability at most one over B that I will be mapped to the same spot. And then I can try to improve the probability so I can repeat the same choice in, let's say, k coordinates independently. So now for any such pair of vertices, u and v, the probability will be one over b to the k that they map everywhere to the same point. And if And if it happened that my graph would actually be small, would have such a polynomial in B, e to this constant k, let's say. Then there is a choice which will distinguish all pairs of vertices with this property. So now there are basically like, well, there are a lot of things that we need to overcome here, but one is, of course, that our graph does. One is, of course, that our graph doesn't have to be of bounded size, which we will kind of overcome by splitting it into pieces of bounded size and working inside each piece. And the second thing that we need to overcome is that here we have this condition that we only get this for the vertices which are far apart from this set as from this boundary. So we will also need to make several different choices. To make like several different choices to ensure that everyone is far enough from the boundary in at least one of them, and so this motivates this notion of decomposition. So, we will need a couple definitions. So, okay, by a cluster, I just mean a subset of vertices. And now the boundary of the cluster up to distance. The cluster up to distance S is the set of vertices in the cluster that are at a distance less than S from the boundary. So if I have a look at vertices at a distance at most S from V, not all of them will be inside the same cluster. And so now we will consider some parts. We will consider some partitions of vertex set into these clusters, and we will say that the partition is R narrow if the diameter of every cluster is at most R. And I mean, because we are working in graphs with polynomial growth, this obviously will imply that they have also bounded size. And for a partitional And for a partition, like the boundary of the partition, it's just the union of the boundaries of its clusters. Let me just to be clear, mention that this diameter is measured in the whole graph. I'm not measuring the distance within the cluster, but the distance in the whole graph. And now, for some integers s and r, where s is smaller than r, s r that decomposes. SR per decomposition is a system of partitions where each of them is R narrow. All the clusters have bounded diameter. And moreover, for every vertex, there exists at least one of these partitions such that I am not contained in the boundary. So at least one of these coordinates, I will have a cluster such that I will have a cluster such that me and the whole ball around myself is within the cluster. Okay, let me do an aside, or I mean, it's not an ascetic, it's very much related to the asymptotic dimension. So, what is an asymptotic dimension? So, I mean, I don't quite have an intuitive understanding by this fault. Understanding by this sort, but I mean, if you know what Lebesgue colouring dimension is, then you are on good shape to understand it. So, okay, so what does it mean? Let's say that a space has dimension one. Well, it means basically that you can divide it into small pieces such that every point belongs to at most two of the pieces. Two of the pieces. And similarly, if the dimension is two, you can cover the plane by small pieces such that every point is in at most three of them and so on. And for this Lebesgue dimension, you really want to hold this for arbitrarily small PCs. So the Lebesgue dimension is a local ocean. And Gromov introduced a symptotic version of this, a symptotic dimension, which is on the end like a global notion. So, I mean, for example, if you take a bunch of balls and connect them together in a line, then, well, this has Lepec dimension three because locally it's three-dimensional, but If it's three-dimensional, but growing of dimension one because it's just once one simple line, and so basically, well, you can eat the definition. Basically, you want the same covering thing hold on large scales. And we will not use this definition, we will use a different one, which turns out well. Okay, I will use this not for arbitrary metric spaces, but just for graphs, right? And also, I will use this very much reformulated definition, where I mean the equivalence from here to here is not at all various, but trust me, it's equivalent. So we will see that a graph class has an asynthotic dimension at most m. If for any distance s, there is. any distance s there is a larger distance r such that every graph in this class has this s r parity decomposition of size n plus one so at uh at at any at any scale i can cover my my graph by these n plus one partitions which are still into bounded size size parts such that everyone is inside one of the parts Inside one of the parts. And so this notion has been started to be studied in the context of graph theory relatively recently. And there have been a couple of amazing results by first Jung-hoon and then these other people, Bonami Buchara. Espera, Greno, and Gerald and Scott, they had another paper which looked at it from some slightly different direction, and then they made a joint paper, which among other things shows that, let's say, the asymptotic dimension of any class of graphs in bounded three ways is at most one, and the asymptotic dimension of any proper minus close graph class is at most two. So, this is this is actually like what you what you would is actually like what you what you would like what you would expect i mean from the from the structure uh structure theorem proper man closed classes are just like surfaces merged together so you would expect this this to have dimension too and so it's extremely nice that it actually happens to have dimension uh okay i would also like maybe like to mention one of One of our results: that if you take the intersection graph of balls in the dimensional space, and you don't need them to be thin or anything, any intersection graph of balls, it will have a linear dimension in the dimension of the underlying space. So, this, I mean, again, kind of good evidence that this really behaves as a nice dimension concept. Dimension concept. And this paper of Kraut, Gallery and Lee that we now study, basically implicitly shows that the asymptotic dimension of any class of graph where the growth is R to some constant is linear in C. They didn't formulate it in that way because they didn't realize this connection, but it's there. It's f and actually for the proof they need something something stronger. So in this definition of asymptotic dimension, this R is some function of S, but there is no bound on how fast growing this function can be. And that wouldn't work for the proof. For the proof, what is needed is that What is needed is that this R depends polynomially on S. So let me call it stronger symptomic dimension, which I just made on the spot. No one else calls it that. It might actually have some name. If this polynomial is a linear function, it's called Asfat Nagata dimension. It's plausible that someone else considered it with the polynomial function, but I don't know. Function, but I don't know. And so there are some results on this. So there is an old result of Klein, Podkin, and Rau, which implies that even the strong asymptotic dimension of a class with proper minoclose classes is bounded. Although I don't know whether so this probably is not optimal, this probably can be improved. I don't believe it can be improved to linear, but to a constant. That to a constant of independent on k, but I'm not sure. And Kraus-Ganner and Lee prove that sure, even a strong asymptotic dimension, if you have groups at degree D, of degree C, is linear in C. Okay, so let's say for now that we would believe this guy, I will actually go back. claim i will actually go back to this and give you a proof that that the of this as in three damaging claim but for now believe it is true and uh so let's uh let's show the following thing suppose i have a stronger symptomatic motion less than m and my growth is polynomial then i claim that i have well this many m squared times c M squared times c one which is function which distinguishes every pair of vertices with the property that they're uh that they are somewhat far apart but not not too far. So they are in some range square root of r to r let's say and for for so for any fixed value of r I can get such a such a system of functions and so And so, how do I get this? I do kind of a two-level clustering. So, I will have, I will use this strong asymptotic dimension device. I will have these large clusters of diameter polynomial and R, such that the ball of radius R is covered by each ball of radius R is covered by at least one of the partitions. One of the partitions. And then I will have a finer decomposition on this scale. And I will assume that it actually refines this one, that every cluster here is a subset of. Clusters up here, which intersected. It's not completely obvious, but if you think about it, if you get any partition here, you can adjust these two decomposition here. You can adjust these two decomposition such that this holes by breaking out some parts into smaller ones. And so now, how I get these functions. So for each ij between one and m, I will get linearly in C functions, which distinguish all pair of vertices u and v, such that u and v belong to the same part of this larger partition, larger decomposition. Partition larger decomposition, but they belong to different parts of this final decomposition, and moreover, let's say u is not contained in the boundary of the final final. So, if we can get that, so this way we will get these m square c functions, and I claim that they actually distinguish all pairs of vertices with this property. Why is this the case? Why is this the case? Well, because u and v are at distance at most r, there is some i such that here the ball around u of radius r is contained in single cluster of pi, and therefore u and v will belong to the same. And V will belong to the same cluster of I. And because all clusters in this smaller in this finer decomposition have diameters R to D one half, and these are farther apart. They must belong. They must belong to different parts in any of these partitions at the smaller level. And again, using this edit property, there will be some j such that u is not in this boundary of pj. So whenever I have vertices that are whose distances. That are whose distance is in this range, they will satisfy these. And so, yeah, actually, so how do I get these functions? Well, I will use this thing that I mentioned here on this slide. So now you and we live within the same cluster. Live within the same cluster whose size is polynomial in R. And well, U is far enough apart from the from they live in like these in these different subclusters and US far enough from the is far enough far enough from the boundary of its subcluster. Here B is this R to the epsilon, where epsilon is some phase constant and the size of the large cluster is polynomial in R. R, yeah, polynomial in R. So I will just need to repeat this k times where k turns out to be like. times where k turns out to be like linear linear in c so using using these these ideas we get this this thing so we can distinguish the vertices whose distance is in some range and And then, okay, so with this, we could already get the product structure where the number of paths would be like double of local of the diameter of the graph because we use We use this construction to cover this range, and then again in separate coordinates to cover the next range, and so on. And because the size of the ranges goes up by the square, after I do it log, log diameter times, I've distinguished everything. Except I don't want this to, the number of coordinates to depend on the diameter. On the diameter, so I have to be a bit more careful. So, what they basically do is some kind of inductive argument. So, basically, what we did, we kind of fixed how the functions must look within the clusters of this larger partition. But we can still shift them. Shiv them basically okay yeah we have we have we have set set it set it up in these in these clusters of this larger partition such that the boundary of these larger clusters is mapped to not to zero but we can still like shift things we can map the the boundary of these large clusters not to zero but let's say to adjacent Adjacent points. And so we have still some freedom which we use basically by contracting these large clusters to single points and then iterating the same argument on this contracted graph. I will not go into the details. This is not the interesting part. What you get from this is that now you will have this thing. You will have this distinguishing property at many different ranges of distancing. So, we'll have at this range, then you will have skip something, then there will be another range that you distinguish, and you skip something, and so forth. And then you take the same thing, but shift it by one. And it turns out that you can just take a constant number of these shifts, so you can repeat this just constant number of times to cover all possible range. Times to cover all possible ranges. So if you do this, we get this colorally that if the asymptotic dimension is, strong asymptotic dimension is less than m and the growth is polynomial with degree c, then we get that our graph is product of m squared times 10. m squared times times c mass. And now using the fact that we know that the asymptotic dimension is also linear in C, we get this being the product of roughly C cubed bounds. And then you need to fight it more to get this to the optimal bound, but it is very technical and I don't want to go into that. But let me maybe remind Let me maybe remark that we have also seen that if, in addition to have polynomial growth, your graph class avoids something S minor, then its asymptotic dimension is constant. And then this would give you actually linearly many paths in C. So there are some natural subclasses of polynomial growth things which have like Growth things which have like optimal dependence, where we know optimal dependence on the number of paths on the growth. Okay, so how much time do I have left? Okay, yeah, so I would like to show you this part, which I think might be the most interesting, which is. Interesting, which is the bound on the asymptotic dimension. So this is just a reformulation of the thing. So this is another way of saying that the asymptotic dimension is linear in C, being more explicit about these padding constants. So we will be able to break into class. able to break into clusters of diameter r such that even this cube root boundary such that this any ball of of of diameter cube root of r fades into at least one part at least one cluster and And the first step is to get one part of one partition, which doesn't, of course, work for every small ball, but works for almost all of them, works with. Almost all of them work for with high probability alone. So let S be this of cube root of R. And I claim that we can now choose two R narrow partition randomly such that the probability that I'm bad, that I'm contained in the boundary of this, as boundary of this partition is very small. It's on the order of one over square root of. The order of one over square root of r. And the assumption that I need for this argument to work is that the weak coloring numbers at this stance r are at most r to the c. So I don't even meet polynomial growth at this point yet. What is the round of partition here? I will well you do some random choices which will tell you how to. Which we tell you how the partition will look like. Otherwise, I will explain in a minute. This thing with the we coloring numbers also isn't what Kraus-Ganner and Lee did, and we actually don't need it. So that I don't overcomplicate things unnecessarily, I will only tell this with the polynomial growth, but you can you can adjust the You can adjust the argument. And okay, so if we are able to prove this lemma, we can now repeat the choice independently roughly C times. And now the probability that A vertex is in the boundary of all of these random choices is this. And moreover, it will turn out that the choices made in the selection of these partitions are local. So it will turn out that whether a vertex belongs to the boundary of this partition or not depends only on the choices made in some 3R neighborhood of the. Neighborhood of V. And so completely independent on whatever happens further out. And so you can use the local Emma. And also, yeah, so this vertex depends only on this boundary and because the growth is Because the growth is R to the C, this neighborhood is size roughly R to the C, which is like the same as inverse to the probability here. So the local lemma tells you that there exists a choice of these C partitions which work for everyone. And that's what we need to do. So what remains to do is. What reminds to do is to tell you how the proof of this works, and it goes like this. So for each vertex, I select a number in range from S to R independently, but not uniformly. Each of these numbers will have a different probability, and I will tell you the probability must be. The probability must be in a couple of minutes. So we will, for each i in this range, we will assume probability be i. It's actually more convenient to work with the probabilities that the solid number is at least i. So I will speak about that. Now we fix an arbitrary ordering of the vertices. If we work with the Vicoloric numbers, it's clear what ordering we have to lose. Here it's Here it doesn't matter. And now I will gradually assign vertices to clusters. So I go over the vertices in this order. I take, look at a ball of radius L of V, L is this assigned number, and whoever is not And whoever hasn't been yet assigned to a previous cluster is now assigned to this cluster of V. So, obviously, this ensures that every cluster has a radius at most R. So, that's fine. And we just also, it is clear already from this description that which cluster. That which cluster I'm assigned to only depends on these numbers assigned to vertices that are not very far from me. So locality is also clear. So we need to show that every vertex has relatively small property that it will be contained in the boundary of a cluster. So let's Let's do the analysis. Let's consider some vertex x, and let's try to check whether it belongs to the boundary or not. So, for this vertex x, let v of x be the first vertex in this fixed ordering, for each this l ball around the vx intersects this s ball around, well, this should be x, obviously. And so now what does it mean for a vertex to be in a boundary? So certainly, so this is the first point where someone in the ball around X gets assigned to a cluster, and this vertex X is in the boundary. Is in the boundary if and only if this whole ball doesn't get assigned to this first cluster. So what is u here? Well, u is x. Sorry about that. So x will happen in be in the boundary if and only this ball, s ball around x is not contained in the l ball around dx. And let's do the probability. And let's do the probability that this will happen. So this probability that this will happen. So we will do it as the probability over whether that the vx is selected vertex v, and then the conditional probability over the sink. And now we will. Now we will split this sum into two parts. One is over the vertices v, whose distance from x is in this range between r minus s and r plus s, so those that are far apart, and then over the remaining ones whose distance is smaller than r minus s, and we don't need to care about the line. Don't need to care about the larger the ones that are at a larger distance because then it wouldn't be possible for V of X to be V. And so for the first part of the sum, we will just use a fairly simple bound. I know that the number of such vertices Number of such vertices v with this property is at most the size of this ball around S, around X, and the probability that V of X is chosen to be V is certainly at most this probability. I mean, if I'm assigned something smaller, this intersection would be empty. For the second term, we will focus on analysis of this probability. What does it mean for V of X to be V? Well, it means certainly that this number, this radius assigned to V must be. Assigned to v must be at least the distance between x and v minus s. So that ensures that that's just a rephrasing of this thing that this must be non-empty. And there should be a couple of other terms saying that for the vertices that precede in the ordering, the opposite inequality also. This indeed the first vertex which is with this property, but actually, this part that I'm writing here depends only on this choice for V, and the choices made for different vertices are independent, so I can ignore these choices made at the vertexing. And I mean, now the probability that this S ball around X is not contained is certainly bounded by above by saying that this radius must be at most the distance from X to V plus S, because if it is larger than certainly the whole ball, the S ball around X will be. X will be contained in this album around V. Okay, so basically I need these two probabilities to be small and here I'm just rewriting this conditional probability. So we need to bound this first term, which is like So, this first term, which is like R to the C times the probability that the distances at least are minus s. And we also need this fraction to be bounded. And so now if you fix this fraction to be the probability that we want r to the minus one half for every for every i, then this determines what the probability should be. The probability should be at a very high, and then you just compute that this lasting holds. I mean, how you should look at this, I mean, you have this thing here, which means that the probability that, let's say, uh that let's say um probability that that it's let's say greater than i plus well forget s that is at least i plus i plus one is just slightly can be just slightly smaller than the probability that it is at least i just well this this factor of r to the minus one r but uh well then if But then if you iterate this roughly r times, it will become some exponentially small thing. And so it beats this polynomial. And if you know what this is some kind of, I guess, called truncated exponential distribution or whatever, I don't know. I don't understand the sex. Okay, so this is the end of the proof. And I have maybe like five minutes left. So let me tell you maybe an application to product product product structure things. So This is from a paper which we have with Tony Hugh, Ben, Jung-hun, and David Wood, where we considered like a bunch of simplish questions about what happens if you take a product of class each with some properties. And one thing that interested us was what happens if you What happens if you take two classes with polynomial expansions or sublinear separators? Under which conditions is it true that in the result we also have polynomial expansion? And we came up with this exact characterization. So of course the two car C's that you are taking the product of must have polynomial expansion. But additionally they must have here They must have here the polynomial growth in some kind of a joint fashion. So at every distance r, the growth must be polynomial in at least one of the classes. But at each distance r, they can choose different one. Which kind of complicates things. Like if the condition was just that one of them must have polynomial growth, then we would be fine because we could use the result of Krabs-Gammar and We could use the result of Kravsgammer and Lee, but if they widely alternate in this way, we really need to work a little bit. So what we actually use is just this lemma that I have just proven at any fixed R. So at any fixed R, we can do the, well, the proof of the Remal only looks at this distance R. So we can just take this class. Can just take this class which has polynomial groups at this distance R and apply it in that one. And so the proof is that let's say you assume that at this distance r the growths of g2 is polynomial. So apply this lemma. So we have this partition where every part has Every part has size as polynomial size, and the probability that a vertex belongs to a boundary. Here, let's say just be considered just a one boundary. So the probability that a vertex has a neighbor outside of its cluster is small. So we will do it at this parameter r squared. R squared, so probably that's it. And so now we want to bound the density of in R shello minor in product of G1 and G2. And so you have a look what happens if you delete from G2 the things that are in the boundary. In the boundary. So now, when you have an edge in your R shell minor, it corresponds to a path of lengths to R, or at most to R. And now the probability that this path is hit by one of these deleted vertices is this probability times 2R, and actually, this probability should have been 1 over 4R. Should have been one over four R. So the probability is at most one half. And so the expected number of edges in your R-shallow minor after you delete these boundary vertices is still like half of what you had before. And moreover, every component of G2 after deleting this boundary is as As polynomial size. So now we are just looking at a product of G1 with something of size polynomial in R. And now there are just results which tell you how the expansion behaves when you take the strong product with a green, and from this, you get that the From this, this you get that the expansion is polynomial in the expansion of G1. So, this is one kind of application of this idea, let's call it fractional asymptotic dimension. And okay, let me summarize like things from my talk which I consider to be important. So, one important thing is that One important thing is that asymptotic dimension seems to be useful. That's not something that was completely obvious to me. I mean, it's clear that asymptotic dimension is interesting. I mean, the fact that the asymptotic dimension of proper minoclaws class is two, I mean, that by itself should persuade you that it's a nice parameter. But it's good that it seems to actually have even some. To actually have even some further application, that it's not just a concept that would be nice, but it's also this sectional version that I was saying also works for classes which have polynomial with coloring numbers. And although I don't have any application for that, I can imagine that perhaps there might be. And the last thing that you might I would like to maybe speak about this where you can kind of generalize things. I mean, of course, you can't generalize this product structure for paths, but maybe you would like to generalize it to, well, the usual product structure, the path with one tree or one thing of bounded tribes. And I mean, there are a lot of places where things really did. Well, six really depends on the polynomial growth. I mean, we depend on the clusters having targeted size to be able to ensure that we distinguish everyone in the cluster and we depend on it again when we use this whole clemma to ensure that. This fractional version can be turned into the non-fractional one. So there are a lot of sticking points, and I have no idea how that will be overcome. So although kind of the outline of the proof seems like it could give something more general, the fact that that kind of depends on this asymptotic dimension, which we can bound in much more general circumstances or different circumstances. Circumstances or different circumstances as well looks promising, but I can't really, unfortunately, say anything useful in this direction. Okay, thank you for the attention. Thank you.