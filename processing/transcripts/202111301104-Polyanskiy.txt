Without skiing. So, and I was trying to eyeball any skiing masks in the video, and I couldn't. So, yeah, so I guess that's maybe it's not a good season yet. But anyway, so I'll talk today about this problem. Actually, interestingly, I mean, related to optimal transport, just like Andrew's talked before. So, this is a joint work with our student, our joint student, Ziu Gia, with Sasha Raklin. Gia with Sasha Raklin and another student from MIT math department, Adam Block. Okay, so right, so I'll be talking about empirical measures. I guess I don't need to introduce this object for this audience. So given a distribution P, right, we draw IAD samples and then we form a sum of these delta functions, right, equally weighted. Empirical measure itself will not appear much in my talk. What will appear My talk, what will appear mostly is the smooth empirical measure. Namely, I will take this hedgehog, right? This collection of delta functions, and I will put a little Gaussian bump with width sigma square. And I will do it not just in dimension one, in multiple dimensions. So here, instead of sigma square, we will have sigma square and diagonal, sorry, identity matrix. Right. Okay, so this is the smooth. Right. Okay, so this is the smooth empirical measure. Most of my talk is about this object, and the first couple of minutes I wanted to spend explaining why I actually care about this object. So it's been a few years, but basically, if you guys were there in 2017, there was this interesting paper on information bottleneck by Tishby, which kind of stirred the pot a little bit and created some. The pot a little bit and created some controversy and lots of follow-up. But basically, I mean, whatever that paper was talking about, it's not very important for my talk, but what's important is that they were trying to think of DNNs as information processing units, right? So I mean, a DNN takes as input this very high-dimensional vector of pixels, right? I mean, the picture as a pixel. The picture, right, as a pixel map, and then it processes it through the layers, right? And in the end, it introduces this binary output, right? Y hat. And THB had this very beautiful picture of information funneled. He would say, you know, the sort of the information starts with, you know, like a huge amount at the input, and then as layers sort of become thinner, you kind of shed the information. And he would measure the information between the input and the layer, and also the layer. And also the layer and the true label. And then somehow he would say his claim was that, you know, the mutual information, you know, as you go deeper inside the DNN, what happens is that the mutual information kind of with the input shrinks, but with respect to the optimum label, it kind of stays the same, right? So basically, it's a beautiful metaphor, right? Because it shows us that. Right, because it shows us that somehow DNN squeeze the relevant features, right? The features relevant for classification and shed unimportant information from the input, you know, which so it's a very, you know, captivating sort of mental picture, except it's completely wrong. Why is it completely wrong? Because, well, you can't make sense out of these objects, right? Iy or IXTL and IyTL. Why? Well, because I Well, because, I mean, if you consider a smooth, let's just consider smooth, let's say, tunch activations, right? So then what happens is that on any discrete input, generically, any DNN, regardless of its weight, is going to be injected, right? So you can take this, you know, million-dimensional image, right, and map it to this one-dimensional output, right? And just by tiny minuscule variations at the output, you can reconstruct. Variations at the output, you can reconstruct which image was there, right? From the entire CIFAR database, right, or image net. So, for this reason, no matter how you change the weights, right, the IXTL is basically just logarithm of the number of images, right? If you consider empirical distribution on the images, right? So, in some sense, I mean, this is a quick thing that people should teach be aware, right? They said, okay, you know, this is a very beautiful picture, but it's completely wrong, right? On the other hand, I mean, that's On the other hand, I mean, that's a sort of very cheap way to attack that because you know, if there is some information which is embedded in like you know 75th digit after the dot, then you shouldn't really think of it as relevant, right? That's not how neurons work, right? That's not how my neurons work, I hope, right? So practically, you should shed the information, you know, which is embedded in the tiny variations of these values of neurons, right? And then when we wrote papers about this phenomenon, we proposed this idea that, you know, And we propose this idea that you know, to sort of make this more rigorous, right, and make sense out of this information funnel, we can just add noise, for example. So we replace every neuron, right? Neuron is a linear combination of inputs plus tens, right? We say it's a linear combination of inputs plus tens plus independent Gaussian noise of tiny variants, right? And this is what led us to lots of studies of this phenomenon, what happens to empirical measures if you smooth them by a little bit of Gaussian. them by a little bit of Gaussian blobs, right? So somehow the mental picture is that you kind of remove the dependence on this tiny variations, right? So that's sort of, I mean, that's how I came to this question. And so I think two or three years ago, when I was giving first talks on this, Richard Nico actually pointed out that, you know, that I mean, I was always apologizing for studying smooth empirical processes because I assumed nobody in the world cares about them. Nobody in the world cares about them. But actually, I mean, they have this, you know, 13 pages in their very beautiful textbook by Janae Nicol, right, on the smooth empirical processes. To my shame, I realized when I was preparing this talk that after all these years, I haven't opened that chapter. So I still don't know why they studied it, but I know they did. Anyway, so yeah, because for me, I mean, the results themselves are just so funny that, okay, so what, so what are the results? So let me. So, what are the results? So, let me start with again. I assume most of you know what Wasserstein distance is, right? Especially given that Andrew just introduced it, right? We've just taken WP is infimum overall couplings of B-dimensional vectors. So this is the Euclidean distance, right? We raise it to power P, minimize coupling, and take one over P power. Okay, so now what is important for me is the empirical Wasserstein test. Is the empirical Wasserstein distance. So, okay, so again, so we have some distribution. So, think of this strange distribution on Rd, right? We draw IAD sample. So, this empirical distribution, you know, dense here, not so dense elsewhere, right? And the question we are asking is what's the distance between the empirical distribution p and the truth, right? So, this is this is, let's say, let's consider expected distance, right? So, I mean, if you've been around the optimal track, The optimal transport, you know, this is a very classical question, right? So, lots of people studied it, starting with Dudley, who basically, you know, more or less showed that the expected distance between P and the empirical measure scales as n to the minus one over d. It's very easy to understand why this is a natural scaling, right? I mean, if you have n points, then just from the volume argument, right, there will be lots of cubes of size whose edge is n to the minus one over d. Edge is n to the minus one over d, which contain no points. So when you build the optimal transport, right, you have to traverse these distances, right? And that's what gives you n to the minus one over d. Okay, so technically this is only true for continuous p and dimension bigger than three. But in any case, roughly speaking, oh yeah, and today we also know through lots of work of people like Wiedenbach, Thibault Legwig, and so on. So they also extended. weak and so on. So they also extended this scaling to Wp distances. There, I think you need to take D to be bigger than 2p in general. So, okay, but in any case, the scaling is n to the minus 1 over d, right? It's very slow if you take d to be 20, right? This is super slow scaling. Of course, that's bad. This is a curse of dimensionality. So here is the funny result which started sort of our interest in smooth empirical measures. So we observed through a very simple So we observed through a very simple one-page proof, you know, which I could give here if you had more time, that actually the distance, if you convolve the true distribution with Gaussian and the empirical measure with Gaussian, you get the scaling, which is of speed one over square root n. And this is true for every k-sub-Gaussian distribution p. Okay, so we noticed this, right? And this was shocking because, you know, there is no dependence on dimension. Because there's no dependence on dimension here, right? So, in arbitrary dimension, this is true. Okay, so now after we noticed this, we started looking at the question: okay, so what else can we prove about other distances, right? Maybe W1 is very special with this regard. So, we asked ourselves, what happens to W2 square, Kl distance, and chi-square distances? And the answer was actually really, really funny. So, I wanted to mention it. This is still all the results. So, turn the square. So it turns out that the only quantity you need to know is this high square version of mutual information. So what is high square mutual information? KL divergence is the distance between joint distribution and the product of marginals. High square information is just the high square distance between joint distribution and product of marginals. And so x is this unknown distribution on d-dimensional space and x plus c, so y is equal to x plus sigma z, right? So this is the Gaussian perturbed. Gaussian perturbed input, right? And we are computing this mutual high-square mutual information. So here is the theorem. We proved that in any dimension D, if this high square mutual information is finite, then W2 square, DKL, and high square, all three guys, scale as one over n. So this is equivalent to one over square root n scaling, right? Because these are squared distances, right? W2 squared is like square of the distance and k l and Is the square of the distance and k and high square. Right now, conversely, we proved also that if high square distance is infinity, then well, high square does not scale, does not converge at all. And W2 and Kl are probably slower than one over n. So we pin down exact phase transition from one over n to slower than one over n. Okay, and this is completely governed by this strange object white appears. You know, strange object why it appears, we have no idea, right? Just happens to be in true. Okay, so there are some caveats for w2 squared, but let me, in the interest of time, skip them. So I wanted to mention that this result is actually interesting already in dimension one. So here is why. So let's consider p, which is Bernoulli half, right? So you have only two atoms in the true measure. So then empirical measure is going to be basically this one with, you know, with slight square root n imbalance. Slight square root n imbalance, right? So instead of half half, you will get half minus square root n, one over square root n, half plus one over square root n, right? So when you put Gaussian bumps, they will be like slightly imbalanced, right? When you convolve empirical measure. But this, okay, so now since this mass needs to traverse this distance between, right, this order one distance, then it immediately tells you that expectation of w2 squared is going to be order one over square root n. On the other hand, our theorem shows that, well, Theorem shows that, well, the convolved measures, right, they have speed one over n, right? So somehow, once you put this arbitrarily small Gaussians, right, the speed of convergence instantly changes from one over square root n to one over n. Okay, so but here sigma is constant, right? Yes, yes. The constant here depends on sigma, of course. Yes. I mean, otherwise you would take sigma to zero and violate this, right? Take sigma to zero and violate this, right? Sure, right. Yes, yeah, that's a good question, right? So, this is for fixed sigma and n going to infinity, right? Yeah, and actually, even for if you take Gaussian measure, then it's known that empirical converges at speed log log n over n and ours converges at one over n. So, even for the smoothest example of all, you already have speed up for smooth measures. Okay, now the question is, right, so I told you that the speed is governed. Right, so I told you that the speed is governed by high square being finite or infinite. And the question we asked ourselves back in 2020 as well, so what guarantees I high square being finite? So the first is easy condition. If px is compact support, then high square is always fine, is always finite. Okay, that's good to know. Now, the second result was considered, we were actually interested in tail behavior as well. So we asked ourselves, okay, what if px is k sub Gaussian, where we define Is k sub Gaussian, where we define k sub Gaussian exactly like in Gabor's book. Um, right, so uh, okay, so we proved this uh characterization, which was slightly uh unsatisfying, that if k, right, so if the variance of the noise is four times the variance of the Gaussian, then I chi square is uh less than infinity, and otherwise, if the variance is smaller than half the variance, the variance proxy for k, then it's infinity. For k then it's infinity, but there was a gap between these two. And so one first new result was to actually close this gap. So this was very annoying, right? So now with Zu and Adam and Sasha, we closed this gap. Now we have exact characterization. Basically, if the variance of the Gaussian you are smoothing with is bigger than the sub-Gaussianity proxy, variance proxy, then high-square information is guaranteed to be. Then high square information is guaranteed to be less than infinity, and otherwise there exists some sub-Gaussian distribution p for which high-square mutual information is infinite, right? Okay, so again, I think I probably shouldn't spend time on the proof, so let me skip this. Let me just say that the counterexample is what we call dyadic hair comb. So it's a sum of delta functions where It's a sum of delta functions where gaps between this gaps between locations are exponentially increasing. And the weights are basically taken so that you satisfy sub-Gaussianity constraint. So the weights are, so it's like, it's kind of like discretely sampled Gaussian at this exponentially scaled locations. Right. And yeah, so then you just need, I mean, the hard part is to show its case. Need, I mean, the hard part is to show it's case of Gaussian, and then you need to also show that things work out. Okay, let me skip this. Let me rather spend time on explaining what is the summary of after you put our new result with the old result, what do we know about minimax or max worst case convergence rates for this smooth empirical measures if you take supremum overall sub-Gauss case of Gaussian distributions, right? Okay. Distributions, right? Okay, now this is a characterization true in all dimensions, right? So, first of all, W1 and TV are always converging at speed one over square root n. This is the old result. There is nothing new here. Okay, so W2 square is going to be little low, sorry, the capital of will conversion speed one over n or strictly slower than one over n, but never slower than one over square root n. That's a consequence of this. that's that's a consequence of of this proof essentially now so kl same story for kl we always have rate one over n or omega one over n and never slower than one over square root n and high square is either one over n or exactly infinity uh the worst case rate okay and in all these cases right the the the phase transition is exactly we know what exactly is right if if sigma is Right, if sigma is below k, then you're in the bad regime, right? You're not smoothing enough, and if uh if sigma is above k, then you're good, right? Um, okay, so so this is the story, right? So what was unsettling here is that when I was showing this to my friends working on things like maximum likelihood estimation, they would say that, oh, you know, in NPMLE, non-parametric maximum likelihood, there is sometimes this phenomenon that There is sometimes this phenomenon that the rate jumps from one over n to exactly one over square root n immediately without any intermediate stages, right? And that you know, so some people conjecture to us, right, and say, okay, you know, maybe that's the case, right? You guys should work harder and try to prove this sharp phase transition, right? So, and that's the second set of results I wanted to mention today. So, right, so we studied this question, right? So, when rate is omega of one over rate is omega of one over n, little omega of one over n, does it switch to one over square root n right away? And the answer is no. It turns out that you have a gamut of rates. Okay, so our sort of results is the following. So it's only for dimension one, and I will explain in one second why it's only for dimension one. So we prove that, you know, if you give me K-sub-Gaussian distribution, then the rate of convergence is bounded by this. Convergence is bounded by this function, by this exponent. And on the other hand, we construct an example, a cake-sub-Gaussian distribution whose convergence rate is, you know, this exponent. Okay, so, I mean, you can probably see they're not equal, which is very upsetting. And so I'll show you how they compare to each other in a sec. So, yeah, so why is it dimension one? It's because. Mentioned one, it's because so in the previous methods, we never actually constructed any couplings. We used optimal transport to sorry, the transportation information transportation inequality. So we would bound KL and then we would use some form of Talagrand inequality to transport the Kl bound to W2 bound. But here we actually work with the optimal coupling, right? In one dimension, there is only one interesting coupling, which is quantized. There is only one interesting coupling, which is quantile-to-quantile coupling, right? So, and that's what we do here, and basically analyze it, you know, hard. And for the second part, the counterexample is exactly like before. It's this dyadic hair comb. And yeah, and okay, since I thought we are sharing the slides, so I uploaded them. So, I wanted to clarify that O tild here, I'm abusing notation. This is not up to polylog factors, but this is the arbitrary small correction in the exponent. So, that's I just wanted to show that. I just wanted to show that that's the case. Okay, so this is how our upper and lower bounds compare, right? So we know that the rate is always between one half and one, or minus half and minus one, right? And this is the ratio of sigma squared over k squared. So this, our bounds do not match, but they obviously show that there's lots of rates which are, you know, at this, right? So for example, here, right, you're obviously strictly in between minus one and minus one half, right? And minus one half, right? So other rates different from minus one and minus one half are possible here. So that's the funny thing. Okay, so right, and the next question I wanted to discuss is the following. So, okay, so we learned that W2 squared experiences this change, right, from as As the smoothing parameter becomes smaller than the variance proxy of the distribution, right, then suddenly the conversion speed changes from one over n to something slower, right, and the slower rate. Now, for Kl, as I mentioned, our old results already demonstrated the same effect, right? So we knew that when sigma is bigger than K, or if P is finite support, then you always have one over N, right? But once we n right but once we transition this threshold and sigma is smaller than k then suddenly there exist distributions where you're slower than one over n and the natural question is how much slower right okay so if you again actually in the beginning i mean when i was giving this talk when we didn't have these new results i would always say that it's obvious that w2 square and kl should behave the same way here just from transports Here, just from transport, smoothness, kind of, you know, everything. W2 squared basically is almost always when some Gaussianity is present, right? It's almost always the same as Kl, right? So, so it's very natural, right? So, given these results and the ones I showed on the previous slide, to conjecture that KL should also experience a polynomial slowdown, right? At the rate, it will just change. Is there a question? Sorry. No? No. No, no, no, okay, okay, that's it. Just my normal hallucinations. So, yeah, so anyway, so this was right, so this was something very natural. And to our shock, actually, the answer is, I mean, this guess is completely wrong. So, it turns out that KL somehow is much more friendly to this regularization. So, for KL, regardless of, you know, Regardless of relation between sigma and k, the expected regularized and the true convolved measure, the behavior is always polylog over n, right? So you change from one over n when sigma is when sigma is very large, right, to polylog one over polylog over n, right? Again, I mean, this polylog, I mean, it sounds great on paper, right, but in reality, I mean, this is. Paper, right, but in reality, I mean, this is uh raised to power d plus one, so like, yeah, there's no cheating here, right? I mean, all these constants are exponential in dimension, obviously, right? So there's a I'm not claiming that you can somehow violate the laws of nature, right? So this was very surprising to us, right? That somehow, and I thought it's worth sharing with you guys that some funny effect like this happened, but also it led to. Happened, but also it led to a resolution of some functional analytic conjecture. Actually, so here's what I want to mention again. So, there was this line of work starting from Zimmermann in 2013, the article in Journal of Functional Analysis. So, he was, what he did is he proved that if you take any compactly supported measure P and convolve it with Gaussian. Then it must satisfy Log-Sobolev inequality with constant, which only depends on the support of the measure and the amount of convolution. And then this pair of guys, Wang and Wang, in 2016, this is a paper in annals of Poincar√© Institute. So they proved that this also holds for K less than the k. For K less than sigma, right? So, for and for sub-Gaussian distributions P, right? So, Zimmermann proved it for compact support, and these guys proved that LSI also holds for K-Sub-Gaussian distributions, as long as sub-Gaussianity is thin enough, right? And Log's Oblife inequality implies this inequality, right? This transportation T2 inequality, as we all know, right? And one paper. One paper also asked the question: They said, Okay, our proof is so nice and elegant, but how come we can't beat this k less than sigma barrier? Right, so they said we should try to do it in the future, right? But now we can see that surprisingly, right, somehow the stars aligned perfectly for us, right? You see, we proved that if you take q to be the empirical measure regularized by n sigma, right, then in this regime of k bigger than sigma, Bigger than sigma, right? We know this is polylog over n, right? And we know this is strictly slower than one over n, right? Which tells us that there cannot be a finite constant in transportation key to inequality for such measures. And also, it implies that there cannot be a Log Sobel of inequality either, right? So, you know, there's, I thought it was a fun, you know, fun set of results because, first of all, I mean, I like this eigen. Results because, first of all, I mean, I like this high square characterization because high square was never in our radar, we didn't care about it, but somehow it emerged, right? And then there is this magic threshold, k less than sigma. Okay, I guess, I mean, maybe for Gabor, it's natural because he believes that k is just the variance proxy, right? He even gave it this name, right? So, but for me, it would be, because remember, I mean, when I, I remember when I was telling these results initially, right, people say, oh, yeah, I kind of see it because if you have, you know, something like Bernoulli, right? Like Bernoulli, right? You spaced one apart, you need to smooth this by more than one, so you kind of wash this away. No, but this is completely wrong intuition, right? Because as I said, for compact support, you always get one over m, right? So it's only a fact of tails. So don't be confused. Like sigma here means not scale, right? It somehow washes away some, you know, some dust in the tails, right? And then somehow this K over sigma is exactly what was proven by these guys and exactly answers this conjecture. Guys, and exactly answers this conjecture in LSI. So I thought this is a, you know, kind of too many miracles to not, you know, to not be excited. So I am. Okay, right. So yeah, exactly. So that's what I just said, right? So we proved that there is no T2 inequality for T bigger than sigma. I think I'm, yes, I'm out of time. So this is the summary of the results. Again, I already mentioned all of this. And Um and uh thank you questions so if you open the chapter in June and Nicol, then what you will find is that they take sigma that goes to zero as a function of n. I see. So would that be interesting? interesting to check i mean to to see what happens when when when does it become uh uh dimension free for example or there must be some kind of a transition i see i see yeah okay now i now i start to understand why they probably are interested in kde kind of consistency right so that's why right yeah that's right i see i see okay yeah that's right so i think in in my language this would uh translate into a question Uh, translate into a question: How does this all depend on sigma? Right, trying to learn the optimal dependence on sigma here. Yeah, that's a great question. I mean, it would be a miracle, right? If there's dependence when you plug in the correct scaling, right, would recover the KDE rates, right? I mean, I don't know. Yeah, that's a good question. I think we should study this. Thank you. Okay, thanks so much. It's lunchtime here. Lunch time here. Have a good lunch, guys. Thanks for inviting me. Thanks, man.