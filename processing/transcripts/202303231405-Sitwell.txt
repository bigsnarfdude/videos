Frameworks for comparing error covariance estimation schemes. So basically, I'm going to be showing you a lot more plots of triangles. So I'm sorry about that, but they will be different triangles. Okay. So as we've been talking about a lot this week, the air covariances that we use in the simulation system. That we use in the simulation system will have a large impact on the quality of the analysis that they produce. So, at the very least, you're going to need your background error covariance B and your observation ericovariance R. But in general, these are going to differ from the truth. So, you, in general, have just modeled or estimates of these two quantities, which we'll denote by the totas over here. So, you'll actually have. Over here, so you'll actually have a prescribed B matrix and an R matrix. So, because we all know that these are very difficult quantities to estimate, it'd be really useful to have a method that will either diagnose or help you tune these ericovariances. So, for simplicity, we're just going to be talking about tuning the variances only. So, we're going to want to find scaling factors, S, B, and S O that you can apply to. An SO that you can apply to your previous estimates of your B matrix and the R matrix to scale them. That will hopefully lead to estimations that will be closer to the truth. So the methods that we're going to be talking about today will all be observation space residuals. In particular, we have the innovation, which is the spectral here, which equals the observation. Which equals the observations minus the background over here. And what's nice about this difference over here is that the true parameters cancel out, and you're just left with a collection of error terms over here. And if you look at the error covariance of the innovation stig D over here, it will equal to the sum of the B matrix plus the R matrix, at least in the case where the two are uncorrelated. And this is nice because it doesn't have any true. This is nice because it doesn't have any true values in it, which obviously we don't know. But we have this sum over here. What we really want is to tune this B matrix and the R matrix separately. So for these methods, they're going to have to devise some way of separating the two. If we're going to be reliant on the innovation to be able to give you the tuning parameters for the B and the R matrix cell. So, just a quick review of some of the methods that Rouchard mentioned in his talks. So, one of the first estimation methods is usually attributed to Holland Surthing Lombard, which developed in the 80s. So, the big assumption over here is assuming that the observation errors are uncorrelated, or this R matrix is diagonal. So, in other words, you're going to attribute all these. Attribute all these spatial correlations in your innovation errors to the background. So, in this case, you could say do something like making a histogram of the innovation covariances at particular spatial separations, making some sort of fit to it, extrapolating to zero distance over here, and then taking this value and subtracting it from your total innovation. From your total innovation covariance, to then give an estimate of the observation variance over here. So, this way, this extrapolation to zero separation is the way that this particular method is differentiating between the observations and the background over here. So that's nice, except that there's two basic caveats to it. You really are restricted to this assumption that R is diagonal. Also, kind of when you're doing it in practice, So kind of when you're doing it in practice, this extrapolation to zero separation sometimes can be a bit tricky. So another method that was mentioned, DeRosier and Ivanov. This method is from 2001. So this method is really only applicable to variational systems. So if you look at some of the quantities that you calculate, anyways, in a variational system that are A variational system that are parts of the cost function. In particular, the part of the cost function that's attributable to the background and the observations. If you do some math, that could be a bit cumbersome, but in the end, you could get it expressible as part of these big traces over here and here. But then one thing that Richard mentioned in his talk is there's something called innovation covariance consistency. Covariance consistency, which is in the case when your prescribed innovation error covariances equal the actual innovation covariances that you would actually calculate with real, real data. If this is satisfied, then these trace terms simplify a lot because you get cancellation here and cancellation here. But in general, this will not be satisfied because you usually don't, at least at the start, have this innovation. Least at start, have this innovation covariance consistency. But what you could do is derive an iterative scheme where for each of these tilde parameters, the modeled or estimated parameters, you could assign an iterative number towards them and get an iterative scheme such that when you iterate of it, hopefully you'll get closer and closer to satisfying this relationship over here. So, in the case where you're just So, in the case where you're just tuning the error variances, what you get in the end is these two scaling parameters, SB and SO, over here. And in the numerator of both of these expressions, you get this big trace term over here, which looks a bit wild and hard to calculate. But if you're using a variational assimilation system, this is calculated automatically for you. So you basically get these two terms. So, you basically get these two terms at zero cost. However, you can see that in the denominator, there are these traces that involve like the inverse of this innovation covariance and whatnot and these multiplications. And these terms are actually challenging to calculate. There are some different clever methods that you could use, but kind of in the end, this scheme has a pretty high computational cost. It has been used in a couple It has been used in a couple systems, but it's not used that much anymore because of this high computer cost. A couple years later, there's this method of DeRosier et al. 2005, which I'll just denote by D005. And there's a lot of similarities between the previous method that I talked about. It again forms these consistency relationships where if you take Where, if you take particular products in observation space over here, this is the innovation. There's here this particular background minus analysis over here. And you take the expectation value. You get these expressions over here. And again, if you assume that there's innovation covariance consistency, you get a large simplified. You get a large simplification of these expressions over here. Again, in general, this won't hold, but in the same way, you could devise another iterative scheme that will yield a new set of scaling relations for your variances, very similar to the previous method. However, because of the particular consistency relationships that you're looking at here, it just so happens to evolve traces that are simpler. Traces that are simpler and one that you could evaluate a lot cheaper. So, this method of the DO5 has been used quite widely in various different cases because you can compute it pretty cheaply. So, that's sort of the overview of three different methods that we're looking at. But so, now I'm going to ask a couple questions, which is really kind of fundamentally why do we have so many of these different Have so many of these different methods to diagnose or tune these errors. So you might think that maybe it's just down to the numerics, but if you kind of go beyond that, you actually see that they could produce very different results irrespective of any particular numerics that you have in your computation of it. And also, there's another question of, unlike the Hollingsworth and Lungbard method, where it's very explicit to see your Where it's very explicit to see you're doing a particular fitting and extrapolation, when you have these two De Rosier algorithms, the one from 2001 and the one from 2005, it's not really evident how you're taking these innovation covariances and extracting different parts from the background and the observations to get these two different scaling parameters of the variance, one for the background and one for the observations. For the background and one for the observations. So, what we wanted to do for this work is really develop a mathematical formalism that really makes this separation between the two much more explicit and a lot more intuitive, as opposed to just saying, okay, we have this iterative method and answers pop out of it. And we also want to understand the different regimes where for each method, you can expect them to give you reasonable results and Results and maybe fail under some circumstances. So, I'm going to talk about a little bit of a digression and talk about the well-known property of constructing an analysis that has filtering properties to it. So, the first thing that we want to do in this case is we want to transform to a basis that simultaneously diagonalizes the B matrix or the modeled B matrix. The modeled B matrix and the R matrix over here. And so this is a generalized eigenvalue problem. And then we could form this quantity phi over here, which is the spectra of R over the spectra of B. So when I refer to spectra and modes in this context, what I'm talking about is the eigenvalues and the eigenvectors of this generalized eigenvalue problem. Generalized eigenvalue problem. So with this phi, you could then go and construct these filtering operators, which I'll denote by f and i minus f. So this operator f over here, what it does is dampen modes that are prominent in R as compared to B. Whereas the complementary operator I minus F dampen modes that are prominent in B as compared to R. So just illustrate. R. So just illustrating this with a 1D periodic domain, in spectral space, the operator F is this blue line over here, where in this case, I'm just assuming that the observations are spatially uncorrelated and the background has just some correlation like that I just made up. And the I minus F operator is this representing the spectral space by this red curve over here. By this red curve over here, which, as you can see, is this red curve filters out these low wave numbers, and the blue curve over here filters out the high wave numbers. And so you can write your analysis equation in terms of these two operators, and then we can understand the analysis in terms of this filtering operation. So, but what we're going to do in this case is use these two operators f. operators f and i minus f on the innovation covariances themselves. So in particular, we're going to take our innovation error covariance d over here and operate it with f, which will give us something that I'm denoting by dB. So I'm just putting this b over here because this f operator should retain the modes that are prominent in b but not in r. And then we can also But not an R. And then we could also operate with this operator here, I minus F, to give us what I'm going to denote as DR, just given these formal definitions over here. You can see acting with F and acting with I minus F. And if these filters are very successful at separating the parts that are just due to the background or just due to the observations, then this. Then this DB and DR parts should be approximately equal to either the background or the observation error correlations. So again, I have an example of a 1D periodic domain where I've specifically picked filters that are efficient at doing this separation. And we could see that our, at least in one location, we have our B matrix over here with the blue. Over here with the blue curve, which is very close to this red curve over here, which is this filtered innovation covariances for the B, and they're very close to one another. So in this case, this is an example of filters that are very successful at separating these parties. Just one more mathematical digression. So, to make some of the math a bit simpler, it will be It will be helpful to introduce the concept of vectorization of matrices. So, if we have a matrix over here, A, which is just formed by a bunch of columns, the vectorization of A just takes all these columns and stacks them on top of one another. So we get this basically this super column vector over here. And what's nice about this is being able to picture all of the colours. Picture all of our matrices, these error covariances, as simple vectors over here. So we could have this diagram of this relationship with the innovations over here of the error variances, D equals the B matrix plus the R matrix, as just this simple triangle over here. So we can see reintroduction of triangles. And this will allow us to visualize what we're doing with some of these algorithms. We're doing with some of these algorithms better. And so, one last part of mathematics that I'd like to introduce is the Frobenius inner product, which all it is, is the inner product between two matrices A and B here, which is just the simple dot product of its two vectorizations. So using by defining this inner product, we could then also define angles between error variances as well. Air variances as well. So now I'm going to go and return back to our error variance scalings over here, which I just brought up one over here. So for the 2001 De Rosier method for the observation scaling, here I've written it actually in terms of this vectorization over here. And at first it kind of looks kind of unwieldy. Kind of unwieldy. You can see that I've written it in terms of these filtered innovations, the R in this case. We have over here, this is a Kronecker product between the two, which just takes every possible product between the two matrices to make this larger matrix. But as I've showed over here, if we just generally write down, say what we're doing is What we're doing is trying to fit a model m over here represented by this matrix as a function of some parameter x, and we get observations y over here associated with some correlation over here. Just, you know, very simple model. If we ask what the linear least squared solution is, it's this over here. And why I bring this up is this is the exact form that I've written, this variance. That I've written this variance scaling parameter is over here. So, if we then use this to just interpret what all these terms are, we might call this term over here the observed data vector, because this is the actual observed or measured innovation covariance, then filtered to only take out the R modes over here. And over here, we have the analogous modeled quantity, as you can see, because it's denoted by this. Because it's denoted by this tilde over here, and then some sort of error covariance in between the two. So, what this allows us to do is really see this scaling factor over here as part of some sort of fitting parameter. So, we could then associate a cost function to it such that when minimized, it yields this solution. Yields this solution. So over here, we've associated this cost function to fit with our scaling parameter SO over here and the model value of the filtered innovation covariances with the same quantity that's actually measured. So you can see over here I've just written what we often see in the actual cost function that you use for the minimization in a simulation system. In an assimilation system, just for instance JO over here, you see a very similar pattern that you have this quadratic term over here that you're trying to minimize and an associated weight that is, in this case, just the inverse of the R matrix. But what this weighting over here is it's the inverse of the correlations that's associated with this problem over here. So we have the exact same. Here. So we have the exact same form over here, too, except our weighting over here is a bit more complicated. It is the inverse over here of some covariance over here, but it's kind of a bit more complicated term. So we're just going to ignore about this exact weighting, about the specifics form of it right now, but we're just going to get the overall impression that these are part of some sort of fitting algorithm we have trying to fit models. We have trying to fit model to observe quantities of these filtered innovation covariances. And we could do that with both of the innovation, both of the scaling parameters for the backgrounds and the observations of both DeRossi algorithms from 2001 and 2005 and derive associated cost functions from them. So we can also then by So, we could also then, by changing the weighting of these products just a little, we could write them in terms of dot products, in terms of the unfiltered measured innovation covariance, and its analogous modeled quantity. So in the case of the 2001 algorithm, if we use this weighting over here, we can see that this scaling factor equals the projection of this innovation. Projection of this innovation covariance onto the direction that the B matrix is pointing in. And also the same thing with the R matrix. And so this scaling factor over here, for instance, if we then introduce our triangles over here, which we could see the modeled innovation covariance is equal to the B plus the R, the modeled quantity projection onto the direction of the B matrix. The direction of the B matrix is this length right over here. Then, if D over here represents the actual measured quantity that we have, its projection is over here. So this variance scaling over here, what it's trying to do is take this triangle and stretch it over here. And also, for the observations, what it's trying to do is take these projected quantities and take this triangle and kind of stretch it over. Triangle and kind of stretch it over here so you have this triangle over here that's more similar to an actual triangle that has its hypotenuse of the actual measured innovation covariance. But for this particular example, I just happened to write it that the angle between the B matrix and the R matrix forms a 90 degree angle. But in general, this might not be wrong. But in general, this might not be true. You might have a triangle that looks like this. So, what's unfortunate about this example is if you try stretching your triangle in this direction over here to make the observation error covariances consistent with one another, that's going to affect your scaling for the background error covariances. So, you have some sort of interdependency that is not actually desirable for in the case where this dot. For in the case where this dot product over here is not equal to zero. So the way we're going to understand this angle between the air covariances is with this idea of spectral distinctiveness of these two filters. So returning to our example of the 1D periodic domain with the spectral example of the F and I minus F over here. And I minus f over here, we're going to introduce another quantity in green over here, which is just the product between these two over here. So I'm going to call this the spectral overlap of our two filters over here. So you can see that for modes over here, the high modes, we could pretty unambiguously attribute all these modes to the observations over here because that's its filters basically at one. Its filters basically at one, and similar to these low modes over here, attribute them just to the background over here. But this green function is basically where we have an overlap between the two, or in other words, modes where we unambiguously can't attribute them just to the background or the observations. So if we integrate over this green curve over here, we get an integral that we can actually Integral that we could actually relate it to an actual angle between the B matrix and the R matrix, which I'm going to call theta over here. And when I actually go and calculate all these quantities, for this particular example over here, we get an angle that's very close to 90 degrees, which I calculated, to 88 degrees. So, in other words, for these two error covariances, we could consider them being very close to being perpendicular with one another. But if I then go Another. But if I then go and shorten the background error covariance length over here, so it's more similar to our uncorrelated observations. We see that there's a lot more wave numbers over here that the overlap is non-manageable between. So we can see that all these wave numbers over here, we may not really know how to attribute these to either the background or the observation. Or the observations over here. So when we go and calculate the associated angle in between the two, we can see it's 78. So, in other words, these two error covariances are further away from being perpendicular to another. Let's see if these email messages popping up. I'm sorry about that. So, we can also see that with these angles phi that we've calculated over here, they are directly related. They are directly related to how good our filtered innovation covariances are at minimicking the actual error covariances of our respective term. So here for the background error covariances, we can see that for this particular example where the angle is close to 90 degrees, the filtered innovation covariance does a very good job at approximating the blue curve over here. But for this example, Over here. But for this example over here, where it's further away from 90 degrees, the approximation is worse. You can see that these two curves are not exactly close to one another. So we can actually then go and calculate statistical properties of our various error covariance scaling terms. I'm really not going to go through most of this. You can calculate biases. Most of this, you could calculate biases and variances of the algorithm. The only thing I want to point out is when you actually do this, you could explicitly see that the bias actually depends on this inner product between the B matrix and the R matrix, or in other words, the angle between the B matrix and the R matrix. So using these statistical properties, you could actually go and then calculate expected performances for each one of the algorithms. Each one of the algorithms. So I just invented two different cases over here on this 2D grid over here. I'm not going to go through the details. The only thing I'm going to do is point out that in one case, there's an angle close to 90 degrees, one case where the angle is closer to 0 degrees. And the only thing I want to highlight over here is if you calculate the root mean squared error of your two scaling parameters over here. Parameters over here. The expected RMSE is quite low in this case, where the B and the R matrix are close to being perpendicular to one another. But when they're close to being parallel to one another, you can see these RMSCs are a lot higher. So you're going to get a lot worse, poor performance from this scaling algorithm in this particular case. So now we're going to go and return to those sort of weird. To those sort of weird weighting factors that you had in this fitting that I talked about before. So, in general, you could say, well, what if I don't want those two particular weightings that we had before for two algorithms? And I'm just going to call them W1 cross W2. So this would give you what I call basically a generalized algorithm that the two different algorithms, DeRozier, Ivanov, 0, 1, and DeRozier. Ivanov 01 and DeRozier 05 are part of this larger class, and these are just sub-algorithms. And each algorithm of these DeRozier algorithms are really just defined by choosing particular weightings in this fitting that we were doing before of the filtered innovation covariances. So we notice that we could just choose kind of whatever weightings we particularly want. In particular, if I choose this weighting and this If I choose this weighting and this weighting over here and over here, this gives us an algorithm that will satisfy the chi-squared diagnostic that scales both the observation and background error covariances equally. You might then ask, well, at least from a statistical point of view, what would then be the optimal weighting in this minimization problem that we have? So at least in the case, if your innovation is Your innovation is drawn from a multivariate normal distribution, the sample covariance of it would follow what's known as a Wishart distribution, which has this form for the variance of that estimator right here. And what I would like to point out is for each of these methods that uses these particular weightings, that on face value looks kind of strange and everything, you know, why this particular term. You know, why this particular term, why this? It actually looks pretty close to what the ideal statistical distribution would give you, in that it is some sort of product between these covariance matrices over here. And at least when you look at a lot of cases, what's the ideal weighting over here is actually not that far away from these particular weightings in these different algorithms. So, why these algae? So, why these particular weightings that show up here that look a bit odd? Really, it's just to make algorithms that you could compute conveniently. No real other reason than that, but it does kind of follow pretty close to what an ideal weighting would be from a statistical point of view. So, because I'm short on time, you could then go calculate the biases of various different algorithms. The biases of various different algorithms. You can see when each algorithm can be better than the others, when it might fail. And in particular, I would like to point out that it is dependent on the spacing of the observations over here because that angle that I calculated before, theta between the B matrix and the R matrix, is actually we've sort. We've sort of given a more intuitive, or at least what I hope to be intuitive, understanding of what's happening in these two different DeRosier algorithms in that it's fitting different forms of the innovation covariances that apply some sort of filter to try to extract out the parts of the innovation covariance error that's due more to just the background or do more to just the observations. This fits. Observations. This fitting is explicit for algorithms like the Hollingsworth and Longberg method, but it's implicit for the two different DeRossi algorithms over here. And conceptually, the O1 and the O5 algorithm, they really only differ from a conceptual point of view by this weighting of the cost functions that we use in the associated fitter. But from a practical point of view, they have different numerics and different ease of computations that, in practice, Computations that, in practice, makes the difference between these two algorithms very important. We could characterize the performance of the two different algorithms based on quantities that we could interpret geometrically, like the angle phi between the B matrix and the R matrix, and we could also get analytical results for the scaling statistics. So, I apologize if I had to just really rush through this. It's kind of only It's kind of only kind of scratching the surface of a lot of the different interpretations that we found through these different air covariance tuning algorithms. So if you want to learn more about this or if I just went too fast through it, I encourage everyone to look at the paper that I wrote with Richard given over here because you can make lots and lots more triangles that you can then go interpret. That hopefully gives you more of an intuitive. Hopefully, it gives you more of an intuitive understanding of what's happening in all of these error covariance algorithms. So, I'll just end it right here. If you go back to slide slides and see what I have seen as a question, when you're looking at these curves and When you're looking at these curves in particular when you're comparing where a filtered matrix D background is projected at all, that the filtered one is always underestimating the other curve, and is it true? So I would just like you to talk about the difference between these two panels. Under I see the under approximation, it's can you talk about how Zmander relates to correlation length or Yeah, so I guess I was really kind of rushing through it. So basically, because of the particular example that I picked, which has the observation errors spatially uncorrelated, this one over here has a larger correlation length of the background. So we can think of this as it being more spectrally distinct than the observations, which leads to them being closer to being perpendicular to one another. Closer to being perpendicular to one another. Where in this case, I've made the background error correlations shorter so it's less spectrally distinct, which makes this worse fit parameter, and they're further away from being perpendicular to one another. So this basically gives you how bad of a mismatch they are to one another, because when you short in the background here, correlate. Short in the background error correlation length. It's less spectrally distinct than the observations, at least in this case that I just made out. To follow up on that, would you say this is a different light or dependent on the difference in the lights? Well, in this case, there's basically different views. The difference in the air correlation length causes. Air correlation length causes the difference in the angle. So the angle really is, in this particular example, a quantitative measure of how different the error correlations is in a way that directly relates to the performance of the algorithm. So if they were the exact same, the angle would be zero. So there's like a one-to-one correspondence in this case.  