Duke, the Department of Electrical and Computer Engineering. Hans received his doctorate at the University of Pennsylvania and joined Duke as a postdoctoral associate in 2022. He works on a diversity of subjects, including control theory, machine learning, multi-agent systems, topological data analysis, and algebraic topology. That sounds a lot more impressive than it is in reality. So I guess the goal of this talk is, as I said, it's a hodgepodge. Hodge Podge is that there'll be something for everybody in this talk. So, we've talked a lot about sheaf theory yesterday. We've talked about social choice theory. And so, I'm going to explore both of those topics and maybe try to elicit some discussions in the future that can draw some connections, some new connections between. Connections, some new connections between those. So I'm going to begin with sort of a little crash course in lattice theory. And by lattice, I mean an order lattice. So it's a partially ordered set with two binary operations, which is the greatest lower bound and least upper bound. Then I'm going to describe how you can view preference relations and choice functions from a lattice theoretic point of view. Theoretic point of view. And I will do the same for notions of knowledge and belief. And I'll have to define a very specific logic in order to kind of translate between the lattice theory and what knowledge and belief is, one interpretation of it. And then I'm going to talk about some of my PhD work, which aims to develop a high Aimed to develop a Hodge theory for sheaves on networks valued in lattices. So instead of vector spaces being the stalks of these sheaves, lattices are now these stalks. So they're kind of a combinatorial flavor. Even more combinatorial than network sheaves, but you're making the data combinatorial as well, and not just the base space. So to begin with, So to begin with a question, we've seen a lot of different Laplacians over the course of the past three days. We've talked about non-backtracking matrices, Hodge Laplacians, continuous versions. We saw a Laplacian that took curvature into account. So there's all these different kinds of Laplacians. And I'll share my perspective for Laplacians on networks. Networks, you know, fundamentally it's a diffusion process. So here's sort of a little illustration. And I have a video for this, but I couldn't get it to play on Beamer for some reason. But we have all these nodes, and they start with a color, and you can imagine they're passing these messages between all their neighbors, and maybe they're averaging their colors, maybe they're taking the maximum of their colors, but they're taking some kind of aggregation. Taking some kind of aggregation operation locally, and eventually maybe they'll all have the same color, or maybe there'll be another kind of equilibrium that we can interpret. So this is sort of the paradigm. So what is a lattice? A lattice, as I said, is a partially ordered set. So partially ordered means it's symmetric, transitive, and non- None, so x is less than y, less than or equal to y, and y is less than or equal to x, that means x equals y. That has these two binary operations. So x join y, which is the minimum, so it's the least upper bound, and x meet y, which is the greatest lower bound. And in lattice theory, we always use these nice simple. Use these nice symbols, these wedge and V symbols. And I guess it could be a little confusing if we're trying to do Hobbes' theory with lattice theory and we might have all these exterior derivatives in comparison, but just remember that these mean essentially taking a maximum of two things and a minimum of two things. Or if the max and min don't explicitly exist, you're taking the soup or the inf. The soup or the inf. But we also assume that these lattices have like a smallest element and the greatest element. And if you know these partially ordered sets are infinite, we can talk about meets and joins of sets and not just of binary things. So this is, we say a lattice is complete if these joins of some set of elements exist in the meets exist as well. As well. How large is your set of indices? That's a really good question. So there's probably some kind of set-theoretic things that you don't want. But in practice, it's probably like countable in practice. I haven't really thought about the case where it's uncountable, but yeah. Okay. Yeah, so. Yeah, so we're sort of defining a category or sort of a data structure of lattices, and we want to be able to compare two different lattices with maps. So these are lattices or more generally just partial order. We say a map is monotone. If x is less than y implies f of x is less than f of y. But if we have these lattice structures, we probably want more structure to be preserved than just Structure to be preserved than just the order we want, you know, either the meat or the join. Sometimes we want both, but in this talk, we'll mostly focus on join-preserving maps, where if you have F applied to a least upper bound of a bunch of guys here, then it's equal to the least upper bound of all the the images. And just as a consequence of this, and this is like the empty set, then you also require sending the bottom element or the least element of the loudest view. Or the least element of the loudest speed, zero. And a very kind of contrived way, or potentially contrived way, a reason for making this assumption is that in terms of category theory, this is really saying this map was preserving co-wimments. And if you don't know what that means, it doesn't matter at all. But one theorem that comes out of that really is if you have a joint-preserving map for If you have a joint-preserving map from a lattice K to a lattice L, then there always exists this map which I call F sharp, which goes from L to K such that if there is an inverse of F, then it will be exactly equal, but otherwise it's sort of the order approximation. So if you compose F with F sharp, you're sort of an inflationary map. Inflationary map, and if you compose F sharp with F, you're a deflationary map. So you have these two properties, and there's also equivalent characterizations as being like agile in a sense. So, you know, one intuition you might want to have is that this F sharp is kind of like the Moore-Penrose inverse in linear algebra, in the sense that it approximates. Sense that it approximates an inverse, but it's also kind of like the linear adjoint in linear algebra. So it's not really either one of those, but as an analogy, we can think of what would Hodge theory look like on lattices if this were the adjoint of something were replaced by these sharp operators. So kind of shifting gears to So, kind of shifting gears to a more economic flavor, you know, I'm going to argue that preferences, preference relations, can be viewed as elements of a lattice. So what are preferences? It's a binary relation. And various people have come up with definitions of what are rational preferences. What someone who's a rational actor A rational actor, what axioms should a preference relation satisfy? And Candith Arrow in his social choice and individual values came up with these three axioms. One, that you either prefer X to Y or Y to X, which he called connected. Unfortunately, they call it complete now. They call it complete now. Some people use this term complete, which is confusing because I'm also talking about complete lattices, which is not the same thing. And then reflexive, saying that these preferences are weak, that you prefer x is preferred to x for all x. And then you have this transitivity axiom, which some people challenge. So people challenge. And then some notation. So these axioms together imply that you have a set of alternatives, which is things to choose from. You know, what restaurants you're going to, for instance. It could really be anything. It could be like rankings of competitive nature, right? But we have this notation that the set of all orders on a set of X is A set of X is called ORD of X. And we also have a notion of a preference profile. So instead of individual preferences, we want to consider group preferences. And it's just a tuple of preference orders for agent I. So X, P, I, Y means that voter I or consumer I or what have you prefers alternative X to Y. I can't help. I can't help but mention the most famous result, which is the arrows and possibility theorem. I know we had some questions on the Slack about this. So I think this is, you know, if you view these as relations, so they're just subsets of the Cartesian product of X with itself, right? So if we view these as relations, I think you could state it pretty simply that the three axioms are. That the three axioms are weak Pareto. So if for all i, so every agent prefers X to Y, then the sort of aggregate order on the alternatives, X is preferred to Y in the aggregate. So that's Ric Pareto. IIA is that, so if you take a preference profile pi, and you just restrict it to two particular alternatives, X and la, and this is true for any. X and Y, and this is true for any two alternatives. Then, when you aggregate this preference profile, it should equal this preference profile. And I think there's a little bit of a technical thing here where maybe this isn't equal, but they're like, you're indifferent between this and that. But I don't know if that's that important. And then finally, this axiom, which is what we kind of want to avoid, and this is why it's a paradox, is that a problem. Is that a preference aggregation function f is dictatorial if there's some j such that essentially it's a projection onto j's preferences. So there's one, and maybe j doesn't even know that it's a dictator, but there's a single, it's a projection onto a single thing. And another axiom, which is kind of weaker than dictatorial, is called oligarchic, where there could be a subset. There could be a subset of agents and it's somehow a projection onto the subset. So the theorem says that any social welfare function, okay, I probably should have said this is called a social welfare function, that satisfies weak Pareto and IIA is necessarily dictatorial. So this is a sort of a disappointment that if you Disappointment: that if you were to take these axioms as being the pinnacle of what you would want in a voting system, then there really isn't any voting system that's consistent. But I think, as I sort of argue, a lot of these axioms are kind of ill-founded, or at least they don't apply in every single circumstance. For instance, you know, completeness doesn't really take into account Doesn't really take into account that some you could be indifferent between two different alternatives, but also maybe you haven't encountered having to choose between one or another or having to rank it. So I argue that indifference and decisiveness should be able to be distinguished. That's what was argued in this paper. And also, as I don't And also, as I don't really focus on transitivity, but there's been, you know, several, there's a famous study of Staversky, and there's probably a lot of other studies that show that human preferences are not actually translative. And I think there's like a particular number. Like if you have about five alternatives, then we're mostly translative. But as you get as many alternatives, it's very hard to kind of keep your preferences transavailable. So here's the lattice structure. So it's kind of obvious, the first one is obvious, that if you allow arbitrary preference relations, so these are just arbitrary binary relations on X, then you have a lattice structure, and the meet is the intersection of preference relations, and the join is the union of preference relations. Okay, I haven't really said anything interesting. That's just the power set of the cartoon. That's just the power set of the Cartesian product of x and max. But what might be more interesting is if you require your preference relations to be transitive and reflexive, but not connected. So you can't necessarily compare all alternatives. Then I think there's this very interesting lattice structure where the beat is still the intersection of these relations, but the join is the transitive closure of this. Closure of this these two. So I might kind of draw a little example here. So if you have alternative A and alternative B, or C is here. And you prefer C to A and C to B and then I want to take a join of that with an want to take a join of that with another preference relation where I prefer B to A. So let's say, you know, this was my preference relation. I prefer Indian to both pizza and burgers, but I don't have any preference relation between pizza and burgers. Relation between pizza and burgers. Maybe it's not like in the cards. Like, I'm not going to have to decide between those two in that situation. And then maybe you prefer burgers to pizza. So how would we take the join of these two preference relations? Well, so if we took the union, we would add this arrow here. Well, I guess this isn't a good example in the sense that you already have transitivity. But in general, like if you had, you know, like D or something, and this was A or uh should have worked this out. Then you would add this thing. So what I'm working on a project right now with an economist who studies these kind of things to try to model certain group dynamics based on the Group dynamics based on this Wildis theoretic notion of preferences. And one thing you might want to consider is, you know, how do you define dynamics of preference relations? So it is fairly well known that preferences do change, or at least they can change. And one way to model preference change is with a dynamic Change is with a dynamical system. And so suppose you have this dynamical system phi, which takes all these transitive and reflexive preference relations, so a preference profile, to a new preference profile. So you keep updating preference profiles, and you might ask this really simple question. So if you have this discrete time dynamical system, you start with some preference profiles. You start with some preference profile, and you have phi is monotone with respect to this lattice structure here. You might ask, can you characterize the equilibria of the system? I mean, the first question is, is there an equilibrium? And it turns out that there's this very famous result in lattice theory due to Tarski, Alfred Tarski. And it says that if L is a complete says that if L is a complete lattice and F is a monotone map from L to L, then not only is there a fixed point, but the fixed points form a complete lattice in and of itself. So in this case, we know that if we have this dynamical system of preferences, that the equilibria form a complete lattice. This is just one possible direction of how One possible direction of how lattice theory could help enhance the study of preferences. And I'll also say that Manjard and others have studied choice functions from a Lattice theoretic point of view. So a choice function on a set of alternatives is a map from the power set of x to x, and you're required to And you're required to take in a set of possible alternatives to compare. And a choice function satisfies the property that when you choose something out of A, that you're contained in A. So you might not choose a single thing out of A, you might choose multiple, but you're narrowing down the possible alternatives. And you can see that if you look at all the set of choice files, The set of choice functions on a lattice, then you can take unions and intersections of these functions. And preferences and choices are related, so there's a nice correspondence between given a preference relation, you have this sort of canonical choice function and vice versa. Yeah, so now knowledge and belief are also important. Knowledge and belief are also important in economics, especially in game theory, where you might have asymmetries between knowledge. And understanding that is very important. So one approach you might have is what I might call multi-agent epistemic logic. In epistemic, I mean the logic of knowledge. So suppose you have multiple voters and you have a Multiple voters, and you have a set of atomic propositions, phi. So the language of this logic consists of the following symbols. We have these propositions phi, kind of the tautological truth value, true. And we can take, you know, conjunctions of formulas, negations. And then this is where it gets a little interesting. KIP means that I agents. phi means that agent I knows that phi is true. And similarly, you know, not KI, not phi, you could interpret as agent I thinks that phi is possible. So it's a notion of possibility. Usually in modal logic you have this, you always have this sort of duality between one modal operator, which is, in this case, is knowledge, and the other is possibility. And depending on the additional axioms you place on this logic, you might encode something that's closer to knowledge or something that's closer to belief. So for instance, in knowledge, that if I know something is true, then that should imply it's true, because you can't know something that's false if it's knowledge. But in belief, that's not true. You can believe something that's false. So in belief, False. So, in belief, you probably have to impose a different axiom that you can't believe nothing, you have to have something to believe. And so the semantics of this logic consist of a set of states, S, and a binary operation, case of I, which encodes which states can be known in relation to the state. Can be known in relation to others. So, this is called a Kripke model where you have all this data of states, binary relations for every agent, and atomic propositions in this map mu which takes a given state and it specifies which atomic propositions are true at that particular state. Are true at that particular state. Sometimes this is called an evaluation map. And so, in the very simple case of like Boolean logic and forget all these KI's, this would just say which propositions are true or false. So, the semantics is defined inductively as follows. And nothing is that interesting except for the last line, which says, interesting except for the last line which says that given the model the state satisfies agent i know phi if t satisfies phi for all t that are reachable within from s. So this is really what is encoding knowledge in this logic. And so one thing that thing that you know we observed is that there's this natural Galois connection so I should have probably said what a Galois connection is but I know wrong wrong way so these so these uh this map F which is join preserving and this map F F sharp which is sort of the adjoint like operation Which is sort of the adjoint-like operator. They form, as a pair, they form a Galois connection. Yeah, so I should have put that here. The Galois connection consists of a pair of maps, which consists of a joint-preserving map and its adjunct. So you have this Galois connection here, and this essentially, it sends this semantic content. Semantic content of a formula to the semantic content of agent I believes phi is possible. So you have this really nice correspondence. And I should say by semantic content, I mean the set of states such that satisfy a kit performed well. So far we've sort of shown how, oh, so you know, why lattice? Why lattice theory? Well, these are both lattices because they're power sets. And we can use a lot of facts about Galois connections to try to understand this relationship between a formula and KI phi. So we can go back and forth between semantics and syntax. Now this may seem entirely unrelated. Now this may seem entirely unrelated, but it actually is. So, you know, going back to the beginning where we define these lattices and Galois connections, you might think about how we might define Hodge theory for networks. But first we need a kind of, you know, a data structure where we can talk about lattice value data over a network. And I think the most convenient one. Convenient one, unfortunately, because there's a lot of kind of lingo and a lot of language involved in sheaf theory, is a sheaf. So just as sort of a review, we have this network, and for every node and edge, we have f of i, which is a lattice in our case. So it's not a vector space, it's a lattice. F sub i j is a lattice. F sub ij is a lattice, F sub J is a lattice. And these maps, F, I, include Ij, are no longer linear maps. I mean, they could be in some cases, but they're maps that preserve the joints. So they're joint-preserving maps here. And so this is called a lattice-valued sheaf. And yes, the lattice value, here's the definition: a lattice-valued network sheaf is a data structure that assigns. Data structure that assigns a lattice to every node of the network, a lattice to every edge, and join-preserving maps for every i including into an edge. So a joint-preserving map from F sub I to F sub ij. And it also does the other, the head. The head and the tail. There's a joint-preserving map for both of these inclusions. This is kind of the opposite definition of Joe's definition of sheaf. And you might ask, why is this a sheaf? And it really depends on what topology you put on this incidence. Incidence incidence postect. So if you like, so you have like a network here I JK So the incidence postet looks something like this. Something like this. K includes into this JK. J includes into here J. So this post set looks like a zigzag. So this is like P sub J is the post set. Then the topology can I call Alex. might call Alex a P sub J, so the Alexander topology. So it has opens that consist of the following, basic opens. So sets of, I'm just going to say simplicies, because they could be edges, or, you know, such that I guess I'll call this the basic open of tau, okay? Such that Such that um Al is a face of so if you want to look at the topology if you look at the open around J, you look at everything, so not including this actual node, but everything here. So it's the open star. And if you want to look at the basic open around JK, it's just this part, so not the nodes. Same as this. Get the whole topology by taking all the, I mean if you're assuming the graphs are finite, you just take all the unions and intersections you could possibly take. And then it's a result of Justin and Kirby that if And Curry that if F is a sheaf, oops sorry. So basically it says the category of sheaves from the Alexandra topology to some data category D. So in the case of, you know, this works. Of this work that I'm presenting, this is the category of complete lattices of showing preserving maps. In the other talks we've seen today, this is the category of vector spaces. There's other categories you can think of, just like sets, for instance. So you don't care about the sort of algebraic structure. So the result says that the category of sheaves, I should put op because these kind of functors are contravariant. And if you don't know what that means, Are contravariant. And if you don't know what that means, the idea is that if you have an inclusion from this open set to this bigger open set, that the map actually goes from the sheaf on this bigger open set to the smaller open set. So the arrows are reversed. So this result says that if this is a complete category, then this category of chiefs on this Alexander Papal. Chiefs on this Alexander topology is the same as the category of functors from this post set to D. And that just means that whenever you have an I including to Ij, then you have this induced map from F of I to F of IJ, which is what you see here. So basically, long story short, So basically, long story short, it doesn't necessarily matter that these are sheaves or not, because the notion of being a sheave is equivalent to just being this map that sends data structures over nodes and data structures over edges, these compatibility maps. So that's how you might kind of reconcile why these things we're talking about are sheaves. Yeah, so just to continue. To continue, a zero code chain is an assignment to the sheaf. So you have this data structure. You could say if a sheaf is a data structure, a code chain is an instance of that structure. So it's a choice of an element of f of i over every node. And we say that a zero code chain is a global section if it's in the following solution set. Solution set. So it's a set of tuples of lattice elements such that if you apply this restriction map, which remember is a joint preserving map to X of I, and you apply it to X sub J for an edge ij, that you have agreement here. So in the very simple case, we still have this notion of a constant sheaf. So, you know, choose a lattice. It could be your favorite lattice. So this is sort of less than or equal to relationship. And you have like a network here. So the constant sheaf is what you would think. Is what you would think it is. It's the identity map here, and it's the same lattice over every node of the network. So now we've defined lattice value T. You probably could guess what I'm going to do next. I'm going to define a Laplacian of Latta. A Laplacian for Lattice White Sheeps, which in a way is kind of like the Haas Laplacian. And we call this La Laplacian the Tarski Laplacian because this Tarski fixed point theorem that I talked about earlier guarantees the existence of harmonic co-chains. So that's why we call it a Tarski Lepos. So here's the data. Suppose we have an undirected graph or a directed graph. Or a directed graph, but that's just describing kind of an orientation. But orientation actually doesn't matter in this case. So just an undirected graph. And a Lattice value network sheaf over G. Tarski opposition is an operator that goes from the zero code chain, so this is the product of all the stocks of the sheaf F, to the product of all the stocks. So it's an operator. So it's an operator, and it's given by the following formula. So maybe it's better to sort of draw what's going on. So we have this node I, and we're going to look at what happens at node I. We have these nodes that are neighboring I. So instead of taking this map, let's say this is J, J including T, I, oops, sorry. We actually take the adjoint of that map. So we look at all of our neighbors and we pull their data, so their data from J, and we pull it to the edge. So it's kind of like a message passing scheme. So we pull that data to the edge. Or sorry, we push it to the edge here. So this is the Fj including Ij. And here's the data XJ. And then we take the adjoint of the restriction map, I forget this all messed up, adjoint of the restriction map here, which is F sharp of pi including into Ij. IJ. So we do this, and this might look something like parallel transport. And maybe I'll say something about that in the discussion or whatever. And so we do that for all the neighborhood edges, and we have various data here. And we aggregate all that information by taking a meet of all this data. Meat of all this data that we push to the edge. So it kind of looks like the graphical question in the sense that we're aggregating this data from all the neighbors, but instead of taking a sum of things, an alternating sum, we're taking just a meet. Because after all, the category of lattices is an abelian. We have no notion of adding things. Or more importantly, there's really no notion of subtracting things. There's really no notion of subtracting things. So you can't define cohomology and things like that in the way that you'd want to. So our approach, as we'll see in the next slide, is to define cohomology using the fixed point theory of this thing. Put a lot of names, but it's more like an adjacency matrix. Ooh. I mean. Good question. Let's talk about that. Oh, it's a very good question. Yeah, so here's kind of the main theorem. Alright, so suppose we have a lattice-valued sheaf, and L is the Tarski-Laplacian. Then the fixed points of identity meet L. So, which I really mean sort of the set of X. of the set of x such that L of X meet of X equals X. You could also write that as the set of X such that L of X is greater than or equal to X. So the fixed points of the identity meet with equals the global sections of the sheep. So why is this Of the sheaf. So, why is this like the Hodge theorem? Well, so in the case of sheaves of vector spaces or any abelian category, the degree zero cohomology is equal to the space of global sections. So, and in particular, if we can write sort of the kernel of the Laplacian as the fixed point. As the fixed points of I minus the Laplacian. Which may seem like I'm cheating or pulling some dirty trick. But after all, I'm allowed to because I'm only claiming this is an analogy, for now at least. So the Hodge theorem is the fixed points of I minus a delta is equal to the cohomology. And this is kind of like the fixed points of I. kind of like the fixed points of I meet L equal to the cohomology. So I mean the first question is we could define degree zero cohomology as being the fixed points of I mi L. And you could read more about other approaches in our paper of defining In our paper of defining cohomology and where things break down. So, things break down if you try to do it in a traditional way with boundary maps. And we kind of explain exactly how that works. But the next question is, what about higher degree cohomology for a network? And maybe we have a sheaf over a simplicial complex or a cellular complex. What is cohomology? What are the different Laplacians? What do they look like? Different Laplacians, what do they look like? And just as with the Haj Laplacian, we have a heat equation, we have this harmonic flow, and we can do all these amazing things with it in signal processing and extensions thereof. We have a heat equation defined by this discrete time equation. Equation. And so another consequence is like the time-independent solutions of this equation. Assuming that we have these, that these lattices are nice enough. So for instance, if they satisfy these technical constraints called the descending chain condition. So if we can't keep going down in the lattice infinitely, well, we have to hit the bottom eventually, then we can say that. Then we can say that this recursion terminates in finitely number of steps. So we can compute, we can choose some initial condition and then harmonically flow to a global section in a finite number of steps. And in practice, it's like very quick. I mean, almost too quick, because often we, you know, when we're meeting a lot of things together, we end up killing things. Killing things. So, one extension of this is maybe doing this asynchronously and maybe meeting a few of these and then updating and there's all kinds of things. The real line is now totally already set. So the LX join X is the mass of my temperature and my neighbor's temperatures. Yeah, I should definitely say that. So let's go back to here, right? So if Joel starts here and I start here, then I'll look at, I'll update with, I'll take my state here, which is Xi, and J is control, Xi here, and here's Xj. and choose xj and I'll update that with xi meet xj which is join xj or actually is meet so it would be zero here. I'd go like kind of in here, right? And you would do the same so we actually in this case reach consensus. An important point is that if f is the constant sheaf over some lattice, so f is if Lot of this. So F is, if F is L, I don't want to use L because I used K, some lattice KG, then the global sections are consensus. So we get exact consensus. So this is, there's been a lot of work on max consensus and sort of the control theory literature. And as far as I'm aware, I mean, this vastly generalizes those ideas. Those ideas. So here's, yeah, so here's kind of a depiction of what's going on. So kind of read this diagram. We start here and we go around here. So we each have, each of these nodes has an element of their lattice. And then we push that to the center and then And then we take the meat and then we pull it back. So we have this sort of process that keeps continuing. A heat equation, if you will. I think I explained it better over there than this shows. Yeah, so one application from this knowledge-belief logic system is you can construct a sheaf from a Kripke model. So you're assuming that your agents are the Are the communication patterns of your agents are organized in a graph? And in this case, we define a sheaf. So over every node, we put the power set of states, and same for every edge. And then for each node, including to an edge, we have this Galois connection. Connection, so this map that sends the semantic content of a proposition to the semantic content of that proposition, agent I think that proposition is possible. So this is a sheaf where the restriction map coming out of every node is the same. I don't know if there's a good name for those kinds of sheets. Is a good name for those kinds of sheaves. But each I has a unique restriction map coming out into each edge. And we get a Laplacian here, which kind of acts as a diffusion process of knowledge. So as we keep iterating that, eventually the possibility sets of all the agents will coincide. Also, I might think of all of these sheaves as having the same underlying lattice at each point in the vertex. Or is that or not necessarily? No, not necessarily. In this case, yeah. Um I think there's probably more interesting examples, you know, extensions of this, but I haven't thought about it in a I haven't thought about it in the middle. So I guess we have 10 minutes left, but I just want to pose some potentially open questions. One is, you know, what we have these sort of continuous models from Continuous models from the Hodge-Helmholtz decomposition to model preferences. And what are sort of the combinatorial analogs where instead of flowing with the Hodge one Laplacian, we might be doing some kind of taking meets and joins of preferential. Joins of preference relations. And, you know, in particular, what really is preference diffusion? Can we have a diffusion process on preferences? What does that mean in the Hodge theoretic context, but also in the combinatorial context? And the same with knowledge. Like, what does it mean for knowledge to diffuse in the system? And you know, a very specific question is: you know, can you come up with a Hodge decomposition of C0 instance if we have our Co-chain X, which is in the global sections of this sheaf, we have some like Y or rather, you know, if we have some X in the co-chains. The code chains. So an assignment to every node, can we write like x as equal to maybe something, something? I don't know if this is going to be a meet or a join. But anyways, is there sort of a lattice theoretic analog of the Hodge decomposition here? And another question is: so I described So I described flows on assignments of nodes, but what about flows on assignments of edges? Like is there an Helmholtzian for a lattice value network sheath? And is there, I guess, the same question, is there a Hodge decomposition for that? And I guess I'll open the floor for questions. Okay, well, let's thank Hon. Okay, well it's thanks honestly. You try to choose where you're just adding, but you're working in tropical geometry like P-adix. So there's kind of a know that everybody around me thinks the function has a polo order. Since the button has a pull of order five or less, I'm not going to believe that it has a pull of order six. It's you have an addition, but it's topical in nature, and it's not that sort of yeah, so one thing that is interesting, so if we take, you know, when You know, linear sheaf where we have these matrices representing linear transformations between Boolean spaces. So we can define an alternative Laplacian where instead of matrix multiplication, we have tropical matrix multiplication. And by tropical, I mean you replace max, or sorry, you replace plus with max and times with plus. So if we were to multiply these two things, we still do the same thing. So we take a max of like a 11 plus x1 and a 12. Uh 12 plus X2 21 plus X1 22 plus X2 So one fact about if you view this as a map between like R2 and R2 As a lattice, so with the max and min being the meet and join, the coordinate-wise max and min. So if you have like one minus one join, two minus two, right? You would take that, you would get two minus ones. You take the max coordinate. So this is. So, this is actually a map that preserves joints. Therefore, by this sort of adjoint functor theorem, you have an adjoint to this map and you can define this Laplace and the Tarski Laplace. One thing I haven't really, besides very simple synchronization problems between discrete Discrete event systems. I haven't really thought of any interesting applications of these kinds of systems yet, but this is something I'm currently working on. But I don't know about p-adic, that sounds very interesting, p-adic tropical geometry. The only problem is that, you know, you can cancel. I had a question going to way back in the talk when you were defining lattices and you sort of introduced joint and meat. And the definition of joint and meat seems fairly symmetric to me, just in sort of placing like large and lower bound with small and upper bound and sometimes. Just like way here. Yeah, so here, right, so you have these definitions. And then so much of the rest of the talk is built on the idea of joint preserving. Ah! Why not me preserving? If these two things are so. Preserving if these two things are so seemingly symmetric. Well, so you can always take a partially ordered set and take its opposite and say A is less than B if and only if B is less than A in the new thing. And then you can replace this and the Tarski LaPlashian with meat preserving maps. Adjoint is now joint preserving. Instead of taking the meat of everything in the middle, like instead of taking the meats of these two guys, The meats of these two guys, you would take. Because think about it: what are the two most basic consensus algorithms for this case? One is like take meats of yourself and the neighbors, the other is take joins of the neighbors. So there's sort of this duality baked into everything. And is that, I mean, so I guess I, so why I asked this question, I wonder whether this is just a sign convention that sort of feels like multiplying by negative one, negative two inverse everything. If you were talking about consensus, though, does that change? Consensus, though, does that change? Like, would the dynamic be different? Yes, instead of people becoming polarized instead of OSV. No. Well, so going back to the talk by Arne on Jacob Hanson's work on opinion dynamics, you could have a global section that's not consistent. Section that's not like consensus, but it's like it's still harmony in the sense that, like, my opinion, I express my opinion, it might be the opposite of my real opinion, but they still agree in this like discussion space. So, so I suppose, yeah, maybe it wouldn't lead to like consensus necessarily. Okay, okay, I think it's a good idea. Okay, well then that's cyclones.