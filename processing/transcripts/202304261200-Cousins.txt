Having these kind of different fake data studies to to us is kind of like something we would do like a robustness of how how robust is this excess to what we're assuming about the data study. But it wasn't clear are you were you suggesting to then add an additional uncertainty because of that or is that as far as you go? If it's above this if it's greater than fifty percent of our systematic uncertainty, we basically need to decide what to do. So if we see a bias on delta m squared three two delta M squared three two, which is nice and Gaussian, then we'd probably just smear it. But if it's on something like delta C P, which is unlikely because it's so you know it's basically stats limited, or on sine squared theta two three, then there is no clear, you know, we can't smear that because they're very non-Gaussian. So we basically would add in a new Newton's parameter to deal with this. It's interesting. I think it kind of ties in with the two points. In with the two-point system. For example, if you say, okay, I discovered something greater than three-sigma, or I have evidence greater than three-sigma with one model, if I change it, it goes to 3.1 or 2.9. To me, that would say, okay, it's robust, so that change, there's no systematic to be added. And you're saying, I don't know, if it went to 2.9, you would add an extra systematic. Yeah, I mean, this thing, you can, yeah, obviously when we're talking about excluding some region, we're talking about units of sigma. About like units of sigma, really, right? Because that's what we'd want to publish. So practically, we don't do that if it changes from 2.9 to 2.91. But yeah. I want to go back to the smooth angle where you said the metropolis can deal with the discontinuities. So is this a feature or a bug? I mean, we don't really think that this is the likelihood, right? It's like an artifact of our finite sample-sized statistics estimation of what the likelihood is. Statistics estimation of what the likelihood function looks like. And so, like, maybe we should be smoothing anyway because we're somehow doing inference on the wrong, I mean, on the likelihood that we don't really think just real likelihood. And so, then, like, this the fact that the troublers can deal with it maybe prevents us from actually doing the hard stuff. Yeah, maybe. I mean, actually, maybe since this plot is relevant. So, I didn't show this, but this is actually the posterior. But this is actually the posterior for one of these parameters where we're shifting events. So you see, you do really just get these Poisson fluctuations. So you get at some parameter values, suddenly enough events migrate into a bin for, yeah, you suddenly get quite a different likelihood value. I mean, there are, yeah, some, you know, other analyses do this smoothing, but I guess from sort of a pure physics point of view, it's quite nice just to think, you know, especially if you're thinking about a nuclear effect, right, where you've changed some nuclear potential. Where you've changed some nuclear potential, well, that will modify some outgoing lepton momentum by some amount, which would have meant it would have ended up in some different bin. Yeah, it will be binning dependent. Can I just, oh, sorry, that's the way I do this. But this cannot be done by template morphing. It's something I've got to confuse. Yeah, I mean, maybe it could be. I haven't really thought about it, to be honest. I know two-lane questions also, like so. Tooling questions also, like so, what type of software do you use for these types of applications? We have our own framework, C. Yeah. I mean, most of it isn't the MCMC part, it's mostly reading in the Monte Carlo and everything else. Yeah, Metropolis, I think, is pretty simple. Just a simple, maybe another tooling question: the use of the covariance matrices, does that restrict? Covariance matrices, does that restrict you to symmetric errors? Yeah. Yeah. Okay. Are they all small, or can you have some big ones that hit boundaries or have big asymmetric things? Yeah, you mean in the covariance metrics itself? Or did you have to shoehorn some asymmetric things into your co-covariance metrics just in the form? We haven't added those in, so particularly sort of, you know, I imagine, you know, this. You know, I imagine this difference between the two fitters is probably because we don't have these asymmetries in our covariance matrix, or we don't have an expanded term somewhere. Because at some point, we're just using the Hess around this minuet result, right, and expanding around that. I don't know if there are ways to uh add in asymmetries there, but it's probably something we should look at. Um going back to the smoothing thing. Going back to the smoothing thing. Yeah. I was, when you started use flies or detect a response, I was thinking, is it possible that you're introducing resolution that you don't actually have? Yeah, so when the binning that we pick is broader than the detector resolution. Right. So I mean in the fit they will almost definitely become correlated, right? Yeah, it's either a pinning or if you have another system. Or if you have another kind of systematic from your choice of spy fit for it. Okay, yeah. Oh, Tom, you finished a different question. Oh, yeah, so I'm always puzzled when there are sequential fits. So is there a fundamental reason for fundamental to consider in the To consider in the same footing, as far as I understood, the sequential fit in the simultaneous foot. Sorry, the sequential fit and the simultaneous A and B in the other slide. If I understood correctly, A is the full simultaneous and then B is the sequential that then opens itself to the caveats that you just pointed out. So is there a fundamental or not fundamental reason why both of them are sort of a Reason why both of them are sort of human rights citizens? I mean, to be honest, we just use the two analyses as a cross-check. So, you know, this analysis A came after analysis B because we were worried about this sort of thing. So B was first because you wanted to see what comes out of the nearby. I mean, also, it's just faster. You know, when you've got so many nuisance parameters, you know, MCMC is. MCMC isn't known to be incredibly quick, right? Practicality is easy. Yeah, yeah, yeah. Unless you're on the same subject, you want to see no different subjects. We're off that subject now. I can ask that thing. Experts is back to you. Yeah. Okay, let's go. Sorry. You know, just correct me if I'm wrong, but on that point, I would say the result here basically says to me that your fit is not influencing the near detector. It's not constraining those parameters, but the near detector is constraining the file. That's why I would get a kind of similar picture here, even if you separate it from those. Yeah, I mean, definitely, like, it's not so bad at the minute because we've been hide behind statistics, but obviously, flux and cross-section will correlate with our. Cross-section will correlate with our signal parameters. So, if we're changing the constraint on those nuisance parameters, then it's going to shift our contours in some ways. There's also a physical boundary at 0.5 here on this plot, so some of the shrinking is quite hard to maybe interpret, but um yeah. Okay, all right. The previous slide. The previous slide. Yeah. So, some people know when I see flat priors, I have questions. So, you want to say something about all the frequencies properties of these priors, or are they just flat because they happen to be the maybe says? Yeah, so for you know for the for these flat priors are nuisance parameters, we don't worry too much about We don't worry too much about them. For the priors on our signal parameters, we do check different choices of a prior and make sure that it doesn't impact our result much. But we haven't really studied these in our detail, to be honest. So if we go select in the standard between oscillation, let's just say one of the angular One of the angular parameters. Which metric are you flattened? So, yeah. Usually in, so sine squared theta23 is flattened sine squared theta23. There's a, you know, you can check what it would be like if you were flattened sine squared two theta two three by you know doing some Jacobian to change to re-weight the prior. We do that check, but more generally. But more generally, there are some specific priors for the PMS matrix, so like the HAR prior and more sort of objective priors that we're starting to look at as well. So, one of the things, just so the priors here, they're like the older priors, right? Because in your likelihood, you actually have a constraint term of the ability parameter. So, if you would do like Newtons parameters. So, if you would do like what they were suggesting, kind of like the two-stage fits, you have like the calibration measurement, you would kind of do the Bayesian update first on that, and then you have the Poisson piece, and then the priors are not flat anymore. So the question is, should you worry about like a flat one prior? And not, right? But some of the constrained terms are already fairly strongly constraining and we just forever. Has anybody done a covered study just to know what the frequency spark ends are? It compares well to a frequency. Yeah, so analysis B is confidence intervals, right? And also, you know, when they throw from a different near detector sort of model, that's also still a confidence interval. So things seem to agree. Yeah, Glenn. So you're producing posteriors. Posteriors for various parameters. But then you also talked about so-and-so-many sigma discovery. Do I understand that here if you're only going to use the Bayesian methods for producing credible intervals of posterior densities or parameters? I think you're going to be comfortable with this when it comes to establishing the discovery of an element of CP or CPU. Yeah, I mean, we report Bayes factors, but people wouldn't consider a Bayes factor as a way of doing discovery. Group. I mean, again, that's sort of why we have these two fitters, right? Because we have a more frequentist approach and then we sort of have this comparison. There, I was happy to say that they give the same answer to, as far as I would care. It's the question of whether or not you're going to say, quote, a basic result for intervals, say, and then a frequentest result to establish discovery of something that. To establish discovery of something there. I don't have a particular backstory right on that. Yeah, I mean, I think currently that is what we would have to do because there aren't really the Bayesian tools to... I would be keen to know if there's that assumptions. Yeah, I mean, if people... Yeah, I was saying this to a couple of other people. I haven't... I've tried looking and I haven't found many papers. Well, any papers that are like, this is a Bayesian discovery of something in particle physics. I mean, I think they're in Astro and Cosmo quite a lot. Astro and Cosmo quite a lot. I'll come on that when someone else's cherry is my other question about the fake studies again. This was something I always had fun because I got my own collaboration dealing. Isn't the criterion not how big the effect is, but how independent it is? So if you are saying that to dismiss Saying that to dismiss something because it's small, it's because it's contained in the systematic variations that you already have. But when you say you've changed the model, does this mean that you've changed it in a way that's completely independent of the variations you already have? Or, say, you swap Genium for Neuro or something, is that the change that you've got remote, or do you change primary Genium of that thing? We do both, right? So sometimes like, you know, sometimes they aren't. Sometimes they are, we completely do a different generator, so particularly on Dune, right, they're a way of motivating design choices, right? But yeah, most, so for example here, these ones that I've shown are complete changes for how we parameterize our nucleon form factors. So they are sort of, we don't sort of explicitly have uncertainties going from our nominal sort of model to any of these. So they are sort of independent. So they are sort of independent. Why not? Aren't those uncertainties? So, Dune has this funny fake data study where they take 20% of the protons and turn them into neutrons. Yeah, yeah, yeah. And they always show a plot where all the fitted parameters are all very different when you do that, of course. But then the question is, isn't that an uncertainty or is that somehow separate? So this is the thing about that basically that the bigger ones of these eventually become ad hoc. Eventually, it become ad hoc parameters in our normal analysis. Why not the small ones? What's wrong with that? Yeah, I guess. I mean, people have commented this, right? Oh, you have all of these small biases, why don't you wrap them all up? Yeah, sure. Yeah, I mean. Yeah, I guess it it's known unknowns and unknown unknowns. Yeah, that's like no, the double kind of ones you should. Yeah. Once you should. Yeah. And again, I don't always know where the double counting comes in, right? Because if you're going to, you know, generator versus another generator, even, like, they might have tuned on the same data, but still come up with slightly different answers. So it's kind of difficult, I guess. But yeah, this is we get into debates about these studies every analysis. You're planning to do a carbonation with normal? Yeah, it's going on actually. It's kind of tricky. So this is sort of results from different neutrino experiments. So blue is TTK and then red is NOVA. So there's a lot of a lot of d a lot of people say that this slight difference in the one sigma credit for confidence regions is tension. Confidence regions is tension. I don't think so. And all of the global fitters just say, ah, well, clearly this area in the inverted ordering is the correct place. But yeah, we're currently doing a, you know, I'm part of the joint fit. I can't say too much about it, but because they're completely different generators and the experiments are at different neutrino energies, it's really not clear how to correlate systematics. So, a lot of it is sort of doing these fake data studies and these sort of robustness checks to make sure that each other's systematics models are robust enough to basically increase our sensitivity by including the other experiments' data. Yeah, really not far actually. Well can you just go again the page twelve? Yeah, I I'm thinking all the time if you apply anywhere some kind of You apply anywhere some kind of template morphing. Because when we discuss smoothing that meets in this plane, right? You smooth the distribution from one over this plane, right? But while the template morphing would be now as a function of the looseness parameter value, right? You could plot this for the different values. I'm just wondering if you lose this anywhere. Not that I'm aware of. Maybe you should. Yeah, maybe we should. But I don't think it really solves anything because you'll end up with templates that have completely different wiggles and then you. Completely different wiggles, and then you try to interpolate, and it's just meanings. You get all the same problems, but now all the noise is in the template variation. So I don't think it solves that problem. We have the same problem when we do template mapping of this. It's just the problem appears elsewhere. It's just a statistic uncertainty. Yeah, you have a venture job between bins, whether you see that variation of the more you. My question was more template. I mean, you know, like we're using template official for anything. I mean, you know, like me using technology for anything, even without statistical problems, and that's like for extra politicians outside plus or minus plus sigma, then all of the currents for all doing stuff and that's it. Yeah, no, we haven't looked at this. As I say, sort of, you know, in next slides, there was sort of a talk about regularization of this smoothness based on sort of bin width. So that was something that Christoph looked at. But again, sort of for the MCMC analysis, we were quite happy to basically apply these shifts and do. To basically apply these shifts and do the bin migration and basically take the hit of keeping track of where your bins are, or sorry, which bin an event is in your analysis. So is an unbin fit not feasible? Unbin fits hurt my head a bit to think about, to be honest. We haven't sort of considered it. We've got all these bin issues. This is a way to unbin it, but. Been issues, if there's a way to unbin it, but again, one possible simple solution is being a bean. I remind you: if you have a point, you fill not just one beam, but two beams in such a way that its centroid location is preserved. I wasn't reminding. I've got a naive question. So, normally, how many events do you have to fit? Do you have to fit just to see how data or Monte Carlo? The actual data. Yeah, so the far detector is about 1,000 events in total. And then near detector, it's about 200,000. Again, these are selected. So actually, for 1,000 events unbinned, okay, I'm not sure how complicated your likelihood is, right? But for Xenon, we always do unbinned type of thing. Too unbin's type of thing. And 1,000 is okay. 200,000 might not be, yeah. But I'm not sure how complicated or that means. Yeah, I mean, again, this is probably, you're probably right for the file detector, this probably wouldn't be so bad. The near detector, I don't know. And again, maybe. Yeah, it's probably worth thinking about, though, you're right. But do you know the functional form of the I mean I thought the I mean the point in LHC the reason why we do bin sets is because we need to use Monte Carlo. Because we need to use multi-colour to if we don't know what the shape is, yeah. I guess if you have your detector response simulations throwing their old ads at it surprising well, surprisingly not too much, to be honest. I mean uh we do have some like We do have some like BDT analyses for some stuff, but for the oscillation analysis, we tend not to because we're worried about our nuisance parameters, right? So it's much easier to keep track of where these go if you've got a more traditional analysis, I guess. Which is a bit of a boring answer, but right. Well, we're officially have 10 more minutes where we can use it like meeting. Thanks very much for again for. Thanks very much for coming. Am I supposed to turn this off now?