Weijun's introduction and also thank you Weijun, Chenjia and Xin's organization. It's my honor to give a talk here. Actually two talks. So the topic on the random data for PDs. So basically this is my first talk today and another talk on Wednesday. So the first talk basically I will Basically, I will focus on the random data for the non-linear Schrodinger equation or a little bit more general dispersive equation. Okay, so this is my model, just the periodic non-needle shredding equation and the non-neurality. It's very simple, just this type p-order polynomial. And because we consider the pure order, Consider the periodic boundary condition, so which means it's equivalent to like we put the x in the on the torus. So for this type of shooting equation, it's very one important concern quantity is Hamiltonian. Okay, and another is a mass. We will talk about that later more. So here the P, let's say it's a Here, the P, let's say it's odd P, and the sign is a non-linearity. Here it's not that important. So, positive is a focusing, positive is defocusing, negative is a focusing, but here we only consider the like local time theory. So, it doesn't matter of the sun. So, let's recall some. Uh, let's recall some important scaling symmetry. So, we know for this Schroedinger equation or a lot of different equations, we have the scaling symmetry. So, basically, we can use the scaling symmetry to like generate a class based on one solution of the equation. We can generate like infinite many solutions and this so-called scaling critical theoretical. So-called scaling critical strategical means if we put this class of the solutions in one soblift space, then when we select the regularity, it's this number d over 2 minus 2 over p minus 1, then the scaling symmetry, the scaling transform keeps the solid norm. So So, this is important because, so, based on if we just select a very simple examples, for example, some solution support on some box finite time, then we can use the scaling symmetry. It's easy to see when the regularity, when we consider the data like smoother than the bigger, the regularity bigger than SCR, then we can. CR, then we can expect the solution, it's a local wild post. So, and this one can be proved in the Schrödinger equation. Okay, here the SCR should be bigger than or equal to two. So, by Bougain's and Bougain dimeters, the three herstimate on Taurus. So, this is some like a classical thing. And one IC equals ICCR, which is a mass critical case. Which is a mass critical case. For example, the cubic shooting on 2D, the local wild postness is even open question, huge open question in the dispersive PDs. So one lesson ICR. So based on the symmetry, we don't expect that the solution is well posted. So actually, one can prove somehow the new post a little bit. Post a little bit. So, this new post is like we lost the continuous dependence on the initial data. So, there are some results like this. So, what is a random data theory? Basically, random data is if we consider, if we consider initial data, it somehow Initial data is somehow like randomized. So, so this type randomize initial data. So, there are many ways to randomize. Here is one simple way. We write down the initial date into the Fourier series. Then we randomize each Fourier coefficient. This GK can be understood just as the ID complex Gaussian, standard complex Gaussian. Complex quotient. Then we put the power of the k. Okay, this Japanese bracket k just means absolute value of k squared plus one, then take the square root. So we can put the alpha here. Then this alpha, this coefficient alpha, determines the regularity of this random initial data. So basically, it's easy to see: okay, if we Okay, if we put this the this Gaussian the measure of this ID Gaussian measure Gaussian random variables then this Gaussian measure supported in the soblet space like this so this ice is just alpha minus d over 2 so in particular once this alpha equal to 1 This alpha equals one. That's a very important special case. That's a like here, the Gaussian free field. So you can see this one alpha equal to one, the regularity is one minus d over two. That's exactly what we have there. Of course, here it's in the soft lift space, but for this random data, it's easy to see this. Easy to see this fall list HIS is almost sure just the CIs, the holder space. So also alpha equal to one, that's also the case, it's very important to hear. So we will see that later. So what is the almost sure what local app person is? So when we consider the So, when we consider the random initial data, which means we try to study this PDE with a random date, then the so-called well-postedness can be weakened as almost sure well-postedness, right? So generically, we can just think: okay, we exclude some measure zero set, probability zero set, then we consider, okay, if the remaining Okay, if the remaining random data, the almost sure the remaining random data is like a probability one, right? So if the remaining one is well posted, we say it's like almost so well post. So of course, so for the so-called super critical case, which means if we consider the initial data, I less than I see. IS less than ICR, we don't, so deterministically, we don't expect the well post. So we know very little. So no uniqueness. And so there are some uniqueness results. So in groundbreaking work Bugan 1996, he showed if we consider the kinetical random date in some deterministic super In some deterministic supercritical space, I less than ICR, almost surely what I get is the strong solutions. So, which means we can show us if we exclude some probability zero set, the solution is still, the equation is still good in this so-called deterministic supercritical space. So, this is reasonable. And here, And the here Bougain is, we will see the here Bougain's idea is just similar like this W de Bouche trick. So we will see that later. So why is the random data can get the almost well posedness for the lower regularity? So because for random data, we have the better linear estimate and non-linear estimate. So we will see that later. So, some neutral questions to ask for this random data problem. First, so can one describe the solution in terms of a random structure, at least for short time? So, which means can we know the structure of the solution with a random initial date? So, of course, the Sheng Hao just shows the double debush. double debouche this structure is one type like a structure right we know the solution can be write down like one is y one is z y is from the gaussian z is just the sum in the high regularity right so here we want to see can we have the similar structure for the non-ingress ratinger and what's optimal regime so how low what's the lowest Low, what's the lowest regularity data? We can see the almost sure well posteness. So basically, so later you'll see actually these two questions are linked. If we know more about the first question, we can know, we can get the lower regularity for the almost raw postness. So, this first two questions actually linked to the Linked to the third talk in this theory by given by Yudon, the wave turbulence theory or wake turbulence theory. So if we know, basically if we know more structure of the solution, this random data, initial data problem, basically we can know more about the wave turbulence. And the third one is what about the long-time behavior, right? So, of course, first, one important thing for the long time behavior is the invariance of the Gibbs measure. Actually, I want to say the Gibbs measure in this background is called Gibbs measure. So, it's exactly the same like what Like what here, yeah, like here. So, FIFO model. So, basically, it can be understood with the Gibbs measure. It's a FIFO measure, of course, for cubic equation. And here, we study the FIFO measure or Gibbs measure under the flow of the Schrodinger equation. So, what should just show us is like Just show us it's like the same measure, but under the dynamic of the stochastic heat equation. So that's a difference. But of course, if we change the dynamic, so basically we change too much, right? So, and also some other, the almost sure global well postness that can be, if we have the Gibbs measure, we can have the almost sure well global well postness and also some And also, some like the almost gathering. Scattering is one important long-time behavior, asymptotic behavior for the shading equation. So here are some classical historical literature. So Labor Stross Beer, they are basically the mathematical physicists. They are the first group to study the stochastical. Stochastical mechanism of the Schradinger equation. So basically, this shows the Gibbs measure under the flow of the non-linear Schrading equation. Then it's Bougain, 1994 and 1996. Bougain studied the 1D Schrödinger equation, the Gibbs measure, the 2D cubic. Of course, this zero. Of course, this serious work of Bougain is based on the question, needs to answer the question asked in the first paper. So, of course, the main difficulty to study this is a Gibbs measure supported in very low regularity. So, especially for the Schrödinger equation, to study the low regularity while posting it, it's very difficult. So, we get serious work. So, Boogain series work is like the first several work can get that low. And then the vertical build for the WAVE, Colliander-O, the Global Wellpostness, combined the random data with the Hilo method, and the Dominion-Lorman-Maldison. This is the first paper to study the almost short scattering. Of course, that's in the Euclidean space. Euclidean space. Here we are more like the power. So Chenjia's work with Maldison. This is, I think, maybe the first work to study the random date, not just around the zero. It's a randomized around some ground state, near the ground state. That's very different. Okay, so let's back to the Gibbs measure problem. The Gibbs measure problem. So I want to quickly, because it's already introduced a lot, so I want to quickly take a sketch. So as we already know, the alpha equal to zero, that's a Gaussian-fuh field. Basically, the Gaussian-fuh field and the Gibbs measure, they are supported in the same function space like this: one d sorry, two dh0 minus. 2D is H0 minus, which is below L2. 3D is H negative half minus is even lower. 1D is a little bit better. It's H half minus. So if we write down this Gibbs measure, okay, this is like a formula. Formally form like the FIFO model or Gibbs measure. Of course, P equal to 3 is a FIFO model. Raiser Fife model. So, this one, we can, if we expand this Hamiltonian, then it can be written down like this digital one is a Gaussian field field. And this exponential negative P, this is difficult case, it's a minus some power. So, this can be understood as a weight. So, basically, the Gibbs measure can be. The Gibbs measure can be understood in at least 1D, 2D. It's like the weighted Gaussian field field. So we have this weight, which means they are absolutely continuous with respect to itself. So if you have a measure zero set in the Gaussian measure, then it's also the measure zero set in the Gibbs measure. So at least in 1D and 2D. So, at least in 1D and 2D. In 3D, they are very different. So, but anyway, which means we can just consider this Gaussian field, this Gaussian random data instead of the Gibbs data. So, okay, this already showed. So, okay, basically, this Gibbs measure is like the FIFO model. So, they are list of the mathematical physicist. Microphysicist. So, okay, in 1D and 2D, we have the absolutely continuous 3D, they are very different. So, we will talk about this 3D case in the second talk. But there we will see how we use the stochastic quantization together with our method to study the 3D Gibbs measure for the wave equation. The wave equation. So 1d is bigger than or equal to 4. Basically, we don't expect the Gibbs measure of FIFO model. It's like non-trivial. So, okay. So, the difficulty to study this is because of the okay, the Gibbs merit supported are in this very rough regularity space. So, if we want to If we want to show the invariance of the Gibbs measure, first we need to show the short-time like global short-time wild postness, almost short-wild positions. So like here, like there, right? They are the same. So basically, in the Schrödinger equation, if we want to study this type of low-regularity initial data, it's very difficult. So, okay. So, okay, in 1D studied by Bougain, for all p bigger or equal to 3, d equal to 2, p equal to 3, that's Bougain, another Bougain's paper. So 1D equal to 2, P bigger than or equal to 5, that's very difficult because the Gibbs measure is the same data, but the deterministic scaling is go higher, so the gap is bigger. So 1 d equal to 2 p equals 1 d equal to 2 p equals d equal to 3 p equals 3. This is still open, it's very hard. So, why it's very hard? We will see that from the following argument, the so-called probabilistic scaling. So, the probabilistic scaling, so first show in our paper, introduced in our paper, in our paper with DOM amount. So So this tells us if we consider any initial random initial data, the regularity is bigger than this SPR. You can see this SPR is just negative 1 over P minus 1. It's nothing related to the dimension even. So of course, it's much smaller. Usually it's much smaller than the SCR. 1 at D equal to 1, P equals 3, they are the same. Really are the same. So we can see, we can show one ice bigger than this SPR. We can always obtain the local in-time strong solution to the Schrodinger equation for any P bigger than 3 and for any D. That's this, why we can have this, why this is smaller than the SCR? The basic ID is from the square root cancellation of some. Cancellation of some of the independent random variables. We will see that later. And you can see 1d equal to 2, p bigger than or equal to 3, the Gibbs measure is supported in the H0 minus space. And it's always probabilistic subcritical for NEP, right? Because this one, this number is always negative. This 0 minus is always. 0 minus is always bigger than the SPR. So in this sense, we expect it can be solved. But okay, we can see this probably scaling only like 2019, we first see that. So for a while, for a long time, people don't know that. So why, so what's the heuristic of this scaling? There's a heuristic of this scaling. So, if we consider a very simple initial data, random initial data, like this. So, here we only consider for any constant n, so this n can be anything, can be goes, can go to infinity, right? So, if we only consider this type of random data, very simple. This data, the frequency, it's only depends, it's only like near the end, like for example, from n to n. Like, for example, from n to the 2n, then with id Gaussian, and we put n to the negative alpha, then to make sure, okay, then this alpha determine the HS norm of this U0. So basically, it's easy to check almost surely this I, this random I, is in the HIS. Basically, okay, if we compute the HIS norm, almost sure. Compute the HS norm almost surely is bounded by one. So then if we try, if we suppose, okay, we have the almost sure local while posteness of this nonlinear shredding equation, then at least we expect, for example, the second order iteration still stays in the same HIS space. So the second order iteration, we just plug in the We just plug in the linear solution, the exponential i as Laplacian I. This is a linear solution of the Schrdinger. Then we plug in the linear Schradinger into the non-duality and put the Duhamail form. Then we want to estimate this U1, whether this U1 is in HS or not. So, okay, if we just So, okay, if we just fix the t equal to 1 and we fix the k is always near the m, then we can write down the Fourier coefficients, Fourier transformation of this U1, Fourier transformation only taking spatial direction. Then it can be written down like this. Here, n to the negative p to the alpha, that's from the initial. To the alpha, that's from the initial data part, right? The decay. And this summation of the multilinear is like this: right, that's k1 minus k2 plus k3 minus until kp equal to k. This is like the convolution thing, right? Because here it's a product of several functions. When we use a Fourier transformation, the product becomes a convolution in the Fourier size. Then, here you have a multilinear Gaussian summation. Multilinear Gaussian, summation of multilinear Gaussian. This one over omega thing, that's from this integration. So this one over omega is like a k squared minus k1 squared plus k2 squared, blah blah. This one is also from here, from the because the linear shading equation supported in the, you know, on a In the, you know, on a parabolar in the space-time frequency. So that's why we have the omega. So when we sum over this omega, so one over omega, so more or less, so if we like omit a log loss, it can be understood like this. So we can fix the omega like a constant, for example, like a zero. So actually, it could be any constant. It could be any constant. Then, of course, here we fix our omega, but because we have a one over omega, right? So up to some log loss. So this one can be written into the like the like this. The multilinear Gaussian takes the absolute value. It's this. So down here, we have the square root of gain. So the square root of gain. So, the square root of gains comes from the large deviation property of this multilinear Gaussian. Of course, here we cheat a little bit because here we need to assume k1 not equal to k2 or k2 not equal to k3 exact. Because here we have the gk1, gk2 bar. If k1 equal to k2, then this part will lose the randomness. We lose the randomness, right? So, also, this thing related to the so-called renormalization, the weaker renormalization. We have to do that. As also, I think, where? There, I think there. The weak power. This is a weak power thing. If we plug in the weak power, so automatically we remove this type thing. This type of thing. So then this square root again gives us something better than the deterministic case. So, okay, of course, this one is a summation. So here involves some lattice counting problem. So when we plug this one into the HS norm, basically we have this one. We want to make this power less than zero. So if this power less than zero means This power less than zero means when n goes to infinity, this is like convergent. Otherwise, it's something divergent. We don't expect even the second order iteration is good. So this gives us the condition. I should be bigger than this SPR. Okay, so this is a lattice counting thing. Okay, basically. Okay, basically in the end, this lattice we need to count how many K1, K2, K3, and Kp in this. So it's nothing difficult, just the last step, we need some counting the lattice point near the circle. So this type idea can be also applied to some other equations. To some other equations. So, for example, the stochastic heat equation, like just like here, like here, right? The same thing. So, we can use the same idea, apply to the heat equation. So, we see in the end, we do the similar thing, consider the second order, the Hanel. In the end, we get some this probability for heat is negative. For heat is negative P negative 2 over P minus 1. You can see this is lower than the trading case. So this one is exactly, so later we found this one is exactly the same as cause parabolic scaling, parabolic scaling. I think in one paper of the Martin Kerry, they also introduce some, but they get this scaling by This scaling by using some other way. So finally, it shows this two-way kind of like equivalence. So when we apply that to the nonlinear wave equation, we can get another one. This kind of like between the Schradinger and the heat. So this probably scaling heuristic, just a guiding principle. We don't say, okay, it's a rule. We can always get the well-post-net one, I think. Always get the well-post-nets one IC is bigger than SPR. There are still some cases, some discrepancy. So basically, main case is when you have the low dimension and the low power, for example, project on duality. Sometimes it fails because you can see we consider the high-high to high interaction, but sometimes the high-high to low interaction is bad. It's worse than. It's bad. It's worse than the high too high. So, and some other cases, some other discrepancy, one may consider some other equation, like, for example, fractional, Schrdinger. If we have some other dispersion relation, the counting estimate, so like it becomes very complicated. Okay, so this is our first main result. The first mean result, we study, we can prove all the eyes bigger than this SPR. We can get the almost sure local wild postness. So here, the almost sure local wild postness means for almost sure this type of random initial data, then we can get a strong local solution, local in-time solution. Okay, this time, actually, this time is something. Actually, this time is something, this can be understood like a random variable time. But this time, it's strictly bigger than zero, almost surely. So here, the renormalization always we need. So basically, we need the so-called weak ordering. As I just showed you, we need to remove the, we call it like pairing case. For example, the k1 equal to k1. case for example the k1 equal to k2 k2 equal to k3 that type of thing so the reason it's so some reason one reason is the uh as we said there so there are some term it's it's it's never like it's always diverged goes to infinity if we don't remove that we don't expect that we have the even the local wildpost almost sure local outpost is also also also for the gift measure the construction of a gibbs Gibbs measure, the construction of Gibbs measure, the renormalization is also needed in the 1D in the, for example, 2D, 1D, 2D, and maybe 2D. And the 3D case, you even need more than the weak ordering, like a further renormalization. So we will talk about later next time. So this uniqueness, the wirepost uniqueness, so in the sense of that solution. So, in the sense that that solution is a unique limit for all possible choices of the canonical approximations. So, here in our proof, basically, we first cut into the finite dimensional system, like the finite dimensional system with a frequency less than n, then we take the n goes to infinity. Actually, some other way to take this limit also works. So you can see we barely missed the almost local operation for d equal to 3 because in the h minus negative. So if we can get this, then maybe we can expect we have the 3D cubic inverse deep measure for the shading. But this is the case like the super, like the probabilistic. probabilistic critical one. So that's why the 3D cubic is the only problem it's open until now. So as a back product, so this random data can be considered for any data, even for the smooth, well-paired random data. So then for this type of random data, smooth one, we can show One, we can show the existent time of existence is longer than the deterministic one. So this is related to also the wave turbulence theory. So second one is a 2D Gibbs measure for p equal to bigger than 3. So let's give this. So then let's quickly So, then let's quickly go over a little bit the Bouges method. So, Bouges method, so the model is a cubic Schrodinger equation on T2, dimension 2. So, this one is so-called the weak ordering. Basically, you can see when we consider the initial data in this 0 minus the H0 minus, then 0 minus, then this mass that goes to infinity, right? Basically, this one, we should define this in other ways. So this is not well defined, but you can see basically we minus some linear part with the coefficient is infinity. So that's so-called that's a renormalization. So this one, of course, is bigger than SPR. If we consider the H0 minus support. 0 minus support the regularity supported the supported of the Gibbs method. So Bougain's method is make a linear and nonlinear decomposition. So this is very similar with the Watto-DeBouche trick there in the stochastic heat equation, parabolic equation, the community. So actually, the Bougain use this. The Bougains used this similar idea, I think, even earlier in 1996. So basically, Bougain constructed the solution in the one linear part. So this linear evolution of the initial data can be understood as like the Gaussian part. As a like the Gaussian part, that's similar with the Y in the black ball. So then the V is a remaining like the it's similar like the Z, it's a smooth remainder. So basically initial data were in the H0 minus, but Bougain like conject, like see this solution actually, when you remove When you remove the linear evolution of the initial data, the remaining thing goes to something like smoother. The V goes to a positive regularity, HF, which is positive. So then if you consider this V, you need to put the solution of the V, right? Of the v, right? Solution, equation of the v as a fixed point argument. Then you plug in the answers of the u into the orange solution. You can get the following equation for v. So this v is like that equation. You can expand the cubic thing. So the Gaussian part, the linear evolution part, we call it all. Part we call it R for the rough and the random. The V, we call it the D for deterministic. So actually, it's still random, but we treat it in a deterministic way. So then we can expand in different cases, right? Like that. So here you can see if you have all randoms, A, like all R, so we can use. So we can use a similar multilinear logic deviation estimate. We have the square root of gain and we use the lattice counting. Here, by using that, we can gain. So this, all random case can be put in the smoother space. So if we consider the case D, all deterministic, of course, that can be treated like the deterministic. Can be treated like the deterministic HIS local theory. So it's not simple, but by using the street estimate, we can put this part also into some space in the S bigger than zero. So the B and C cases, we need to somehow treat it like the random operator act on the deterministic screen. So for this random So for this random operator thing, we can use a TT star argument, then it can be translated to the large deviation. So all cases can be put it into a smoother space. So if we consider the p equal to bigger than or equal to 5, why Bougain's argument doesn't work? Why this linear, nonlinear decomposition doesn't work? So this one. Doesn't work. So this one, actually, when you consider p equal to 5, this ISOCR equal to 1 half, then we do the Bougain's recentering argument. We also expand all the cases. So if we consider the case A, all deterministic, the all deterministic case requires either Case requires i should be strictly less than icr, like a half, based on the deterministic local theory. If we consider all the random thing, basically we can, when we estimate the HS, we have something n1. Let's say n1 is high frequency, highest frequency is as minus a half, n2 is minus a half, right? So, of course, you also have. So of course you also have some N3 and four, but they all like negative power. So if we only focus on the top frequency and the second highest frequency, so the top frequency tell us ice must less than a half, right? Otherwise, we only can have some decay for the N2, but N2 can be much, much smaller than N1. So the gain from N2 cannot cover the low. Cannot cover the loss in one in most cases. So, so that the Bougain method in the case A and case B, okay, one we need S bigger than half, one we need as less than half, so which means this method doesn't work. So, but of course, based on the B, we can see in many cases actually it's good. So, if we consider the if we consider So, if we consider the N1s and the N2Ns, 3 and 5, they are all low frequency. So, if the N2 actually is much smaller than N1, for example, N2 is L, which for example is 1, then it's impossible to control the case B, right? So, otherwise, if the L is bigger than N1, 2. Than n1 to some epsilon, right? Then we can use a gain from n2 to cover n1. So then we are good. So the bad case, the only bad case we cannot control by using Bougain method is this high-low interaction, high-low, low-low interaction. So this high-low low interaction to be controlled, to be handled, so we need to use more than Bougainville's method or double debouche. So, we at least need some counterpart theory, like in the stochastic heat equation case, like the regulatory structure or power control calculus. So, of course, this two method theorem doesn't work in the Schridinger case, but we need some theory at this level. It can be handled, yes. Can be handled, yes, yes. So only this high, low, low cannot handle in this case. So this two method is called the random average and operator method and so our method to solve this case. So our method, so the idea is similar like the regular structure of power control. We need to focus on this high-low interaction case, right? So high-low interaction case should. So, hello interaction case should be somehow like single out from the remainder, from the V. So, how to so what, so if we put this case, it's the X, so we need to, okay, our way to, we need to control this in the like a bioinduction on frequency. So, you can see this one, the PNI X, let's see. PNIX, X, let's see, the PNX means high frequency of this part. So it can be described by like the high frequency of the linear with the low frequency of the P L U. This L is much smaller than N. So we do the induction means we suppose we already know for the frequency, low frequency, like L, the structure of the U. Then we use that to define the high frequency, right, of the Frequency of the P and X. So this is similar like the power control rather than structure. So we use a recursive reduction of this. So then if we consider this, so for this type thing, we consider this high-low interaction, this P and X. P and x. So clearly in the Strigger equation, this P and X should be in the Half minus norm. So which is below a half. So if we only consider, okay, this is in, treat this is like a structure like in some HS space, like H minus, H minus space. So this one is hard to control. So we shift the point of view from the term itself into like the operator. So if we single out the operator part, so the Foley operator P L, this like operator act on the high frequency. So this operator only depends on the frequency, the solution frequency less than L. Lesser L. So this P and L actually is only determined by the low frequency thing. So that's independent with the high frequency initial data. So we call these operators random average operator. So our answer now we write down in this way. Down in this way. So, first the terms like Bougain, we have the linear evolution of the initial data, which in the H0 minus. Then, second, we put all these random average optimal terms into this answer. So, then we can prove all the terms of the regularity at the level of the half-manus, they all in this. They're all in this, the random average operator terms. Then the remaining has to be, goes to a half more smoother, the H1 minus. So the problem is how to run this answer. We run this answer. We prove this answer by using induction on frequency. So to run this, So, to run this, we need to bonds of this random average operator. So, this operator PL1 is an operator norm is L to the negative delta 0 is something less than 1. So that you can iterate this operator norm. It's always less than 1. Then the Hibbert-Schmidt norm of this operator, so this Hibbert-Schmidt norm can give us the Norm can give us the random average operator term. It belongs to the HR half minus. So the first norm is essential norm. So if we have a general function in the access B space half minus as the input, we can never close this in the stringer case. So why we Why do we need this random average operator? So, if we start from this very simple case, so the high frequency is a high frequency linear evolution, it's a blue ball, and with two gray ball, the low frequency, you iterate once, we know this one is something in the H or half minus, right? It's something not in the not bigger than half, it's something bad. So, when we eat. Bad. So when we iterate this once, it's still in the same regularity, h of half minus. So this is a nature of the Schrödinger equation. So if we consider the parabolics, the heat equation, all the wave equation, when we iterate, every time we iterate once, we can get some improvement of the regularity. We can make it higher and higher. So that And higher, so that we can stop in the finite time iteration. But for Schrödinger, it's impossible to do that, so that we need to control the infinite many of these iteration terms. So that's another way to see why this power control calculus doesn't work in the Schrodinger. We need this operator because we need to iterate this operator one more time. This operator one more times, one more time, okay, infinity. So if we write down these compact all of this term together, okay, this we can have the identity minus p and l to the negative one. So because we already know the operator norm of this p, operator p is less than one, so by the Newman series, right? So the identity minus this to the negative one is well defined, and the norm is also. Defied and the norm is also bounded convergence. So, and also if we write down this in the fully side, the random average operator becomes a random matrix. Okay, so this is our fland in the fully side. Okay, so Okay, so let me finish that quickly. So, this is a random average operator. So, as you see, the example we consider is H0 minus, which is a half better than the SPR, because the SPR probably scaling is negative, man, negative a half. So, if we want to push the initial data closer and closer, maybe an abstract. Maybe an epsilon better than the SPR. So if we want to cover the full sub probably sub-cradle case, so this random average operator is not enough. So in that sense, not just the high-low-low interaction is bad, but we need to separate more cases. For example, you have high, high, low, low, low, low. This is good in the case we just discussed, but it's not good. Discussed, but it's not good in the case like we consider initial data rougher like close to the SPR. So that we need to expand the H, the random matrix to the random tensor. So for each index, K1, K2, KQ, they are for the high frequency. So basically, the answer is look like this. So we need to sum over all We need to sum over all the uh h, the random tensors. So h, k1, k2, unt2, kq, and the until some large D, the size of the tensors. So the size of the D depends on how close we select our initial data to the SPR. So, okay. Maybe I don't have enough time. This is some examples. This is some example how the random tensor comes. That's similar as before. So, to study, to really run this hand, because you can understood this one is like we consider many, many high-order iterations. So, how to organize them together? We need some algebraic structures. We call it merging and trimming. We call it merging and trimming. So maybe I don't have enough time to say that today. So basically, similar like that, the random tensor enzymes, we need to, if we want to run it, we need to also equip with some tensor bounds, tensor bounds, like from K1 to KR to KR plus 1 to KQ. Plus one to kq. We should have all this type random tensor bond. So the difficulty is how to get this bond and how to get this bound and how to prove this bound by induction. So this is a very, very technical thing. So, okay, so here is some application of random average operator method and random tensor series. So basically, they all most work in Most works in the Schrödinger equation, wave equation. So you can see we already have many other papers using our method. Okay, that's all for today. Thank you.