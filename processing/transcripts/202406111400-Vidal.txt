was how to design architectures that have good performance and that the challenges ahead were beyond architecture design, questions about optimization, non-convexity, but more importantly, why is it the case that architectures trained with simple algorithms have good generalization properties? In thinking about what are examples What are examples of these questions that were presented at the moment? Here's one on the optimization side: why is it the case that very simple algorithms like grain descent can find a global minimum even though the function is non-convex? And I think this has been an area that has been heavily studied in the last seven or eight years or so. And we have today much better results than was available in 2015. The second question, and this was kind of beginning to be explored in 2015, was Be explored in 2015 was why gradient flow and gradient descent have the ability to find a global minimum that has good generalization properties. And that was not very well understood at the time, and there's certainly been a lot of progress. The third question is maybe relatively new. That wasn't there. I think the question of robustness has become much more critical in the last few years. And I'm putting it out there because I'm going to be talking about that at the end of the talk today. And the direction is a little bit the opposite. The direction is a little bit the opposite relative to generalization. So, here the question is: why gradient flow finds a minimum that is not very robust to adversarial attacks. So, it generalizes well on clean data, but not so much on corrupted data. So, in thinking about some of the progress that some of my students and postdocs have made over the years, this is maybe the first work we did that was primarily in optimization and primarily the question of understanding the landscape of this complex. The landscape of this complex network. And the kinds of theorems, and this is just one slide on Ben Haypel's work, was a few properties of the architecture that are important or relevant in order to make optimization easier. So one property that you can see maybe from the picture is these types of architectures where you have a lot of parallel structures. So this is one architecture, you've got multiple copies of it in parallel, and then you add them up. Simple architectures that do satisfy that might Architectures that do satisfy that might be a two-layer ReLU network because all of these architectures just become a single neuron and you have many in parallel. And the notion of width or over-parametrized here, that means that you have many such architectures in parallel. Second property that was part of his work back in 2015 was the question of positive homogeneity. The fact that it is important for optimization, or at least in terms of sufficient conditions, for the input-output map to be positively homogeneous. Output map to be positively homogeneous, something that you do achieve if each layer is linear and then you have something like a RELU nonlinearity. And so the types of results back in 2015 were of the kind that if you can find a local minimum that has the property that all of the weights in one of the sub-networks is zero, that's a certificate of global optimality. So that local minimum that all the weights are zero is a global minimum. That's it, all the way to zero, it's a global minimum. And that was one of the key results. And the other was sort of a meta-algorithm that would do architecture search and optimization at the same time, where you would keep, let's say you would begin with one architecture in parallel, you would find a local minimum, and you keep adding one at a time until this condition holds. And in that case, you can have the guarantees of global option. But all of these results were primarily about landscape. There was no really an algorithm, and in fact, many of And in fact, many of the steps of this meta-algorithm are NP-hard, like finding a local minimum for a non-convex problem or guaranteeing that you can have a descent direction by adding a new neuron or a new architecture in parallel in this case. So the most recent work in my lab has been focused much more on algorithms and super simple algorithms, so gradient descent and gradient flow, and trying to derive rates of convergence for fairly simple. Convergence for fairly simple networks. At the very beginning, we began just working with two-layer networks. But broadly speaking, the most complex case we've been able to do thus far before today's paper was really linear networks. In some cases, two layers, three layers, some particular types of architecture were multi-layered. But the key ingredients really to global optimality is, I think the one that is important is PL losses is one ingredient. And the second one, the second important... One, the second important assumption is these assumptions on the initialization that make convergence easier. And so, this notion that the initial weights need to be sufficiently imbalanced became sort of critical to the analysis or have sufficient margin. And this actually shows up almost explicitly in the convergence rate of a gradient flow, for example, where you get an exponential rate and the rate depends on the imbalance and the margin. On the imbalance and the margin, both quantities are really determined at initialization. And what's really funny, I think, in these non-convex problems is that the rate of convergence depends on the initial point, in a sense. And we were also able to extend these results primarily for gradient flow to gradient descent, where quantities that appear, like imbalance, are not preserved anymore. This imbalance is preserved in continuous time, but not in discrete time. But not in discrete time, and by choosing an adaptive learning rate, we were able to also get exponential rates. Yes? That's the lower bound. The lower bound is tight in some particular architectures. So, for example, if you've got a square loss and If you've got a square loss and the symmetric matrices, so some particular order initialization is spectral. In those cases, it's type. There is a mathematical formula for that, but the basic notion. So imagine you just had a two-layer linear network with weights u and v, for that matter. Roughly speaking, in the scalar case, it means that u. Speaking, in the scalar case, it means that u squared is equal to v squared. In the matrix case, it's something like u transpose u is equal to v transpose v. And so that means zero, that means perfectly balanced network. So roughly speaking, means that the weights of the first layer and the second layer are on the same norm, the same order. Imbalance means that there is a difference, but the part of the theoretical analysis for theorem one is that that difference is constant throughout the trajectory. Is constant throughout the trajectories of the radiant flow. So it's one of the invariances of the dynamics that gets exploited to do a tighter analysis. All right, so moving on from there, the issue with this work, though, is that, yes, this was about optimization and about getting a convergence rate, but this didn't have much to do with learning. Like, am I finding a solution that generalizes? And when we think about the question of generalizations, About the question of generalizations, sort of the dominant theme from 2017 onwards, or maybe even earlier, has been the question of implicit bias. The question that algorithms tend to prefer certain trajectories, and therefore they tend to prefer certain solutions over others. One major piece of work in this area is the so-called neural tangent kernel regime that has been widely studied. This regime applies for networks that are very deep and very wide, and you can allow And very wide, and you can allow for non-linearity. It's a regime that is typically obtained by initializing the weights at random with a Gaussian initialization. And I write their large initialization to mean that the variance of that Gaussian is large. And typically, it's been observed that in this regime, there is no inductive bias, like we've had many talks in this workshop about low rank and sparsity. Rank and sparsity. So you don't observe that necessarily. But the most important weakness of this regime is that sometimes called a regime where because you get a kernel and that kernel is determined at initialization, then you don't learn a lot. So the features that are learned. And so that has been kind of a big weakness. And I think it's been acknowledged by many. So the other end is the sort of feature learning. So the other end is the sort of feature learning regime where you do learn, whatever that means. And the one distinctive feature is that the initialization has a small scale as opposed to large scale. And in this case, the analysis is not done for very complicated networks. Here we see kind of the trivial architecture. So linear networks with diagonal weights is one. And in that case, it's been shown that you do get implicit bias in the sense of. Get implicit bias in the sense of either L2 regularization for two layers or L to the two over L, where L is the number of layers as L grows. And so you get sort of these sparse inductive biases. And as you change the architecture, so this being extended also to matrix factorization type or two-layer neural networks, you get lower-ranked structures that get in use. So the work I'll be presenting today is exactly in this place. Today is exactly in this flavor. We are going to be in the feature learning regime. We're going to have small initialization and we're going to be looking at two-layer RELU networks. So the main difference relative to the first two, I think, is that we will have a non-linearity. And what we want to do is to understand the learning dynamics of gradient flow and try to figure out what is it that gradient flow learns and why is it successful. And why is it successful? Why does it learn good features? This is going to be the key question of the first part of this talk. Now, more broadly, this work fits on the grand scheme of many, many papers in the last several years that roughly speaking, say a network trained with gradient descent generalizes well if these conditions happen. And more precisely, a network can be a linear network. A network can be a linear network or a non-linear or a transformer. Frame by the SGD generalizes well, and the reason might vary. In some cases, it's because of max margin type arguments. In some cases, it's because of this sparse and low-ranked frame. But the second part of the talk today is going to look at a recent paper that shows that these inductive biases are not always a good thing. They're good maybe for generalization, but when it comes to a question of robustness, comes to a question of robustness to say adversarial attacks, they can actually harm performance. So the second part of the talk, which will appear at ICML this year, is how do we fix these issues with the inductive biases? And the fix is going to be incredibly simple is instead of using RELIN networks, we're going to use P-Rele networks, which I'll define in a moment. But think about the difference between the RELIN network and the RELIN network to the P power. Network and the Relevant network to the P power or something like that. So, to summarize, this work fits, you know, these are the nearest neighbors papers to the work that I'll be presenting. And so, the contributions are of the following kind, relaxing a little bit the assumptions on the data. So, one example here is I'll define some of these things so that there is notions of separability on the data that we need to assume. And this notion of mu orthogonally separable will come up. Orthogonally separable will come up. Previous work, a lot of the learning dynamics analysis sometimes is you get very complex differential equations and you kind of look at them and make some simplifications and you think that they are going to converge to something. So it's a little bit hand-wavy sometimes. So some analysis is qualitative. I think one contribution of this work is that it's very quantitative. There is precise division of the learning dynamics. Division of the learning dynamics into multiple regimes. How long does each regime take and why does it happen? So I will describe it in qualitative and hand-wavy ways, but there are theorems behind with very precise statements as to what those regimes mean. The other condition that is typically required is what are the width requirements. Let's say the neural tangent kernel typically requires width going to infinity or relatively large. Relatively large. So, a nice aspect of the work is that the width is just omega of one, as opposed to exponential on the number of vectors. Now, all of this first part of work was really inductive bias and convergence and optimization style of work. And so it didn't focus at all in the question of robustness that I also write. So, the question of robustness was recently raised by this paper by Frey. And look, that is a little bit complementary, right? Work that focuses on. Complementary, right? Work that focuses in convergence didn't worry about robustness, and work in robustness does not focus too much on the convergence part yet. So that means that there is a little bit of open problems. So this paper was very nice in really suggesting and analyzing why non-robustness emerges with these inductive biases, but there was no fix to the problem. I think the main contribution of Hanson's work here is to propose. Here is to propose a fix for that non-robustness so that you can get both generalization and robustness with gradient descent on a Rayleigh network. That's going to be the program. So with that in mind, most of the talk will be devoted to the ICLR paper that is mostly on inductive biases and optimization. And then the last 15 minutes, hopefully, I'll be able to talk about the robustness app. Right? Okay, so let me get started with the first paper. So, we're going to consider a super simple classification problem. So, we've got just a two-class data in RD, and here it's illustrated by the orange and the blue point. We're going to consider a two-layer RELU network. So, WJ denotes the weight of the input layer and VJ denotes the weight of the output layer. Denotes the weight of the output layer, H here denotes the number of neurons. And we're gonna typically call WJ neurons throughout the presentation. Okay, so when I say the neurons do this and this and that, that means that the WJs have a certain particular value. And the sigma is just the standard ReLU nonlinearity. In many of the pictures that are coming, I'm going to think of the neurons as vectors in Rd, which is the same dimension as the data. And for reasons that we And for reasons that will be important in a moment, looking at the unit norm versions of the neurons is going to be important. And so this is the unit norm version here, which lie in the two-dimensional case, they line up. So the results that I'll be presenting apply to the exponential loss throughout, but the logistic loss, I think it also works for that case. So, we will be analyzing the algorithm, in this case, gradient flow, which is nothing but the derivative of w is minus the gradient with respect to w, the derivative of v is minus the gradient with respect to v. And we are going to make assumptions on the initialization. In this particular case, the assumption is that the initialization is Gaussian, and epsilon is going to denote the scale of the initialization, and that scale... Initialization. And that scale is going to be small. How small it will emerge eventually from the theorem. So, probably I'll come back to that question in maybe 15 or 20 minutes as to how small it needs to be. How small, obviously, as a function of potentially the number of samples, the dimension of the data. Okay, so that's the initial set. So I am going to now tell you. I am going to now tell you at a very high level and in pictures what are the main results that we've been able to show. And again, let me repeat the question. The question is, I'm just running gradient flow on this two-layer network, and I want to understand what features are learned. And so that is what the W's are. The features are going to be W transpose X. And in a sense, what we want to understand is why this network, when trained, learns features that make classification easier, why the features are there. Easier, why the features are there? And so this picture simply, again, is the negative data, the positive data. Because the initialization is small, at the very beginning, kind of all of the neurons, the H of them, are nearly at the origin. And so that's what this red dot means over here. But if you normalize them, they're small, but they're not zero, then they're kind of randomly distributed in the unit sphere. So that's going to be the initialization. Initialization. For reasons that I'll explain in a moment, the loss is going to have these two-phase behavior, a phase number one where the loss doesn't change too much, and then it's going to drastically go down in the second. And so what happens in this phase number one? As the picture illustrates, all of the neurons begin all over the place, but they will converge to only two main directions. To only two main directions, and roughly they converge to two main directions that are exactly the average of the positive plus and the average of the negative. And what you can see on the left figure is that the red dot at the center is not moving much at all. So in this first phase that we call the alignment phase or early alignment phase because it's really at the very beginning of the learning dynamic, the idea is as stated here, the norms of the As stated here, the norms of the neuron remain small. So that's why that red dot is just at the center. The directions begin to move and they align with the two clusters centers. So this is exactly kind of a mode collapse phenomenon, but now we prove that this exactly happens for relevant. And the other aspect is that the loss will remain roughly constant as illustrated there in that way. And so once And so once the neurons have aligned with the class centers, that's when the second phase begins. And in the second phase, you can now see in the picture, the neurons are already aligned and that alignment doesn't change anymore. They will remain more or less with the direction pointing towards the cluster center. But as you can see, what is changing is really the norms of the neurons. And they move now from the origin, you know, each neuron will have potential. Each neuron will have potentially a different norm, but they will essentially share the same main direction. And that is exactly the phase where the loss goes dramatically down up until eventually there is convergence. And the spoiler alert, there will be a convergence of one over t, but those kind of the second phase is well studied. In fact, Matthus has done a lot of work in that area in particular. So we're the second phase is we're taking many of the results and doing some minor adaptations, but the first phase is the one that we've Is the one that we believe is very important? Yes. Yes. Yes. All right. So let me begin then with an explanation and an understanding of the first phase, which is the alignment phase. So some questions you might have are: you know, why does this happen? Can I prove it that effectively the learning dynamics is such that? Dynamics is such that they just move in angle and they will align with the cluster. And the other question is going to be: how long does this alignment phase last? And the intuition maybe going back to the intuition at the beginning is that there's just going to be this amount of time that the norms are going to remain small. And I better align before the norms begin to grow. We'll come back to that more precisely in a moment. More precisely in a moment. So that's why this timing of how long the phase lasts is going to be critical. And the epsilon, the initialization scale, is going to give us the tuning knob to control how long this phase lasts. Okay, so let's begin a little bit with the analysis. So we begin with gradient flow. The only thing I want to emphasize on gradient flow is that the right-hand side here, a priori, is a function of all. Is a function of all the neurons. So I've written, you know, the left-hand side is just the gradient flow for the jth neuron, but the right-hand side is a function of all the neurons. So in principle, the differential equations are all coupled and all of the neurons are going to influence each other. That's the starting point. So we are going to make this particular initialization where the weights are exactly what I said, Gaussian with epsilon on the variant. This is exactly the notion. This is exactly the notion of balance for this particular case that one of you asked me earlier. It really means that the weights of the second layer are equal to the norm of the weights of the first layer in this particular case because there is a single output. And so this is at the level of the square, so there is still the choice of choosing the sign of the second layer. And so we're just going to randomly initialize with either plus one or minus one. So that's the starting point. The starting point. So, with this initialization, if you just take the equations of the Release network and take your loss, in this particular case, the exponential loss, and compute the gradient, this is what you get. So, on the left is just the J-th neuron alone. This depends on the sign of the Jth neuron alone. This depends only on the J-th neuron. Depends only on the Jth neuron, that's the data, so there is no dependency on other neurons. This is also the norm of the Jth neuron. So, throughout, there is no dependence whatsoever on the other neurons. All of the dependence on the other neurons is hidden here under that O of epsilon. So, that's the first part of the result is to actually show that because of the initialization, everything else, all of the Everything else, all of the coupling in the dynamics is essentially of very small order and tiny teeny. And so that's why, in this first phase, my goal is: I want to make sure that epsilon is small enough so that it doesn't affect the dynamics at the beginning. And so I want to see for how long can this and so we define precisely the alignment phase as the period of time such that the norms of the wave. That the norms of the weight will remain small of epsilon. Second aspect of these dynamics is the role of the sign. So as you can see actually, the sign of the VJs is the dynamics are the same. It's either plus or minus. And the picture of everything is that the simplicity of all of this is that if you randomly initialize with the positive sign, that neuron is. Positive sign, that neuron is going to converge to the average of the positive class. If you're randomly initialized with the negative sign, you're going to converge to the negative. Because otherwise, the dynamics are really identical. So as a consequence, from that perspective, for the remaining of the talk, I'm just going to assume that the sign is positive. And since all of the neurons are decoupled at this point, I can just analyze one neuron at a time. And the dynamics are essentially the same. And so this. And so this approximate that is written there again is because I'm neglecting that O of epsilon, and I'm going to be able to do that if epsilon is small enough just at the very beginning of the dynamic. Okay, so what we're going to do next is decompose these differential equations on the neurons into two components, the norms, the dynamics of the norms, and the dynamics of the angular component. So think of this as a polar decomposition, right? The polar decomposition, right? So, how much does the radius change versus how much does the angle change? That's the only change we're going to do. And here is the decomposition. So, that's the dynamics of the neurons, approximately. And if you want to get the dynamics of the norms alone, well, you just use chain rule, whatever. Look, you get nearly identically right. That and that is almost the same. The only difference is that xi.prog with xj shows up here instead of just the x. J shows up here instead of just the x. So it's just taking the almost taking the dot product with w and then normal and normal. So that's the dynamics of the norm. As you can immediately see, the dot product of these two guys is at most the norm of wj times obviously the norm of xj, right? It's the standard inequality. And so with that, I can immediately get this inequality where Get this inequality where this is bounded above by this quantity. And this quantity here is just purely data dependent. So I'm just going to put it into some constant that doesn't depend on values at all. And look at the beautiful simple inequality that we get, which is that the derivative of the norms squared is bounded above by the constant times the norms themselves. And once you get these inequalities and you've done your PhD. Inequalities, and you've done your PhD with in dynamical systems and controls, then you remind yourself: oh, this is the famous Gron-Waltz inequality. And with the Gron-Waltz inequality, you can actually get a closed form solution for the upper bound. So if you've got that, you know, actually, this should be true. I mean, if you took differential equation 101, if this was equality, this would be equality. But there is the Grong once inequality is the same thing as in linear differential equations 101. Okay, so what does this say? Okay, so what does this say? It says that the norm of the weight is less than or equal to the initial norm, and then there is an exponential growth. So they are going to begin really small, but they are going to grow very quickly as determined by this data-dependent quantity C. And well, but if I really initialize them very small, they're not going to grow too much. So that's exactly what we do. We begin with the. This we begin with this being O of epsilon squared at initialization. And we are going to choose a certain time so that we get to all of epsilon. And you can compute this analytically. It's a super simple calculation, but this is the time it takes for the norm squared to go from epsilon squared to epsilon. And so this immediately tells us quantitatively and precisely, this is how long. This is how long you can do this approximation. So, this is the length of the alignment phase. And evidently, if you want that phase to last a long time, you better choose a very small initialization. That's what this does. All right, so long story short, what I've convinced you thus far is if you initialize small, the norms will remain small for some time. And I can tell you how long that's gone. So, that's for the That's found. So that's for the norm only, though. So let's go now to the angles. What happens to the angles? So I go back now to the differential equation on the weight. And what I want to do now is to get the differential equation on the angles. The angles is nothing but the dynamics of the normalized weight. And if you do the algebra, the dynamics of the normalized weight is exactly what's written here. You're going to get this quantity here projected. Projected. Let me explain that in a little bit more deep. What is this quantity really? Forget about that at the bottom for the time being. This is just the weighted average of the data points, right? Xi is a data point, y sub i is just the label plus one or minus one. So if I did this summation only over the points with the positive class, this is just the average of the positive class. If I do it with the negative, this is the average of the negative. It with the negative, this is the average of the negative. But the indices here are determined by the w. So it's kind of I hypothesize a classifier, a linear classifier that determines the split of the data. And now I do the weighted average with plus or minus, depending on the class that the data points take. So it's a weighted average of a subset of the data point, if you will. And then that is projected orthogonally to the weight, and that is the dynamics of the. Dynamics of a unit norm vector. What does that mean? So, in dynamical systems, we care a lot about equilibrium points, right? So, we hope to convert to the point where the derivative is zero. And so, see, if this quantity here, if the weighted average was equal to w, then the projection of w or total to w is zero. So, immediately we know that an equilibrium point would be if If the W is chosen such that I take, so in other words, this is kind of already telling you that, oh, if my classifier was so good that I identified the correct class, then I take the weighted average of the class. That's what the W should be. So it's kind of already telling you that I have this expectation at this point that I'm going to convert to the weighted average of the class. Again, that obviously will require some assumptions of separate. It will require some assumptions of separability on the data, but immediately you probably get it. If the data was really easily linearly separable, then this would be happening. I'm not going to give you the full details of theorems of proof, but I'll give you the intuition as to why convergence happens given these equations. The theorem and the proof is all in the paper. So, what is this dynamic? So, pretend for the time being that this weighted average was 50. That this weighted average was fixed and it didn't depend on W. So going back to here, if that w was not there or if that w was constant, this is just a fixed quantity, right? Okay, so that's therefore my vector, and that's fixed for the time being. Suppose that I begin with this initialization, so that's the initial neuron. What does the equation says? The equation says take the weighted average, project it orthogonally. That's exactly the projection of that. That's exactly the projection of that onto the orthogonal component gives me the black vector. So, this is telling me this is going to be the direction in which the differential equation is going to move. And as you can see, it's moving towards that average that I started with. Okay? But that is if it was fixed. What's going to happen is that as soon as I move, this guy is also going to move. So it's like chasing a moving target. So how much is this going to change? Much is this going to change? So, this is what I said. So, the xw actually depends on w. So, I've got a moving target to. So, let me go through the picture of what's happening. So, suppose that this vector down again is the initial w, and this selection process is going to select the data points that have positive dot product with that point. So, in this case, it's going to select the two red points and the blue point. Okay, so that's the selection it makes. Okay, so that's the selection it made. Based on those, I now need to compute the weighted average, but weighted also by the pluses. And in this case, this is plus one and this is minus one. So I need to flip these two guys before I take the average. So that's what's going on here. So the average of these two negative points flip, and the average of that positive point give me that purple guy. And as you can see, I begin here, but I am going to move that way. So I'm moving towards that guy based on the previous one. That guy based on the previous one. But as soon as I move, I change the selection of the point. And as you can see, it's going to eventually drop one of the negative points. And eventually, it's going to pick up one of the positive points. And I guess you get the point. I keep doing this over and over. Eventually, there's going to be a time that I only choose positive points. Again, some separability assumptions are needed. And once I choose the positive points, that weighted average becomes constant. Becomes constant. And from then on, the idea that I will converge to the average is where you come. Okay? And so that's exactly what we call here this early alignment phase. And so in words, what I just said, the target is always moving, but it doesn't change continuously. It's piecewise constant, and it only changes when I pick up a point or when I drop a point. And eventually it becomes constant. And I already said this. Eventually, this process. Eventually, this process will converge to the positive average. Okay? So let me now, and I think I already said this. So eventually I'll choose all of the points correctly. And so I'm going to divide now this alignment phrase in sort of two distinct parts. Part number one is select the points from the correct class. Part number two, once you have selected them, you still need to convert. Once you have selected them, you still need to converge to that app. And now the quantitative analysis begins, and now is finally Mato's question as to what that separability means, and how do we use the notion of separability too. So here's the notion of mu orthogonal separability, which means that the dot product between any pairs of points weighted by the classes is equal to mu, and that mu is positive, and mu is the notion of mu is whatever. Muse, the notion of muse, whatever. What does that mean in pictures? So, if you have two points that come from the same class, right? In this case, it's the negative class, so that's why there is a minus in both cases. So, points in the same class must have positive dot product, and the minimum cosine must be mu. That's what it says. The negative, you know, if I pick one from this class and one from the other, now only one of them has a negative point. Now, only one of them has a negative point. So, in that case, it means that if I flip one, then I still have that cosine needs to be above mu. So, that's the notion of. So, this means that to some extent, data from the same class needs to be sufficiently close, although the mu could be close to zero, and data from different classes needs to have the opposite. Okay. Okay. Okay, so under these assumptions, here is the first quantitative result of this alignment phase. So how much time does it take for gradient flow to converge to a situation that I've chosen? Situation that I've chosen all of the positive points. And this is the amount of time it takes. So n is the number of data points, and μ is that separability notion. So logarithm of n divided by mu. That's theorem number one. Theorem number two is once I have chosen all of the right points, how long does it take to convert precisely to the average? And there is previous work, so this is a minor adaptation of previous results. But what this said. Adaptation of previous results, but what this says is that it takes this amount of time, again, it depends on the number of points and the separability of the data, to arrive at a distance delta from the average. Measure again in cosine distance. And so now that I know precisely how long does it take to select all of the positive points and how long does it take to converge to to the average within delta. To the average within delta. Now I can say, well, this is how much time I had for the norms to be small. So if the first part plus the second part is less than the amount of time that the norms will be small, then I'm in business. And so that's the criteria for selecting epsilon. And it's as simple as what I just said. And so you can see here how epsilon depends on the number of data points, how close I want to be, and the separation. Be and the separability of the data. So now this gives you sort of the formula for selecting the initialization so that this happens. So long story short, what I've said is that if epsilon is small enough, as stated here, the norms of the neuron will remain of epsilon, the dynamics will be decoupled, and all of the neurons will converge to either the positive plus or the negative plus. positive class or the negative class depending on initial okay once uh i'm there but the loss was still high at this point so now we move to the second phase and the second phase is more standard so once the neurons the the angles are correct this means that now i'm in this picture but i still the norms are kind of small and still the loss is kind of high now what happens is that either you have a positive sign on the on the Positive sign on the V. But if you have a positive sign, it's like training a linear network on top of that. Or you have a negative sign, and in that case, it's another linear network. So this is really the learning dynamics of just two linear networks on top of fixed features right now. And so we can leverage results from training linear networks for this purpose. And this is results. The result is Results. The result is roughly that there is a time, which is log of one over epsilon, and a constant such that the loss is one over C. And this is T minus T2, right? Because this convergence only happens once the alignment has finished. And the alignment is this T2 that is. And these, I think, Matus had some results that are very related to this. Very related to this part: that you know that the neurons keep good alignment with the data once you arrive at this phase. Okay, so to summarize, the summary of the first part of the talk is that we analyze the dynamics of Rayleigh networks with small initialization. There is this initial phase where the neurons just align with the positive or negative class, and the loss is high, and the norm is small, and then the second. The norm is small, and then the second phase where the norms begin to grow, the directions remain the same, and you get a convergence of one over t roughly. Yes. Yeah. But the selection of epsilon so that this happened. Like going back to the previous slide here. Ah, there it was. Ah, there it was. Right. The epsilon needs to be chosen according to this form. First one is: it seems to me that the situation by might be pretty strong. Certainly, if that is violated, but it's holding its station, I should still. I have not, but Handchain has. Yes, so that's exactly the kinds of things that we're working on. There are multiple extensions to this, so I should begin. One is getting rid of that assumption, the other one is The other one is the: can we go beyond binary classification? And this idea that you get the weighted average of xi, y sub i. If the y sub i is not a scalar anymore, that's going to change things. The third extension that we've already done is what about having a bias term? That's the same. You just work in homogeneous coordinates and you're done. So there are multiple extensions, but I think you're right that the Multiple extensions, but I think you're right that the most critical is the assumption of separability. Yeah. And by the way, all of these results are results in probability. So I think what you're saying is true. It's because the ReLU, what it shows up at the end of the day, is the sign of the initial selection. It was what gets clear. Yeah. All right. So let me move now to the second part, which is much shorter, which is about the question of robustness right now. So I guess everybody is familiar with the fact that this. Is familiar with the fact that these deep networks are very fragile and you can do imperceptible changes to the input that would make classification go down dramatically. If you look at the last five years of research in robustness, 90% of it is really practical, developing defenses and developing new attacks and whatnot. But the theoretical work has mostly focused on certified robustness. What is the largest radius that I can preserve the input? That I can preserve the input so that the classification doesn't change. The work that I'm going to present today is of a different flavor and much more connected to learning dynamics. It's like, I've got this algorithm. Does it learn a network that is robust or not? And why? So it's a little bit different. And I think it connects implicit bias style of analysis for robustness purposes. So here's the picture. And most of the presentation, because I'm already very late, going to be very high level. Already very late, going to be very high level and just with pictures. But here's the basic idea: suppose now that the positive class had maybe multiple clusters, more than one. And the same thing is going to be for the negative class. And so in this picture, I've just illustrated that with the positive class has two clusters and the negative class has one. There are technical assumptions that are very strong in the paper, but it's a kind of a common model these days for this analysis is that the cluster sensor. The cluster centers μ1, μ2, μ3 in this case, are all orthogonal to each. So that's probably a very strong assumption. So, based on the analysis that we had thus far, you know, which this is still, I don't think it's a big if, we have not proven this, but this is the picture we have in mind, is well, if everything I said thus far were to work, then we would have convergence to the average of the positive class and the average of the negative class. And the average of the negative class. And so this is exactly what you see here, right? But because the positive class has two clusters, I did not converge to either cluster centers. I'm converging to the average of the. That's the expectation. And so if you now take that trained network and now you attack it after, you get the standard behavior that you do very well, but very quickly performance goes down to zero. Okay, so why does this happen? Why does this happen? So, this paper showed that effectively you can only tolerate attacks whose size is one over square root of k. K is the number of plaster centers. Okay, and so this is, and when you begin to learn averages, that's because the averages are now closer to the other class. And so a little bit more formally, but still just primarily in pictures. Formally, but still just primarily in pictures. The data assumptions are that we have the positive class comes from, say, K1 cluster centers. In this case, all of them are Gaussian, have the same variance. And the only difference is that mu1 through mu k are orthogonal to each other. The same thing is true for the negative class. And so there is a positive class average, which is just the average of the cluster centers of the positive class. Of the cluster centers of the positive class, a negative class average, likewise. And so, if it is true that we get these neurons that only learn the class averages, then the data will be classified correctly. So you would have good generalization, but they will be not robust. This is the maximum that you can tolerate. And this is illustrated with the picture. So, to fix this, what we're going to do is What we're going to do is the following: is make a tiny change and a tiny modification to the architecture. And that tiny change is that if p was equal to one here, right, then the denominator is not there. And then I get the standard ReLU network. So the modification is that we're going to raise the ReLU to the Pth power, and we're going to divide by the norm of WJ to the P minus one power. So that's the new network architecture. And what we've shown is that if that happens now, you will get the network to convert to the cluster centers of the data, as opposed to the class averages. And if that's the case, then you can now get robustness with a radius of O of one instead of O of one over squared. That's the picture that is behind that. And in the next few slides, I'm just going to hand wave my I'm just going to hand-wave my way around as to why this phenomena happen with a few intuitions as to what is the role of P primarily and why we go from learning the algorithm of the class to learning the cluster. Okay, so this is the intuition of what happens. So this is again just the P ReLU network in there. We can rewrite it in a new form where effectively it just looks like the ReLU network, but there is this. Network, but there is this cosine of the angle between a data point and a neuron raised to the p minus one power, but essentially equivalent. And therefore, the p Relea network is just like a weighted Releo network with a weight that depends on the angle between the data point and each one of the neurons. And so if you look at the famous equation that I presented at the very beginning, where epsilon is very small at initialization. Epsilon is very small at initialization, you get essentially the same dynamics that I talked about. Again, weighted average of xi with y sub i, but now there is an extra weight that shows up from that modification. Okay, so the thing that I'm going to try to do now is just hand weight with pictures what is the effect of this weight and why that changes the way you can. So if, and I'm going to illustrate that with only two cluster centers here, mu1 and mu2 in this. Here, mu1 and mu2 indistinct. So if p is equal to 1, that was exactly what I talked about in the previous talk, right? And we know that in that case, we are going to convert or we expect to converge to the average of the two clusters with p is equal to 1. And again, remember the pictures that I was showing that because you take these and you project it orthogonal to the w's, that the entire explanation. If p is equal to 2, If p is equal to 2, obviously that 2 doesn't matter. So it's just that you get a cosine weighted error. So now you have mu1 times a cosine, mu2 times the cosine. Just think about mu1. You get effectively a direction of movement that is perfectly aligned with what the w was. This under extra assumptions of everything is balanced, whether or not, but the intuition is that for the p equals 2 case, P equals 2k's, nothing happens. It's like you don't move. So to break the ties, you need a higher p. So I'm illustrating that here with p is equal to 3. And now you're going to get a much higher weight, cosine squares is what changes. And that's going to produce a weight that biases the situation to be either mu1 or mu2, depending on which one you were closest at the very beginning. And so effectively, the And so, effectively, the role of P is to kind of break the ties of these averages, and that's why you get convergence to the cluster cell. There is a lot of extra stuff in the paper. Yes. I don't know. I think we have the other. Han Cheng, did you hear the question whether you can do P less than one? I don't think that's a good question because I have been lazy about thinking about other regimes, but I think it's doable. But then I think the definition, based on the definition, there needs to be some changes because in the denominator, I have p minus one. So I cannot do p less than one in its current form. But us is. But as of P between one and two should be possible. Another possible conjecture is that you will just get the weighting to be the opposite. So instead of moving towards the closest one, you will move to the other one and you still break the ties. But I don't know. Maybe that's a conjecture, but we have to see. Conjecture, I'm just conjecturing. Yeah. All right. So these are just some experiments showing that a little bit of what happened. That's a little bit of what happens in practice. So, okay, nothing super fancy, standard, and this digit, so nothing super fancy. So, let's look at this picture here on the far right. So, the x-axis is the radius of the adversarial attack, and each one of the different curves corresponds to different values of p. So, this is p equals one, two, three, and four. And you can see how, as p increases, the prime network is more robust. On the left, you see the accuracy, and the accuracy is a little bit the other way around. It generalizes better with smaller t, but the main difference is that here the change is quite large, while here the change is a little bit more minor. So the generalization is not impacted too much. No, we don't know. We don't know yet. We don't know yet. And we don't know yet because the part that is missing in this work is the question of proving all of these converges. What we have is we know from experiments and theory what we expect it to converge to, and for the theory. To converge to, and for the theoretical networks that we expect convergence, we have generalization results and robustness results. But the link of proving all of the steps that I said about convergence, that's not done yet. Han Cheng, did you check the KKT points for the other piece? Uh you mean the KKT points for for the new P radio? Uh Or the original ReLU. I know that those papers about KKD points, but then checking the KKD point for the P Releo will be super complicated because of the denominator. I tried it for one week and then I switched gears. Is there someone speaking? I can still cannot hear the question. Yeah, what he said is that for higher P, he expects the KKT points to be exactly the cluster centers. So he does not expect the calculation. So he does not expect the calculation to be so complicated. Yeah, agree. All right. So just to summarize this second part, the main message is that there is some interplay between implicit bias and robustness, and that RELU networks or data that is clustered in multiple cluster centers have good generalization, but they are not. But they are not robust, and that a simple fix of changing the RELU network to a P-RELU network makes the network more robust. All right, that's all. Thank you. Any other A two-layer, two-layer network, exactly as, but it was done for two class classification. Yeah. It can be used. We have not done experiments, but the theory was restricted to finance. Straight through binary. So that's why we did it out. Yes. Are we trying to get the line? They're both on the small so I cannot get the So we have not done the analysis, but I believe that that's the case. In fact, most of the analysis on our collapse is about what the global optimum looks like, but there is no analysis about an algorithm following trajectories that do that. And so I conjecture that it's exactly the same, that this is effectively a proof of neural collapse for red. A proof of narrow collapse for ReLU network, and that the proof for the matrix vectorization case should just be easier. Yeah, I mean, again, there is the restrictive assumption of mu separability. So it cannot be any matrix vectorization. It needs to be matrix vectorization with data that has that property before you start. But it's it's not enough to just have low rank because you need this mu separability condition on the angle. Yeah, this is yeah, this is for classification. So you would need to have u be transpose x, and the x needs to have x response. Hencheng, the question is for the mu separability that is very block dominant, whether you need strictly the assumption to be in To be in both in the dominant block and in the off-diagonal block? I think it should for all the blocks. So it's kind of like for the main diagonal blocks, I need positive things and the off-diagonal, I need negative things if you're talking about inner products. Mentioned that there's a two-week question first. So, in the two expressions, there's a lot of good there are papers that talk about that as much as we only think that doesn't make the algorithm is the papers I want to talk about as well. That is also a case that we're going to continue pages of the market. 