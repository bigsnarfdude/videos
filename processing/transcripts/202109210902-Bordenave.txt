Adam Maras, who is in Marseille with me. And so we speak about random trees and spectrum. Okay, so I will try to be a pedestrian. So I will start with some basic notions on the spectrum of random graphs. Okay, so the adjacency operator is defined like that. So you have a graph which A graph which is locally finite and non-directed. Then you define its adjacency operator the usual way. It acts on finitely supported function on L2 of the vertices like that. Then when under my assumption your operator is under my assumption it is Under my assumption, it is essentially self-adjoint. I will see some of these assumptions maybe in two slides. And when it is essentially self-adjoint, you can define a unique probability measure, which is called a spectral measure at the vector, which is, for example, characterized by the fact that its moments... There is a big delay on the slides, so it would be difficult for me to make a picture. The moments of this measure coincide with the moments. Insights with the moments of the powers of the diagonal of the operator. And if your graph happened to be finite, then this spectral measure at the vector is simply a sum where you put direct masses at the eigenvalues and you put a weight proportional to the amplitude, the square of the amplitude of the corresponding eigenvector. Okay, and so we will. And so, we will be interested in talking random graphs and, for example, percolation graphs. And the kind of graphs which will be particularly interesting for us will be infinite unimodular graphs. So, what are those? They are rounder-moded graphs which satisfy the mass transport principle, but which you can think about also as a kind of reversibility or stationary property, which can be retained as. Which can be written like that, but for any function which are isomorphism by, which takes as entry what's a graph and two distinguished vertices, you want the expectation of the sum of what is sent from the root to the rest of the network should be equal to what on average to what the root receives from the rest of the network or the brain. Okay, and the very basic And the very basic unimodular graph that you can make is when you have a finite graph, you take a deterministic one, you just choose a root uniformly at random among all vertices, and you will get a unimodular finite graph. And as probably everybody knows or here in this audience, this unimodular graphs, they play a central role in the Beje-Minischram convergence topology. convergence topology because they are the limiting points of the they are the limiting points of Benjamin Ishfarm sequence of a converging sequence of curves okay so there is a theorem here maybe I should have written TRM I forgot but I will try to write with it's difficult I don't know why I don't know why there is a big delay. Maybe. Ah, it's better with the maps. Okay. Theorem, which is due in fact to Nelson. It is that if you have a unimodular graph, then the adjacency operator is almost surely essentially self-adjoint. So when you have an essentially self-adjoint operator and you have a distinguished vertex, which is given by the Distinguished vertex, which is given by the root of your graph, then you can define the mean spectral measure or the spectral distribution or the density of states, test various names, or even the Planchet measure, which is defined as the expectation of the spectral measure at the root. For example, if G is finite and has n vertices, the expectation of the spectral measure, you take the root is uniform. The root is uniformly sampled at random, so it's an average, it's a spatial average of the spectral measures at each vertex of the graph. And if you plug the formula I give you and you use the fact that it is an autonomous basis, you just come back arrive at the empirical distribution of eigenvalues. Okay, so this is a very natural object to study. And the mean spectr in particular, you can see at least in finite graphs, but the mean spectral measure is blind to eigenvalue. Spectral measure is blind to eigenvectors. And the same comment can be made on unimodular graphs when you look at the mean spectral measure. Okay. What will be interesting are question of quantum percolation, which I now defined. So you take your favorite unimodular random graph. The question of mean quantum percolation, you can rephrase it like that. Does the mean spectral measure as a non-trivial? Measure as a non-trivial continuous part, okay? Where you decompose your measure as a sum of continuous part and with respect to the big measure and singular part, okay. Pure point part, sorry. And so this is a mean quantum percolation question. You can also ask a quantum percolation question, which is about this about the spectral measure, this guy. Spectral measure, this guy, which is a random measure, whether or not it has a non-trivial continuous part with positive probability. Okay, or I put absolute, absolutely continuous in parenthesis because you can ask the two questions hardly. And with the quantum, it's an observation that the quantum percolation, if you have quantum percolation, then you must be, then your expected, the expectation will have a non-trivial continuous part. And it also implies a classical percolation. And it also implies a classical perpolation, okay, which is defined as the fact that you are with positive probability you have an infinite connected component. So all this, so these are kind of spectral refinements of the notion of classical percolation. Okay, so they have been introduced by Deugene Lafor in physics, by Deugene Lafour and Miot, and I have quoted names of physicists who have contributed to that. Contributed to that. Okay. So I will speak on about only in this talk mostly essentially about infinite graphs, but I should observe that the mean spectral measure and also the spectral measure have quite kind of good continuity properties for Job Jenny Li-Schram convergence. Okay, so in fact, so as a if you want to understand the spectrum of finite graphs, you should first understand the You should first understand the spectrum of their Beje-Nichran limit. And as you know, many graphs, many random graphs have random trees as the Beje-Lichran limits. So for example, the Herdolsoni random graphs on n vertices, where you put an edge independently between any pair or vertex of probability d over n, or if you take uniform graphs with a given Tego sequence. Okay? Okay, so this brings me to what we did with Adam: to try to study the spectrum of Galton-Watson trees, which are obviously the simpler random trees that you can think of. Okay, so but we have to, our Galton-Watson trees have to be unimodular. So the way you do that is that you have to tilt the root degree. So you start with a measure P, which is a probability solution of N. distribution of n with finite expectation and then you you define a unimodular return tree where the root has distribution p star and then all offsprings independently have a number of neighbors a number of offsprings which is sampled according to the size biased of p star which is uh which is uh this definite with this measure okay it's almost impossible It's almost impossible. Okay. So, for example, if P star is a Dirac mass of D, what you get is an infinite director tree, so a deterministic tree. And if you take P star to be the Poisson with parameter D, then P equal to P star, and you obtain the Poisson better than tree with mean of spring distribution D. Okay, so here are simulations of the spectral measure of this. Of this for the Gelton, for the Poisson-Gelton-Watson measure, for the Poisson-Gelton tree when D goes from one, two, and four. Okay, so you can see that there is an atom and the Erdogan graph with parameter L and D over N converge to the Gettin-Watson tree, which is the Poisson Getton-Watson tree. Okay, so that's how I did this simulation. So, what is known about this guy? So, for the Smith spectral measure, what was known before Pukanan? So, when you take the average, we had a formula 10 years ago now for the value of the atom at zero, okay, an explicit formula. Justin Sales has proved that Has proved that all atoms, in fact, that the set of atoms are dense, and in fact, there will be a positive mass for this probability measure, will have a positive mass at each totally real algebraic integer. And he has even a kind of formula in a more recent paper for the atoms, a kind because it's not complete. Okay, he cannot give you an exact number, but he has a very nice formula for that. Okay, and Okay, and with Arnab and Benit, we proved a few years ago that this measure will have a continuous part if and only if the tree, the Galton-Morton tree is supercritical, d larger than one. Henriquez and Menard have proven asymptotic around the semicircular law when d goes to infinity. And And cost and distance arrays, similar cost and resistance arise, have proved the existence of a density for the around the atom zero. So you look at the mass between minus lambda and lambda, you subtract the atom, you divide by lambda, and they prove that this is positive if and only if d is positive. If d is larger than e. Okay, and there are similar results for general. Similar results for general unimodular guttural Watson tree, but I just illustrated it for Poisson. Okay, so just to understand the fact that there are atoms everywhere, it's relatively easy to understand, even when you are supercritical, even when you are conditional non-extinction. Okay, because imagine that you have the following picture. Imagine that you have the root here and The root here, and you have your graph. Your tree is like that. You have a pending to the root, you have a sub-tree T1, pending to a neighbor of the root, you have a sub-tree T2. And here you have whatever you want. Okay, if T1 and T2 are isomorphic, and they have, let's say, an eigenvector of psi, then when you form this eigenvector, when you put zero here, psi here, and minus psi, it's easy to see that it will also be an eigenvector with the same eigenvector. An eigenvector with the same eigenvalue of your true. Okay, so as soon as you have finite pendings and trees, you will have atoms everywhere. And it's not the only reason for having atoms, but this is an important reason. Okay, so for the spectral measure, so when you do not average, so there had been some work on the stability of the absolutely continuous parts, okay, of the spectral. Okay, of the spectral measure of the host when the degree distribution, when you are very close to an infinite diagram tree. So in some sense, if you are close to a diatomus at D, but you have no leaves, then Keller proves the stability of the absolutely consumed part. And when you add leaves, meaning that you are allowed to have P star equal to one, so P of zero, the sum of your it's supposed. well some of your it's possible some of the springs can have uh no no springs then you have leaves uh then uh i also proved a simple statement okay but this was perturbative method by comparison to the infinity urgulant so in this talk uh what i will do is we will prove a stability of the absolutely continuous part or the spectral measure at the root Uh, as soon as for the unimodular gate-made entry, as soon as the relative variance is small, okay? So, where relative variance is a ratio between the variance and the expectation square. Okay, so I will illustrate our results on the Poisson-Berton-Watson tree. So, we have this theorem. So, you take a Poisson-Berton-Watson tree with parameter D, you fix an epsilon, and once we Fix an epsilon. And what we prove is that if D is large enough, so there is a number of D0, then conditional non-extinction, the absolutely continuous part of the spectral measure will be non-trivial and will have, in expectation, a mass at least epsilon. And the proof, in fact, exhibits a deterministic boreal set where Borel sets where conditional non-extinction, this random measure will have a positive density. Okay, so the deterministic Borel set is horrible because it has to avoid all totally real algebraic integer, okay, because we know that there are atoms here. So this Boaset is horrible, but still it exists. Okay, so an interesting question is the quantum percolation. The quantum percolation threshold: what is the value of D zero? What is the limit of D zero when epsilon goes to zero? Okay, so Bauer and Golinelli had some evidence that this could be E, but then I look at the older paper by Aris and Evangelo and Economou in Physics, and they made a prediction that this should be around 1.42. So I didn't understand the argument, so we don't know what is the answer. What is the answer? Okay, but it's a very interesting question. And also, if you want to come back to the mean spectral measure, by Fuinist theorem, the fact that you have an absolutely continuous part for the random measure implies that when you take average, you have an absolutely continuous part. Okay, so our result also proved some results on the absolutely continuous part of the mean spectral measure. Okay. Okay, I will illustrate our result to the second corollary. So, oh, I was supposed to make a picture here. Let's see if I manage to do that. The curve setting soft core is what you get when you prune a iterative. So, imagine that you have a graph, so sorry, because my iteratis. Imagine that you have a tree like that. Imagine that you have a tree like that, okay? So, what you want to iterate is to you will go from this configuration, okay? You will remove one vertex, one leaf, and its part, okay? And then you will arrive. I made a wrong picture. You reverse. If you remove that, you will arrive at a forest with that. Okay. Okay, this is one step of the pruning step, and then you will hide that. Okay. And you look at what remains, you look at the infinite, if you do that on an infinite tree, and you look at what remains. What remains, so you will have this will produce isolated vertices and possibly an infinite forest. And you look at what remains, okay? And what remains will be non-empty if and only if D is larger than A. Okay, and the law that you will get, the law of the random forest, if you condition of the wood being on it, it will be a unimodular vector-watson tree with a Poisson distribution, whereas the Poisson is conditioned to be at least two. Okay and the way the reason I mentioned this this score is that it's conjectured by Berl of and Geralini that this core should be the backbone of the absolutely continuous part of the random tree. Okay and what we can say about this core is the following that if t as if you take a random tree which has this Gatan-Watson tree unimodular Gatton-Watson tree where the degree distribution is a Poisson condition to be at X2. So this is a low Two. So, this is a law of a core. Okay. Then you first the same statement than before. For any epsilon, if the degree is large enough, you will find some absolutely continuous part, which has mass larger than epsilon. Okay, so I should put epsilon between zero and one. Okay. Otherwise there is a problem. There is a problem. And if you take any E, so this is, if you take any E, which is between 0 and 2, you can prove that there is a threshold where almost surely on this interval, on this open interval, mu 0 will be absolutely continuous on this interval with almost everywhere positive density. Okay, which is a stronger statement because you know what there will be an interval where you have. There will be an interval where you have positive density and it will be abstracted continuous. Okay, so these are some simulations, but I realized after doing that that they were pointless because it's very difficult to detect absolutely continuous parts. Okay, I hope my theorem is correct because there seems to be an atom here, but we prove that no. Okay. Whatever. So we have also an application of our result for the Anderson model. Of our result for the Anderson model. So, the Anderson model you take on a tree, you take for its simplicity the infinity regular tree, and you look at the adjacency operator and you add lambda times v, where v is a diagonal operator, is a diagonal operator where you put a potential on each side, which are ideally random variable. Okay, and there is a classical theorem by Klein from the end of the 90s. From the end of the 90s, which we have been able to prove a kind of uniform version for that, which is so we prove the stability of the absolutely continuous part, a statement which is uniform in the degree D. Okay, so if you have forced moment uniformly in D larger than three, you will prove that for any epsilon between zero and one, if lambda is less than a constant time square root d, then times square root d, then you will have the absolutely continuous part will be have mass at least epsilon. And the same statement as before, if you take an energy level E and there will be a threshold such that if lambda is less than square root d times is constant, then mu naught will be the spectral measure of the root will be absolutely continuous with the positive density on this interval. Okay, so they have been done. Okay, so there have been various proofs of this result, and the novelty lies in the uniformity. It's not that important, I think, but it's more that we have a more, we have proved a kind of relatively new proof of this classical result. Okay, so I will give you the main technical result. Okay, so it starts with the reservant equation. Equation. So we focus just for some just take Gaton-Watson tree with offspring distribution D. Okay, let's forget about the immodularity. So we take D to be the expected number of offsprings. And you look at the Anderson operator that you write like that, where you rescale the adjacency operator by square OD. And V is the idea on the variable. Okay, so if Okay, so if by using the recursion of the tree, you find that the resolvant at the root, if you define it by the diagonal of the resolvent at the root, okay, so it's a green function, if you want, at energy at z, you take h minus z minus one, where z is any point outside the spectrum of h, satisfies this repertoire equation, okay, where g of z has the same laws and minus z plus v and one over d times the average. over d times the average of the gi z where gi are i decopies of g okay and v and n can be arbitrary they not they need not be independent but okay so you want to use this recursion to prove that to prove the statement I mentioned so what you would like to do is to study g of z when z gets arbitrarily close to the real axis okay Axis. Okay, so this I've written down again this recursive distribution equation satisfied by the resolvant. And n is the expectation of D. D, sorry, is the expectation of N. Okay. So if you imagine that L is equal to D and all Gi are constant, you will arrive at the fixed point of the semicircular law, which is gamma of Z equal to minus Z plus gamma Z. Okay, and you put also V equal to zero. Okay, and you put also v equal to zero. Then our main theorem is the following. So I will try to walk you through the statement. So you assume that you have no leaves, okay, but with probability one, n is at least one. Okay, we'll see on the next slide what to do when this doesn't work. Then you fix an energy E, and the results say that if a control parameter is small enough, then is small enough, then you have a control, you have a L2 bound between G of Z and gamma of Z, and also a L2 bound of the inverse of the imaginary part of G of Z. And importantly, this bond is uniform on the real part, on the imaginary part of Z. Okay? And what is this control parameter? So it should be small if lambda is small, where lambda is a five, look, just forget about that, just look at this. Just forget about that, just look at this event. You would like this event to be of high probability. What is this event? Is the fact that the potential v is small, but you have at least two offsprings, and that n over d is close to one. So that's where we retrieve our relative variance estimate. Okay. So as soon as this event is likely for small lambda, for small enough lambda, you will have the fact that G of Z and gamma are G of Z and gamma are not so far away. Okay? And this theorem implies the result I mentioned you. So all constants could be made explicit, but we didn't make this effort. So I did it for Gatton-Watson 3, not unimodular, but there is a code RE for Gatton-Watson tree, but you can write down. I also quoted the result in LP, but you can do it for general LP. And it implies, for example, implies for example there's some consequence for the localization of eigenvectors because they are using a result of an underman and survey on the uh uh quantum uh uni herbalicity on the for laplation on crafts etc and uh what happens when you have leaves for example if you enter it stick by percolations or poissons you you will have to put what do you do for leaves To put what do you do for leaves? Is that what you do is you have your recursion? So, imagine that this is the root. So, if you have leaves, there are some trees which are finite, you have some subcritical trees, okay, and you have one which are infinite. Okay, so what you will do, okay, what you will do is that all vertices, all of your offsprings which gives birth to, give rise to To give rise to finite pending tree, you put them into a potential V, inside the potential V, which will depend on Z. Okay, and you kind of apply the result. But then to apply the result, you need a bound on that. So you need a bound which is uniform in the imaginary part of Z. And for that, that's where you restrict yourself to a set of Z whose real part is some complicated. Is some complicated set, or I'll set B. Okay, so I don't want to put more details here because not pushing. Okay, so that's how you go from this result on a Gelton-Watson tree with no leaves to Gelton-Watson tree with leaves and, for example, POSIN. Okay, so hopefully we followed the approach which was initiated by Frozen Asler and Spitzer. For the Hassler and Spitzer. So, for the Hassler and Spitzer, they are given a geometry proof of the Abel Fland on the stability of the atrocious continuous spectrum on the regular trees for the underside model. And the way that I observe, it's a very nice observation, is that instead of looking at, we would like to control the distance between G of Z G of Z, we would like to control the distance between G of Z and gamma of Z. Okay, but we will not control the Euclidean distance, but we will control the hyperbolic distance. Okay, where the hyperbolic distance here is as this formula, and so we will count, and so we will just consider gamma of gh, which is g minus h, modulus of g minus h square divided by m of g m of h. Okay, so for example, Okay, so for example, I will try to make a picture. If I re if H is here and this is the upper half, the point L F plane, this is a ball of points at a given distance from H in the hyperbolic plane. Okay, so this is a ball, but it's not centered here, it's centered H is not at the Accident center. Okay. Okay, and so the strategy of proof is: so let's assume, for example, for simplicity that the degree is constant, okay? So we are in the simple setting where we have no, we have just understood model, okay, which there is no randomness on the offspring distribution. So n is equal to d it's always fixed. Okay, so I recall you the distribution equation for the Distribution equation for the reservant and the fixed point equation for the semicircular Launch law, for the Stiches transform of the semicircular law. Okay, there is a deterministic statement, which is the following, but if you take V to be equal to zero, okay, so you forget about this term. If you look at the gamma, the distance between G and gamma, okay. G and gamma, this will be exactly equal to the distance between gamma and the average of the g i's. The reason is that the map on the hyperbolic plane, the map which to phi of h associates minus one over, if you look at this map. It's easy to see also, but this is a massive symmetry. So gamma of H and gamma is exactly equal to gamma of phi of H and L. Okay. Okay, gamma depends on Z, okay, but Z is fixed. Okay, and then so you are you arrive here, and then there is some convexity which has not been observed. Convexity, which has not been observed, I think. There is some convexity in the definition in gamma, which you can prove that the gamma is convex as a function of this guy. And this is less, so this is less than 1 over d times that. Okay, so it's nearly a contraction, but we would like a strict contraction. So what we prove is that if G I are I D copies of G and independent of V, we improve this convexity. Improve this convexity inequality, this kind of weak contraction equality, in a strict contraction equality, which is of this form. It will be true as soon as the force moment of V is small enough. And epsilon and C are not depending on D because we want something uniform. So this is the goal, and this is a goal followed by And this is a goal followed by Frost, Asser and Spitzer. Okay, so we have to find a way to make a strict contraction. Okay, so that's where we have simplified the previous argument along this line. Okay, so look at this. So you take, you fix some elements on the hyperbolic planes, okay, which are G1, Gd and V. Which are g1, gd, and v. Okay, you look, you define g as being that, okay, which corresponds to the to the recursion equation, okay. And now you define, so the gir are deterministic, v is deterministic, okay, and now you define a random variable which is which takes this value, so g of gr is equal to g with probability one half, and otherwise it is equal to one of the gi with probability one over two d. 1 over 2d. Okay, and then you take a deterministic yet another point on the hyperbolic plane. And we prove the following, which we call asymmetric uniform contraction. Okay, that if you look at the distance, the hyperbolic distance in L2 sense between gamma and the average of G L and G R. And G R. Okay, so remember everything G L is deterministic, all the GIs are deterministic, V is deterministic, and so on. Okay, this will be strictly less with the 1 minus epsilon to what you would get by just using convexity. This will be strictly less, that's the average of the distance, so gamma squared here over 2 times that, provided that V, the absolute value of That v, the absolute value of the modulus of v, is small enough. Okay, that's once if you are once we prove that, it's easy to come back to our problem and prove what I claimed. Okay, so the reason for having a uniform contraction is that whatever, when you make an average, you will contract, provided that you have a little randomness on one of the two elements, and this little randomness. Two elements, and this little randomness is either you apply the Recursion equation or you don't, and you take a sample at random. Okay. I will try to convince you that this is not, why is it true? The first one is using this AMG inequality and the convexity, you can realize that it is enough to realize that it is enough to to consider the case when d is equal to one okay and when the distance and when g one and g l are the same the distance to the semicircular okay and so maybe i will try to make a picture so this imagine that this is gamma okay so this is huge Okay, so I assume that G L and G1. So imagine that here, for example, you are GL, they are the same Euclidean distance together. Okay, so here this is G1. Okay, and the point is that you are saying that what I say is that either, and we should put another point, which is so phi of T1, I recall you, it's I recall you, it's minus. Let's say that this is minus Z plus minus Z plus G1 minus one. Okay. And phi G1 will be also somewhere on this disk, on this board. Okay, but there will not be a line. And the point is that you easily to pull, but either you easy to prove that either GL plus G1 over 2 or G L plus V L over 2 will be much closer to gamma than G L and G R are. Okay, so you can make a picture, maybe G1 is equal to G L, but then phi of G1 will be on the other side of the circle and then on the ball, and when when you pay the average, one of the two should be very close should be much smaller than uh than uh G L and G one. Than GL and G1. Okay? So you have to put some effort into that, but once you make this picture, the proof is true. And our proof is not wrong. Okay, so I think that's now we make some final comments. So once you have done that, you have to put constants and so on and quantify everything, but that's essentially the reason why you can prove this uniform contraction. You have you can prove this uniform contraction, okay? So, uh, so some nice open questions. So, we don't know what is the quantum percolation threshold. Okay, so for Poisson, we know what is the mean quantum percolation threshold by so I think for random for unimodular tree, with thanks to the work with Balint. Thanks to the work with Balint and Arnold, we know what is the mean quantum percolation threshold. But for the quantum percolation threshold, meaning when, for example, if you take Poisson D, okay, for which value of D mu zero is absolutely as an absolutely continuous part, it's not clear with positive probability. So we only give an upper bound. We just say that this there is a D subset, the quantum pattern. That the quantum percentage threshold is not trivial. That's what we prove. Another question is: which we prove absolutely continuous, we prove that some absolutely existence of absolutely continuous part of mu zero, of mu of the spectral measure of the root, but we don't know. I would like to prove another regularity for this random measure, but using the same kind of ids, but there are some problems. Of ideas, but there are some problems. In particular, the complexity argument fails. So I don't know what okay, it's not obvious. I hope. Okay, maybe it would be nice to extend this method to more general structures where you have trees lying lurking behind. Okay, so this has been done in some settings, but which the settings. But which the settings where it has been done is are non-unimodular and curves, so which for me are not the one I'm aiming at. So, for example, for the Anderson model, we are currently trying to consider instead of looking at the free group. So, if you take the free group with the natural generator, you have the free group FD, you have the 2D infinite repeal R3, okay? But you could take another set of generators of the free group. set of generators of the free group and ask and look at the Anderson and look at the Anderson model and ask whether or not at small disorder, so when the diagonal component is small, whether or not you have absolutely continuous parts. Okay, so there will be atoms when you take away a set of generators, but the fact there will be absolutely continuous part also, and whether or not these absolutely continuous parts are stable, okay, at small disorder. So in the same vein, you could try to do And in the same vein, you could try to do some free products of finite groups. The reason for these two models is that instead of, if you want to study that, you don't have a scalar recursive equation like for the Girl-Watson tree, but you have a matrix-valued, you can write down a matrix-valued recursion equation. And then you can put on the on a matrix. A matrix analog of the hyperbolic Procar√© half plane, you can put an hyperbolic metric and try to prove that, try to push the argument, this geometric approach to stability of absolutely continuous spectrum in this matrix value setting. And more generally, it is not clear whether or not the Whether or not the stability of the absolute equant spectrum is a root property. Okay. Meaning that if it's true for a set of generator, okay, maybe there could be some, meaning whenever there's an absolutely continuous part for a given set of generators, you could ask whether or not there is some, you can put some disorder, a disorder, and preserve the function. Disorder and preserve the obsessive functions part. Okay, so this is a possible extension. Another extension, which is more difficult, I think, but that is first to observe that if you look at the percolation on Z D, you look at edge percolation on Z D where you keep each edge with probability lambda over 2D. Then in the Benjamin-Schranz sense, as a dimension goes to infinity, it converges to our unimodular pattern of. Converge to our unimodular pattern of sentry with Poisson Lambda distribution. Okay, and since we have been able to study this one, it is not clear whether or not we could try to say something about high-dimensional Euclidean matrices, about the existence of absolutely continuous spectrum, or again on the for the for example for the percolation. Okay, we don't know, but it would be very interesting. Know, but it would be very interesting to have any answer in this direction. And a final comment: so I mentioned non-directed trees, but you can also introduce the adjusted operator of directed trees. And there is an analog if the directed tree is a unimodular, then there is an analog of the mean spectral measure, which is called the Brown spectral. spectral measure which is called the brown spectral measure and essentially nothing is known on the brown spectral measure of random directed trees and and in particular the regularity the regularity so it will be a spectral measure on on c now not it is not a probability measure on r but it's a probability measure on the complex plane and it could be nice to see if some of what i've presented can be used to prove something non trivial about the bond spectral measure. Trivial about the bound spectral measure of this Gerton-Watson type theoretic trees. Okay, thank you. That's all I had to say. Questions either for Zoom or for the team? Charles, so you were focusing on what's somewhat, but you said some results on general trees, right? Only on general, any unimodulous tree? On any Guten Watson tree, yes. But for specific questions, how about you just calculation on the regular tree? Ah, that's fine because. Ah, that's fine, because if you do perturbation on a regular tree, then it's a unimodular Gutton-Watson tree with binomial distribution, degree distribution, and then you can apply the result here. What is the result here about the absolutely continuous spectrum? What is the condition? So, it's exactly so that we don't you have so if you So if you look at so what you're asking is if you you look at binary n d over n, if you have a you do percolation on the n on the n regular infinite tree, right? Okay and so you look at so you you have the same uh you have the same result okay and so exactly but here you have to And so exactly but here you have to put n. Okay, and this row is uniform in n. So you have this statement is true for this guy. Okay. Thanks. So thanks, Marlika. Thank you. We'll do it in 30 minutes, that's fifty fast. Thank you very much.