Waiting. Okay. So let's start right away. Okay, I'll start then. So, this is Jason. Thank you for joining, Jason, from Duke University, and you do the rest of the trick. Okay, great. I'll try not to take up too much time. Thank you all for being here. It's an honor to speak here. And I'm unfortunately not talking about the thing that it says on the schedule, just based on after there was a schedule change, I thought this fits better for the time slot. And also, it looked like it was related. And also, it looked like it was related to sort of more of the theme of this morning. And I hope you'll agree after seeing those nice talks to set up this problem. And Michael, you know, I'm obviously more of a statistician as well, but I'm trying to, these are motivated by the kind of problems I hear by all of us talking about. And I'd like to maybe also apply these to sort of more modern data settings. So we're going to talk about yet another clustering method, even though nobody asked. And this is joint work with my visiting student of mine, Saptarshi, who is now at Berkeley. Tarshi, who is now at Berkeley. Okay, so as we've seen, clustering is kind of a cornerstone of unsupervised learning and pattern discovery. We're just trying to find, you know, find groups in the data. And so there's a lot of methods for this. And, you know, especially within a pipeline, people tend to pick the popular approaches based on simplicity. You'll do something like k-means or maybe hierarchical clustering or density-based methods. But a lot of these enduring methods, they struggle quite a bit in high-dimensional settings. And regardless, whether it's just Settings. And regardless, whether it's just kind of a large number of clusters, whatever it's a non-trivial problem, they tend to be quite sensitive to local minima because they're non-convex objectives. Okay. So we've seen a little bit of this already in the previous talks. This is just one example, simple example because it's in the plane. Two dimensions, maybe like 50 clusters. This is just, if you run k-means on this, this is, you know, this data generated under ideal conditions for k-means. It's like nice Gaussian things. They're pretty separated. And yet the partition you learn by running k-means under this initialization. You learn by running k-means under this initialization, you see that you've kind of split this obvious cluster in two by accident. This one split in three, whereas other times you have two parts of clusters that should be separate in the same cluster. And of course, you can try to get the best of many restarts, but that becomes harder when it becomes less trivial. So one kind of classical approach is, like I said, we know this is an NP-hard problem. It's combinatorily intractable, but there's a lot of greedy approaches out there. And what you might try to do is just either do clever seeding. Either do clever seeding so you have a good initialization, and/or take the best of many random initializations, right? So that's limited. I have some work too on another line of approach, which is to smooth out the bad solutions using some kind of techniques like annealing. So you, in our work, which is power k-means, sorry, getting these mixed up, this is the k-means surface. And by kind of upping the temperature in a way that preserves the simple updates, you can kind of avoid those peaks and valleys in the intermediate stages. Those peaks and valleys in the intermediate stages, but then still target the exact same objective. Because, again, people want to use k-means, it's interpretable. And so we don't want to produce too many new objectives. Similarly, what I'll actually focus on in this talk is, you know, if you're doing something like hierarchical clustering, when to do merges and splits in the dendrogram of the data is also greedy. And so there's usually no guarantees for optimality here, except in very special cases, like single linkage. Okay, that's the clustering. So now I want This clustering. So, now I want to talk about the other line of approach that many people have taken in statistics to problems that are, you know, non-convex. So, a lot of people will look at things like convex relaxations for ideas such as regression. This is all over the place. But the idea is to propose some kind of stability to a problem that does have a lot of peaks and valleys in the zation. So, this is not our method. This is called convex clustering, also called sum of norms clustering, that is actually reasonably recent in the convex. That is actually reasonably recent in the context of just convex relaxations, which is huge in learning. And so, the idea here is it's different than k-means. It's actually related to hierarchical clustering as we'll see. So, here are your data points, X sub I. So, I goes through one to N. That's your data set. And, you know, they might be P-dimensional. And so for each data point, you're going to associate a cluster, stand mu. So there are n of these mus. Kind of silly, because if I were just to minimize this measure of fit, I would set each mu to each corresponding x, and I'd be dot dot. To each corresponding x, and I'd be done. That's the optimal solution. The loss is zero. So, of course, instead, you might want to realize this by having some kind of penalty. This might look like some kind of group penalty because what we want to promote is some kind of shrinkage in the differences across, right? So, n rows of these, and all the pairwise differences I'm going to penalize. So, I'm trying to promote a smaller number than. To promote a smaller number than n of unique, right? So, what that means is that penalty is hoping to merge some of these n unique cluster centers so that I'll have fewer of them, of course. And I can crank up the penalty to have some reasonable number of clusters that's less than my number of data. And so, as you can see, this is related to hierarchical clustering because what does hierarchical clustering do? Under some measure of distance, such as this, you're going to You're going to enforce that there are at most T different clusters, right? So it's a weird way to write it, maybe. A lot of people just see this as a tree, but you have a bunch of data points, there's different distances, and as you move up the tree, you're saying, I'm going to now merge more and more of them. Or I'm really bad at this pointer. So hopefully this kind of makes sense as a way to write hierarchical clustering. But it turns out, just like we saw in some previous talks, when you have these kinds of penalties, it might actually end up being a very hard non-confirmation. Actually, it ends up being a very hard non-convex, in fact, combinatorial problem. Okay, so this is actually the strict convex relaxation of this. And so, of course, we know that convex problems tend to be nice mathematically, but at the same time, it's a proxy for the original objective. So you might be kind of far from the truth. So in this convex clustering problem, notice at this objective, I also have these terms phi. And so these are actually a little bit heuristic to choose, but they matter a lot in the performance. So this kind of allows. In the performance. So, this kind of a lot of sort of recent last 10 years, people have been studying the empirical performance of this. But you do need to set those fees, which are called affinities, very carefully. So in this kind of somewhat recent paper in statistics years, you have this convex clustering implemented, and we're choosing either those fees to kind of be constant. Sorry, we're either going to choose them to be constant, or we're going to choose them to be based on the k nearest neighbors and zero L. Nearest neighbors and zero otherwise. So, in this particular data set, when you have constants, like when you're kind of doing the shrinkage everywhere, you actually do not get any merging of those cluster centroids until the trivial solution. Whereas when you have a different setting of those affinities, as you crank up the penalty parameter, you know, so everything starts with one center per each data point. These are the mus identical to the X's. As you crank up the penalty, you're going to actually see merging along this. You're going to actually see merging along this path, kind of like the lasso path, and in a non-trivial way, right? So, once you crank it up at a certain point, you'll see like three clusters that kind of makes sense. And then, of course, if you crank it all the way, you get the trivial solution, just like you get the zero solution under lasso. So, does that kind of make sense as an overall framework? This is past work. So, now this kind of problem, we already see there's these challenges, and this kind of specification heuristically is really hard to do in high dimensions. You kind of run into more problems, as you might expect, because many of us have worked with high dimensions. Might expect because many of us have worked with high-dimensional data. And when you have high-dimensional data, when you have more features than samples, you need to do something about that problem, right? In regression, you have an underdetermined system where you have infinitely many solutions. And unless you're looking for some structure, like a sparse structure, you don't even have a unique, meaningful solution to your problem, right? Sort of related, because this is not a linear algebraic version in the context of clustering, I still have Euclidean distances and my measures of fit. Euclidean distances and my measures of fit. And this kind of related problem, these don't really become meaningful in high dimensions. All the masks kind of goes to the corners, and things are kind of, the distances between things tend to be kind of washed out in uniform, right? So this is becoming less and less meaningful in high dimensions. Now, something that I think a lot of people apply, especially maybe in comp bio. So people in this room, correct me if I'm wrong. If you have too many genes or you're in some high-dimensional setting, you might apply some generic dimension reduction first, right? You might do PCA. Reduction first, right? You might do PCA if you're a statistician, some kind of spectral embedding and do something like spectral clustering, which is basically instead of PCA plus k-means, you do spectral embedding plus k-means in the eigenspeak. There's also other embeddings, of course. And that's a great idea if your main goal is simply finding groups in the data, like I motivated in the beginning of this talk. But these things, while they reduce the dimensions for computation and for ease of use, you do lose interpretability of the features. And this was also touched on in the previous. The features, and this was also touched on in the previous talks, right? If your original data is n subjects and p genes, and p is greater than n, so you don't like that. Well, once you run PCA, you now have principal component one, two, three, instead of genes that mean something, right? So it would be nice to do these kind of variable selection techniques where I'm saying I'm zeroing out all but the relevant ones because now I can look into, hey, what does this gene have to do with this group of cells by type that I've kind of discovered? And of course, there are even more heuristics. People might do some kind of manual filtering based on. Do some kind of manual filtering based on like what things kind of look like they matter and what kind of don't, do kinds of normalization. These things scare statisticians like me, and they don't really come with guarantees. Okay, even more previous work. I think my goal in this talk is really just to sort of motivate the challenge in the area. And so one very natural thing to do, we already had a convex relaxation of hierarchical clustering, we being previous scholars. And this group by Wang, they actually just, you know, to overcome high dimensionality, your first idea might be, Idea might be: I've already looked at sort of a group lasso sparsity on the row differences in mu, right? I'm promoting a small, unique number of cluster centers, so making them merge. On the other hand, I could also penalize just the columns of the mu matrix, right? So I'm saying, well, I'm just going to have a lasso-type penalty on the features, one through p. That's an obvious thing to do. And it's nice in some ways because if this is a convex relaxation, this preserves convexity in the objective. And, you know, the sort of solution method is similar to what people have done, but I actually think that these methods aren't super accessible to people that aren't in computational statistics. So the way that convex clustering was solved, this part, was using things like ADMM, which are kind of primal dual splitting methods, and not in every statistician's toolbox, even though I like them. When you add another penalty like this, you get even more computational cost. Moreover, different implementations, rather, different algorithms that should solve a convex problem, even in their paper. Convex problem, even in their paper, give you very different results, which is kind of worrying, right? Because it's theoretically, there's actually a unique optimum when you kind of put it in a nice convex framework. Even though mathematically, that's nice to have this nice convex framework, you know, you are paying a price because every time you have a convex relaxation to what you really want, whether it's sparsity or whether it's hierarchical clustering and merging, you are going to entail a lot of shrinkage toward the origin because of the excess panelization of all the variables. So you can get a lot of spurious inference false positive. That's well documented. Of spurious inference false positive, that's well documented using these kinds of penalties. Okay, so we're finally going to arrive at what we are proposing to do, and um, it's kind of basically rather than doing a lot of convex relaxations one on top of the other, we took the original convex formulation of hierarchical clustering, but we actually took a step back closer toward something more non-convex. But our argument is that, well, that's a small price to pay to be able to also do adaptive feature selection and overcome some of those heuristics. And overcome some of those heuristic problems in specifying the initialization of convex clustering. And also, in particular, to formalize that we're paying a small price in terms of making it more non-convex, less convex, we're still able to prove a lot of nice theoretical guarantees because this is still biconvex, which means in each variable, it's convex with the other held fix. Okay, so let's talk about this objective. We're modifying the convex clustering criterion. It looks like a small change. It looks like a small change rather than the Frobenius norm or the L2 norm. We're now introducing this thing called W. And W is going to be part of our optimization variables. Okay, so originally we were trying to find the centers that fit the data subject to a penalty that we want them to merge. Now we're saying, hey, the measure of fit is actually modulated by something that looks like this. And this almost looks like Mahalanobisch distance, except we have an L1 and L2 part that resembles elastic net. In fact, In fact, it's not actually like elastic net where we kind of want to compromise between ridge and L1. It's rather, you could imagine that if you want sparsity in the weights, right, for the high-dimensional setting, you could imagine that if some of these weights become zero, because when you have an L1 type thing, like this term, subject to a constraint, you will promote many of them to be zero. This is just kind of a known fact in convex analysis. But without some term here, this will actually put all the mass to the most important feature. Put all the mass to the most important feature. You actually get a degenerate solution. Yeah, question. It's a great question. We actually thought, yeah, the question is, okay, if you believe that this W norm is kind of, you know, representing your data better, why not also put it in here? We actually are afraid it's sort of a double counting type thing, okay? Because Because as we'll see, we are going to put the this is already telling you where to take these differences, and we're going to put some W's into the fees. So, yeah, so we'll see that in the, hopefully it'll be convincing when we get there. Because another sort of lurking thing, like I said, one thing is we want to be able to do feature selection, so we better have something that entails feature selection, which will be the W subject to a constraint. The other is, like I mentioned, there's this nasty problem of these fees. Okay. So as we see, this is. So, as we see, this is a sort of strict generalization because if I just took all these weights to be equal, I just recover convex clustering. But of course, this is going to try to weight. There's a W for each feature, right? So we're learning these feature weights and we're imposing something like sparsity, but we're not assuming sparsity strictly. It'll just promote sparsity. We'll see that we actually get an even simpler algorithm despite adding something here by doing one other relaxation in a sense. So this is sort of just the norm penalty, like a Q norm, and it happens to be. Like a Q norm, and it happens to be that Q is two. But what we'll do is we'll also square that penalty. Okay. And so when I square this penalty, this is more like relaxing the group sort of lasso type thing to a group ridge type thing. And when I do this, I actually lose exact merging of the centroids, but I'll still have the mus agglomerate very closely to one another. And that's important because even under convex clustering, they never equal each other numerically. And one has to do something like a tree cut or something at the end. And we will basically. Something at the end. And we will basically preserve that cost. So we'll see that later. Right now, I just want to hope that the objective kind of makes sense. We're now learning sort of a representation of the feature dimensions. And in fact, reducing the dimension by setting some W's to zero. That will help us make the measure of fit sort of more faithful to the data that and sort of the manifold they should live in. And then we also, of course, just still have this kind of sparsity-inducing penalty. So, this is kind of the part that I'm saying. Is kind of um, this is the part that I'm saying we kind of do want that information in here as well, right? So, uh, when we want to evaluate or set these affinities on where I'm even going to decide whether clusters are close to one another, you could imagine that in the high-dimensional setting, especially, even if I initialize this with something like K-neare neighbors, which is what's done in the literature, well, those distances originally didn't make much sense, right? Euclidean distance wasn't good. So there's not going to be a lot of information in that, and there's no way to recover if you sort of. To recover, if you sort of define an objective that's kind of garbage in the first place, so when we're learning the feature representation adaptively with our centers, we're also allowed to put that information in here if these fees were defined with respect to some distance, such as K-nearest neighbors. So let's see what that looks like. Oh, first I had an illustrative example, right? So this is taken from a sparse K-means paper by Witten and Tipshirani. This is kind of a landmark paper in this area, and we just made it slightly harder. And we just made it slightly harder. So, what they did is they took two very well-separated obvious centers, obvious clusters in two dimensions, but then they appended 12 features that are just noisy just to make it a harder problem. So this is just to confuse the algorithms. And the ground truth, basically, we just plotted our method solution because it matches the ground truth. We found the cluster centers exactly where they should be. When I don't take into account the fact that it's high-dimensional, just try to run it in the sort of way that these are specified with content. In the sort of way that these are specified with convex clustering, I am missing all of the clustering structure. Under sparse convex clustering, it fails similarly. Sparse k means you know at least I'm able to tell it where to cut two clusters, but it's confusing them because we made this example slightly harder than the analogous example in this paper. And then sparse harkle clustering is doing okay, but it has some confusions. The interesting thing here is, again, I want to sort of emphasize the fact that we can learn these representations, these affinity weights. So even under this Affinity weights. So, even under this competing method, using the same implementation that kind of got this bad solution, if I then give them the fees, the affinities that I learned from my method, use that as input in their method, it actually can obtain the perfect clustering here. But what's interesting still is that the cluster centers it estimates are quite shrunken toward the origin. They're very close to each other. So, when I assign labels to the closest center, I'm getting 100% classification accuracy. But you can imagine for a less toy example, because Imagine for a less toy example, because these are in the wrong place, if there were more sort of noisy data, it'll end up with a lot more error, whereas this looks to be doing the right thing. So, putting some of the results first, if the affinities, how do people usually set them in convex clustering, they tend to do something like a K-nearest neighbor sparsification times this kind of Gaussian-looking kernel. And there's some intuition behind this, I think, but it's kind of loose intuition, which is this resembles ideas from spectral and. This resembles ideas from spectral embeddings, right? So you kind of want to sort of do this kind of Gaussian kernel thing to look at a different representation of your data. But when you run this kind of K-nearest neighbor stuff on a data set that's even moderately high dimension, you see that there's a lot of noise here. Whereas if we learn them using our method, you get a very nice separation of the clusters. So that's kind of a clue to performance. Any questions so far? Great. Great. So, how does the algorithm work? Because we squared that penalty, we have a really nice block descent algorithm. So, basically, the centroid minimization is a closed form given W, and in fact, can be parallelized component-wise. So, this is super easy to do. To update the weights, we still need to do something like lasso, but you end up having a nice soft thresholding. You know, this is a proximal step, so it's very well behaved mathematically. And you do need to do univariate root finds to get the sort of dual parameter, as I like to think of it, that you. Parameters, I like to think of it that you that come out of the Karsh-Kucker-Kahn-Tucker conditions. So, the idea is though that really you just have these two steps, and each sub-problem splits in variables, so it's kind of really easy to implement. My student was more of a theoretician, so our implementation is a little shoddy, but you can imagine that this is much more transparent than a lot of these ADMM primal dual splitting algorithms. So, I have a screenshot sort of of the algorithm in the paper. You implement these steps, and then optionally, the cool thing about our method. Optionally, the cool thing about our method is because W's are changing, you can choose when you want to recompute the K nearest neighbors. And this step is actually going to make a big difference in practice, but it's kind of well justified because, of course, your nearest points should be learned once you've actually learned the feature representation. And it's not always obvious that updating steps is going to lead to a good solution, right? This is also in k-means and other greedy approaches. You kind of update between steps and you don't know where you'll go. But we can prove nice statistical properties of this particular objective. Statistical properties of this particular objective. Again, let's just kind of convince ourselves that this works well. So, revisiting the motivating example, but from the perspective of the dendrograms that come out of these methods, if I have a sparse hierarchical clustering method, this is also in the Witten and Tipshrani paper, you can see that it's really, really confusing the data in the sense that if there's nowhere I can cut this tree to kind of result in the right two clusters. When we use our method using these adaptive affinities, it's Using these adaptive affinities, it's a really clear separation. So, you know, if I just use dynamic tree cut in R, there's no way I'm gonna make a mistake in classification. I can, this is a really easy problem to solve. Again, with the sparse convex clustering approach, it actually looks even messier than the hierarchical clustering with sparsity. But when I give it the learned inputs from our method, we see that the method itself is okay. It still has a little more bit of noise here, but really it's because we failed to learn the feature space properly because we don't have adaptation. Okay. Adaptation. Okay. More affinity stuff. I'm going to skip this slide because I do want to kind of get through this quickly. I want to show at least that we have some statistical properties, right? So this is the stuff that we do because we're not computational biologists. We're kind of waste a lot of years on this. But the convergence properties are not on, they're not surprising. We have a nice block coordinate descent algorithm. So we're going to get a coordinate-wise minimizer of the objective, not necessarily global, but we have global conversions in the sense that it's. Global convergence in the sense that it's agnostic to starting position. We also have nice finite sample error bounds on this, which is a really nuanced analysis because we have kind of the norm that we're trying to assess error is also adaptive with the data. So we have to use these new stochastic Hansen right inequalities. But the idea here is that this implies a type of consistency with respect to the population level minimizers. That's hard to come by with these sort of optimization-based clustering methods with no model-based assumptions. Very good question. This is fixed affinities. That's a really hard analysis. Sorry, the question is: what about the affinities assumption here? This is only for fixed affinities. And the previous methods actually only also have convergence theorems for uniform affinities that are fixed, but do not have such statistical results, just have kind of convergence results. This is a kind of finicky analysis, but we were excited because there was a recent preprint that allowed us to push that through. Preprint that it allowed us to push that through. Okay, so very transparent simulation studies. Again, just making that illustrative example probably a little harder. So just very nice, well-separated clusters in five dimensions, add a ton of noise features, and just see how things, how sanity checks go through for things like feature selection, clustering accuracy itself, as well as stability, right? Because we're still not convex, but I'm claiming that we're much closer to convex in that kind of performance than the chaos. And that kind of performance than the chaotic performance of hierarchical clustering in a sparse season. So, in the five real features we have, when we're learning the sort of features under these different approaches, we are able to recover sort of, we're able to assign mass in the five true features in a meaningful way, and still with sparse k-means. But, you know, on the other hand, we're consistently zeroing out or putting near-zero mass to all the nonsense features, whereas sparse k-means is not necessarily doing that, right? So, again, a lot of false positives. Right, so again, a lot of false positives when you have a lasso type method. Um, this we're just going to skip this, but basically, when you look at the t-SNE embedding of the clustering accuracy, there's confusion in all the other methods except our method, even for a pretty simple example, because we're able to tweak the signal-to-noise ratio. And as we increase the signal-to-noise ratio and irrelevant features, we see that all methods struggle more and more, but our method is maintaining a reasonable performance in terms of adjusted rand index and similar for other methods like other metrics like normalized. Like other metrics, like normalized mutual information. I'm going to skip this case study. It works really well on this movement data. I just want to mention, because you know, this is a bio conference, we sort of ran this on an old data set, but it's like a classic one where we have some baselines to check against, right? So a lot of people did qualitative analyses on this leukemia data set where we had much more dimensions than samples. You're pretty poor in samples here. And what we did was we plotted the top feature weights because we're able to. Feature weights because we're able to do adaptive feature selection with the clustering. And we, first of all, classification error, we classified all but one. And like there's some, this could have been an outlier as suggested by previous papers. But sort of more interestingly, when we actually look at the genes sort of assigned highest weight, many of them are in the intersection of many of these qualitative analyses that only agreed a little bit. So the small intersection in these qualitative analyses were all in the top 10 genes selected by our method. And of course, this is more principled because we're doing a joint analysis in a quantitative way. Joint analysis in a quantitative way. Right now, we're extending this to bi-clustering, right? Sometimes you want to have this array data where you cluster it by genes and columns together. It's actually non-trivial to make this go through and to perform well. But that's something we're working on right now. And I will just wrap up quickly so we get out of here on time. But this is a novel framework that kind of takes one step back from convex relaxations of clustering, but assigns this adaptive feature weighing idea. So we get, we're trying to overcome some of these, you know, partially over. Trying to overcome some of these, you know, partially overcome and tackle a lot of long-standing challenges in clustering because you have high-dimensionality, you have sensitive initialization. And of course, when you have this agglomeration and the whole tree, you can kind of get away with specifying a hard number of clusters in advance and you can choose where to cut that tree. And sometimes it shows that it's very obvious that there should be two, especially under the sort of learned adaptive weights here. So there's an implementation on GitHub by my student. And this was recently is actually in Was recently actually in press at JCGS, although in statistics world, it means this was finished pretty much two years ago. The annealing work, just in case you're curious, is called Power K-Means Clustering. And then the work I was supposed to talk about on the schedule, if you're interested, I'm happy to talk at a Blackboard with you over this week. This is on integrating data via subspace factor analysis with a focus on identifiability. And it just appeared on archive today. So if you sort by new in statistics, you can find it, but it's a little dense. So I'll stop there. So I'll stop there and thank you very much for your attention. Yes, questions? Thank you. Yes. Just keep going. No, here. Tasting. Okay. So based on this, it's really working. Is this doing anything? I think it's to zoom. Okay. So based on this plot, if I just run TSNE on the data and I run k-means in the T-shirt. The data, and I run k-means in the t-SNE space, the problem is solved. Is that correct? No, that's a great question. So, it's like I clustered already, and then in the t-SNE space, I can, you know, I'm able to visualize the separation. But it's just like these. If I did t-SNE first and then I run these, it's, you know, these are, these are clustered by, these are colored by true labels. And so it's running this after we've done the tree cuts from these algorithms. Cuts from these algorithms. So if I do t-SNE on the raw data, it doesn't look so nice. And if I try to do, we have this in the appendix of this paper. So it's a great question, but if you do t-SNE and then you do k-means and stuff, it has a lot of confusion. Well, just on whatever output was. So this thing told me to cluster a certain way. I'm visualizing those in two dimensions. You're visualizing the output, no. The output, exactly. The raw data does not have little chunks like this. And in fact, I think these are actually, you know, these are all aligned. These are the visualization. These are all aligned, these are the visualization of the output of this, but then colored by the clusters learned by the others. Yeah, and only ours had such a nice acceleration. Yeah, I actually meant to skip this slide. So the other, okay, so then my second question makes sense. So when you say you add nonsense features, what is the, where do they come from? What's their variance? Are they like actually low? Is there some low-dimensional structure to the nonsense features? No, there's not structure, right? Features, no, there's not structure, right? So, so in the end of the day, the data live in 100 dimensions, but there is low-dimensional structure because all the cluster centers, like it's like you know, one, zero, zero, zero. They're well separated in the true features. These ones are just Gaussian noise around zero. So they just, you know, like in terms of like mean squared, like Euclidean distance, they're just confusing things on purpose. Okay, so, but they're like random Gaussian. Absolutely. Yeah, there's no, they're garbage in the sense that there's no structure, they're pure noise. And you can tweak, of course, the variance to the level of your level. To the level of your liking, just to see differences. Right, right, right. And sorry, just a follow-up question: if you take this data set and you run it through like ICA, what do you get? Do you find your five dimensions? I don't know if I did run the ICA thing, but like I said, I think in a lot of these data sets, you could probably compute the clusters if you do some generic dimension reduction. But the difference is then you would lose the feature selection. And we're interested in both tasks jointly. I don't know if we ran ICA here. Yeah. Yeah. Ran ICA here, yeah. Yeah, those are great questions. Yeah, I unfortunately skipped an important slide. We don't have time or okay, I just okay. Okay, um yeah, is there a plan to make it as a package? Uh, it's it's available on GitHub. It's very hard to convince this particular student to make it into a cramped package because I see it's like uh the few functions as a Jupyter notebook, but uh it's not bad if it's well that most. If it's as well, that most people be a bit more, I know. It's not even made into a package yet. It's just on GitHub. And so part of it is we'd like to integrate it with the biconvex biclustering version when that's done. And also, I think we actually realized this is published already, but there's actually a faster way to do one of the updates. So there's a faster way to do one of these. So yeah. But we should. I'm actually bad about this. Yeah, great. Just library. I know. I know, I know we have to do that. It's a good point. Yeah. So I suggest because there's some time, why don't we change the other talk? And in the meantime, perhaps someone wants to ask you another question because Amil Car was going to ask, but you can start changing to the next talk. There's no next question. Yeah, Clara. Yes. 