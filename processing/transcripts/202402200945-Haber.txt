Talking about the combination of how to add information to neural networks. This work is done by a lot of people, so this is kind of a combination of work of things that we started doing more or less in the last, I would say, eight years almost. Moshe, who started in the last talk, actually was part of this thing as well. And then I had a bunch of postdocs there. And the guy I was a postdoc with, or he was being a part of this latest work paper that we just estimated. Well, actually, review. Actually, revision submitted a revision, so we're in that region. So, what am I going to talk about? I'm going to talk a little bit about physics-informed neural networks, what people consider to be, and then my claim is, anybody here is doing physics-informed, we'll see how bad I can get myself into this here, is that this is done in the wrong way. Because you don't want to inform the neural network, the physics in the way that people do it, but you want to do it through the architecture. So, basically, through the dynamical system, and again, people hear that in the dynamical system for them, it's System, for them it's actually an easy sell. It's not so hard to sell if you have some kind of a manifold you want to be on, you want to put it into the architecture and not to say something like, well, I'll put something in the lock. So I think that it will go fly, I think, relatively well here, we'll see. But in the general community that I try to talk about it, sometimes people are up to arms on these things. So again, I'll talk about DNA dynamical systems, CNNs, graph neural network, we have a bunch of examples. I'm going to try to run code in the middle. Who knows what will happen? Code in the middle, who knows what will happen. We'll see how it goes. So, okay, so first thing, why are we doing this here? We're assuming to have a bunch of different cases. You can assume you have a PDE and you want to have a network to solve the PDE, which I'm actually not considering this. I don't like this case. Again, sorry if other people do these kind of things. I think that we have very good PDE solvers most of the time. And if there is a PDE solver, I prefer to use it. However, there is in many cases I have data. There is in many cases I have data, and I can take, and I want to take very large steps in the data. So we know that if we want to take large steps, large time steps, we have to do implicit methods, and then we have to do many steps. And now that becomes computationally very complicated. So you can think about it as you have a function at time t, and you want to have a time large step, time t, and then you want to take these large steps. You want to basically, it's a function approximation. You want to go from small time to large time, but you don't want to solve nonlinear equations, basically to rubridity. Basically, it will be and you want to stay with the accuracy. So, that's one thing you can actually try to do. There is a grant being written, hopefully, by a few of us here on doing it for atmospheric sciences, but these are the type of things people may want to do. More interesting things is that you don't have a PDE, but you kind of have an idea that there is a PDE, or you think there is a PD and you want to discover it. So, you have maybe even a problem in biology or in social networks or something that you say, I don't know what the PD is, but there should be something. Say, I don't know what the PDE is, but there should be something here and I want to discover it. So you model it in some way, and the neural network is a natural one way to model it, and to try to put it in. Now, in many cases, when you have things that are coming from physics or biology or anything else, you have things that are invariant. So in physics, we all know it. You have mass conservation, netromagnetic charge. If you're doing molecular dynamics, it's bold length, you have collision constraints, you have many, many different things you know about these kind of things. And you want to put it into the dynamics. And you want to put it into the dynamics. So, and in that case, these things, by the way, may be more important than the equations themselves, because you may be arguing about what equations are there, but mass conservation, I think, we would agree. So if you're doing modeling of, let's say, crowd movement from one side to the other, you may want to say, well, I have a model, I treat the people as molecules. So, okay, we don't know how people would actually move from one place to the other. We can use a machine learning to try to do this. To the other, we can use a machine learning to try to do this. But most of the time, nobody will, you know, start to be born in the middle of the crowd moves. You start with the same amount of people, you end with the same amount of people. So, conservation is more important in this case than the physics, because it's not really physics, but in some way how things people are moving. So, you want to keep these constraints in a much more serious way than you actually want to do necessarily the dynamics. Like we're willing to give a little bit of dynamics, but we are not willing to give up the constraints. But we are not willing to give up the constraints because the constraints are really what makes it real in some way. So, again, I mean, this is something that you can again put it in single. You have a constraints here, environment here that you want to build in. And the goal is to try to find them out. And what people did in this business, many times, they say, well, I mean, what we're going to do is we have a network that we're going to go ahead and try to predict whatever we want to do. And we add the constraint as a penalty. And this is great, but. Is great, but you know, first thing first, it doesn't necessarily keep the constraint, even in the state of when you're training it. Not to talk about inference. So, this is there is almost a religious belief that if this happened, the inference, this will also happen, the constraints will also apply. And in many cases, practical experience suggests that it's not, and this is not necessarily something good, and now you have problems with this. Okay, so this is what happened. Now, for people in the For people in the world that are doing it in the physics informal neural networks, sometimes it comes as a surprise, and I say to them that anybody who did something about these criticizations knows that this is no surprise whatsoever, because if you ever worked on, for example, Maxwell's equations, Army-Stokes equations, and you did not use, for example, staggered grids or mixed finite elements or all these kinds of mechanisms that we did for many, many years, worked hard to do these kinds of things. Well, you did the discretization for the PDE, you did not apply the For the PDE, you did not apply the invariant inside, and guess what happened? The invariant was violated. So, this is not new to anybody who was doing PDEs. It may be new for people who are doing physics-informed neural networks that you hope that this would happen. So, again, I mean, this should not be really surprised by this thing. And this was the whole push of using staggered grid methods and Maxwell's equation, particularly these finite elements. So, to me, it was not really any surprise in this thing. And the other thing is that we have to realize that, again, one of the things that Realized, I think one of the things that also is an interesting cultural difference is that some people say, Well, you know, I don't really care. And you kind of look at this and say, Wait a minute. I mean, for 50 years, we worked so hard. I mean, you know how many papers and books are written to get Navier-Stokes equation to be conservative. And now suddenly it's like, well, I'm going to use my neural network for Navier-Sok. Who care? Conservative, not conservative. I'm just going to run them. It's going to be fine, right? I don't know. That kind of bothers me when I look at these things because maybe because I dedicated some of my life to make these kind of things conservative. These kinds of things conservative, but it does look to be unreal and needs to be taken care of. So, that's kind of maybe the motivation to what we're doing. So, again, I mean, this is not happening in one place. It happens in many places. I'll show you a few examples. Multi-body pendulum systems and mass transport on graphs, and then long memory RNNs. I mean, all these kinds of things. You have you have some things that you want to happen, and then they don't happen, and then, well, you try as the best you can, and then you sometimes give up and you do something else. Then you sometimes give up and you do something else. Okay, so that's the usual thing. So, here I'm going to show you a simple example that we did today. I'm going to show you a few examples, and they're going to be a little bit different topics. I'll try to wrap them together. This is an example from a recent paper that, as you said, just submitted the corrections. We have a multi-body pendulum system, a chaotic system, so predictions, who knows? One of the things we do know is that the bond length is staying constant. The pendulum is not going to change the length of the arms of the pendulum. So, you can go ahead and say, you know. So you can go ahead and say, you know, I'm going to do my best, and it's the same things. I'm going to start at time t, and I want to go to t plus delta t, and delta t is large, and I want to try to use a neural network to do these kind of things. And you basically run a simulation, so you can assume about this kind of things, and you run a simulation. You see many, many, many examples. Now you're trying to take these examples and predict the dynamics with long time steps. And again, the idea is that you need this constraint to work. You need to actually have this kind of thing work. And when you do it, And when you do it in a regular way, you'll see that things are not necessarily working. You'll see that suddenly the length between the arms of the pendulum become shorter. And that is not something you would like to necessarily do, and now you want to overcome this thing. This is one simple example. Another example coming in graphs. So, we just saw a nice talk about graphs, it really saves me a lot of time. One of the things that we're talking about is the diffusion of graphs. And if you look at basically If you look at basically, I would say 99 of the architectures on graphs are basically assuming some kind of diffusion. Actually, people on graphs were obsessed with something which is called over-smoothing, which I think now is solved, but still people are obsessed with over-smoothing, which is not, if you know what you're doing, it shouldn't be a problem. But that means that if, for example, I have source, so I want to, my goal is to get this. I have a source here, so this is graph number one. This is graph number two, sorry, graph number two. And I want to get from here to here somewhere. So what I want to do? I want to take the map. Here somewhere. So, what do I want to do? I want to take the mass of these three points and I want to move it into here, this point. Well, that's not exactly what you do in graph neural networks, because graph neural networks spread the information around. They don't necessarily go ahead and concentrate the information. Now, in mass transport, usually we don't have a problem to do these kinds of things. If I have a mass transport problem and I want to take three delta functions to put them together, I know how to do this. Graph is having a hard time to do it. And just using the regular architecture, you try to train it, you don't manage to get anywhere. So you have to really. You don't manage to get anywhere. So, you have to really think about an architecture that can use or do something like this in order to get there. I'll get to this problem and I will leave it on. Finally, I'm going to talk about an example which I'm going to try to show code with, is when you look at a time series and you use, let's say, an RNN and you basically don't manage to get what you want. You can't predict much with a regular RNN. Even regular LSTM. And the question is: what happens when you want to use long-term interactions? What happens when you want to use long-term interactions and why doesn't it work for you for long-term? So, the point is that I'm trying to say all these problems, which I'm going to try to suggest a solution for these problems almost one by one, is that you cannot just go ahead and say, well, you know, I have a neural network because everybody's saying I have a neural network. We're going to run the neural network. We'll have the loss function. The loss function will take care of everything. Don't worry, it will just going to be okay. So I'm like, no, it's not going to be okay. And you have different tasks. You have different tasks, and if you have different tasks, you need to design different types of neural networks. You can't just go ahead and say, I'm going to use neural networks to solve everything. By the way, this is not again not new to us, people that are doing mathematical modeling, it's a mathematical modeling exercise. All these problems that I've shown here, they're different problems, right? If I'm using physics, stuff we used before, I have a different ODE for a pendulum or for something that moves on graphs or for something else. Why would I have the same neural network? Why would I have the same neural network for everything? This is a, again, I'm hoping I'm insulting somebody here. If you're a physicist, you know, physicists believe that there is one equation that describes the world, right? But most of us are trying to say, well, you know, maybe there's not one equation that describes the world. I mean, we are trying to kind of say there is a variety of phenomena, and different phenomena are described by different equations. And what my claiming is that we have to build different architectures to different phenomena and really think what we want to do, which is a really big problem in these. Which is a really big problem in these days because we don't like to think we want to grab the network from the internet and run it. That's what we let's use. That's what we teach. It's a really problem. Okay, so what's the key ingredients in this business? The first key ingredient, and I think that was the first thing that my previous talk says, but just embed things in high dimensions. Why is a good question? I think that we see in practice that it works much better in high dimensions. We have an idea that it works better in high dimensions. There are more degrees of Works better in high dimensions. There are more degrees of freedom. You can move around easier. I can wave my hands on this. I don't have a firm answer for this. Maybe somebody here has a better answer than me why you have to go to higher dimensions, but higher dimensions seems to be a key to make all these things work. So that's the first thing. After I'm in high dimension, I can now start playing. So now I have to start doing my model. And now I can just say the next thing is that in high dimension, I'm going to view the the neural network as a basically as a as a O D E or As an ODE, or if you're looking at it, we'll get to the PDE in a second, or in about 10 minutes or 15 minutes, and you can think about it as an ODE. Okay, that's the resonance, the equivalent to an ODE, which we noticed, I don't know, like eight years ago. And you can think about the different type of layers, and this classical layer that was around basically uses two types of matrices. There is one that multiplies here, other bias. There is non-linearity, which now depends on the day, you decide something a little bit different, but important is that, I mean. But important is that I mean, there is a theory about what it should be, not going to go in there, but it's a non-linearity. And then there is a second matrix here, K2 that multiplies the whole thing. And then people typically use a simple method like a forward Euler, not necessarily this is right, but that's what people typically use. And then they go ahead and they basically think about integrating it. And after you integrate it, you get to the final stage and you're minimizing something with your observed data, whatever the data is. So if it's going to be an image, is it going to be something else? Images, it's going to be something else. That's what you're doing. That's a standard structure of all. So there are really two things that you have to do. You have to pick the right structure, so the right layer. So again, we have matrices, K1, K2. Even in that thing, there is a lot of things to choose. And the second thing, you have to choose the right expertization, because, just said before, forward Euler, but some of you know that forward Euler is not unquotitionally stable, right? So I think we can establish in this kind. And it can have its own problems. And it can have its own problems. So you may have to choose a different discretization. And again, it's a mathematical modeling exercise. I like mathematical modeling courses. It's kind of fun courses because you're not coming... I know there are a bunch of students and postdocs here. So it's one thing that I'll say maybe more than once in this talk is that if you have a PhD advisor that is doing Navier-Stokes equation, you come to work with it, you're stuck on Navier-Stokes equation for now, for the next three years. That's it. You're stuck. That's it. That's all the equations you're going to see. But in this business, if But in this business, if you're doing mathematical modeling, well, you don't like this equation, we'll change it. Who cares? Nobody says to you that you have to be stuck on one particular type of neural network. If you don't like the architecture, change the architecture. So as long as you're open-minded about saying, yeah, I mean, I understand that this is not working for my problem, that's good. I mean, you can just change it. So let's talk about dynamics types. This I prepared when I was in London. So because this is a London bridge, the side chart. The tide chart, and your idea is that you have a naive model. So you basically say, Okay, I have this data, I want to model the data. So, this is by the way, I hate these courses, but universities now really like to teach them, including our university. And what they do is they give the students amazing things. They teach them how to call Python commands, which is an amazing thing. So, they tell them: look, you have a time series, great. You have a residual network. Sorry, you have the RNS. And what you're going to do, you're going to run your RNS. And what you're going to do, you're going to run your RNN. And if you're even more sophisticated, you're going to run your LSTM and you're going to model the time series. That's what you do. And then what you're going to do is we're going to tell them that it cannot fit. Now I'm going to show you it doesn't fit. I'm going to take a risk here. We'll see what happens. We're going to parenthesize here. It should be fair. So again, I'm going to, this is my point for this thing. I'm generating, don't look at how I'm generating the data because that's where I'm cheating. I generated some data. It's a non-linear. I generated some data. It's a non-linear data. It's basically a non-linear data that I generated. Here's my time series. So I have a time series, and I want to fit this time series. And I'm a fourth-year student at UPC, so what do I do? I basically go ahead and I write my network. And here is my network. So my network is very simple. I'm defining an LSTM. For those of you who doesn't know, it's an equivalent to an RNN with a little bit more bells and whistles. With a little bit more bells and whistles, okay? Not only that, I have like a lot of different things I can do. So, for example, I say what is my input size? Well, I have only a single time series, so my input size is one, because I have one time series. And then I can say I have hidden size, so I can open it to many, many, many, many neurons inside, so I can decide to maybe I want to open it. And I have a number of layers that I can actually take. So, again, the nice things about code today is that we see, I mean, I did not write this code. This is coming with my torch, right? So, I can just go ahead and code. Bytoach, right? So I can just go ahead and call this code. And eventually, as we saw before, I have a closing layer that brings it back to one net thing. And in my code, what I do is I basically take the input, run my LSKM, close it, that's it. Done. This is as simple as it can be. So that's a, again, this is a classical third year or fourth year student at UBC in data science course. We'll teach them to do all these amazing things and they will know everything about machine learning programs. So that's what happens. And I'm actually going to. What happens, and I'm actually going to stick to not here but to here. So, again, input size is equal to one, and I'm basically calling the network. So, those of you who know PyTorch, maybe they can read this thing. If you don't, then just bear with me. And I have my training set, and my training set, I'm taking my first set of time series. I have my prediction, I'm predicting only one time step forward, which I'm predicting, I basically taking the time step, predicting the next one. Taking the time step, predicting the next time step. So, that's going to be Predicting the next time step. So that's going to be my white train. I have an atom optimizer, I have a history, and now the only thing I'm doing here, I'm keeping in the history two things. So one thing about prediction, like time-dependent predictions, there is what people sometimes call consistency or trivial prediction. The trivial prediction is this. If yesterday was here, I think was 60, minus 6 degrees, today is going to be minus 6 degrees. It's a very good prediction, by the way. Most of the time, it works amazingly well. Way. Most of the time, it works amazingly well. 72% of the time. 2% of the time. 98% success. I mean, come on, I mean, you give me a method with 98%, and you're going to argue with 98%. It's very hard to beat the decisions forecast. Yeah, so I'm going to do here two things. I'm going to record what the neural network is predicting and what is consistency predicting. So I'll run this then. And let's see that it runs. Also, print some stuff here. Wants to print some stuff here. So it's printing stuff. So I have basically the loss that is given to me by my neural network, and this is the loss done by consistency. So basically, DN means do nothing. All I do is I look at what happened today, I say it's going to happen tomorrow. So first thing, by the way, this is a pretty good number, correct? I mean, of course, I have a small time step, correct? So this is a pretty good prediction. And as I go along, you know, this is getting closer. And by the way, one of these things that we also don't get, I'm bitter because I'm teaching the data science course by the way. Teaching the data science course, right away, so you can see this thing, it's kind of coming. One of the things that we don't teach the students as well is we look at this number, we never look at this number, so they never really have a common sense type thing. And they look at this and they say, look, this is really small, I'm doing really, really well. Until you ask them, but you know, there is some common sense, how do you do compared to just the do-nothing thing? And then they look at this and say, well, it's not really doing that well. So you see, I'm not doing that well here. I mean, I have a neural network and it's not doing that well. So, and I'm not going to beat this number. I think I've got 20,000 iterations here. I got 20,000 iterations here, not going to be this much. So now, you know, it's a small crowd. Why are we not doing this? Like, how come I have an LSTA, sophisticated network? Five layers, I think, I put over there, whatever they allow me to do is this kind of small thing. How come I'm not even beating consistency? I mean, well, actually, I'm beating in the second thing. Whatever. But how come I cannot really beat consistency? We have some students in postdocs. We have some students in postdocs. They are allowed to answer. If you're a researcher, oh wait, you're not allowed to answer this. Nobody wants to try. But look at the data. Say to students, look at the data. So what kind of data is it? If I want to first order a kind of idea, what data is it? Like what does it represent here? Oscillations. If I have oscillations, can I model oscillation with a first-order equation? With a first-order equation. Doesn't make any sense. I mean, if I'm at this point, if I have a first-order oscillation, I mean, there's no way I'm going to modulate with the first-order method, correct? I will need a second-order method. So what does it mean second-order method? I can't look only one time step back. I need to look two time steps back. So indeed, what I'm going to do now, where I skipped in the beginning, because unfortunately I kind of did it in the wrong paragraph way, is I'm going here and I'm going to say something like this. The input size is not going to be one, it's going to be two. Input size is not going to be one, it's going to be two. I'm not going to take only one level back. I'm going to actually take two. So I'm going to try to predict the future based on two time steps. Okay, so that's what I'm going to do here. I'm going to take two time steps. And you see, so I'm going to, if this is two, I'm basically concatenating. I'm just adding another thing into here. And I'm going to do exactly the same exercise. And it better work. But it did work before. Okay, so again, I mean, I started to train my network. Well, it's doing. Well, it's doing the same thing at the beginning, it's kind of doing the same type of things. And hopefully, in about two minutes, less than two minutes, it will be in the right place and I can get to something lower than the consistency error here. And again, I mean, the important thing here, I'm using in this case LSTM, nothing smart. I did everything out of the box. But if you do out of the box and you don't think a little bit about what your dynamics are and how you should model the dynamics, don't expect things to be as well. Don't expect things to be as well. Here, what I'm starting to do is better, and I can go to lower, and this is going, I think it goes 10 to minus 6 or 7 or 8 or whatever. So, if you just do something really small, like just incorporate the second thing, and by the way, this is a trivial example, correct? I mean, this is not something, I mean, but you'll be surprised how many people, when you talk to about these kinds of things, especially practitioners, they don't think about these kind of things. Just go ahead, yes, I mean, we have a code we can download from GitHub, we'll run the code, everything will be just fine. Let me go back to. Let me go back to the to the talk. Okay. So again, I mean say this is this is not reworking and now you can go ahead and say, well, you know, I can inform, how can I better inform it? Well, I mean, you can actually do this. You can actually put a second order derivative if you decide that's what you want. You can decide you don't want to put a second order derivative, and you're just going to put some function of these two things and say, well, no, let's go to let it learn the thing. I'm going to show you in a second. Let's go to let it learn the thing. I'm going to show you in a second a little bit more. So, we heard a lot of people here talking about Hamiltonian stuff. And again, Hamiltonian stuff is another way to inform your neural network. So, I think we wrote the first paper, we actually had an Hamiltonian neural network of similar structure to this. This was, I think, seven or eight years ago, and we said, you know what, if you are going to introduce another variable, and now you're going to do a Verlette-type integration, that allows you to also do, again, something different and gives you a different type of information. You have some conservation half. Of information, you have some conservation happening, it depends what you want to conserve, but you can come up with something like this. More interestingly, recently we've done something with this paper, is already going to appear, something that looks like this. We have basically said, okay, what happens if we have a second order? We now know how to deal with it. But if we have an arbitrary order, maybe it's third, maybe it's fourth, the data can be very complicated. We don't know what order it should be. So what we do is we're going to basically have some coefficients here, and we have constraints on the coefficients. And we have constraints on the coefficients. Ray's notes were very helpful. We have notes online to help you about looking at high-order ODEs. We actually used these notes to learn how to do this a little bit better. And basically, this is just a sketch of the whole thing. If you think about it, in this case, you're taking three time steps, three of the data that could be different times, combining it, putting it to the network, and going from the network forward. So again, you can do this with arbitrary order, and that allows you to recover better dynamics. Cover better dynamics. Now, it's actually showing in examples that it actually helps. So, again, coming up with ideas like this is always easy, but the question is: does it really help in something? And this is an example that does help. There are some data sets that are public. I'm not going to go over the length of these things, but there is one which is a chickenpox in Hungary data problem. There is a chickenpox in Pelos and Mi, which is the rental of bikes in London per month. And they want to see how this thing behaves. And they want to see how this thing behaves. And when you add a higher dynamics, and by the way, the bikes were kind of obvious to us that it's going to work, and we thought it's going to be second order because you think about it, like in the summer, there are more bikes, in the winter, there are less. So you can think about oscillatory type behavior. The chicken pox, I had no clue what's going to happen. And we just started to see what happened when we're going to add more coefficients. And you can see that you actually improve your prediction as you're increasing the order of this particular problem. And again, these problems. Particular problems. And again, these problems, there are some problems that are very easy to come up and to say, I know what the dynamics are, I have a guess of what the order of the equation should be. In that case, just use it. But if you have node, you can actually use this kind of mechanism to get a little bit better. I'm going now to space-time, talk a little bit about space-time. And we're looking now at convolution neural networks. So convolution neural network is basically a space-time, because now you have a convolution, and you can think about the convolution as a. And you can think about the convolution as a discretization of a PDE, rather than thinking about it in the ODE direction only. So you had the time stuff, but now you can think about convolution. So let's say I convolve in one dimension right now, A, B, C with E. So I can change basis and think about it as alpha times 1 minus 2, 1, beta minus 1, 0, 1, gamma 1, 0, 1. And of course, why do I do that? This is the second order derivative scaling, first order derivative is scaling, and of course, the mass matrix is scaling. So you can. So you can look at this and say, convolution, I can express it as a combination of derivatives. So now I can really think about my CNN not as just, okay, who knows what convolution is, as a differential equation with combination of first order, second order, whatever it is. And you can build higher order or mixed derivatives if you're in two dimensions and so on. It's not going to be a tough idea. So that, of course, goes to the fact that if you have this idea, now you can start thinking about architectures to solve. Start thinking about architectures to solve problems that mimic what happened at PDE form. So, for example, remember that I said, okay, if I now had a simple neural network, these will be matrices. But in the world of CNNs, these are convolutions. So I have a convolution here and a convolution here. Now, if I decide that the first one is a convolution and the second one is a convolution transpose, basically I get a heat equation, a heat-like equation. So now you'll see that your dynamics starts to do, basically the results basically start to be more and more boring. Basically, it starts to be more and more boring as you go to more layers, right? And of course, if you have a problem that what you want is to smooth the thing, so or maybe even to do something not exactly smooth, I mean, to smooth is a little bit exaggerated because there is this classification. But think about a problem like segmentation. In segmentation, you're trying to take a complicated picture with a lot of structure and make it relatively simple. So, this falls into exactly that category. You can work with these kinds of architecture to get something like this. And you can prove something about the energy, and you can. Proof about something about the energy, and you can do some proof of these kind of things. So, again, I mean, we can look at these things, and we've done these kind of experiments, and we looked at the energy of the system, and you can see that it's reduced the energy of the system, you can prove some stuff on it, it's not very complicated. However, it's not always working because there are some problems that reducing the energy is exactly what you don't want, because you want to keep the things a little bit more complex. I'll show you in a second example. So, as I said in the beginning, you don't like the equation changes. In the beginning, you don't like the equation, change it. Okay, so in this case, why don't we use second-order equation here? If it's a second-order equation, now it's hyperbolic. Okay, now you're in the business of basically waves, if you think about it. And now, well, strange things happen, as you know, with waves. Who knows what happened with waves? And now you're thinking about it, that I have my wave equation that needs to be moving around. And that thing, of course, will not just keep structure, it will make things who knows what, right? And this is really an example that we could show. And I'll show you the second example for this. I'll show you the second example for this. Last thing I want to do, because I want to keep the time and I don't want to delay coffee, is to talk about how we move from time to space two to unstructure grid. And this is really where the graph comes. So until now, we talked about everything on a mesh, but you can think about the whole thing on a graph, where rather than pixels talk to each other, now you have nodes that are talking to each other. And again, you want to do exactly the tasks that we were talking about a second ago. So again, the standard. So, again, the standard layer in this case, you'll have some convolution, the Laplacian here. This is a little bit different than what we were talking about before. And what we call one-by-one convolution. So, just looking at the node, local node information over here. That's a classical thing. And again, this is known to be smooth and not surprising, right? I mean, if you think about this, you're basically taking the Laplacian of a, it's basically a forward Euler on the heat equation, non-linear heat equation. Non-linear equations. So there's not surprising this is actually smoothing data, and that's what's called over-smoothing. In this community, a lot of people are still busy in thinking why there's over-smoothing, although again, papers on this thing, I think, already like three or four years ago, show you can, there's really no problem if you know what you're doing here. It's not really a big deal. And again, one of the ways to do it is to think about these networks in different forms. So, this is the architecture that you just showed us before. That's exactly passing into a gradient. Opening the passing into a gradient and basically a divergence, which is a gradient transpose. And the convolutions are coming here as k3 transpose. And you can think about that as basically the discretization of a parabolic equation that looks like this. Now, you can actually do the hyperbolic equation, and this is going to be the hyperbolic form of the equation. And you can even think about the mixture of the two things. You can have something that is mixing things. And to show you an example for this, so this is a data set that people are using for trying to match different. For trying to match different images and to know which part is a registration problem. You want to know which part of this match to which part of that. So clearly, if there is a lot of structure on the image here, you don't want to make it smooth because it doesn't match to anything on the other one. If you're running a parabolic equation, you're running this part, you're getting something very boring at the end. So this is why people say, well, you can't run neural network for very long, GNN for very long. If you run it more than three layers, bad things happen. Not true. Okay, if you just change the dynamics, you can run. If you just change the dynamics, you can run it to as many layers as you want. And actually, you can actually see the kind of the wavy type of equation. You can see that this is becoming bluer and then becomes, so you can see like kind of the oscillation of the wave. So actually you can keep the structure as much as you want. There is no problem. You can do it as many layers as you want. We have papers that we show we integrated it for 500 layers. There is no, you don't see any difference, right? I mean, not sure this is good computationally to do, but it's not really a problem. So these are some of the So, these are some of the results on these things, and show you that, again, you can get, really, I mean, this is the method we were talking about, you can get much better results for almost everything we've done here compared to other things that people have done. The last thing probably I'm going to show is this advection problem. So, this advection problem was really something a little bit different, and we realized that none of these methods that we have are really able to solve this problem. You want to take and move things around in one direction. Move things around in a one-direction type approach, not to just spread them around, which is what people typically do. And for that, we basically look at a diffusion-reaction-reaction equation, where you are just doing something that is having a vection term, a diffusion term, and a reaction term. The reaction term is obvious because it's basically a point-wise operation. The diffusion is also kind of obvious because we have done this a second ago. So, the only thing we have to come up with was the advection part. And for the advection, we came up with a very simple mechanism to an upwind. Basically, it's an upwind. Mechanism to an upwind. Basically, it's an upwind on the graph. For people that know what I'm doing, I'm not going to have that much time to talk about it, but this is basically what it is. I mean, you're basically looking at mass that is moving from nodes j to i, you have a velocity that you're learning, and you're basically doing it. It's basically learning the velocity on an upload method. That's all it is. It's a fancy way to write it, but that's all what it is then. And when you do this kind of thing, suddenly you can move things very easily, and you can generate things that move. Easily, and you can generate things that move from mass here to mass here without actually going and making things very blurry or something like this. It's very easy to generate these kinds of things. We use it for traffic flow and models that are made for, there are some models that are clearly made for these kind of products. The way we have done these kind of things is classical discretizations of basically the IMEX type discretization. People here, Steve here, definitely knows much more than me about this kind of thing. Much more humiliate about these kinds of things, but you basically have an implicit way for the diffusion, you have an explicit way for the reaction, and you basically put them all together in three steps. You do diffusion. Well, depends what you want to do, advection, diffusion, reaction. By the way, what's it, right? Wait, advection, diffusion, reaction, reaction, diffusion, advection. What would be the... We argued about, because you see, this is going to be doctor. And on the other hand, the other one will be not as nice. So there was a big question about what's the right order to put it. Question about what's the right order to put it in, but we'll give a nice acronym because, in this community, acronyms are everything. Okay, so for this, actually, now you start talking about very big improvements. So, an anecdote on this particular paper, which is going to be shown in AAAI in two days. So, this is one of the big machine learning conferences happening in Vancouver right now. We submitted this paper to NERIPS, and the referees did not want to take the paper because. Referees did not want to take the paper because we claimed we improved the methods too much and did not believe that the improvement was that much. And we have to put the code online to be tested. So it was actually quite amazing because they looked at some of these results and they tell us, no, these improvements are too big for what we expected to actually have. So that's why the paper was rejected from Europe. It's going to appear again in two days from Cooper. Gilvindica. I think this was the result that really threw them off. But really threw them off. This one was really a substantial improvement over the previous method before. So this is lower, is better. This is where we got numbers that people just have not managed to get anything close to that. So we felt pretty good about ourselves, but again, we had problems with this kind of things. But again, I think that this is not surprising. If you're trying, again, mathematical modeling, you take a heat equation and you try to fit a wave phenomena, good luck, correct? That's basically, I think, what happened here. Can you say again what kind of problems? Say again what kind of problems? So, this one is the propagation of. So, this one is chickenpox, and this one is pedal me. This actually is the same problem that you showed before, I think. You showed Wikipedia as well. Wikipedia is not an interesting problem, right? It's actually not a state, it's not a time-dependent problem. You're just trying to classify Wikipedia nodes. These ones are more interesting to me. So, what you have is you have the how, so you have shops in London and how many bikes they rent. I think it's a week. I think it's a week. If I remember correctly, it's every week over basically a few years. So that's Feddel and me. And Chicken Pox Hungary is the propagation of chickenpox in Hungary. So this one, I think, not so much the reaction, the advection term. This one is questionable. But we looked at the size. Is this really important or not? But it's definitely the diffusion reaction is very important for these two problems. That's for sure. By the way, the chicken pox was clear to me that the fusion reaction. By the way, the chicken pox was clear to me that diffusion reaction is important because how do we model diseases? Diffusion reaction, right? So you take a diffusion reaction in high dimensions and you're modeling something that we know works in lower dimension for diffusion reaction. Really, what a surprise. I mean, come on, it's not big yet. Oh, I want to say one thing. Oh, I'm feeling this very bad. Okay. When you look at, so these two things, I'll tell you a dirty secret. That everything here, including what we did, looks really bad. Okay, so if you actually plot the curves, like how So, if you actually plot the curves, like how well you fit, none of them really fit the data. Okay, we do a much better job in fitting the data, but if you look at how we fit the data, we're still very fine. So, there is no method right now that can fit this data very well, as much as what I've seen so far. I've not seen a single method that fits the data. People publish papers all the time, but nobody shows the curves. I was wondering why, and then I look at the curves, I'm like, okay, now I understand why people don't show the curves, because they don't fit the data basically. But that's a dirty secret, nobody talks about it. But that's a dirty secret nobody talks about. They just want to show the numbers, everything looks okay. Don't tell anyone. Actually, no, I'm not going to talk about invariance. It's cool, but I'm not going to talk about this because it will take longer than I would like to take, and there's no point of me rushing through the slides. All I want to tell you is that there is a way to put invariance into the equations, basically length invariant or divergence-free invariant, or all the things that we know and love from PDEs. There are a way to do it in similar ways to what we're doing. Are way to do it similar ways to what you're doing. So I'll rush this only to the conclusions. Okay. So, really, again, two things to inform the neural network networks about the physics is really one is through the loss, which I don't recommend. Okay, I mean, you can do it, but it's not recommended. Through the architecture. Through the architecture, well, I like it because it makes me think about what do I want to do, rather than taking something from a box and running it. And it's a mathematical modeling exercise, so you really need to think about what you're doing. So, you really need to think about what you have to do. And again, sometimes people say, I don't know anything about my data. So, first thing is not true. You always know something about your data. Start looking at the data and start asking the question what the data is. There's this thing about people who try to devote, like this is the sin of modern data science. It's like we don't care what the data is, we just grab the data and we run something, right? That's the wrong thing to do. We're losing the connection with what we used to do for, I don't know, more than 500 years, looking at the apple. 500 years looking at the looking at the apple falling and asking what type of process is this, rather than just putting something to run on it. So you need to think about the data and you need to start thinking of how do you build this kind of information in. You need to build dynamical system information into the whole thing. All your information about dynamical system, and people here know way more than me about dynamical system, is coming handy when you're trying to build architectures. And that's really problem dependent. And again, if you do these kinds of things, you can get better accurate. You can get better accuracy. So, again, treating them as SPD. And again, the whole thing: not all data sets are alike, not all PDs are alike, and you have to build things that are based on that. There's a bunch of papers here at the end, and it said that we're just quoting ourselves here, not somebody else, but this is work I've been doing for a few years now. Time. Coffee. It was probably a dumb question, but if I think I know a PDE model for my problem, which is essentially what you're saying here, right? Differential equation model, if it has these terms with some coefficients, why don't I just use classical parameter estimation methods for finding coefficients of my PDE? So I use neural networks. So I started by saying that if you know your PDE, use your PDE. Use your PDE. I would say, again, I don't care a lot to say this these days, you have to use neural networks. No, if you know your PDE, use your PDE. I mean, I argue a lot of times with people in my community. They say, well, going to do this for Navier Solx. I know Navier South. What I need to learn it again? No, I know Navier Solx, right? The case is that you don't necessarily. There are two beginnings. I'll go to the beginning, just to kind of put this thing in where you want to use it. Where do you want to use it? So, this case, people do it. Again, I said in the beginning, I'm not a big believer in this. This is the first case. I mean, you could solve your PDE, but what happens if I want to take a very large time step? Do you know how to do it in a PDE sense? I don't know how to do it. So if I have the prediction today and I want to predict next month, all I know is I need to integrate very slowly. If I have the climate, for example, climate models, they're notoriously bad. We basically integrate weather-dependent systems for 70 years. That takes a lot of time. So if I can avoid integrating 70 years climate model, In 70 years weather systems and integrated in a time step of six months, I'm in way better shape. So that's basically number one. The second thing, I don't know the PDE. I have an idea, but again, I'm looking at high dimensions and not in the same dimension I'm looking at. Then I'm like, okay, that's a reasonable thing to do. But if you know your PDE and you're not looking for shortcuts or anything else, do your PDE. I mean, I cannot guarantee you with neural network anything close in accuracy to whatever you get when you're doing an Avier-Stokes or Maxwell or whatever it is. Or Maxwell, or whatever it is. In my previous life, I solved Maxwell's equation for a living. I cannot generate even close to the same accuracy for Maxwell's equation when I'm doing neural networks. So don't use neural networks if you know your PDE well and you want to get an accurate solution. Record it. That's going to be this. My career is over. Not quite learning physics you know already. Yes, if you know the physics, we had arguments about this with a few machine learning people in my department. In my department. I said to them, Well, if you're learning this, I'm going to learn your model tomorrow. But why do we continue, correct? I mean, why not? Again, I mean, there is a thing about you can say, I'm going to learn a reduced model, so I can compute it faster. But that's not the same as learning the yeah, so this discussion about architecture versus loss is really interesting. And there's also another third part of the picture, which is the data, right? So, like, how do you kind of put together your training set? Like, that also matters a lot. So, in this big picture. Matters a lot. So, in this big picture, I think, like, how much of the importance is also designing a good training set or a good experiment for your problem? So, clearly, that's very important. And especially, by the way, it becomes more important in today's world when we're looking at, I did not talk about this at all, it's one of the big things we're doing right now, is looking at semi-supervised method or self-supervised methods, right? So, the question is, if you have, I mean, we are now training things in a standard way. Somebody gives you an image. What happened if you want to train a network and you don't have any, I mean, all of this data. And you don't have any, I mean, only this data, you don't have x, y, you have only x. So you're designing the y, correct? So either adding noise, reducing some of the, doing something. So if you're doing something to the data, you need to decide that what you're doing is making sense as well. Again, unfortunately, it's hard to teach common sense, right? That's one of the problems we have here. It's like you have to do something that makes sense to the problem you're doing. That's all I can say. I was thinking when you presented, but is it the other side of the argument somehow? Because you wanted someone to be solved. I know, because ticks time, when you're solving PDEs or like the system-based physics, as you said, or you analytics and then numerics, and then your networks engage in that. But that's not the beauty of all of this, or why explode is because it's general, no. And then people, like the most successful medical. People like the most successful methods are the most general, general ones. But it's not general. I mean, if you look, that's my point. I mean, okay, if you try, I mean, when you actually look at it, if you're looking, for example, at the architecture for ChatGPT, it's very different than the architecture for DALI2. It's very different than the architecture for design. Yes, they all have built-in, an activation, maybe a convolution if you're on a mesh. But that's the same as saying I have a differential operator, because differential operator is at the end of the day on a regular mesh convolution or on a graph. It would be just as far. On a graph, it would be just a sparse matrix, right? What's the difference? So, I don't see any difference. I mean, I'm sorry, they like us to think that everything is new. It's not new. I mean, what's new? Convolutions, operations on graph which is sparse matrices? Yeah, but I would say for the JTP, yes, it's like this specific architecture there, which is formed somehow for the data. But that architecture can work with lots of different data. That's how it is. Not data that is not coming from natural languages. You cannot work on it on images, correct? But these are these groups of generality. You can PDE is a very small one. I disagree. I would say that I could exactly lump PDEs in the same way. I don't think that you can say that this is very general. I think that these things are very, very specific. And by the way, exactly when you try to take these things to work on things that have not been designed for, then you're running into trouble. So again, sometimes it does work. Sometimes it gives you something, and it's giving something reasonable. But again, I mean, if you talk with somebody asked me, who asked me what fitting parameters to a P D. Asked me about fitting parameters to a PD. I can take a wave equation and I can try to fit, and we tried modeling with a heat equation and fit coefficients to the heat equation. And if you allow me to change the coefficients in time, by the way, I can actually do quite a good job in this. I tried, I know. So there was a field in this in geophysics. Some people try to do this. So it's possible to do something. Is it serious? No, it's not. It's not going to be very general. It's not going to work very well. But you can do crazy stuff. We've done this. So. 