Talk about global automatic integral conditions and allow working for multi-objective problems. So, I'm Everton. So, I'm a PhD student in mathematics at Malve School of Science and Technology in Portugal. This work is a joint work with Elizabeth Daguercaras and Musilina Bachitz Dussan from the Federal University of Parana in Brazil. So, this work is related to my master's thesis because I'm in the beginning of my PhD. Of my PhD. So, this is the outline for this presentation. So, firstly, I will give you a brief motivation behind this work. After we will see the global automatic integral conditions, firstly, the scalar optimization case. So, when you have only a single objective function. And after, we'll see also to multi-objective optimization. And for using the global automatic integral conditions for the multi-objective optimization, we will have an algorithm. We will have an algorithm for to solve the multi-objective problem after we have the convergent results for this algorithm, and after we will see some very preliminary computational results. And to finish the talk, we have some conclusions and future work. So, as we know, there are several works that in how to How to establish the ultimate conditions for a problem. However, almost all these attempts are local and still tends to require several degrees of differentiability unless some convexity or generalized convexity assumption is involved. But the integral calculus being useful in compact sets and with no empty interior, no empty sets. No empty sex, we want to use this kind of integral characterization because using integral calculus has more global characterization. And for some case, it does not even require continuity. So let's start the global automatic integral conditions. So firstly, we want to minimize a single objective function, subject to some feasible region. Where the feasible region? It will be Where the Pisburg region will be a no-MP set. So, the first global optimal integral conditions was first proposed by Falk in 1973 for maximization problems. So, what we did first was rewrite this global outmode conditions for the minimization case. So, here you need some assumptions. So, firstly, the feasible set is a compact set with interior no empty. The objective Empty. The objective function is a negative function in continuous. And for x-bar in the feasible region, we have that the image of this point is equal to minus one. So the first term that we have, that this integral defined by these expressions converged when t goes to infinity, if only if x-bar is a global solution of the problem p. Here, the hypothesis of the function is negative is not so restrictive. Negative is not so restrictive. Since we can consider this another function that is negative and the minimizes coincides with the original function, and also the assumption that this point x-bar, the image of this point is equal to minus one, is also not so restrictive. Since we can consider the quotient of the original function by minus f of x. So here we have the color about that theorem. About that theorem. So let the sequence defined by epsilon given by this expression. So then we have x-bar is a global optimum solution for the problem if only if the sequence of the integrals converges. Moreover, if for some element of the sequence n, we have that the element n plus one is strictly greater than element n, then x bar is not a global optimal solution for the problem. So this can be useful. So this can be useful in eliminating candidates that are not global minimizers. And to illustrate this corollary, we have this simple example. So we want to minimize this polynomial function subject to this interval. So as you can see here, this function does not satisfy the conditions that we had in the first slide. So what we did was we can take this expression and play with this expression and get this another function. And get this another function, xi, where this non-other function has the same local and global minimizes that the original function. Now, if we compute the element 20 in 21st, we have that the element 21 is strictly greater than element 20 of that sequence of integrals. The next bar equal to 2. The next bar equals two, that is the candidate to be a global minimizer. In this case, it's not a global minimizer. So now to see the another kind of global conditions, we will need to define the level set. So for each real number C, we can define the level set HC given by this expression. So now we need another kind of assumptions. So the first assumption that we need is the feasible. First assumption that we need is the feasible set is a robust set. That is, the closure of the feasible set is equal to the closure of the interior. Also, we need that the objective function is lower semi-continuous and upper robust. And we'll say that the objective function is upper robust over the feasible set if only if this set is a robust set for each real number C. And also, we need this assumption that ensures that we have a solution. So there exists some. So, there exists some real number C such that this intersection is a compact set. Can you go back? I lost up with that for sure. Can you tell me what happened here with the 20 and the 21? Like, how did you know that two wasn't optimal from 20 and 21? Sorry? You said because of this, you knew that, like, two couldn't be a global optimal. X bar is two is not global optimal. Can you what is happening with 20? No, no, that's back. We have this. Well, that's right. We have this. Last time I say that. Yeah, just you know, two is not a global option because the integral from zero to three. Yeah, because the element of the sequence, 21st, is strictly greater than element 20. But where's two in this expression? It does not appear. So then how do you know two is not a goal optimizer? We need to ensure that the image of the candidate is equal to minus one. Where's the candidate showing up? So the candidate expression. Up so the candidate expression it's here in sorry in the 20 in the the y20 or y21 the expression it's this expression yeah where does two show up it's not appears okay we just need to ensure that about two without having to put it in anywhere here no you just need to ensure that the image of the candidate is equal to minus one but where do you calculate the image of the candidate we don't compute We don't compute. So, then how can you say it's not minus one? If you take a look here in the graph, it's not minus one. So, we just play with the expression of the original function and rewrite the function, try to get the function that the image of this point is equal to minus one. Can I add something, Chef? Please. So, this is not my work, but from what I understood. So, E has a function and So he has a function and he has a point for which he wants to decide if the point is the global minimizer or not of the function. Okay. So using uh their point and using the function, he transforms the function and give me function five at C. Sorry. So two was used to make this C is like where does two go in here? Because I didn't have seen C of X bar equals minus one. Yeah. C of X bar. Yeah. If it's a global option. No. It has to go a lot. No. He transforms a function on the left with xc that we see that x bar minus one square s. And then within the secret, it gets cut down. That the c that exactly. So I put in x bar to make x. Thanks. I think I'm closer. Every time that you have to satisfy a point, you need to build a new c. That this new c must satisfy the hypothesis. And it must be monotonic, and because it was with this way, then I knew that point that made the XC couldn't have been go off. Yeah. Thank you. Sorry to be so. No problem. So this problem, this is a new theorem. We have that the measure of the intersection of the level set related to C bar and the feasible region is equal to zero. Then C bar is the optimal value of F over X. Over X, and this intersection is the set of global minimizers. And here you use the measure, the Lebesgue measure in Rn. Now you need to establish another kind of global automatic conditions. You use the mean value of the function over this intersection. So for the case of C, this is literally greater than C bar, where C bar is the mean value of F over X. value of f over x. So here we just consider the case of c is strictly greater than c bar just to ensure that this definition is well defined because when we have c equal to c bar we have this measure equal to zero. So the first expression we have the mean value of function over this intersection. As we know the mean value is a is a good measure but it alone is not provided all the information that we want. Information that we want. So, we also want to compute the variance of the function over this intersection. And also, we have the modified variance, where here the only difference that we have is because here we consider the true value C and here we consider the mean value. Now, to the case where C is greater or equal to C bar, we can extend this kind of definitions using the limit approach. So, we just need to consider this decrease in C. So, we just need to consider this decreasing sequence converge to C. Again, we have the mean value, the variance, and the variance multiplied by the assumptions A1, A2, and 3, these limits exist and are independent of the choice of the decreasing sequence. So, just to remind the assumptions, so we have this theorem. So, suppose that the assumptions A1, A2, and 3 hold. A1, A2, and 3 hold. The following statements are equivalent. So, X bar is a global minimizer for the problem, and T bar is the global minimum. We have the mean value of the function over the intersection of the levels at H C bar if the feasible region is equal to C bar. And also, we have the variance of the function and the modified variance equal to zero. Just to illustrate. Just to illustrate the theorem, we consider this simple example. So, fixed at some number a greater than zero, we want to minimize this model function over r. So, if you compute the level set for this function, we have this interval. If you compute the mean value of the function over this interval intersection of R, we have that only the measure here is one over the measure of this intersection. One over the measure of this intersection. So here we have just the length of this set times the integral. This is equal to this expression. If we compute the modified variance, we have this another expression. Now, if we take c bar be the global minimum of the function, we have that by the theorem that mean value must be equal to c bar and the modified variance must be equal to zero. Must be equal to zero. Therefore, if you solve this both equations, we have that C bar must be equal to zero and the set of global minimizers equal to zero. So now the multi-objective optimization case. So now we want to minimize a vectorial function that has that have our components that we want to minimize, subject to some feasible region. Right again, the feasible region is known. But again, the feasible region is non-empty, and the objective functions are often completing. So, to extend the automatic conditions that we had for this for single optimization case, we use the scalarization techniques. To do that, we need to define these two sets of the 18 vectors. So, the first set contain only vectors that all the components is greater or equal than zero, and all these vectors are. And all these vectors are normalized in the normal one. And another set is almost the same. We just need to consider that all the components of the vector is strictly greater than zero. So the first scalarization technique that we consider was this 8-lit sum. So we just consider the convex combination of the object G functions. Here we have the really classical term. Classical term that states the relationship between the solution of this problem, H sum, and the original problem. So, if there exists some vector of eights, that all the components is greater or equal than zero, such that x bar is a feasible point and is a solution of the eighth sum, then x bar is a weak paradox of solution for the original problem. Moreover, if we consider all the components of the 8th vector established greater than zero, Vector is literally greater than zero, we have that the point x-bar is a parental optimal solution. We also consider this Chebichev 80 problem. So firstly, we needed to consider the topin objective vector defined by the difference between the ideal vector and this another vector that all the components strictly positive. So here we have this kind of mini-max problem that are defined. Max problem that are defined by this Chevyv norm. Again, we have this theorem that we have the relationship between the solutions of the Chevy Chev 18 problem and the original problem. So the point X bar is a wiki period optimal solution of the multi-objective optimization problem. If only if X bar is also solutions for this Chevy Chev 18 problem, for some 18 vector where 18 vector where all the components is typically greater than zero. So to establish the first automatic conditions, we need to ensure that we need to consider that the objective functions are continuous and the feasible region is a compact set. For this, both functions, phi and psi, we cannot ensure that the image of these functions Of these functions is this interval, so we cannot ensure that functions be negative. So, of course, here we cannot ensure, however, we have that these both functions are continuous functions in the compact set divisible region. So, using the vaccine function on the detection, so you've got the infinity normal, and then there's a W that's in the superscript. Yes, it's a maximum between Maximum between W times this difference here. You're welcome. So using the visa theorems, we can ensure that we have these upper bounds for both functions. So now we can define these another two new functions, phi and psi, two, that are negatives. And also, if x bar And also, if x-bar is a global minimizer for the original function to the 18th sum, if only if x minimize the function given by this expression. And this expression we have that satisfies the original assumptions that we have. So for x-bar, we have the image of this function as equal to minus one. And also, we have that this function is known, is negative. And of course, we have a similar results for the. And of course, we have a similar result for another function related to the sclarization, agent clarization to the Chevy Chevy 18 scalarization. So we have this theorem. So first related to the phi function, so the 18 sum. So consider x bar in the feasible region and thus vector of 8, that all the components are greater or equal than zero. If this These integrals converge when we consider t going to infinity, then x-bar is a weak paradox solution. Otherwise, if we consider all the components of the 8 vector distributed greater than zero, we can ensure that we have a paradigm optimal solution. Now, considering the Chebichev scalarization, a point X bar is a wiki paradigm optimal solution for the original problem if only if. The original problem: if only if there exists some vector of eight or where all the components is strictly greater than zero, such that the function defined by these expressions converges when t goes to infinity. So, again, for the phi function, so if you assume that the phi function and so firstly, the feasible set is a robust set. Set is a robust set and the phi function satisfies the assumption a1 and a2. If we consider the vector of 8 where all the components is greater or equal than zero, we have that the following conditions are equivalent. So first, x-bar is a solution to the 18 problem, 80% problem. The mean value of the phi function over the intersection of Over the intersection of H C bar and the feasible set is equal to C bar. The variance and the modified variance is equal to zero. And moreover, in these equivalent situations, X bar, it will be a weak period of solution. If you consider the vector of H, if all the components are strictly greater than zero, we have that Parietopton solution. Now, Now, to ensure that we have a necessary and sufficient condition, we will assume this is another assumption. So, firstly, A1 prime, the feasible set is a robust set and closed. The objective functions are continuous, and there exists some index L0 and some real number such that this set is a compact set. Also, we can prove that these assumptions A1 prime, A2' and A3' imply in the original assumptions. So now using these and other assumptions, prime, if we consider X bar in the feasible region, then the following conditions are equivalent. So X bar is a weak variety of solutions to the regional problem. To the original problem, there exists some vector of 8s with all the components strictly greater than zero. And the mean value of the function over the intersection of the level set related to C bar and the peaceable region must be equal to C bar. The variance and the modified variance must be equal to zero. So using this theorem, we can state an algorithm that is this algorithm. That is this algorithm mean value of level sets for multiple objective problems. So we have three parts of the algorithm. So the first part is related to stellarization, the second one, initialization, and the last one related to the iterations. So the user should provide the parameter related to the stopping criterion, the vector of H with all the components is strictly greater than zero, and also the vector of we need to. We need to compute the utopian vector. So, in the esclarization part, we sclarize the problem. So, we use the Chev Chevy sclerization. In the initialization, we need to ensure that we have a real number such that this level set is not empty to ensure that we just can consider this real number is a really large number. In the iteration, we will compute the level set. We will compute the level set and also we will compute the variance modified and the mean value. So, in the end of the iterations, we will get the optimal value that corresponds to the mean value and also the set of solutions h bar. So, the convergent results. So, under these assumptions, we can ensure that the sequence generated by the outward is convergent. It's conversion and the limit C bar is a global minimum for the original the Chevichev function over X. And furthermore, we can ensure that this intersection is a set of global minimizers of this function over X and consequently a subset of a weak palette of new solutions for the regional problem. Now, some considerations. We want to ensure that this intersection is no empty. This intersection is now empty to try to initialize the algorithm. So we just consider the feasible set is equal to the one kuboid. So we just have bound constraints. For this kind of problems, we can ensure that the distance between these points inside of this level set when k goes to infinity is the distance goes to zero. And also, if we have the problem that we want to minimize f over d is equal to minimize. d is equal to minimize f over this intersection that is equal to minimize f over dk where dk is the the smallest kuboid that contain this intersection so throughout the algorithm you use uh i explain how we reduce the the boys so we for a simple example if we have only two variables we have the upper and lower bound for this variable The upper and lower bound for this variable. So we discatize these two intervals. So we construct this mesh, we get all these points, we evaluate the function in all these points, and we'll select only the best point that correspond to all the points that has image lower or equal than CK in that iteration. So using the maximum and the minimum valve for these points, we construct. For these points, we construct this new void and we continue the process. So, here I will say some, it's a very preliminary results, numerical results. So, we performed these experiments in this high-performance station with this specification. The implementation is made on MATLAB 2018 version D. To initialize the algorithm, we consider CZ. To initialize the algorithm, we consider C0 equal to 10 power 8. The stopping criterion was 10 power minus 8. And the components of the vector related to the topian was considered 10 power minus 4. The test set of problems we considered 26 unconstrained and both constrained multiple problems with dimension at most four presented in these works. And so the primary results that I will say now it's That I will say now. It's about 3,000 runs. So, the first simple example that we have here, we have a noise move problem. As we can see here, one of the functions is noise move. And the right-hand side, we have in black the exact paradox front. And in red, we have the points generated by our algorithm. And here we have the average of the time and average of the iterations. And the average of the iterations for the 3000 grams. Also, we performed some experiments to the ZDT collection. As you can see here, again, we have in black the data palette front. In red, we have the points generated by the algorithm. As you can see, we were able to get points in the true palette front. And the second result was a The results were using the same test problem, and all the parameters were the same. Now, to be as fair as possible, we allowed a maximum of 20,000 function evaluations for all the solvers. The comparison was among our augur, moif and DMS, given by these papers. And for MoIF and DMS, we consider the default valves. So we consider to match. We considered two metrics for performance profile. The first one was the Burt metric that computed the percentage of points generating the reference plate front. And the hypervolume that computed the volume of dominated region. So here we have the first results for purity. As we can see here, in terms of efficient, we have that direct mood search is better than all solvers. As we know, the performance profile is sensitive to the number of solvers, so we decide to compare by pairs. So, in the first picture, we can see that direct mood search is better in terms of efficient. However, in terms of, if we just compare with MoIF, we have that our is slightly better in terms of efficient. However, in terms of robustness, when we take a look in this picture. We take a look in this picture. All the solvers are competitives. Now, in terms of hypervolume, as you can see here, we have that our algorithm is better, is like better in terms of robustness. That can be confirmed by these two pictures and some conclusions and feature work. So, in this work, we could extend to multi-objective optimization developed optimality integral conditions. Malty integral conditions. We propose an algorithm to solve a multi-objective problem based on Chevy Chi-8 scalarization and we prove the convergence. We take some preliminary numerical tests just to illustrate the effectiveness of the method. However, the conditions are stated in terms of multiple integrals, which may narrow the application of search theory to problems with many variables. Theory to problems with many variables. And as you could see in the pictures, the points in the paradigm front approximation are not overspread. So this work was accepted publication two weeks ago in the numerical functional analysis optimization. So future work, we want to implement efficient method to compute integrals with many variables. That could be using a Monte Carlo simulation approach. Approach. We want to also smart choice of eights, trying to generate points well spread in the paradigm from the approximation. How we can do that, we don't know yet. And we want to also explore other scalarization techniques. So this is the variable thing. Thanks for attention. Thank you very much.