And thanks to Shu for organizing this wonderful workshop. And so today, so it's interesting. Andrea was run, well, was short on time. I wish actually I could allocate some of my time to her because comparing to her work, I feel that mine is more like a light dessert. So hopefully, I will, and it's pretty easy to understand. I can summarize. Pretty easy to understand. I can summarize my main message maybe in just one sentence. But anyway, I will try to finish as soon as, like, hopefully in like 35 minutes or something. So anyway, the talk, the title is A Deep Learning Model Superior for Missing Data Imputation: Evidence from Empirical Comparison. So by the title, you see that it's an empirical comparison. So essentially, it's a simulation. So that's why I say that. So that's why I say that it's going to be light. But the message is useful, it's actually also consistent from my previous experience. So, this is a joint work with my former master's student, Zhang Hua Wang, who's currently a PhD student at Missouri, and Michael Nkande and Jason Polis. So, the paper is forthcoming in survey methodology. So, just okay, so if I summarize the whole talk, what I want to say is. Whole talk, what I want to say is just deep learning models recently has been advocated to use for model imputation or for imputation in general. And so my, by doing this work, I realized, did some real simulations and I realized they are just not as good as they advertised. So then this is really a cautionary note. So just quickly, we all know missing data is prevalent in surveys and the observational study. Surveys and observational studies. And so, a main approach for handling missing data is model immutation. Of course, there are also other imputation methods, but this one is getting a lot of traction and people use that. So, the core of model mutation is really model-based. Essentially, you build model based on the build model on the observed data and then impute missing data by their posterior predictive draws from. Predictive draws from this model. Okay, so it's again, I assume this audience are very familiar with this type of model imputation, just in general imputation. So there are two, but I will give a quick overview. So there are two classes of model-based imputation methods. And the first one is so-called joint modeling. Remember, model imputation is really model-based things. The first one is called joint modeling. Essentially, you just, so if you have a data matrix. So, if you have a data matrix, then you have p variables. So, you just specify a joint multivariate distribution for all variables. Okay, and then once you have this joint modeling, of course, you will be able to generate, you will be able to generate, derive the conditional distribution and generate all the posterior predictive distribution in theory. Okay, so the second one is the so-called four-conditional specification. Specification approach. So that is going the other way. So it's specify a univariate conditional distribution of each variable that is missing data. So given all the other variables, essentially for each variable y, j that has missing data, you just specify a conditional, a conditional on the others. So if you have a continuous variable, you assume, you can assume a normal distribution. And then if you if And then, if you have binary, you can just log this model. So, it is very easy. So, then once you do this, you impute the missing data variable by variable iteratively. So, you do it at first, and like this is first variable, and then you generate, you fit the model, and then you impute the missing values, and then you do it. Just you iteratively go through all the variables with missing data. Variables with missing data. So, this is all pretty well known, right? So, obviously, the joint modeling sounds, you know, statistically, it's valid. But for high-dimensional data, in high-dimensional data, especially in real-world data, we have not only high-dimensional data, we also have different data with different missing type, different types, right? We have categorical, continuous, and binary. So, it is extremely. So, it is extremely difficult to specify a joint model for those variables. So, because of this, the so-called MICE approach, the model of implementation by chain equation, which is a special case of this full conditional specification, is very popular. So, MISE is just to do this one variable at a time, and then you iteratively impute all the variables. All the variables. So it's very flexible because what depends on whatever variable you have, whatever type of variable, you can just specify GOM, simply as saying. And then they're also implemented in several software package in R, also in SAS. So it's a Famburian has this paper, has this book and also software. It's very widely used. And of course, as a CRE monks, you will start to look at the mice and then you will immediately realize the mice. You will immediately realize MICE has a theoretical drawback. So essentially, when you specify all these conditional variables and univariate conditional distributions, they might not be compatible. They might not have a joint distribution actually corresponding to that will give you all this individually specified marginal distribution. So that is known as incompatibility issue. And this is a particular problem when you have variables. A problem when you have variables of different types. And I had some old papers discussing the possible, like the theoretical, the implication of this theoretical drawback. And so again, this motivate the staff community, statistical community, because when we have a obvious theoretical problem, we want to fix it. So over the years, there's many attempts tried to deal with this, address this incompatibility issue. And so in like And so, in like, for example, there are people who specify advanced joint models like Bayesian non-parametric models by Jerry Ryder and colleagues, and the copular models, just name a few, and myself even had some attempt. But in reality, so I've done this. This was actually one of my first projects when I was a PhD student to try to fix mice. But then when I did simulations to check, To check mice, the behavior of mice, I realized mice is extremely hard to beat. So, in other words, mice, particularly when you have actually the marginal, so mice work very well in practice. And so in 2017, I had another master's student, Michael Akande, at the time. We did just simulation. We did simulation use real data, use real survey data to see that. Survey data to see that how mice compare with the more recent Bayesian unharmatric models in imputation. Again, this is just purely for personal, I'm curious how it works. Now, what we find are two things. First, mice, for mice, you know, in mice, when you specify the universe, the conditionals, you can use the standard GOM or you can use tree-based models, for example. Uh, tree-based model, for example, CART. And then we find actually CART, mice with CART, like comprehensively outperform mice with GLM. So that's the first finding. Second finding is mice with CART, with CART, those three-based marginal conditionals actually works also beat the Bayesian non-parametric approach, the more theoretically justified Bayesian non-parametric methods. So that was our So that was our takeaway in the 2017 paper. And then after that, I was basically telling people that I would just go with mice. If the real problem come to me, I would just say that if the high-dimensional is a lot of different type of variables, then I would just use mice with card rather than GOM. So that was, and mice with CARD is implemented in the Vamburian's mice problem. Amburance mice problem software in R. Okay, but then of course, see the our, we're now living in this machine learning or deep learning era. Um, so not surprisingly, everything, so we started to see the literature emerge using advocate using machine learning or deep learning-based methods to impute for imputation, and particularly for the joint model imputation. Because arguably, there are several theoretical advantages. Are several theoretical advantages. First, we know that many of those, you know, those deep learning methods, many of those methods are built for this high-dimensional complex data. And so they are some advantages like avoid making distributional assumptions. And it can easily handle mixed data types and high-dimensional types. And supposedly, it can capture the non-linear complex relationship between variables. And also, it scales very well to high-dimensional. And also, it scales very well to high-dimensional settings, so it's fast. And also, it can leverage GPU type of parallel computing. So, there's a lot of good advantages, theoretical advantages. And then those papers are usually published in the, you know, in machine learning conference journals. And then, you know, conference journals, they usually have one section of experiments. And then they always do some simulation. And all the papers I see say that, well, look, they did some simulation. They did some simulation and say that these methods are superior to everything else and to the old-fashioned methods and things like that. But with my own experience with mice, kind of my cautionary note is like mice, it's pretty hard to beat. I become very suspicious of those claims. So this kind of motivates my study. So this is kind of a simple study, but I really. So, this is kind of a simple study, but I really want to answer the question: Is those deep learning methods as good as advertised in those papers? So, particularly, then when I look closer to those papers, I notice a few things. First of all, all of the simulations they claim are based on this small simulations on the so-called public, on small simulations of so-called public benchmark data. Those data, like this breast cancer data, whatever, only have 500 units or a bunch of them. Or a bunch of them. But they're all, they're nothing. Those data, to be honest, are not really representing real survey data. Again, this paper when we originally did this paper was for survey. And it really doesn't representing real survey data, which has a lot of like ordinal data and the sample size is big, the sample size is relatively big and things like that. So that's the first thing, kind of disturbing finding that they are using benchmark data. That they are using benchmark data. And the second, I think more problematic to me is they usually only report when they do the simulation, they only report usually two after one or two overall performance matrix. Basically, they treat this as a prediction, and then they will report the prediction mean square error and overall of the data matrix. Okay, and so they're not, I never see actually one paper in machine learning. Actually, one paper in machine learning this conference to try to evaluate the repeated sampling property properties in missing data. And again, if we were coming from START background, you know that when we talk about imputation, particularly like model mutation, one of the key points is when you do imputation, you have uncertainty into building to those imputation. So we also need to talk about, and also imputation. Need to talk about and also imputation is we care about the statistics so we don't care about we don't care about the individual the imputation of the accuracy of an individual the imputation of a particular data point right we care about actually a statistic a target estimate and how they perform how the repeated sampling properties of that so notice all of that then that motivates our simulation Motivate our simulation. So, this study. So, this study is, so our goal here is just, we want to conduct extensive simulation based on a real survey data and to evaluate different machine model imputation methods. And we want to repeat, use, we want to examine the repeated sampling properties and also some more relevant matrix that the statistical community has established. You know, has established. So, what do we choose? So, we compare four methods and the mice, as I mentioned earlier, in an earlier simulation, we realized a mice with card actually works better than mice with GOM. So we use a mice with card. And the second one is just instead of use mice, sorry, card for the conditionals, we use random forest because the argument is ensemble tree methods work often better in other problems than just the In other problems, that then just a single tree card. So we call it a mice RF. And then for deep learning, there are just many methods out there. So, what we choose is the first, we choose two. And the first is based on the generative adolescer network. So again, so this they just in this paper, in Yung et al. 2018 paper, they extend up to machine to imputation. And this paper has been pretty well cited over. Pretty well cited over 500 times, so it's pretty well known in the machine learning literature. And then the other one we choose is usually based on denoising autoencoders. Again, autoencoders, I think it has the machine learning literature, they're very good at giving names. So we know statisticians know this, the core of this is kind of factor model or low rank representation of the data. Representation of the data. But this one is also kind of pretty well known. And then, of course, this is related to later the matrix completion, but at the time, we decided just to use this too because they are pretty representative of what people are using. So those are the four methods we're going to compare. And again, just a few notations because I need to introduce the metric. But this, I will not go through too many details. So we use y again to y to. Again, to y to denote a matrix, a matrix with missing data. And Yij, so we have variable J with P variables and have N units. And then we have M is just the missingness indicator. If it's missing, then it's one. If it's not missing, it's zero. And then we denote the missing part and the observed part just by y of z and y miss. And then q is the target, it's a target population estimate. It's a target population estimate again. This is all pretty simple. So, I already talked about mice. I'm not gonna talk again. So, it's just for completeness, I put it: mice is essentially you do this one variable at a time, you specify the conditional models of one variable given others, and then you go through all the variables. So, you first you initialize just by join from the marginal distribution, and then you go through it. And then you go through it, and so that's that's nice. And so, deep learning methods. Oops, see, oh, then it's deep learning methods. Again, just a few, because we're talking about deep learning methods, it's a comparative, so we need to talk a little bit more based on that deep learning methods. Before I dive in that, I'll just say that I, as I mentioned earlier, in all the recent proposals in the machine learning literature, they claim that the literature they claim that the the security based on simulations and then they they what they are the the matrix they use for that claim are these two things one is called overall rmse essentially this is missing data matrix and then you just you have this white hat which is imputed data and then you just calculate the overall mean square error of those of all of the the missing missing data and then you you you calculate that divided by the number of the Divided by the number of the missing entries, and that's the overall mean square error. And then the other one is for you have four categorical variables, they call it so-called accuracy. Essentially, it's a proportion of the imputed values that are equaling the true values. And again, they treat every single unit, every single missing data as essentially SES demand. And then they have an overall summary of that. Okay, so those are the Okay, so those are the machine learning, the metric they've been used. And as I mentioned, we're going to close exam two methods. One is based on GAN. Again, this GAN is one of the most widely used deep learning methods in the last eight years. So the main idea, so it's a deep generative method. So the main idea is really to generate new data that resemble existing data. So like I will show you. Listing data. So, like, I will show you. Again, this talk is not really about introducing how those methods are. So, I will not talk about details, but I mean, before again, just in general, everyone told, so you have a generator and then you have discriminator. It's actually you generate, you based on the, you kind of based on the data, the observed data, and then you generate data, you use this generator to generate this, the data, generate this data for basically the. generate those data for basically the imputed missing data missing data and then then you have discriminator and then you you hope that then try to to see that how this to the generated data how far that is from the uh um generated data and of course then you do this iteratively so in the end the uh kind of reach uh equilibrium essentially a set of point uh so that that is and of course in our in our case that we just want to generate the missing That we just want to generate the missing data. We want the imputed data to be as similar as the observed data. So then there are details here, but the main idea is just revised again for multi-dimensional observed data matrix and try to generate data very similar to the observed data and do it in a joint modeling type of. joint modeling type you don't really specify arbitrary you don't really specify the specific explicitly specify the joint model but essentially you are doing it in a whole so that is the um that is scan the other one is meta basically denoising autoencoders again autoencoders if you are if you you know this you know that it's it's just a fine and low rank representation of a high dimensional data and so the denoising autoencoder is a special Denoising auto-encoder is a special thing, is it instead of using the original data observed data, it will add some noise to it, random noise to the input, so that the learning is more stochastically stable and then its performance is better. So again, same idea, you are kind of doing the low-rank representation, try to learn low-rank representation of the observed data in this data matrix, and then you use that to impute. And then you use that to impute the missing data. So that is all, you know, that's kind of background. And then, so, how do we do this? How do we do the evaluation? And this is a procedure. The procedure I'm talking here is, I would say, pretty standard in missing data imputation literature, in the STAT imputation, in the SAT missing data literature. So, the idea is you choose, so this is simulation-based. Choose so this is simulation-based, but it's it's slightly more complicated than just simulation. So you actually choose a complete data set as a true population, and this data can be real data, can be simulated data, but anyway, this is kind of ground truth. I mean, the true population. And then you choose a set of target population, the target estimate, for example, the marginal probabilities and things like that, and then calculate the Q from the population data. Like the population data is usually pretty big and then that's Data is usually pretty big, and then that's called your grand truth. And then, what you do is you randomly draw this replacement of sorry, without replacement, age samples of size n from the population. And then you create, and then once you have those age samples, you punch holes. So, you create missing data in each of the sample according to a missing data mechanism, MCR or MACR, right? So, you punch holes. And then, so you have this each sample of the data. The data, you know, small samples from a general population that has missing data. And then you go to use your favorite missing imputation methods to impute those missing for each of this sample. You impute L data set and then you construct the, then you have this imputed data set, you construct, you will get the point estimate and the. will get the point estimate and the variance and then you combine then you get a you will get an estimate for each of this for each of this age sample use the Rubens rule by combining this L imputed data set again I'm not gonna reiterate what is Rubens rule assuming everyone probably knows that in this audience and then once we have this you know estimated once for each sample we we we calculate our the calculate our the calculate the q hat basically the q hat from the from the empirical data set then we can calculate the the performance matrix so of course the the key okay let me um talk about so so those are the kind of the the the general procedure and of course they there's in so in our simulation we follow this and but we need to have a make a few choices so first what is our population so we see american communities So, we use the American Community Survey that that's, you know, we particularly use 2018, the ACS public use microdata sample. So, it's pretty big data set. And this, it has a rich set of, so the here, the sample units are households. Well, actually, not households, not only household, also individuals. So, have a rich set of variables. We have household level variables and also individual level variables. But in the end, Variables, but in the end, so the settled our population we have is over 1.25 million units, and then for each unit, we have 18 binary variables and 20 categorical variables and eight continuous variables. So this is this is, you know, this is represent a pretty typical large survey. And then the choice of estimates. So what are the estimand we want to check here? So for the, and it's high, this depends on the And this depends on the type of variables. So, for binary and categorical variables, we just look at the marginal probability of each category. Like when you have a binary, we just do the P. And when you have categorical, we know what's the marginal probability of category one, two, three to j. And then, and then we actually also want to check how multivariate distributional properties are preserved. So, then we also. Are preserved. So then we also calculate those bivariate probability. The one of the estimates is also the bivariate probabilities of any two-way combinations of categories. So there are quite a lot of estimates here. And for continuous variable, there are many ways of doing that. You can have the mean, you can have the variance. But we, because we noticed that in theory, this really the categorical variables are common. So we try to facilitate. Common. So we try to facilitate the meaningful comparison between continuous variables and the discrete variables. So what we did was we discretize the, again, when we do imputation, we still impute them as continuous. But when we do the, when we calculate the S demand, we discretize them into K categories based on the sample quantiles. And then it becomes just the same as about marginal, the binary and categorical verticals. So we just calculate the marginal and the bottom. So, we just calculate the marginal and the bivariate probability as before. So, that was our estimate. And then, the key question is: what are the matrix we use? As I mentioned earlier, that you can look at the overall RMSE or accuracy of the imputations. Those are useful, but I would argue that it provides a quite incomplete picture of imputation because here there are many estimates and in statistic imputation. In statistics, imputation, the core idea of imputation is not you want to, like the goal, almost impossible goal, is you find a perfect prediction or perfect imputation for each missing entry. What you want is you want the imputed data to preserve the estimate. You can get a good estimation of the estimate. So, what we did is we focused on three metrics on repeated sampling properties. So, the first is Sampling properties. So the first is this absolute standardized bias. So what is this? Remember, we have age, so we have a population, but we have age samples. So we essentially for each sample, for each estimate, then we can calculate the absolute standardized bias, and then we average over all the age, or all the sample, like the age samples. And then we also have the relative MSE. So that's the same thing. So that's the same thing. It's just the denominator is the protocol, like the true, the pro, sorry, the protocol, the estimator. And this is, and the numerator is really the difference, the mean square error between the imputed estimate and the estimate. And then coverage rate. Coverage rate is just we will see that, you know, when Is just we will see that you know what you the 95 for example 95 confidence interval based on the imputed data how often the true estimate fall into that fall into the the imputed the the confidence interval from the imputed data and average over the age samples um so the and of course a key point of all of this is uh implementation is uh hyperprime selection um so here for mice So, here for mice, we just use the default choice in the R package. And for the there, the default maximum number of trees allowed in random forest is 10. So again, we tend to fine-tuning, but we choose this because this mimic really what in practice how people deal with this type of thing. Like the practitioner, they just use default. They just use default. And for gang and for MIDA, what we did is we actually read their paper and then we use the default architecture. We follow their architecture. And then we did fine-tuning of the hint rate, for example, and use a agree to balance the generator loss and disagreement loss. And the same thing for the denoising auto-encoder, we're basically used what they propose in the original papers. So again, this is very important. This is very important, but a lot of details are in the paper, so I will not, you know, I will not go through, I don't have time to go through that. But the key thing is, when we do simulation, you're doing like 100 imputed simulated samples. So that caused a big problem for us because we cannot really manually tune each of the H samples in the simulations. So what we did was we actually just tuned for one sample and then use that hyperparameter for That hyperparameter for all the samples. Again, I know this is not optimal, but for the simulation, like that, and that's space for improvement. And that's probably the big problem there. But we find that this will still be very informative. And we tried some like single imputations also later. But this will give us, again, what we choose is the hyperparameter selection is really mimic what the people in practice. Mimic what the people in practice would do. So then this is the result. So after all this setup, this is the result. So what we did, so this is just show you some results. It's like we each will have 100 samples. Each sample have 10,000 samples and we create 30% missing computer at random. And for each of them, the sample, we generate 10 imputed samples. So first of all, look at this computational time. This is pretty striking for deep learning methods. For deep learning methods, they are really quick. So, for all of this, it takes again 1.5 minutes and middle 4.2 minutes. But mice, probably everyone use mice with high-dimensional data know that it's really slow. So, mice with card is 2.8 hours, and then mice run forest is even longer because you feed more trees. Okay, so in terms of computational time, like the deep learning methods really dominate the mice as expected. Mice as expected. This is just on a typical desktop. So, and then we also look at the overall RMSE and accuracy as people, as machine learning literature report. So, you can see here, the information is not too much. Like, this is overall. So, it's, you will see, look at this table. So, this, we actually tried both, sorry, both MCR and MAR. And so, you can see the MIDA in terms of RM. Can see that MIDA in terms of RMSE is pretty good and it's the best, actually, but it's hard to say that how much better they're than the car, like how big advantages here. But they look appear to be good. Gain doesn't look to be good in RMSE, but when you look at the accuracy, it's like actually card works pretty well, and gain gives you good accuracy. So it's so I'm saying. So I'm saying that this is this is what this is pretty still pretty murky. So now we go to our simulation strategy, this is the simulation procedure, our evaluation procedure, look at the repeated sampling results. And so this is, we first look at the absolute standardized bias. Remember, we have a lot of estimates here, the marginal probabilities and bivariate probabilities. So what we did, again, So, what we did, again, there are just so many of them. So, how did we summarize those? So, what we did is we actually just look at this is categorical variable, this is the binned or dichotomized, sorry, the discretized unit variables. So, we just look at these. So, we have like one, I don't know how many, many, many, like 200 S-MN, for example. And then we just look at their quantiles of this 200 S-MN, like for each of the S-Men. For each of the S men, of course, we have an absolute standardized difference. And this is times 100. So this is kind of percentage. So this is pretty striking to me. And this is for marginal probability. This is for bivariate probabilities. So it's very striking when we look at this table. So you can see that basically it says card is really dominant the random forest. Well, particularly card and card dominate GAN and meetup. Dominate gain and meetup. So if you look at this, the lower, if we look at this, we can see that, okay, so card works very well, but the up until like 50 percentile is still the difference, well, it's still is kind of it's better for 4 percent because this is percentage really 4 percent. But then the huge difference really start to see in the 90 percentile. So remember, again, this is 90 percentile of the um of the standardized bias of this whole slew of whole slew of biases. So you can see that for categorical variables, you can see that GAN and MIDA, especially again, when you have, so it looks like in the high, like occasionally it's like if you, so when you have so many variables, when you have so many, sorry, so estimates. Sorry, so as demands. So you can see that it can create some really wacky results. You know, it's pretty unstable. And this becomes even more obvious when you start to look at the continuous variables. It seems that in continuous variables that again often just do things so liberally that the upper quartile of the ASP, the standard eye bias is huge. It's huge. Is huge. It's huge. So then let's also look at it. So this is about bias. Then let's look at the relative RMSE, right? So this RMSE in the same setting, the same story. So for card, it's pretty stable. But then when you look at the again and MIDA, the numbers become really crazy. It's like in the high percentile. And then you basically say that, you know, you do this many, when you have this many, you know. This many, um, you know, we have all of this estimate, and then if you for each estimate, you calculate you do the imputation and you calculate the you have the estimate. And if you look at the RMCE, like they so this like can produce very, very spread out imputation from the from infutation from simulation to simulation. So those numbers are really striking to me. And another striking thing to me. And another striking thing to me was a sort of unexpected is after card works better than running forest, especially at the high quantiles. And then later, so this is a graph. This is about coverage. So let's look at the coverage. So this is a graph and shows better. So you can see the coverage is also again, this is the coverage because we have many estimates. So that's why we draw the box plots. This is the same story. It's the same story. The card works better, works pretty well. It's retain the nominal rate, the coverage for quite many estimates. The striking thing is really MIDA and GAN are sometimes just doing pretty miserably. So then we also, I have some results for the MAR. The previous was for ACR, MAR. The result is essentially the same. Are the result is essentially the same? So, so I'm not going through this. And again, so we find so one thing we find is interesting is how can random forest supposedly work, ensemble trees supposedly work better than a cart works poor. So we did some evolution of the, we draw this graph basically in the again, we use the mice the software. So their curve is kind of the almost Is kind of the almost the same as a random forest with just a maximum tree of one because then you only have one tree. So you can see that again at the lower quantiles, we just draw the relative RMCE. But the point is that as the number of trees increase, the upper quantile of those, the relative RMC across all this estimate is increasing dramatically. So we figured, and the same story for bias, I just didn't show here. For bias, I just didn't show here. So it seems to me that what it seems to us, the issue seems to be really overfitting. And again, this is so like just summarize this, we look at this and then we will ask why. So it's like most likely it's because overfitting. So the as we see it in the random forest case. And another thing I'll say that, you know, so try to understand this because deep learning supposedly works so well, why? Suppose it works so well. Why does it fail so miserably here? So, say that deep neural networks excel in detecting complex soft structures of big data, but maybe it overkill for data with simple structure by overfitting. And another thing I was thinking is maybe deep learning models are pretty hard to train and they usually need large sample size. Our sample size here is 10,000. Maybe they're not adequate to train those models. So, we actually tried the larger ones. Models. So we actually tried the larger ones with n equals 100,000 samples, but the results actually are pretty similar. And what's really striking is the machine learning papers, they use the so-called benchmark data. Often the sample size is actually much smaller than that, than what we consider here. Sometimes it's 500, sometimes 1000. So, but again, based on the RMSE, the overall RMCE, they can claim that. IMCE, they can claim that. So that is what I find interesting. And another thing is, like, I believe, I do think maybe the tuning we're doing here is not the best. So maybe fine-tuning parameters can vastly improve the performance. But my question is, for practice, how often is this kind of fine-tuning done for imputation? And remember, imputation is really a secondary job of, say, in analysis. It's a lot of people. Say in analysis, it's a lot of people do practitioners do imputation because they have missing data, it's a nuisance, they need to deal with that, and so that's why they go to use mice or software or the IV wear just by use the default choice. And then, you know, they use that because it's easy. And once they impute the data set, then they can proceed, right? So, if you mimic the practice, what the practitioner do is they just most time they will not have the opportunity to actually. Have the opportunity to actually fine-tune the parameters, and not to say there are so many different modeling, the deep learning methods there. So that's what we find. So this is just a quick update. So again, I didn't finish this in 30 minutes, sorry, but just a quick update, quick recap. Here we use a real database simulation to evaluate. So we find that mice. So we find that mice with card as conditionals consistently outperform often by a large margin again and with a with default parameters in repeated sampling properties. And within mice, we find also card consistently out from random forest as the and the advantage increases as the tree of maximum tree in the random forest increases. Again, this is using the Again, this is using the result using the mice package. And so the possible explanation I would think of it could be the choice of parameters and also overfitting. So what is the takeaway of this simple exercise? At least to me, it reveal a few answers some of my initial questions. First, definitely, deep learning is not a panacea. It might sometimes even be a poison pill. So we have to be very careful. And the second, Be very careful. And the second is a hyperparameter choice is really crucial, but fine-tuning is usually not feasible or available for practitioners of mobile imputation. And another point is missing data imputation, it looks like a prediction problem, but the goal is different from prediction or like what it tried to preserve. You know, it tries to preserve the key ideas. You do the imputation and you try to preserve these statistical properties, the population level properties. So a single prediction MSE type of metric is incomplete and sometimes can be misleading. But that being said, I have to say that it's clear deep learning methods have vast computational advantage over massive data. So if you imagine you have a thumb like EHA. You have something like EHR data, it's a huge millions of records. Then, mice, good luck with mice, you do mice, and then it might take days or months to run. So, machine learning can, the deep learning method can just run very quickly. So, that's one huge advantage in large sample. But in the smaller data sets, or in smaller, even like 10,000, 100,000, mine so far seem to be doing really well. So, of course, the one limitation here is I just choose again and middle. Just choose again and MIDA. There are other deep learning methods like matrix completion. So, how do they do? I have no answer, but my hunch is my again, if it's not, if it's reasonable, if it's a moderate data size, moderate means like in this case, 10,100,000, it works actually really well with this card. And so, I would say that more evaluation is needed, and we shouldn't another thing. Needed, and we shouldn't. Another thing I learned is that we shouldn't take a lot of this conference papers, the conclusion on those conference papers at face value. We have to be critical and then examine their properties. Thanks. That's all. And there's some reference here.