So, okay, so next, I'm happy to introduce Joel Cohen, and he'll tell us about prime numbers, variance functions, beta larvae, aphids, and tornadoes, and about research in mathematical population biology leading to unexpected applications. Thank you, Noah, and thank you for the invitation. And thank you for the invitation to participate in this conversation. I'd like to express my appreciation to Mark Feldman for his leadership with theoretical population biology, where I've published a number of my papers too. And I think it's been a great force for good. And I love the story you told today, Mark. So thank you very much for joining in and telling that story. Noah gave me a menu of topics. Menu of topics, and I picked research in mathematical population biology leads to unexpected applications. Are you able to see this laser pointer? Good. Okay. So there are two parts. One is to tell you what Taylor's law is, and the other is to talk about unexpected applications. In 1941, a statistician in Connecticut named Chester Bliss. Named Chester Bliss, did a study of the distribution of Japanese beetle larvae. He had four areas, and in each area he had 144 samples, and each sample consisted of 16 one square foot quadrats. He counted the number of Japanese beetle larvae in each of those 16 quadrats, and for that sample calculated the mean and the variance. mean and the variance. And he plotted the mean on the horizontal axis, the variance on the vertical axis, both on log-log scales. And he found that the dots, one dot represents a 16, a sample of 16 square foot quadrats. The dots fell along a first line, a straight line. That was true for areas one and three on the left and areas two and four on the right. And four on the right. So he said the log of the variance is approximately a linear function of the log of the mean. Why did he care? Because in analysis of variance, if you want to compare the sizes of two populations, the mean sizes, you assume that the variance is equal in ANOVA. So he needed something to stabilize the variance. A lot of other people made the same empirical discovery, and it became known as Taylor's law because Taylor did not discover it. So it's a power law variance function for samples. Given a set S, in the previous case, we had 144 samples in each area, of size N, 16 in the previous example. A sample Taylor's law holds if If there are numbers A positive and B real, such that the sample variance right here is near A times the sample mean raised to the power B. That's the power law form of Taylor's law. If you take the log of both sides, log of the variance is near log of A plus B log mean. mean that's the log linear form and b is the slope of the log linear form we'll refer to this slope a lot so please make a note it's the relation it's the slope of log variance as a linear function of log mean or we could divide by this factor so the variance divided by the mean to the power b is near some constant Is near some constant for some value of b. This is called a moment ratio. This calculation does not require that the population mean or the population variance exist or are finite because we're talking about samples. We're not talking about the underlying probability distribution. So the data structure for the sample Taylor's law is we have a bunch of samples. have a bunch of samples, s equals one, s equals two, s equals three. And each sample has a number of observations, and the number might vary from sample to sample. Here it's three, here it's five, and so on. We calculate the mean for this sample, the variance for this sample. We plot them on log mean, log variance coordinates, and we ask, do the dots fall along a straight line or something else? In addition to the sample Taylor's law, which we just saw illustrated by Bliss, there's a Taylor's law for probability distributions. For a set of non-negative random variables, X, labeled by the sample number, or probability, it might be a continuum, or probability distributions on the non-negative half-line, a population T. A population T L holds if each random variable has a finite positive mean and variance, and there are two numbers a and b down here, such that the variance is a times the mean to the power b. Example, Poisson distribution, mean equals variance. So we have Taylor's law with slope b equal to one. Exponential distribution, mean, variance equals. mean variance equals mean squared. Variance equals mean squared for every exponential distribution. Also the half normal distribution, the variance is equal to the mean squared. So we have slope two. In the walled distribution, if you keep the shape parameter constant and vary only the mean parameter mu, you have a slope three. The variance varies as the mean cubed. Cubed. And the physicist version of Taylor's law, which was completely independently discovered, is called fluctuation scaling. And in physics notation, they write sigma twiddle or tilde bracket x bracket alpha. What that means, translated into probability theory is sigma is the dispersion or the standard deviation, the square root of variance. This is the expectation. This is the expectation, the population mean. So if we have physicist fluctuation scaling, then we can square both sides. We get the variance equals some constant times the mean 2 alpha. So alpha and b are related. 2 alpha equals b, b equals alpha over 2. Okay, let's go on to some examples. Okay, let's go on to some examples. And I have published about 40 papers on Taylor's Law. And this is a small sample of the surprises that I've encountered. Tornadoes. Why am I talking to a population biology group about tornadoes? Because there is no biology in tornadoes. And it's important to demonstrate that the mere observation of Taylor's Mere observation of Taylor's law does not give you information about the biological mechanisms underlying it. So there's something important to learn here, in addition to what we learn about the natural world. There is no aggregation. Tornadoes don't like each other. They don't dislike each other. There is physics involved. So let's look at tornadoes. The USA has more tornadoes than any other country. An outbreak. An outbreak of tornadoes is defined as at least six tornadoes starting successively less than six hours apart. In the recent past, 79% of tornado fatalities and most economic losses occurred in outbreaks, not the isolated tornado. The day before yesterday, we had an outbreak of at least 10 tornadoes here in the United States. There are no trends in There are no trends in the number of reliably reported tornadoes and no trends in the number of outbreaks in the last half century. But the mean and the variance of the number of tornadoes per outbreak and the insured losses increased significantly in the last half century. What's going on? You take the same number of boxes, each outbreak is a box. Is a box and the same number of balls, each ball is a tornado, and you increasingly concentrate them in a smaller number of boxes. So the mean and the variance of the tornadoes per outbreak increases. This is the number of tornado outbreaks per year, not significantly between 1954 and 2016. Okay? Okay, slightly downward trend, but not significant. The mean number of tornadoes per outbreak increases by 0.66% per year, significant increase. The mean, the variance of the number of tornadoes per outbreak increases by 2.89% per year. So the variance increases 4.3 times faster than the mean, and it's a pretty good discussion. and it's a pretty good description of the last 55 years of tornado data. There are some exceptions, like this outbreak in 1974, which was really off the charts. The variance of the number of tornadoes per outbreak is proportional to the mean to the power four plus. The higher percentiles increased faster if we use quantile regression. Use quantile regression. So the 20th percentile of the number of tornadoes per outbreak was flat. That's this blue curve at the bottom. Slightly increasing for the 40th percentile, more rapidly increasing for the 60th percentile, and very rapidly increasing for the 80th percentile. The extremes are becoming extremely more extreme. Now, let's turn to prime numbers. numbers. Why prime numbers? Integer sequences are model systems for studying Taylor's law because there's no measurement error and there's no sampling error and every mechanism at work is specified by the mathematics of the integer sequence. So there's no fooling around with quality of data. We know what the data are and we know how the And we know how the sequence works. So let's talk about primes. Let's set x equal to 10. There are four primes that don't exceed x, 2, 3, 5, and 7. The number of primes that don't exceed x is 4. So the mean of those 4 primes is 2 plus 3 plus 5 plus 7 divided by 4. That's the mean. And the variance is calculated in the same way. And the variance is calculated in the same way. Okay? Simple, no funny business. Top left graph. X goes from 10 to 200 million. And we count how many primes, that's pi of x, there are less than each of those x's. Well, in total, there are 11 million primes. We calculate the mean of all the primes less than each. Of all the primes less than x in the top right. We calculate the variance of all the primes less than x in the bottom left. We plot the variance as a function of the mean on log-log coordinates for all those primes. Looks pretty straight. I superimposed on that a plot of v, the vertical axis, is one-third times the mean squared. And what I claim, and And what I claim and will prove later is that the variance is asymptotic to one-third the square of the mean of the primes less than x. How do you get that? It comes from the prime number theorem. If you have two real-valued functions, we use x, f, wiggle, g if both of them go to infinity and the ratio goes to one. That's the standard definition of asymptotic. Asymptotic. In 1896, a century after Friedrich Gauss conjectured the prime number theorem, Hadermard and Delavale-Poussin proved it, 1896. Let pi of x be the number of primes less than or equal to x. Then x is asymptotically equal to the logarithmic integral, the integral from 2 to x of 1 over log t. And this is asymptotic to x over log t. This is asymptotic to x over log x as x goes to infinity. That's the prime number theorem. The prime number theorem implies that the variance and the mean of the primes less than x asymptotically obey that quantity. Why? I'm not going to tell you now, but I'll tell you in a little while. Twin primes. A natural number p is defined to be a twin prime. If p is a prime, prime if p is a prime and p plus 2 is a prime. For example, 3 is a twin prime because 3 plus 5 is a prime. 5 is a twin prime because 5 plus 7 is a prime. 7 is not a twin prime because 9 is not a prime. No one knows whether the number of twin primes is finite or infinite. If you want immortality, answer that question. The mean twin prime less than 10 is 3 plus 5 divided by 2. It's 4, and that's the variance. It's 1. It's one. So I did the same plot for the 813,000 twin primes, less than 200 million. Here's the number of them. Here's the mean of them. Here's the variance of them. And lo and behold, they obey the same function. The variance is asymptotically one-third the mean squared. Why is that? Well, if you believe the Hardy-Littlewood twin prime conjecture of 19... Twin prime conjecture of 1923, which is stated here, then the variance and the mean of the twin primes must obey this Taylor's law, which is the same as for the primes. There's a generalization of the twin prime conjecture called the Bateman-Horn conjecture on admissible prime configurations. I'm only going to give examples here because it gets hairy. 5, 7, and 11 is a configuration, admissible prime configuration. 11, 13, and 17. P, P plus 2, P plus 6 is a three component prime configuration. And what I proved is that the mean and variance of these integers obey, once again. Once again, the variance is equal asymptotic to one-third the mean squared. Something is going on underneath here. So you need to know what slowly varying and regularly varying functions are. A function is slowly varying at infinity if the function at ax for any positive a divided by the function of x is equal to one asymptotically. one asymptotically. Example, let L of X be the log of X. So the log of AX is the log of A plus the log of X. If you divide that by the log of X, log of A over log of X goes to 0. So log of X over log of X goes to 1. Or you could do the reciprocal. It's equally true. A regularly varying function says instead of this ratio, Instead of this ratio being one, it's a power of a for some real index of regular variation rho. Every regularly varying function is a power of x times some slowly varying function l. This is Karamata's theory, 1934. For example, if f of x is x squared, rho is equal to 2. That's this exponent. That's this exponent. For the prime number counting function, and I'm giving away the story right now. Prime number counting function is x over log x. The log x in the denominator is a slowly varying function. So this has rho equal to 1 because x has rho equal to 1. x is, I mean, x is x to the 1 power. So any increasing, here's my general theorem. Here's my general theorem. I'm very proud of this. Any increasing integer sequence with regularly varying asymptotic counting function n tilde of index rho obeys Taylor's law with variance equal to one over rho times rho plus three times the mean squared. Example, if this is the prime number theorem, it's regularly varying with index one. So put one. X1. So put one in here for rho. That's one. One plus two is three. So it's one over three. That's where this one-third comes from. Okay, so this theorem explains everything you've seen before. Here are 10 examples of integer sequences covered by this theorem. The primes, the twin primes, prime constellations, integers to a power from... Integers to a power, primes equal to a certain number, modulo, some other relatively prime number, polynomial primes, perfect powers, triangular numbers, constant probability sequences, and Pareto probability sequences. All of this is in my paper, Integers, which is just published last year. But it ain't always true. Some other integer sequences extend Taylor's law, and that's why. Law. And that's why I'm doing this exercise to find out what are other conditions that require extensions of Taylor's law. Variance functions of asymptotically exponentially increasing integer sequences go beyond Taylor's law, Journal of Integer Sequences. For example, Fibonacci numbers. Well, Fibonacci numbers obey this extension of Taylor's law, the variance function. The variance function is this thing times the mean squared. What's this thing? It's a slowly varying function of the nth element of the sequence. This is just a constant, which I give here. So this whole quantity is a slowly varying function. That's an extension of Taylor's law. Turns out that the Lucas sequence has exactly the same property and the Catalan sequence. And the Catalan sequence has the same property. And the difference is the ratio of two successive Fibonacci numbers converges to the golden section phi. The ratio of two consecutive Lucas numbers converges to the golden ratio phi, 1.618. The ratio of two consecutive Catalan numbers converges to 4. So, the sequence is growing exponentially fast. It's getting thinner and thinner on the upper part of the line. By contrast, for the primes, the ratio of successive primes converges to one. And that means they're more uniformly distributed. And that explains why you get a different form of Taylor's law. So, we're learning something about the limits of Taylor's law. Now, Noah, I think I've been talking for 20 minutes. How much time do I have, please? Do I have another five minutes? Yes, you have five more minutes. Yeah. Thank you. Okay. So I want to turn to another example now. Taylor's law assumes that you have finite mean invariance. What happens, at least the population laws, what happens if the mean is... What happens if the mean is infinite or the variance is infinite? And I've had the privilege of collaborating with these people from whom I've learned a lot. Mark Brown and Victor de la Peña at Columbia, Chuen Fa Tang and Sheng Chi Yam in Hong Kong and Australia, and Richard Davis at Columbia and Gennady Samaradnitsky at Cornell. And we published four papers. And we published four papers on the sample Taylor's law for stable laws and heavy-tailed laws. Starting in the Journal of Applied Probability in 2017, what happened was I gave a talk at Columbia in 2014 on Taylor's law, and I said, What happens if you don't have a finite mean invariance? Two years later, Mark Brown. Two years later, Mark Brown and Victor de La Peña came back and said, We think we know the answer to your question. And that's where this paper started. And then it went further. So this is a beautiful definition, which goes back to Paul Levy in 1924. A non-negative random variable is strictly stable with index. strictly stable with index alpha if for IID copies of X and X1 up to Xn, the sum of the first n terms, why am I pointing, you can't see my finger, the sum of the first n terms has the distribution, is equal in distribution to n raised to the one over alpha power times the same distribution. And we write that. And we write that x is equal in distribution to a stable law with index alpha, or x is alpha stable. Now, we're going to do some higher math. Set alpha between 0 and 1. Then 1 over alpha is greater than 1. So 1 over alpha minus 1 is positive. So the sample mean is this quantity divided by n. Quantity divided by n shown here. So we have to divide this quantity by n. It's n to the 1 over a minus 1. We just proved that this exponent is positive. So as n goes to infinity, the sample mean goes to infinity with probability one. Okay? That's the defining feature of these stable laws. And if that's true, then every one of these moments greater than alpha is infinite. Two examples, the Levy distribution, normal distribution. Well, I'm sorry, I don't have time to show you a lot of beautiful results, but I want to show you what a simulation looks like. Here, I simulated. Okay, if you want to simulate Levy's law, you take a normal Law. You take a normal random variable, you square it, and then you take the reciprocal. That's the Levy law. One over the normal squared gets you the Levy law. Very easy. So I simulated a sample of size 10 and looked at the logarithm of the mean. It's here. Then a sample of size 100. It was here. Then a sample of size 1,000, 10,000, 100,000, million, 10,000. thousand million ten million hundred million one billion ten to the ninth observations they fall along a straight line this line is not fitted we have theory that tells us the slope is given by this formula this is taylor's index b in relation to the index of the stable law 2 minus alpha 1 minus alpha so look if alpha is equal to a half as in these simulations As in these simulations, two minus a half is three, three halves, and one minus a half is one half. So three halves divided by one half is three. That's why the slope here is one, is three. And we have some beautiful theorems, and we generalized it to not only independent data, but correlated data and to heterogeneous data, beautiful theorems. And then we applied it to COVID. What does this have to do? What does this have to do with COVID? Well, I'm only going to show you the cases. The same results are true for deaths. If you take each state in the United States, it has on the average 50 counties. There are about 3,000 counties. On the average, 50 counties. For each state, calculate the mean and the variance of the number of COVID cases on April 1, 2020. April 1, 2020, May 1, 2020, etc., for the next 15 months. Do that for each of the 50 states. They fall, lo and behold, right on a straight line. That's Taylor's law. And the exponent B is not significantly different from 2. Now, if you try to model the data, you can fit a Weibel distribution, it drops off too fast. You can fit a log-normal distribution, the black dots are the data. 99% of the distribution is covered by the log-normal, but not the top 1%. We zoom into the counties with the highest 1%. These are Highest 1%. These are just the highest 1% on these dates, the 15 months. And we fit a straight line. And lo and behold, the slope is always between 1 and 2, minus 1 and minus 2. And that tells us that the variance is infinite while the mean is finite. The upper tail doesn't drop off fast enough. Enough. And we have theorems that show that sampling from regularly varying random variables explain both why Taylor's law holds, that's the bottom here, and why the slope is two. Beautiful theory. So what? If the variances of cases and deaths per county are infinite, Per county are infinite, then facility and resource planning should prepare for unboundedly high counts. No single county, state, or other jurisdiction, including country, can prepare for unboundedly high counts. Cooperative exchanges of support should be planned cooperatively. I ran over, but I apologize for that, and thank you for the opportunity.