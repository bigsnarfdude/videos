And so in this area of non-linear roles, it has contributed a lot to the study of these laws with measures and considering the structure given by gradient. And then they also did a lot of work on Did a lot of work on closing metric spaces. So it's my pleasure now to have you here. See you then, Prison. Thank you very much for the invitation, for the presentation. What I want to present today is some research in collaboration with Giulia Cabanari, who is now a researcher in Politecnico of Milano, and Giacomo Sudini, who is doing a postdoc. Who is doing a postdoc in Vienna? I want to extend the theory of gradient flows to more general evolution of probability measures. So I will discuss a bit what are the main objects driving this evolution. So probability, which we call probability vector fields. Then I will discuss a bit how to code in these probability vector fields the notion of dissipativity, which is the Which is the counterpart of geodesic convexity for functionals. Then I will present a particular strong condition of dissipativity, which allows for Lagrangian parameterization of the evolution. And if there will be enough time, I will also discuss a functional analytic tool that was extremely useful for this. Extremely useful for this theory. So let me start with probability vector fields. We will work in the space of P2 of H of probability measure, usual condition on the quadratic moment. I will denote with this usual sharp symbol the push forward of a measure through a map X. I will often use this reference probability space omega, but in general this notion is well defined. This notion is well defined for every measure. And given two probability measures, I will denote by gamma the set of all couplings, so probability measure in the product of the spaces whose marginals are mu and nu, respectively. And usually, if mu is the law of a variable x and mu is the law of a random variable y, clearly the joint law of x and y. The joint law of X and Y will provide a coupling between the two measures. And the Vassey distance is this L2 vassel stain distance is just the minimum of the L2 distance among all the couplings between mu and mu that you can also write in this way. And gamma optimal is just the set of all the optimal couplings connecting to measures. And the main example are And the main example are discrete measures, where now μ is just a convex combination of Dirac masses in point xn. And then you may use a discrete set omega containing just n parameters. The random variable x here is just a map assigning to the n element of omega xn and p. Xn and P is just a uniform measure on omega, just to have a parameterization of your discrete measure. Or absolutely, in the let's say, second extreme example is the case of absolutely continuous measure in Rd with respect to the Lebesgue measure. Just to have an idea of couplings, if you have never seen a vast distance before. Vases time distance before is you can follow all my discussion thinking to discrete measures. So, discrete measures are a way to model a bunch of n capital N points where the order of the points is not relevant. So in um so you you you can represent those points by mu and the point YN by mu. And the point Yn by ν. And the vastest time distance by Birkhoff theorem can also be represented as the minimum of this little L2 distance between the two vectors with respect to all the possible permutation of the indices in one of the two entries of the vectors. You see here, here you have in black mu, in blue μ, here is a possible period. μ here is a possible permutation of the points. The optimal capping is when you obtain a connection between the two sets of points, which minimizes this expression here. So you may think that couplings in this case are just permutation of the orders of the labels in order to get a minimum here. What is so, as I mentioned at the beginning, So, as I mentioned at the beginning, my point is to study the evolution of probability measure driven by some notion of vector field. This is this strange F means probability vector fields. What does it mean? We have at least two cases. One case is what I call the deterministic case. So, you may think that you have finite, just to think to a finite measure, mu. What is Measure μ. What is a probability vector field is a map that takes the entry as μ and at every point of the support of μ induces a vector which depends on the point x1, x2, x3 here. So you have x1 and of course it also reflects the distribution of your points. So at every point you have So at every point you have a vector. You have a vector field in the support of the measure, which depends on the measure and on the point x. Clearly, the crucial point is that f does not depend on the order of the points. So if you make a permutation of the points here, the vector field F is invalid here. So this is the determination. So, this is the deterministic case. What does it mean? This evolution means that you are following the continuity equation. So, in the distributional sense here, so you are looking for an evolution of measure driven by a velocity v, and this velocity v at every point, almost every point of your measure, is given by this vector field F, which of course evolves since μ t is. Since ν is evolved. Or there is the more complicated situation where the probability vector field is not deterministic. This means that at every point you don't have just one vector field, but you may think that you have a sort of probability in the space of velocities. This probability depends on the measure μ. Depends on the measure μ and on the point x. Of course, you have to assume some measurability of this field, and you just know that you don't know a deterministic velocity at every point, but a probability distribution in the space of velocities. For every point x, perhaps here you have just two Dirac masses in the space of velocities. In the space of velocities. Here, perhaps, you have three possibilities. Here, maybe at the point x1, you have a distribution, diffuse distribution of velocities. Okay, so this is the non-deterministic case. For every measure, at every point on the support of the measure, you have a distributability distribution in the space of velocities. Okay, so I will try to first focus. So I will try to first focus on the deterministic case and then try to show how the situation can be adapted to the non-deterministic case. In the deterministic case, things are simpler since at every point you have just one Dirac mass concentrated in the precise value of the velocity given by this vector field. And the situation is even more complicated. Situation is even more complicated since, as in the Bertian case, it could be useful to think to multivaluate probability vector fields. So, for every mu, you may have many vector fields that your trajectory can follow. But I will discuss this later on. Okay, so what are examples of deterministic vector fields? Perhaps the simpler example is given by a vector field which arises from the sum of a drift q. From the sum of drift, Q drift, and an interaction part term, which is a convolution with μ, A and B are suitable maps defined in your Hilbert space. So H is an Hilbert space, but you can consider a finite dimensional Euclidean space. The theory is interesting also in the finite dimensional situation. And what is a typical natural assumption that one can start? Assumption that one can start with, a Lipschitz-like assumption. So your vector field depends on X and on the measure. You assume that it is Lipschitz with respect to X and also Lipschitz with respect to the measure in terms of the vast stain distance. This is the typical assumption that you, for instance, find in this paper by Bonnet and Frankovska. They are developing a theory of this evolution in a more general situation, but just to give you an example. Situation, but just to give you an example. A more general condition, you see here that this is a point-wise Lipschitz. Given μ and μ, you have a Lipschitz dependence with respect to X and Y. You may move a bit in the direction of optimal transport. You can measure the dependence between X and Y, not pointwise, by Ninel 2, but of course, you have to first select a dependence. First, select a dependence between a point in the support of and a point in the support of μ. This dependence is given by a coupling. And since we are in the optimal transport setting, it's quite natural to select an optimal coupling. So you may say, according to a paper by Picoli, that F is a Leapshift probability vector field if there is for every new one, you For every μ and μ, there exists an optimal coupling such that the L2 norm of the field with respect to this coupling is controlled by the vast estate distance between the measure. So you pass from an L infinity to L2, but clearly, since you are working with respect to different measures, you need a coupling between them. So if you think to the previous example, you have three points here, and your vector field. Here and your vector field, and now you have another measure, perhaps here, this is new, and a vector field. Then you have first to select what is the optimal coupling, say perhaps this one. Okay, and then whenever you have connected the points, you use this coupling to measure this integral, which is a finite sum in this case. Other example, we were motivated by the notion of gradient flows in the space of probability measures. Those are driven by the so-called Wasserstein sub-differential of a given functional. And assuming that this functional f is geodesically convex, for instance, you may think to an entropy functional where this entropy phi, Where this entropy phi, which depends on the density of mu with respect to the Lebesgue measure, satisfies the McCann condition, then this f of x mu is, at least formally, you have to justify better what I am saying, but it is minus the gradient of the derivative of phi with respect to rho divided by rho, sorry, here. And it satisfies this. It satisfies this, let's say, above the tangent property of sub-differential. So, f of xμ satisfies this property. So, if you integrate this quantity along an optimal coupling connecting mu and mu, you always satisfy this convexity inequality. Okay, those are natural probability vector fields generated by convex. By convex functionals. More interesting situations are given by the case of when, for instance, if you start from the logarithmic entropy and the gradient is the gradient, the vector field is minus the gradient of the logarithm of rho. And now you may modify it by using a fixed symmetric positive definite semi definite matrix. Semi-definite metrics. This is another vector field which is not a gradient flow, but could be interesting for many applications. And as we will see, this is monotone, so dissipative, but is not the gradient of a convex function. Another example is taking the same gradient of the logarithm and multiply it by a function a smoothing smooth sufficiently regular depending on x. Okay, notice that in the Notice that in the previous examples, if A and B are Lipschitz maps, then this vector field is a Lipschitz function with respect to the vastest and distance. In this example, the domain of f is not the whole space of probability measures, since you have to restrict your gradient to sufficiently regular densities. And moreover, the map from μ to F. The mark from μ to F is not continuous with respect to the vast stain distance. This justifies the interest of working with the dissipative, trying to avoid continuity. But still, the continuous case is interesting. So, if we want to move to the non-deterministic case, the natural perspective, as I mentioned, is to assign to every point a distribution in the Every point, a distribution in the space of velocity. What is the natural object to represent this family of probabilities? You integrate with respect to mu and then you obtain a probability measure, which is now take values in the tangent space of your domain. So this tangent space is just H, since we are in an inverse situation, it's just H Cartesian product, Cartesian product. Cartesian prod Cartesian product with H, but it is useful to think that the first coordinate is the position, the second coordinate is the velocity. So, in this way, you obtain a probability measure, which is represented in this case, which is a measure in the tangent space, in space and velocity. Okay, so this is the natural object. If you give a probability measure in the tangent space, In the tangent space, you can disintegrate this measure with respect to the first marginal, and you obtain a family of probability measures just in the velocity space. So there is a sort of one-to-one correspondence between probability vector fields, which are interpreted as Borel probability measure depending on x and probability measure in the tangent space. And if you adopt a sort of probabilistic viewpoint, Sort of probabilistic viewpoint, parametrizing your measure mu by a random variable. You can also use another random variable to parametrize the set of the velocities. So a probability vector field is just the image of a pair of random variables, one for the space, one of the velocity from a given probability space. And since you are considering general probability. General probability vector fields, you may consider that f is a map from measure in your space to measure in the tangent space, such that for every mu, the first marginal of f is mu. And even more generally, you can neglect a bit the dependence of mu and think that a probability vector field is just a subset of the set of probability mesh. The set of probability measures in the tangent space. This corresponds to the natural identification of multivalued maps with their graph. So you may think that a probability vector field is just a collection of measures in the tangent space of your ambient space. Examples. Perhaps one interesting example is given by what I call the The what I call the stochastic superposition. So, suppose that you are assigning K capital K deterministic field, for instance, these two. I can interpret this deterministic vector field by taking the mean value of the maps, but I can also interpret them by considering the corresponding measures. So, for every So, for every μ, I consider the uniform measure distributed on these capital K points. So, it's exactly this situation. Every point X has two possibilities, as two direct masses at these directions. So, this is the probability vector field. And this is quite, if you think to the case of stochastic gradient descent, where you Gradient descent where you want to minimize a given function capital phi and your phi is the sum of the arithmetic mean of capital K functions just depending on x, then every function has a minus gradient. Clearly, you may take the arithmetic mean of the gradient and then you obtain a deterministic field, but you may consider But you may consider the non-deterministic field, which is distributed according to these Dirac masses concentrated in each direction. And so you have a non-deterministic probability distribution. So in practice, at every point, you have k contribution, and you may uniformly choose one of this k contribution as a direction. This is an example of a non-deterministic field which doesn't. Non-deterministic field, which does not depend directly on the measure. There is no interaction here. But you can also consider more general situations. And another example, which is my initial, our initial first motivation, is the case where your function is not smooth. For instance, consider this situation where you have the absolute value of x, then clearly this differential. Clearly, this differential of the absolute value, the opposite of the differential, is minus the sigma function. And clearly, at zero, you see that you have at the level of maps, you have a set, the set of minus one, one. If you think now that you have measures supported in R, for instance, you have In R, for instance, you have a measure here, if I'm not wrong, is a Dirac mass in minus two, a Dirac mass in zero, and a Dirac mass in one. What is the natural probability vector field induced by this minus signal function? Here, clearly, you have to select just one. Here, you have to select just minus one, but in zero, you have the possibility to select. You have the possibility to select any measure alpha whose support is in minus one, one. And so there is no reason why you have to select a deterministic field here. Zero can go in an arbitrary direction parameterized by alpha. So whenever you have some non-smoothness in your function, it is quite natural that at those points you have a distribution in the space of the velocity. Another important case is given by a confined. Case is given by a confinement potential where you have a convex closed set in H, and your functional V is just zero if you are in the convex set plus infinity of side. Clearly, when you reach the boundary, you have all the direction pointing inwards. This is multivariate, and then you have a distribution in the space of velocities, not just a single one. Just a single one. Okay, so what is how can we introduce the notion of dissipativity for those kind of vector fields? We can use as a model problem the case of Hilbert spaces. So, what does it mean that a given operator is dissipative? Dissipative means that if you solve these differential equations, Differential equation, the trajectories are stable, so you have a contraction in the space of evolution. And you write this condition by saying that the scalar product of the difference of two vectors v in vx, w in vy with x minus y is non-positive. And more generally, it is useful to include the lambda dissipativity in order to include also the example of Lipschitz maps. It's maps, then you will assume that this color product is bounded by lambda, the square of the distance between x and y. So, lambda is just a parameter. There is a sort of meta theorem that whatever you are able to prove for dissipative can be extended to lambda dissipative. Just to, of course, taking into account the constants. So, what is the metric interpretation? The idea is that you start from two points x and y, you pick up Points X and Y, you pick up two directions V in VX, W in Vy, you move those points in these directions, you compute the square distance and you compute the derivative of the square distance at zero. Since you want dissipativity, this direction should be initially decreasing the distance. So you compute the derivative, you get this scalar product, and you impose that this is non-positive. This is the meaning of. Positive. This is the meaning of dissipativity. Now you have three ingredients that you want to import to the measure setting. How to compute the derivative of a curve from a given point. How to compute the derivative of the distance between two curves. And how to do the explicit Euler scheme. Here is just moving. Here is just moving one step forward in the direction of B, or if you like, how to compute the implicit Euler scheme if you want to solve this equation for singular vectors. Whenever you have the explicit or the implicit Euler scheme, then you may perform the discrete evolution starting from a given point. You connect, you obtain all these points or with the explicit. Points, or with the explicit, or with the implicit scheme, you consider the piecewise linear constant interplant, and you look for convergence of this family depending on tau, and tau goes to zero. And in this way, you hope to construct evolution. When B is the gradient of some functional, this is the implicit order scheme, is what is called minimizing movement or JKO scheme for the. Or JK Yoskin for that function. Here we don't have the potential, so one of the difficult points is how to implement, to construct an implicit Euler scheme. The explicit is quite easy. You move in the direction of B. Okay, so let us now go back to the case of measures. What is one step of the explicit Euler scheme? explicit Euler scheme. It is easy, you have for every, you fix your move, you have this vector field, you take the identity, you move the identity in the direction of your vector field, and you take the push forward of your measure. So if you have a discrete measure, I always use three points, you have these three vectors, suppose that this is exactly of size tau and this is Tau, and this is your measure, this initial point. After one step, you will have a measure concentrated in these three points. Sorry? This is like the explicit looks like an explicit method. Yeah, this is explicit. If you have a non-deterministic case like this one, the argument is. The argument is exactly the same, but now, after one step, you may start from a discrete measure, but after one step, you will end up perhaps with a measure which has a larger support, perhaps also diffuse support. But the idea is the same. You obtain, for instance, perhaps the simpler way is to introduce a parameterization of your vector field and you move your measure along. You move your measure along this vector field. You see here the example. Okay, so as I mentioned, we have to compute these three ingredients. The first one is how to compute the derivative of the distance along a curve. You remember that the derivative is just the scalar product of the velocity and the direction of two points. Here the situation is slightly more complicated. Slightly more complicated since now this scalar product involves a coupling between mu and mu. So you have mu, mu is fixed, you move mu with this one step of explicit. Clearly you have this color product, but you have some flexibility. How do I connect mu with new? I have to introduce optimal coupling since there are Coupling since there are maybe there are many optimal couplings, the right derivative of the square of a sustained distance selects the coupling which produces the minimal scalar product. I will denote this by this r. This is the right derivative of your distance squared. If you take the left derivative, so you take the derivative from tau equals zero from the left, you obtain the maximum scalar product. So in general, these two values. In general, these two values are different, and you have to select one of them, taking into account that the vastest and distance square along linear move direction like this one is a semi-concave and not convex like in the Hilbertian case. So, semi-concavity is one of the more severe, say, technical difficulty to develop the theory. In any case, we have two possibilities. Two possibilities, and we select the first one, the right derivative, since we are looking for the explicit Euler scheme, and the right derivative also produces the weaker notion of dissipativity. When you want to move in two directions, you just use the splitting formula. You take y fixed the moving x and then the moving y. So we have computed. So, we have computed the previous derivative. So, we say that a probability vector field is metrically lambda dissipative if the derivative of the distance moving along the vector field starting from mu to nu plus the corresponding one obtains which the role of mu and nu are below lambda distance squared. So, less than zero if you are just looking for. Less than zero if you are just looking for dissipativity. Okay, this is the natural metric notion, the weakest one, if you want to consider dissipative vector fields in the space of measures. And what is interesting is that is quite well adapted to the Gradient flow space. If you have a convex functional, then a probability vector field is in the element of the sub-differential. Of the sub-differential, if and only if it satisfies this inequality. Remember that this is a sort of surrogate of this color product. Remember, just this formula. So, this formula here is this color product when you minimize with respect to all the possible optimal connection between u and u. So, you have this classical scalar product, the minimum, less than the increment of f of f. And it is interesting that f is geodesically. F is geodesically convex if and only if the probability vector field is dissipative. So the theory in this way is consistent with the notion of convexity. Okay, but of course you have many other possibilities. This is the metric one, the weakest and there is the strongest. I want to go more quickly now. The strongest is when you impose this Impose this condition of dissipativity not only for one selected coupling optimal between mu and nu, but you impose it along all the couplings. What is was, and clearly the weakest, the strongest implies the weakest, and there are intermediate ones that I'm skipping now. What is interesting is that if you have dissipativity and your vector field is And your vector field satisfies a weak notion of continuity, then all these conditions are equivalent. So you gain much more information if the vector is continuous. And then it's natural to see what happens using the strongest definition instead of the weakest one. Okay, so the strongest definition is dissipativity along arbitrary couplings connecting mu and u. And I will And I will continuity is just weak continuity from maps from P2 of H to P2 of the tangent space. Okay, what can you prove in this case? First, that the implicit Euler scheme is solvable, that the map sending to the equivalent of the JKO scheme, you have a uniform convergence of this scheme to a limit measure, you have an explicit. Limit measure, you have an explicit error estimate which reproduces the Krandal Ligate estimate at the level of measures of order square root of tau. You can define a semigroup and the semi-group is expansive with a constant exponential of lambda t and u t is the unique solution of the continuity equation, okay, with this given vector field. So the theory for deterministic continuous and dissipated Continuous and dissipative vector field is quite simple, and one wants to see what happens in the case of non-deterministic. The situation of non-deterministic is more complicated by this reason. When you have a deterministic vector field, you have many ways to couple mu and to represent f. Since also f now is determined, is non-determined. F now is non-deterministic, may be represented by a pair of random variables. So you have three random variables: the random variable X parametrizing mu, the random variable Y parametrizing mu, and the random variable V parametrizing the possible direction of your vector fields. So, if you compute the derivative of the bus stain distance, you have a formula like the previous one, where you have to minimize. The previous one, where you have to minimize with respect to three random variables, so a three uh a trip line, say, connecting f and nu with the extra important property that the marginal with respect to x and y has to be optimal. Okay, and again, you have if you this is the feature, you have a random variable for mu, random variable for mu, this connection should be optimal, then you have a random. Should be optimal. Then you have a random variable of parameter in the direction, and you have to select all these three plants in order to minimize this color problem. This is the formula for the derivative. Again, it's an expression of the semi-concavity of the distance. And unfortunately, even for continuous probability vector fields, now if you have a The uh, if you have a dissipativity, there is no guarantee that you can solve the implicit Euler scheme. Since, for instance, your probability vector field diffuses a measure. So, if you want to find a measure such that the application of the vector field gives you a direct mass, this is impossible. So, the implicit Euler scheme cannot be solved. So, again, in this case, we focus. Again, in this case, we focused on a stronger notion, which we call the total dissipativity. As before, this condition is much stronger, implies that you have the dissipativity inequality not just along one coupling, but with respect to all the possible couplings. This is a much stronger assumption. This is realistic? Yeah, say that I will comment. Say that I will comment. For instance, in the case of continuous deterministic field, this is true. In the case of perhaps one of the examples are the case of functional, which are totally convex, so are convex along arbitrary interpolations, not just optimal. I will comment in a moment. Okay, what is the advantage here? What is the advantage here? That in this case, you can study, you can have a sort of Lagrangian parametrization. So it means that you fix a reference probability space and you consider the operator in this L2, now is an inverted space. Say that Xv belongs to B if and only if. belongs to B if and only if the law of Xv is in your probability vector field. So you obtain an operator which is dissipative in an Hilbert space, okay, in this way. And it is interesting that f, the probability vector field is totally lambda dissipative in the space of measure if and only if the Lagrangian parametrization is lambda dissipative in the Hilbert space. And then you can apply In the Hilbert space, and then you can apply all the machinery for dissipative vector dissipative operator in Hilbert spaces, not just in measure spaces. And this is what I mentioned, and I will conclude with the result. Perhaps an interesting example is given by functions which are convex along arbitrary couplings, not only geodesical ones. And in this case, the This case, the total sub-differential of this F is maximal dissipative in this sense. And there is this interesting fact showing a sort of dichotomy of the theory. So if your function f is geodesically convex, is everywhere finite and it is continuous, then for free is totally convex. So So, the idea is that you have a functional like as the potential energy, the interaction energy, which are totally convex, functional, which are as the entropies, which are not totally convex. In this case, those functional cannot be approximated by geodesically convex and continuous. So, whenever you have continuity and geodesic convex. And geodesic convexity, you gain total convexity. This is, and say that the idea is that whenever your evolution is given by a discrete set of measure, this functional, this probability vector field is totally dissipative, otherwise, no. And I will conclude just with this theorem of Theorem of which let's say contains more or less what we have seen in the deterministic case for an arbitrary totally dissipative maximal probability vector field to have the same generation result. Okay, stopping here. Thank you very much. Are there any questions? So that's not quite sure there's an example of this one minus one gradient adjustments value. But you mentioned very quickly this is sort of a moderating example, right? But this is in fact a gradient, right? So yeah, this is a gradient. Okay, so you may consider, for instance, this gradient plus non- Non-gradient field. This remains monotone or dissipative. It is still diffuse. So, but yeah, can we say in some sense that this is sort of an extension of investment, the monotonical theory of strength? Yeah, yeah, yeah. So I'm using dissipative since I don't put the minus, but it's a part of my. But a part of minus is exactly the message is if you have say that if your probability vector field is sufficiently regular, say, is dissipative, sufficiently regular, it is totally dissipative. And you can construct a Lagrangian theory, Lagrangian parametrization, use all the tools of Hilbertian saying. If it is more singular, then It is more singular, then you have to modify a lot of the argument. This is the next step of the future. It seems to me that this procedure is like for every point you have an independent choice of sensor, given by the measure of the target of the point. Yeah. So could it be natural to the zero to be not independent of if there are some potential? No, no. If you let's say that is independent but given the point X, so I can consider an interaction. For instance, you may consider Consider suppose that you have this situation where now B is not a map but is multivalued. So you replace here some elements psi here, where psi is the V of X minus Y. Then, in this way, you generate non-deterministic vector fields, which is obtained by an interaction and still is in this framework. Psi depends on the x. Sorry, it's this one. Psi depends. X is given. U for every psi of psi is a selection of B of is a selection of B of X minus Y. B is a multivariate map. Now psi depends on Y and you integrate with respect to B. So you have different contexts, but clearly, yes. This is what maybe this would be nice to have like I choose once and for all selection of side, but then this side is Society is the same around the side. You select, so this is side X of Y say. Okay. And then you may also consider measure concentrating this point. I don't know if this is would you want something more even more general. Either more general, could be continued later to another time. I have another question that worries me. I mean, if you go to this maximal monotone operator by vertices and follow the evolution by sub-differentials, then you get multivalued maps. The sub-differential can be multivalued, but the evolution chooses the solution, chooses the choice. You don't need to. Choice: You don't need to consider the multi-value thing because the evolution will go along the property. Exactly, exactly. Exactly, this one. This one. In fact, the evolution selects the element of minimal norm in this distribution. This element is a map and not a distributed field. You need the diffuse case exactly to solve. Case exactly to solve the one step of the implicit method. Since perhaps your vector field, the situation is that perhaps you have a diffuse measure and after one step you end up with in a Dirac mass. And you want to connect backward the Dirac mass with the diffuse measure. And if you see your vector field from the Dirac mass, you have a diffused vector probability vector field. But then the evolution selects the minimal element. That is quite in the end. By trying to solve this game, you solve the solution. I have to think. 