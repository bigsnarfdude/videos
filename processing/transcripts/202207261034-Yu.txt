Yeah, today I'm going to talk about the communication complexity of nfold XOR function base. So first, what is an n-fold XOR function? So let f be some function that takes one input z and output one bit 0, 1. And its n-fold XOR is a function that takes n input Z1, Z2 up to Zn and outputs the XOR of F evaluated on the Z on the N inputs. on the z and the n inputs f of z1 x or f of z2 up to f of z n so this is a function that takes n times more input but still output one bits one bit so for any function z we can define its n fold x or in this way and the main theme of this talk the main topic of this talk is try to understand what is the relation between the communication complexity of a function f and the communication complexity of its n fold x or f to the n. So I will come back to the exact setting and the theorem that we will prove in a few slides. But first, what is the motivation to study this ample XOR? So why do we care about it? So it turns out that this is a very effective way to amplify the hardness of a function. So what do I mean? Why do I say this? So suppose in some computational model, let's say we have some function f and it can be computed using resource C with probability two-thirds. With probability two-thirds, it could be communication or computation or query complexity. And then, suppose we want to compute its n for x opt to the n. What is the naive way to do this? So one naive way to do this is just to compute the n copies independently, and then we get n output bits, and some of them are wrong, some of them are correct, and then we just output the XOR of these n bits. So, this naive algorithm obviously uses at most n-time series. Is at most n times C resource in total. And it turns out that we can show the success probability is slightly better than the non-trivial, but slightly better than the trivial guessing probability. It's half plus something exponentially small in n. It might not be obvious at the first glance, but it's also not very hard to show. Okay, so suppose for some now suppose for some function f, we can actually prove that this is the best possible. So we cannot do much better than this. So, we cannot do much better than this. Then we have amplified the hardness of this Boolean value function f. So, beginning with some function that is moderately hard, so it's hard to compute with constant probability, with probability two-thirds, then we get a function, we obtain a function that is very hard. It's even hard to compute with probability better than random guessing by some very tiny amount, something exponentially small. So, this naive of the So, this naive algorithm might not sound very good, especially in terms of the success probability, but it turns out that in some of the settings, this is actually true. This is actually the best possible. And such a statement is usually called a strong XO lemma. So for some model of computation and for some class of functions, the strong XOR lemma basically states that f to the n cannot be completely much better than. Much better than solving all instances independently. So, basically, it means that either you have to pay n times more resource, or the advantage of random guessing is exponentially small in n. So people have proved such XOR lemmas or sometimes slightly weaker versions of it in different models of computation. So, for example, XOR lemmas, strong XOR lemmas holds for the query complexity, and people have proved. And people have proved XOLMS for certain complexity and streaming algorithms without n times more resource. So, giving the same amount of resource, or even sometimes less, than what is needed to compute one copy prove that you cannot do better than random guessing to compute F to the N by an advantage of exponentially small n. And exolemas have been proved for information complexity without the exponentially small advantage. Without the exponentially small advantage. So basically, with less than n times more resource than computing what you need to compute f, you cannot compute f to the n with the same problem. And it has been proved for specific special classes functions, for example, in communication complexity and for functions with low discrepancy. Basically, for functions that you can prove its communication lower bound using the discrepancy method by bounding its discrepancy. Its discrepancy, it has been proved that strong XOL lemma holds for such functions in communication complexity. And there are other examples. But for general functions, no strong XO lemmas were known for communication complexity. And this is the main result I'm going to talk about today. We prove a strong XO lemma for a bounded round of communication. So the model of communication is Computation: The model of communication is a standard randomized communication model with R rounds. So we get input pair X and Y, and Alice sees X, Bob sees Y, and there's a random public random bits R that both pairs see, and then they start to talk alternatively in a total of R rounds. And at the end of the communication, the transcript, which is the set of these R messages, must determine the output. And the communication cost is simply the maximum. Communication cost is simply the maximum or the worst case total size of this transcript. And what is the n for the XO function in this model? We just get n input pairs, x1, x2, y1, x1, y1, x2, y2, up to xn, yn. And then we give all the x to Alice, and we give all the y's to Bob, and we have them compute the XOR of the n outputs, right? Outputs, right? So this is clear, yeah. And then we prove a strong epsilon all lemma for the boundary round of communication. And so we fix a number of rounds R. And let's say we use this notation R sub Q of F to denote what's the minimum communication needed to compute this function f with probability Q. So the main theorem we prove here is that We prove here is that for any function, the communication cost, the communication complexity of f to the n for probability half plus two to the minus n is at least n times the n times r to the minus r times the communication complexity of f with constant probability minus one. Okay, so the left-hand side is the communication cost of f to the n with some exponentially small advantage. Exponentially small advantage. And for the right-hand side, it's something that depends on the communication complexity of f with constant probability. And in particular, when the number of rounds is a constant, the right-hand side is simplifies to omega of n times the communication complexity of f with constant property minus of one. So basically, this says that for some function f with communication complexity greater than some given constant, this little Uh, this little one here in R ones, then this shows that the naive algorithm that we saw a few slides ago is the best possible constant factor in the communication. So, to solve to compute after the n with probability half plus to the minus one, you have to pay o omega of n times more communication. Yeah, is two to the minus n really correct? Is it that sharp or? Correct? Is it that sharp or? Yeah, this actually could be anything exponentially small by. Yeah, you can prove the same statement for any to the minus n, except that the constant factor here would be different. By just running it, you get three advantage three to the minus n? Yes. That's pretty sharp, but yeah, but you can get any to the minus n, three minus n, four to the minus n. But then you can't have r two thirds. It has to go to one or oh yeah, that's true. So the two thirds here will depend on this truth and that's right. So it cannot be like too large. Sorry, too small. Too small. So, yeah, and also the minus of one here is needed. Because also in the previous work, it's noted that there's a simple counter example that if you just let a function f to be the XOR of the first bit of your two inputs, then this is a function that can be computed with O. With one bit of communication, but its enfode XO is also a function that is the XOR of a bunch of input bits. And half of the, Alice has half of the bits, and Bob has some other half of the bits, and that can also be computed with one piece of communication. But in this case, we know that the XOL lemma cannot hold. And so in general, And so, in general, we cannot hope to prove XO lamma without this minus O of one term here. Yeah, and also I want to mention that Bremman and Brock and Chen and Rao proved an XO lemma for information complexity, which holds with constant advantage instead of this two to the minus n on the left-hand side. On the left-hand side. And later, Aprilman and Ra also proved that for constant runs, information is essentially equivalent to communication. Then combining the two, it implies that for constant runs, we get an XOR lemma without this 2 to the minus n advantage here. So basically, it says that this left to the end, this n for the XO function takes n times more input. Takes n times more input, but it also requires n times more communication to solve. So it does not amplify the harness in terms of the success probability or advantage. And I want to point out that this XOR lemma for information complexity is actually the starting point of our proof. But our proof is actually also needs to be very different from this lemma because a strong XOR lemma is false for information complexity. We cannot hope to get much. Cannot hope to get a much smaller advantage on the left-hand side than the right-hand side. And this constant advantage that they prove for information complexity is essentially the best possible for the information complexity case. And I will get back to this towards the end of the talk. And I also want to mention that what we directly prove is actually this distributional version of the strong XO lemma, where we have a fixed input distribution. Where we have a fixed input distribution mu. And then we show that, and we measure the success probability over this distribution mu and prove that if every function c-bit communication protocol that computes f under this info distribution has success probability or has advantage at most alpha, then with communication cost n times c over r for this n for the x o function of the product distribution mu to the n, we can. Distribution mu to the n, we cannot have advantage more than alpha to the omega of n. And this basically says that if mu is the hard distribution for f, then mu to n is the hard distribution for this and for the excellent function. And here, we also require this alpha to be sufficiently small and c to be something sufficiently large. And by applying Yaus minimax lemma twice. Twice from and to this distributional setting, and then apply this distributional strong XOLM in the middle, and then doing a few repetitions to boost the success probability of your protocol. This basically proves the main theorem that we saw on the previous slide. So, in the rest of the talk, we're just focus on this distributional version. So, we have a fixed input distribution mu. The distribution Î¼ and we analyze everything on this distribution and its product variance. So, any questions? Yeah, so in the rest of the talk, I will first talk about this exo lemma for information complexity. Actually, I will then present an alternative view of this lemma. View of this lemma, which is which we'll see that is actually useful. And this lemma proves the doesn't give us exponentially small advantage. Then we'll talk about how to obtain this exponentially small advantage. So, first, maybe a quick recap of information complexity. So, we fixed some input distribution mu and we fixed a protocol. Fix a protocol for some problem F, such as defines a joint distribution over everything, over the input pairs X and Y, and the public random bits R, and also the transcripts M. So the information cost of pi is basically a sum of two terms, and the first term is the amount of information that the transfer reveals about Alice input conditional, everything Bob knows. Conditional, everything Bob knows y and r, and the second term is the other direction, the amount of information, and reveals about y conditional, everything Alice knows. And the information complexity of the function on the sum input distribution is the infinite over of the information cost over all protocols that computers have. So, what BRAC, Bremen, and Rao proved is that if there exists a protocol for the This is a protocol for the n-flow XOR after the n under this product distribution mu with information cost smaller than i. Then there's another protocol for f on the single distribution mu with information cost that is a factor of n small up to this additive of one. So for simplicity, let's assume the success probability is one now. So the protocol actually computes f exactly. So giving some big protocol. So, giving some big protocol that computes, that takes n inputs and computes the XOR of the n outputs, we want to get a protocol that computes the function on one pair, but with a factor of n smaller in terms of the information. Okay, so see, so here's how we do it. So, let's fix some protocol for after the end with information cost i, and here is an Information cost I. And here is a new protocol tau for function f. So the idea is that we will embed the input into one random coordinate and then try to sample the rest of the inputs and then generate a big input for this after the end problem and feed it to this protocol tau, a protocol pi. And I hope that because now the input is much smaller, so the information revealed about the actual input is not. The actual input is also much smaller. So, x more formally, what we do is that we sample a random coordinate i and then we embed the input xy into that coordinate by setting xi to x and y i to y. And this is done locally by Alice and Bob themselves. And then we will use randomness to sample the rest of the coordinates. We sample the yellow coordinates. For example, the yellow column is using public randomness. So everything after XI and everything before YI using public randomness. And then we will sample the blue part using private randomness. We will let Alice sample this input condition on y smaller than ice. And then we will ask Bob to sample this input y greater than i condition on x greater than n. So everyone has the right. That. So everyone has the right has enough information to do the sampling. And at this point, the two players have collectively generated the input pair for the big problem. And it's not very hard to verify that Alice actually knows everything in X, all this NX, and Bob knows everything in the input Y. So they can, from there, start to run this big protocol pi. Start around this big protocol pi that computes the XOR of all n input pairs. So if pi is grabbed, then they get one bit. That's the XOR of everything. Now they want to actually from that recover the XOR of this important coordinate f of XIYI. And they can do this because just by the construction of the protocol, Alice knows everything in the first I minus one coil base, so she can further sense. So she can further send one bit that computes XO of the first I minus one coordinates. And similarly, Bob knows everything in the last n minus one coordinates and can just compute it locally and send this one bit equal to the XO of the last n minus i coordinates. And if as long as pi is breaked, then they can just compute the XO of the three things, and that recovers the f value of the ith coordinate. So as long as pi is break. So as long as pi is correct, this protocol correctly computes f of f of x and y. And for its information cost, and because now, because this in this protocol, we effective embed the actual input in a random location in this n coordinates, then run pi. And pi does not know which one is the location that we embedded. So we can actually prove that the information that this transcript of pi reveals about This transcript of pi reveals about the actual input is only a factor of n smaller than it reveals about the entire input. And more formally, we can prove it. It's not very hard to show it using the chain rule of mutual information. And we can do this separately for the two terms. And we can show that the information cost of this protocol is at most I over n plus some constant O of one. And O of one is exactly. And all one is exactly because of the last two bits that they they said. Okay, clear. Yeah, so this is basically what we showed in the last slide. Okay, so now I want to say, I want to talk about an alternative view of the same proof. It's equivalent to what I just talked about, but we'll. I just talked about, but we'll see later that it's actually has some other benefit that will allow us to extend to the strong XRMI version. So instead of doing this random embedding, what we view this proof is as it decomposes the big protocol pi into two separate protocols. One protocol pi sub n that computes one copy f and it has some information cost i1 and another protocol pi smaller than Protocol high smaller than n that computes the n minus one fold of f with some other information cost i2. And this I1, this I1, I2 could be arbitrary, but we guarantee that the sum is at most is equal to i plus of one. Okay, so we decompose this protocol into two such that the information costs sum up to i. And then if this is true, then we can iteratively do this thing to the n-one fold. The n minus one fold protocol, and then we eventually get n protocols with total information cost i plus over n, and there exists one that has the claimed information cost. Okay, so how do we do this decomposition? It's basically the same as what we saw in the embedding, it's very similar. So, in the first protocol, where we want to compute f of one pair, then we view this input pair. Then we view this input pair as the last coordinate xn, yn. And then we let the pairs use public random bits to sample y smaller than n. And we let Alice use private randomness to sample the first m is one coordinates of x conditioned on y. Okay, so this is some protocol that generates the inputs x and y, and then we run pipe. This generates the inputs. So in particular, Alice knows all the x and Bob knows all the x. Alice knows all the X and Bob knows all the Y. So we do this, and then we finally let Alice, who knows everything in the first n minus one coordinates, to send the XOR first n minus one coordinate. And we can analyze this information cost. And for the second protocol that we generate from the decomposition, which we need to compute the XOR of n minus one pairs, we just embed the input into the first. We just embed the input into the first n-minus one coordinates, and then we publicly sample xn and Bob privately sample yn conditional xn, and then we run the protocol on this input, on this generated input and let Bob send the last one. And then we can again analyze this information cost. And by this, basically the same analysis, we can show that the first term information costs sum up to the first term in the information cost of the big protocol of the original. Information cost of the big protocol, of the original protocol, just by changing over mutual information. And the second term is similar. And by adding the two terms, the information cost of these two protocols sum up to I possibly one. It's clear. And then by iteratively decomposing the protocol, it basically gives us n protocols for f and it's. F and it's equivalent to the original build just because I last protocol it generates is basically the original protocol when we embed the input into XIYI. We can verify that. Okay, so now I want to point out a very useful view of this decomposition that we'll keep in mind in the rest of the talk, in the last few minutes. So, in this decomposition, there are actually three protocols invoked. So, the original protocol pi, and also the two smaller protocols for one input and also for n-1 inputs. So, it's useful to view them as the same distribution. So they're actually sampled from the same underlying distribution, X, Y, R, and M. And we just view different parts of this distribution as different components of the protocol. Of the protocol. So the inputs, the public random bits, the transcript. Yes. So, Hua Chang, so can you go back a second? I'm confused on one point. So when you do this reduction, there's this last coordinate where Bob is sending f of xn yn. Yes. So when you do the decomposition, it seems like you're paying. Position, it seems like you're paying one for every one of the coordinates, as opposed to in the previous case, you just paid, you're paying a little bit more. Yes. Yeah. So I just wanted to, yeah. Yeah, yeah, that's right. So if you look at the exact protocol that we generate seeing this iterative decomposition, they actually send one bit for every coordinate here rather than this one bit that is XOR of everything. This one bit that is XOR of everything. But when we analyze the information cost, that's actually the same. Ah, okay. Yeah, that's a good point. Yes. Yeah. So yeah, I guess I was just saying that it's useful to view that different parts of the same distribution as different components of the protocol. So the inputs, the public random bits, and the transcripts. And the public random is. And perfect randomness is actually not really important because it doesn't appear in the measure of information cost. So, in the original protocol pi, we view x, y as inputs, are as a random bits, and transcripts as m as a transcript. And this first component from the decomposition, we get inputs. We view the last coordinate of x, y as the inputs, and we move the first m minus coordinates of y to the perfect randomness, and we add one bit at the end of the first group. And for the second component from the And for the second component from the decomposition, we build the first n minus one coordinates from the xy to be the inputs, and we move xn to the particle random bits, and we also add one bit at the end of the framework. So overall, in the three distribution, in three protocols, it's actually the same distribution, just different components. Okay, so yeah, basically this proves, this would prove, this decomposition will prove that with problem. decomposition will prove that with probably two-thirds if you if you compute some function uh the n-fold x or with probably two-thirds then you get a protocol that computes f with probably two-thirds but with the lower cost and to prove the strong solemma we actually need to show that if we are given a much weaker protocol that computes after the end with a probability with exponentially small with slightly sub-exponentially small advantage we can still get a protocol for We can still get a protocol for F that has some non-trivial unfounded B. And the main challenge here is that we're going to apply the same strategy of decomposition. And the main challenge here is that we want to design or analyze a decomposition that actually will increase the advantage over these situations. So, from something very small to something reasonable. And our new And our new alternative view of this proof actually has this benefit that enables us to do this. So, yeah, let me just use this notation to denote the advantage of something. So, advantage of F condition on W be the advantage of F condition on some random variable F, some random variable W. And that's the best, basically the best bias or the advantage that we can have to predict F giving. Predict F given W. And also, the advantage of two random independent bits is the product of the advantages. Okay, so now here is the key and interesting observation of this decomposition. Okay, so let's look at the first protocol from this decomposition. So the last point of the input and y smaller than n is the public. And let's analyze at the end of the protocol. Analyze at the end of the protocol from the perspective of Alice. So, what does Alice know? The probability randomness doesn't matter. So, what Alice knows is her input xn and the probability randomness y smaller than n and r and r and also the transcript n. So, if we let Alice to predict the output, which is the function value of the last fund, then this notion here is basically the advantage that she can get. Basically, the advantages that she can get. The advantage of this thing, condition on the state. Okay, so now let's look at the other component from the decomposition. So at the end of the other protocol, we analyze it from the perspective of Bob. So what does Bob know? So Bob knows his input, y smaller than n, and the public random bits xn and also r, and also transparent n. So Bob knows exactly the same topic. So now it's useful to So now it's useful to view these protocols as well as the original protocol as the same distribution, but just different components, different parts as different components of the protocol. So in the same distribution, in the first distribution, in your first protocol, X knows the same tuple as what Bob knows in the second distribution. So it's the same Xn, Y smaller, N, R and N. And if we let Bob predict the output, then this is basically what. Output, then this is basically what he knows at the end of the communication. And the key observation here is that we get these two advantages with the same condition, and the two things that they want to predict are actually independent conditions on this tuple in this distribution. So, let me remove the redundant text. So, the key observation here is that these two things that they all actually These two things that they actually want to predict for these two protocols actually independent condition on these two things in the condition. And this is actually not very hard to prove just by the rectangle property of the communication protocols. But let me skip it here. And then we use the fact that the XO of these two random variables is exactly the XO or the value of the n. A value of the nfold x or function on the entire input, and then they are independent condition on this tuple. Then we get that the product of these two advantages is equal to the advantage of the entire output, but conditioned on the same thing. Okay. So this basically says that from this decomposition, we obtain a pointwise equality. A point-wise equality for the advantage. And it's in the right direction or in the right, has the right form because it relates the advantage of pi n and pi smaller than n to the advantage of the original protocol. But we need to use it carefully because the overall advantage is expectation, but this is a point-wise equality. But if we use it in the right way, then it basically says that. Right way, then it basically says that if pi n does not have sufficiently high success probability, then we get a protocol for f to the n minus one with advantage factor larger than the original advantage of pi. And then this basically solves the challenge that we mentioned at the beginning of this part: that we want to have a decomposition that actually increases that advantage. Advantage. I think I'm probably running out of time, but the overall proof strategy is that we will do this decomposition and each time we prove that if it has high cost, then if the protocol for F has high cost, then the protocol for F to the N minus one has much lower cost. And if it has low success probability, then it has much higher, then the Then the protocol for n minus one probably has a much higher advantage than pi. And otherwise, then we get a protocol that has low cost and high success probability and it's a good protocol. And if finally we see that if we get something that has, if we begin with a protocol that has low cost and non-trivial advantage, then this iterative decomposition must give a good protocol for F because it has to iterate for n steps. And steps. And at last, let me mention one technical point, a technical novelty that we use in the proof. So this relationship an earlier point that I mentioned, that strong XLMR is actually false for information complexity. And the main reason is that this is an average measure and it actually measures the expected communication, it lowered the expected communication. So for example, information can be a very important measure. So, for example, information can be written as the expectation of log of the range of the probability. And instead of working with information complexity or the mutual information, we will actually work with this exponential version throughout the entire argument. So instead of looking at the expectation of the log of the ratio of the probability, we look at the expectation of the ratio of the probability direction. So in the argument, instead of proving that the information cost is at most something, we prove this exponential version is at most two to this something. So this will actually provide a strong concentration on this log term. And that's actually what we need in the proof. And actually, that was the reason that strong X-Lima doesn't work for information complexity because there's no concentration. And also, And also, we can show that for this exponential version, there is another point-wise version of the chain rule that actually holds, and that turns out to be sufficient for the argument. And let me conclude the talk by just listing a few open questions about these problems. So, we proved this XO lemma in the regime where F can be, we assume. Um, f can be we assume we start with a protocol to compute f to the n with exponentially small advantage, and we obtain a protocol for f with constant advantage. So, one open question is that whether we can prove something similar in the opposite regime. If we start with some protocol that computes f to the n with constant probability, can we obtain a protocol that computes n with a very computes f with a very high success probability? And the current technique fails. But the current technique fails in this regime. And also, there's some technical reason that we need the we will lose some R to the R in this argument, so it doesn't work when the number of rounds is unlimited. And can we prove something for general communication? Maybe start with a protocol with bounded communication and obtain a protocol with bounded information so it doesn't violate the information compression. Information compression variants and also, are there any more applications to this new notion of information of the exponential version? So for example, because it provides a strong concentration on this log of the probability, so one immediate consequence is that it has much smaller overhead when doing information compression. And so maybe for some arguments where we used to go through information complexity and need to do information compression. And need to do information compression, this can be useful in the regime where we cannot afford to lose an additive error. And also, can we understand the relation between this new cost and the complication? Yeah, thank you for listening. Thank you. Maybe you could stop and we have quick questions while she's setting up. Quick questions while she's setting up at the beginning. Probably gonna take half an hour. Anyone have any questions? Uh, Ho Cheng, I'm assuming this should probably have many applications. Do you get anything like black box just applying your technique to? You mean from the XOLMA to something? Yeah, I haven't thought about this question yet. Yeah, but I assume that maybe in streaming there are some problems that have this XOR structure that can be obtained. Okay, who in the audience has a paper that Ho Chang is just improving? We can think about this. Yeah, sorry, I think I went over time, but too much.