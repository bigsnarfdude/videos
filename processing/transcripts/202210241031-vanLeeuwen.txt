There's the Center for Mathematics and Computer Science, but they insist on keeping the Dutch name. Okay, good. To talk about a wave-based US problems, please. Thank you very much. Welcome, everybody. Thanks to the organizers for having me here. It's a great pleasure to be here in a live meeting again and in Banff, such a great location. So, what I'd like to do today is give a rather Today is give a rather high-level overview of basically two separate research lines that I've been pursuing a little bit in the last couple of years. And then, you know, what I hope to do is to somehow bridge these together. And I think with other talks and the people in the audience here, it should give rise to some very interesting conversations, I think, to try to marry these two ideas. So, one is on reducing non-linearity in ways. On reducing nonlinearity in wave-based inverse problems. I know there will be a couple of other talks around this topic as well. And the second is uncertainty quantification for such non-linear high-dimensional inverse problems and see if we can somehow use ideas of one for the other. Connect, there we go. So as you have already seen a little bit in the first talk this morning, there are many applications of images. Many applications of imaging, and in particular, imaging with waves finds applications in many, many different areas on many different scales. So, a well-known example is oil and gas exploration, where seismic waves are generated, they propagate through different earth layers, their reflections are recorded again at the surface, and then from these types of data you try to measure or try to map out the different earth layers and then do something with that information. On a larger scale, On a larger scale, similar principles are used by global seismologists to try to infer or learn something about the structure of the Earth from naturally occurring earthquakes. Then there's actually two inverse problems you need to solve, right? First, where did the earthquake occur? Once you know that, you can then try to use the measured earthquake data to try to infer something about the structure of the earth along the path that the waves took to get to the measurement station. Took to get to the measurement station. On a much smaller scale, medical imaging waves are also being harnessed for imaging. For example, ultrasound imaging is a very, very well-known one. But if you kind of stretch it a little bit, like MRI or optical tomography also use some kind of wave phenomena in a way. So, for the overview, today, what I'll do mainly to set notation is just talk a little bit how. Just talk a little bit how this is usually formulated as a PDE constraint optimization problem. And then there'll be the two main parts. The first is about reducing the nonlinearity of this inverse problem via various ways. And the second is then to try to do uncertainty quantification, to get more information in just a single point estimate. And then to wrap up, maybe, you know, draw some differences and connections between these two and see if we can come up. Between these two, and see if we can come up with ways of bridging these two rather disparate approaches to solving the inverse problem. All right. So, the inverse problem, right, so the typical setup, as already highlighted in the first slide, right, we have some domain with some interesting features or interesting structure in it that we'd like to learn, but we cannot observe this directly. We cannot observe this directly. We only have access, let's say, to the boundary or part of the boundary of this domain. So we emit waves into this domain and whatever comes back or is propagated through, we measure again at either the same locations on the boundary, or maybe at the opposite boundary or some combination thereof. And then this is usually classically formulated as PDE. classically formulated as a P D E constraint optimization problem, where then of course the P D E is some kind of wave equation. So just a set notation I'll I'll I'll go over this here. So typically we write it like a nonlinear data fitting term with a constraint where this A here represents P D E with spatial coefficients M, U is the state, or in this case the wave field, Q is the source term. Q is a source term. P here is a sampling operator that tells you where you measure this wave field, where you sample it. And D are the observed data. Now a lot of things are kind of hidden in the structure. The first one is that typically these are multi-experiment data. So we don't have one source term, but we have many source terms for each of which we collect a data set. We can think of Q here in the discrete setting just as a block vector, let's say, with all the different source terms. In that case, A would In that case, A would be like a block diagonal matrix where all, you know, in the discrete setting, the discretized PDE operators would be embedded in. Similarly, D would be a block vector. And then, depending on whether we do this in the frequency domain or in the time domain, A could have a particular structure. Like in the frequency domain, A is just a discretization of, let's say, a Helmholtz type PBE, so a large sparse matrix. If we do time steps, Paris matrix. If we do time stepping, you know, you could still think of A as some kind of block or banded matrix, a lower triangular banded matrix, but you would never form this matrix in practice. You just do time stepping, but you can represent that in the same way. Now, often this constraint problem is not tackled directly because of the dimension of the state space involved, but rather it's reduced to volume volume. Rather, it's reduced to volume form. So we just solve the PDE, plug it back in, and now we have a non-linear problem in only the parameters m. And we can solve that with any black box, non-linear optimization method. The required gradient information can be computed with the adjoint state method, which then involves solving an additional adjoint state P. But that's all fairly standard, but just to make sure that we are all on the same page. Many computational challenges if you try to do this, right? First of all, this is a highly non-linear problem in the coefficients m. It's often rather high-dimensional optimization, right? This parameter m is a gridded function, so it could be up to 10 to the 6, 10 to the 9 parameters, similar to what Simon was talking about this morning. Function evaluations are expensive, also highlighted in the previous slide. Expensive, also highlighted in the previous talk, right? We need to do PDE solves for every evaluation of the objective function. And remember there that we typically have many right-hand sides. We could have, let's say, three-dimensional acquisition, we could easily have thousands of individual sources for which we collect data. And then on top of that, to make matters perhaps even worse, is that the exact adjoints may not be readily available. So, whatever grading information we compute, this will be approximate grading. This will be approximate gradient points. Now, I don't want to focus too much on these computational issues for today, but rather on the non-linearity. So, the sources of non-linearity in this problem are a couple that we can kind of try to separate from each other. So, first of all, we're dealing with wave propagation, so we have oscillatory data. So, you can imagine if you try to fit two oscillatory signals, there could be many partial overlaps. There could be many partial overlaps of these two signals, which give rise to perhaps local minima, or at least places where the objective is very, very flat that would be hard to get out of for any local optimization method. Absence of low frequency data in many applications exacerbates this, right? You can imagine if there's less oscillations, there's less room for partial overlaps. So that makes me. So, that makes matters worse. And we usually have a limited aperture. So, we can only access the domain, let's say, on one side with some kind of limited sampling operator. Now, to give a small example of this, I want to go through this kind of simple toy example. So, this is a 2D simulated inversion example, but it does highlight kind of the main sources of non-linearity. So, here in this topic, So, here in this top picture, you can see a gridded parameter in the wave equation that represents the sound speed in the subsurface. In this case, some measurements and sources are taken here at the top. That would be the surface of the Earth. And then, as this is a non-linear optimization problem, we need to come up with some initial guess of these parameters to start the optimization from. And here, there's two scenarios: the green one, that's the good one, has an initial guess that's somehow a, let's say, Guess that's you know, somehow a let's say a smoothed version of what we have here. You see, it has some lateral structure in it that captures the main variation in it. And we have, let's say, the red one that's the bad initial guess that doesn't capture any lateral structure, but it does still have the vertical structure in there. Then, if we look at the data coming from these two initial guesses and compare it to the actual data going from this model, we can see here that they fit pretty well. So, the black comes from this one. So, the black comes from this one, the dashed one comes from this model. You can see there's a fairly good fit. There's only small differences in amplitude, and those we can deal with. It kind of indicates that we're kind of in the regime where this relation between data and difference between these two models is linear. Looking for this other initial model, though, we can see that now these oscillations are completely out of place, making data flipping much, much harder. And indeed, if we don't try to And indeed, if we then try to run a local gradient-based optimization method, we can see the left in green is successful. It's able to fit the data everywhere, come up with a good picture, let's say, of the various earth layers. The one on the other side for the bad initial guess is able to get a pretty good data fit, except for this little piece here. And apparently, this small misfit in the data here gives rise to very large. Gives rise to very large perturbation in the null. So, this is typically what you have to deal with in these types of inverse problems. So, reducing the nonlinearity tries to basically tackle this by hopefully making, let's say, the objective function somehow have less local minima, whatever that means, or less be less less nonlinear in a way. Um but you know, what that actually means is of course vague still. means is of course vague still. So one approach that I would like to highlight here is one that I've been working on myself and that basically kind of relaxes this PDE constraint. So the PDE constraint it enforces the physics but if we have a very bad initial guess for these model parameters the physics they don't mean very much right because they're not a good representation of what's actually going on that gave rise to the data. So let's relax these constraints and instead incorporate them Constraints and instead incorporate them as a quadratic penalty with some trade-off parameter rho. And then, similarly to what I showed a couple of slides ago, we can reduce this formulation, eliminate the state, in this case not by solving the PDE, but by solving some kind of over-determined system that involves both the PDE and the sampling operator. Then, doing this reduction, we can end up with a non-linear optimization problem that only depends on the median parameter. But we have introduced this trade-off parameter rho that now we need to somehow pick. It somehow balances to what extent we believe the data and to what extent we believe the physics. And of course if we take rho to infinity, we're back at the constraint case with a hard constraint, but if we take it somehow smaller, we're in between. And actually, you know, this works. This seems to give you nice results. To give you nice results and at least give you some robustness towards bad initializations. That's depicted here. So, in green, we can see that the reduced and this relaxed version for different values of this trade-off parameter for at least the green model all converge to the same thing, right? That's good, as it should be. For the bad initial model, we see though that for small values of this trade-off parameter, we go. Straight-off parameter, we converge to more or less the same thing as on the left. If we take this trade-off parameter too large, however, we're back at the constraint case and we converge to the same thing as the reduced approach. But it seems that for small values of this trade-off parameter, this kind of constraint relaxation gives you some robustness towards initialization process. There's of course challenges with this as well, right? We need to choose this trade-off parameter, and I believe there will be a talk. And I believe there will be a talk by Susan Ninkov later this week that tackles this issue. Another one that I've been looking at myself is that this kind of data assimilation step, which I think I misspelled, should be done in a computationally efficient manner. So if you remember, we had this A transpose A thing here, and of course, in practice, you don't want to form this thing, right? Especially if you do time stepping, there's no way that you can actually form this matrix, so we need to come up. We can actually form this matrix, so we need to come up with clever ways of trying to solve this over-determined system. And then we need to more, you know, understand the limitations of this because it doesn't solve all the problems. It's constraint relaxation. It will break down at one point, but we need to understand when and can we predict how it will break down. Now, other approaches to reducing the non-linearity, of course, have also been approached by some of the Have also been approached by some of the people in the room here and many, many other researchers over the years. So, there's other types of extensions or relaxations that can be done. There's other data fidelity or loss functions that you can impose. Recently, there's been a resurgence of interest in the ASQL inverse scattering method. So, right in 1D, you can basically write down an exact solution for the inverse problem that goes directly from scattering data to the median parameter. 2D medium parameter. Can we extend those to 2D? And in the geophysical community, there's actually been some success in this direction. And then there's a class of reduced order models that try to kind of achieve a similar type of reduction of nonlinearity by a two-step approach where you first estimate the reduced PDE from the data and then try to distill from this an estimate of the internal state or internal solution. State or internal solution from which you can then easily extract the median program. Now, to kind of broadly summarize all these approaches, I would like to think of them as data-driven versus model-driven. So, the kind of classical approach where you solve it, you simulate the PDE, you try to fit it to data. That's a typical model-driven thing, right? You have a medium parameter through simulation, it gives rise to a wave field. You see if it fits the data, if it doesn't, you update. The data, if it doesn't, you update it. On the other hand, these other approaches, in one way or another, what they try to do is to incorporate the data at an earlier stage. So very explicitly in the constraint relaxation, we saw that the data is involved in getting an estimate of the wave field. Then instead of checking the data fit, because the data was already used, you check the model fit. You can see, right, you try to check if this wave field actually obeys the physics that you assume. If it doesn't, you don't. That you assume. If it doesn't, you then update the median parameter to get a better adherence to the physics. So it's a really fundamentally different way of looking at the problem. Now, going forward to the uncertainty quantification, it is usually thought of in this corner here. You have some likelihood term that says something about noise, it tells you something about data fit. There's a prior on M, and you can write down the Bayesian formulation as we saw in the previous. The Bayesian formulation, as we saw in the previous talk in that setting. But for actually getting a map estimate, in you know, here things appear to be a little bit easier, but then how do we do the uncertainty quantification in that, in the left corner? So that'll be the next bit I want to briefly highlight are some directions that that maybe go towards trying to to marry this. Two words trying to marry these two approaches. This I don't think I need to go over since we have our previous talk still fresh in memory. What I do like to briefly highlight is what we perhaps like to do eventually is not only sample from the posterior, but to go a step further. We need to think about what to do with these samples. I don't have an answer to that, but it's maybe something. To that, but it's maybe something that gives rise to further discussion. But with these samples, a user doesn't want to have a whole number of samples to look at. I want to somehow summarize it and maybe look at reliability of particular features of interest. Maybe pull apart the influence of the prior and of the data on certain features that you would like to interpret. And in the particular case of these types of monolinear inverse problems, we'd like to also have some. Inverse problems, we'd like to also have some information on how sensitive is the result I got to the initial guess that I put in. And there you can already see there will be a difference between which formulation you use, right? Are there the model-driven or the data-driven type of approach? A very, very classical, let's say, you know, Bayesian approach has been used already. Has been used already with quite some success in these types of inverse problems, in particular. Here's an example that I worked on a while ago with some global seismologists. What they would like to do is they have a large-scale inversion of some earthquake data in 3D, and then they would like to, you know, not look at samples of the posterior, but they would like to characterize the resolution. They would like to see what features can I interpret and which features can I not interpret. Can I not interpret reliably? So then, what we ended up doing is assume that this posterior is locally Gaussian. Now, in this setting, that's reasonable. And on top of that, we assume that this covariance of this posterior is a blurring kernel. In particular, it's just a Gaussian blur. And then what you can try to do is estimate basically the correlation length in this blurring operator in different directions: on north, south, east, north. Different directions, so north, south, east, west, and the radial direction. And then the particular trick that made this computationally feasible is that we could extract these correlation lengths by applying this Hessian to well-chosen random input vectors. So then through some kind of, you know, it's kind of similar to randomized trace estimation, except that now you try to estimate some correlation lengths, and that makes. Lengths and that made this actually computation feasible. So, I guess the takeaway then from this is that if you know what kind of things you'd like to learn from the posterior, you can bypass sampling altogether and go directly for the type of information that you need. Yeah, I think I mentioned these Monte Carlo sampling was extensively or yeah, was it was mentioned at least in the in the previous talk Talk, there has been a lot of work on this Langevin dynamic type of ways of sampling from the posterior. And in principle, since we have all the required gradient information of the posterior, we could apply those to these large-scale inverse problems and then try to, from samples, characterize the moments, represent the mean, the variance, or the skewness. The challenge here for these The challenge here for these large-scale high-dimensional problems is then that these typically still tend to converge way to slow, which is basically like a first-order gradient type method that you're using. So then what we tried to do is basically speed this up by having some kind of adaptive step size that was heuristic and there are no proofs that this actually converts, but at least comparing it to, let's say, in this case, a benchmark, which is a metropolis-adjusted land. Metropolis adjusted Lantervin. We get very similar results with an algorithm that was unadjusted, so it has a higher sampling efficiency and an adaptive step size. You can see at least for the mean we get a very good correspondence. For the other moments, it may maybe overestimate them a little bit, although on my screen it looks worse than it does here. So I guess that's for here that's good. What's nice about these is right, they actually send. What's nice about these is they actually sample the posterior in theory. What's challenging still is that if this distribution is actually multimodal, if you have a really non-linear problem, it may still be very challenging to cover all the modes. And there's still then the question, what do you do with all the samples that you get there? Another thing that I recently became very interested in to try to push that towards these large-scale ingress problems. These large-scale inverse problems is the normalizing flows that we also heard about in the last talk. So I don't have to go over this because it's very much the same setting as we've heard about. What we try to do here is learn the full posterior from samples. So we have samples of median parameters and their corresponding data. We try to learn the full posterior from that. Once we have that, we can condition it on given data if we use these triangular structures. If you use these triangular structured maps. And again, this is reasonable, but there are a couple of tricks that seemed very important to get this to work. So I'll just take you through these pictures. So what we see here on the left is an example of a you know the median parameter. In this case, this is a biomedical example. So these are like blood vessels. What we're doing here is in What we're doing here is in opto-acoustic imaging. So these are, you know, the tissue is somehow activated with a laser, then it will produce sound waves that we can then measure at the boundary, and then from these measurements, try to image back where the sound waves came from. What we do to learn this posterior is instead of putting in the parameters and the data, we put in the parameters and the data. And the data we put in the parameters, and the adjoint of the forward operator applied to the data. So then this maps the data back to the image domain, and now we have to learn the posterior that has its input two things in the image domain. So there's somehow a dimension reduction that's given by the physics here that we use. So we're somehow incorporating the physics by basically having in our transformation a first layer that transforms the data into the image. That transforms the data into the image domain. The posterior samples you can see here that are in grants to be learned. We get a mean that seems to be a reasonable fit of the actual truth in this case, except for this part here, which is somehow appears difficult to image because the structures there are mainly vertical. The absolute error, and you can see the standard deviation somehow indicates this reasonably well, this error. Well, this error. So it seems somehow that you can actually apply these techniques to these wave-based inverse problems, but there's still a lot of challenges to get this to work. In particular, you need to choose an appropriate architecture for these transformations. Ideally, you'd like to incorporate the physics in a case, learning from samples. I always have a question on, you know. You don't know the prostitutes. So, what are the samples that will be representative? Yeah, that's a good question, right? So, that would really depend on the application. So, in this case, if you know you're going to image blood vessels, it's reasonable to come up with, let's say, a prior distribution for which you can sample these. And then, through simulation, you can get the corresponding data. You know, a remaining question of You know, a remaining question, of course, is then if you apply this, let's say, to a patient, how do you know that this is not too far out of distribution? So that's an additional uncertainty that then you need to try to quantify. Even if you simulate samples, you may need to simulate many. You don't know if this is sufficiently covering the actual distribution that you're trying to learn. So, as I said, that's an additional source of uncertainty that we'd like to try to quantify somehow. The other thing that's worrying me the most is that you've learned this posterior, but then it's, let's say, tied to one particular machine, one particular sampling scheme, or one particular image resolution. Then, if you want to refine the resolution or have different sampling or whatever, you need to do the whole thing all over again. So, ideally, what they'd like to do is to have acquisition parameters as in the Have acquisition parameters as an additional input that you can learn on and then condition on both a data set and its corresponding acquisition parameters. And again, there's the question, what do you do with all the samples, right? Should you maybe think about learning the distribution not on X, but on certain features of X that are of interest? And in particular, maybe the dimension reduction, right? You can perhaps include the dimension. You can perhaps include the dimension reduction in such a way that it highlights features of interest directly. So, to wrap up, then, you probably already know, or maybe you've seen some new ones, many computational challenges in solving these non-linear problems and sampling high-dimensional spaces. There are many practical challenges, again, with all of these, but there's also more fundamental. But there's also more fundamental problems with this uncertainty quantification for these types of problems. In particular, if you think about the data-driven approaches, what is the posterior distribution that we should sample from? Do we stick with this Bayesian formulation that we know is more challenging because it's multimodal and has very non-linear? Or should we rather sample from a distribution that comes from the relaxation of the problem? What do those samples mean then? What do those samples mean then, etc. So, then ideally that leads to a question of merging these data-driven approaches for these nonlinear problems in particular with uncertainty point of case. Yeah, so with that, I'd like to thank you. And if there are any questions, I believe there's still a couple of minutes time to answer those. Thank you very much. Question? So, on the slide, you have this role as a parameter. So, how did you choose that one? Yeah, so here, I mean, in this particular example, I had, so I should say these values here, right? This is the value of lambda here, but that's rho, it's 0.1 and 1 and 10. And this is the scale. Scaled with respect to the largest eigenvalue of PA inverse. So that seems like a natural scaling. If you kind of factor out the A here, PA inverse, that comes up as a natural scaling here. So then if you take the large eigenvalue and then scale it with respect to that, at least you have some reasonable scaling. You can either look smaller than that or bigger than that. Yeah, but how to choose it? Yeah, but how to choose it in practice? That's still a good question. I mean, in this case, I didn't, other than you know, I just added Gaussian noise, so I didn't use the noise level or anything like that. But I know there are approaches that use the noise level and then try to do like a discrepancy principle-based thing. But I'm thinking, you know, I'm kind of wondering if that's not the whole story, right? that's not the whole story, right? Also somehow the to what extent your initial model represents the the true physics at you know that particular of that particular data set should also come in there. So if I interpret it physically does that mean if I use higher frequency the rows would be smaller. Yeah so in in the way that you in the way that you I'm not sure if I see that because the mismatch is much more easier yeah oh yeah sure yeah yeah indeed yeah sure yeah for higher frequencies yeah exactly yeah yeah how expensive to solve that production there this one yeah that's uh it depends I mean if you do like 2D and helmets then If you do like 2D and Helmholtz, then it's easy, right? Because it's just a large sparse matrix and you can do LSQR, you can directly factorize it. If you do time-stepping, even in 2D or 3D, it becomes, you know, you need to do some kind of iterative method to solve this. And then this problem is very, very, very opposed. So it takes a very long time to converge. So you need to think about preconditioning, and I have some ideas about that, but it's not true. Uh I guess when you solve that force and minimization, you do the second equation first to get the U and then you fit it together. So I wonder how much the success like in the next slides come from the change of the objective function from the soft constraint or because you have this particular decent choice because you here is fixed M is locally optimal, right? And then you have so there's a split, there's a some split So there's a split there's a some splitting going on. So for example if you use where are now ingredients for both M and U in the first one rather than doing this type of thing will you say see the same thing? Ah yeah that's a good question yeah so I mean in principle if you do grading descent and M and U simultaneously eventually it will converge to the same point but it will converge much much more slowly. This kind of reduction step makes the optimization problem itself also a less ill condition. So I was thinking if whether this particular way of doing it, you can avoid certain local minima. Maybe there are still some, but you avoid some. Yeah, you may avoid local minima in this problem, right, over M and U jointly. Yeah, potentially. More questions? Let's thank the speaker again.   All right, our next speaker today is Cranak Obo from Han University. Had work university. She will talk about our optimal experiment design using transport maps. Please. So, thank you for the introduction and thanks for the opportunity to talk about our ideas and this recent project. So, what I want to talk about today is how you can use transport maps for optimal experimental design for Bayesian inverse problems. So, this talk is based on an ongoing project that I'm working on with Roland Herzog and Robert Scheischel from Heidelberg. And Robert Scheischel from Heidelberg University. So, I first want to give you an outline of the talk to kind of give you an idea of what you can expect. So, I first want to motivate optimal experimental design. So, I'll give you two concrete examples of situations where optimal experimental design is really important for Bayesian inverse problems. And these two concrete examples are going to be pretty typical of the types of Bayesian inverse problems I'll focus on in this talk. Problems I'll focus on in this talk. I'll then get into the background and mathematical tools that you need. So, this is Bayesian inference, and I'll talk about what optimal experimental design is and some of the key ideas and key challenges present there. And that will lead me into transport maps and how they can be used for optimal experimental design. So, this part of the talk is based on an ongoing project, so it's a bit of a preview, and obviously. And obviously, there's a lot of open questions and potential future directions, and I'll touch upon those at the end. Okay, so I don't think I have to tell the people in this audience that inverse problems are everywhere through the sciences and engineering. And we tend to have a pretty good idea for the mathematical models governing real-world phenomena, but these models involve unknown parameters. An accurate estimation of these parameters can rely on having. Parameters and rely on having access to informative data. Unfortunately, in real life, data collection is often limited due to cost, physical, or time constraints, and that's where the optimal experimental design question or problem comes in. So here we're interested in the design of experimental cognitions for parameter inference problems, and for this particular talk, we're going to focus on inference problems where the governing model involves some partial differential equations. Differential equations. So, two questions you can ask are where to measure, or what can you even, or what should you observe. Alright, so for the first concrete example, this has to do with earthquake-generated tsunamis. So, tsunamis can be generated by earthquakes beneath the ocean floor. So, this earthquake underneath the ocean floor causes a deformation in the ocean floor bathymetry, which causes tsunami waves to propagate. And tsunami warning systems rely on. Tsunami warning systems rely on knowledge of this pathometry change, which we can't observe in practice. But what we can measure is the water depth at various different locations. If I have those depicted as these black dots here on the ocean floor. So the inverse problem here is given these water level measurements, we want to reconstruct the ocean floor deformation. Unfortunately, in real life, we have cost constraints. These sensors are very expensive to play. These sensors are very expensive to place. So, if we can only place a small number of them, that's where the optimal experimental design question comes in. And here, we're interested in placing a small number of sensors on the ocean floor for optimal tsunami source reconstruction and accurate tsunami forecasting. Okay, and the second concrete example has to do with photoacoustic imaging. So, this is a project we're currently working on. So, photoacoustic imaging is just a non-invasive hybrid. Imaging is just a non-invasive hybrid imaging modality and it works via the photoacoustic effect. So you have some object you want to image, you illuminate it with a very short light pulse, the tissue absorbs this light and expands causing localized increased pressure that then gets propagated as pressure waves. And the inverse problem here is given pressure readings at this transducer or detector that's placed somewhere along the boundary of your domain, you want to infer various optical and acoustic tissue problems. And acoustic tissue properties. So the optimal experimental design problem is a little bit different here than in the previous example. Once you have access to this apparatus, there's no cost constraints. You only have time constraints in this particular case. And it's also actually a sequential optimal experimental design problem. So you're really interested in performing multiple experiments. So you want to find a sequence of placements for your laser and transducer for optimal inference of these opto. Inference of these opto-acoustic tissue parameters. Alright, so those are two concrete examples, but they're pretty typical for the types of inverse problems we're going to focus on in this talk. So our aim is optimal experimental design for Bayesian inverse problems that are governed by nonlinear parameter to observable maps, involve computationally intensive models. In the previous two examples, both of them require solving time-dependent partial differential equations. Differential equations, and they can contain many potential sources of uncertainty. And we'll see later that finding optimal designs, in some sense, requires solving many Bayesian inverse problems. So what that means is that OED inherits all the computational challenges that are present in Bayesian inference, and in our particular case, that are present in nonlinear Bayesian inference. All right.