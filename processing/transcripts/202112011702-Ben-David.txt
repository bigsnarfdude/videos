It's gonna work. What? It's gonna work. It is going to work now. One second. Okay. Okay, so you see it now? Yeah. Yeah. Okay. So I want to talk about this is kind of a accumulation of quite a few works over a few years. So when what I'm talking about is fundamental models of statistical Fundamental models of statistical learning and combinatorial characterizations. And it's mostly negative results and open problems. So what is about? So the fundamental result in statistical learning theory is the equivalence of Spark learnability with the finiteness of the VC dimension. But of course, this clean result only applies to binary classification. To binary classification. So, the question that I want to address is to what extent we can have similar results, characterization by some dimension of more rich models of learning, not necessarily binary classification. Okay, so in general, the problem I'm looking at is the following. A problem you have a collection of really A collection of real-valued functions. They could call them queries, or you can call them random variables over some domain x. There is an unknown probability distribution, and it generates a sample s. And from this sample s, you want to know what is the value of those estimators. So if you want to know to estimate the expectation of every H in H, this is called the uniform estimation problem and the uniform estimation. Problem and the uniform estimation problem is well understood. But in most cases, the uniform estimation, estimating the expectation of every random variable in your class, is an overkill. Sure, it's a good tool to get us to what we want, but it's more than we need. What do we actually need? What we actually, so this is well understood, but what we actually need is expectation maximization. So it's the same setup. It's the same setup. You have a collection of real-valued functions or unknown variables of some domain. There is a sample S from unknown distribution. And you want to output some H in your class that has as high as possible expectation with respect to this unknown P. So that you see when we are learning, uniform convergence, uniform evaluation of your variables will tell you that. Your variables will tell you that for every classifier, you know what its risk is, what its loss, but all we need is to find one classifier with optimal loss. So this, we call it the expectation maximization problem. And this is a very general way of viewing learning. And there are many particular concrete problems in learning that we can model in this expectation maximization. So, for example, binary. So, for example, binary classification, you can think of it as you have all your classifiers. Each classifier is a subset of X cross Y, and you want to know, you have an unknown distribution, you want to know which classifier is being hit most often or is highest probability in terms of the classifier, in terms of the unknown distribution. So, that's exactly the pack learning of binary prediction, the same for multi-class prediction. The same for multi-class prediction. Even KE sender clustering problem, we can phrase it as an EMX problem. Your estimators is the risk of different locations of the centers. And you don't want to find, not necessarily want to find the risk of every location of the centers. You just want to find a location that has risk close to the optimum. So, and linear regression can almost regression can almost every problem you can think of in terms of statistical learning you can phrase in this framework of expectation maximization i think it was first proposed by by vapnik he called it general problem of statistical learning so we want to look at this problem and the question is do can we find the dimension that characterizes learability in this model so what Learnability in this model. So, what tools do we have? Usually, we have park learnability, we have we can address it with ERM learnability, weak learnability, uniform convergence, existence of compression scheme, finiteness of dimension. In the case of binary classification, all of those are equivalent. If H is park-learnable, then you can learn it with ERM, then it's weak learnable. And everything here is if and And everything here is if and only if, and even quantitatively. So, for the binary classification, that's the clean case. The fundamental theorem of classic statistical learning tells us that given a class H, all of those are equivalent, and the relevant dimension is the VC dimension. But what happens when we go to more general problems? So, the theorem breaks down already when we go to multi-class classifications. When we go to multi-class classification. So, more than 10 years ago, we had a result that showed that for multi-class classification, there are more than two classes, then if we have infinite many labels, then we can have two ERM algorithms that one of them will learn the class very well, converge nicely at rate one over square root of n. And another ERM classifier, also an ERM classifier. Also, an ERM classifier will fail to learn. So, the equivalence that says every ERM classifier is a learning algorithm, if there exists a good learning algorithm, it breaks down already when you go from binary to multi-class. So extending from binary to multi-class breaks down the nice equivalence of this fundamental theorem of Wapnik and Chabonic. Okay, so we want. We want to here to focus on a more general problem and the problem that we call probability maximization. So it's a sub-problem of the Emax, but here my random variables are just a collection of subsets of the domain set X. And the random variable is the probability of hitting each of those subsets. So my random variable is just one if I hit the subset and zero if I don't. Set and zero. If I don't, so what I'm trying to do here, the problem becomes you're giving a collection of sets and you have an unknown probability distribution, you get a sample, you want to find a subset that probability is close to the maximum weight over all subsets. So, is the problem clear? It's a very, yeah, okay. So, example. So examples is, for example, again, the classification with 0, 1 loss, you can think of it as that my domain is x cross 1, all pairs of points and labels, and the class is the class of graphs of functions from x to y. And I want to know which graph of a function is most likely to be hit by the probability distribution. So that's exactly equivalent to proper PAC learning. To proper PAC learning, regardless of whether Y is binary or multi-class. What I want to show you is very strongly that there is none equivalence of those conditions that are so nicely equivalent for binary classification once we talk about Pmax. So, if I take an infinite set and my class now is going to be the class of all subsets. So, you have So, you have an infinite set, say the natural numbers. You consider the class of all subsets, unknown distribution, you see a sample. You want to tell me which of those subsets is going to have maximum probability. So, what's the answer? Which is going to have maximum probability? The domain. The domain, the whole domain. That's trivial. But uniform convergence fails because it has infinite visible dimensions, because I have a collection of infinitely many. Of infinitely many, all possible subsets of an infinite set. So, what I'm saying is that P-max learnability is a weaker notion than uniform convergence. If all we need to know is one set that has maximum probability, we can do it very well even if uniform convergence fails. So, the question is, can we characterize P max learnability? Do we have a dimension that characterizes when a class is P max learnable and when not? Classes P-Max learnable and were not. So, in 2017, with Rubes and Moran and Spilker and Yudayov, we proved this surprising result. So, back to you, Morgan, for surprises, that the class of all finite subsets of the real. So, if you take your class to be the, you take the real interval and your subsets are all finite subsets of the real interval, the question of whether this is P-max learnable is independent. Pmax learnable is independent of set theory. So we have models of set theory in which it is learnable, models in which it is not learnable, and it is determined by the value of the continuum. And that's kind of was a big surprise, this connection between set theory and learning. And it's a very basic problem of learnability independent of set theory. So let me Let me talk a little bit about characterization by dimension and then I'll go back to this phenomenon. So, the question is: how do we define, how do I define a meaningful dimension for learning? So, in that paper of the independence of ZFC, the way we define dimension was what we called combinatorial dimension. So, I have a class and I want to have a property of the class. Have a property of the class that will try to tell me if the class is learnable or not, or how difficult it is to learn the class. I say that a combinatorial dimension is a property of finite character, which means that I can express it by a first-order formula such that all the quantifiers in the formula are over X and the class H. So this can sound a bit abstract, but think of this dimension. But think of VC dimension. What is the VC dimension? What does it mean, VC dimension greater than D? It means that there exists x1 up to xd, and there exists h1 to h2 to the d in the class, such that, and now I have a very simple formula, every labeling of x1 to xd is realizable by one of those edges. So we have here a very simple formula that exists over the domain. Exists over the domain, exists over the class, and then a first-order formula. So now we say, okay, I allow you, as a dimension, any formula, no matter how many quantifiers, any formula which is quantification over X, quantification over H, and then a first order formula in terms of the value of H's on X. And I even allow you quantification of real numbers or Quantification of real numbers or natural numbers. So, almost every dimension that we are aware of is such a finite character. This includes the Fed Shattering dimension, the graph dimension, the Nathanodron dimension. Any dimension that I'm aware of that was proposed for learning is such finite character dimension. But the conclusion, the corollary from the independence of set theory, that we show that it's independent whether the class That it's independent whether the class of all finite subsets of the reals is learnable or not in the P-max model implies that there can be no combinatorial dimension that characterizes P-Max learnability. So the answer, you see, we were set to the motivation was the paper that we had in 2010 that showed us that we don't know how to characterize multi-class learning. The usual characterization The usual characterization breaks down. Multi-class character learnability is a specific special case of Pmax. So we said maybe we can characterize Pmax. And here the result, there can be no dimension that characterizes Pmax mobility. And the reason is that once you have a characterization by such a formula, that's finite quantification, and then a first-order formula, and a first, then this character. First, then this characterization will not change its value between different models of set theory. And what we showed for the learnability is there is a model of set theory in which this class is learnable, and there is a model in which the class is not learnable, but any finite character dimension will stay the same. So the finite character dimension cannot tell me if a class is learnable or not. So whether you care or not, we think that the class of all finite That the class of all finite subsets of the real line is interesting or not, we definitely have here this kind of result that is telling you as a corollary, as no dimension can characterize P-max learnability. So what are kind of arising questions, the natural questions that one can ask? The first natural question is where this independence of set theory is coming from, because we should. Because we showed in that paper that not only it doesn't, the nobility doesn't break down only when you let epsilon and delta go to zero, some kind of asymptotic bound, but the breakdown is already in weak learnability. Already, if you say, I want to learn up to precision 0.3 with confidence 0.7, it is already independent of set theory. So, where can this independence of set theory come from? It seems like we are talking about notions which are very concrete. So, the secret, or if you think about it a minute, you realize that the problem is that the notion of learnability that we talk about, the notions of learnability that come from Vapnik and Chabonenkis, or basically what we had defined so far, they are defined in terms. Far, they are defined in terms of learning functions, not in terms of learning algorithms. So, when we talk about sample size, when we don't talk about the computational complexity, we just ask what sample size suffices so that there is a function that maps samples to hypothesis. But we don't look at the question of whether those functions, how complex can they be? So, what happens if I require those functions to be algorithms, to be? Algorithms to be something that you can write a program from. So, if functions of infinite domains, they're infinitary, but programs are finite objects. So, the ZFC independence really relies on this flexibility, that you can have learning functions and not necessarily learning algorithms. If I had learning algorithms, then availability would just be a quantification of a program. Quantification over programs, which is quantification over natural numbers, and we cannot show independence of ZFC of any statement that quantifies over natural numbers. So it kind of shows us that this maybe oversight that we define zonability in terms of functions and not in terms of computable functions. This is what causes, this is in the heart of. us this is in the heart of the independence result. So can we fix it? What will be the cost of fixing it? So in a follow-up paper that I did with my students and with Wood, we defined learnability in terms of algorithms. So we said, let's redefine learnability. Everything repeats the same definition, but both the learner and the output predictor are required to be computed. Are required to be computable functions. Maybe this will definitely solve our independence issue. However, other issues arise. So we call it C-path learning, C for computable path learning. And we showed that the characterization of binary label learning by finite piece dimension breaks down. So once we require that all our learners are computable, you can have a class of You can have a class of VC dimension two that no computable program can learn it, can back learn it, which leaves us kind of in a completely unknown situation. This is a very natural thing. I mean, no, the valiant definition from his seminal paper of 84 of the introduced PAC learning, he talked about polynomial time learning. And the Vapika Chauvonenki. And the rapid Chavonenkis were just statisticians. So that's a very strong restriction. You want learning with polynomial functions. The statisticians don't ask about the computational complexities. They jump all the way to arbitrary functions. If we go halfway between, I don't require it to be polynomial, I just require it to be computable, then suddenly everything breaks down. We just don't know anything. All we have is counter-examples. We can give examples of classes of easy dimension two that know Of VC dimension, too, that no computable learner can learn them, although the VC dimension is fine. So, my first open question here is: is there any combinatorial dimension that characterizes CPAC learnability, that characterizes learning with computable learners? So, here is my first question. We don't know the answer. We don't have a clue how to address it. So, that's the That's the advantage of talking to such a smart audience. Maybe you can find some insights into it. So, another approach to this is to say maybe our definition of dimension was too restrictive. In the independence of set theory, our definition of dimension was this finite character formula that defines the dimension. Another way of Another way of defining dimension is to say that I can call it learning rate factorization dimension. I would happily get suggestions for a better name. So what is a learning rate factorization dimension? It's a property of the class H. So it is a function D of H, such that the sample complexity for Sample complexity for every class factors into a function of the dimension times the function of the accuracy and confidence parameters. So we can, I don't care now what is the dependence here on the dimension, what formula expresses it. All I care about is that I can, if I know the dimension, then now I have a fixed function, no matter what the class is, of epsilon and delta, that will tell me what the sample size is. Delta that would tell me what the sample size is. So that's a very natural, another natural notion of dimension. We want those two functions to be fixed. And where are we? Yeah. And just note that for the VC dimension, for the binary case, we do have a classification. The VC dimension has this property that you just have a function of D. And I mean, if I fix delta to be, say, 0.3, I just have a function on d. I just have a function on this times one of epsilon squared, a simple function of epsilon for the sample complexity. So the Visc dimension does satisfy it for the binary classification, but not for multi-class, because for multi-class, we have the natural dimension and the graph dimension, and one of them gives an upper bound, one the other one gives a lower bound, and the gap between them can be as large as log the number of labels. So the number by sample complexity, you mean. Here by sample complexity, you mean the optimal sample complexity, or this can be an upper bound and a lower bound that are close to each other? Right, I'm talking here about upper bound and lower bound that are within a constant or within some logarithmic factor in one over epsilon and one over delta. I think even that will be very interesting. Yeah, so that's factor. So that's factorization dimension, and I'll get back to it in a second. We have some kind of a result about that. So another variation of the problem is to talk about realizable PMAX learning. So what do I mean by realizable learning? I mean that a class is realizable learning if I restrict my attention to probability distribution. Strict my attention to probability distributions for which one of the hypotheses has the full distribution, has the full probability one of being hit. So I have a collection of sets. I have a promise that one of the sets is the full support of the distribution, and I just want to find any set that is close to it. So that is the case with proper realizable learning. If it's realizable, If it's realizable learning, then I know that one of my H's really supports all of the distribution. So I can have this realizable P-max learning. And the point is that our independence results, when you look at the set of all finite subsets of the real line, we don't have the realizability assumption. So it's not clear whether if we add the realizability assumption, The values ability assumption, maybe we will be able to get a dimension that characterizes the loadability. So that's my second open problem. And in an old result, I think from 2017 even with Chaba and his student Bernardo, we showed that with some natural restriction on the set of learning functions, that there's kind of natural learning functions. Kind of natural learning functions, they're monotonic in some way, we could prove that there is no learning rate factorization dimension. So we didn't need set theory, it was just a combinatorial work, and we showed that for some variant of proper multi-class learning, there can be no factorization once you restrict yourself to a certain family of learners. It's not the restriction that we have for computable learning here, it's a restriction. Here it's a restriction of some natural defined learning. So we also have a negative result on existence of dimensions in terms of factorization of the sample complexity. Okay, so to conclude, I want to show you that there are many questions that one can ask and many models that one can consider. Consider and recently I've been talking with Hassan Ashyani and Shan. And we, this is the model that'd be very happy to hear if any of you saw it before or thought about before. We somehow thought we invented it or reinvented it. And we call it self-bounding pack. So what is self-bounding pack model? You have a function m of epsilon delta such that for any data generating distribution. Data generating distribution P you can take M of epsilon delta samples from P and once you see them you can tell how many samples you will need for epsilon delta learning with respect to that P. So somehow you want to estimate the hardness of your underlying distribution. You want to take a finite sample from the underlying distribution and Underlying distribution, and this should be a predefined function of epsilon delta. And once you see such sample, you can tell the learner how many examples it will need. This kind of finite sample from the distribution tells you how complex is the distribution. So we call it self-bounding pack, and it has some weak connection. We were hoping to be able to show that. We were hoping to be able to show that it's independent of set theory to characterize it. We haven't been able to do it yet. And so, here is other questions: is there a dimension that characterizes multi-class learnability? We don't know the answer yet. Is there a dimension that characterizes self-bounding pack learnability? This is all I prepared, but let me just mention one more model, but I'm really interested. One more model, but I'm really interested to hear if any of you saw these models before. Another interesting model, which is related to this SDPAC, is a model that we call finite least decoding. So finite least decodable learning is that you see this sample of size m of epsilon delta, and based on this, you can output a finite set of hypotheses with a guarantee that one of them is going to be good. So it's another variation of those models of learning, and we still don't know how to characterize learning in these models. So I'm really posting those problems here with the hope that I will get some interesting insights from you. So that's what I wanted to say today. Thank you. Questions? So what does this class of VC dimension 2 look like that you cannot learn with any computable one? Yeah, that's a good question. The trick is that we encode the halting problem into the class. We kind of make sure that if you can learn, then you can from that figure out the halting problem. The halting problem. So we do some kind of gymnastics with turning the halting problem into a class of functions that if you will see enough finitely labeled examples and you could find a good classifier, then you will answer whether the halting holds for that class or not. And the class. The class has recent images too. I mean, I don't have the time here to. Oh, no, it's okay. Yeah, yeah, yeah. So your negative result about the factorization, you so I don't know, take a step back. So there's this very nice paper from 1995 about the relationship between a lot of dimensions for multi-class learning that Fast learning that showed that I'm one of the authors. I know. That characterized that all of these dimensions were finite. This was equivalent and characterized multi-class learnability. It seems like when you say binary versus multi-class, you mean finite versus infinite. Is this also for the infinite multi-class case? Again, so what is the question? Again, so what is the question? Is this this negative result of bad factorization? You know, you're saying here that in the infinite multi-class case, you have... Yeah. Right. Yeah, it's for the infinite multi-class case. So you have infinitely many labels. But we, I mean, we hope that we'll be able to show it for just plain multi-class, but we needed to add all kind of restrictions. All kinds of restrictions in order to get the non-characterization. We had to restrict the learners to be some kind of simple learners. So basically, the question of characterizing learnability of a multi-class with infinitely many labels is still open. Last I heard from Shaim Owan is he thinks he's getting close to solving it. Let's wait and see. Okay, thanks again one more talk.