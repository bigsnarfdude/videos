And explain briefly how you can make some formal proof rigors. I probably will not have time to explain how to use this for Cafareli Kogni. Okay, no point, yeah, okay, in Cafarell Cognier inequalities. So my main focus will be on interpolation inequalities on the sphere and on the example of the logarithmic Sobolef inequality. And then we will see what goes from the sphere. What goes from the sphere to the Euclidean space and what doesn't. So, let me okay. So, there is a long history behind these entropy methods. Let me say that the business was started on one side by people in poetry theory with Dominique Bakry, essentially. So, it is what they call the Carri-Duchamp method. It's a completion of a square at some point. Also, from the non-linear analysis point of view, non-linear elliptic. Analysis point of view, non-linear elliptic point of view for people interested in having uniqueness results for semi-linear elliptic epidems. And it was introduced into the field of diffusion equation by Jose Peterscani at the end of the 90s based on some results of information theory. So I listed a long list of people. I don't have time to comment all. To time to comment all the contributions of these people, but basically say it's in the spirit of this group coming from kinetic equation and investing diffusion equation. For instance, Jose, who was an important actor at this study. Now, the point is that you can optimal functional inequalities with this entropy method, but you learn more with this method and some extensions. Some extensions, and learning more means getting stability results. So, what do I mean by stability results? Is that just that you have an inequality with the optimal constant? You make the difference of the two sides, and you want to use this functional to measure a distance to a set of optimal functions. Now, of course, since you expect that the minimizer is achieved, you cannot do it just like this. So, you need to impose something else. So, you need to impose something else or to look for something a bit different. So, a bit different would be some non-linear quantity which disappears when you get close to the manifold of the optimizers. And that would give rise to non-linear improvements of the inequalities. The other strategy is to impose some orthogonality condition. And then, since you have a constraint, then you can expect to have a spectral gap in the limit and then recover some stability. Then recover some stability with it. So, actually, you can do both and you can mix them. So, just a brief overview of how Galiardonier-Handlag inequalities on the Euclidean space are related, can be studied with this entropy method. And we'll do it at a very formal level based on this generalized Rainy entropy powers approach. And the idea here is to relate Galliardoni-Hanberg's model of inequality, which are written here. Equality which are written here with the fast diffusion flow, which is here, and show you that it is a natural quantity, two quantities which naturally combine. So, here the Galliardoni-Handagin equality is the interpolation of an L2 norm of the gradient, the usual gradient on Rd, and an Lp plus one norm on Rd. And you use this two terms to control an L2P norm, which is on the right-hand side. With p bigger than one, less two p less than the critical exponent, so you are below the subof exponent, and in the case two p is the two d over d minus two in dimension bigger equal than three, the critical exponent, then it's just the subof inequality with theta equal one. Now, the fast diffusion flow is, we have already seen it is morning, and that is u to the m, and I take m less than one in all of this lecture. This lecture. So, with this flow, well, you have the mass conservation, it was already mentioned this morning. If you compute the second moment, that's quite interesting. You get something which is the integral of u to dm. And u to dm is a polar, sorry, an integral that you identify with the entropy. I don't want to go too much into the details. You will see more examples. And now, the point is when you compute one more derivative, you compute the time derivative of this entropy. The time derivative of this entropy term, and you get something which is kind of Fischer information. Actually, it is the Fischer information in the limit m going to one, where inside the gradient, you replace u to the m minus one by gradient of log of u. So you get the Fischer information greater than fu squared over U. U gradient U over U squared, the same. So the two important quantities will be the entropy which is here and the Fischer information which is here. Okay. Okay, so next step is to relate the flow with the Gagliardoni-Hander inequality. So it's the Galliardoni-Hander inequality, you have it there. And you make a change of exponents so that F to the power 2p is the density. So the integral of U, the mass of U is the integral of F to P, which is the mass for the evolution problem. Then U to the M is F to the power of P plus 1. Is f to the power of p plus one, and the term which appears in the feature information here is just gradient f squared. So it's just a change of variable. We'll take p equal to one over two m minus one and it does the job. Okay, so the mass is the L2P norm of the function. The entropy now is the Lp plus one norm of F, and the Fischer information is this one. And then, okay, there is a computation which tells you that if you compute the time derivative or the entropy. Derivative of the entropy where you get Fischer. Fischer, you identify it with the gradient F in the Galliardoni-Handler inequality, and this gives you a differential inequality that you can integrate. So what does it tell you? It tells you some growth. After integrating the OD, which is here, you get a certain growth of the entropy term in terms of the time and the initial entropy, which is here. Okay? And getting the optimal concept. And getting the optimal constants here from the Galliard-Nierrendagh inequality means getting the optimal coefficient, which is here in the t-dependent term, while the exponents are just fixed by some scale. Okay, so the message here is with Gagliardoni-Hanberg, you apply the flow, you get a rate. Here, it's a growth rate because u to the m, m is less than one. So the mass is conserved, the solution diffuses, the integral of u to the m increases. Okay, now the friend of the Bahrainblatt self-similar profile, the friend of Juan Luis appears in the sense that you have this self-similar solution, the Bahrain-Blatt solution, which has here for some R of T that you can take as a power if you want at T equals zero a delta initial datum or any uh solution solving the the appropriate to the E. I don't want to go to the details here. The point is that the profile is this one plus x squared to the power one over n minus n. square to the power one over n minus n and uh if you remember that b is a density of mass if you take uh phi to the power two p equals to b this phi is an ovantal anti-type function so it should be p minus one not minus two so open talented profile which is going to be an optimal function for a galliano so now what is the technology behind these uh rainy entropy powers well you take the rainy Well, you take the Rainy entropy power is a power of the entropy. You take the time derivative and you get the property which is about there. And then, if you introduce a pressure variable p and recompute everything in terms of p, which is convenient but a bit lengthy, then the derivative of the Fisher information has a nice property. It's negative because there is a minus sign in front of the integral term. There is a Hessian of the pressure which is. Is a Hessian of the pressure which is here. This is a square. This is a good term. It has a sign. And here you have the Latin F P to the square, so that by the arithmetic geometric inequality, T's term is controlled by T is one if one minus M is less than one of the D, which means L bigger than some M1, which is one minus one of the D. So if L is close enough to one, since as a sign, this is decaying, you combine it with the entropy, you make some additional. The entropy, you make some additional computation, and what you find is well that the quantity above there is a monotone, so you are kind of measuring a growth. Actually, one more derivative. Okay, if I to the theta e to the power, which is up there, is the derivative of the Rainy entropy, you are computing the second derivative of the Rheni entropy powers. Now, Rainy entropy powers, they are just done in such a way that for self-similar solutions. A way that for self-similar solution you have a linear growth. So, what you are measuring is the deviation to this linear growth here. So, the sign of this is the concavity and it measures the distance in some sense to the optimal function of the self-sliner solution, which are actually optimal for Galliardoni Handel. So, then doing some analysis for the large time behavior using the gross property, you realize that you have the limit is going to be. Have the limit is going to be given by the set signal profile, and because of the sign, you get that at any finite time, this product here, time derivative of Frani entropy, is bigger than its asymptotic value, which is given by the self-signal solution and doesn't depend on time because this is the self-signal property. So, and this is exactly the Galiardoni-Handle-Astrolog inequality with the optimal constant. So, actually, you close the loop. So, you use Galiadoni-Handley. The loop. So you use Gagliardoni-Hander to get trace, but you take one more derivative and you recover the Galliardoni-Handa. It's a way of proving the Galliardoni-Honda inequality. That's the deep relation that you have behind this fast diffusion flow. Okay, on RD, so there is a long paper that we have done during the COVID time and we wouldn't have done without COVID probably, which is presented here. And there is a short version here if you are too lazy to be. Here, if you are too lazy to read the 170 pages. Okay, so this is what Nikita is going to present more in details tomorrow. Let me just explore a little bit what he's going to do. What I told you is essentially formal because you do plenty of integration by parts which are hidden there and you have to justify them and it's not so easy, in fact. So it's simpler to go to the self-similar variable that we have seen this morning, okay, and then eventually transfer. And then eventually truncate in a large ball, then compute the boundary terms, then remove the boundary to prove the identities. Anyway, you introduce a relative entropy, which is this F, which, oops, sorry, which is here. It is a relative entropy now written with respect to the Barenbad profile, which is now the stationary solution. You have a relative Fischer information I of V, which is here. V, which is here, and you have this entropy-entropy production inequality, which tells you that the relative entropy has an exponential decay. Now, if you rewrite what this means, this is exactly again Galiard-Ni-Hander-Shabov inequality. Good. And this is what is written here. So the optimality case is achieved exactly on the Barrenbat profile by this, and this is a proof by a flow. What is hidden behind this is the fact. Hidden behind this is the fact that the optimality case is achieved on the long time range in T self-similar variable. So you converge to the Bahrainbat profile and how you identify the base constants is just the spectral gap when you linearize the difference of Fissure and entropy. And you write the condition that it's non-negative. So the spectral gap of the linearized operator in the proper variable. Operator in the proper variables associated to the fast diffusion flow gives you the optimality case. In other words, if you make the quotient of Fischer divided by the entropy and you try to minimize, that's the way of getting the best constant in the inequality, actually it's achieved only as t goes to infinity when both numerator and denominator go to zero because it's relative to the the barren lattice. Okay, and then you linearize around the barren latt and you get a spectral cap. That and you get a spectral cap. So here is a plot of the spectrum of the linearized operator in terms of M, the exponent of the fast diffusion, and it's the value of the eigenvalues in the appropriate scale. So the e to the minus 40 is twice the gap corresponding to the length from zero to the first positive eigenvalue, which is two. Now, there is one thing which Now, there is one thing which is nice. The green line is generated by the translation of the Baron dat. Actually, the X derivative, the Xi derivative respect to each of the coordinates of the Baronplat. Of course, your problem is autonomous. It doesn't depend. There is no center. So you can always translate a little bit, okay, and then you can actually center the. And then you can actually center the center of mass, and that would kill the eigenfunction corresponding to this eigenvalue. Okay, so remember, green means translation, translation, it can be killed. What does it mean? I take an initial data whose center of mass is at zero. The center of mass is preserved by the evolution. So the corresponding, when I will linearize, the corresponding eigenfunction will not show up. And then instead of Show up, and then instead of having a spectral gap which is twice this value two, which is here, you get you go up to the blue line. The blue line corresponds to scaling invariance. This is a bit more complicated because scaling is not preserved even in self-similar variables. Okay, so what is the strategy that we developed with so it's with Mateo, Anikita, and Bruno Nazaren, who is not here. And Bruno Nazare, who is not here, is to say on the large time asymptotics, we have an improved spectral gap if we have well-prepared initial data. So here, instead of having e to the minus 40, we will get a bit more, okay, in terms of decay. But this means a better constant in the inequality. On the initial time layer, this is where the Carriedition method plays a role. You also have some improvement, but it's a more linear kind. I will come back to this later. Kind. I will come back to this later. So, the idea is just to stick this initial time layer with this asymptotic time layer, and you have to decide when you enter in the asymptotic region. This is the threshold time, which is here. You will see the picture again tomorrow, and some explanation on how you compute T star, which is the delicate matter. Once you have done that, you have all the ingredients. So, what you get is instead of having that the left-hand side is non-negative. that the left-hand side is non-negative, you can actually look at the second line, the green line. You get that I minus 4 times the entropy is bigger than some constant zeta positive times the entropy. But of course, you can do an oscillation and actually you can put on the right hand side also the Fischer information. Fischer information seems to like H1 norm, H1 distance to the barren button. Okay, so the result is here. Okay, so the result is here. If you make the difference of the two sides of the Galliard-Nier-Hander inequality, so you recognize the L2 norm of the gradient, the Lp plus one norm of the function, and the L2 norm of the minus sign here. It is the deficit of the inequality. Then it controls a distance measured by the Fisher information where you minimize over all the Bahrain Blatt or Orban Talenting projects. That's a strategy. To get to higher, to get read. Higher to get rid of the blue line, you can also do it, but it's more delicate. It's like introducing a time shift, okay, which corresponds to scaling a little bit the Barrand Lat and adjusting the Barohand LAT at the next order. So you're actually doing a Taylor extension for large times and killing the next order too. And this is a bit delicate, so I don't want to explain this here, but this can be done, and this is crucial in one case. Crucial in one case. You see, these lines they cross here, this is exactly the exponent corresponding to the subole F inequality. So, if you want to have a stability for subole F in R D, then you need this. Now, I have been hiding a little bit an assumption. Actually, there are two. One is that you need a second moment, which is not granted a priori when you work with just the grade interface in L2. And so you need And so you need a second moment which is here. Now you have to fix this. And the other one is that you have some decay of the tails, and it is crucial for computing the threshold time. Once you have done that, then the deficit in the critical case, so the difference of the two terms in solar F, is computed in terms of an explicit constant, which is here, times the Fisher information. Okay, so this is an explicit result with some constraints. Okay, this I definitely don't have time to present it, so let me skip this and go to what happens on the sphere. So on the sphere, well, it's a compact manifold, you don't have boundary, life is much simpler. There are many things that carry from the sphere to the Euclidean space just using the stereographic projection, but you have to be extremely careful with the functional spaces, with moments and this kind of things. So I will. These kinds of things. So, I will just focus on the sphere, it's a bit simpler, and explain you how the Caribbean method works. So, here I'm going to present a series of three papers with Giovanni Brigatti and Nikita Simonov, some of the work, and it applies to the Laga epics of AF and to Galli-Antoni-Henberg inequalities on the sphere. So, the Logal Pixable F inequality, when you write it on the sphere, you get that the optimal constant is z over 2. Constant is d over two there. On the right hand side, you recognize the f square log of f square, or write it for u equal f squared if you prefer to have the gibbs entropy. And the only thing I'm doing here is that I take the uniform probability measure on the C, I divide by the area, the surface of the C, the body of the manifold. And so that you immediately read that if F is a constant. You read that if f is a constant, then this is the equality case, huh? Because if f is a constant, f is equal to its norm because you have a quality measure. So the log of the constant divided by the same constant is zero, and the gradient of a constant is also zero, and that's the equality case. So any constant realize the equality case. To get the optimal constant, the other two there, you have to work a bit more. You can first see that it cannot be bigger than the other two just using a test. Than the other two, just using a test function. Okay, so you take one plus epsilon x, a scalar product with one direction, take the limit epsilon going to zero, expand, use the spherical harmonics, and realize that it cannot be bigger than mu. Unless you have some orthogonality conditions. Okay, so if you put, for instance, the orthogonality condition with respect to all coordinates on the sphere, which correspond to the The sphere, which corresponds to the spherical harmonics, corresponding to the first positive eigenvalue. Okay, then you go to the next one in some sense, and you have an improved inequality in the sense that the deficit that you had before now controls the term which involves the gradient of f square. Or if you prefer, you have an improved inequality where instead of having d over 2, you have d over 2 plus 1 as the optimal answer. Good. So that's the first thing. So that's the first thing, putting imposing an auto-electric condition and getting an improved inequality. That's the first type of stability result. Second type of stability result, well, it's the observation that if you don't put the orthogonality condition and use still the same function and you Taylor expand, there is not much you can do. When you expand it, you compute the leading order term, it's on the order of epsilon to the fourth. So there is no way you can control something. There is no way you can control something bigger than the gradient of f to the power four on the right hand side. Okay, however, the power four turns out to be the short exponent, and you can even get the constant in front in a constructive way, and this is the result that we have. So, without orthogonality constraint, we get the gradient f to the power four. For homogeneity, you need to normalize. Need to normalize here. I took the L2 norm equal to one, so this is what otherwise you would have an L2 norm squared here. Okay, and yeah, and this is the expression that you get. At least when the gradient is small in L2 norm, you really have something which is of the order of del two norm to the power of four, which is written here. Okay, now the third type of result is just mixing the two. Of result is just mixing the two, and I think this is quite interesting. So you have a parallel direction which corresponds to this spherical harmonics associated with the first positive eigenvalue, D, by the way, and what I call the parallel direction. And then you have the orthogonal direction generated by all the other spherical harmonics. And what you get is a stability result in which you have the power for term in the parallel. Power four term in the parallel direction than the power two of the projection in the perpendicular direction. Okay, so maybe just a taste of, okay, first extension. Actually, you can do it not only for the Laguid mix of length on the sphere, but also for Gagliardoni-Hander-type inequalities. So there is this family of inequality which is quite well known. Many people call it. Well known. Many people call it by the name of Beckner. It was also proved by Bidot-Veron and Veron a bit earlier, actually, by elliptic methods. And it's even in the paper of Gidas and Spruch of 81, but there is something which is not totally correct in the computation, not this strategy. And okay, so here you have an LP norm, a p has to be less than. P normal p has to be less than the critical exponent 2d over d minus 2, the 2 star which is here, if the dimension is bigger than 3, bigger or equal than 3. Dimension 1 and 2, there is no limitation, no upper limitation. And actually, it is quite funny because if you take the uniform priority measure, well, the L P norm controls the L2 norm if P is bigger than 2, but it's the opposite if P is less than 2. And if you exchange the two terms which are here, you Terms which are here, you exchange the two terms which are here, you still have a positive right-hand side. Okay, so actually, the inequality is also true for p between one and two, and this is actually proved in the paper of Backry and Émerie about Carrie Duchamp in 1985, I think. So, part of the inequality was already known before Beckler and Bigger Veronte. Okay, now to recover what is the optimal What is the optimal constant? Is we can use the test case to get an upper bound. So the constant cannot be bigger than d. D is the first eigenvalue of the Laplace-Beckrane pressure on the sphere, plus positive eigenvalue. Okay, and well, you have the same type of result. If you are orthogonal to these eigenfunctions, then you get an improvement with a constant, which is completely explicit, but would fail in the critical case. In the critical case. So, here this is probably a flow of the technique. I mean, we are using an estimate which is not optimal when you approach the critical exponent. This could be pure. From now on, I will call entropy the quotient, the L P norm squared minus the L2 norm squared divided by P minus 2 for P different from 2, so between 1 and the critical exponent, or infinity in dimension 1 and 2, except for P plus 2, and for P plus 2, you just click. For people 2 and for people 2, you just take the limit and you get the F squared log of F squared. So the log sub is actually included. Now, there is a nice result which appeared last year, it's two years ago by Roberta, where he was using the Bianchi-Egnel strategy and was able to derive the inequality which is there. And so it's a little bit different. Well, we introduced the Bit different well, we introduce the average here, so he has an L2 term, but in fact, using the Poincaré inequality on the sphere, this turns out to be equivalent to the inequality that we got. So, in his method, he was using compactness method, so he was not getting the constant in front, but he already showed that the exponent flow is optimal. So, this is for the type 2 stability result. I wanted to mention it because our profile. I wanted to mention it because our proof borrows a lot to his work. Now, Carrie-Ducham method, it's the approach of Bach-Hémery made more precise. So what Bach-Hémerry and Hémery did was just to compute the deficits, so make the difference of Fischer information minus entropy, show that it's decaying along the flow, that in the limit as time goes to infinity, it goes to zero. So if it's decaying and goes to zero, it has positive. Decaying and going goes to zero, it has positive D4, for instance, for the initial data. That's it, you have a proof of the inequality. Of course, you have to work a little bit to prove that the limit is what we expect, and so on. Now, we can do a refinement, which is to say if you are in the subcritical range, so two star is now excluded and strictly above p equal one, so p equal one is Pancare, then you can use the remainder terms in the Carré-Duchamp method to get an additional. Method to get an additional information that you can inject, and this gives you an additional term where psi is a context function which is zero at zero with zero derivative, so it's growing, and typically it behaves like psi of s behaves like s squared close to s equals zero. That's the way you see the exponent form. Okay, now the third type of result is the commit. The third type of result is the combination of the two, and this is actually where we borrow a lot to the proof of that. Good. How do you do the stability under constraint? So it's just a kind of spectral analysis based on the representation of the inequalities through the Funke formula. Again, I don't want to go to the details. And what you can prove is that if you are orthogonal to k eigenspaces, K eigenspaces, then you have a control on the perpendicular projection, the perpendicular direction to these k eigen spaces with a constant that you can compute, which is and as I mentioned before, the k equals one corresponds just to the coordinates, the first eigenstates. So, how does it work? Well, it works so through the funk equal formula, you get Funke formula, you get an estimate of the entropy by some duality, and the Fj denotes the decomposition on spherical harmonics of increasing orders. And you have explicit coefficients for this method, which is the important point. They are here. Now, you work on this coefficient. They have some nice properties in terms of the parameters. And using a complexity estimate, you get essentially what I was writing. Estimate, you get essentially what I was writing. Okay, before using the Carrie-Ducham method, I want to show you how to prove the inequality. So now, okay, that's the nasty part. You have to compute, and this computation, they are not totally nasty. So you are nicely sure at the end, but in the meantime, you have to work a bit. So what do you do? You use a fast diffusion flow. This fast diffusion flow doesn't look like before, it is the one which is on the first line, but in fact, First line, but in fact, if you take u to the power beta p and you call it rho, you are just asking that rho solves the fast diffusion equation with exponent m when m p and beta are related by this thing which is a work. Okay, so think fast diffusion flow, except that there are some technicalities. You take part. It's already in the paper of Gidas and Schrock, for instance. I don't know how to avoid this type of This type of difficulties it makes the computation unpleasant. Now, what is the good point? Rho is u to the beta p. So at least here it's only written for, yeah, I should have u to the power beta or take m equal one here. So the L P norm is the A1 norm of rho and it's zero. Otherwise, you have to put a beta inside. Beta inside, which I forgot. The L2 norm is the Fischer information. That's nice. That's the structure we had before. And you have to compute the time derivative of the Fischer information. And that's what we call KLF. What we want to prove is actually that the difference of the Fischer information and the entropy has some decay property measured by something like this. In other words, we want to estimate this gradient v to the 4 over v squared in terms of k of v. V squared in terms of k of v. Okay, there are two ingredients in this game. One is just an integration by part. This is what I call the first identity. It's when the Latinovi is this term re-integration by parts. And the other one is just the Bovnalish Nirovich formula, which is interesting because essentially you are changing derivative. If you were in the occupant space, you write that the Latin is. That the Laplace is the sum of the derivative of our xi squared. And if you have the Laplace of V times Laplace of V, you want to write things in terms of second derivative in terms of xij. So you change the derivative between the two terms. And in the meantime, well, since you're on the sphere, you hit the curvature. That's the way you get this term. You get the dH's here. Good. So here is the computation of K of V. I don't want to. Of K of V, I don't want to get into details. You use identity one, identity two, and you write a discriminant condition to have a sign, or actually you do it by hand, you write the k of v as the sum of squares. So to do this, there is a condition, it's that c minus v square with the c and b there should be non-negative. Be non-negative, okay? It's a discriminant condition, and it tells you something about the parameters. Because so far, I didn't say what is the relation of M with the exponent P and the sphere. And there is some freedom. So here are some diagrams of the admissible parameters. And this, by the way, these ranges are short. You cannot extend them. But the horizontal axis is the exponent p. So let's take, so it's dimension one. So let's take, so it's the dimension one, two, three on the first line and five four, five and ten on the second line. So p equal one is the vertical line which is here. This corresponds to point. The vertical line which is tangent, it corresponds to p equal two star. And the horizontal line which is marked, it's m. So on the vertical axis, it's m, and m equal one is the horizontal one. And as you can see, there is a limitation. You cannot cover the You cannot cover the whole range of the peaks with n equal 1. So the heat flow is not enough. That's why to get up to the p equals 2 star, you need to have an m which is not 1. It's the flash diffusion exponent. And actually at the end is m1, which is 1 minus 1 over d. Remind the case of the Euclidean space. So this is the picture. And yes, and that's the proof of the inequalities. The remainder term has a sign. So the difference of the two-term decay, the limit is zero. Okay, stability is zero. Can you go beyond this? Well, we have this beautiful term which is here. So let's use it to make some more interpolation. So here the idea is very simple. You want to interpolate the Fischer information, which is something like gradient of v to the square in terms of In terms of the L2 norm of v to the square and this term gradient v to the four over v square, just the Cauchy-Schwarz inequality. This is true for L equal one. For m different from one, you have to work with it more. And this is what is done here. Okay, so if for BBT, I write E as the feature information, sorry, I and E for the entropy on the left, then take. Then, taking into account this interpolation, what you get is not only that pressure minus d times the entropy is decaying, but actually that this quantity is controlled by the right-hand side with E prime, which is negative. So now it is a differential OD that you can integrate, and this gives you the first improvement, which is that you control a function phi of the entropy where phi Of the entropy where phi of 0 is 0, phi prime of 0 is 1. So linearization of phi at entropy equal to 0 is just the usual entropy entropy production inequality. And since phi is convex, you get a bit more. Now you manipulate this a little bit and you get the expression which is here. So far, so good. Well, and now you have to go to the details, and that's things I well. Things I well, I want to show it to you because there are plenty of things that happen. For instance, if you just use m equals one, you are limited to the backliner exponent, which is here. A little too sharp. And is the kind of expression that you get for the files. Explicit, but not so simple. So let me turn to the last part very quickly and come back to this inequality within MST. And this is an answer, by the way, to a question raised by. By the way, to a question raised by Henriettism: why the Gaussian? Okay, Gaussian is high-dimensional, okay, large numbers. Think to a statistical mechanics. This is deep actually. Statistical mechanics, you have n particles and you study them on a phase space. Okay, and then you want to do statistics on it. Okay, so you make some averages or you take some marginals of a distribution function when you derive the Boltzmann equation. Okay, and then you have to sum. Okay, and then you have to sum on large spheres. Spheres of dimension 6n, if you have a space space in L3 times R3, and you take the large n limit, or large d limit in this framework. Okay, so this has a name in statistical mechanics as the Poincaré-Maxwell lemma, which is probably not totally appropriate. It's the idea that a sphere of dimension D with a radius. With a radius of the order of square root of d. When you take the limits, it behaves like the Gaussian measure in the asymptotic region. Actually, there is a very nice paper by Meller in 1866, which clearly inspired Maxwell, and where there is a computation I'm going to show to you. So it's an old stuff. So, what you do is you take these Galiardoni-Handberg separation equalities up there and take the large dimension limit, but you take it. Dimension unit, but you take it on a sphere of radius square root of d, or you rescale in that way. And then you make the difference, you take the deficit, you make the difference. And what you observe is, oh, okay, and you take a function which depends only on a finite number of coordinates. So, what did I say? Yeah, now so you have n coordinates, n fixed. Coordinates, n fixed. You take a function which depends only on these n coordinates, and you take the dimension d going to infinity. So your function v is an n-dimensional function, but you use it on this d-dimensional sphere. Now, you write the deficit of Galliardoni-Handel, you pass to the limit, and you get the Cauchy measurement. Okay, so that's a justification. Good. Actually, it is very Good. Actually, this is very consistent with what I have shown to you before. You have on the sphere, you have this deficit. So, remember, this is p equal 1, p equal to the Lagrange interval of k, p equals 2 star, the critical exponent. 2 star is 2 d over d minus 2. d goes to infinity, it goes to 2. So this thing shrinks like this, and when you pass to the limit, you get this. You get that the unmissible range for the fast diffusion flow here is the between. flow here is between P equal one and P equal two, where you have additional turns coming from the flow. So you get some stability results. So actually for P any P between one and two, this also gives you a stability result exactly like before. Okay, so this I don't have time and let me go very quickly to a last remark. Stability for the critical cases. So on the Euclidean state So, on the Euclidean phase, I showed you that under some tail condition, we are able to get a stability result for Soblef. It is a restriction. So, Sobolef, it's the two terms which are here. The urban paralyzed profile are optimal. And it's known from Bianke and Egnell or 91, that there is a constant which controls the distance of the function to the Obantarity function, which are in the optimal Obama function. The optimal open-guarantee function. Now, the question which was open so far was: okay, what is this concept? And you argue by contradiction or you use compactness arguments, so you never get any information with the standalone methods. But actually, if you do it carefully using a flow which is slightly different from the one I have shown to you, then you can make it constructive and get an estimate of the constant beta which is here and the dimensional dependence which is D here. Is D here. Okay, you can take the large D limit, and so sorry, I meant I forgot to mention. So, this is a recent result with Mario Steven, Sophie Gali Rutherford, and Michael Loss. And you cannot get a rate which is better than one of a D because you have test functions. So, T is really the optimal dependence. And when you pass to the limit, then you get a stability result also for the logarithmic sub inequality. That also photologied mixed-belief inequality on Rd or with the Gaussian measure, which is presented here. And what you get is again an explicit constant where beta now is the lim nth of the beta depending eventually on V. They are bounded from below, but eventually depends on view. And here what you get is just the H2 normal. So there is now an open question is which is whether you could really Is whether you could really get the strong norm? Could you get the H1 norm? The answer is no without second moment condition. And with second moment condition, we have some special cases in which we are able to prove something, but we don't have to generalize it. So it's an open question. So let me thank you for your attention. Thank you very much, John. It was not inequality where I think that this is completely the point because the constant was allowed to depend on F, somewhere in the middle. It was depending on F through the tail, the behavior of the tail. So, behavior of the tail you compare with the Bah and Lat. So, the assumption that we have to do for the method. The assumption that we have to do for the method to work is that the function decrease at least as fast as the Baham data at infinity. So you take the mass outside of a large ball, you divide by the mass of the Baham dad outside of this large ball, and take the limit as the radius of the ball goes to infinity, and this has to be bounded. So this is the dependence in the constant. And that's true. If you And that's that's true. If you discount, you you abandon discounts, at least the method fail. The last result shows that you should be able to do something, but we don't know how to do it. Maybe, okay, I should insist a bit more. The fast diffusion flow works on the sphere, but for the subcritical piece. It works on R D, but we have this additional condition. And there we lose the addition. And there we lose the additional term of the carriage domain. So, understanding how to mix the two is something which is passing.