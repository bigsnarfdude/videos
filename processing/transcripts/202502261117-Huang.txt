So my name is Emily. I'm an assistant professor of statistics at Wake Forest and the title of my talk is Using Busy Mobile Monitoring for Analyzing Posture of Hospitalized Patients. So first, uh-oh, I can't go anywhere. So at a high level, my research in digital health is in two different topics. I mostly work with smartphones doing Work with smartphones doing smartphone-based digital phenotyping. We collect smartphone sensor data from patients and medical studies, and I develop statistical methods to analyze those data. And I'm particularly interested in activity recognition, especially using accelerometer and gyroscope data. Recently, I've also begun working with a device called VisiMobile. VisiMobile is used in hospitals, and that's Hospitals, and that's going to be the topic of my talk today. This talk corresponds to a paper we just published. This is with my student, Cindy, and my collaborator, Dr. Clancy Clark, who is a clinician at Way Forest Medical Center. What is VisiMobile? So, this is a wearable device that is used at many hospitals in the US, and the device looks like this. Like this for people online, the device looks like this. It does continuous monitoring of vital signs like heart rate and blood pressure and a bunch of other vital signs as well. It also measures patients' posture. And from the doctor and nurse's point of view, they get to see all of their patients' measurements on a single monitor, one row per patient, where you can see the vital signs as well. Can see the vital signs as well as the posture of the patient. So these data are used in real time in the patient care setting. We have been working with the posture data from VISI, and the VISI device has accelerometers inside. It uses the accelerometer data to classify the patient's posture. And this is done using a proprietary algorithm by Zotero Wireless, which is the company that makes busy. Company that makes busy. From our perspective, we don't get to see any of the accelerometer data, raw or medium-rare. We just get to see the postures that come out. And we get a new posture recording every 15 seconds. The yellow box shows the different posture codes that we can get. U90 means upright, which can be sitting or standing. U45 is reclined, which is having the bed at an angle. At an angle. And then there's a bunch of different codes beginning with L for lying down in different positions. W-O-K means walk, and FO means falling down. UNK is unknown. For Wake Forest Medical Center, VISI is used all the time and it has been used routinely since 2015 on most of the wards in the hospital, pretty much all of the wards except. Pretty much all of the wards, except for the ICU, this device is used. And on these wards, all inpatients get busy mobile from admission all the way through discharge. And here's an example of some data from one patient. The yellow is lying down, then the taller orange is reclined, and the tallest red that means upright. This particular piece. This particular patient had a surgery before they were admitted. They had a surgery here. On day one and day two, they spend most of their time lying, which is shown in yellow. And then by day two, they have also some short bursts of upright, 20 minutes long bursts of upright. In day three, we can see that they have more upright and recline times, and by day four, there are longer upright and recline times. Longer upright and recline times. So, from these data, we get to see how this person's recovery is going. When Dr. Clark told me about Vizi, I was quite intrigued because I think that there are some useful advantages to this. So, number one, this is a hospital instrument that collects different kinds of data continuously. The VISI monitoring is done routinely at the hospital. At the hospital. So, this data is being collected every day. And the data collection and storage are all automated. In fact, at Wake Forest Medical Center, all of the busy data is saved and stored. So, these data are archived. And I suspect many other hospitals also do this. So, this provides a resource for researchers that we could possibly tap, and they could contain a lot of insight for us. For us. With this come challenges, and the main challenge is that these data were collected for real-time use by doctors and nurses. It was never planned that they would be used for research. So we don't get the quality controls that you get in a research study. And these data are messy. You can get errors from different sources, the user, the device. And I think the hardest part for us was going into this fresh, we didn't know what. Fresh, we didn't know what those errors might be. So we didn't know what could go wrong. For us, the research questions that we're most interested in are, can we use these routinely collected busy data to predict patient outcomes? That's very of interest to the hospital. But before we can even get there, we have to ask the question: can we identify the errors that can occur? The errors that can occur, understand them, and then correct them so that we can use these data for research. Because, as is, I don't think they're ready. It is used for all of the non-ICU patients. So, some of any condition. So, pretty much all of the patients in the hospital, except for the very sickest patients in the ICU, some of these patients have. You. Some of these patients have had surgery, some of them haven't had surgery. I thought those are automatically collected by the VC. So what do you mean by errors? I'm going to show a couple examples of errors that we found. There are a variety of errors that can happen, which we've discovered. So here's the first one. So here's the first one. In our quest to find errors, we started with a validation study to see how accurate is Vizi's posture recognition. And this was done on a small sample of patients in the oncology ward where Dr. Clark is a clinician. In this study, we asked patients to walk out of their room, around the ward, and back to their room in a big circle. Patients were also observed over a five-hour period. Observed over a five-hour period. And luckily, we had medical students to watch these patients and see what they were doing. The results were that the Visi device did well on most of the postures, but it had a lot of trouble with walking. And it tended to classify walking as the U90 code. That was the one for sitting and standing, rather than WLK, which is for walking. And the sensitivity was only about twenty-seven percent for walking. 27% for volume. So pretty bad. What we think is that perhaps the proprietary algorithm that Zotero Wireless developed might have been trained on healthy people, perhaps their employees, like 20, 30 year old people. For sitting and standing, it doesn't look that different between a sick and a healthy person, but walking certainly does. So for these unhealthy people in the hospital, their walking can be kind of Their walking can be kind of slow. Sometimes it's more of a shuffle compared to a healthy person's. The way we're dealing with this right now is we've combined U90 and walk into a single category that we call upright. So all of them are being put together right now. After we found that, we decided to dig into the archived busy data at Wake Forest Medical Center. Medical Center. And we've been working with the 2019 data set. This data set has 1,330 patients and over 31 million posture recordings. And it has patients from all different kinds of wards, not just one particular kind of patient. I had a wonderful student, Cindy, who did a very careful exploratory analysis on this data and a lot of quality control checks. And through this process, And through this process, which took us about a year, we found two errors. And both of those errors were unexpected for us. The first one I call record overlap. And what it means is that for a given patient, we found that sometimes the patient can have data for multiple busy devices at the same time. And the way we can tell is in the data set, we not only get timestamps and postures, but we also have the serial number. Postures, but we also have the serial number of the busy device. When we saw this, we knew that something is not right because a person can definitely only wear one of these at a time. You can't layer them on top of each other. For the overlap, it is kind of interesting because sometimes it's just an hour, but other times it could be days long. And for the overlap, it was pretty shocking when we saw. Pretty shocking when we saw it because sometimes it's just two devices, sometimes it's like seven devices. We were really curious to understand why this is happening. And I talked to a lot of people in the hospital. Over time, we came up with a hypothesis. We think it has to do with charging of the device. So, this device runs on a battery and needs to be charged every day. And the way it works at the hospital is that once the device is about to run out of battery, The device is about to run out of battery. The nurse comes, removes the device, places another one on immediately to keep the data collection going. The first device is placed on a charger, and then when that device finishes charging, it goes to another patient. So with these busy devices, in the life of a busy device, it's being transferred from one patient to another every single day. And from the perspective of a patient, they're getting a different busy device every single day. They're getting a different busy device every single day. Fortunately for us, the nurses actually document this process, this very complex process, but it's pretty complicated. So it's very easy for an error to come in. And I think that there could be errors in documenting which patient has which busy device at what time. And I suspect that is the reason behind the record overlap. After we found this error, After we found this error, another one came up, and I call this time inconsistency. This is where busy data can actually appear outside of the hospitalization window. So you can sometimes get data before admission or after discharge. And for this one, we found three possible explanations. One is that the EHR sometimes doesn't have exact admit discharge times. Another one is Another one is sometimes it takes a while for a patient to get a bed. So the nurse will put a busy device on before the person is admitted, which can explain this portion. And then the last one is it could be data from another patient, just like record overlap. And for these red periods, sometimes they're short, like an hour, sometimes they're long, like days. And I think when it's days, it's probably this one. Today's it's probably this one. So, in this case, the first two are, I would say, innocuous. The third one is worrisome. So, we found these different kinds of errors. And to handle them, what we did first was prepare a code that searches all the busy data for these two different kinds of errors. And it will flag all the overlapping records and all the records outside admin discharge. Admint discharge. So, this code is meant to help researchers find what are the suspicious posture recordings. And right now, our current way of handling them is a conservative route. We have excluded the flag records. And what I want to show you is what happens. So, the effect is that we go from about 31 million recordings down to 23 million. And I want to emphasize that just to say that these errors are not. To say that these errors are not trivial. They don't impact just a tiny percentage of recordings. They impact quite a lot. It's 25% if you do the math. So I think for researchers, if you're not aware of these errors to begin with, it's kind of risky because having data from another patient when you think it's all from one patient will make it harder for you to predict the patient outcomes successfully. Outcomes successfully and harder to do reliable statistical inference. So, one needs to be aware of these sorts of errors. That being said, I do think that our approach is kind of conservative right now because I suspect that although these records are suspicious, sometimes it's okay. For example, in overlap, probably one of those streams was correct. So, we're also thinking for the future about how we could refer. Thinking for the future about how we could refine this, and I'll talk about that at the end. Yeah, this is more for us. I've encountered, this is from Eric. Thanks, Eric. I've encountered similar device ID, patient ID, ambiguity. ID, patient ID, ambiguity with CGMs. The CGM scanner has a device ID in the data, but the CGM patch did not. Really good point to point out. Yeah, so it's not just busy. We had a device that got recalled in the middle of a study for that. They found out that the barcode on devices was different than the barcode on the applicators. So they got. Are you working with any help? Easy company? And you try to reach out to them? I haven't yet, but Dr. Clark has a couple times. After I went to the industry session yesterday, I think I'm going to try and insert myself into the group trying to pack home. Yeah, because I see a lot of opportunities here with the busy device. For example, Device. For example, I think we could help them improve their walking recognition. I think we could also help them figure out how to save this data better. That's positive messages. Yeah. Yeah. How long did it take to figure all these things out? It took us a year to figure these out because we really didn't know what we were looking for. So we plotted data. We plotted data patient by patient manually, and through that, we figured out that the busy devices are changing every day. And then I had my student try out different quality control checks. So one was looking for duplicate times in a person's data. And actually, that wasn't able to find record overlap because we discovered that these devices all give you 15 every 15 seconds is a data point, but they're not. Seconds as a data point, but they're not synchronized. So some are at second 0, 15, 30, another is at 3, 18, 33. So that didn't catch it, but we were getting close. Then I had her look at the number of recordings per day, which we know what that should be, and it was sometimes like three times that, and that's when we discovered record overlap. After we found record overlap, I had a feeling that maybe we might have a mismatch of record. Maybe we might have a mismatch of wrong patient to the person.