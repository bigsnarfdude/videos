About high-rate storage codes on triangle-free graphs. Sasha, it's all yours. Thank you. Yes, indeed, you can see the title. And this is joint. Most of the talk is based on this joint work with Gilles de Mour. And if I have time toward the end, I'll mention some other problems and then some other works would be. Problems and then some other works would be involved, but we'll see how this develops. This is a plan, I will not spend too much time on that. Let us first remind ourselves of locally recoverable codes. They took a large part of yesterday's session, and so most of the audience could be familiar with them by now. But to put my talk in To put my talk in context, recall that an LRC code addresses the problem of correcting a single erasure. Hey, Alexander, sorry, we can see you. I don't know if you have your video on. So what is happening, yes, my video is sharing screen, but if you want to see me, I think he's using two devices. Like the video is on the video, and he's using the. I'm using the tablet and the computer, so you know, to make this less confusing, I disabled the desktop. If that's okay. Thank you. Right. So an LRC code corrects a single eraser by correcting by accessing a small part of the code word. Simple examples, as opposed to accessing the entire code word, of course. Simple examples, if we take a repeat. Simple examples: if we take a repetition code, then to correct one eraser, we just need to access one bit in the code word, so that's the smallest possible locality. Single parity check code is an opposite example where we need to access the remaining unerased coordinates to correct the single erasure. Ritz Solman codes locality K and well, we can do locality one if we replicate Ritzolman. We replicate red-solvent, take red-solment codes of length n over two and just use two copies of the same code word. This will depress the rate, but the locality will still be one. There are some other constructions of LRC codes based on subcodes or its Solomon codes with intermediate localities. And there is a general definition due to Parikshit Gopalan and his co-authors 2009, which 2009, which says that a code C of length n has locality R if for every coordinate there is a subset of coordinates, obviously not including that erased coordinate, let's say J1 to JR, and the recovery function such that for every code word, the ith coordinate of that code word equals the value of the Code world equals the value of the function evaluated on the coordinates indexed by the labels from the set R. That's LRC codes as we know them. And indeed, they were discussed yesterday. And today I would like to add additional constraint to this problem, namely, now instead of allowing ourselves to access whatever recovery set we design, let us assume that our access is constrained. Access is constrained. Oh, yes, sorry, before saying that. So, the motivation is, of course, the erasure codes for the coding for distributed storage. Now, we constrain ourselves by assuming that communication between the nodes, the coordinates of the code word is restricted by some graph, such as this one. And the requirement that we faced is that a value of the vertex, A value of the vertex, let's say every node, every vertex stores a single bit, and every bit is uniquely determined by its neighbors. So we force the recovery set upon the vertex, upon the coordinates, saying that you can only look at its neighbors. For this graph, of course, there is still no change, but for the sake of example, let's look at that. We can store, let's say, b1 at x1, b2 at x2. At x1, b2 at x2, and their sum at x3. And then, of course, each of the vertices can recover its value looking at its neighbors. And therefore, we can store two bits of information. And if we denote the log cardinality of the code by capacity or of the largest code possible for this graph, then for this graph, the largest is two bits. And these two refers to the And this too refers to the alphabet size of the code. Sometimes they also consider the best possible alphabet, supremum and Q. And for this graph, this is still two, but for other graphs, increasing the alphabet may be beneficial. Let us look at some other examples. This is a famous example from the literature. Let's say we have this cycle, a pentagon cycle of length five. Pentagon cycle of length five. Again, we write one bit per vertex, and the problem that we are trying to solve is to be able to recover each vertex from its neighbors. So this code of five words, in fact, fulfills that condition, satisfies that condition, and here are the recovery rules. For instance, this says that. Instance, this says that the first coordinate is a logical end of coordinates two and five, and this works for each of the five code words. And the other rules can be also verified by hand. So, this code indeed is a good storage code for this graph. So, its log cardinality or the binary capacity, in fact, log cardinality of this code is, of course, there are five code words. So, log25. Words so log25, but this happens to be the largest binary code with this property for this graph. And if we increase the alphabet, we can get from 232 to 2.5. And in fact, we don't have to go that far. Q equal to 4 would already suffice to attain this value. A few references, so foundational work on storage code. Work on storage codes. They were introduced by Arya Mazumdar independently and by Alex Demakis and his students around the same time. And then it has gradually transpired that some version of the index coding problem is more or less equivalent to the storage code problem, where for the index coding problem, we view this graph as the side information graph of the problem. And there is yet another version of this same problem. Of this same problem, one variant of guessing games on graphs. Guessing games were popular in the years 2000, were also known under the name of head guessing, hat color, head guessing games. And one version of those indeed is equivalent to storage codes on graphs. Let us give a definition. We start with a finite and directed graph, connected for that matter. Connected for that matter, I should have written, but let's assume once and for all that our graph is connected. Let's say there are capital N vertices. The neighborhood of a vertex is the set of all vertices u such that vu is an edge in the graph. A code over finite alphabet q, a code of length n, so one coordinate per vertex, has the property. Vertex has the property that for every code word and every vertex, every coordinate, the value x subscript v is a function of the vertices in the neighborhood. It's exactly the LR definition, except that our recovery groups are now forced by the graph. Rephrasing, we could say if two vectors x and y in the code coincide. Coincide on the neighborhood of some particular vertex, then also the values of those code words at that vertex are equal. So it is the same requirement as the previous line. In general, of course, the recovery relation doesn't have to be symmetric because here we are assuming that if u is a part of the recovery for v, then so is v for u. But if it is not symmetric, then our graph would be directed. Be directed. We will not spend that much time on that, mention just briefly the directed case. We define the rate of the code, so we compute the log cardinality normalized by the length and take the largest possible code on this graph for a Curie alphabet. So, this is the parameter that is of interest to us. Perhaps I could. Perhaps I could pause here and ask if there are any questions, because this is the main definition that we shall be using. If that's understandable, we can move. All right, let's move. Maybe I keep this video after all. Not such a big deal. All right, so let us briefly look at some known constructions. The edge vertex construction works for Construction works for alphabets of size q square where q is the number of edges. So again, our alphabet is square of the number of edges. And we want to place symbols from the alphabet q square on the vertices. Instead of doing that, let us start of placing curie symbols on the edges, such as here in the pentagon. the Pentagon. Any announcement is permitted. Sorry, any assignment is permitted. And so there are cardinality of the set of edges, possible assignments. Having done that, let us place on a vertex two symbols that are written on. That are written on the edges incident to that vertex. A and B. So A is on quote unquote the left edge and B is on the right edge. So implicitly we assume some order of the edges. It doesn't matter which, it just has to be fixed. And we shall assume that it's fixed for the purposes of this talk. So we place AB here, B C, and so forth. And to recover a vertex XY. To recover a vertex x1, let's say this is erased. We look at the left neighbor and take the second symbol, and we look at the right neighbor and we take the first symbol. And this is the symbol of the vertex x1. This works for any of the five vertices. So this gives the promised code. For instance, if the alphabet of size 4, sorry, yes, the alphabet of size 4, then Of size four, then q is two, and so there are five possibilities for the edges. So there are two to the five thirty-two possible code words, and this gives rise to a code of rate log432, one over five. So this is before dividing by one by five, we have 2.5 and well, maybe let us stop at 2.5. Let us stop at 2.5. So, this is the promise 2.5 result. So, this is the edge vertex construction or the rate one half if we go to the rate. And indeed, for any regular graph, this yields codes of rate one half. So, one half is an easy value to attain in this construction. Construction. The same result can be accomplished using matchings. For instance, in a regular bipartite graph, there is a perfect matching, and then we simply place an alphabet symbol on every edge in the matching. And after that, we spread those symbols to the vertices. So instead of looking at the entire neighborhood, the vertex simply looks at its partner in the matching and recover its value. And recover its value, and again we get to the rate one-half. We can quickly generalize this construction by moving from edge to vertex to click to vertex. So if there is a click in the graph, we simply place a single parity on the click and then we distribute the coordinates to the vertices that form the click. And if every vertex is incident. If every vertex is incident to the same number of k clicks, the construction gives a very high rate, k minus on over k. On the next page, I have a little example. Let's look at this graph. We can partition this graph into two clicks, this triangle and this edge. And edge is a click on two vertices. So we place a single parity on the edge, x1, x2, and this single parity on the triangle. And we have And we have three free bits in the encoding getting to the rate three over five. So that is making use of the clicks in the graph and going back to the previous page. For the complete graph, obviously, on n vertices, we attain a rate very close to one. And well, we already mentioned a few equivalent problems: index coding and guessing gauge. Index coding and guessing games. So here they come with some references. Let us not spend more time on that. Instead, let me mention a couple of bounds for storage codes. The log cardinality of a storage code, QR storage code in a graph, is at least the size of the largest matching and at most the vertex cover and minus the independence number of the graph. And there is an extension of this upper bound for directed graphs, where instead of the independence number, what we see is the size of the largest directed acyclic graph in G. Right. So let us now state the problem that we shall consider as we saw the presence of clicks. Presence of clicks makes it easier to attain higher rates, and in particular, if the graph is itself a click, then the rate is close to one. So let us start with the basic case when there are no clicks at all, meaning that there are no triangles. So if there are no triangles, these forces that the graph have have no clicks. And indeed, this may be the first problem to address. So, what is the largest size of the binary linear storage code? My checks, my recovery rules will be linear, namely simple parities. And the graph is triangle-free. Moreover, let us narrow it down a little bit further, namely, let us say that all the vertices, all the variables. That all the vertices, all the variables in the neighborhood are essential. So they all enter the parity. This doesn't have to be the case in general, but this is the case we consider. So we say that we consider full parities for the recovery of the vertex. All right. Let G be a connected graph, no triangles, AG, adjacency matrix. So in the adjacency matrix, of course, every row. Course, every row accounts for the neighbors of the vertex. The vertex itself, though, is not included in the adjacency matrix, but we need to include it because the vertex has to be a part of the parity or otherwise we cannot recover it. So, what we do, we add the diagonal of all ones. And this is the matrix that we consider: adjacency matrix plus the diagonal. Our code is linear, so. Our code is linear, so every row is a parity. So our code in general is nothing but a kernel of this matrix, A tilde. If that matrix has small rank, therefore our storage code has high rate. And this is what interests us. This is the underlying first step in our constructions. So, this is the problem, right? Is the problem, right? Connected graph without triangles, construct graphs such that this modified adjacent symmetrix has small rank. By the way, this simply corresponds to adding self-loops to every vertex, right? The diagonal in the adjacent symmetrix, this is simply a loop. All right, this is the problem. One half is attainable for, for instance, for regular graphs, as we have already convinced ourselves by the Already convinced ourselves by this simple edge-to-vertext constraint, edge-vertex constraction. Can we get rates greater than a half? Initially, in fact, it was conjectured that one half was the largest possible. That was in the context of guessing games. That was refuted in 2016 in this paper, Cameron Down Grease. And this is the graph. And this is the graph that does better than one half. So, this is a well-known graph in algebraic graph theory. It even comes with the proper name. It is called the Klebs graph or a folded cube. This is a unique, strongly regular graph on 16 vertices, five regular graph with these parameters. So, this says that every pair of That every pair of adjacent vertices has no common neighbors, and every pair of non-adjacent has two common neighbors, and that's a unique graph on 16 vertices that has these properties. So we write this adjacency matrix just looking at the graph, add the diagonal, compute the rank, for instance, by computer, and it turns out that the rank is six, so divided by 16, it's three. By 16, it's three-eighths, and the rate, therefore, is five-eighths, which is more than a half. So, that's an interesting example observed in this paper. Now, what this paper did not notice, but it is also true that this graph is a coset graph of the binary repetition code of length five. Repetition code of length five. So let us talk a little bit about coset graphs because they form the core of our construction. We have a code such as this little code of length five. There are two to the n minus k cosets. Vertices of that graph are the cosets of the code. And two cosets are adjacent. There is an edge between them. If the distance between them, so these are If the distance between them, so these are the cosets c1, c2, little c the distance between them, the cosets as sets is one, the Hamming distance, then they are connected with an edge. That is a standard construction well known in combinatorial coding theory, Cosgraph. Of course, gamma can be also viewed as a Kelly graph of the group F2 to the g g g g g g g g g g Of the group F2 to the n minus k in the previous example, F2 to the 4. What's a Kelly graph? Well, we take the binary space F2 to the 4 and we let's go to the previous page maybe. We connect two vertices if there is a column in the parity matrix such that we can add that column and obtain. That column and obtain this new vertex. So that's an action of the set of generators given by the columns of the parity matrix. They act on the syndrome space. In other words, again, rephrasing every vertex corresponds to a syndrome in the syndrome space, causes the syndrome that's more or less the same thing. And if we can move from one syndrome to another by adding a generator in the pair, I A generator in the pair, a column of the paracheck matrix, then we draw an edge in the graph. So, in this graph, there are five generators: the canonical generators, and there are some. And it is now clear that this indeed corresponds, moving from a vertex to another vertex corresponds to Himing distance one between the cosets. So, that's the coset graph, and in general, and this. And in general, and this graph on 16 vertices is indeed a costed graph of a binary code. All right, then. So we have now two types of codes. There are small codes, such as this of length five, and big codes, such as the code of length 16 in our example. And when we construct coset graphs and then their storage codes, we go. And then their storage codes, we go from small codes to big codes. This process can be described as we just discussed. We start with a small code, length and dimension n minus r. Let S be the set of columns of the parity check matrix. A, the adjacency matrix of the cat graph on the space of syndromes with generators given by the columns of the parity check. Given by the columns of the parity check matrix. We add the diagonal. By the way, adding the diagonal now in terms of the calligraph is simply adding zero to the set of generators, because what is generator zero? We add it to the syndrome and it goes nowhere. So it means that the vertex stays in place. So that's the self-loop. So that's adding a new generator. And we finally consider the big code, which is a binary linear storage code, which is the kernel of the Storage code, which is the kernel of this matrix A tilde. Now, yes, what about the triangles? It's very easy now. If the distance of the code is at least four, then this caused graph has no triangles because there are no three columns in the parity check matrix that add to zero, that are linearly dependent. So no distance for no triangles. No distance for no triangles. So it is easy to enforce this condition using this language. And this is what we shall do. So motivated by that, are there examples of codes on coasted graphs of codes of high rate? Yes, for instance, the binary goali code has rates 41 over 64, which is clearly greater than a half. Half and two correcting BCH codes. All of this is done by computer, by the way. Analyzing is difficult, at least for the moment. We can offer no theoretical analysis of these families. So to err correcting BCH code of length 2 to the s minus 1, dimension is length minus 2s. So the graph is on 2 to the 2s vertices. And if we compute the rate, And if we compute the rates for four to eight, we get these values. So the rate keeps increasing for eight. This 2 to the 16 by 2 to the 16 matrix, so it's 65,000 by 65,000, which is a good billion entries. And it's quite amazing that the computer can still handle it. But yes, it can, even though it starts heating up. But the next case is already not reachable. So that's where we stop. But in principle, That's where we stop. But in principle, nothing seems to contradict the idea that in the limit, this rate could be one, which would really resolve this problem of constructing storage codes, at least in the asymptotics. Right. So these are the examples. And let me take a brief detour by saying that I was very fortunate to work with Gilles on this problem because this connection to Causet graph. This connection to Causet graphs, in particular, this repetition code. It had turned up in Jill's earlier work on quantum codes from the CSS Calderbank-Schwarstein family. I will spend very little on quantum codes in general because it is a sidetrack for this work. But just to remind ourselves that if we have a code of length and binary code, we generate a map. Lengths and binary code with generator matrix A such that AA transpose is zero. So the dual of the code is in fact the subcode of the code itself. This gives rise to a quantum code of length n and dimension n minus twice rank a qubits. So we consider or rather in this work by Couver de Fause de Mont. Work by Couverd and Fossemore some years back, they considered codes, quantum codes that are formed that are given by the calligraph of the repetition code of lengths r plus one and of course dimension one with generators exactly as we discussed in the general case, proving that the rate of the code is half plus one. One over two to the n plus one over two. So as little n increases, the rate goes down, ultimately approaching one half. So in fact, the largest of the rates is obtained for R5, the example that we have observed. Now, they have also computed the distance of those quantum codes, which is in fact more difficult than computing the rate, or at least as difficult. But we shall have no use for the But we shall have no use for the distance in our work. We are interested in the rate of the storage codes. Right. So we know something, we know what we can obtain from the repetition codes, and this takes us slightly above one half, but not that much. There are some necessary conditions that adjacency matrices or Kelly graphs give storage codes of high rate. Storage codes of high rate. For instance, if we take the set of generators S of size N, then if N is odd, then we always get full rank. So the rate of the storage code is not very impressive, namely zero. For even N, on the other hand, the number On the other hand, the number of generators. Remember, we had the code of length five plus the zero generator, so n was even, so a transpose is zero. This is a little lemma. And so the rank of the matrix is at most n over two. And this means that the rate can be greater than a half. This condition can be extended. All of these are necessary conditions, not sufficient to answer, but still they. Sufficient to us, but still they enable us to at least rule out codes or small codes that will not give rise to high-rate storage codes. The conditions are as follows. Let me maybe show just one, the last condition. If the rate is greater than three quarters or something like that, two to the k minus one divided by two to the k, then the k minus first sure power of the dual code. Sure power of the dual code is contained in the code itself, where by sure product we mean the coordinate-wise product of the code words. So this condition, in fact, indeed helps us to weed out implausible candidates among small codes. Now, the main constructive result of this work: there are other results such as the necessary conditions, the examples, there is little more there. Examples, there is a little more there, which I will not mention. It's in the paper, of course. But the main constructive result is this family of codes, storage codes of rate three-quarters. By the way, as far as infinite sequence of codes are concerned, this is the largest known rate at this point for storage codes. So this is, in a way, an interesting code family to keep. interesting code family to keep in mind. Now, this claim about the largest rate, I'm sorry, the only known family should be taken with a grain of salt, because of course, if we have a code of high rates, such that the code on the Klebs graph, we can take as many copies of that graph as we want, building individual codes on each of them, and this will go into infinity. And this will go into infinity, maintaining the rate. So, this still carries the rate throughout. But if we ignore those families and look for quote-unquote genuine constructions, then this is the only known construction. And the BCH example is also gives the best known value of the rate, as we discussed. So, three-quarters, we don't quite get to 0.8, but But we still have an infinite assembly of codes. Right, so we start with the parity check matrix of the extended Heming code, which appears here, of course. And add the zero column, and then we add one row. One row to this matrix, and this we can do for every R. So, this accounts for a family of small codes. How do we add this last row? Well, we need just two things, that it carries one in the zero column, and for instance, one one elsewhere. And this we can do for every R. So, this is indeed a family of small. So, this is indeed a family of small codes. Looking at the Hemming matrix, what is the Kelly graph that corresponds to it? Well, it's quite clear. It's in fact a complete bipartite graph because if we have a syndrome that ends in one, then whatever generator we add. Whatever generator we add, the new syndrome will end by zero. So the syndrome space is naturally partitioned into two parts. And since there is every column in the possible column in this matrix, we have a complete bipartite graph, so its adjacency matrix is as shown here. And the problem that we are really solving is what happens with the What happens with the COSED graph if we add one coordinate to each of the generators? Now, this problem is not as easy to analyze. I will not say much about that because it gets technical apart from maybe just mentioning a general idea. So, we study what happens if we add one coordinate to each of the generators. Our vectors are of length 2 to the r and we view those vectors as functions. So, the adjacency matrix now acts on those functions as shown here. So, we even can call it the adjacency operator because now it acts on the space of functions. On the space of functions, and it is possible to decompose that operator depending on the last coordinate of the generators into a0 and a1 and do the same with the functions because now this function f can also be viewed according to the zero or one and this action recursion of this action. Recursion of this action can be handled with some effort because it is possible to compute the dimension of the kernel of the desired matrix using the sum subspaces related to those matrices from the decomposition that we just discussed. And that's about as much as we shall say about that. So there is some kind of recursion in those operator action. In those operator actions. And this indeed yields this family of rate three-quarters. So I would like to mention a few open problems for storage codes in the wake of those results, in the wake of that construction. Of course, constructing codes of rate one should be possible, might be possible. Uh, should be possible, might be possible, and uh, what happens for two BCH codes? What happens for second-order Ed Muller codes? The recursion in the previous page is vaguely reminiscent of the UU plus C construction. I'm not sure if there is a link, but at least you know that jumps to mind. So I will spend maybe five or a few more. Maybe five or a few more minutes on a variation of that problem. Storage codes in general, in recent years, I have been interested in various codes that correct erasers on graphs. One of them is the storage code problem. With my student Edway, we studied regenerating codes on graphs. Regenerating codes, of course, is a big topic in distributed storage. I will not speak about it today. Will not speak about it today. And there was another line of work, again, graph-related that of recoverable systems. So recoverable systems are nothing but the same problem, but for infinite graphs. With my former postdoc, Ohad Lisko, we studied one version of this problem, but today I would like to say a few words about this. Say a few words about this rather recent work with Ohud, Ryan Gabriz, who already mentioned by Venkat in his previous talk, and Etienne Jacobi, on recoverable systems on infinite graphs. So let's be specific. Consider graphs, a graph of integers, z to the d, maybe d1 or two. I will not say much about two. Let's say one. So there is a So there is the set of integer numbers by infinite graph. Let us say this is 0, minus 1, 1, and so forth. We write bits on the vertices, assign a bit to each of the integers, and again ask ourselves what happens if we require that, let's say, every vertex can be recovered from the left neighbor and from the right neighbor. Left neighbor and from the right neighbor. And that for every each of those vertices from minus 15 to plus 17, and if you think that the quote-unquote rate of this code is one half, you are of course right. But let us first say a few words, maybe give a definition, then say what we mean by the rate. The definition is as follows: Q is a finite alphabet. Finite alphabet. We our recovery set is now denoted by R. So in this example, the recovery set is neighbor to the left and neighbor to the right. R recoverable system, X on Z, a set of bi-infinite sequences Q to the Z such that for every vertex there is a function that That recovers the value of that vertex. Looking at the vertices, so we shift xi by the recovery set. Namely, here we shift by one to the right, one to the left. So our recovery set is plus one, minus one. And for instance, to give a construction in this for this system, we assume that the That the vertex is repeated, every bit is repeated twice, x to i plus one is x to i, then the recovery function is x. Well, we look either to the left or to the right, depending on the parity of the vertex, of the vertex index. Let's define the capacity of the system. For that, we restrict the words in our code. words in our code to a set n and let's say from 0 to n minus 1 let us say bnx is the total set of all the all the words of length n that appear in our code then we define the rate or the capacity log cardinality of that set normalized by by n by the size of the set take limp soup that's a very standard definition in many similar contexts such Contexts, such as, for instance, constraint systems, well known in coding theory, and then we are interested in the largest attainable capacity of the graph. So, capacity of that recovery set is the largest attainable rate of the storage code that uses that set for recovery. And there are a couple. There are a couple of results that we had. For instance, let us say our recovery set is as follows. So, this is the erased vertex, and then we look at R segment to the right, at L segment to the left. And we permit ourselves to use those neighbors for the recovery of the erased coordinate. What is the capacity? Well, we have a complete answer. If M is minimum between L and R, so in this case, it would be L, then the capacity is M minus 1 divided by M. And the proof, it uses the click-to-vertex construction that we discussed. And for the upper bound, it uses this direct acyclic graph, upper bound. Graph upper bound directed acyclic graph, and in fact, the two values coincide. And then we have several other similar results for Z and Z square. And overall, this looks an interesting problem, also a new one to coding theory. And there are some combinatorial connections, in particular, to Bill Sart's bound, code and anti-code bound, well known. Anti-code bound, well known in many combinatorial coding problems. And this paper was just accepted to this ISIT, forthcoming ISIT. So we hope to present this in detail in a couple of months. That's it. Thank you. Thanks for your attention. Thank you very much, Sasha, for the nice talk. Are there questions for Sasha? And again, as usual, feel free to. And again, as usual, feel free to unmute yourself to ask the question. So, I had a question on the alphabet size, Sasha. Yes. Yeah, for these constructions on triangle-free graphs, did you focus on the binary alphabet or? Yes, yes, yes, yes. Yes. So is it known that the rate can approach one otherwise for triangle-free? Yeah. No, no, no, it's not. So this three-fourth. So, this three-fourth is or the whatever the 0.81 is the best known in general as far as you? Yes, if we discard this, you know, this funny idea of replicating one graph infinitely many times, then yes. So, when you computed the rank of these things, you only tried the field F2, you didn't try like F3 or other things and see no, no, we didn't. No, no, we didn't. And you see, no, not quite. Because for large matrices, it gets difficult. For small matrices, we could have done that, but no, this has not been done. Okay, cool. More questions for Sasha? Okay, if not, let's thank Sasha again. Thank you very much. Again, thank you very much.