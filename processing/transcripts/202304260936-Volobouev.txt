So that you know what the shape of a light niggle is for that. And I just wondered if you'd actually tried sort of combining the results from sort of two samples like that, which presumably ought to give you the same answer as you would get from having one overall sample of eight paid times, say. I have done some stuff like that. I don't remember exactly. It was a long time ago. Yes, but. Yes, that's something one can do when there's a useful test. To the best of my memory, we do pass that test probably by the way. Well, you started with a third definition of error, and the one that statisticians like because they like to distinguish between the error and the distribution of the error. They like to conceptualize the uncertainty of the distribution. In terms of the distribution of the error. And if you do conceptualize the uncertainty as the distribution of the error, which might be theta hat minus theta, or maybe you might like to think about things like theta hat over theta in some problem. But if you do, then the central limit theorem is the leading term in an approximation to the large sample behavior of theta hat minus theta. And there's a correction, an asymptotic. An asymptotic correction, but you were talking about models in which the Gaussian curve was kind of not far from being right. So if it's not far from being right, then an edgeworth expansion with one more term with this unit explicitly in it as the incorrection. It's not always a PDF, the edgework approximation. Sometimes there are, if it's too skewed, it can get below zero at some places. Zero at some places. But if you wanted to say, well, now I want to combine, but you combine approximations into another approximation by doing exactly what you said. Adding means, adding variances, adding skewnesses, and writing the result. Now, that's not the convolution of the two approximations, but it's a good approximation to the convolution of the two approximations, and it's easy to do. I mean, if you have a skewness. If you have a skewness, you, I think, were showing us how you would get the skewness, but the statisticians in the room would probably like to analyze the skewness of theta hat minus theta directly, rather than thinking about the likelihood function that might come from that, I don't know. When I'm talking about skewness, I'm talking about skewness in the PDF. Yeah, so am I. I'm talking about the skewness of theta hat. Yep. Hacked. And as a random quantity with the distribution depending on theta. And so Edgeworth expansions combine in a very natural way. They're approximations, but so was your Gaussian limit. I did have a look at Edgeworth expansions for likelihoods. I didn't get very far with it. They didn't seem useful. I didn't think of applying Edgeworth expansions to people. Applying eight-worth expansions to PDFs, and that is definitely something I shall go away and try. Thank you. So when you talk about a normalized skewness, this is just the sort of Q months, right? Yes. Okay. Because normally when people talk about skew, they divide by sigma cube to get a damage in this content. Well, thanks, I had a question, but first, just comment on what Louis said about the lifetime example. As Louis Tommy, that is a proportional gap. That is a proportional, what do we call it? Proportional Gaussian. Proportional Gaussian. Proportional Gaussian, which we use that functional form a lot, and I was claiming there are actually not any examples. Really, what our detectors do it, but then he pointed out his little paper on that. So if he's checked proportional Gaussian, he's probably checked the examples. The question is on slide 15. I think I really quick business of likelihood. And we, in the past five stats, there's been a lot of discussion about how from the likelihood function itself you can't get goodness for in your likelihood ratio. But so just clarify what you're doing. This difference is expressing the goodness of fit of 33 to the 30. This difference between. Oh, so it's a difference in line. Oh, so it's a difference in log likelihood. So it is a ratio. Okay. Let me ask if there are any questions from Zoom. Yeah, I wanted to go back to understand that. So in the original example, you make a minus one sigma, plus one sigma give you different deviations from the value that you get at zero sigma, right? So in an experimental setting, you're basically saying that this might correspond. Saying that this might correspond to taking three estimates, right, for variance minus one, sigma, plus one, sigma, minus zero. And we are, we marry the variance at zero as our standard result. But in this case, this corresponds to the median and not to the mean. Yes, if that's why I have always biases coming in. Yes. Yeah. So could one decide to take the mean and give weight to the minus one sigma plus one sigma estimates and Sigma estimates and make that problem disappear, the normalization problem disappears. You wouldn't make the normalization problem disappear. It would be hard to argue. You could try with your colleagues to say if you've got an asymmetric distribution and you want to quote the mean rather than the median. Mean rather than the median, you should have a bias. There are other places where we actually like to quote medians rather than means. When you are adding stuff in producture, then you really do get pushed around in the biases. And that's, I didn't, I think I gave the numbers on the shades, but you can actually talk them through for reasons of time. But those biases ought to be included, and it may turn out to be. May turn out to be reasons for argument when we get to actual cases. Because one could take the attitude of just not caring about mu equals zero and only caring about mu equals minus one plus one. So I what we what it's just a straight line that the incarcerated point of view. Um no. No, we want to keep. We're trying to get from a Gaussian distribution in U to the appropriate distribution in theta hat. If we can map out truly what that functional dependence was, we could do it exactly. We don't, so we can't, so we have to do it as best we can. So I guess maybe I could in phrase a Maybe I could uh erase a little bit what Master said. How bad is it to replace the sigma plus and sigma minus with sigma plus plus sigma minus over two and use that? I would say if you could possibly get away with that, then you should. But there will be cases, this is probably one of them, where you really can't. Nobody's asked me for my backup. Nobody's asked me for a backup slide. I have one question for the statisticians, which is have you ever had to deal with this sort of asymmetric operations that all does? That's the thing. I don't know how common this is as mathematical. It doesn't happen at all, but I can actually talk to you quite a lot about that. I mean if for Bayesian the posterior calculation sure that's kind of no but that's so that's fine but then when when you do have a distribution you can go and you know you can code the distribution way and just have linear distribution just three numbers which is you know like the central value so I guess that's like yeah so you can take PDF out of the columns right because if you have the whole PDF If you have the whole PDF, you could do everything. Exactly. So you're saying I guess that's the kind of question I had is that you know somehow the mean, the variance, and the skewance. Okay, yes, true. So that leads me to my second question, because I was just introduced to Edgeworth expansion. You know, and we've puzzled a lot in publishing some of the combination results that we do. Because you have, like, I don't know, a factor of 10 parameters. A vector of 10 parameters of interest. And then what we publish is we publish those central values and we publish a matrix which is covariance. Yeah, covariance matrix. And it has always been obvious to many of us that these are, you know, it's an extension and we are only giving one term. And then there are things which are highly asymmetric and therefore there should be another term. What you just said is that can I translate it into we should publish central colour, some sort of symmetric clearance and then the skewness? And then the skewness? Well, your formulas involve the skewness, so I assume that you have the skewness available. What I'm saying is, what we have is the central value and the sigma plus and the sigma minus. From them, within one of my two models, we can get the skewness of the variance. It's kind of a price to make the skewness. Yeah, under an assumption called the model and stuff. I mean, sometimes with you know so publishing the likelihoods, you know, publishing the P D F's you could do that. Publishing the PDFs, you could do that too, right? Yes, so that's the thing. So, this is where, you know, this Edgeworth series, would it be a properly statistically well-founded way for us to publish the different moments of a multi-dimensional likelihood? I mean, though maybe you want to think more about the the Edgeworth expansion itself has the difficulty that it's The difficulty that it's you're expanding like the log of the characteristic function or the moment generating function. And then you're inverting the result, you're truncating the thing after a certain number of terms, then you're inverting the result to get back a density. Inversion might not produce a density. Not guaranteed that the inversion produces density. Guarantee that the inversion produces it. The part that's problematic is the dipping below zero. But it doesn't always do that. I mean, it's just that sometimes, you know, depending on the size of the skewness. If the skewnesses are small, not huge. And the point is that the skewnesses combine very nicely, right, across independent, adding independent measurements. And they typically diminish relative to the very... Diminish relative to the variability. COVID. And the edgework expansion will get better as you average more objects in. And you're doing, I mean, you're just doing moment calculations, feeling the coefficients. And there are no varieties. There's a lot of skewness terms, right? Right, because third moments are the tensors, the order goes up, the rays go up. So there's a conceptual variance, there's an analog for this cumulus. There is actually an alternative. You can keep track up to the fourth cumulative, then you could use uh spa subtle uh subtle point approximation, which I think Alexandra would a lot prefer to HR's expansion. Short eight months expansion. But then you have to keep up to the fourth quarter to keep certain things correct. Absolutely. So just two points. Actually, following this conversation about sort of how close the up and down uncertainties have to be before you sort of start treating it as symmetric, the values you showed right at the very beginning of your talk did look like good cases where you might. good cases where you might just take the the average of two and the sort of uh plot of like one point five plus one minus point nine that's a little if i had been doing this i think this would have appeared as eventually errors but this is not my my experiment the other thing i was going to say was about um correlations uh and andre alluded to it um you haven't mentioned how you deal with correlations Mention how you deal with correlations between these asymmetric things, but presumably has to wait until you. I think that would add another element of complexity to already quite complex situations. For instance, you were saying your variances add up when you have well, they don't look if they're correlated. So you can we go back to Louis' the example of the lifetimes, and I wanted to ask you to try to connect this to the discussion about the energy exponential expansion. So in the lifetime example, you're estimating the mean of an exponential, and suppose you imported the maximum likelihood estimate and a central 68% confidence interval. And so that's how you get your asymmetric error. And then suppose two people have done that, and they're both very low-sample things. They're both very low sample things. So you have one measurement that is one plus 0.5, minus 0.3, and the other is something also with an asymmetric shape. They give you these numbers, and then they disappear from the base of the earth, and all you have are those numbers that you want to then combine. So, in that case, would it not be more appropriate to say, because we know that those are MLEs of an exponential mean, you know that. mean, you know that the sampling distribution of the MLE in that case is gamma distributed. And so from the confidence interval and the MLE, you could reconstruct the sampling distribution of your MLE and then you could basically cobble together a proxy for the likelihood for each of the two measurements and then combine at that level. So the point is that if you have more information, so therefore you can say that they were, that MLE was. You could say that MLE was gamma distributed. From the central confidence interval, you could cobble together the parameters of the gamma distribution. Suppose it wasn't a lifetime measurement, that you could say it was kind of like a lifetime measurement. Why not use a gamma distribution instead of an edgework? Because I think that's absolutely right. I mean, it's one, I mean, the gamma distribution is going to be a, it's going to have a truncation on one side. And, you know, so you better have a reasonably No, so you better have a reasonably large uh shape parameter that you know that comes to prevents the truncation from bothering this previous discussion at that uh to Andres point about like basically like what what to publish. So I guess the huddle question really is like do you want to publish the sampling distribution of your estimator? The sampling distribution of your estimators, or do you want to publish something like a confidence set? These are kind of the two different perspectives here. And I think, especially in the case of asymmetric constraints, it leads to different answers in the sense that if your sampling distribution, as was really nice to find out here with the figure and there with the bands, if your sampling distribution has a right skewness, then it tends to be that the confidence tests have a left skewness. So it's kind of exactly exactly the opposite thing. You said the opposite thing. So I think this is a kind of a case where it actually matters. If everything is symmetric, then it doesn't really matter which one you're reporting. They all kind of look the same. But then you have this asymmetries, now it becomes kind of important. Are you reporting the sampling distribution of the estimator or confidence that they give excellent the opposite answer? So, I mean, it will be kind of interesting here. Like, what are you focusing right now, and what would you like to focus? I was asking this in the context of, let's say, simple. Think is in the context of, let's say, simplified lifelinks. Because it's a real hand. I think Sarah was the one who asked: please do not give us software that takes realized parts of our lives and then it changes again. So there's been an effort, and Nick can probably say more about it, of trying to package our likelihood functions in a way that it's a bit more than just central values and covariate spectrums because we know that there are PCC and PCs. These are seeing the fees. Right, right. So then it's not so much a question of like when we think about the moments of a distribution through that. How do you what are the expansion for the lightning, which I guess brings up the moments of that? Where is a moment of a lightning actually? Maybe much simpler, but we started out with a new which was very nicely after semantic encouragement. So what if the new that I'm starting for is also? So what is the new that I'm starting to form is already isomorphic? Um yes, I'm just trying to make it simple, but before rapid it again complicated. Um yes, one would have to go into further algebra. To follow that, I'm sure it will be possible. And if you have a particular case, I would love to hear that. Yeah, so the image which you have to be mathematically. Elisandra? Yes, I'm thinking. Okay, I really hope I don't say something silly. I just try to understand what's going on. So the idea is we have measurements. You know that those measurements behave asymmetrically, right? And you try to get some vaccinations for the PF. Okay. And uh so what you're saying is you can't use Is you can't use the normal because they behave symmetrically, and the normal is a symmetric distribution. The question to the business is: do you know about the asymmetric normal distribution? So Andre mentioned, you have an idea of what the expectations mean of the variability and of the skewness. And the asymmetric normal distribution has these three parameters. Its parameter aspect normal has Parameterized like the normal, has an additional parameter, behaves nicely like the normal does. So, for instance, you can with parameter theorems of driving forms as you do for the normal, but the shape of the distribution is asymmetric. So when I said, I hope I don't say something silly. But I'm not sure whether this is a solution, but I remember a colleague of mine who did some work on using... I have to look it up. I'm using this asymmetric normal in the context of astro statistics where the shapes involved in the measurements were symmetrically. I was not aware of this. Thank you for bringing it up. I will add it to the Gamma distribution or the it work distribution as things that we should try as well as this. As well as this. Thank you. Can I just Google asymmetric model? And he is a mandatory professor at the University of Padre. Actually there are several definitions of things which are normal, but his definition is actually the one who goes first. I think it's the PDF is twice the human. Yes, I got it. Actually, it's an AZA. Adza. But asymmetric. Yeah, actually, if you could go to slide nine, there's a use case that is usual in particle physics, but we don't quite bring it up. And that is ionization density along tracks. Oh, Landau distributed. That is Landau distributed. And that's the distribution of the PDF. So the measurements are very asymmetric, but you have many, many of them. And the traditional thing to do is to try. And the traditional thing to do is to truncate the mean, is to chop off the tail of the light though, because it doesn't tell you about particle IV. It does tell you about average ddx, because sometimes you're interested in the mean, and sometimes you're interested in the most probable value, because one tells you about energy, and the other one tells you about particle ID. So, how would you use some of the things you're talking about for, let's say, DDX measurements? Yeah, the Landau distribution has no variance and no mean. So you have to impose cut-offs somewhere. It's not quite Landau because Lando goes on forever and like DDX doesn't. Yes, yes, your Lando distribution goes up to infinity even for a 20 MeV particle. And have a go. Can I say something about the latest? I mean for the standard distribution, it's no I mean for the standard distribution, it's known if you do a variable transformation of one over square root of this analytic, that then then, you know, then it gets more version and then we can combine it better. And I think for certain cases, this is also a recipe, right? Like a transformation. But cutting out outliers is a different thing because outliers don't tell you as much about what you want to know and it's better just to ignore them. Yeah. The delta rays in Tails on Landhouse are usually don't tell you about particle ID search. The distributions we got here might be absolutely type Gaussians or Edworth type Juda Gaussians. I really hope they're not random distributions. I don't think there's anything in the mathematical physics that would get us to that particular problem. Just on that point. Just on that point, I think that that would fall into the category of what I tried to say earlier, and that is that if you knew that sampling distribution of whatever you measured was like a Lambda distribution, then you would take whatever information you have and you would try to cobble together a proxy for the likelihood you would then combine at that level. So if you think it's a Lambda distribution, then yes, you could cobble together a new likelihood based on Landau distributions. And you would fit beta gamma or something. Okay, if there are some more questions, we are almost on time. 