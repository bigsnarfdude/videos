And let me start by acknowledging that this is work in joint collaboration with a group of very talented people. So, there's Samir Karam, he's a PhD student at Concordia in my group. We won is a former postdoc in my group. At Concordia, Nick Dexter is an assistant professor at Florida State. Nicola Rasfranco is a postdoc, Politecnico di Milano. Benako was my former postdoc mentor at SFU, and Sebastian Moraga is a PhD. SFU and Sebastian Moraga is a PhD student at SFU. Clayton Webster is a distinguished researcher at the University of Texas, Austin. Also, I'd like to thank the support of ANSER, FRQNT, Concordia, CRM, and PIMS to make this research possible over the years. So, okay, we're here. There's no secret that deep learning is going to be revolutionizing our lives, AI. We've seen a bunch of breakthroughs over the past few years, starting from a half ago, and then advancement. Ago, and then advanced editing in the computational biology alpha fold, and the latest breakthrough Chat GPT that now really kind of aggressively entered our day-to-day lives. And also, this workshop is motivated by the success of deep learning in traditional, to solve traditional problems in scientific computing. Stuff like learning dynamical systems from data or solving partial differential equations with detailed matter. And there's this larger movement that perhaps this workshop. Larger movement that perhaps this workshop is part of is called scientific machine learning. So, kind of try to use machine learning for scientific computing and hopefully to solve scientific computing problems as efficiently as we can. So, this is great, there's a lot of excitement, there's a lot of promise of deep learning. However, I think it's necessary in this context to kind of think about the theoretical foundations and to kind of prove theorems that guarantee that our methods based on deep learning. That our methods based on deep learning work well. And to kind of inspire my talk and maybe some of the other talks that will follow in the workshop, I want to mention this article by Sigma Smale that I would like is called Mathematical Problems for the Next Century. There's a list of 18 problems that Smale wrote, kind of invited by Arnold, who asked several mathematicians to write a list for the 21st century inspired by what Hilbert did for the twentieth century. Did for the 20th century. And the last problem, listed by its male, is called limits of intelligence, problem 18. So, what are the limits of intelligence, both artificial and human? So, it's a really, really big question. We're trying in our kind of small scope to tackle this question. But what I like is also in the description of the problem that Smail was pointing at a highly interdisciplinary approach to tackle this challenge. So, I was looking at a direction where learning, problem-solving, game theory. Learning, problem solving, game theory plays a factor role together with the mathematics of real numbers, approximations, probability, and geometry. I really like this quote because it really describes what's happening right now. A lot of experts from different disciplines are tackling these challenges in AI and in particular in deep learning. So, inspired by my smell satient problem today, I will try to convince you that there's a new tool that, together with some collaborators, we introduced called practical existence theorems. Called practical existence theorems. It can be useful to prove theoretical guarantees for deep learning-based methods in scientific computing. Specifically, I will focus on methods that try to solve problems in high dimensions. And I will consider three case studies. First, problem of approximating a high-dimensional function, then the issue of reduced order modeling in parametric PDEs, and then solving PDEs defined over high-dimensional domains. So, this will be the three. Dimensional domains. So, this will be the three case studies with which I will try to convince you of the kind of flexibility and efficacy of this framework. So, oh yeah, disclaimer, I feel a bit of an imposter here because there's not going to be any time-stepping. Somebody apologize. Hopefully, it's not going to be too disappointing for you. Okay, good. At the end, I think there is scope to apply to combine what I'm going to talk about, especially in the application to PDEs, with time-stepping methods, but there's not going to be anything about it. But there's not going to be anything about it in this particular talk. So, yeah, that's to set notation, that's probably an overkill. But okay, we all know what a field-forwarded neural network is by now. Just a composition of affine maps and nonlinear activations like radio, for example. And just to set up notation, I will mention that to train a neural network, you typically minimize some loss function, for example, mean square error loss. But you could also do something more, right? But you could also do something more, right? So you could regularize your tradable parameters, for example, and you could consider minimizing the L2 norm of the parameters or L2 norm squared. Actually, here there should be a B hour, it would be more correct. And that's similar to what's called rich regression in statistics, where you could have a one norm and that will promote sparsely in your trainable parameters so you have kind of sparsely connected neural networks. That's more similar to the LASSO optimization problem. Great, so. Great, so let me talk about this first case study, high-dimensional function approximation. So, first of all, the motivation of the problem. We want to, in this first problem, we're going to approximate a function from point-wise samples. And we are assuming that the function f is defined over a high-dimensional, d-dimensional domain, u, and d is much larger than 1. Typically, you know, in applications, f would be representing some black box model or parametric model. Box model or parametric model where you have some input parameters. You're going to think about a big, maybe parametric PDE that is involved in some aerospace simulation. And so you give the parameters as input. You maybe run a DNE and then you solve your PD, that that's your output, or maybe you observe a quantity of interest in your solution, that might be a scalar output. And then you maybe want to, you know, every time you sample the solution, you need another numerical solution. You need another numerical solution of the numerical resolution of the model, for example. And so that could be very expensive. Therefore, to do stuff like uncertainty quantification, people have considered trying to build a surrogate model for the black box. And so that's where function approximation can be useful. You might want to approximate f, for example, with a polynomial. And that's what we're going to be talking about at the beginning, or with a neural network, as more random approaches do. And this can be applied this paradigm. And this can be applied, this paradigm is very general, can be applied to many, many sectors. Of course, the main challenge is the so-called curse of dimensionality. This is a terminology introduced by Bellman in the 60s. It refers to the fact that if you try to just try a simple approach to this problem, you will stumble very easily upon an exponential dependence on D, on your computational cost, or on the number of samples that you need to approximate F. You need to approximate F. Just think about if you just have a uniform grid, you tensorize a uniform grid of two points each dimension, you already have two to the d. So computational cost is going to explode very fast here. So you have to do something better than that to tackle the so-called curves. Great, so before talking about practical existence for deep learning methods, I will do a little detour because the practical existence theory relies on several fundamental elements. One of these Fundamental elements. One of these is the emulation of neural networks, sorry, of classical objects like polynomials, of rear functions, big neural networks. The second one is going to be convergence theory for compressed sensing-based approximation. So now we'll do a little detour to talk about compressed sensing approximation, in particular Sparks polynomial approximation via compressed sensing for this problem. So probably everyone is familiar with Legend polynomials in one D. Jean polynomials in 1D. If you want to approximate a d-dimensional object, you just tensorize them. Now we're assuming to be in a hypercube. So you have a family of d-dimensional orthogonal polynomials of the hypercube. And if you have a function L2, then you can just simply have an infinite expansion of this object with respect to your orthonormal basis. However, to be clever, you will like to want to produce an efficient approximation. So you want to consider here a sparse vector of coefficients. Here, a sparse vector of coefficients. Maybe we want to compute an approximation that has as few coefficients that are non-zero as possible. So, something of this form, right? So, you want to have an approximation f hat where the coefficient, the L0 norm that counts the non-zero entries of the coefficient, is as modest possible. So, that's kind of the beginning of the story in sparse approximation. So, is that a reasonable idea? Well, it depends on which class of functions you're trying to approximate. Class of functions you're trying to approximate. Sparse approximation is not always the best thing to do, but under suitable regularity assumptions, for example, one regularity assumption that we'll consider in this talk is this property of a function having a holomorphic or analytic extension to a so-called Bernstein polyllips. So that's kind of a classical object in approximation theory that allows you to give convergence rates for polynomial approximation, cathodes. Approximation methods. So, if your function is smooth enough to admit a holomorphic extension, then you can actually prove convergence rates for S-term approximation. So, if you consider the best S-term approximation error, so essentially the best linear combination that you can pick of polynomials by only activating S of them out of the infinitely many ones. If you take the best linear combination, you are able to get an error that goes to zero with this rate. So, essentially, exponential. With this rate. So essentially, exponential in this term, s to the 1 over t. And there's also other types of convergence rates that you can see. They're algebraic type. In that case, that will go to 0, like s to the minus some power is actually independent of d. So this is one possibility to kind of give a convergence rate for sparse approximation in this case. And that's not an ideal assumption. So it actually happens in models that are relevant in practice. For example, if you have a parametric. For example, if you have a parametric diffusion equation where, or a parametric harmonic oscillator, or a parametric heat equation, or a PDE where the domain is parametrized, the geometry of the domain is parametrized in a smooth way. In all these cases, you can show that the map that brings the parameters, sorry, maps the parameters to the solution to your PDD has this type of regularity. So, that's something that has been studied in quite detailed by people. Quite detailed by people like Cohen de Borschwab, and then a lot of literature has started since then. So, we can consider this kind of approximations, sparse approximations. How do we compute them? Well, we want to use cover sensing because we want to sample the function as little as possible. So, we will consider m random samples of your function. Okay, first of all, you truncate the function. You have infinitely many Legend polynomials. Have infinitely many Legendre polynomials, you cannot have a method that deals with all of them. So you have to pick a truncation set that is large enough. For now, I'll tell you a bit more about that later. So let's truncate in a large enough space. You sample F and M random points, uniform IID points in the hypercube, and then you set up your under-determined linear system. So that's a typical feature of compressed sensing. Again, we are dealing with a high-dimensional problem. Again, we are dealing with a high-dimensional problem. The set lambda would be very large. We don't want to have as many samples as the big approximation space. So we'll end up with another determinant system with many more columns than rows. However, the unknown signal that you're looking is sparse or approximately sparse. So, this problem, there's hope to solve this problem thanks to sparse recovery or sparse regularization. And this was something that also. This was something that also a method that has been studied over the years. It's the first papers in 2011, 11 and 2012. But now, one of the best things you could do is like trying to minimize a regularized weighted L1 minimization problem. So there's a recipe to pick the weights of the L1 norm. The L1 norm, just so I forgot to put the definition, the L1 norm is something like that. So uj. Is something like that, so uj at some value of zj. So if you put the weights in your L1 norm as the L infinity norm of your polynomials and you minimize this regularized loss that combines a data fidelity term with the sparse regularization term, then you can build your sparse approximation from data and you can prove a convergence theorem that I will show you now. And here I want to mention the square loss. So that's one particular choice of optimization. So, that's one particular choice of optimization problem that you can pick, and I'm a big fan of. It's very popular in statistics, not as popular in numerical analysis, but I'm trying to kind of promote it because I really like it. The fancy feature of square LASSO is that, so if you remove the square root, if you only have the last two, you would have a square hour here, okay? The square data field editor. Square glass, so you simply remove the two, and now you still have a convex problem, but what you're gaining is that. Problem, but what you're gaining is that the optimal choice of lambda will be independent of the noise corrupting the samples. Okay, so here I'm not talking about noise, but of course there could be physical noise corrupting or numerical noise corrupting your data, and that's always the case in practice. So that's why this could be very useful. Also, another thing is that B, the samples contain the truncation error by definition. Okay, so we are truncating this expansion. So by definition, This expansion. So, by definition, they will always be affected at least by the truncation error. So, sample error is kind of by definition within the problem. Therefore, picking the optimal lambda in a way that is independent of that is very desirable. All right, so that's the compressed sensing approximation method for high-dimensional functions. And one of the contributions that we had with Benako, Clennam-Webster, else later. Leno-Webster, and also later with Nick and Sebastian Moraga in an improved form, is to have convergence rates for this kind of approximation. So, if your F can be extended in a holomorphic way in version of polylips, if you want to approximate F from M samples, and if you pick your truncation set in this particular way, that's a so-called hyperbolic cross of order S minus 1, then you can achieve a convergence rate that is S. That is as good as the one that I showed you before for the best S-term approximation error. But now the good news is that F hat is not just an ideal best S-term approximation where you pick the best S-term, S coefficients. Now F hat is computed from data. So now with M data points, you can achieve something similar to the best M term approximation up to this tilde. Okay, there's a tilde here where, so it's essentially up to log factors. Essentially, up to log factors, up to a log cube m and log d factor. But it's good that we only observe a log d here, which means that the curves of dimensionality is somehow lessened. So we say that this is a near-optimal recovery guarantee for comprehensing approximation in high dimension. And this is, of course, a guarantee in high probability because the method is a randomized method. You draw your samples at random. There will be more to tell about how to get this guarantee, but since compress sense. To get this guarantee, but since compress sensing is not the focus of this talk, I will just tell you that the hyperbolic cross is really important because it's the smallest multi-index set. So here, the polynomial, these are indices corresponding to polynomials. For example, 0, 0 is the constant polynomial, or 0, 1 for polynomial that is constant in the first variable and linear in the second one, right? So we move in this polynomial multi-index space. And if you consider lower sets that are Consider lower sets that are multi-index sets that do not have holes, these red ones. The union of all S sparse lower sets forms precisely the hyperbolic cross. And it turns out that lower sets are particularly good to have nice approximations of holomorphic functions. And these are kind of key ingredients that need to put together this theorem. That will be one of the main building blocks that we need to establish this so-called. We need to establish the so-called practical existence theorem for deep learning. So, again, I'll stop here for compressing. So, if you want to know more, there's a shameless plug for the book recently published in collaboration with Ben Artko and Cleo Webster by Siam in 2022. You can go to the companion website, sparsehdbook.ca. But now let's move to back to deep learning. So there was a bit of a tangent on conference sensing, but that's one of the key elements we need One of the key elements we need to prove the theorem. So, back to deep learning. There's been a lot of work in deep learning recently, in the deep learning approximation theory, actually not so recently as well, starting from, I think, in the 80s. But recently, it really exploded. So, people are able to prove so-called universal approximation theorems for different classes of functions. For example, Soblev. For example, soble functions, function soable spaces, piecewise moved functions, belly unit functions, and so on, even holomorphic functions that we're interested in in this particular talk. Universal approximation theorem usually tells you for, so fix your function, the function, sorry, the class of functions you care about, fix an accuracy level of sine, then there exists a deep neural network that is able to approximate a function in that class with accuracy of sinum with respect. With accuracy design with respect to a certain norm that is suitable for your function space. And that's great. Sometimes you're also able to give explicit architecture about, like, okay, the network has to be at most this deep or at most this wide, there's at most non-zero a number of non-zero weights. However, there's some critical questions that remain unanswered by this type of universal approximation theorems. First of all, First of all, how do we obtain, can we obtain such a network from data? So theoretically, it says, okay, there exists a network. Sometimes the proof is constructive, but it doesn't tell us, okay, can I compute that network or a network like that given some samples? And how do I compute that? Which kind of loss do I minimize? Do I need to minimize a regularized loss, something like that? And also, how does deep neural network approximation compare Approximation compare with, for example, near-optimal approximation schemes like polynomial approximation via compress sensing. So, these were the two key questions that we had in mind when we put together this practical existence theorem framework that relies on what I was talking before. So, that's the theorem. That's how it looks like in the case of function approximation. Again, we would have a function f that can be extended in a holomorphic way to a Bersonite polyllips. Now, we take data, so that's your training. Now we take data, so that's your training set, MIID uniform samples in the domain. Again, we modify M tilde up to log factors, that's the sampling complexity M. Now we can prove that there exists first a class of random deep neural networks for which we have explicit bounds for the depth, the number of trainable parameters, the number of non-zero parameters, and all these parameters depend at most polynomially in M tilde, in the number of samples up to log factor. And the number of samples up to log factors. We also need a regularization functional, R. And then we're able to prove that any minimizer of this regularized loss, so that's very similar to what we would do in practice, that's kind of a mean square error loss with a square root factor plus a regularization term. If you solve this problem, then you can compute a neural network now, it's not a polynomial, a neural network that is near optimal. That is near-optimal. Okay, so you have this near-optimal exponential convergence rate, similar to what we had for Besses-term polynomial approximations. This framework can be generalized. So in the paper, in this paper, we have a generalization to the Hilbert valued setting. In another paper, we also have discussion of the Vanach valued setting, where you have some extra technicalities to deal with, but it can be even generalized further beyond the scalar value. Further beyond the scalar value case. So that is the theorem, and I would like you to give you a quick sketch of its proof. So, the high level, I think, the proof is reasonably easy to be included in a talk. So, that's how the class of neural networks looks like. So, up to the second last layer, we'll call the network phi lambda delta, and the motivation will be clear very soon. And then we're going to call the last layer Z. The last layer Z. In our proof, and that's the main limitation of the argument, only trained layer will be the last layer. Okay? We only allow to train Z, and then on the other hand, we explicitly construct the network up to the second last layer. How do we construct this network phi lambda delta? The idea is simply to emulate Legend polynomials. But you could do that with other basis functions. For example, with Legend polynomials, we just exhibit a network. We just exhibit a network. So you have to construct a network phi nu delta that can approximate the Legend polynomial of psi nu, where nu is the multi-index corresponding to your particular degree, your particular basis function, up to an accuracy that ideally infinity naught. And these type of constructions are available off the shelf. For example, in this paper, you can find the one for Le Jean-Polynogans. But typically, you could do them for your own basis. Your own basis, the usual first building block is the observation that neural networks can approximate products. So, ready networks can approximate products between numbers, and then you start building up from there. Like, you can approximate products between D numbers and so on. So, you can approximate polynomials, you tensorize them, you kind of build it from scratch to approximate a polynomial object. And then you train the only trainable layer here with the square lasso program. With the square lasso program. We can also use the lasso here, but again, I like the square lasso, so that's the formulation with the square lasso approach that I talked about before. Only the last layer would be trained. And then you get, so essentially this is going to be an approximate polynomial comprehensive problem. Because now these are networks that approximate polynomials. Therefore, your assembling matrix will not be exactly the comprehensing matrix we had before. The compressing matrix we had before, but it would be close enough to it to make the whole compressing convergence theory work properly. And that's the final training network that you consider. And then you just invoke the convergence theory that I showed you before from sparse polynomial approximation. To have a picture in mind of this network construction, you can, I was sketching this yesterday, so I didn't have time to do the proper tick-z thing, but that's with the iPad. So that's how the construction looks like. This would be your D input variable. This would be your D input variables. These are the modules of your network that approximate polynomials. So each of them are, you know, go back to a scalar valued, go back to a scalar. This is the first Legendre polynomial, the second Legendre polynomial, and so on. And you take all the Legendre polynomials in a big hyperbody cross. And then you join them in this final linear linear, which just realizes the linear combination. And you train that. So essentially, this neural network. So essentially, this neural network is emulating a polynomial approximation scheme. Then, of course, in practice, that's how the theory works. In practice, you could do fancy things like: okay, maybe you have this architecture, you can use this as an initialization, and then you start training everything from there. That's something you could do. But there's no guarantee that they do approximate the right portion. No, so, okay, this we construct. So, we construct this piece of network so that it will approximate. Of network so that it will approximate the regional genomics. I was just mentioning you could use this not just as a theoretical idea, you could use this as an initialization, as an initializer, and then you run from there. Let's say, okay, an initializer scheme that I know at the beginning will do as well as polynomials. Maybe it will do better if I squeeze more performance, more accuracy. That's how the big network looks like. And yeah, final discussion about the practical existence theorem for. Practical existence theorem for function approximation. So, first of all, you can think about practical existence theorem as an enhanced universal approximation theorem in which there is an extra aspect involving training. And then we train the network from data. The proof stars is very general. I showed you the argument for holomorphic functions and emulating Legend polynomials. You can do pretty much anything. You can have your favorite class. You can have your favorite class of function, your favorite basis, your favorite basis. Now you construct networks that emulate that basis, then you just link them with the last layer, retrain the last layer, you will have the argument working, assuming that you also have a convergence result for a compressed sensing type approximation. You could also work, by the way, compressing is not the only option. For example, you could even consider a least squares-based approximation and do Based approximation and do a practical existence theorem based on that. Another selling point, I think, is this. So, other results that I'm aware of that involve deep learning approximation from data, usually, to the best of my knowledge, you always see the Monica rate, one over n. Not aware of results that get away from this, those that are based from typical machine learning generalization models. Instead, here we see a neural. Instead of here, we see a near-optimal exponential rate in M. I think that's a big selling point, but I'm happy here if there's any expert in the audience that want to prove me wrong about this. I'm happy to go to the paper that proves me wrong. I actually would like to know that paper. And yes, another selling point, I think, that the class of networks is sparsely connected. If you look at this figure, this is not a densely connected network. So this is structured in blocks, so not all the connections are implemented here. Are implemented here, so you never go from here to here, for example. And within each module, if you inspect the construction, they're also sparsely connected within each subnetwork. Great, you also check in practice that effectively this example with a parametric 30-dimensional diffusion equation. So there's 30 parameters, not a third-dimensional domain. So the domain is 2D, but 30 parameters. So, the domain is 2D, but 30 parameters. And the continuous line is compressed sensing approximation as a function of the number of samples. This is the L2 norm approximation error. This is the H1 norm approximation error. Here we're approximating actually a Hilbert valued function. All the dashed lines are, so it's very tiny things, but these are all different network architectures. We were trying to tune the network to see how. To see how it compares with comprehensive. And the thing is, the practical existence theorem, sorry, I didn't emphasize the Gaps between theorem practice. So the practical existence theorem tells you only that deep learning is able to do as good as copper sensing, but in practice it could do better. So you can actually outperform copper sensing, and that's what these numerics are showing you. The neural networks can do better. And also, of course, let me emphasize again, I told you before in the Again, I told you before in the construction, but only the last player is trained in our argument. How to overcome that bottleneck is, I think, a non-trivial research direction and might require some non-standard new ideas. So yeah, so in practice, if you're careful with tuning the network, even for various move functions, you could outperform cover sensing. Although, yeah, if you don't get the architecture right, conference would do better. So here, again, there's a gap between. So here, again, there's a gap between theorem and practice, but a practical existing theorem tells you that deep learning has the potential to do as well as a neuro-optical scheme, like in this case. Okay, so maybe it's a good time to stop for questions on this part, if you have any. Because then I will move on to reduced order modeling, which is a bit different. See if there's any burning questions. Yes. Just as a small technical question. Yes, you said before. You said that you resolved for. Your result for where the activation function is the Ready function. Ready, yes. But do you actually use that it's precisely the Ready function, or do you just use properties of it? The main thing here is that really a RAID network can approximate products. And then you start building polynomials from that. Any activation that will be able to replicate products, you can do the same construction. Alright, so maybe take more questions for. Okay, so maybe take more questions for later if you have any. Reduce order modeling, that's the second case study. By the way, how am I doing in terms of time? Okay. Okay, so you don't know how many slides is left. Carry on to four. Okay, good. Right, because I started a bit early, so probably have like 15 minutes or something. All right, good. So, second case study is reduced order modeling. Case study is reduced order modeling. So, okay, we already have heard people mentioning this in talks and coffee breaks. So, this is a second application to showcase the practical existence theorem framework. In this case, I'm thinking about having a parametric PD as your problem of interest. So, maybe an operator depending on some parameters, mu. You can think about that, for example. You can think about, for example, a domain, that's your physical domain. Domain, that's your physical domain, there's some subdomains, and maybe you have a parametric diffusion equation, and in each subdomain, the diffusivity constant could have slightly different values. So this could be your parameters, right? So the diffusivity value in each of the subdomains. This could be the parameter you're moving. And then typically a full order model would map the parameters, a choice of the parameters, into a solution. Here we're assuming to discretize and write in the solution, for example. Discretizing radio solution, for example, and in this case, you would have a mesh on this domain and then having the vertices of the mesh. So, the full order model is what you want to approximate. So, from parameters to the full resolution solution, so to speak. And in the so-called DL ROM, so deep learning-based disorder modeling, what people do is to actually train a neural network to approximate this parameter to solution map. Parameter to solution map from a training set of so-called snap socks. Okay, so just set up parameters and a set of solution values. But that's still a function approximation problem. The only difference is that now your function is vector value. Still the same framework. There are related methods that will not be talking about, but that they're worth mentioning because they also have similar goal, depot nets, and neural operators. In particular, I'll show you a practical existence theorem for auto-encoder-based deep learning reduced order models. So, in this case, we have three spaces that are important. Again, the solution space, NH is a big number. So, we can think about your very fine mesh, and like the number of vertices in this mesh, or the number of maybe triangles, whatever you want to discretize your solution on. L R L would be a latent space. So, somehow a space that captures the key latent features behind your data set of solution snapshots. So these are kind of hidden features in your data. And think about it as a kind of a high-dimensional clustering, kind of high-dimensional set of clusters, and maybe the latent features tell you, okay, in which cluster you are. And yeah, RD is the parameter. And yeah, RD is the parametric space in this application. So, how people do this in practice is as follows. So, you first train a so-called autoencoder from the solution space to the latent space. So, you need an encoder and a decoder. And the encoder brings you from the solution space to the latent space, the decoder brings you back from the latent space to the solution space. And you want to train them so that they're. Train them so that their composition, so encoding and after decoding, approximately gives you the identity on your data set of snapshots. But of course, you want to have a very small layer space, have this kind of bottleneck. The usual figure to have in mind is something like that. So this is your R and H, this is your R L, your latent space. So you squeeze this to kind of make the network learn only the relevant hidden features behind the data, because then it will be forced to replicate the data exactly as much as it can. And there's a third network, a so-called reduced network, that essentially learns the parameter to latent variables maps. Okay, so it goes from the parametric space to RL. And after, so there's different ways to implement this scheme, but one way could be to. Implement this scheme, but one way could be to train first the encoder and the decoder on your data. Then you train the reduced network, but actually you discard afterwards the encoder because you don't need it to build the deep learning reduced order model. You only need the decoder and the reduced network. That's going to be your reduced model, okay, for the solution. Okay, great. That's the method. And again, I started working. And again, that's. I started working in this field. I'm not an expert in this particular field, but I started working on this problem with Nicola Radastranco, who's a postdoc at Politecnio di Vilano, who has a lot of experience on this. And we were able to prove together this practical existence theorem for that particular application. So here the things to notice: okay, so first of all, the parametric space for us can be very high-dimensional, but we are forced to have a 1D physical domain in this first result. We think it can be the other. Result. We think it can be, the argument can be generalized to higher-dimensional physical domains, but the construction is not trivial. So assume you discretize that with an equispace grid over the 1D domain, and then your solution map takes values actually in a smoother space like HS over with that. Then we can prove a convergence rate of this form. So here we have a uniform control over the greed point of the approximation error of your reduced order model with respect to the Of your reduced order model with respect to the true parameter to solution map. And we control, so rho is a random probability, it's a probability distribution that you use to sample your data. And then we control the error in the L2 sense using that probability measure. And we have two sources of error. So that's interesting because first we have something similar to what we had in the scalar-valued function approximation case. This exponential. This exponential decay rate, because again, we're assuming that we have a holomorphic expansion. So our parameter solution map at least a holomorphic extension. So we see the near-optimal rate here. There's this extra square root L, the square root of the latent dimension space. This depends on the fact that now you're trying to approximate vector-valued functions, not scalar-valued functions anymore. So this is, you can think about this as a sampling error in a way. As a sampling error, in a way. This depends, essentially. You can improve this by taking more samples, by taking a larger training set, having a larger M. This is kind of a modeling error. So this involves latent dimension L and the smoothness of your parameter distribution bound. So if S is not that large, for example S1, the minimum possible, so that this will decay like one over, this term will decay like one over square root L. Square root L. So you'll have to take, you know, you have to make that go to zero, taking a large enough latent space. But then all of a sudden, if you have a very smooth parameter to solution map, then L can be smaller. So this is capturing the fact that if the solution manifold is very smooth, then this can be captured by a smaller latent space, right? So now we have two, again, the practical resistance theorem tells us two things here: one about the sampling and one about this kind. One about the sampling and one about this kind of design of the data space in the order encoder. And similarities to what we had before, of the three networks involved in our argument, only phi, the reduced network, is the one trained using the regular light clause we saw before. Instead of the argument, we explicitly construct a psi and psi line. In particular, psi is a convolutional network, and we did that because also practitioners have noticed that it's convenient to have convolutional audio encoders in practice, so we wanted to have. Auto encoders in practice, so we wanted to have that in the theory. And also, we have explicit architecture bounds for the networks involved, not for psi prime though. That's the only one where we use an implicit argument. So, that's something to work on. It's an open problem. And yeah, so this hidden cost of actually, there's an inflated norm of f, just that you have enough space to put it here. That's it. So, that's the second case study. Maybe another, if there's any other question about. Other, if there's any other question about that part, otherwise I'll proceed with the last. Any questions? Alright, yeah, so I'll conclude with the high-dimensional PDE application. So now, so far we talked about high-dimensional functions or high-dimensional parametric PDs. Now instead of high-dimensional PDE would be PDE defined over a high-dimensional physical domain. Okay, first of all, motivation of PDE based on PDE solvers based on deep learning. Solvers based on deep learning. They're some very popular early ideas starting in the 90s, maybe even before. And so, you know, recently, very popular kind of wave due to the pins area. And then a lot of people have been trying to use deep learning to solve high-dimensional PDs in particular. Sorry. So, my question was: okay, can we apply this framework to a PD solver based on deep learning? So, I have a convergence. Deep learning, so I have a convergence theorem for this particular application. Elementary PDEs are relevant in a series of applications. We heard this morning about the Fokker-Planck equation, with this many other computational chemistry, also in finance, the Black Scholast model. Here we will consider a model problem, very easy, but high-dimensional, so this makes analysis accessible. I will consider a high-dimensional diffusion equation over the totals. So, periodic boundary conditions, we also have this extra. Conditions. We'll also have this extra zero average condition just to have a unique solution to this problem. And so, focusing on this particular model problem, first again, to put together a practical existence theorem, first we need a comprehensive sensing-based convergence theorem, and then combine that with a neural network-based emulation of some basis. On the Taurus, the natural choice would be the Fourier basis. Would be the Fourier basis. So, for the conference sensing part, there's a literature that we can use. And I actually started working in this area. That was my beginning of my journey in research. That was my PhD work, where I was working on petrol-galerkin methods for PDEs based on copper sensing. But before people had proposed Galerkin methods for the PDEs based on copper sensing, and then also Fourier-Galerkin methods were considered with this very interesting work about sublinear type. Work about sublinear type sparse recovery algorithms for identical LEs and also spectral collocation that I will discuss here that I've been working on together with my former postdoc, which you went. So the idea is it's very simple. You pick a spectral basis, your truncation set lambda, could be a hyperbody clause that we had before. You truncate your solution, you take n uniform samples from the torus, and then you sample, you create the sampling matrix and the right-hand side, simply And the right-hand side, simply by sampling the PDE operator evaluated at the basis functions and sampled at the collocation points. That's going to be your comprehensive matrix A and then your right-hand side B. Now, using sparse recovery, you can do this by taking only a few collocation points, so an underdetermined linear system, and then doing L1 minimization to compute the solution. So we picked the three bases. So, we pick the three basis because of the periodic boundary conditions. And there's an easy case where, you know, if the diffusion coefficient is just identically equal to one, then this is just a Laplace equation. And if you rescale the Fourier basis in a suitable way, then you apply minus the Laplacian to it, you get back the Fourier system. So, in that case, minus the Laplacian applied to your basis is a boundary of the normal system. So, we're back in a scenario. System. So we're back in a scenario where we can apply standard compression theory. On the other hand, if the diffusion coefficient is not identically constant, then you have to be more careful. So you'll have to consider the PDE operator applied to this rescale free basis. This will not be an orthonormal system anymore, but we can prove that it can be a bounded resistance. So that was the main effort with my work with my Orthodoxy one. Of course, the witchy one. And then you could use compressing theory in bounded risk systems. That was a recent contribution with other collaborators from this paper. And then you can prove sparse recovery guarantees similar to what we have for fractional approximation, but now for a high-dimensional spectral collocation method, where essentially the number of collocation points will scale logarithmically with the dimension B. So, really, we had the convergence theory part. And in terms of In terms of the neural network emulation, so that's work in progress. I was hoping to have the archive by today, but it turns out it was too hard to just submit it by today. So, hopefully, we'll submit by the end of the month. So, check out the archive in the next few weeks. But the main building blocks needed to prove the practical existence theorem in this framework are the following. So, in this case, we need to, we have kind of a nastier, sorry, we have a nastier problem here because we don't only need to Problem here because we don't only need to replicate the Fourier system, but we need to replicate a differential second-order operator applied to the Fourier system. So essentially, if you just emulate Fourier, you also then need an error bound, not only the L-infinity norm, but the L-infinity norm of the first and second derivatives of your approximations. So that makes it probably more challenging. We bypass that for now. We bypass that for now. That will have to be probably improved in the next work. But for now, in this paper, we're just doing exact replication of Fourier functions. And this could be done if you're using a rep unit, so a rectified polynomial unit. In that case, you can build networks that exactly reproduce Fourier if you also use the so-called periodic layer. That's something that people have proposed in the PINS framework. It's an initial layer where you're allowed to use cosine activations to. Cosine activations to enforce the network to be periodic by design. So if you use these two elements, you can actually, perfect, I'll catch your applicate 3A, and then we combine that with a convergence 34, compressive convolution. So that's the roadmap towards the practical existence theorem. Okay, so last few remarks. Hopefully, I try to convince you that practical existence theorems are kind of enhanced, universal approximation. Enhanced universal approximation theorems that give you neural optimal convergence rates for neural networks from trained data showcase that application to three-case studies. The argument is really easy. You first emulate bases with neural networks, then you leverage convergence theory from other fields like compressed sensing or could even do these squares-based approximation. Several open problems. So, first of all, okay, the work in progress about high-dimensional PD solvers that I mentioned. Are the classes beyond holomorphic? Other classes beyond holomorphic, we're working with Jason and others, and Martian Neumann and Anders Pineda on approximately ben limited functions. So, having a practical existence theorem for that class. There's many open questions. So, the most important, the fundamental one about the tool itself, okay, how to handle more than one trainable layer. That's kind of a hard thing to overcome. Time-stepping for this workshop, I think it's possible to combine, for example, PD application, time-dependent. Application, time-dependent PDs, you could combine compressive spectral collocation with your favorite time-stepping method. And I think it's possible to put together a practical existence theorem for that. Also, practical existence theorem for dynamical system discovery, that's something we've been chatting about with Jason as well. And there are other problems. If you have other problems of interest that you apply deep learning to and you think this could be a good framework, I'm happy to talk about it and convince you that this can be done. Okay, thank you very much for your attention. This is the link. Thank you very much for your attention. This is the list of references. Thank you very much. So any more questions? Thanks. Yeah, that was great. So the, I don't know if this is actually that important, but your practical existence theorems. So you have a, they give you a finite depth to your network, right? I mean, a fixed depth, you network. Right? I mean a fixed depth. You never there's nothing in the yeah the network the class the the network class has a fixed depth. It's not like allowed to kind of go arbitrarily deep. But the width comes from the delta, right? How you want to approximate the polynomials to that? Right. No, both, okay, no, both the depth and width will depend on delta. Yeah, yeah, there's a dependence. Here I'm not showing it, but the paper we track all that dependence. Yeah, you actually have architecture bounds depending on this delta. Architecture bound depending on this delta. And yeah, so if I remember correctly, both depth and width depend on delta. But once you fix that, then, yeah. Okay, then my real question is: do you know how your depth and width for these compared to other universal approximation theorems? Yeah, it will be the same. So we inherit, yeah, this was a universal approximation theorem for particular construction with polynomials. So we inherit those limitations from that type of construction, but then we kind of Type of constructions, but then we kind of augment that with the additional compress sensing trainable layer. So, in a way, whatever, you could combine this with whatever universal approximation theorem construction you have, and then you put an untrainable layer on top of it. So, in a way, you inherit the limitations, right? So, it's really a higher level. This framework is kind of a higher level kind of box to combine more elements. So, I can't remember the So I can't remember the exact balance. They're very messy. They're at most put an onion in M. But this is not pleasant formulas. We have an onion in the paper or not. As good or better at the C of the R. Yeah, exactly. Yeah, yeah, yeah. We narrated that from C of the R. Yeah. Okay. That's really good. So most compressive sensing results, one RIP condition is needed. Yeah. Do you need an analogy of that? That's a great point. Yeah. That's a great point. Yeah, so yeah, that's where we need it here. So we get, so that's the real compressing sampling matrix. We actually have an approximation to that sampling matrix because we are approximating polynomials. So we're sampling these approximate polynomials on A. So usually in the compressed sensing theory, you would have to show the RIP for this matrix. Now we need to show the RIP for this approximate matrix. And actually, we do something slightly different. We go through the robust null space property. I know if you're Robust null space property, I don't know if you're familiar with it, but it's something that is implied by the RMP. So the RP is a stronger condition than the robust null space property. And that's what we use here. And the robust null space property tells you that the kernel of the sampling matrix cannot have vectors that are too close to be sparse in it. So that's what we use. Yeah, in the argument, that's one of the main technical kind of things to establish. Yeah, that's a very good point. Starlish again, that's a very important point. You want for any more? Okay, all right. So, let me explain what's going to happen now. Dinner is about six, so we've got between now and six for quotes free discussion which will take place in the foyer out there. Placing the foyer out there. Think of it as an extra property. And then we will be tomorrow at 9 o'clock. We will be. I was sent a message by the bank managers to say if anyone's interested as a concept tonight, but there's still some biggest question. I think they might have sent me a new question. Yeah, yeah, that's the second.