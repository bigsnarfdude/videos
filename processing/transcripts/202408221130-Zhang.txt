Okay, so everybody can hear me. Okay, first of all, I would like to thank our organizers for the invitation and for organizing this wonderful event. And today, I'm going to talk about the risk coverage lower bound for quantum microsemigroup. Thanks to Meshro's very nice talk, the most difficult part was done already. And this part. And this part will be joint work with Mesho here and Florentine Muchen from Max Planck Institute at Leipzig. So I will keep the notation. I will try to follow the notation in Mesho's talk. So from now on, PT here will be a quantum marker semi-group. That is very nice. It is defined over the D by D matrix algebra. It is symmetric with respect to the normal LISE identity. The normal lies identity. So, in the general framework, you should choose the GNS symmetry. But in this case, to make a notation simple, it is just a traitor symmetry. And in this case, the generator will have the generator of the quantum market semi-group. Once again, I'm following measure's notation. So, the generator will always be a non-active operator. So, L over A will have this very simplified form where this VJ are the cross operator or the jump operator. Cross operator or the jump operator, and as Mashra mentioned, you can actually choose each VJ to be self-dejoint itself. But in certain applications, this flexibility of general VG here will be helpful. So I will just write down the general formulation with VG and VG start there. Okay, then in Measure's talk, he explained why we consider the following gradient estimate for this. Gradient estimate for this given symmetric quantum Marko semi-group. It is very helpful in deriving exponential decay along the quantum relative entropy. And this is the case whenever you have this gradient estimate k here. Here in general, it is a real number, but you expect this k to be strictly positive and if possible, as large as you can. So that will be the main aim of this talk. So just to give you some. Talk just to give you some ideas how to improve this gradient estimate. And for that, let me extend, let me recall you this gradient estimate statement. So here is your semi-group. And from the generator, you can define the derivation. This partial or this is the data. This partial or this D operator and this partial again following measures and notation is a collection of these partial derivative operators and this partial derivative operator is given by the commutator operator given by Mutator operator given by this each VG. Let me collect some facts or examples in this blackboard. For example, for this partial J, it satisfies Lebesgue's rule. So we view this as a partial derivative or derivation operator. But in general, you do not have the chain rule. But in general, you do not have the chain rule. You have a very nice function f, then for f of a, if you compute this partial derivative, different from the clax setting, you do not have this f prime of a multiplying this partial j of a, which in general are not commutative. Which, in general, are not commutative. But the good thing is that you still have something, even in this non-commutative case, where you need to define this multiplication in a nice way, which was already discussed in Metro's talk when you consider the partial derivative with respect to the logarithmic function. So, you actually have You have this identity. So the derivative of rho is equal to some multiplication by rho of the derivative of logarithmic of rho. Okay, so you still have something in this non-commutative setting, and this corresponds to some. and this corresponds to a certain multiplication by row or divided by row operation. So here in defining this gradient estimate here, I'm choosing a general family of multiplication which I will define in a second. Well in the measure's talk he he choose this row hat that corresponds to the multiplication essentially given by that general identity row logarithmic function. So here I will make this more flexible. Okay. So essentially this Okay, so essentially this row dot operation denotes to a family of multiplying by row operation in the non-commutative setting. And the partial PTA of A or partial A here is a vector, is a vector of operators. So this multiplication means multiplication entry-wise. Okay. If you do not like this derivation d operator, this means I will write down entry-wise. So this is the summation. Entry wise. So, this is the summation partial J P T of A then row multiplying. This is what this formulation means. Okay? So now what is this multiplication? What is this multiplication? I'm not sure if you can see the lines below. Lines below. So let me maybe let me put it here. In general, this row dot operation means an operator, even in terms of this lambda. This lambda. And this lambda is internal is the so-called operator mean. This definition will not be important here, but the crucial thing is that it has all the nice property that Mesher mentioned in the proof of gradient estimate, just as that special choice corresponding to Uh to this one, okay. To this one, okay, but the point is that you can also choose this lambda in other forms. For example, you can choose Maybe I should put here. And here I will choose this very special one: arithmetic mean, which means. Which means this is nothing but the average of left multiplication and right multiplication. Okay, so of particular interest to us is this one and this one. Well, this one was explained in Mesher's talk, and this one corresponds to the battery MRI criteria that was mentioned in his talk. Maybe I can explain in more details here. Any questions so far? Okay, so these two choices correspond to two definitions of receipt curvature lower bounds in the non-commuted setting. And in the collector setting, they are equivalent. They are equivalent. And in the non-commutative setting, the basic setup is we have this semi-group here together with this generator. Then for this battery M-Ray approach. You are doing the so-called gamma calculus. So, let me remind you this Gahe-Duchan operator, which is defined as follows. Okay. And in our context, so this is something that you can define whenever you have a nice quantum Markov semi-group with L here being the generator. And in our case, this quantity has a very nice form. Which was essentially mentioned in Eric's talk. This is when you choose A and B to be the same, this will be the sum of the square of the graph, of the pathway derivative. Okay. And this is the Gahe-Duchant operator, gamma. And we also need one more operator, so called gamma2, that is defined in a very similar way. And this time, in our example, it has a very explicit form. Which looks like this. So, in particular, when A and B are the same, this will be the sum of essentially the height. So, this is the height part. So, this is partial J partial K of A, sum of partial J, partial J, partial K, A square. Okay. Ah, yes, yes, you are right. So there is one more term which is which is a real part. Real part of partial L of A real part of this, right? Okay. But actually, this gamma two formulation, the exact formula is not so important to us. Just to give a formula, just to give a feeling that this is computable. Why do we need this kind of thing? In the Claque theory of battery MRI, the risky curvature is bounded from below is equivalent to the following so-called gamma 2 condition. So for the classical Riemannian manifold, So, for the classical Riemannian manifold, if you define the Gahe Douchan gamma and gamma2 here for the heat semi-group, then this inequality encodes the is equivalent to saying that the recurvature is bounded from below by k. Here, I'm using the convention that gamma of A is equal to gamma A A. Well, gamma two A is the same as gamma two A. Maybe just this one. Okay, so following a similar idea, you can define the background Mary Risk curvature lower bound in a more general setting, for example, for quantum market semi groups. Example for quantum market stem groups. And this was first studied, I think, by Maris Jung and Chang Zong. And the question is: under this research curvature lower bound, what can you say? Okay, what can you say? And by the way, there is the equivalent formulation, which is the gradient estimate. That is given in terms of Gahi-Dushong gamma and the semi-group only. Douche on gamma and the semi-group only. Okay, for this definition of receipt curvature lower bounds, it is useful, say when k is positive, that you may have the spectral gap inequality. But as Mesher already mentioned, but this definition of recycled curvature lower bounds is not enough in deriving modified log-so-blaffer inequality or the exponential. Inequality or the exponential relative entropy decay. And that's why you need the second approach. The second approach here is more in the spirit of the so-called Lord Stormy that uses Roughly speaking, optimal transport goods definition will not be recorded here. So the point is that you need to define the Watts and distance, L to Watts and distance in a reasonable way so that you have certain gradient flow structure. Then you can define the recycled lower bound here using the convexity or the so-called k-convexity of. The relative entropy functional along the geodesic of the Washington 2 distance that you defined here. So I will not recall the definition. The point is that this approach in defined risky coverage lower bound is equivalent to the gradient estimate defined here when you choose this multiplication. Choose this multiplication row in this way. So that is essentially what Measure defined in the Grasman estimate in his talk, and that is equivalent to this formulation. Okay, so in other words, both approaches, both approaches approaches. Most approaches have very similar characterization in terms of gradient estimates. This one that is linear, the first choice that is linear in rho, it is linear in rho so that you can reformulate this inequality as an operating equality, which is this. It has nothing to do with rho, it is only inequality, operating equality in A. But the second approach, it is very much non-linear in row, so you need to compute something that is very. So you need to compute something that is very complicated, which is essentially integral inequality. But this one is a point-wise inequality. So compare, if you want to compare to approach, this one might be easier to compute. This one will be somehow harder to compute, but it has an advantage that whenever k is positive, you have the modified logical preview inequality as well as the exponential. As well as the exponential decay along the quantum relative entropy. Okay? So we are going to study in a general unified framework in the sense that we do not specify the choice of the gradient of the operator mean here. So we have a family of multiplication by row. In this case, we are going to recover the result in both setup, in the both setup. In the boss setup, so the question is how to prove gradient estimate for general operator mean. As long as you can do this, you can recover results here and here. And in the other way, you need to borrow the ideas from this part and this part. So, in the following, I will give you some idea how to prove this in concrete examples, and that will be the goal for the rest of the time. Yes? On the left, on the right, and then you pick half of the. But it is the same because, in this case, multiplying from left and from the right, when you write down the exact formulation, it is the same. You just multiply from left, from right, by the cyclicity of the trace, it becomes essentially the same thing. But the product like you can uh but here w we are always assume the sigma is identity. The sigma is identity. Yes, yes. I choose arithmetic mean here. In fact, you can, if you want, you can also choose just one of them. Left one or right one. Yes, yes. I write in a symmetric way because for certain properties, you may need the operator mean to be symmetric, but you can also take the just left one or right one. Okay, so this is a general picture. And the question is, how can you prove such a thing? This was already. This was already. Any other questions? This was already explained in measures talk. You need to use some, you can use some intertwining identities to prove this. So the first idea was Or theorem, let me call it ideal zeros that motivates the little work due to Eric and Yamas, where they essentially prove that as long as you have this in the finding identity, you have this bread in estimate. Okay. This is the first result of this cand. You can ask where this intertwining identity comes is from. And I will give you here a classical example. Before moving to the deepen semi-group or the non-commutative example, this is a classical Hochenstone-Wullenbeck semi-group. Wullenbeck semi-group, which is generated by Laplace minus x times gradient. This is defined over the Gaussian space, where gamma here is a standard Gaussian measure on Rn. And in this case, you can actually write down this semi-group explicitly. Down this semi-group explicitly or the so-called metal formula, saying that the semi-group looks like this. Starting from this, you can verify this intertwining identity directly. You take a partial derivative of PT of F, then let's. Then let's assume that you can exchange the order of the integral here and this partial derivative. Then, here when you take the partial derivative with respect to x, here you will have a factor of e to the negative t. And this implies that in this setup, you have the receipt curvature lower bound one, or you say that the gradient estimate is satisfied with k being one. And that is actually sharp. And that is actually sharp for this example. This is a starting example in the classroom setting. And in the quantum case, a type of example you can ask is the bosonic Ochan-Woolenberg semi-group, which you can verify this property as well. And that was proved by Eric Carl and Jan Mas to resolve an open problem in quantum information. I will not write down this quantum OU semi-group example here. The point is that to verify this identity, Is that to verify this identity? I use this argument of explicit form of Pt, but sometimes in the quantum setting, it's not easy to see such an expression for Pt. For this reason, it is very useful to consider the equivalent formulation of this identity for the semi-group. You simply take the derivative on both sides and you evaluate t is equal to zero. Then this becomes an. zero. Then this becomes an identity. This becomes an identity for the generators. Let me let me write down here. This becomes the identity about the partial derivatives, each jumping operator, and the generator L. And this is something that you can verify for certain quantum marker semi-groups, for example, for the bosonic and Ochlen-Ulumbeck semi-groups. Okay. Okay, another example that we are going to discuss in more detail is the so-called depolarizing semigroups. Since we will use this frequently, I will record it here, your semi-group. We will map each matrix A to the convex combination of A itself and the multiple of identity. And the multiple of identity here, this multiple is given by the normalized trace of A. So, in particular, if you consider any density operator here, pt of rho will be the convex combination of rho and the normalized identity. This is a deployment semi-group. For the generator, if you want to, if you write down, this is just a minus tau a times identity. So, it is a very simple. So, it is a very simple quantum mark standard group. And in the following, we will try to prove the gradient estimate with a good estimate of k. Okay. Then you can try to adapt this idea of intertwining, which works well for Gaussian case, but the problem The problem is as follows. So for deprising, the point here is that we do not need explicit form of partial J for this example. We did not write it down explicitly, but the point is. Done explicitly, but the point is that the semi-group is so simple that we do not need the explicit formula for this partial BJ. Okay? Why is this the case? You can verify. You can verify this directly. Partial J P T of A will be will be this by definition. Will be this by definition, but the partial j acting on identity will be simply zero by definition. Okay, so this is nothing but e to the negative t partial j of a. Now you compute the other way around. You compute p t partial j of a. This will be by definition. Convex combination of this and that. But here, the trace of partial j of a by definition is zero. So this is also e to the negative t times partial j of a. So they actually coincide, coincide, which means partial derivative and the semi-group, they commute. And the semi-group, they commute. This gives us gradient estimate with k equal to zero, with k equals zero. But we need something like strictly positive. So this is not enough for our use. And you can actually verify that you should something strictly positive here. So in other words, this is entwining original formulation with entwining idea does not work directly to this deploying semi-group, even if this. Deploying semi-group, even if it is very simple. So, we need to try to improve this idea. And the first improvement was briefly explained in Meshou's talk. This is the second idea. So, the point is that maybe here you do not need this part to be pt itself anymore. You can have some flexibility. Say it is something else, which I call it PT vector, pt vector, but let me try to remove this constant. I absorb this constant in the semi-group following the notation in measure. Sample group following the notation in measure. Suppose that you have this intertwining identity, you plug in this gradient estimate formula, then measure proves that, okay, this under certain condition plus the following criteria. Then you can prove that it satisfies spreading estimate P infinity. So the flexibility here is that this P T vector here does not have to be exactly of this form, it's a multiple of P T itself. Multiple of PT itself. Let's try to adapt this idea to this deeply semi-group. Okay. For that, we start from here. The first identity, partial J of P T, that is exactly of this. Okay, in the last idea, I choose P T vector to be multiple of P T, but maybe let me choose this as P T vector. So So this time I'm going to choose pt vector to be simply e to the negative t times identity. This is a different choice. And then let's verify if these conditions are satisfied. The first one is this intertwining identity, which is automatically satisfied because this is exactly PT vector partial J. Partial J of A because of this computation. So the first algebraic intertwining identity is satisfied. There's a question: Is the rest of these two inequality? Here I write this in the operative form. Well, in Meshwar's talk, he writes this as a very definition, which is some integral inequalities, essentially something that you can prove in many cases using Caddition-Schwarz inequality. Cadition-Schwarz inequality. Okay? Inequality. Okay, let's try to see, for example, why the first inequality is satisfied. And here we are going to use a very special structure of Pt. So Pt of rho by definition is convex combination of rho itself and the normalized identity. Normalized identity, which I call sigma. identity which I call sigma. So for every t this is bigger than or equal to e to the negative row e to the negative rho. With this you can plug this in here then L of P T of rho will be bigger than or equal to equal to negative T L of rho. But what is the left hand side? Left hand side P T vector and P T Left-hand side, Pt vector and PT vector vector, they are just a multiple of identity. So the left-hand side is exactly. Is exactly this one after a factor of e to the t, after a factor of e to the t. Okay, because this is just e to the negative t, e to the negative t. So altogether is e to the negative 2t times L of rho. Well, right-hand side, L P T of rho is always bounded from below by this e to the negative t of rho. So this means we prove the gradient estimate. with k being one-half because of this additional factor of e to the t, if e to the t. Okay? Then this estimate does not turn out to be sharp. Does not turn out to be sharp. So we try to improve this idea further. And this is And this is the main result of this paper joint with Florentine and Mesher. The second idea is that, okay, we need to verify this algebraic identity. We need to verify this operating equality. This operating inequality is linear in row, and you have semi-group here. So why don't you just differentiate this inequality? You just differentiate these two operating equalities. Differentiate these two operating qualities that will be something like a Gahe Duchen and a gamma two, something like a Bucky-Emory condition. So we try to prove something on the right-hand side of this blackboard, which is non-linear. And now we reduce the problem to something on the left-hand side that I erase that is linear. So the second idea is to differentiate this inequality. So the idea is that if you have the following. idea is that if you have the following two conditions adjust the different everywhere so this is identical semi-group so equivalently this is identical generators L dagger being the generator P T dagger a P T vector then for this inequality you differentiate it then you obtain the following form obtain the following form so Which reads as follows. I will explain this in a second. But if you compare this with, if you still remember, the Gahidu Shang and the Gamma 2, this is essentially something like gamma 2 bigger than our... Something like gamma 2 bigger than or equal to k times gamma. But here you have L dagger coming from differentiate this P T dagger, P T vector. Well, here you have denoted L coming from semi-group on the right-hand side, which is PT. So it is not exactly the partial shock. Okay? So with this, we want to improve the estimate here. Then the problem is to choose a very nice generator L dagger, L vector. So it has some constraints and have some flexibilities. The constraints is that you must define whenever this actor of this partial, it is of the form partial. Partial, it is of the form partial j of something, then according to the first identity, it must be partial j L of A. We don't have any choice, but we are going to define this generator over the general matrix algebra or the direct sum of this matrix algebra. So you have this range. So, you have this range of partial J as a subspace, okay. But you also have this orthogonal complement. So in case your capital A looks like partial J of A, you know you don't have any choice. You must define Don't have any choice. You must define this to be part of J L of A. But in case you have something appearing in this complementary part, also on a complementary part, which we call eta, then maybe you can define L dagger whatever you want. The point is to make the second inequality hold. So our choice is that we just take this to be some multiple of this atom. Multiple of this eta, yes, okay, okay. So, so the following is that you want to prove gradient estimate. You want to choose this real number lambda here, such that Such that the gradient estimate this inequality holds with this constant k as big as possible, as big as possible. So, this boils down to certain calculations of the second inequality because now the first one is verified automatically, is automatically satisfied. Here are some detailed Here are some detailed calculations, which I will just skip. So let me just give you the final computation. So, if you plug in the definition L vector here in this In this inequality, here, by the way, this is this is how we define it for a vector of operators. This is just for shortening the, to simplify the notation. Then, if you plug this in there in the second inequality, it becomes the following. It is equivalent for the following scene.  For this inequality to be, for this part to be non-negative, for non-negative. So, what does it mean? If you do not consider anything about this eta coming from the orthogonal complement, then you get exactly the macro-mary criteria. Okay. And if you Okay, and if you plug in this eta into play, then you have this term and this term. So for this term corresponding to eta and eta square, this part you know, which is always non-negative, it's easy to control. The annoying part comes from the orthogonal complement, come from the cross part, which you don't know if it is positive or negative. So what you can do is to make the corresponding coefficient to be zero. Coefficient to be zero. I guess there is a there is this, no, this one. This is lambda minus 2k here. So this is lambda minus 2k. You can only make this to be zero. And then you plug this in. You want this k to be, you want this part to be non-negative, which is equivalent to saying that k must be. Is equivalent to saying that k must be at least one half. And this part you want this to be true, which is equivalent to the Buckley-Americ criteria. So altogether, you have three conditions. This one, this one, this one. So this boils down to the computation of this non-linear reciprocal lower bound to something that is linear, which is essentially the Bucky-Emory criteria. This is something that you can compute. And this K is. k is equal to one half plus one over d plus one d being the dimension of the matrix algebra. This is best possible in this Background M-Ray estimate, but it is not best possible for the gradient estimate. So our conclusion is that using the last idea, we can prove for the deploying semigroup with this gradient estimate. Reading estimate. Unfortunately, this is not sharp in general. You can prove something better. Actually, it holds for K being one half plus one over D, which does not follow from this intertwining approach, but something else. That's something else. Okay, so maybe I will stop here. Thank you very much for your attention. Thank you very much, Shawdan. And we might have time for some, like maybe one quick question. Yes. You can actually even replace this by a general conditional expectation. Then in this case, the argument still works, and your final answer would be one half plus C of E over one plus C. of E over one plus C of E, where this C of E is a so-called Popa index of this quantum channel, which is known for certain examples. So in particular, it applies to the general state case as well. Any other questions or comments from Homer? Then I think we can thank Onan once more. Thank you.