Sen, thank you for the introduction. And then Sen, thank you, Bruce, Jamie, and Yuhong for the invitation. And thank you all for coming. Although it's not like there are other sessions you can go to. So to be honest, I really feel this is the best audience I have ever got. So I really am grateful. I'm grateful for this opportunity and I look forward to any feedback on this work. So, this all started a few years ago when I started to work with Bruce on this biological project. So, our collaborator generated the sequencing data, and they want us to recover the gene regulatory network from the time series expression level data of genes. level data of genes. And without how there are many smart people who came up with all the algorithms and there are even smarter people like you guys who can prove that the algorithm works with high probability to exact recovery. So it doesn't sound like it's a very hard problem. But then we look at the state of the art. So there's this dream challenge. This dream challenge in 2011, I think. And they so basically have both synthetic data sets and real biological data sets. And they post that and then they ask researchers from all over the world to submit algorithms to try to recover the structure of the causal network for biology. And these are some of the These are some of the results they have. So just remind you, the AUPR is the area under precision recall curve. So ideally, you want precision to be close to one. So every edge that you say are there are actually there and the recall to be close to one. So you don't miss any of the true edges. So that would be an error that's close to one. But if you look at, for example, the But if you look at, for example, the synthetic case, the algorithms don't perform more than like 40% in the area under the PR curve. And it's even worse for the real data sets. For example, this is the E. coli and this is the East. So they are the two of the most well-studied organisms. And it looks like we have around 10% AUQ. 10% AUPR for E. coli, and for East, it seems to be around 2%. So basically, this is prosthetic, right? Because that's just basically saying the algorithms are giving you a lot of predictions that's not true, or it's missing a lot of the actual edges. And especially for the EAST data set, so I would imagine it's a reasonably sized. It's a reasonably sized data set, but if you look at it, everyone performs really bad. And here, this R here is a random guessing. So you can see it's no much better than random guessing. So obviously, this means one of at least one of three things is happening here. Either the algorithms are all really bad, or they have the labels all wrong, or the data is trash, basically. Basically. So that's your question. So precision recall curve. It's related. Yeah, so they should have another row on the top. That's the score, which is basically average of area and ROC curve and area under PR curve. But for some reason, they're showing PR curve here. I mean, if you if you If you recover the structure exactly, then the error is one. So you want to be as high as possible. You speak of this as if there is ground truth. Yeah, right. So for the in-circle case, that's the synthetic case. So they have ground truth. For these two, they have this called golden standards, which are pretty well accepted, but those are real data sets, so it's hard to say what is the ground truth. Hard to say what is the ground truth. I'm pretty skeptical about the gold. Yeah, that's what I was thinking, right? Yeah, okay. Right. So we look at this, and this looks like it's terrible. And we start thinking, what if the data sets that we got from our collaborator are like the East data sets, then maybe there's really not much you can do. So that basically motivates us to study lower bounds for these problems. For these problems, especially for the case when you don't have an asymptotic regime where you have more and more data, and in the case when the data set quality could be bad. So here's the outline. I'm going to mostly focus on the model of the problem. So maybe more people can be interested in it. Maybe more people can be interested in. And we provide two types of lower bounds for the information requirements. One is through the biochar coefficient, which is our last year's ISIT paper, basically. And this other one is based on estimation of the RC curve through likelihood ratio samples. Likelihood ratio samples. So let me explain the model. So we consider this very simple model where we have these row vectors, x for all the times. And we have, so each time the row vector is given by the previous row times this matrix A plus some noise W. Noise W and furthermore, we have some observation noise other than this driving noise. We have this observation noise Z that's added on the X. And the goal is to recover the structure of this unknown and random matrix A from the observations Y. And we assume this A has a sparse prior distribution. Prior distribution, where the sparsity means the number of non-zero entries is relatively small. And we call this the causal network inference problem because the A here can represent the adjacency matrix of a directed network. So if you have this graph, then the Aij would be the causal influence from node I to node J. To node J. And in particular, for the gene regulatory network, that's how much gene I can regulate gene J. And it could be either positive when it's activating gene J, or it could be negative if it's repression. And the basic question is, how well can we recover the support of A, which are the non-zero, the positions of the non-zero entries, given a Entries given a finite amount of samples. Can you observe one trajectory? Yeah, we're assuming we observe this trajectory from one to t. And x0 is we haven't specified it yet, but to make it simple, basically we're going to assume this whole process is jointly Gaussian given A. So we're going to set X0 to be Gaussian. We're going to set X0 to be Gaussian and all the driving noise and the observation noise to be Gaussian. But do you assume this is stable? Right, good question. So there are basically two obvious choices. One is a stationary regime, which is actually taken from the work that we mostly look at. But that requires the stability of the matrix A. Basically, you want a spectral radius of A to be less than one. That's someone. So, that actually affects how the different entries of A are dependent over each other. So, another possibility is this transient case where you can start from all zero vector and you don't care if it's stable or not. You just take T samples and you try to recover that. So, I'm going to mainly focus on the transient case. Yeah, X0 is 0, yes. X0 is zero, yes. Okay, so that's the model, and there's some closely related work. The first one that's mostly inspiring for us is this Sun Taylor Bolt work, where they basically have a slightly more general Markov framework, and they show that perfect support recovery is possible. Recovery is possible if you are able to estimate the conditional mutual information exactly, which means if you have infinite data, then with some other minor conditions, you can actually find out the correct support. So that's sort of the case when you have infinite data. And there are also a study on ordinary least square algorithms. Algorithms where they were looking at a different metric, and if you translate it to a support recovery, it's slightly loose. And there's also a lot of study on LASSO, but they usually require certain conditions which may or may not hold for the data sets. So and you're not citing any of the classical system identification literature because they don't say anything about support? They don't say anything about support? Most that we saw, I think, they either use a different slightly different metric, like exact recovery because we're going to focus on partial recovery, or they require certain conditions on the matrix as a lasso here. So it's an exact recovery condition. That's, I think, one condition that's the biometry property of these things. Property of these things, yeah, that's similar to the isometry uh condition. Like talking about work from 70s, right? I mean, this is a in 70s, right, this was a very popular thing, right? You observe trajectory, try to estimate A. Right. Okay, anyways. Yeah, Lamar Leon. Yeah, exactly. RLS useful. Yeah, but they probably didn't focus on support so much. That's important. Right. So these are some study on alpha. Some study on upper bounds on error. For lower bounds, there have been fewer works. One relevant one is by Bento and others, but they study an exact support recovery and the results are in the asymptotic regime and it doesn't have the observation noise. So that's what we are trying to focus on. So, and for the past, So, and for the partial recovery, we're considering this particular metric, which so one is the false negative rate for the entire network. It's given by the fraction of the expected value of the fraction of edges that you miss in your prediction. And you can write it out like this, and if you exchange the expected This and if you exchange the expectation summation, you actually get this convex combination of the false negative probability for this single edge from i to j. So this basically means for this particular metric, you can instead focus on the binary hypothesis testing problem for this one. For this one single edge from I to J. If you can figure that out, you get this error probability, then you can get the error probability for the whole network as well. And similarly, we define the false positive to be the probability of non-edges, which means that there's no edge. Which means there's no edge between the order pair out of all the non-edges positions. And similarly, it can be written as a convex combination of false positive probability for a single edge. Now, so here, one other thing is this convex combination weight only depends on your prior distribution of your network. Prior distribution of your network. So we can, to be more concrete, we consider this particular prior distribution that we call the earlier training network prior. And it's generated as follows. Firstly, you generate a directed earlier training graph with probability P for each. A probability p for each entry. And then you flip the signs infinitely with a probability of one half. And then you scale the whole thing by a scalar C. So that gives you the distribution on the matrix A. And as I just explained, we consider the transient case when X starts from all zero vector. Zero vector. So now maybe is there any questions so far? Yes. Do you know the size of the sport? We don't assume we know the size of the sport. Yeah, so you can actually notice there's nothing preventing the matrix to be like. Preventing the matrix to be like non-spar non-sparse, but basically, if it's really dense, then it doesn't make sense to look at the support recovery anymore. So basically, that's the reason why we're only looking at recovering the support. So basically, again, we see that it basically reduces to a binary hypothetical testing. To a binary hypothetical testing problem. So you basically want to find the ROC curve for each edge. But in general, it's hard to compute the exact error properties for a general binary hypothesis testing problem. So that's why we are going to try and find lower bounds for the ROC curve. For the curve, it would be an upper bound, but it's lower bound for the errors. And the first type that we were looking at is That we were looking at is through the bar character coefficient. So, first, in the general binary hypothesis testing setting, it's known. So, the average error probability can be hard to compute, but it's known that there's a lower bound for that average error, which is given by the biological coefficient squared over four, or there's also a more complex sign. Also, a more complex slight better form. So, this is known and it's good because the batcher coefficient can be computed in certain cases, for example, when the two distributions are Gaussian distributions. But then the problem is if they are not Gaussians, then you may not be able to compute the biochar coefficient. So, what we realize is that if you start. Realize is that if you start with something that's very complex, you can decompose the two distributions into a mixture of many distributions. For example, here the blue distribution is decomposed into F1 and F2 with a certain mixture coefficient, and the red is decomposed into G1, G2 with the same mixture coefficient. And then with this. Then, with this site information, which is basically, you know, you're assuming, you know, which of the two. So, to assume you have, for example, a coin flip, and after coin flip, you choose either this pair or this pair. And you assume you know the side information of the coin, then you can apply that by chart character bound again, and you can get this other, it is a different bound. That's the This other, it is a different bound that's using the row square bar, which is the average of the chair coefficient condition on the side information of this coin. And we noticed that this bound could be better or could be worse than the previous bound because there's no concavity property of the function. Of the function row square. So that means if you decompose your original distributions in different ways, you could get either better or worse lower bound compared to the original simple version of the bipartisan bound. It could be anything as long as F and G have the same. As long as F and G have the same set of mixing coefficients. But then for the robot, the average, that's always a. Oh, yeah, right. So here I'm listing the simple version where it's a one-half, one-half. Otherwise, it's a yes. Otherwise, this is going to be the coefficient that you use. So that's for the general binary hypothesis testing version. Testing version for our particular network version. Basically, since it reduces to the binary hypothesis testing on this one edge, you can write out the two distributions of given either there's not an edge or there is a niche. So you can look at the distribution of the observation why. Then in principle, you can. Then, in principle, you can get this lower bound, but that's usually hard to compute or estimate. So rather than using this direct pilot bound, we can basically find the side information for the two distributions. And one obvious way is to condition, so basically use this genie's information. Use this genie's information on the rest of the network and look at the binary hypothesis testing for this one-edge. I think that's a very common technique. And I think Julia also mentioned this type of genie's information yesterday. So, with the site information of all the other edges, Of all the other edges, we can basically have this mixture decomposition of the observation distribution. And then we can write out the lower bound based on this average version of the battery coefficient. And now we have actually a lot of terms in this mission because we need to count every. Uh, we need to count every configuration of the site information, um, but at least this uh can be estimated by a Monte Carlo simulation if you you are given the prior distribution. So now this is something we can at least uh estimate and uh to compare this uh uh lower bound to some upper bounds. To some upper bounds, we implemented two algorithms. One is, so what we do is to stack all the rows of the system states into a matrix and then look at one column. Then basically it's a compressive sensing type of format. Then you can use either LASSO or this OCSE. This OCSE algorithm, which is proposed in the case where you have exact value of the conditional mutual information, and that's also a greedy or iterative version of this compressive sensing type of algorithm. So we implement these two and we compare the budgetaria lower bound. The bad charia lower bound with the two. So, again, so here I'm showing you the ROC curves. So, the lower bound is actually an upper bound for the ROC curves here. And we see this purple and this gray are the two algorithms. So, we see here for a particular value of C, which is a scaling factor of the Factor of the network prior. So these two algorithms perform, so this is the ROC curve. So the x-axis is the probability of false alarm, y-axis is the probability of detection. And these two curves seem to be close to the 45-degree line, which means they are close to random guessing. They are close to random guessing. And this bound by lower bound gives this upper bound that's above the two algorithms. On the left, we have no observation noise. And on the right, we have an observation noise of a level of one. And we see everything shift downward a little bit so the gap between the two are smaller. So, the cap between the two are smaller. And for different values of C, we see everything is closer to the ceiling, meaning the recovery is getting better, but there's also this pretty significant gap between the algorithms and the bound. But nonetheless, this bound is for the finite. is a for the for the finite scenario and it's so so it so so so so you can't feed this down for for for this final setting yes what's the sparsity of your graph um so right yeah so so it's actually p equals to 0.2 so you have uh n n nodes and uh the probability nodes and the the probability for for the for the connection is 0.2 so um and when you increase uh the value c even further um everything uh becomes a slight but um even better um so so this is good because we we have have this uh bound but there's also this uh pretty significant gap um so then we So then we're wondering if there's a better way to get a better bound. And that's so that motivates us to study a different type of lower bounds. And this time we start over again from a binary hypothesis testing setting, but now we just look at the likelihood ratio. Then the obscure curve is given by the two functions: the probability of false alarm and probability of detection as a function of the likelihood ratio, or these are basically the distribution of the likelihood under different hypotheses. So we want to rather than give Rather than give a bound, we try to estimate the ROC curve. And we consider the case where we're given a number of samples labeled by the hypothesis label. So each label tells you whether this following sample is sampled from hypothesis H0 or H1. And we assume we're able to actually observe. We're able to actually observe the likelihood ratio given that observation. So, the observation of a general binary hypothesis testing could be really complex and high-dimensional, but the likelihood ratio samples is just a scalar. So, we assume we're given the labels and the likelihood ratio samples, and the task is to estimate the ROC curve. Estimate the ROC curve. So, for this estimation problem, so you may wonder how come you can get likelihood ratio samples. So, in general, if the density functions for hypothesis K can be decomposed into two parts, and if one of the two does not depend on the hypothesis K, then we can see that the Then we can see that the likelihood ratio can be given by only the part that depends on the hypothesis K. For example, if you have a time series observation and you can write the densities to be the product of two parts and if one part does not depend on your k, then it's canceled in the ratio. In the ratio. And it turns out to be the case for the network inference, causal network inference problem that we study. Because if you think of trying to decide if there is a natural from this node to this other node, it turns out we write out the likelihood, many of the other terms that now depend on this hypothesis. On this hypothesis, so you get a lot of the terms to be canceled. So, this is why we were looking at the scenario where we are given the likelihood ratio samples. So, if you are given these labels and these likelihood ratio samples, one obvious way, thing to do is to The thing to do is to do this empirical estimator for each of the two hypotheses. So, what you do is basically look at all the likelihood ratios that's under hypothesis zero and do this estimation for the probability of false alarm. And you do the same thing for the For the samples under hypothesis H1, so you get an estimation for the probability of detection, and then you combine the two to get this estimation for the ROC curve, which is this staircase type of shape. And since you know that the actual ROC curve should be a concave function, you can even Concave function, you can even concave that to get this gray curve. That's the least concave major of the ROC empirical. So this is something you can do, but you could also pose this problem as a maximum likelihood estimation. Likelihood estimation problem. And if you solve the KKT conditions, it turns out the maximum likelihood estimator, which tries to maintain the relation between the two estimation, probably the false alarm and the probability detection. So it turns out the mathematical estimator is given by this weighted version of the empirical. Empirical estimator, where the probability of the false alarm is again this weighted empirical distribution with this weight one over one minus lambda plus lambda r where lambda is to be determined. And the probability of detection is also given by this weighted version with weight r over one minus lambda plus lambda r. So in So, in the figure, and one more thing, one more observation is that if you look at the maximum likelihood estimator, you don't really need to look at the labels because it doesn't matter where each of the likelihood ratio sample comes from. It's going to be the same when you're trying to maximize the likelihood. So, you just color all the samples. You just color all the samples to be black, and basically, you have these different weights for each of the samples. And when you sweep the pair of property of false alarm and detection by choosing this lambda such that the probability of false alarm is a well-behaved one, then you get this. Then you get this maximum likelihood estimator of the ROC curve, and that's guaranteed to be to look like the ROC curve because it's guaranteed to be concave. So this is the maximum likelihood estimator. And one interesting observation is a different view of the maximum likelihood estimator. So if you think of the set of Think of the set of all the ROC curves and the set of pairs of false alarm, probability of false alarm and probability of the detection functions. There's obviously this bijection between the RC curve and the pair of the distributions, because you can generate from the RC curve to the distribution or back. Workback. Now, can consider this mapping phi that maps a pair of the false alarm and detection distributions and a number lambda from zero to one to the distribution over the entire zero to infinity extended real line. Then by doing this, the mixture, then Then it turns out this function is actually bijection, and it turns out the MLE estimator that we described in the previous slide is essentially the inverse of this mapping. And I say essentially, because, of course, the mapping would also output this lambda, but lambda is basically taken dropped by MLE and you. By MLE, and you then have to map the pair of the distribution back to the ROC curve. So, this is another observation or interpretation of how the maximum likelihood estimator works for this ROC curve estimation problem. PFA, PD are those functions? Are those functions? Yes, these are functions of the likelihood ratios. So they are basically the distribution of the likelihood ratio under the two different hypotheses. So basically, what this means is that given any distribution from Um, from on the extended line from zero to plus infinity, um, you can apply MLE and get your AULC curve plus this lambda. So, so given a probability distribution of a likelihood ratio, then you can uniquely decompose that into Um, and you don't, and it comes from a mixture of under hypothesis H0 and under hypothesis H1, you generate the likelihood ratio. Then there's a unique value of lambda and a unique distribution under H0 and a unique distribution under H1 that gives you that probability distribution. Yes. That's what we're saying. Right. And then the maximum, and you're estimating the maximum likelihood estimator of the ROC curve. Of the ROC curve. Given the observations, right? Right. And then, and it's given by the inverse of this function phi applied to the empirical distribution of the observations. Yeah, that's right. And it doesn't matter. Some of the observations could have been generated under H0, some of them could have been generated under H1. It doesn't, you can throw that information away. It's irrelevant. Information away, it's irrelevant, right? In terms of maximum likelihood, it's irrelevant. All right, so this is the MLE. And to study how good the MLE is, we consider this particular distance for the ROC curves. It's called a levy distance, and basically it's the smallest size of the square you can find such that when you such that when you uh when you when you when you swipe the square along one one of the uh curves the region that it it's swapped through covers the other curve um um completely completely um so this this gives you this uh metric um and uh we show that uh the ml e is consistent under this levy distance metric um Metric when you have IID samples from a mixture distribution of your ROC. So that's good. We also look at some numerical results. So we consider the simplest case where you have the two distributions of the observation to be Gaussian. Um, um, Gaussians with various one and different mean, then you can calculate the likelihood ratio to be this, and our C curve is given by that. Um, and then we if we are given 10 samples under H0 and 10 under H1, then we can apply both the MLE and the empirical estimators to get the ROC curve estimator and the Curve estimator and the red here is the true ROC curve given by this formula. And if you run this one time for these 20 samples, this is one possible instance you get. Average over 500 times, we can look at the average levy distance for the three estimators. And MLE is a significantly MLE is significantly better in terms of levy distance than the two empirical estimators. So, this is with 10, 10 samples. If you have both 100 under each hypothesis, then everything improves, but MLE is still much better. For 1,000, they get even better. And what's interesting is when you have much fewer lack of ratios. were like a ratio samples under one of the two hypothesis hypothesis. So if you have 10 under H0 and 100 under H1, then MLE is still, then in this case, MLE outperforms the empirical ones even more. And in the extreme case, if you only have samples under H0 and no samples under H1, then you can't really use the empirical estimators. Really, use the empirical estimators because you don't have samples to estimate your distribution under H0 anymore, but you can still apply the MLE because it doesn't care which hypothesis did the samples come from, and it's still performing reasonably good. And we can also look at how the errors change with, for example, the different With, for example, the different mean difference or different fractions of the samples and MLE are all pretty consistently better than empirical estimators. And this also suggests a possible good convergence rate uniformly, even though it seemed to be slightly slower for the case when. So, for the case when most of the samples are under only one hypothesis, then we can try to apply this to our original network inference problem. So, it turns out we can use the same Gini's information to calculate the likelihood ratios as we did for the battery bound. Bound and so we can directly compare the MLE bound and biochar bound. So the MLE bound in this case will be guaranteed to be better than the biochar bound, but it might still not be the correct ROC curve because it's based on the genius information, not the original hypothesis testing problem. So, anyway, by applying the MLE, we get a better lower bounds for the errors. This is C equals to 0.1. But here, we don't have observation noise. If we do, then calculating the likelihood ratio is slightly more complicated. But in principle, you can always try to estimate the likelihood ratio. Estimate the likelihood ratios by another Monte Carlo simulation. So, in principle, you can actually find out the correct IOC curve by estimating the likelihood ratio samples and then apply MLE. But it might not converge very fast. This is when the C is higher, so we see the MLE is again better than the chariot bound, and this is when even higher C. See. So, in summary, we look at the causal network inference setting and we try to find the lower bounds for a finite sample scenario. And I think that's how I have. I'm not very familiar with this bottle with on technique, but it it sounds like you were using a decomposition by like a pair of different ways. But is there any way you can get balanced by somehow like three different weights of the different books? So, if you start with different weights, you can always cut it even smaller so that they have the same weights. Same ways, but I don't know if there's a way to optimize the choice of decomposition. Because as I mentioned, the decomposition doesn't necessarily guarantee that the bound is better. So some decomposition must be better than others. So there might be an optimal way to do the decomposition. But it only seems to work if you. seem to be uh to to work if you um do the decomposition uh in the same way for for the for the two distributions meaning that they they have the same same uh weight is there a way to formulate the search for optimal weight for some kind of optimization problem yeah it's it's it's it's definitely something one can do yes so at some point you mentioned about uh its connection to compressive stanza i didn't quite Resident sensing, I didn't quite get that part. Oh, yeah, it's uh basically so go back to your slide with the axis and A because I think as I understand the representing you have a sparse vector, right, right, yes. Um, so here we have two two two matrices on both sides, but if you take the column on left side, then that'll be the matrix times the one column of A. times one column of A plus another column. So if A is matrix, A is sparse, then column is sparse. So the data, let's see, the sensing matrix is the data and A is like the vector you're trying to recover. So A is sparse. Right. Yeah, A is the sparse signal that you're trying to recover. Design your sensing matrix. Which is why it's hard to. Why it's hard to ensure it's going to satisfy the restricted isometry property, which it actually does if we have a preferential attachment network or if you have an Erdash Brenya network with high probability, because there's high probability results about the adjacency matrices for those. So you can actually prove when you let N go to affinity, and you know, Montanari has some paper, Ben Reck has papers on asymptotic. On asymptotic, as you let like t go to infinity, recovering the matrix A. Finite, you know, the biologists we're talking to have small numbers of samples. And when they tell us to do something, we'd like to say it's impossible. And those papers don't tell, they don't help one single bit because they say it is possible if you have tens of thousands of samples. Of samples at least squares algorithm like accept about the uh so analyze. Yes, can I ask a question about the original biology motivation? So, I guess here you're making an assumption that the process is marked off for these different types of incidents in terms of gene expression network and things like those like yeah. Yeah, right. So, basically, the data set that we have is a time series data set of the expression level. So, for example, human, we all have very similar genes, but the cells express the genes very differently for different bodies and for different individuals. And that's what makes life possible, I guess. And so, if you look at the expression level, So, if you look at the expression level of all the genes in the cell, it's sort of this time series, because the expression level at right now might depend on the expression level one second ago, for example. Or you can choose. Why not two seconds ago? Yeah, it depends on your model, but usually the expression. The expression level, the change rate is basically determined by how much is produced, the rate of production, and rate of degradation. And the regulatory network mainly change the rate of production of each gene. So if you have the regulator protein that's bound to this target protein, then it can either It can either make the production rate higher or lower. So, in that sense, if you know the expression level of the regulator gene, then you sort of know how bound, how much the regulator is bound to the target. And as a function of that bounding activity, the production rate is pretty much determined. Much determined. So it makes sense to think of the time series as a Markov process, or even if you think of the actual biology, it's not discrete time. So maybe a more realistic model would be an ordinary differential equation or stochastic differential equation. Even in continuous time, I should be approximately more to yeah, as long as you have all the information. And a simpler version would be that. Simpler version would be that the expression level determines the change rate of everything. But you can easily fix this, right? You can depend on two minutes ago by just, you know. Yeah, you can make the stay space bigger. Exactly. Yeah, actually, if you talk to biologists, they would say this is wrong because you don't only have protein concentrations, there are also the micro mRNA concentration and other hormones. And other hormones that's affecting the gene expression. So the real biological system can be more complex. But it's believed that transcriptional regulation is the main part of the gene expression values. So I think this is the right model to start. Although linear Gauss seems a bigger part. Gaussian's a bigger problem. Yeah, linear Gaussian is obviously not the best. There's non-linear stochastic differential equations based on chemical reaction networks. Yeah, like in the figure that I showed at the beginning, where they have this dream challenge and this in silico data sets, that's generated by a pretty complex model where A model where you have so it's sort of like a circuit. You have different input and output of different modules, and that one is considered the most biological plausible model for gene expression at that time. So it could be made more biological by adding more components of the system. It's got a lot of parameters. So if you give us some data, you can fit that. Right. So if you give them some data, you can fit the data. But then that doesn't mean you've recovered the biology. So one comment I have is: like, suppose you were trying to learn the equations of motions of the solar system, right? And one way to go would be to do this, right? Just fit every position of every planet. But then probably you need observation time, which is like 10 to the 10 years to do it, right? So how we humans were able to do it. Humans were able to do it was because we first observed a simple system, right? An apple, right, falling to the earth, right? And then we feed that very well from a lot of samples, right? And then we did inductive assumption on the triangle on the matrix, right? And then we feed, right? So I think, so if, like, even if you prove to biologists that their data is not enough, right? They would say, well, this was too uninformative a prior, right? Like, for example, this, you know, this cause of this directed graphs, right, they may have. Also, those like directed graphs, right? They may have a lot of extra structure, which is arbitrary directed errors, right? Doesn't capture. Maybe, for example, I don't know, they have they're connected, right? And I don't know, right, things like this, you know, isolated loops or something, right? So could you go back to your graph again? My opinion is like this is like trying to feed a solution. So like a little solar system they know they're going to well they're not the graph of nodes. Yeah, like that. So like when you edge two edges are coming in the node three in a linear Gaussian there's additive but it could be like an and or it could be an or both of those need to be there in order for node three to be a espresso Order for node three to be a dispressor or just one of them. And the nonlinear equations, and they kind of encode and and or and some people have boolean approximations. And then there's there are physics-based models. But there's besides having lots of parameters, there's lots of choices for the that actually appear in the biology. And plus, there's like soybean. Like soybean has 50,000 genes. And all this clustering we're talking about here, you can take it to the biologist. They don't know what to do. They just cluster like crazy. Say these genes are together. So you've got 200 genes here together, but what does that mean? For this dream challenge, is it single-cell expression like this? Are you looking at like whole orders? Is it pretty cool? I'm not sure about the recent ones, but this one is like 10 years old. So at that time, they don't have single-cell technology. But I imagine nowadays they probably have single-cell data sets which are even bigger and harder to understand. Yeah, so I guess if it's not single-site, you can imagine your process itself might be a mixture of like several different simple processes. Yeah, right. So the traditional bulk sequencing is basically averaging. Sequencing is basically averaging all the single cells out. So you can probably argue that the single cell sequencing gives you more information because it doesn't leverage the important differences out. I don't know, you just have to kill that too or something. What's that thing that you said? Wa w w w what's that thing that you said? Oh yeah, this they do gene knockouts where you'll just like remove a gene and suppress its activity. Gene gene knockouts, yeah. Oh, knockout, yeah, yeah. That's uh yeah, I think the knockouts are one way to do so called intervention. So actually, that's one thing that our collaborators are good at. They send in, so they try to try to do this a trend. So, they try to do this transgenic plants where one gene is knockout and look at the expression levels. So, that's one way to determine whether so that's basically by changing one edge for the entire network and look at the expression levels. So, that's one way to try to determine if that edge exists. That edge exists over there? No, but you could try to do some more sophisticated kind of thing, like what you're doing, where you're really trying to understand the dynamics of how things are behaving. But you have this kind of fine-grained data. I don't know, it's unclear how. I think it's an interesting question, but just curious if you thought about it. But yeah, it seems like there's a lot of. Yeah, I think now they have very precise gene editing techniques. So it's definitely. It helps when you try to design experiments like this. But I think by changing, by modifying one gene, you sometimes also change some other genes. So I think sometimes it's not clear if you can do modification on a single gene. Without changing anything else. Another question? Okay, on the schedule, we have a break followed by