Jacob from Pittston about optimal agnostic control. Thank you. So thank you to the organizers for having me. I'm great to be here. Thanks to Fred for funding the workshop. Okay, so I'm going to be talking about optimal agnostic control. Okay, so what is agnostic control? Okay, so this is the type of adaptive control. So adaptive control is when we have some system that we want to control, but we don't know the underlying dynamics. We have to learn them. So in our case, we want to say that we know the laws governing the system, but we don't know some parameter or parameters. Okay, so maybe you have some prior belief about your parameters. Maybe you don't. When you don't, that's agnostic control. So that's what we're doing. Okay, and typically when people do this, you evaluate the performance of some control strategy by looking at the regret. So this is a quantity that compares the performance of your strategy to the performance of some opponent who has perfect information. I'll talk a lot in a lot more detail about this later. This is just a preview. And typically, when people do this, they try to achieve a regret that has order of magnitude as small as possible. And they try to do this after some long time horizon. So they're looking at the limit as your available time goes to infinity. Okay, but what if you don't have the luxury of What if you don't have the luxury of waiting a long time? So, for example, maybe you're flying an aircraft and some dramatic event happens. So, maybe it's an airplane that loses a wing or a quadcopter that loses a rotor. Okay, so obviously in this case, the pilot has to learn what to do very quickly, or they'll crash. Okay, so our goal. Okay, so our goal, remember most people try to get best order of magnitude possible after a long time. We're trying to do absolutely the best thing possible, so achieve absolute minimum possible regret over some fixed finite time horizon. Okay, so usually when you're doing learning, you have a lot of data and a lot of time. Here we have neither, right? If you don't act fast, we'll crash. So we need new ideas. So, we need new ideas. Okay, so I'd like to thank Fred a second time. This research is funded by a MURI grant from Fred, featuring people at Princeton, UT Austin, and Northeastern, and all of these different departments. The specific work I'm talking about today is joint with the group at Princeton. So I'm in math, also in math is Charlie Defferman, and then in aerospace and mechanical engineering. And then in aerospace and mechanical engineering, Clancy Rowley and Max Eckel. Max is now at the University of Maine's Germany. Okay, so to capture this situation I outlined before, we're going to look at this simple classical LQG problem. So this is an instance of it. It's not the most general version. So we fixed some time horizon t at some starting position. And now we're looking at a particle. And now we're looking at a particle Q governed by these dynamics. So, you know, forget about U as our controller, forget about that. This is just a simple exponential growth or decay model with additive noise. Okay? But we're allowing this parameter A to be unknown. So we don't know whether we're growing or decaying, and we don't know at what rate. Okay, so like I said, u is this control variable. So at each time t, we get to choose u based on what we've seen. So based on the trajectory q of tau for tau between 0 and t. And we're going to say that a strategy is how we specify u. So a strategy is just a choice of this control variable at each time given a possible history. And we'll denote strategies by sigma, sigma prime. Strategies by sigma, sigma prime, etc. Okay, so for any strategy and any realization of the noise, we incur some cost. So this is a random variable depending on all of our parameters as well as our choice of strategy. Okay, we're going to, given some strategy and some value of this parameter, we define the expected cost. So this is just the expected value of the cost. We denote Value of the cost, we denote it by J, sigma A. Okay, so our goal is going to be to find a strategy sigma for which this cost is as small as possible. What does this mean? This is very vague. So this is intentional. Exactly what we mean by as small as possible is going to depend on what we assume about the parameter A. So we're going to work through a series of cases in which we assume different things about A. Different things about A. We'll start with classical control where we actually know the value of the parameter and we'll assume less and less until we're assuming nothing about this parameter A. Okay, so the first variant we're looking at is just classical control. So here we know A. And we characterize optimal strategies by requiring that they minimize By requiring that they minimize this expected cost. Okay, so this is like very classical stuff. To compute the optimal strategy, you derive a PDE, which is known as the Hamilton-Jacobi-Bellman equation for this quantity, the expected cost to go with the optimal strategy. I'm not going to go into this in detail. I'm just saying it because it will motivate some stuff we do later, so I'm reminding you of these ideas. And when you do that, you find that the optimal strategy is given by what's called the Given by what's called the linear quadratic regulator. So this is the smooth function kappa times q. We'll refer to this as the optimal known A strategy. We're going to come back to this throughout the talk. Okay, and just to give you an idea of what this control is, if A is a large positive number, then for most values of T, this is just like minus 2a times Q. If A is a large negative number, then as A gets very, very You know, as A gets very big, it's like we're basically doing nothing, which makes sense, right? This particle is depending on its own, in that case. Okay, this is kind of unmotivated now, but it'll be important later. u satisfies this estimate, so it's always smaller than some constant times a times the absolute value of u plus 1. Okay, and so remember when we define regret, we're going to be comparing our performance to that of this opponent. Performance to that of this opponent who knows the value of A. So let's just look at how they do. So obviously the most dangerous case is when A is large, right? Then this particle can grow very quickly. So the optimal strategy in this case, how does it perform? Well, remember, we set U to be roughly minus 2AQ. So if you think about what our expected cost will be, cost will be. Q is bounded by, or sorry, U is bounded by some constant times A times Q. So our expected cost is going to be, the order of magnitude will be like A squared times the integral of Q squared. If you think about what Q would be with no noise, you'll find that Q is like some constant times B to the minus AT. So you get that the order of magnitude of this thing will be like A squared times. Of magnitude of this thing will be like a squared times one over a. So this thing, the expected cost of the optimal strategy is linear in this parameter A. Okay, so that's our first variant. The second one is going to be Bayesian control. So we fix some number, A, some positive number, and we're going to assume that we have some prior belief on A that's given. Belief on A that's given by a probability distribution that's supported in this bounded set minus A to A. Okay, so now there's sort of a natural thing to do to characterize optimal strategies. We want to minimize the expected cost with respect to this prime. So this is the expected cost of our expected cost. So this thing is the Uh right, so this thing is the expected cost with perspective Roundian motion. Okay, so remember a strategy specifies the control variable at some time t in terms of this history up to time t. So in principle, this is a one-parameter family of functions on an infinite dimensional space. So this is a big space. But we're in luck. But we're in luck. If you compute the posterior probability distribution for A, given past observations, you find that it's actually determined by your prior and then these two observable quantities, zeta 1 and zeta 2 that I've written down here. And so the nice thing about this is that we can now assume that optimal Bayesian strategies are just functions on R4. Right, so it's a function of So it's a function of your position at time t as well as what you've observed for these two quantities. Yes, oh sorry. So I said standard Browning motion at the beginning. What I meant by that is that we normalize it so that the variance at time t is equal to t. Okay, so Okay, so we can actually solve for these optimal Bayesian strategies explicitly. So, inspired by the known A case, we'll let J denote the expected cost to go of the optimal Bayesian strategy, starting from some time t, some position q, and given history captured by these quantities zeta 1 and zeta 2. Okay, so very quickly, I'll just go through the heuristic of deriving the Hamilton-Jacobi. Characteristic of deriving the Hamilton-Jacobi-Bellman equation. So the idea is that the cost to go, starting from time t, is the integral of our cost from t to the end time capital T, obviously. And here we're thinking of U. So this is the expected cost to go the optimal strategy. So this U in our formula is that optimal strategy. Okay, so if we let delta T be some small time increment, then Increment, then we can write the cost to go at time t like this. Write modulo errors that are a little low of delta t. Okay, now what we'd like to do is tailor expand this second term on the right-hand side, and we'll get some PDE. So define a bar to be the expected value of this parameter a with respect to the posterior probability distribution. And from our formulas, we can compute all of these terms that are going to show up when we Taylor expand that term on the right-hand side of the previous slide. And the point is just, you know, once you plug this in, you combine with that previous equation, which is called the Bellman equation, and you end up with the following equation. The following equation. So the U that corresponds to your optimal Bayesian strategy is going to minimize this quantity on the right-hand side. And just differentiating, you find that the optimal U is given by minus one-half times partial with respect to Q of your cost of go. Okay, and so we call this the Hamilton-Jacobi-Bellman equation when we substitute the optimal U back in. Put the optimal U back in. This is a PDE for J. The terminal condition, our cost to go when we're at the end of the game should be zero. Okay, so the upshot here is that if we can solve this Hamilton Jacob-Bellman equation for some prior, then we have an explicit formula for the optimal Bayesian strategy corresponding to that prior. So we've solved this. So, we've solved this PDE numerically for certain values of the parameters and certain priors, but we don't have a rigorous proof of existence or regularity of the solutions. So we're going to proceed by imposing some assumptions. Okay, so we'll refer back to these throughout the talk as the PDE assumptions. So we're going to assume that this Hamilton-Jacobi-Bellman equation has a smooth positive solution. Solution, and we're going to assume that the resulting control strategy satisfies this estimate here. Okay, and to motivate this estimate, recall the estimate satisfied by optimal strategies for known A. So remember this strategy here is supposed to be for A between minus capital A and positive capital A. So looking at this, this is a natural This is a natural, maybe conservative estimate because we've allowed powers of A there. And I should say, you know, I said that we've solved this PDE numerically. When we do that, we find that this estimate holds. Okay, so there's some evidence to support our assumptions. Okay, so here's our main theorem on Bayesian control. Theorem on Bayesian control. If the PDE assumption holds, then the optimal Bayesian strategy is given by the thing you get from solving the Hamilton-Jacobi-Vellman equation. What about the priors here? I don't think I see what you want to have as priors. Oh, so the so we're assuming that this parameter A is given, we have some prior belief about that. We have some prior belief about that. So D prior is a probability distribution on minus capital A to A. D to the data. Yeah. Are you assuming like a uniform driver? Like headspeaker driver? No, this is just a probability measure. Arbitrary. Yeah. But you need valid, right? Because he has a. Bounded, right? Because it has a catheter A, it was even. Yes, so it's a probability measure on a bounded set. Yeah. Okay, so now the third variant, bounded agnostic control. So we continue to assume that A is a bounded, A belongs to some bounded set, but now we're assuming that we have no prior belief about A. So we're doing agnostic control now. Okay, so remember this vague bull I stated earlier. Remember this vague goal I stated earlier: find a strategy for which cost is as small as possible. Okay, so in the last two cases, there was some natural expected value of cost that we could take and minimize. Now there's no natural expected value, and so we have to new idea, and that's going to be the notion of regret. Okay, so the setup, we're competing against some opponent who knows. We're competing against some opponent who knows the true value of A. So obviously, this opponent is going to play the optimal known A strategy that we saw in the classical case. And to compute the regret of some strategy sigma at A, we're going to compare the expected cost that we incur to the expected cost of our opponent. So we're going to do this. We have three different notions of regret that we'll look at. So the first is multiplicative, we take the ratio. We take the ratio. The second is additive, we take the difference. And the third is what we call hybrid regret. So we introduce some parameter eta. And this is the definition. The idea here is, if the expected cost of our opponent strategy is much smaller than this parameter eta, then we're really just comparing to the parameter. To the parameter eta. So I'll say a little bit more about that on the next slide. So let's compare when these different notions of a red are useful. Okay, so if you think about a situation where the costs are very big, so say our opponent incurs a cost of $10 million and we incur a cost of $10 million and $1,000. This is probably pretty good, right? Where we're only spending $1,000 extra. We're only spending $1,000 extra and the magnitudes are big. So multiplicative regret will reflect that this is good. Additive regret will read this as being fairly bad. So when the order of magnitude are big, we like multiplicative regret. Okay, similarly, if the costs are small, so if our opponent's spending $10 to the minus $9 and we're spending $10 to the minus $5, we're spending very little money, we're probably happy. We're probably happy, but our multiplicative regret is huge, our additive regret is small. Okay, so hybrid regret's supposed to give you sort of the best of both worlds here. Though if we're willing to neglect the cost of, say, a tenth of a cent, then hybrid regret with the parameter 10 to the minus 3 is going to give us meaningful information regardless of the orders of magnitude here. So we like hybrid regret. So we like hybrid regret. Here's a computer for these high-cost and low-cost examples. You see that it reflects the fact that we think these are good strategies. Okay, so we define the worst case regret to just be the supremum of the regret over all a in this bounded set minus a to a. And we now have a well-defined goal. We'll say that a strategy We'll say that a strategy is optimal if it minimizes this worst-case regret. Okay, so here's this is going to be a key idea for us. So these optimal Bayesian strategies are going to come back into play. So I claim that if I have some probability measure and I compute the optimal Bayesian strategy for that probability measure, if that thing has constant regret, If that thing has constant regret, then it minimizes the worst-case regret. So it's actually an optimal agnostic strategy. Okay, so this gives us a way to go from our Bayesian strategies to actually getting optimal agnostic strategies. I'll show you a proof of this because it's very simple. So suppose we have some other strategy, sigma. Since our Bayesian strategy is optimal for that prior, there has to be some value of A for which Some value of A for which sigma does at least as bad. Okay, but our Bayesian strategy has constant regret. So its regret at that point is equal to its worst case regret, while our opponent's regret is higher there. So the worst case regret of our opponent's strategy has to be at least as big as ours. So therefore this Bayesian strategy is optimal. It's an optimal agnostic strategy. It's an optimal agnostic strategy. Okay, so like I said, this is very useful. It actually gives us a way to find these optimal agnostic strategies. We need something slightly different than what I said, but that's sort of the easiest version to state. So that's what I showed you first. But this is really not much more sophisticated. So suppose that we have some prior, instead of being supported on this whole bounded set, it's supported on some fine. This whole bounded set, it's supported on some finite subset. And suppose that its regret on all of minus A to A is maximized on this finite subset where it's supported. So it has constant regret on this finite subset, and that's where its regret is the biggest. Okay, so that argument that I just showed you applies in this case too. So in that case, this sorry, this shouldn't say mu, this should say the optimal. Say mu, this should say the optimal Bayesian strategy corresponding to mu is an optimal agnostic strategy. Okay. So notice that this also gives you a recipe for computing these things numerically. So you guess some finite set and some prior, and then you check if your regret is constant and maximized on that prior. If it's not, you adjust the choice of finite set. The choice of finite set and prior. And sort of just by making ad hoc adjustments, we've had this converge for certain values of these parameters, and we can actually compute these optimal agnostic strategies. Of course, this requires, well, actually, no, sorry, this requires solving the PDE, but we can do that numerically. But if we want any sort of rigorous proof of existence of these strategies, Proof of existence of these strategies, we have to assume this PDE assumption, right? Because we're going to use these optimal Bayesian strategies. Okay, so here is our main theorem on bounded agnostic control. So fix a parameter A and some notion of regret and assume that this P D E assumption holds. So then there exists some probability measure, D prior star, for which the optimal Bayesian strategy. For which the optimal Bayesian strategy minimizes worst-case regret among all possible strategies. So it's the optimal agnostic strategy. And as expected, this prior is supported on a finite set, and this finite set is precisely the set of points at which regret is maximized. Okay, so the proof of this is technical, so in the interest of time, I won't say anything about it. I won't say anything about it. Sorry, did I start around five after, ten after? Closer to ten after. Okay, okay. Right. So finally, let's talk about fully agnostic control. So we're going to assume nothing about A. So it can be any real number. We want to redefine our notion of worst-case regret. So now this is premum is over all. Now, the supremum is over all possible values of A, but the goal is the same, right? We want to choose a strategy sigma that minimizes the worst case regret. Okay, so the punchline here is going to be that modulus, some factor of 1 plus epsilon. We can reduce this to the bounded A problem. So, just a little bit of notation. Of course, these quantities, the expected cost and the regret depend on our time. And the regret depends on our time horizon. The expected cost is an integral from zero to the end of our time horizon. Before, we left this implicit, but now we just want to make that explicit for reasons that will become clear in a moment. Okay, in a definition, we're going to say that a strategy is A bounded with these constants if it satisfies this estimate. And the idea here, right, so our optimal Bayesian strategies. So, our optimal Bayesian strategies in particular satisfy this estimate. So, this is like: we think of A-bounded strategies as strategies that are designed for under the assumption that A belongs to some integral. Then it's natural to satisfy this estimate. Okay, so here's our main theorem on fully agnostic control. So, we have to fix some parameters: the time horizon, the starting position, and these. Time horizon, the starting position, and these constants from our estimate. But then, given any epsilon bigger than zero, we can pick some capital A for which the following holds. So if sigma is an A-bounded strategy on a slightly larger time horizon, T plus epsilon, then we can produce a strategy sigma star for this on time horizon t, which satisfies the following estimates. So if A So if A is between minus A, sorry, I should have called this A max or something, but we're in too deep. So if A is between minus capital A and positive capital A, then it does almost as well as our opponent, or sorry, not as our opponent, as the original strategy, sigma. And if A is outside of this interval, then it does almost best possible, right? It does almost as well as the optimal known A strategy. A strategy. Okay, so the corollary of this theorem is so recalled the hybrid regret. If we let sigma minimize the hybrid regret on this slightly larger time interval, t plus epsilon, among all strategies on that time horizon and for a in this bounded interval, right? interval, right? Then sigma is obviously an optimal agnostic strategy on this interval minus 1 plus epsilon A to 1 plus epsilon capital A. And if we assume that our PDE assumption holds, then by our theorem on bounded agnostic control, we can take sigma to be an optimal Bayesian strategy. And in particular, since it's an optimal Bayesian strategy, it will be A-bounded and satisfy the hypothesis. bounded and satisfy the hypotheses of our theorem. Okay, so then this strategy sigma star from our theorem will then satisfy the following. So its hybrid regret, its worst case hybrid regret on this slightly smaller time interval, will be within a factor of one plus epsilon of the worst case hybrid regret for any possible strategy on this slightly larger time horizon. Okay, so maybe I have a couple minutes. I'll rush through some a bit about the construction of this strategy, Sigma star. So the first thing to note is that there's these two regimes in which we can do almost as well as the optimal strategy. So that's when the starting position Q is large and when the value of the parameter A is large. So the idea here. So, the idea here, if we look at our dynamics, suppose u is zero, right? If either A is very large or Q is very large, then this term AQDT is going to drown out the noise. And so in that setting, you can get some estimate for the value of A just by observing the system for a short amount of time. And so that's, you know, suggests a natural strategy, which is just to observe the system. Which is just to observe the system, use that guess, and then play as though your guess is the true value, right? So when Q is very large, this works. When A is very large, this doesn't quite work. You have to do something a little fancier so that the problem is: suppose you're wrong and you guess a value of A that's much smaller than the true value of A. Of A that's much smaller than the true value of A, then this particle will grow exponentially and you'll see some exponential costs. And the probability that this doesn't happen isn't good enough to cancel it out. But there's a trick, right? We have a strategy for when Q is very large. So by choosing some value Q star carefully, you can put in place like a system to save you. So you wait until your particle hits this. Until your particle hits this carefully chosen position, Q star. And if that happens, then you switch to the large Q strategy. This is. Okay, so let's see. So very quickly, let's just run through how we construct the strategy sigma star from our theorem. So we start with some strategy sigma. That's A bounded for the slightly larger time horizon t plus epsilon. We're thinking of sigma as a strategy for the We're thinking of sigma as a strategy for the problem assuming A is bounded. Okay, so the idea behind our strategy, sigma star, is roughly we do nothing for a short amount of time, we just observe the system, and if A is huge, right, we'll see some growth in a short amount of time, and then we'll play our large A strategy. If we don't see growth very quickly, then we'll assume that A is bounded and we'll play. And we'll play the strategy that we have for bound today. Right, so this is the rough idea. I think I'll skip through the details since we're running out of time. Okay, so just a little bit about future problems we could consider. So we could look at this problem with an additional parameter b corresponding to the control variable. The control variable. So when A is known and this parameter B is unknown, we think we have a strategy that gets bounded multiplicative regret. But it'd be very interesting to find optimal strategies, not just bounded regret strategies. But this is really hard. So even when A is 0 and B is either minus 1 or 1, so you can think of this as a car that's either in drive or reverse, and you have to learn which. Reverse, and you have to learn which, it's very difficult to find the optimal strategies. Okay, and eventually we'd like to look at this problem with multiple unknown parameters. Okay, so that's it. Yes, John, can you hear that? So this looks like a problem that This looks like a problem that calls out to be thought of in terms of reinforcement learning. Have you done that? No, we haven't done that. So I think other people do that, but they typically get results that are good for large time horizons. So what we're trying to do is focus on this problem where the time horizon is very short. This is now like online learning. Yes, this is like online learning. So, this concept that the worst case comes from a particular prior, is it a general concept or is it just true for this problem? With the prior being constant. I mean, so that proof that I gave is very general, right? So you could use this in a lot of different settings, I think. Yeah, just sort of. Yeah. Just to remark, there's a wonderful picture of an Israeli plane with a wing blown off, and the pilot was able to land it safely. Oh, that's great to know. I should include that in my. Yeah, you just like Google that and see what comes up. Thank you. Yeah, that video and the plane is what I used in my review art to privatize this material. So it was a training exercise for paint tech. Was it tensional? No, they were doing a training exercise. One of the planes was upside down and they hit each other. And then when the pilots figured out how to stabilize the plane without the right wing, they couldn't see that there wasn't the right wings or smoke. So they actually sped up and stabilized the plane, and they got out and they're like, wow. It's like a nut. Wow. If they knew the wind was off, they were. The wind was off, they were early. Yeah, they have a meter. How difficult I think the jump would be from the scale of two vector case. Oh, okay, so that's a good question. I haven't thought about it. So I've thought about it a little bit for getting bounded regret strategies. And I think it's doable in that case. I haven't thought about doing it. In that case, I haven't thought about doing it for the actual optimal strategies. Yeah, I don't have anything to say. 