Okay, so the next talk is by Alex Schutter, and he will speak on zooming on Querix. Thank you very much for the introduction, and thank you to the organizers for inviting me here and giving me the opportunity to give a talk. This is my first time in India, and it's been a great experience so far. Okay, so I thought I'd begin this talk by trying to talk to you. Begin this talk by trying to explain how we might arrive at the kind of problem which I'll be talking about. And I'm going to do this through the medium of a flow chart. So the flowchart goes like this. We start with a projective variety V and we go on a journey where we try and find more and more rational points on V. So the first and most fundamental question is, does V have a rational point at all? Is does V have a rational point at all? And if we prove that it does, then we might try and find some more rational points. So we might ask, does it have infinitely many? Or we may also ask, are the rational points Zrisky Dense? Then we might start to wonder about things like weak approximation, which are kind of saying that there are rational points everywhere on the variety and that they can be, rational points can be used. Rational points can be used to approximate idelic points. If we make it that far, then we could start getting more ambitious about the quantitative arithmetic of the variety v. So we can ask things like, can we prove Manin's conjecture, or can we find an asymptotic formula for the number of rational points of bounded height? Okay, so if we've got this far without getting Got this far without getting chucked back to the beginning of the diagram, then we may congratulate ourselves and say, Great, we've proved some new results. However, if we're still curious about finding out even more information about the distribution of the rational points, then we arrive at the topic of this talk. So I face it in the purple box here as how well equidistributed. How well equidistributed are the rational points on V? And hopefully, throughout the talk, I'll make this question a lot more precise. For now, we can sort of think of it as a combination of this weak approximation stuff and the asymptotic formula stuff. Because when we do the asymptotic formula stuff, we're counting points on the whole variety. And when we do the weak approximation stuff, And when we do the weak approximation stuff, we're trying to find points that get closer and closer to a particular idelic point. And in this purple box, we're kind of doing a mixture of the two. So we're trying to count points, but not on the whole variety, only points which are particularly close to a given idyllic point. Okay, so if there are no major objections to this diagram, then I think I'll move on because it's probably quite misleading. Probably quite misleading and oversimplified in a number of ways. So, I'm going to now kind of go back to the beginning. Now we've sort of got some context for the talk. And I'm going to try and discuss this fundamental question. How good are our brains asymptotically? So, what do I mean by this? Well, basically, I'm talking here about dipantine approximation. Talking here about Diefantine approximation. And Diefantine approximation is about how easily we can approximate irrational numbers by rational numbers. For example, if we took the irrational number pi, then probably the most natural way to try and approximate it with rational numbers is to truncate the decimal expansion. I guess because we've got 10 fingers, that's kind of the way we think. That's kind of the way we think. However, these rational approximations by truncating the digits, in some sense, they're not the best ones because they're ways of finding rational approximations for pi, which use fewer digits for a given amount of accuracy of the approximation. This is what I mean when I say more efficiently, pi can be approximated these ways. So these fractions here, three. So these fractions here, 322 over 7, 355 over 113, these are examples of brains because they're the best rational approximations to an irrational number. You can get better approximations without sort of further increasing the denominators. What we want to know in diaphangine approximation is how good are these brains as we allow the denominator to grow. To make this a bit more quantitative, we've got the definition of the irrationality measure. And it's a supremum of the real numbers mu, such that the distance between the irrational number and the fraction p over q is less than one over q to the mu for infinitely many fractions p over q. So, I've kind of colourcaded it a bit here. So, in blue, we've got what I call the closeness of the approximation. In red, I've got a measure of the complexity of the approximation. So you can think of like that's how difficult it is to write down the fraction. And in purple, I guess this is mu, the zoom factor. The zoom factor. A larger value of μ means that it's easier to approximate our number by rational numbers of small denominator. Okay, so here I just say a few known results about this. So, first of all, if a number was actually rational, now when I said brains, I said it was an irrational number, and that's the most interesting case, but And that's the most interesting case, but that was really just for the acronym. You could also do this for a rational number. If ψ is rational, then the rationality measure is 1. Then there's this famous theorem of Roth from 1955, which says that if we have an algebraic irrational number, then the irrationality measure is 2. That's quite often true for transcendental numbers as well. True for transcendental numbers as well, but sometimes you can have a much larger irrationality measure. So the example I've given here is this sum n equals 1 to infinity of 1 over 10 to the n factorial. And this actually has irrationality measure infinity. And the reason is that we've got some exceptionally good brains by truncating this infinite sum in the obvious way. Way. Great. So in 1962, Lang proposed three directions for future research following on from Roth's theorem. One of them was about making Roth's theorem more quantitative. Another was what we now know as the Schmidt subspace theorem. So that's basically been done. And the third was about And the third was about generalizing this to algebraic varieties, which is what we're going to try and do. Okay, so there's quite a natural way of sort of finding analogues of this classical Diophantine approximation for projective varieties. So we replace the wheel line with some projective variety v. We replace the wheel number. We replace the real number psi that we were trying to approximate with a real point on the variety. We replace the rational numbers p over q that we were using to approximate psi with rational points on the variety. And then we have these notions of closeness and complexity. So for closeness, we need to define some kind of distance. Of distance function on points in projective space. And there are lots of different ways of doing this, but sort of the most obvious way is to take an affine chart where the first coordinate is non-zero and then scale out by that coordinate and take the usual kind of sup norm on what's left. And then the complexity of the rational number, we're going to replace that with the notion of. Going to replace that with the notion of a height of the rational point. Again, there are various choices you could make for the height, but I'm basically in this talk going to restrict to the naive height. And what that means is we take a projective point x, we scale it so that it's represented by coprime integers x naught up to xn, and then we take the maximum size of those. Maximum size of those. With all that in place, we now have this very natural generalization of the irrationality measure. And this was introduced by McKinnon and Roth in 2015. We define the irrationality measure of psi on V to be the supremum of the real numbers mu greater than zero, such that the distance is less than one over the height. Than one over the height to the mu for infinitely many rational points x on v. Okay, so I'm now going to discuss an example of this. And the example is the sphere x0 squared plus x1 squared plus x2 squared equals x3 squared. So what you're seeing here is an affine chart of that sphere. Chart of that sphere, and there's one black pixel for each rational point of height bounded by 2048. You get this rather beautiful picture. And what's kind of interesting here is that I guess if you just proved Mannin's conjecture for this sphere, which I guess, as we've already seen, can be done in a variety of different ways, then you'd have no idea that. Then you'd have no idea that there are these kind of weird white circles appearing. I guess you would naively suspect that the rational points would somehow just be randomly distributed on the sphere. And so this picture would kind of look like a sort of static on an old V. But we actually have these white circles. And these actually were called, this is called like the big holes phenomenon by Sarnak. By Sarnak. Okay, so this is the sphere, and then we can start trying to zoom in. So I selected here a region in red to zoom in on. And if we zoom in, we get this picture. So you can probably see now the individual black dots making up the picture. And you can probably also see that in the center of this big white circle is a single rational point. Circle is a single rational point. So, kind of what we're seeing here is some kind of repulsion effect, where a rational point of small height doesn't have as many other rational points of small height close to it as you might expect. If we keep zooming in, then eventually we find ourselves in a situation where there aren't any rational points in the red region. Any rational points in the red region at all, other than this single point that we zoomed in on. Now, we plotted here points of height bounded by 2048. If you started looking at points of larger height, then gradually we would sort of encroach on this white region again. But then, if you zoom in further, then you can avoid those new rational points and so on. So, there's this kind of tension. So, there's this kind of tension between how quickly do we zoom in on a particular rational point and how quickly do we allow the height of the rational points we're considering to grow. The rationality measure is really trying to quantify this. So it's sort of about the rate that we can zoom in compared to the height while still having rational points. All right, so I've just got a couple of other examples which are supposed to demonstrate one other phenomenon that can occur. I'm going to focus on the one on the left, which is P1 cross P1. And again, here we've plotted rational points of height bounded by 100 or something. And we're going to imagine that we're zooming in on the very center of this left-hand diagram. And we actually have. And we actually have three different things that can occur depending on how quickly we zoom in. If we don't zoom in at all, or we don't zoom in very much, then you imagine looking at this whole diagram. And we have rational points sort of everywhere on the variety. If we zoom in a medium amount, then at some point we only see rational points on one of these two lines, which goes through the center point. Through the center point. And then, if we zoom in a lot, then eventually we will again only see the single rational point in the very center of this diagram and no other rational points. If we think about sort of Manin's conjecture, we have there the notion of accumulating sub-varieties where the count of rational points on them dominates the count on the whole variety. And dominates the count on the whole variety. And we say, those rational points, they're not so interesting. So let's remove those and try and count what remains. We can try and do a similar thing here. So we can say that the rational points on those two lines, which go through the particular point we've chosen, they're not so interesting. So let's remove those. These are called like locally accumulating sub-varieties. So let's remove those and So let's remove those and then let's see how much we can zoom in before we see no other rational points than the one in the center. So this motivates the definition of the essential constant. So it's like the rationality measure or the approximation constant from a couple of slides ago. But this time we also take an infimum. Also, take an infimum over the Zrisky open and dense subvarieties of V. And then, when we do the approximation constant, we only look for rational points approximating psi, which lie on this sub-variety y. So, this definition gives us the flexibility to remove these unwanted lines from the previous example. Okay, so the essential constant hasn't really been that well studied, but there are a few results. So it's known for curves. So curves defined over Q. Basically, for a genus zero curve, if it's got a rational point, then it's going to be birational over Q to P1. And then you can reduce to the kind of Roths theorem from the classical diaphragm T. Theorem from the classical Diefantine approximation. And if the genus of the curve is greater than or equal to one, then actually the set here in the definition of the essential constant is going to be like empty because, okay, so if you've got a genus greater than or equal to two, there are only finitely many points anyway, by Fulting's theorem. By Fulton's theorem, so you're going to have a hard time having infinitely many solutions x in y of q. And then if you've got a genus one curve, so you've got an elliptic curve, then it might have infinitely many rational points, but they're still extremely sparse. So the number of rational points is only growing like a power of log. And because the inequality, like distance less than one over the height to the mu is a kind of poly The mu is a kind of polynomial inequality. It's still only going to be satisfied for finitely many of these rational points on the curve. The essential constant is also known for a few other examples, like some products of projective spaces and certain toric Del Perze surfaces. In this talk, we're going to be focusing on quadrics. On quadrics. So I've got the following proposition which tells us about the essential constant for a smooth quadric in four more variables. And it says that for any rational point on this quadric, the essential constant is equal to one-half. I'm going to try and sketch a proof of this because it's quite nice. So the proof goes by a So the proof goes by establishing the two inequalities less than or equal to a half and greater than or equal to a half. The less than or equal to a half, this is a repulsion argument. It's essentially actually a generalization of Sarnak's big holes phenomenon that we saw in the example. So we're trying to show that there aren't that many other rational points close. Other rational points close to the point psi. All right, so the way we do this is we apply this linear change of variables, which sends the quadric to the quadric x0 x1 plus f2 of x2 up to xn for some other degree two homogeneous polynomial f2. And also that sends the point psi to the point 1,000. 00. So after a bit of thought, you can see that the essential constant is unchanged by this procedure. Now, because we're looking at the essential constant, we're allowed to restrict ourselves to a Zariski open and dense subvariety y. And we're going to pick the particular one given by x0x1 as non-zero. We then have to choose a height and a distance function. And we're going to choose the same ones as before on this new variety, V primed. So you may wonder, hang on a minute, like, does this not depend on the choice of height and distance function? And this was discussed by McKinnon and Roth in their paper. And of course, it would change if you. Change if, like, you had, if you change the height to be like a maximum of something like the x i's to the power of something else. If you choose an equivalent height, so one which only differs up to a constant of the one given here, and sort of similar kind of statement for the distance function, then the essential constant isn't affected by those kind of changes. So, what that means is it's okay to choose these sort of naive height and distance functions on our new variety v par. Functions on our new variety v primed, even though sort of it's that wouldn't be the naive one on the original variety v. Okay, so with all that in place, the remainder of the proof of this part is quite nice. So we have this chain of inequalities where we start with the distance squared. By definition, the distance squared is the maximum for i between 1 and n of xi squared over x. 1 and n of xi squared over x0 squared. To get a lower bound, we just throw away the i equals 1. So now we're just taking a maximum from 2 up to n. And then this is greater than greater than the f2 of x2 up to xn over x0 squared. And that's just because I'm allowing the implied constant to depend on the coefficients of f2, and f2 is a quadratic. And then this, by the equation defining v primed, is equal to x0x1 divided by x0 squared, which simplifies to x1 divided by x0. And then because we restricted ourselves to be on the sub-variety y, we know that x1 is non-zero. And I've also scaled things so that x1 is an integer. So that x1 is an integer. So we can lower bound this by 1 divided by x0. And then finally, the height is the maximum of all of the xi's. So this is greater than or equal to 1 divided by the height. If we rearrange this a bit, then we conclude that the distance can only be less than 1 over the height to the mu for finitely many points x if we chew the mu. Points x if we choose a mu greater than one half. And therefore, we've shown the less than or equal to a half part. To show that the essential constants greater than or equal to a half, we're trying to show that there are in fact quite a few rational points which approximate the rational point psi. And this is done by looking at conics, which are contained in Which are contained in V and which go through the point psi. If we imagine we had such a conic, then by the result I mentioned before that we know the essential constant for conics, we actually see that the essential constant of psi on V would be greater than or equal to a half. But there's a slight catch because in the essential Catch because in the essential constant, we would be allowed to sort of remove this conic because it's a Ziriski closed sub-variety of V. So to get around that, we actually say we don't just have one of these conics, we've got a whole family of these conics. We can get these conics by intersecting V with a general plane. And as long as we get a smooth conic, And as long as we get a smooth conic defined over Q, then we can use this argument. As we allow this plane to vary, these conics sweep out an open and dense subvariety of V. And so there's just simply too many of these conics to remove them all. And so always one of them will survive, or lots of them will survive. And so we can apply this argument. This is kind of a more Kind of a more general phenomenon that the best way to approximate a rational point psi is actually by using very free curves which go through the point psi. Now that's, you know, that's a conjecture or maybe just a belief, but that's the kind of idea. And these very free curves, moreover, should have like the lowest possible. Moreover, should have like the lowest possible degree, so you want to take the same height that you had on the That you had on the Viet V and then take just the points on the subway T, yeah. No, no, that's right. Yeah, so actually, yeah, definitely the things like the essential constant, they're not like a birational invariant or anything. So, the essential constant on P1 is one, I think, and on a conic is one half. Yeah. One half, yeah, all right. Um, so this proposition is great, um, but it's not quite the end of the story for a number of different reasons. Firstly, we assumed that the point psi was a rational point. We can kind of think of this proposition actually as the This proposition actually as the analog of the result that the irrationality measure of a rational number is one. But we haven't really said anything about real points psi. And if you think about the proof, it's very important that the kind of, I guess in the first part, that the linear change of variables is defined over Q, and in the second part, that these conics are also defined over Q. Another point. Another point is that here we've done a sort of real zoom, but we can also define p-adic zooms or even idelic zooms. So we're going to try and do that. And finally, we haven't so far really made this completely quantitative because in the definition of the, if I just go back, in the definition of the essential constant, we've got Constant. We've got this thing where we're saying we've got infinitely many solutions in q. But sort of how many is infinitely many? We can try and make that quantitative by also only counting points of bounded height here. So I'm now going to try and sort of set up the main theorem of this talk. So, first of all, to define the p-adic zooms, The piadic zeems, it's very similar to the real zeem, except in the distance function, you want to take the piadic norm rather than the sub-norm. What this really means, essentially, is that we're requiring the rational point when it's scaled to be like copim integers, we're requiring it to lie in a particular congruence class modulo. Class modulo some large power of p. Okay, and I'm going to sort of put together all of the p-adic seams into one condition using this parameter L. So we're going to have the condition that the p-adic distance is less than p to the minus the p-adic valuation of l for every prime p dividing l. For the real zeem, we're also going to introduce the parameter R and say that the distance d infinity should be less than R inverse. Then if we want to be even more fancy, we can put all of this together into an idyllic seam condition by defining an idyllic neighborhood of an idyllic point ψ which Psi, which is just the idyllic points on V, such that all of the piadic zeem conditions and the real zoom condition hold. We then define a counting function, n psi brl, and this is the number of rational points on x or on the v of bounded height, which also lie in this idelic neighborhood. Now, if Now, if R and L were fixed and you allowed B to tend toward infinity, then actually we already know how this counting function behaves. So, for example, in a paper of pair, he observes that for any generalized flag variety, we've got this kind of equidistribution result. And these smooth quadratics are a special case of that. But, sort of, the point of But sort of the point of this talk is that we're not fixing RL. We're allowing RL to grow in terms of B. Right, so in this slide, I'm just kind of trying to explain one naive way that we might try and guess what the counting function and psi BRL should behave like. And this heuristic is based on the Battery of Manning conjecture. Is based on the Batyov-Manning conjecture that we've already seen a few times this week. So the conjecture is about counting points on Farnay varieties of bounded height. And it says that after you remove some appropriate accumulating subset, then you should get like a constant times B to the A times log B to the B for sort of explicit A, B, and C in terms of the geometry of the variety. In terms of the geometry of the variety, I'm going to focus here on the leading constant, which was given a conjectural interpretation by Pear. The leading constant takes the form C equals alpha beta tau over A times B factorial. The alpha and the beta factors aren't really very interesting for quadratics. So let's look at the tau factor. So let's look at the tau factor. And this is defined to be a Tamagawa measure. It's a Tamagawa measure of a sort of Brownian set of the idelic points. Using this, we have the following natural prediction for the counting function. We say that n ψ B R L should also behave like a constant times a power of b times a power of log b. And here's And here, the exponents little a and little b, these should be the same as before, but we want to adapt the leading constant to reflect the zoom conditions. And the way that we would naturally do this is we say the leading constant is the same as before, except we now only take the Tamagawa measure of the idelic neighborhood that we're looking at, rather than the Tamagawa measure of the whole variety. Variety. Okay, now if you are sort of blindly to have faith in this kind of equidistribution heuristic, then at some point you come unstuck. Because as we've seen in the example of the sphere, we've got these big white spaces. So if I go one more time to the picture, we've already seen that if you zoom in too much, then we can. Much, then we can't expect equidistribution to hold anymore. So, maybe I should just say: if you're maybe not so familiar with this kind of stuff, intuitively, what we're saying with all of this Manning conjecture with Zoom thing is that if you want the prediction for the counting function and psi BRL, then you take the prediction for the Prediction for the number of rational points of bounded height on the whole variety. And then you multiply it by the ratio of the volume of the red region or the idelic region that you chosen to the volume of the whole variety. So that's really why it's a kind of equidistribution heuristic. Okay, so I'm now going to come on to the statement. Going to come on to the statement of the main result of this talk. The setup is: we take a smooth quadric B defined by some homogenous polynomial F. We assume that it's got points everywhere locally. So, by Hasominkovsky theorem, this means that we've got a weak approximation. We're going to define the distance function to be Distance function to be the distance function that becomes the naive distance function after we apply the linear change of variables like we did in the proposition. This kind of just makes the analysis a little bit easier. Okay, and then we have the following result. So, this is joint work in progress with Demars Chindler and Zhijong Kwang. It should very nearly be ready, at least some of it, to be put on archive. But admittedly, I have been saying that for several months now. So we'll see. Okay, so the first part is we suppose that n is at least four, so that means the quadric has five or more variables. We suppose that L times R behaves like B to the mu. Behaves like b to the mu for some mu between naught and one half. Then we have the asymptotic formula for the counting function, which we would expect from the Manin's conjecture with zoom. In particular, this actually means that the leading constant C L R, this will behave like a constant C which only depends on the quadric and not on L and R. quadric and not on l and r multiplied by l times r to the minus n minus one which is like the the dimension of the uh the quadric okay and then we've got the second result which is when n is three so this is quadric surfaces and we assume that v is a non-split then the same conclusion holds provided that either we take all you That either we take L equals one, so that means that we're not doing any p-adic zooming, we're doing a wheel zoom only. Or if we want to do both p-adic and wheel zooms, then we can do that, but we have to assume that mu lies between naught and one half. So we can't go all the way up to sorry, naught and one fifth. So we can go all the way up to one half like we did in part one of the theorem. Part one of the theorem. Importantly, this theorem is actually optimal. So, what I mean by that is that in part one, if you were to try and increase the value of mu to something beyond one half, then sometimes this Manin conjecture with zoom would fail. And similarly for part 2A, for part 2b, 2a. For part 2b, we would expect to be able to go up to mu is one half, but that seems very hard to prove. As a corollary of this, we kind of get an analytic proof of the proposition we had before, which we can also generalize to zooming in on real points or idelic points or p-adic points or whatever we like. So we can actually. So, we can actually conclude from the theorem that the essential constant is always at least one half. And as we saw in the proposition, if we took a rational point, then the essential constant is equal to one half. So you might wonder: is the essential constant ever greater than one half? We would have to. We would have to pick a non-rational point for that to be the case, but can that be done? And the answer is yes. There will be cases, there will be choices of psi such that the substantial constant is larger. But that's not something that we really investigated in this project so far. Okay. So I'm just going to briefly mention some related results. Related results. So Sadawi studied optimal strong approximation for an affine quadric given by f of x1 up to x1 equals n in five or more variables. And the zoom condition was imposed by projecting onto f equals one. In 2019, Browning and Lochman studied solutions to Solutions to quadratic forms where you impose congruence conditions. Essentially, this was like the Wilson conditions which I was talking about. And they obtained an asymptotic formula, provided that L goes like b to the mu, where mu is less than one-third minus 10 over 3 times 3m plus 1. So this is not optimal, even as the number of values. Even as the number of variables n tends towards infinity. Then in 2017, Browning, Kamaras-Moui, and Steiner proved optimal strong approximation on this sphere, assuming some kind of natural conjectures, like the twisted Linux conjecture, which is a bound on a sort of twisted sum of Glostermann sums. Okay, so it seems in this conference there's been no escaping the circle method. And if you're not a fan of the circle method, I've got more bad news for you because the proof of our theorem and also the proof of all of these results goes via the delta method. So I'm going to sort of give a very brief bash course on what is the circle method. So, this is kind of the classical circle method. And it begins with the identity that if you integrate e to the 2 pi i alpha n with respect to alpha from naught to 1, then you get 1 if n is 0, because the integrand is identically 1. And you get 0 if n is a non-zero integer. We now suppose that we. We now suppose that we've got some polynomial and we've got some finite set A in Z to the n that we're interested in. So this could just be a box defined by our height. And we suppose that we want to count the number of points in A which vanish on F. To do that, we take a sum over all of the candidate values x and a and then we And then we plug in f of x for n in this identity. So we get an integral of e to the 2 pi i alpha f of x. Then we can switch the order of the summation and the integration, and we get this integral from naught to one of some exponential sum s of alpha. To qualify as a version of the circle method, according Version of the circle method according to Simon's definition, we now need to do some kind of nice decomposition of the interval 01. And classically, this is kind of done with so-called major arcs and minor arcs. The major arcs, from which we hope to get the main term in our asymptotics, consist of alphas which are close to a rational number with small denominator. And the minor arcs are. And the minor arcs are everything else. Typically, bounding the minor arcs is the hard part in the circle method. And the kind of techniques are things like vial differencing, where we sort of apply petty sparts lots of times to gradually reduce the degree of the polynomial F until we have something linear that we can deal with. So, to briefly indicate how we might apply this to our particular problem, the first step is we need to pass to the fine k because the circle method doesn't count projective points, it counts points in z to the n. So I've got this related counting function n primed. And in orange here, we've got the condition that the point counts on the variety. In purple, In purple, we've got the height condition, and in green, we've got the zoom conditions. We're then going to shove into the definition of the set A, the height condition, and also the real zoom condition. And then n primed is going to be an integral from naught to one such that this height condition and the zoom conditions hold. And then we plug in f of x into our exponential. Into our exponential. So we get e to the 2 pi i alpha for x d alpha. So that's all very well, but unfortunately, the classical circle method isn't going to be sufficient for our problem. And sort of the reason for this, as we've sort of already seen this week, is that the classical circle method is inherently wasteful when we try and bound the minor arcs. And bound the minor arcs like this down here. So we've crashed through with these modulus signs on S of alpha. And the moment we do that, we lose any hope of getting any cancellation over the different alphas. And actually, you can see that the classical circle method can't count points on quadrics in four variables, even if you have no zoom conditions at all. So if you At all. So, if you assume square root cancellation, which is the best you can hope for for the S of alphas, then you still get an error term of the same order of magnitude as the main term when you try and deal with the minor odds. If you had more than four variables, then you could use the classical circle method to get an asymptotic formula, and you would be able to do some zooming, but you Able to do some zooming, but you wouldn't ever be able to do the optimal amount of zooming like we had in our theorem. So that's why we have to use the delta method. So this was introduced by Juke Friedlander Newanietz in 1993 and then further developed by Heath Brown a couple of years later. And the starting point is this alternative expression for the indicator for. Expression for the indicator function of an integer being zero. Okay. So to apply this, we have to do the change of variables as in the proposition. And once we've done that, we also need to define a smooth weight function in order to apply the delta method. This is what I'm calling WBR, and it's a smooth weight function. And it's a smooth weight function, which is supposed to approximate the characteristic function of the height condition and the zoom conditions. Okay, so then if we apply the delta method, then n primed should be like the sum over cq over q squared. C q is basically one. And then we have a sum over q's and a sum over the c's of some. Of some exponential sum and some exponential integral over q to the n plus one. And these exponential sums and integrals are a little bit messy, so I decided to omit those. But if you're curious, you can ask me. Okay, so I've got, I guess, one minute left. I'm just going to very briefly go through a couple of technical remarks on the proof. On the proof. So one thing is that usually when we do the circle method, we pass the apine cone and then we do a fairly routine application of Merbius inversion to reintroduce the copimality condition. Here there's a bit of a wrinkle because when we try and do this Merbius inversion, we're replacing B with B over D, but we're not changing the amount. But we're not changing the amount that we're trying to zoom by. So eventually, for large values of d, we'll be in a situation where mu is greater than one half. So we need to apply some auxiliary bounds, which are just upper bounds, which also apply beyond this mu equals one half barrier. We actually get a similar issue when we try and remove these smooth weights and get a sharp cutoff. The reason for this. The reason for this linear change of variables is maybe not immediately clear, but actually, it's essentially so that we can deal with the bounds for the exponential integrals better. And this is kind of analogous to what Sadari does when he has this new coordinate system that he uses in order to estimate partial derivatives. There's a disadvantage of this linear change of variables, though. This linear change of variables, though, because the moment that we do it, we kind of lose these intrinsic Diophantine properties of the point ψ. So, this is kind of why we're never able to establish in our current method a situation where the essential constant is actually strictly greater than one-half, which, as I said, sometimes probably happens. Okay, and then to actually deduce stuff of value. And then, to actually deduce stuff about the essential constant, we would have to fix a proper closed subvariety from the start and bake that into our weight functions. Okay, and finally, the reason that we struggled in the case n equals 3 is because the exponential sums are quite hard to estimate. So, what we really need is better bounds for quadratic exponential sums, which are Sums, which are restricted to congruence classes, where the congruence class is modulo L for a large integer L. Thank you very much for listening. Any questions for the speaker? Right, yes. Yeah, I'm not sure actually. Yeah. Yeah, as far as I know, in all examples I've seen, I think it's the same for every rational point. Yeah. Yeah, of course, you have to have the essential constant for that if you just have the. Constant for that. If you just had the approximation constant, then you could have these locally accumulating sub-varieties. But yeah. Probably, yeah. I think, yeah, in the examples you've looked at, it's never quite exotic enough to require a consider. Okay, so at the risk of showing that I didn't understand the talk, can you say a few more words about the white holes? Sure, yeah. So these, yeah, so these white holes basically, if I yeah, so these white holes here, these, yeah. These, yeah. The reason that you see these is essentially to do with the proof of one part of the proposition that we had. So it's kind of this repulsion argument that we had here. So, yeah, essentially, Sarnak gave a proof of this white holes phenomenon for the sphere. And then, this is like a general. The sphere, and then this is like a generalization of his proof. But we can kind of think of it like an analog of the situation for rational points, where for rational points, the irrationality measure of a rational number is one. But the irrationality measure of an irrational number is always larger than that. So it's like it's harder to approximate rational numbers by other rational numbers than it is to approximate irrational numbers. Numbers than it is to approximate irrational numbers by rational numbers. And that's a little bit like what we're seeing here, where in the picture, like we've got this rational point, and it seems to repel the other rational points close to it. Can you repeat the question? Yeah, so the question was: Do we have any upper lower bounds for the essential constant in the case of qubit hypersurfaces? I'm not sure. I think you could, in principle, using the sort of classical circle method like Birch, get some upper bounds for it. Some upper bounds, which I would be surprised if they were ever optimal. But yeah, I think you could get something for the, I guess, which way around is it? So using the circle method, you'd be getting a lower bound for the essential constant. To get an upper bound, Constant to get an upper bound. I think you could probably generalize the other part of this proposition by trying to intersect with some planes and then looking at some curves. Oh, yes, please. I'm not sure actually. This is something I've wondered. Like, I guess, is there any analogue of this situation of the Liouville number I mentioned at the beginning, where you got this infinite sum over, you know, one over 10 to the n factorial? Could you construct any particularly special point like that, which is very well approximable by rational points? It's an interesting question. rational points. It's an interesting question.