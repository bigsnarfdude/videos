Of the afternoon. Very happy to have Stan Nelson from Florida Institute of Technology and the famous ACS. Thank you, Nestor. And I want to thank all the organizers. Like a lot of you, this is my first in-person conference for a few years, so I really appreciate you guys putting this together. Yeah, so everything I'm going to talk about today is joint work with Chris Henderson from Arizona and Andre Tarfulia from LSU, both of whom are in the audience. LSU, both of whom are in the audience today. So, okay, we're interested in the Boltzmann equation, which we've already seen a couple of times today, but there are a lot of different mathematical approaches to the Boltzmann equation. So, there it is, of course, you have transport on the left, and on the right-hand side, you have the collision operator. And so, we're looking for local existence, so existence of the solution on some time interval 0t given some initial data, and we're Some initial data, and we're looking at what I call large data solutions. So, in other words, far from, not necessarily close to a Maxwellian. And we're interested in the non-cutoff collision up, which I'll remind everyone what that is. So, global, this is a regime, right? Large initial data, where global well-posedness is very hard, most likely out of reach at the present time. So, instead, we're looking at the local existence. Looking at the local existence, and the really interesting thing is to try to take initial data that's as general as we can, right, in a few ways. So low regularity, slow decay for large V, could have vacuum regions in the initial data, and we don't assume that your solution decays as X goes to infinity. So, as many of you know, right, these first three properties are in some sense very linked to each other because if you want to prove regularity, well, you need To prove regularity, well, you need to know about the K of F so you can control the collision operator. And also, the vacuum regions are going to impact the positivity properties of the solution, which are what you need for a coercive. All right, so I'll get more into all of those. So, all right, yeah, we've seen it today already, but okay, here is the usual Boltzmann collision operator. And so, okay, you've got these two terms, the gain term, which measures particles. Measures particles colliding from other velocities to V. And then you've got the loss term, which is particles colliding from V to some other velocity. And you're sort of weighting them by the same in both directions because collisions are reversible at the micro scale. Okay, and so how are these four velocities related? Well, they lie in a sphere because of conservation of momentum and energy. And this is how you parameterize the sphere with this sphere. You parameterize the sphere with this sigma. So the formulas are here, or you can look at this diagram. And then this angle theta is the angle between pre- and post-collisional velocities. And so when theta goes to zero, we call that brazen collisions, where the direction barely changes. And okay, what collision kernel are we going to take for capital B? So there are lots of different choices that are interesting and that people study. So we're taking the so-called non-cutoff. So we're taking the so-called non-cutoff kernel. So you've got a product of what we call the kinetic factor, v minus v star to the gamma, where gamma is greater than minus 3. And we have what's called the angular cross-section, which has a singularity of this order as theta goes to 0. So s is between 0 and 1. And it's called non-cutoff because it's the opposite of the Gradz cutoff regime, which is where you assume that your collision kernel is integrable over the Collision kernel is integrable over the sphere. So that's there's a whole body of work about that, but the properties are rather different and it ends up being a rather different mathematical problem. So the key property here is that you have this singularity for Grayson collisions, turns this into an integral differential operator, which is giving you some fractional derivative of order 2s in V. So by contrast, in the cutoff case, you don't have any smoothing effects, and if you have some. Any smoothing effects, and if you have some singularity at time zero, it will probably fall. But this is what gives the equation a smoothing property. All right, just to clarify what we're going to study, so I told you that gamma is anything greater than minus 3. We are going to focus in this talk on gamma less than 0, as we did in the paper. That's the only restriction we place on gamma. So we can do any gamma between minus 3 and 0, and any s between 0 and 1. Yeah, the greater the other cases of gamma are also interesting. I think they're in reach, but the analysis is suitably different that it doesn't fit in the same paper. So I'm still open, though. Okay, and I'll just say that when you get close to minus three, there are fewer tools available. So a lot of the prior results we're using break down or at least. Break down, or at least are not, I shouldn't say break down, have not been worked out in the case when gamma is very negative and your singularity there is more severe. All right, so yeah, what's the motivation of this study? So we want to take irregular initial gaps. So why is this interesting? So, like I said, this Q ends up being a derivative operator of fractional order. And so Fractional order, and so this induces a hypoelliptic smoothing effect. So, in other words, if you just looked at the space-homogeneous case, so no x-derivatives, then you would have something that's like a non-linear, non-local parabolic equation. And then in some sense, that would be a little bit more straightforward because the sort of smoothing operator touches your entire spatial variable. But in this case, right, the full inhomogeneous case, your equation depends on x, so your Your equation depends on x, so your diffusion term doesn't touch x at all, so there's a degeneracy. But nevertheless, people have worked out that you do still get a smoothing effect based on the mixing between transport and the velocity diffusion. So that's basically what hypoelecticity is in this context. Okay, and so in terms of the literature, there's a lot of results I could quote, but I'll only quote a few in the interest of brevity. So for the case we're interested in, which is large data. Case we're interested in, which is large data inhomogeneous. Three of the main ways this has been understood. You have entropy dissipation, which Bob also mentioned in his talk, very well known. You've also got, there's a result from Chen and Heerating hyperelliptic estimates. So, this is basically a velocity averaging approach. If you assume enough initial smoothness and decay, I think it's something like H5 and infinitely many moments. And infinitely many moments, then you can get a fractional smoothing effect in all variables from velocity averaging, which you iterate to get up to C infinity. Okay, but then more recently, you had this breakthrough by Himber and Silvestre, which we call conditional regularity. So this was regularity estimates in all three variables that are global in the whole space and that are only in terms of these zeroth order bounds, mass, energy, and energy. Bounds, mass, energy, and entropy densities, which are also physically reasonable. So, this was a long program, but the most recent paper was published in JAMS this year. And we're going to borrow some techniques from this program in order to understand the smoothness of our solutions. Okay, but the upshot is, right, that this a priori regularization effect, let's call it, has been understood in various ways for some time. Okay, and Okay, and so given that, right, you would expect to be able to take irregular initial data and construct a smooth solution. Well, smooth enough, let's say, smoother than the initial data, right? Like the heat equation is sort of what you're expecting to see. So in some regimes, this has been done, right? So for close to equilibrium solutions, you have several works from the past few years taking initial data in low regularity spaces. Data in low regularity spaces. And there are, you know, Luis and I have one result in this direction. There are a few, so this one here was the first that I'm aware of to take weighted L infinity initial data. I'm maybe lying a little bit here because not all of these results actually some of these results stay in the low regularity setting and just prove that their low regularity solution exists for all time. But as far as I understand, that's kind of the hard part, right? Once you have that, That's kind of the hard part, right? Once you have that, if you're in the close-to-Maxwellian framework, then I think in most cases you should be able to get higher regularity, at least with minimal additional work. That's my understanding. Okay, then the other regime where this has been done is the space homogeneous, right? So you assume your solution is independent of x, and then it becomes like a nonlinear parabolic equation. And then it's a simpler method, right? You can iterate your smoothness in the sort of normal way. Your smoothness in the sort of normal way. And yeah, there's several papers in this direction. I think David Led Venberg is one of the standard ones. All right, but what you don't see is results like this for the large data case. Okay, so yeah, like I just said, the prior existence results for large data, classical solutions. I say classical to rule out the renormalized solutions, which I'm not talking about. Normalized solutions, which I'm not talking about. Okay, so the majority of results in this direction, you require at least four Sobolev derivatives, and you need either Gaussian decay or high-degree polynomial decay in the velocity variable. So just give some examples. Again, it would take too long to list the precise hypotheses of all these works. They apply to different ranges of gamma and s, and they impose And they impose different conditions, both at spatial infinity, some are for periodic solutions, some are for the whole space, and so on. But they all have need at least four sub-web derivatives plus some kind of high decay assumption. There are, as far as I know, two exceptions to this, but they only hold for limited ranges of the parameters, right? So when s is between zero and half, you do have this. Between zero and half, you do have this paper by the so-called Amuxi group, and they needed still at least two solo-left derivatives, so it's still not a zeroth-order space. The number of derivatives they needed depended on S in a sort of complicated way. And then there was another theorem by Chris and a postdoc who's working with Wayne on Wang. So these two results were in the same paper. But basically, I think Chris is going to talk about this later in the week, so I don't need to say much about it. In the week, so I don't need to say much about it, but that was for C1 in XV initial data weighted by some polynomial and velocity. All right, so those are the that's it if you want to get below four derivatives. But what we really wanted to see was something with a zeroth order initial data, quote unquote. And so I think it's easy to make the argument that there was a gap in the literature here, and so that's what we wanted to fill. So let me just state our main result. State our main result, let's just, yeah, we're going to use throughout the talk this standard angle bracket notation. Okay, what are we going to assume about our initial data? It has to be non-negative, of course. You're going to assume that it has pointwise polynomial decay uniform for some exponent greater than 2s plus 3. So recall s is between 0 and 1, so you could get below even q equals 5. Even q equals 5. So, in particular, the initial data could have infinite energy density, which is, I thought, pretty good. And then we also need to assume some kind of lower bound condition. So I'm going to say more about this in a little bit. But basically, okay, so what do we have? We have uniform positivity in some possibly small ball in Xb space. So outside of the ball, you don't care. You could have very, very wide vacuum regions. But But you do need a little bit of quantitative positivity. Like I said, we'll return to that a bit. But I would say it's a fairly mild positivity assumption compared to some other conditions you see in the literature. Okay, so then you have some time depending on the size of the initial data and a classical solution, I'll say more about what I mean by that, so that you're in the same space for positive time and you have local Helder continuity. And you have local Helder continuity of order 2s plus a little bit. And I say in the kinetic scaling, which I'm being vague about, what this means is, you know, just like for parabolic equations, you're relying on Schouder theory, basically, and so we expect to scale the different variables differently, and so you get different scaling between t and x and v. But you call it 2s, order 2s. And then how does it agree with the initial data? And then, how does it agree with the initial data? Well, I'm not going to write it, but basically, it agrees with. I mean, so your initial data could be discontinuous, of course, but you have smoothness for positive time. So there's really not much way to guarantee that you're continuous as time goes to zero. So that's okay. You still agree with the initial data in the sense that if you integrate against test functions, you get what you expect here. Now, under the additional assumption that you have continuous initial data, That you have continuous initial data, then we can, in fact, show that you agree with the initial data in the other license. Alright, so I guess what I should explain next is what do we mean by a classical solution? So what our proof gives is that you have local C2s alpha continuity. This is a Helder norm, and where the alpha may not be uniform, right? But for any compact But for any compact domain that stays away from time zero, you'll get some alpha. And so, what is it with respect to? It's respect to a distance that is somehow adapted to the scaling and translation symmetries of linear kinetic equations with the appropriate order of diffusion. And I'm not going to write it because it's a lot of notation, but that's sort of the, think of it as a generalization of the sort of a parabolic scale. Okay, so it turns out that it can be shown that this norm is enough to make point-wise sense of this material derivative and also of the collision operator at any point V. So we have classical solutions in the sense that the left and right side of the equation have pointwise values and are equal. In fact, can be identified with Helder continuous functions. So what we don't get necessarily without further assumptions is the pointwise existence of Pointwise existence of the time and space derivatives separately from this. So you can get that if you assume more decay for the initial data. So if you're a purist in your notion of, if you want the classical definition of classical solutions, you can assume some larger number of velocity moments initially and you'll get a sort of true pointwise solution in that sense. So, right. Part of our theorem statement, actually, which I didn't put in the Actually, which I didn't put in the last slide, is that you can get more decay, you can get more irregularity for positive time by assuming more decay. So whatever derivative you want to exist for positive time, there's some Q depending on that multi-index so that the derivative will exist for positive time if you assume this high enough moment. And in particular, if you take initial data to K. You take initial data decaying faster than any polynomial, you'll get C infinity for positive time. But of course, all these regularity estimates must blow up at time zero because you're not assuming any nice regularity properties of the initial value. Alright, so that's our notion of solution. So yeah, I said I would tell you more about this local positivity thing. So I'll just rewrite it here. You need to assume that, okay, in some small, possibly small, In some small, possibly small ball, you have some f initial being uniformly positive, right? So the radius and the constant delta are strictly positive, and you just have this at some fixed location. So, okay, clearly, if you had a continuous initial data, you would always get this for free as long as you're not identically zero. But the whole thing is that we're taking discontinuous initial data, so you need to make this extra assumption. Assumption. Right, and so how do you use this assumption? Well, we have a prior result from a couple years ago that spreads lower bounds like this to the entire domain for positive time. And that's what you need for coercivity properties of Q, basically. So you need some kind of lower bound for F, either point-wise lower bounds or for the mass density, in order to apply any kind of regularity. Apply any kind of regularity estimates based on this. So if you have vacuum, then you don't necessarily understand if this is smoothing, really smoothing things or not. Okay, so that would be why you need the local positivity. Now I will say though that if you remove this assumption, then you can still get weak solutions. So I'll talk about that next, at least just to state the theorem. So, what do we assume here? You just assume non-negativity, not necessarily uniform anywhere, and you have the same kind of decay hypothesis. Notice that since gamma is negative, this is actually a slightly weaker decay hypothesis because we can get away with less regularity because we don't, sorry, less decay because we don't need to apply any regularity estimates at all. And so, if you assume just this, then you get a weak solution just living in this same decay space. So, what do I mean by weak solution? I mean, in the sense of integration against smooth enough test functions. So, what does that look like? You have at times, you have the initial data appearing here, and then, okay, you put your t and x derivatives on phi, and then you need this so-called weak form of the collision operation. So-called weak form of the collision operator, and this is which is exactly here, right? So, again, in some sense, you can derive this using symmetrization and the pre-post-collisional change of variables. This has been known about for a very, very long time. It goes back all the way to James Clerk Maxwell. But the point is, you're putting essentially the integral differential part on the test function. And so you can evaluate this thing using no smoothness of x, right? You just need some, you know, our weighted. Our weighted L infinity norms of F are certainly good enough as long as you have two V derivatives for V. So that's our weak solution. This is, in some sense, a stronger notion, I would say, than the renormalized solution, but right. So I think it's kind of an interesting question, but it's still open: do you really need this positivity assumption to get rid of the positive? Assumption to get regularity for these weak solutions. So we do with our method, but you know, one could imagine that a better method exists. So actually, I haven't said that yet. I mean, think about if this initial data is uniformly zero, then of course you expect the solution to just be zero, and that's perfectly smooth. On the other hand, if you're uniformly positive in a small ball, then you're smooth, right? So that leaves open this intermediate case, but you might be convinced that. But you might be convinced that it shouldn't be a problem. But you'd need new techniques. I mean, somehow this, I think, would come down to new regularity estimates that get away with weaker positivity properties for F. I mean, I was kind of thinking about this as I was making my slides. How would I try to prove it? You might try to say something like, well, cut off your solution at some level epsilon. And if you're above epsilon, you're contributing to a coercivity. And if you're below epsilon, you're small, so you might not screw things. You're small, so you might not screw things up too much. You could try something along those lines, in, like, let's say the DeGeorgi C-alpha estimate, but it doesn't necessarily sound easy. Anyway, it's not a famous open problem, but you'll impress me if you do this. I will be impressed too if you guys. Okay, so two people impressed. Okay, so in the remaining time in my talk, I'll tell you some of the ingredients of the proof and sort of tell you why we picked the strategy. Sort of tell you why we picked the strategies that we did. So, okay, so the basic setup is we are going to approximate, find some approximate solutions by smoothing the initial data and then cutting off large velocities. And then you have, you know, C infinity compactly supported initial data. You can apply the prior short-time Welposenist results that I quoted some of earlier, and that'll give you an approximate solution f epsilon. Approximate solution f epsilon. And of course, and you know, we're not linearizing or doing anything with the equation, right? These do truly solve the equation, but they only approximately satisfy the initial condition. So, of course, in order to, of course, you're going to want to take a limit in epsilon, so you're going to need to continue these solutions up to some uniform time, of course, and you're going to need some uniform estimates. So, this is where a lot of our work comes in. A lot of our work comes in is getting these weighted L infinity estimates. So if your initial data at the epsilon level is in this weighted L infinity space, then the same will be true for positive time and hopefully with constants that are independent of epsilon when appropriate. So then we're going to apply this same, so nothing here I should say in our weighted L infinity estimates, it's kind of interesting, we don't use any lower bounds for the solution. For the solution. But then, after you've done this, after you have your approximate solutions on a uniform time interval, you apply the lower bounds to sort of push your local lower bounds to the whole domain for positive times. That's what gives you the coercivity properties that lets you apply the smoothing estimates. And at that point, we go to the regularity estimates from Invert Sylvester. And we can't quite apply them out of the box. I'll talk briefly about what's called. About what's going on there, but the general approach, we're certainly in debt to these two guys for that part. Alright, so yeah, before I sort of tell you, yeah, so I think for most, I'm going to kind of zoom in on this second step for most of the rest of the talk, just because we know you can't do everything in a 45-minute talk. So before I sort of take you through that, we can compare this to some other, to the other common. This is to the other common way of getting closed estimates, which is the energy method. So, okay, why do we work in these weighted L infinity spaces? So, our goal, our sort of definition of success, was to work in a zeroth order space, so a space with no derivatives. And so, if that's your goal, the L2 energy S method has some disadvantages. So, how to see this? Let's just pretend we were going to try the energy method. So, you multiply. The energy method. So you multiply the equation by f and integrate, but you need this cutoff in x. Why do you need this? Because again, we're not assuming any integrability for large x. So if you want your integrals to even be finite, you need to sort of localize in x. And then, you know, usually you would do this and then take some maximum over all the localization over the local estimates that you get. Okay, but anyway, so what do you get when you do this? So, what do you get when you do this? All right, so as usual, you get some weighted L2 norm over here, time derivative, and then, okay, from integrating by parts, you put this transport term onto psi, and then of course you have psi f times the collision operator. So, what you would want to do, of course, for a Grand Wall type of argument is to try to bound the right-hand side by the integral of psi times f squared, right? But you're not going to be able to do that because. Do that because, okay, for one thing, okay, look at the last term. So you see three copies of f appearing, and so that tells us we're not going to be able to use an L2 norm. You're going to have some kind of at least a cubic integral. And also, right, looking inside Q of FF, you've got that v minus v star to the gamma. That's a singularity. You need, I mean, how do you kind of tame that? You use higher integrability in Higher integrability in V, right? Especially when gamma is very close to minus 3, you need to take some possibly high LPV space or norm to control this. So there doesn't seem much hope of getting something like this on the right-hand side. So what do people usually do? Well, you take derivatives of the equation and get energy estimates for the partial derivatives. And then use Sobleov embedding to get back to the LPV norm to actually close this. And that's fine, but that's what. And that's fine, but that's what forces you to go to higher-order spaces, which is the whole thing we wanted to avoid. Okay, so yeah, this forces going to work in a higher-order space. Although, in the regime where you're going to be infinity, I guess you expect F to have, I guess, the to have some space-time regularity, like the integral of the gradient of F squared is bounded, the integral of that space-time, right? Oh, so you're saying you can. So you're saying you can use some coercivity properties for used to say it's like one estimate for one derivative, oh, yeah, like the weighted L2 fractional. I guess that you will get that, right? Oh, for free. The L infinity estimate. Sorry, sorry. No estimate. If you know that your solution is non-infinity, in the integral which is non-infinity, you get the cubic F and you say, okay, that's L2 times the infinity R. You just say that at least you have some regularity in solar response. Oh yeah, that's true. Something without it being somewhat estimated. Oh, yeah, that's true. Yeah, so if you already knew that f was in some way to L infinity, then yes, it would be a lot more hopeful. Right, right. That's true, yeah. But if but if you want to close this estimate, it doesn't right. So that's it. Any other questions? Oh, just because that was our goal. Yeah, well, because it's an equation that has smoothing properties, and so we thought it was a natural question to take initial data that's as irregular as we could and then try to get smoothness for possible. So we could and then try to get smooth swap possibly. Anything else? The transport term isn't it fun? Yes, yes, I'm getting that. Yes, so yeah, this is only the first problem. Yeah, so the second problem, exactly as you say. So this is the same equation from last slide. So the second problem is: yeah, you look at this thing, this is growing for large V. So that's another reason that you can't control this right-hand side with this. Control this right-hand side with this f squared integral without some kind of velocity weight. So this wouldn't be a problem if you were on the torus because you wouldn't need this side and that transport term would actually just go away from symmetry. But because we wanted to talk about solutions on the whole space, this is a problem. And I would say, even if you are on the torus, you get a similar problem, depending on gamma and s, so especially if gamma is close to zero. Especially if gamma is close to zero, you're going to have some growing terms from QF, right? That you also can't, are not going to be bounded in V, so you also can't control them by this sort of this thing, which is bounded in V, at least the weight. So how do people get around this? Well, a common solution, which actually Bob mentioned in his talk for the relativistic case, is, and this was, yeah, I think it goes back to the Amuxi groups, you divide your solution by this time dependent. Solution by this time-dependent Gaussian. So, what does that do? When you study the equation for, I guess, F times this, or maybe it's divided by, anyway, when the signs work out right, this will give you an extra v squared times F term with a good sign, and that can absorb your sort of growing things on here. And that's fine, but it requires you to take Gaussian decay for the initial data, and that's another thing we wanted to optimize away. Another thing we wanted to optimize away. So, we don't want to assume Gaussian decay for initially, and so that's kind of not an option. Okay, there are more intricate methods which I will not go into here. Maybe Chris will tell you about it in his talk. But you also need relatively high, let's say, polynomial decay. Not like a million, but certainly 10 at least. And again, we can do better with the other method. And do better with the other method. We only need, you know, similarly 2s plus 2. Okay, so yeah, so those are two of the reasons that the L2-based method is, at least we don't know how to make that jive with a zeroth order initial data. All right, so let's talk about then what we actually do. So, okay, yeah, we want to prove these weighted L infinity estimates using a barrier argument. So, one just a comment before we explain it is that because we're focusing on gamma less than zero, you can only have as much decay for positive times as you have initially. So this is a difference between the gamma less than zero and gamma greater than zero cases. So every decay estimate that we prove is really going to be of the form, if you have this many L infinity moments initially, then you have them for a positive time. All right, so what do you do? Well, you're using a pair. Alright, so what do you do? Well, you're using a barrier argument. So you uh you uh sh basically you take uh these this n big enough that f is below this barrier at time zero, and then you look at the first crossing point and you try to derive a contradiction so that they never cross, and so they stay ordered. And what that boils down to is proving a functional inequality like this. So f is some function in this weighted L infinity space, and you take the other And you take the other function to just be bracket v to the minus q. And so it turns out you can prove that this has the same decay, v to the minus q. And I'll just point out that you need the same order of decay for both the solution and the barrier in order to make this work. Okay, so that's fine. This might remind people of decay estimates from a work of Inverto Mojo Silvestre from a couple years ago. So what they were doing was something a little bit different. What they were doing was something a little bit different. So they were saying you can propagate decay estimates for as long as the mass, energy, and entropy density stay under control. And so they were interested in basically a long time estimate that doesn't degenerate for large T as long as you have these conditions on the hydrogen and the quantities. So on the other, you know, what were we doing? Something different, which is that we only need our estimate to hold up to a finite. We only need our estimate to hold up to a finite time t, but the key thing for us, for well posing this theory, is that our estimate only depends on the initial data and the time. So we have sort of different, we're using different quantities to bound the solution, to bound the collision operator compared to this. Although it's reminiscent because the form of the barrier is somewhat close. Okay, so I just want to make a side comment about barriers. So that was one of the main, not just So that was one of the main, not just for these decay estimates, but we use barriers a lot in this whole study. And so what makes these kind of a challenging thing to do is that collision operator is what we say doubly non-local. That is to say, if you evaluate Q of Fg, it's non-local in both F and G. And so you need to look at both functions in their entire velocity domain, which makes things kind of challenging. That's why when I say, Challenging. That's why, when I said you needed decay for both functions in order to prove that functional estimate, that's sort of what I'm talking about. And so I want to compare this briefly to the Landau equation. So, you know, some things Landau is harder. Other things, Boltzmann is harder. For this particular kind of thing we were doing, I claim Boltzmann is actually harder because of this issue with the non-locality. So we proved result in this similar. Result in this similar spirit for the Landau equation, and it turned out that barrier arguments are a lot more convenient in Landau because it's non-local in F but local in G in the Landau case. So when you're bounding your, in your barrier argument, you're bounding QL of FG from above, you only need to use information about G at the crossing point. You don't need to care about what's happening in the whole domain. So that was just one thing that kind of punched us in the face as we were writing this book. In the face as we were writing this paper. Okay, so next, and I hope that this slide is not too into the details, but a more subtle difficulty comes up related to these decay estimates. So basically, like I was saying, we want to, yeah, we're using an approximation argument. And it turns out that we need to propagate higher decay estimates up to a uniform time interval that's independent of q. Time interval that's independent of q. And we need to do this even for q that's larger than the decay norm that we're actually working in in the statement of our theorem. So let me explain that. Why would you need this, right? So you would think, right, that if I want to prove a theorem like, okay, if you're in, if you have six, let's say, to take a random number, you have six L infinity moments of type zero, let's work in, oh, this is another question in the zoom. Yeah. I also don't know if I shared my screen properly. Oh, no, no, it's okay. I think it was someone had a mic on accident. That's right. Okay, so yeah. Why do you need this, right? So one might expect that if I want to work, if I want to take initial data in, you know, L infinity with v to the 6, to take q equals 6, then I should only need to care about norms, about the norm with q equals 6. But it turns out, for 6. But it turns out, for what I think are interesting reasons, you need to care about higher than 6, even if your final theorem statement only involves the number of 6. So why do you need this? Because, so, okay, you approximate the initial data with compactly supported initial data, f epsilon initial, and you apply these prior existence results from the literature to obtain a solution on some time interval which may, in principle, depend on epsilon. Depend on epsilon. Okay, so what's the problem? Okay, you need to continue these f epsilons up to a uniform time interval, but you're doing that with a continuation criteria, right? So that's sort of your only option. But the continuation criteria that are available in the literature, you need a qualitative assumption of rapid decay, let's say high polynomial degree decay. And this may sound like a technical issue, but it's actually not, I claim, because, I mean, think about it. Claim because I mean, think about it, right? Where do continuation criteria come from? Well, you're combining some apriori estimates with a short-time existence result, right? To have an effective continuation criterion, you need to use some short-time existence result. And I somehow already told you that the short-time existence results in the literature require a lot of decay. So, somehow, the whole point is that we're trying to come up with a local existence result. Local existence result for weaker initial data, but in order to continue our approximate solutions, we haven't proved our theorem yet, so we need to use continuation criteria for stronger initial data that have rapid decay. So even though the continuation criteria don't quantitatively depend on this, you still need to ensure it. So, okay, that was a long explanation. Let me explain it even more. Because, right, so what do you do? You apply some theorem that says, okay. Apply some theorem that says, okay, if f is in some L infinity space weighted by v to the p, then you'll get some time depending on t, on p. But then you don't have a uniform time even at the epsilon level for all polynomial decay rates. So that's sort of the issue. You can get around it when gamma is greater than minus 3 halves, because when gamma is greater than minus 3 halves, you have Gaussian-weighted local existence results. Gaussian-weighted local existence results. So then it's easy. You can say, okay, because your initial data has compact support, then it has Gaussian decay. And then you propagate that forward to a finite time interval, and that's fine. Then at least all your f epsilons exist on the same time interval, but that result with the Gaussian decay doesn't hold when gamma is very close to minus 3. So you have to do something else. And that's why we need these other decay estimates that I'm going to tell you. Other decay estimates that I'm going to tell you about next. Yes. Do you think that there is something? Is it a technical limitation to propagate exponential decay? Is gamma greater than minus 3 over 2, or do you think there is something more? Good question. I think you could probably prove it. Yes. So yeah, one, yeah. So if you didn't want to do what I'm talking about here, you could go back to that result and say, but it would be a lot of work. No, of course. Yes. Yeah, I don't think it's an essential. Yeah, I don't think it's an essential restriction, but I do, yeah, there was nothing in the literature that would work for us. This good question. Okay, so then what do you do for these more difficult decay estimates? You return to the barrier argument and you try to take a larger value of q, and that's sort of better than the decay estimate that you have for your solution. So, because you want to get some kind of gain. So, what do you try? You try to So, what do you try? You try to, before we had a functional estimate where q and q0 were equal. But we want to take, we want to only measure f in some q0 norm, but our barrier will actually get a small improvement over q0. So I have q is at least q0, and it's less than q0 plus absolute value of gamma. So it's, especially when gamma is close to 0, this is a very small improvement, but it's still an improvement. So you can run your barrier argument, and using this, you can And using this, you can show that this higher decay estimate will hold for a time depending only on the initial data in this smaller decay norm. And then you've got a very small gain, but you can iterate it finitely many times to reach any Q. So yeah, there's work involved in making sure you do this, but turns out you can make it work and your final estimate will be like this, right? V to the Q times F is an L. V to the q times f is in L infinity for any desired, for some time interval that doesn't depend on q, right? Only your smaller exponent q is 0. So that turns out to be what you need. And that's up to 10 equal to 0. I'm more confused. Do we need to assume more about the 10 equals 0? No, that's. Where are you looking? Sorry? So they said V Q F belongs to an infinity. Yes. So this everything you say in the word gain. Everything you're saying, the word gain. So the gain. Wait a little time to improve or the gain of the. No, it's that it's that okay, yeah. So the bound will depend on the initial data, the size of the initial data and this normal. But the time you can propagate it to depends only on the weaker. Yeah, so that's the point. Perfect, thank you. Yeah, like I said, this slide is kind of in the details. So you're not like losing moments as you go forward in time. Yeah, because I'm definitely not on your own. Okay, so. Okay, so right. So that's the, I kind of wanted to zoom in on that step of the proof. Just a comment, then after you have this, you want to use the regularity theory from Hilbert Silvestre. And, okay, there are some obstacles, some of which are more technical, some require a bit of thought to applying these estimates in our setting. The one thing I'll just mention is that, so they take gamma plus 2s greater than or equal to 0, and one of the And one of the key steps in their argument is changing variables to pass from local to global regularity. And we needed to use a slightly different change of variables. So once you do that, then the rest of the machinery works out pretty well. So then once you have these estimates, you apply it to Georgie and Schouder, and you have enough compactness to take the limit and obtain a true solution with your initial value. All right. Initial value. Alright, so yeah, like this is, you know, don't want to say too much about that step of the proof in the interest of time. Alright, so then I should just point out now that nothing in anything that I've said guarantees the solution is unique. So if you want to know about uniqueness, there's a result in a similar setting, and Andre is going to talk about it on Wednesday. Wednesday. So up there. Thanks, Anthony. I guess I cleaning and did something for everyone else to prove that it's existed. Questions or comments? Yeah. You mentioned that the change of you need to modify the change of variable. Yes. And it's such a nightmare to change the variable. To go through this change of viable that I would like you to comment on that because this is so difficult to verify that for the change of IBO you are still in the good ellipticity class. Yeah, my comment is that it was a nightmare. But I would say, okay, so the most, let me be very clear about what I'm taking credit for, I guess. The novel thing was figuring out what the new change of variables should be. Once you have that, then you can follow a lot of the proofs and You can follow a lot of the proofs in your paper with Luis. But it's annoying because the idea of the proofs are all the same, but it's sufficiently different that you actually do have to work through them. Yeah, so we had a long appendix doing this. But we wanted to handle the very soft potentials case, so we felt that it was worth it. Yeah, yeah. Sometimes when I was doing it, I doubted whether it was worth it. We had some debates actually about. We had some debates actually about whether we should really go through with it, but we had decided to. And I think it's really amazing to be to put up to uh more casing challenges. Questions? I was interested in your weak uh uh existence results for weak solutions. Could you go back to the slide? Maybe I I there's some idea. Oh. Okay, don't hold the arrow button down, everybody. Go back to full screen. Uh, here, please. So, here you don't use this uniform positivity condition on a small ball. And I guess you're going to tell me that. And I guess you're going to tell me that the spatially inhomogeneous Boltzmann equation is so difficult that it's not possible to show that for any positive time the mean solution will actually satisfy that criteria, right? That would be something to try. You could try to prove that. And then yeah, you'd have some eventual regularization. Well, for any positive time. Oh, well, sure. Yeah. You could, yeah. I mean, the question is how, I guess. Maybe that would be a better, now that you have an object to work with, try to show that it's okay, yeah, try to show some instantaneous sort of positivity for the weak solutions. Yeah, it's possible. I'm not sure how I would go about trying to do it, but yeah. It's almost unimaginable that it's not true, right? Yeah, because, like I said, right, like if you're at vacuum, you stay at vacuum and that's perfectly smooth, right? If you're like a little bit. That's perfectly smooth, right? If you're like a little bit uniformly above vacuum, you're smooth. So, how could it possibly be otherwise? Yeah, it's got to be technical. Basically, that's more than that. But the design where you showed instant positivity was conditional to the bound. I know this was for Landau. Conditional to the bound of the you need some upper bounds, but you can get them like using our L infinity, our weighted L infinity. Infinity are weighted out infinity. So you don't need less. Question? So it's got to see if I emphasize the difference between your result, your estimates, and say please and you guys have you assumed the as long as the you assume you have classical solutions and as long as the in any interval where the hydroelectric quantities don't blow up, they Quantities don't blow up, then you can show that the estimates were okay, right? So, you're saying is initially, as long as you are this weighted L infinity, then you can construct a solution for some short time, depending on whether how long this L infinity weighted infinity one holds. But that's great because I feel like it really shows that. Like, right, that fully shows that the source of all trouble for Bolshevik is point-wise growl of right, at least. Yeah, so this, I mean, this was somehow the problems at time zero, right? Because the conditional estimates are always going to blow up as time goes to zero. So it's a little bit of a different, it's related, but it's a little bit of a different issue from singularities developing for positive time, I guess. Right, no, right, but at least what I'm going to say is kind of like you're saying. It's kind of like you're saying, as long as there's enough, and if you pointless regularity to the power of hypoelexicity will bring regularity elsewhere. That's right, yeah. You can always buy regularity with decay. That's right. In a sense. I mean, the way that it's natural, right? It was a whole in the literature for this. Well, it's like, ah, capitalism, but it's great that it is fine. Yeah, that was alright. Finally, yeah, that was our answer. Right, that's that's also great. Thanks. Stand for a good talk. No more questions then we'll