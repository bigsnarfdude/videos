My name is over there, as you can read it clearly. All right, so I'm very happy to be here with these people. So I find this conference extremely, extremely exciting for me. And I hope we'll make the best out of it and make it not only very pleasant, but also very, very fruitful. So I'm extremely honored to give a kickstart to the To give a kickstart to the meeting with an introduction to booster population and kinetically constrained models. So, this is going to be a very introductory talk. I hope that everyone will understand pretty much everything. So please feel free to interrupt. So, the first part is going to be about booster propulsion, which is much more combinatorial. The second one is more probabilistic about what Christina just explained. Just explain. So, I think we don't need any more motivation than that. Christina was extremely, extremely good at that. So, let me start by introducing what we're going to talk about, starting with booster population. And then I will mention a bit about the motivation of that, which is going to be a minute. So, we'll work on the square map this for the purposes of the talk. You put the things. You couldn't do other things, but that's where we stand. Each vertex is infected or healthy. I'll represent infections in black. The rest is the white ones or just non-drawn ones that are healthy. And so this bootstrap replication is going to be a simpler automaton. I will first start with an example, the J-Nagler Bootstrap regulation, which has this parameter J between one and four into the action. Green one and four into the actions, and it proceeds as follows. So, simple atoms in general do in discrete time, they perform the same transformation on every vertex of the lattice simultaneously. But each time step, each vertex will look whether it has at least J infected neighbors. If it has, it will become affected on the next step. Moreover, we will have the set of infections with monotones. Set of infections is monotone, so infections will just stay infected forever. All right, so if this is my initial configuration, let's see what happens for the J equals two model. Well, on the first step, I have these vertices, the red ones that will become black. Let's see why. If I take a look at this one, it has two black neighbors, which is enough, so it will become infected. This one has three, which is more than enough, so it becomes infected as well. As well. If I look at this one, this one has just one black neighbor, so it will not become red. Okay. And a solar automaton, I iterase this. On the next step, I make all the red guys black and I repeat. And I keep going like this. And on this concrete example, everything will eventually become infected. Right, so this model was introduced around 1980. This model was introduced around 1980, and you could see it motivated in several possible ways. The most naive one, in my opinion, is to view it as a model for the spread of infection on a geometric graph, like C2, but maybe not very relevant. If you put it on a different graph, for example, a social network graph, you can view this as a model for the spread of an opinion, some news, or something like that. There is a bit more reasonable. More reasonable. But more on the physical side, this was actually, this arose as a simplification of the Ising model at low temperatures. And indeed, studying this was an important intermediate step towards deriving things about the Ising model. And finally, Christina told you all about ACM, and this will be extremely useful for kinetic reconstruction from the point of view of being in. From the point of view of being interested in kinetic constraint models, we should view bootstrap correlation as an extremely important prior requisite. Something you have to talk about, but okay, it's simpler. Fair enough. Now, I don't want to focus on this specific model, the JNA proposition population, but rather tell you about this general universality framework. So, the goal of universality is to study all Universalities study all sorts of constraints, not just J neighbors, and we want to classify them. So, we want to, for any such rule, if I have enough infections around, I become infected, for any such rule, I want to be able to say, aha, so this will have this type of behavior or this type of behavior or this type of behavior. And I want to do this in an efficient way. So, if you give me your favorite model, I'm able maybe within. I'm able maybe within five minutes to tell you a house. Well, this is in this box and it does this and this. Right, so let me first tell you what all the class that I am going to talk about is. It's defined using these update rules, which are represented like this. So they are finite non-empty subsets of Z2 without the origin. I'll always put the origin with a cross so that you know where it is. And I represent And I represent these in black, the sides of that. So this one there is just the set 0, 0, 1. And the way you should interpret this is if my top and right neighbors are infected, I will become infected on the next. So this is one rule. Okay, and I define the data. Okay, and I define an update family to be any finite non-empty collection of updates. Anything you like, just put a bunch of them together as in this picture and call that an update family. Fair enough. Now, this defines a cellular tonaton in a similar way to what I said before. So the rule is going to be if at least one of these things is completely infected, then I become infected on the same. Then I become infected on the next step. So, again, if there exists at least one update rule which is completely infected when you translate it at the vertex which you're interested in, then that vertex will become infected on the next. And infection, the set of infections will be growing. So infection will never happen. Okay, this is a monotone. So let's give an example. Actually, here I cheated and I gave you the same example as I already. And I gave you the same example as I already gave you. This is the two-neighbor process. The reason for that is that here I put exactly the six two-element subsets of the four neighbors of the origin. So this is just a way to express that if any two of my neighbors are infected, that is enough for me to be coming, which is the same as if at least two neighbors are infected. Right, so we'll have more examples in a second, but to But so I want to study this entire class of law for any object math. And what I want to know about, well, I say it's behavior, but the behavior of logs. As Christina said, we're interested in the time scales of this. So a typical observable that, okay, it's my favorite one, but if you have another one, it's probably extremely related. So that's the first infection time of the origin, which could potentially be infinite. Which could potentially be infinite if the origin stays healthy. So we want to know how this kick is. Now, for now, everything is deterministic, right? I haven't introduced a grain of probability. So this is just something that is a function of the initial condition. So to introduce a tiny bit of probability, so you can study this and you can ask extremo combinatorial questions about this and they are interesting and people study those, but we will People study those, but we will only be interested in what happens for a random initial reputation. Random in the simplest possible way, product and Bernoulli with parameter. Okay, so now this becomes a random variable and we want to study its behavior. So it's then very, very standard in statistical physics to define the critical probability, which is the first time when you have when you're almost sure that this infection time will be fine. This infection time will be finite. Right, so in physics, what you would look for is what is the behavior of this when P approaches PC from a model. Because PC is defined as the place where this infection time goes up. You expect that when P approaches PC, you will have finite but diverging. A diverging infection time, and we want to know how fast does it diverge. Okay, are there any questions? Okay, so straight to some examples. Sorry very, very slowly. One neighbor puts her perpletion. I become infected if my left neighbor is infected, or my right neighbor is infected, or top or bottom. What does this do? Well, if I have an Doesn't well. If I have an infection somewhere, the thing that will happen on the next step is that its four neighbors will become affected because they have an APR. So I get this. On the next step, I'll get this. And I continue so on and so forth in L1 balls. So at this point, it's clear that whatever the value of P I take, almost surely there will be an infection somewhere. So the origin will eventually be covered by this L1 ball. So this is to say. One ball, but this is to say the critical probability is zero, and because the critical probability is zero, we are interested in the behavior like B goes to zero. So I have very few infections. We want to know how far away will this infection be, the closest to the origin, be from the origin. And in two dimensions, it's typically a distance of order one over root when b is small. So again, all the asymptotics I will write are I will write RSP goes to P C plus. Right, so to summarize this, I could write that P C is zero and the infection time is one over root. Okay, so very simple. Now, even simpler, the East model. Here, I become infected if my left neighbor is infected. So, this is the same exercise. Here, I don't need much space. Much space. If I have this infection here, on the next step, I'll get the right neighbor infected, then its right neighbor infected, and so on and so forth, until I eventually reach the origin. This will happen almost surely. So P C is zero. I want to look at P small. So infections are very sparse. Typically, the first infection along this line from the origin is a distance one over P. So the infection time is something like one over P. Okay, so far so good. Okay, so far so good. Now, in this case, it's probably some of you already noticed this. You can give the explicit distribution of this infection time as geometric of parameter. So this is completely immediate. Also, in this case, you can give the explicit distribution. It's not very hard. But in general, of course, for general rules, we won't be able to do something this precise. Something this precise, we just want to look for results more like this: what is the value of PC? Particularly, is it degenerate? And what is the scale? That's our goal. Now let's generalize these observations in the following way. We said an update family is super critical if there is a finite set of infections which can infect an infant. Okay, I just parachute this. And the relevance of this is showing. And the relevance of this is shown by Polova Schmidt and Otzol, who established that if your support protocol, then PC is zero and your infection time has this polynomial scale. When I write this, I mean always S P goes to P C plus, and I also mean that this tau is bounded with high probability between one over P to some power and one over P to another. Other so I won't write this out every time it's systematic. Right, so the idea behind this is very simple. Once you have this definition, you know that you have some finite set Z, which if it's completely infected, can produce an infinite offspring. And you can moreover prove that it not only produces an infinite offspring, but actually it produces Finally, offspring, but actually produce and you can find one which copies itself but she maybe manages to infect this. And so once you have this, it's very easy to know how to proceed. By the way, notice that both models we had before were super critical because a single infection produced an infinite. Here it was the entire plane, here it was a half-fly. So, here we will treat this set as our single infection, and we'll proceed as in the EAST model. So, we'll start copying it until eventually you reach the origin, and the origin will typically be at the distance p to some inverse power because this thing has finite size. So that's why you get this scale. Okay, now so far this is great, but it doesn't quite fulfill what I promised I would do. I promise that if you give me your favorite model, I will be able within five minutes to tell you what does it do. And this definition is a bit annoying in that respect because it's not clear, how will I know if your model is supercritical? I need to do some. Critical. I need to do some ultimately infinite combinatorial check to see if this happens. To go a bit further, I'll need a new notion, the notion of stable directions. A direction is just a unit vector, and I say that it is unstable if there exists an embay family which is contained in the open half plane without a normal disdirection. Oh, picturing that second. This is my rule. I just have one. If this is my rule, I just have one here. I will represent the stable directions, those which are not unstable, in red, and the others will be well the others. So let's see why this is the set of stable versions. Well, if I take this one, I have that it contains the half plane contains completely this rule, so it's unstable. If I look at this one, it's stable. Right? Or you do this for every direction, and you're happy. Actually, you don't really need to do it for every direction, just a very few of them. Anyway. Okay, another example, the East model, it has this closed half circle of stable direction. So I should mention if you have, it follows directly from the definition that if you have several rules in your family, what you should do is take each rule separately. Each rule separately, draw this picture, and then take the intersection of the red regions for all of them, and that gives you the stable set of directions for your entire family. You can believe me that you can do this quite quickly in polynomial time in any case, to determine what is the set of stable. Now, this definition might look very coming from nowhere. Coming from nowhere, it doesn't. So, when I draw this, you should think of this half plane as being completely infected. But when you have an infected half plane and nothing else, there's two options. That's easy to establish. Either it doesn't do anything more, it doesn't infect anything else, or it infects the entire plane. And if it doesn't infect anything else, that's when you say that the direction is stable. If it infects everything, you say that the direction is non-stable. You say that the direction is constant. So, these directions tell you in which direction you can go. And the relevance of this is shown in the same paper by showing that family is supercritical if and only if there is an open semicircle of unstable work. So, as in the east gate. This one is super vertical, this one is not super. Right, and moreover, I should say that this open semicircle has a clear interpretation in the model. Actually, if this is my model, it has this set that can reproduce in this direction, then I will be able to find this direction, which is the midpoint of this semicircle of unstable directions. So, this has a very clear interpretation in the model itself. All right, so this is everything I want to say about supercritical models. Let me move on to another example, which again we already saw the two-neighborhood firm. So here I have these six rules. Let's see what the stable versions are. This one we already saw in the previous slide. It makes this quarter of a circle unstable. That will go this one. This one will make this one, this one will make this one. This one, this one will make this one, this one will make this one. Now, these two rules don't make any direction unstable because remember, my half planes are always open going through the origin. And there is no half plane that I can choose that contains both sides. So I'm left with these four isolated stable directions. And in particular, I don't have a half circle that has no stable direction, so this is not superproductive. Is not super, it's going to have a different behavior. So let's see what this behavior is. It's actually, this is the most classical booster portulation model that was heavily studied. The very first rigorous result about that by Van Enter, which shows that DC is zero. I won't tell you how to do that, I'll tell you how to do something more. So Eisenman and Lebowitz slightly after showed that After showed that the infection time is again high probability exponential one over so this is to say non-polynomial. So this has a different meaning. Let me show you how to prove this. The upper bound is both bounds are easy, but they introduced a very important idea. So let's see if I have so the problem is that I don't So the problem is that I don't have this finite set which can reproduce. This is only for supermarkets. Here we will find a diverging size set which can grow. So let's see what structure you will have. I will ask to have an infection here, an infection here, at least one infection in these two sites. If I do so, then I have what is Then I have uh well, if I have this one, then this one becomes infected, but it just has two neighbors, so I get this entire square. If I have at least one infection in here, then I can extend the square upwards in the same way, and now it becomes a two by three rectangle. Then if I have at least one infection among these three, it will spread to the entire line because, well, if it's at the bottom, the next one will become infected because it has this. It will become infected because it has this and this, and so on, and I keep going like this. And if you compute the probability of all of this happening, what you get is this product up to infinity for k from one to infinity of one minus one minus p to the power of k. The power of k squared, because I do each one twice, right? And this you can compute is roughly exponential minus pi squared over squared over three. Okay, so there's nothing mysterious there. You just approximate this by an integral in the exponent. An integral in the exponent, and that's all. So, this gives you, okay, and this is the probability of this entire thing occurring. Now, how far away from the origin will we see that? Well, in two dimensions, you will see that at distance, which is exponential i squared over six. Okay, I cheated slightly because it's not. Achieve it slightly because it's not exactly independent now, but you can turn this into a rigorous argument, not very far. Now, moving further, it was proved quite a bit later by Holroyd that the right constant is not six here, it's 18. That was a breakthrough that introduced a lot of important ideas. On the lower bound, on the lower bound mostly, which is much harder, the upper bound, the main thing. The main thing you should keep in mind is that basically to get this pi-score words 18, you just need to ask not for one infection here and here and here and so on, but for at least one infection in here or in here and at least one infection here or here. So at least one infection in every two consecutive ones of these. And a bit more computation gives you this. You this since then, we uh a lot more is known. I don't, I don't want to go into that, uh, particularly about this, which is mainly interesting because of discrepancies with simulations that you mentioned. But instead of going deeper into this model, I want to generalize this, just like what we did before. So we said that an abounding is critical if there is no unstable open semicircles, but this is to say it's not super critical. So, this is to say it's not super vertical, but there is a semicircle with finitely many stable directions. Let's go back. This guy was not super critical, and every even every semicircle has finitely many stable directions. So, I should mention when I say this finitely many, you shouldn't worry. The set of stable directions is just a finite union of closed intervals. There's no fancy stuff going on. Fancy stuff going. And the relevance of this, as you can guess, is proved in the same paper, which shows that PC is zero and the infection time here is stretched exponential. Oh, very, very, very slow. So I will not tell you about this because Laura will tell you everything about the critical plasma could ever imagine to say this afternoon. This afternoon. So I'll directly move on to another type of models. So starting with an example, as always, the Northeast model, which I also like to call oriented percolation, for reasons to become apparent in this. So here I have just this one rule, and we already saw that it has this three quarters of a circle, which are stable. So this is not supercritical, it's not critical, it will be just what? Protocol will be just what? So let's see first what it gives on an example. Here's the origin, and these are the infected sites in the shock. On the first step, I will get these two sites infected. On the next step, I will not get the origin infected because I need both my top and my right neighbor. I just have the top one for now. I'll just infect this one on the second step. Then on the third step, I get this one. Third step, I get this one. Fourth step, fifth step, I get the origin. Okay, so the origin becomes infected on the fifth step. And here, it was already observed by Schumann in the 90s that the infection time of the origin is exactly given by the length of the longest path which makes steps to the right or up, and which is initially completely held. It's easy to check, but you can believe me about it. Me about it, and in particular, so here it's this path, and in particular, this model is absolutely equivalent to what we call oriented purposes. So, as a consequence of that, because the origin staying healthy forever is the same as the origin having an infinite healthy path, oriented path. So, it's very classical and not very hard to show that PC is strictly between zero and one. Now, I'll disappoint you to the Now, I'll disappoint you to that on the next slide. I will not give you the critical exponents of our unit propagation. Instead, I'll move to something a bit easier with this four-neighborhood strap model, which clearly has no unstable directions because no half-plane going through the origin will contain these sides. And here, I claim that it's very easy to see that PC is one. Why is that? I just need a little picture, which is this one. Which is this one, oops, this one. So I claim that if I have two neighboring infection, neighboring healthy sites, they protect each other forever, even if everything else is in there. The reason is that, well, neither of them will become infected before the other one. So they just bind us and stay healthy for it. And whatever the value of p, smaller than one with positive probability, this will happen at the origin, and then the origin stays healthy. Okay, so these. Easy, PC is one. So let's generalize, as always. I say that the big family is subcritical if blah blah blah, which means if it's not supercritical nor critical. I'm not cheating, nothing left behind. And we further say that it's trivial or subcritical if all the directions are stable. So here we have an example of a subcritical model, which is not trivial or subcritical, and a trivial subcritical model. Critical. And I'm sure everyone guessed the theorem, which is due to Ballister, Bolbach, Shrutsky, and Smith, who showed that for subcritical models, PC is strictly positive and PC is one if and only if you are a trivial subcritical model. Now, I should say this part is the hard part. So they proved this by multi-scalary normalization, and since then we have And since then, we have two other proofs, much more recent ones. The first one by Ballister, Bolobash, Morris, and Smith, who showed that this also in higher dimensions by another multi-scalary normalization again, which has to do with some very Lipschitz surfaces, should make a bell for some people. And even more recently, so this is maybe a couple of months ago, and a couple of Months ago, and a couple of months ago, minus epsilon. There is another proof by Rega Saba and myself of this part, which also works in any dimension, which is this time by Perl's argument, by these two contours that you may or may not hear about. Now, this part is much easier, and it follows the same intuition as for this. For trivial subcritical models, you can actually find this finite set. Actually, find this finite set of healthy things which just remains healthy forever if it is completely healthy. To find it, you can just start from a big blob, which is completely healthy, let the thing evolve, assuming everything else is affected. The thing that will happen for a trivial subtitle model is that you will cut some stuff out, but eventually you will find an appropriate polygon which just stays faulty forever. About those pieces. About thousand pieces. Right, so at this point, I really would like to advertise one of my favorite conjectures, which is due to Schoenman in a slightly more restrictive case and which is stated as such by myself a few years ago, which claims that for any update family and for any P larger than PC, you have exponential decay of the tail of the vector curve. Right, so this is known for some cases. It's known for supercritical models, it's known for critical models. In that case, it basically follows from the proofs of what I've already told you. And the interesting thing is subcritical models, those which have a non-trivial phase transition. For trivial ones, there's nothing to prove. The statement is empty. So subcritical, non-trivial. Now, if we look back at oriented percolation, Back at oriented percolation, this conjecture is exactly. If you look at the correspondence with oriented percolation, this is exactly the statement that subcritical oriented proclation has exponential decay, which is a very classical fact. We know it in that case. More generally, I recently proved this for any model, actually, in any dimension which is contained in a half space. So it has a kind of an orientation. Half space, so it has a kind of an orientation. All the rules are in that half space. But the general case is open, and I very much encourage people who are into percolation to think about this. Right, so this is everything I want to say about bootstrap percolation. I put the definition again. Are there any questions? Yes. Could you repeat what you mean by use of the high specs? The high space? So I mean that all the not really repeating for every u in u so for every rule in the family that there exists a direction u such that for all rules in the family and for all direction for all this. Yeah. Okay. Yeah, okay. Let me still finish. Uh, X of the scale of the product product of X and U is all subordinate? They're all subcritical. No, they're not all subcontracted. There are different. But there are different. For example, oriented calculation, you see all the rules, there's just one rule, and the sites in the rule are in a half plane. For example, this half plane. But it's not easy, also. Yeah, but East is an interesting. There are things from, but the interesting part of that theorem is just for the subcritical. Is just for the sub-critical models. Right, which are, I should emphasize, much more complicated than just oriented preparation and things looking like order. Thanks for the question. Anything else? If not, let me move on to kinetically constrainable. More exciting. To convince you that it's extremely related, I will put the definition by The definition by just changing a few tiny things. All you need to change to get an electric constraint model is this. So let's go over it. The geometry is the same, it worked on Z2, in fact it worked healthy on each site. The date rules and the date families are the same. Now, the kinetic constraint models differ in several ways, which are very important. So now they are Markov processes. They are Markov processes, they are not a cellular data, they are asynchronous, I don't mean synchronous, they are not monotone, and they are also continuous now. You would argue that that's basically all the features of the bootstrap application that I wrote down, but still it's extremely relevant. And the bootstrap application will help. So, let me define this. For now, just read the first line here. If you read just the first line, Here, if you read just the first line, you see the definition of what we call the Glauber doubles, which was already mentioned by Petina and by Fabio about 20 years ago. So you at grade one, so at each vertex, much more, no, but the two Christina more like 40 or yeah anyway, no global. Anyway, October was 50, right? Yeah. Anyway. So the dynamics is much more ancient, of course. So what it does, at each vertex, you have a Poisson clock, which rings at exponentially distributed intervals of time of P1. When it rings, you toss a coin. The coin has a black side and the white side. The black side occurs with probability p, the white side occurs with probability one. TP, the wide-sided urban probably one minus p, and you just change the state of the vertex where the call ran to well, the coin also. That's the global dynamics, something very simple. The glauber dynamics are the product for integers. And kinetic constraint models do the same thing, except that you only look at what the coin tells you if you have enough infections around you in the Enough infections around you in the sense of the update factor. So, if there exists an update rule which is completely infected, then you do as well work. If not, you are stuck. So, that's these constraints that arise in classy things. You are stuck and you wait until this constraint becomes satisfied thanks to some things that come from far away to have. And then you might be able to. Able to know. Again, more examples in a minute. But so, what do we want to look at? The infection time, again, defined the same way. Now it's continuous random variable, but otherwise same thing. Now, we're going to look at, again, a product Bernoulli initial condition, but as Christina said, and this is by no means an innocent hypothesis, I'm assuming that the initial condition is product Bernoulli with the same parameter that appears in my Same parameter that appears in my coin notes. This is extremely important because for this Markov process, you can easily check that it's reversible with respect to the product. We're only measuring this parameter. So I'm assuming that I am at station. Which, from the point of view of physics, is somewhat ridiculous because a glass is basically by definition out of that. So of course, we would very much like to understand what happens if you start from another. What happens if you start from another initial condition? But due to the denials not being attractive, this is quite problematic. So we will stick to where the tools were, the stationary. But again, it's not because it's not interesting. On the contrary, it's because that's what we can do. Right. And then we define the critical probability in the same way. Critical probability in the same way. So, just with respect to the stationary process. When I say stationary process, I should mention this process has multiple invariant measures, not just this one, but this one is kind of the important. For example, the Dirac on all health is invariant. That's not very interesting to say. Now, example from list: the two-neighbor KCM on the same initial condition. ACM on the same initial condition as before. Let's see what it does. First clock, this is asynchronous. Clocks ring one at a time. The first one that rings could be this one. I have two infected neighbors, so I'm allowed to update. I toss my coin. I get white. I put white. Next clock rings, maybe here. I have two infected neighbors. Again, legal update. I toss my coin. It gives me black this time. I put black. Maybe this one rings now. So I have three. One rings now, so I have three infected neighbors. That's more than now, so I will be able to change it. I notice, by the way, this point that you don't really need to remember what there was here. I don't know if you remember. Hopefully not. There was something. But you don't need to remember at all because you're just going to change it to what the point tells you. And that's basically the reason why the product Bernoulli measure is invariant for this point, reversible. Reversible. But in any case, we toss our coin and we put what it tells us, which would be white. And maybe this is the next all the rings. Now I have only one infected neighbor, so I don't need my coin. I know that I can't change the site. I need to wait more. I need to remember what was there. So they're going to need to remember it. That's why I drew it with a cross. So you actually see there is no black. All right. So then maybe this one rings again. Then maybe this one rings again, and maybe this time it gets black, and so on and so forth. So I think that just by running this for a couple of steps should convince you that this is much slower than boost recognition. And in particular, if you imagine it'll take quite a while to move these guys. However, it is possible that this can completely change this configuration. All right. So. So before I go on with other examples, let me give you some general results. The first one is perhaps the most fundamental one in the theory by compring Martinelli Roberto Tuminelli from what is mostly the first rigorous paper on KCM, one of the most fundamental ones, which tells you, okay, if you have unfamiliar words in any of the bullets, ignore the bullet. But what it tells you. But what it tells you is, so at least I'm sure you don't have any unfamiliar words in the first one. And what this tells you is that the phase transition of the bootstrap correlation coincides with the phase transition of the kinetically constrained model when they have the same data. So studying this complicated non-monotone blah blah blah KCM is in this sense reduced to studying the much nicer Woodstrap approach. Much nicer bootstrap proclaim. Of course, you need to do that, you need to know things about bootstrap population, but you manage to transport them to the KCM world. And we have a similar reduction result. You should view this. This is only useful as a reduction, right? If you don't manage to study the bootstrap correlation, don't try to study the KCF. But this tells you that if you manage to study the bootstrap correlation, then you can get the results. The boot reverb, then you can get the results for it. So, second one, which is very similar, so part of it is in concrete modern versions. And then again, when you put it together with recent work of mine, you get this, which tells you that you have the exponential decay for the KCM if and only if you have it for the bootstrap proposition. And remember, this is why I mentioned this conjecture. So this bullet is a conjecture. This bullet is a conjecture that it's always the case for P larger than P. But this is one more motivation to look at. All right, so this slide is confusingly called subcritical, not because any of this is specific to subcritical models, but rather because it's the only general result that we know about subcritical models. Literally, no other general result, which is very frustrating. Which is very frustrating for me, but it's very nice for you because it means you just need to go through too fast. So, starting with the super article, again, simple examples. The one neighbor is M. So, here, if I have an effective neighbor, I can't update. Let's think what happens. So, this theorem tells us that PC is zero. We don't need to study this. PC is zero. So, we are interested. P C is zero. So we are interested, as usual, at what happens with the scaling of this infection time when P goes to P C plus for the station parameters. Right, so we are interested in P going to zero. So we have few infections. The closest one to the origin is a distance one over P as before. And we want to get the origin infection. So we start asking. The origin in hell, so we start asking ourselves, How do I do that? Now, the most naive way to proceed is: well, I can just do bootstrap origin, I infect these things until I reach the origin. Now, this can happen. KCMs allow the bootstrap percolation rate. So, maybe I'm very lucky, and the first call that rings is this one. When I toss my coin, I get luck. Then, this one rings, I toss my coin, I get black. Then this one rings, I toss my coin, I get black, so on and so forth, until I reach the origin. Okay, but remember, we are working with p very small and we are stationary, which means that at any single time the configuration we see is product Bermude in the parameter, p being small. And for that, it's a standard correlation result that the probability of this of seeing this long black path is exponentially small. Path is exponentially small in its length. Okay, so this is to say this way I will not be able to get any bound that is better than something like this. I'm ignoring at the low. All right, so this is just by a union bound on the term that I will complete the process. All right, so we need to do something more intelligent. And the reason why we got this horrible bound is that. This horrible bound is that we try to do something which we should never do, and that is force the process and do something it doesn't find natural. We shouldn't do that. Instead, you should look at the process, look at what it likes doing, and then try to put that in a proof which will hopefully speed out nice results. So let's see what the process actually likes doing, which is not very complicated in this case. So if I have this infection in the beginning, the only thing that can happen The only thing that can happen is that I can update its four names, one of them first. Okay, I keep trying. Usually, I toss my coin and it just gives me white again. But at rate roughly P, I will get a second infection next to that guy. Maybe it's this way. At this point, several things happen. So again, something here will not change. However, I can make one more infection around. This happens. Infection around. This happens again at rate roughly P. But much before that happens, I'm likely to kill one of these two because both of them now have an infection next to them. And as soon as the clock rings, I'll toss my coin and probably it'll give me one. Okay, so now 50-50, which one will die? Let's say it's the left. The result of this was that my infection moved by one step to the right. And then, okay, what's the next thing that will happen? What's the next thing that will happen? How it will move by one step in some direction, chosen at random. So you should at this point think that this is just going to be behave like a random wall. Now, how much time will it take for a random walk to reach the origin, to reach this distance when it makes jumps at rate p? Well, the distance squared divided by p, one over p squared. Okay, so this is true, and this can be shown that in two dimensions, the scaling is that perhaps up to a low correction. In higher dimensions, you need a slightly more subtle intuition, but we stay in two dimensions. Now, I should say, if you look back here, this process, naively, it is just random mode. However, it's not quite because It's not quite because all that I said is probably the thing that happens is this. I mean, it will have probably alpha. However, if you wait long enough, if you observe this process for a long enough time, some other things will start happening. Typically, what could happen is that before you kill one of these two guys, a third one appears, and then maybe you first remove the middle one. Okay, if the third one appears here, maybe first I remove the middle one and what this resulted in is branch. And what this resulted in is a branch of my random block. So that's nasty. And the other nasty thing is that if you have two random blocks now that get close, they start interacting and they could also coalesce. This is a reversible process anyway, so it should be able to put the inputs. And the nastiest thing is that these interactions happen in a non-attractive way, which makes proving such things actually not that easy. Actually, not that easy. Intuitively, now I want to convince you that you can actually prove something. You can take such a mechanism and turn it into a proof. It is a very easy one. Namely, if you work in a setting like this one, so if you work on a box of size one over P, that's my entirely good works. And I have a boundary condition, which I will assume will have one infection somewhere at least. But I only work it here. The rest is proof. And I want to show that the infection time the origin is bounded by some polynomial. Some fairly weak bound. Just to show you that you can do it. Okay, how would accuracy? So this infection time, we will bound it. We will bound it using a bound on the relaxation. We will then see help from that. But for now, what is the relaxation? I didn't mention that. The relaxation time can be viewed as the rate of exponential decay of correlation. So that being said, you should think of it as, well, if I wait for the relaxation time, time, time, I wait for one relaxation time, then typically I see an independent. Then, typically, I see an independent configuration very naively. And I'm working with the stationary process. So, every single time I see an independent product Bernoulli configuration, and a product Bernoulli configuration has probability P to give me an infection at the origin. How many such configurations do I need to see? Well, I should expect that this is smaller than or equal to the TRL over. Over okay, so this inequality you can prove. It's a bit more subtle than that. It uses some, you can view it via quasi-stationary distributions. Now, where I cheated is that what I told you, the time you wait to see an independent duration, that's not really the relaxation time, it's rather the mixing time. So, in fact, just to convince you, let me put the mixing time. Let me put the mix in at which point I think we agree that you see roughly independent configuration. So I'm sure that probably everyone can turn this into a rigorous photo, I mean into a rigorous inequality. And for the mixing time, we know that it's bound in the role by T rel times a factor which is the log of the smallest probability of a state in our Markov chain, which in this case. Markov chain, which in this case is just going to give me a factor log one over p and here p squared. Okay, but because my volume is still low. So in any case, this stuff is polynomial. So I, for the purposes of proving this, it satisfies to just prove a polynomial time on the relosition. Okay, how do I do that? Okay, how do I do that? Now, the relaxation time, which I still didn't quite find, have an alternative definition, which is the best constant that you can put in a Poincaré orbit. So the variance of a function of the configuration in here, lambda, is at most some constant, yeah, the relaxation time times the lyrical form of. Diracle form of this function. Now, the directive form of the function, this is basically the definition of the process, is this thing. So the program deriving of, let me first write and then I will say what the things are. For next f there's a sum. So the sum over the sites in the box. The size in the box of the measure of the constraint at x, which is having at least one infected paper, times the local variance. Local variance being I average only with respect to the state of site X. The rest is conditioned on. So conditionally on everything else, I just take the variance with respect to this one, removing X. One remotely, those who are used to this, which is probably pretty much everyone, can immediately read this as I do at read one the date if the constraint is satisfied. I randomize, I resample site X from the okay. Now we want this inequality and we want to put something here. We want to put something our starting point will be indicator function that was already satisfied. Yes, I'm sure that was a question. So yeah, thanks. So to start proving this, we'll start from something much simple. This will start from something much simpler, namely the Poincaré quality for the Glauber dynamics. The Glauber dynamics, remember, was the same as the KCM without the constraint. Guess what? The theoretical form is the same without the constraint. This is the theoretical form of the Glauber. But the Glauber dynamics is just a product chain, so it's not very complicated. You just have You just have that the relative n is one. So it is known that you have this inequality, this var a var f is at most the sum over x lambda of pi of var x. Now there's polynomially many terms in this. There's polynomially many terms in this sum. Let me focus on one of them. And then, so I want to focus on one of these terms. And the variance of something for any real function, for any real random variable, you can write as up to one half the difference of the squared of two independent copies of that random. It's a classical fact. So you write this pi of R x of f as pi of f of omega minus f of omega prime squared, where this is, I'll put a pi x here. So the way to read this is omega and omega prime. This omega and omega prime are the same, distributed according to pi, except that their x coordinate is independent and sampled according to the Bernoulli X. Okay, at this point, this is just Bernoulli. So, this difference can take two possible values, either it's zero and the two configurations actually coincide, or okay, so whatever. We don't care about this. Whatever we don't care about. Um, so either a zero or the two are different. At which point I will write this as, so I ignore the polynomial factors always. f of omega minus f of omega x. So that's the configuration which is equal to omega, except that it's different at side x. X square and here I put a pi omega some over okay so here I ignore the factor which is I think p1 minus p which is not only polynomial but actually it's in our advantage okay so we are at this point interested in this this thing the way to read this is The way to read this is: I want to flip the side x. I want to prove a pointer inequality n, so I started from here. I want to show that this is at most something polynomial times the Dirac form of one neighbor AC. So, up to here, I emphasize this does not care at all that I work with the one-neighbor kinetic constraint. I didn't use any. Moreover, most of it, so I started with this at this point, so I won't use that. I'm working with a product. That's not extremely important. Anyway, in any case, you can reach here completely generally. So at this point now, I should start plugging in my mechanism for the one neighborhood. One neighborhood. How do I do that? I think with this audience, maybe I don't need to explain, to write down how you do canonical paths. Let me just say that briefly in words. So how do you do canonical paths? This difference here, you can write it as a telescopic sign along this path. So the path that starts at this configuration omega and wants to flip eventually just this side. So the path that I'm going to do is... Just this side. The path that I will use to do that is: well, infect this guy, delete the previous one, infect this guy, delete the previous one, and so on and so forth. I write this as a telescopic sum on that. I use Cauchy-Schwartz to pull the sum, telescopic sum out of the square. Maybe I need to pay the length of the path, but that's polynomial. I'm happy to pay that. At that point, I reach Reach a sum over omega of the differences along the path, and along the path, that's the key thing. I only use moves which are allowed for my unit. They are terms which appear in this variable form. They are here. They have the constraint which is satisfiable. So eventually, when I do this, I want to change variables from omega to the Omega to the i configuration in my path. And what I need to pay to do that is: okay, maybe how many configurations go through the same one? That's again polynomial. But also, the most important thing is how much does this thing here differ from the probability of a configuration that I observe in my path? Right, and here the key thing is it differs by at most two. thing is it differs by at most two infections. At no point during my path I create more than two infections different from my initial. So as a result, the change of measure will be polynomial and in the end I just get my concrete equality with a polynomial relaxation. So I hope this wasn't too fast. But the whole point was to convince you that you can To convince you that you can plug some mechanism that you have in mind and make it approve. Here, of course, I lost in a thousand times, a thousand places a polynomial factor, but you can do this better. Right, so moving on to the East model, which remember was much easier, of course. So, here we can try to do the same thing. Try to do the same thing. One thing we can do is just run a little structure prediction that will give you an even worse down, an exponentially small bound. Okay, exponentially being different. A bit more intelligently, you can try this random walk idea. So the random walk idea was to start at this infection, which was typical. Started this infection, which was typically distance one over p of the origin. And okay, I can infect the next guy and I can try to remove this guy, but then I'm immediately stuck because I don't know if I have an infection here. So I may be not, I may not be able to remove this guy. I can't do this create, delete, create, delete. So then the question becomes, how intelligently can I reach the origin without creating too many? Without creating too many interactions. By default, I can just create all of them. That would give me exponential one over correction. The question is how much better can I do? The answer is this lovely exercise that I very much encourage you, which says that if I start from a single infection and I want to get very far, Want to get very far? I can get exponentially far by using only a bounded number infection. So I give myself at most 10 infections at any point in time in my duration. I can get exponentially far with an infection, but I can't get infinitely far. So in the random case, you can get infinitely far. Here, you can only get very far. So how about the exercise? I'll solve half of the exercise. I'll show you how you can get this distance and your exercises to show you better. So if I just have one infection at my disposal, I can't move it. It just stays there and that's it. If I'm allowed to create a second one, I can put it here. The only other thing I can do is remove it. That won't get me any further. So that's it. Now, if I have a third one, I can put it here. I'm not done. I can do something. I'm not done. I can do something smart. I can remove this guy and then put it here. If I have a fourth infection, I can put it there. Then I can revert what I just did around here. Remember, the damage is reversible. I remove that, I put it back, I remove, I remove it. Now I can redo, but this time not in reverse, but forwards, the same operation here. Okay, so I do this. I do this and I get it. You can imagine how to iterate this with even more, and it gets you exactly to this distance. All right, so in total, this tells me that I need to cross. So typically in the initial configuration, I have this gap of length one over p that I need to cross, which is completely failed. And this tells me that I have to create, if you solve your regular function, at least. If you solve your regular ones, at least log base two of this distance infections in this region, which is very unlikely in the product configuration. All right, so let's see what that is. I'll actually do something even worse. I'll just look at the last interval of length one over root p. If I look at just that, I'll still need to, there's probably no infection. There's probably no infection here. I need to cross that. How many infections do I need to see in this region to get the origin infected? Well, I need to create log base two of one over root P infections. Now, each infection has cost P in the probability, and I have one over root P places to put it. So in total, it costs me at least Something like root p. So I get root p to this power is the probability of the event that I need to see occurring before I affect the origin. Right? Which you can convince yourself that this is an exponential log squared one over t over. Over four log. Okay, so this is a proof of the lower bound that the infection time should be at least this with high probability. This is just a union bound as the one we did. We strained before. Right, so sorry, there is a minus here. And uh this is optional. I'll do the same thing with a plus so it turns out this is the correct scaling. It's exponential log squared. That's the most important. This was proved by Adam Sandy. And since then, actually, the sort of pretty particular event. So, Caprini Mardiner and Beth Dianelli found that actually the right constant to put is two log and to do that they proved a number of animals which I'll tell you how to prove in a minute but before moving on I'll mention that also here better things have been done since then and we have experts in the audience so I don't want to embarrass myself. So instead I'll tell you just how they prove the upper bound because they introduced Improve the upper bound because it introduced a very important technique, a bisection technique, which has this, which is based only on this two-block Macaroni book, which I'm sure you don't want to read. And because you don't want to read it, if you do, it's your home, you can interpret this as just a bound on the relaxation time of the east model on two sides. Model on two sides with a boundary condition, which is the thing. Well, I work on the yeast model on two sites with infected boundary condition on my left. There's two blocks on the quality basically. This sounds ridiculous because I would say this is a four by four matrix and just write it down, find the spectrum and be happy. So that's roughly how they proved this. So they used, they looked at the spectrum of this slightly more general and they found that this is the And they found that this is the eigenvalue that is relevant. So this pops out, this strange expression pops out as the root of some quadratic. I prefer the following probabilistic proof of this, which I find much more intuitive. So the base idea is to use this representation of the relaxation time as the rate of exponential decay of the total variation distance, which I can define in my Find it. So the relaxation time, so this equality is usually used the other way to control the total variation. You can also use it this way to say that the relaxation time will be bounded by the mixing time or something like that. But here we will use it much more directly. So we'll use this indeed as a definition of the Indeed, as a definition of the route. Use exactly. So, how do we bound a total variation difference by? So, we take two copies of this chain, of the east chain on two sides, with this boundary condition. So, this is my black, by the way. So, you take two copies and you couple them in the simplest possible way. You just perform the same of days and the same. Perform the same days at the same time if they work, they work. They don't work, they don't work. Uh, your toilet costs are also this kind. I claim that as soon as I try, as I update this guy and I put an infection there, and then I try to update that guy, I will have coupled the two copies of the ch. That's the way life works. Uh, and the number of times the number the number of times the the number of updated attempted updates at the second site uh which are preceded by an update on an attempted update okay it's always accepted on the first site is a Poisson of parameter t where t is the current time over two that's because you keep waiting for the first clock to ring then for the second clock to ring then again for the first clock to ring and so on so that's where you get the n over where n is a plus form Where n is a Poisson retrogradable. And what's the probability that you don't couple by time t? Well, the probability that each of those couples of load dates doesn't succeed. The probability that you succeed is just p, because the first one should get an infection. So we totally get this down on the probability of not being coupled. So here I just removed on the next line the integer. Moved on the next line the integer parts, which we can really know doesn't cause any change in this limit. So I just put this root one minus b that you see appearing, which will be this one, to the power n. And now this is just the moment generating function of the Poisson random variable, which is very classical to compute. And it's this one. Okay, so here you see very clearly. So here you see very clearly where the root comes from. It comes from every second time you do something and that's it. So I should say that from the spectral approach you see that this is actually the relaxation that is equal. So this bound here is absolutely sharp. It gives you the exact relaxation. Right. So that's the proof. Now how do we use this? The idea of the bisection technique is The bisection technique is well to bisection. You're trying to prove Poincaré inequality for the East model on a bigger volume than two sides, two sides is in the revolution. And how will you do that? Well, you'll cut in half. So you have, always I didn't draw it here, have an infection here on the left, that's your boundary function. And you want to apply induction to boundary relative function. Your induction will be: I know how to resample. I know how to resample this. This is an East model with a boundary condition. So I already know by luxury how to do that. I have a relaxation time, but I don't know what that. And I want to say that I also know how to do this guy. The problem is I don't have the infection here. So that's why we will use this two-block contrainity to say, ah, well, I wait until I resample the first part and put this guy. And put this guy here. And only then, when that happens, I can also resample this. Okay? So it's a very nice idea. And when you apply this recursively, you will get a bound on the relaxation time on the infinite volume, which is just the product of these factors as you keep growing your scales in this. Your scales in this exponential way. Each time you double. And then the question is: will this product, infinite product of this factor, be finite? The answer is no. The reason is this. So here I gave you the asymptotics of this strange expression. I'm sure you can see it immediately. So if P is small, so if you're asking for an unlikely event to occur here, you need to wait roughly two over. Roughly two over the probability of that happening to relapse. So that's how extremely surprising. Much more importantly, if you're asking for a very likely event, then you just need to wait one plus a small thing, root epsilon. And so I emphasize here that this could not have been proved with just a bound using the mixing time. The mixing time of the East model on Time of the east model on two sides is not one plus epsilon, it's something. But here we have this one plus epsilon, so there is a chance that we can multiply these things up to infinity and still get a finite value. To do that, we need to always ask for a likely event. How can we make this likely if we just ask for this infection here? This has probability p, and it's always p. So I get product of. So I get product of 2 over p up to infinity is infinity. But if I change this slightly, so I ask for this infection to be perhaps close to the middle, but not exactly the middle, then when my boxes get big, this can start being likely. And okay, you can trust CMRT that you can choose your scales in the right way so that this infinite product is finite. Infinite product is finite, and bonus you get exactly the right constant here if you set up your proof in the right way. But that's really out of interest. All right, so that was it about the bisection technique now and about the East model. Let's generalize. Super group. So yeah, I forgot to mention before we generalize that this That this scaling exponential log squared is important from two perspectives. One, it's not polynomial, but it's not like the one-neighbor KCM. So this already tells you these two models were supercritical. And then now in the KCM world, they have different types of scale. So this tells you in the KCM, you will have the supercritical models will ramify into at least two different things. And from a physical perspective, this is important because physicists speak about strong classes and fragile classes, strong glasses having what we call Arrhenius behavior. For us, in our language, it's these polynomial divergence. While fragile glasses can be fitted well with different expressions, one of which is exponential of a quadratic function in the law. In the block, something like this. Okay, so it's also particularly interesting from that. Anyway, let's generalize the prominence. So I say that an ablate family, a superpartical family, is rooted if there are two non-opposite stable and unrooted album. So the East model is unrooted, sorry, it's rooted, has lots of non-opposite stable functions. While the one-neighbor model has no state regions at all, so it's uncomfortable. And just the theorem, the only two possible behaviors are this polynomial one and this exponential log spring. And you get those depending on whether you rooted or unrooted. This was proved by all those people, most of them are here. So the proof mostly follows these two techniques. Folllows these two techniques that I already talked to. The most tricky part, in my opinion, is this part, which shows a lower bound here, which relies, it proceeds in the same way as what I already told you, except that there is a key input, which is this exercise for any super critical moment. So that's no longer an exercise. So if you don't manage to do this one last quarter, If you don't manage to do this, one must work that looks more exciting, which also works in any dimension. And as soon as you manage to prove such a logarithmic bottleneck, the proof to the proof. Right, so that's all I wanted to say about supercritical case. Now moving on to critical ones, again, I won't say much. There is a person for that. So for the two-neighbor KCM, PC is zero again, and we see. Is zero again, and we saw this pi square over 18 scaling. Here we can we prove by with Fabio and Christina that the scaling is exponential pi squared over 9p this time instead of 18, basically the square. Despite that, on the when we ran the dynamics for a little it looked like it was very very subtle. So I won't tell you how to prove this, we can actually. Telling how to prove this, we can actually prove a bit more, but in any case, this is an important result in the sense that it's the first sharp, it's a sharp result with a constant for critical kinetics constraint. But instead, let me, instead of going into very specific, very precise results, let me just generalize in a rough way. So, for in the critical case, yeah. So, for any critical KCM, it turns out that we have the same scaling as what happens for loose drug. Just stretch to exponential, there's no ramification at this point. Life is simple. And of course, if you want to know more, Laura will tell you infinitely more about this theorem and especially about what's not in this theorem. So, I won't go for predictors. So, that's basically it. So that's basically everything I want to say. Let me conclude with a few further corrections. The first one is mostly because I know someone will ask otherwise. So let me start by just saying it. For higher dimensions, recently in three quite voluminous papers, Paliser, Bolovash, Morris, and Smith proved the higher dimensional analog of this university for bootstrap. So the upshot of that is the following. The upshot of that is the following. Supercritical models are the same. They are those which have a finite set that can reproduce. They are also those which have a half sphere of unstable regions. Exactly the same. They are those which have a polynomial scale. Subcritical models are the same. They are those in which for every hemisphere there is an open set of stable versions. Are stable versions. There are also those which have PC strictly functions inside of those. There are three trivial subcritical ones, which have PC1 and which have all the direction. So everything is very nice and analogous. Now the most interesting part, which is also most of the work, are critical families. So it turns out that in D dimensions, critical families ramify into D minus one class. Into d minus one plus. In one dimension, there is no vertical plus. We already knew that in a sense. In two dimensions, there is one, which is the one we saw. That's the one that Laura will tell you everything about. And in more dimensions, there's d minus v. And they have this scaling, which is stretched iterated exponential, where the exponential can be iterated, well, one time, two times, et cetera, up to d minus one time. times, etc., up to d minus one times. And then inside you put this polynomial. And they also give you a recipe for determining how many times you need writer. Okay, so to put this to KCM, I can't say this is a theorem because several of us have slept over the past few months, but most of the KCM analog of this result just goes. Analog of this result just goes through without anyone doing anything. So there is the subcritical models, which is by the things I already told you. Lower bounds follow from this. There is a distinction between rooted and unrooted, supercritical families, which is already known. But the parts which are still open are proving the upper bounds for supercritical and critical. Super critical and critical, in particular, to get this scale. All right, so let me conclude with my list of open problems before waiting for tomorrow. So a few things that I already mentioned, starting with KCM in the resulting R dimensions, which needs some work. Another thing I mentioned was this conjecture about proving sharpness of the failure of the bootstrap. Of the phase of the bootstrap population phase transition, this exponential decay above criticality in this parametrization, which is relevant for KCM as we saw. Now, still within the subcritical model, there is a very interesting thing that Christina briefly mentioned: that there are some models like oriented percolation for which it is known that the phase transition is continuous. Oriented percolation. Oriental potential, which is a classical. At FPC, you have zero probability for the origin to stay healthy forever. But Beroly Fisher and Tunell provided an example of a two-dimensional subcritical kinetic constraint model, which has a discontinuous space proposition. There will be both, but we have absolutely no clue. Absolutely no clue which model should do what. And this is a very, very exciting question that many people would like to know the answer of. So even a conjecture here would be extremely interesting. Also, physicists certainly don't know how to say a priority if something is continuous or not. So that's extremely interesting, probably very hard to make. Probably very hard. Now, as I said, we don't have, we have frustratingly little knowledge about subcritical genetically constrained models in general. We just have these two reduction results. Nothing which is not, which you can't read directly in the booster equation. And there are things which we will not be able to read up. So it would be very interesting to look at that. Now, moving away from subvertible, as Christina also mentioned, As Christina also mentioned, out of equilibrium is a very limited understanding. So, mostly we have results which are limited to the East model, which are extremely specific. They can be very nice, but they're extremely specific. We also have some understanding of the very high temperature regime, which is so high temperature that it's actually negative temperature and so negative that it's actually zero minus, which is, I assure you, not interesting at all from a physical perspective. Interesting at all from physical perspective. The study of glasses, like look at them at low temperatures. So it would be very, very interesting to have out of equilibrium results at low temperatures, which go beyond very specific models. That's largely open to the problem. And finally, as I know that we have diverse people here, I wanted to encourage you all to study this universality. Study this universality framework in your favorite setting. It could be, okay, my favorite settings could be other people like other graphs, perhaps more algebraic, located graphical rules, coverage can be many things. Some people like in homogeneous settings, you could ask for a different constraint on different sites, maybe in a random way, for the hub. The simplest way to do this is to The simplest way to do this is to just put what we call pollution, which corresponds basically to removing some of the sites of the lattice. You can do this, by the way, all of this applies to both booster propagation and KCM. First, study booster propagation. If that's very easy, we'll want KCM. Conservative kinetic reconstrain models, or Asak will tell us about an example of that. And it would be very, very interesting to Very interesting to have some universality understanding there. There are also, I know there are also specialists on forget models, so there it also calls for some understanding of which model is what. There's really many, many models which should have infinitely many. All right, so thank you for your attention and 