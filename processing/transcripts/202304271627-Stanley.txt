We have a minute.  I forgot I'm white so that you have to say that we're having a problem. So it's like a lot of fine characters. Okay, I know you're not actually so foreigners kind of meeting me.  All right, I think let's start with business. Our final session has to do with unfolding. And so, let's see, Mike Stanley is going to tell us about accounting for systematic uncertainties in unfolding, uncertainty qualification. Thanks, Glenn. Thanks, Glenn. And yeah, so as Glenn said, my name is Mike Stanley. I'm a fourth-year PhD student at Corner Mellon. And first of all, I'd just like to thank the organization here for inviting me to give this talk. This is all work that is from this one paper that I wrote with Mikhail and our other colleague Pratik, who's now used to work with. And this is the first opportunity, I think, we're getting to talk about it, or at least I'm getting to talk about it. I think Mikhail is a few circumstances. A few circumstances. But why don't we get going? So, this talk is going to first provide a brief mathematical overview of unfolding as kind of we presented in the paper. And so hopefully we can have a common vocabulary to talk about these things. I'll characterize primarily two types of systematic uncertainties, namely regularization bias and widen bias. And widen bias, which affect our uncertainty quantification objectives. And I'll explain exactly what that is in a moment. And finally, you know, to save the day, we present a framework that we believe can address these two systematic uncertainties and a few methodological options that you, as practitioners, can hopefully use to address these particular challenges. Okay, so let's just get on the same page. Let's just get on the same page here in terms of notation. So we're seeing the unfolding problem essentially as some sort of density deconvolution, some sort of inverse problem. And to explicitly state, our goal is to estimate a true unknown probability distribution for some variable of interest via a finite resolution detector of observations from some finite resolution detector. And here's a nice image I saw from Mikhail's. Image I saw from Mikhail's PhD dissertation, which kind of describes these two directions, where we have some true spectrum over on the right, which is folded when observed through the detector into this smeared spectrum. And our goal in the inverse problem is to unfold. It's basically to gather observations from the smeared spectrum and infer something about the true spectrum. And of course, the main challenge here is that for a particular smeared spectrum, For a particular smeared spectrum, there could be several different true spectra which would result in that smeared spectrum. Okay, so to be a little bit more precise, I'll use the, you know, F to be the true particle level spectrum and G to be the smear detector level spectrum. And these are both intensity functions of some underlying Poisson point process. I'll call the true space T and the smear space. The true space T and the smeared space S. And our smeared spectrum is going to be basically the result of this convolution, where K is basically kind of this smearing kernel. And our goal here is to infer the true spectrum F given some observations from China. Okay, so the way we're going to attack this is by discretizing everything into histograms. Everything into histograms with uniform size bins. So I'll call the bins in the true space T sub J's and the bins in the smear space S sub i's. And we'll have n true bins and m smear bins. And the data that we collect is then in this n-dimensional space where the expected value of our data, I'm going to note by this mu, which is, of course, can be just written as the specter of integrals over. As the specter of integrals over our smear intensity function. Similarly, our parameter of interest is then, I'm going to call it lambda, which is again just integrals over our true intensity over each of the bins. And what this means here is that we can define this matrix K using our true intensity function and our kernel. And we can do Kernel, and we can define then basically this linear relationship between the means, and this is going to kind of form the center of how we're going to treat this. And actually, these elements of k in our matrix have a nice interpretation, which is that they represent the probability that a smeared event is in bin I, given that the true event is in bin J. So it's also kind of a nice probabilistic interpretation of what this matrix is telling us. And here on the right, I plotted a few. Here on the right, I plotted a few heat maps of these matrices where the white is indicating a higher probability, basically. And so, kind of as you might expect, there's not very much probability that an event observed in bin one is going to end up in bin 40. So, anyway, and also I'll just mention too that for illustration purposes, I'll have kind of these two These two sizes of these matrices. So the 40 by 10 means that we have 40 smear bins and 10 true bins, and the 40 by 40 means we have 40 smear bins and 40 true bins. Okay, so once we do this discretization, this means that essentially when we observe a y, we're thinking about that as a draw from this Poisson distribution. And we're also going to invoke the We're also going to invoke the Gaussian approximation here since we're dealing with bin counts and this turns out to work pretty well. Okay, so I started this talk by saying, you know, we have kind of this amorphous UQ goal. So as I think, you know, Michael nicely talked about, there are many ways that we can talk about uncertainty quantification or UQ. So what exactly do I mean here? Well, our main goal is to compute. Our main goal is to compute confidence intervals for the true in-means. And more precisely, the way we're going to characterize this mathematically is by considering these linear functionals under our true parameter space. And so h here is a vector, which basically you used to take a dot product with some input lambda. And it turns out that this simple mathematical form kind of gives us quite a bit of flexibility. Form kind of gives us quite a bit of flexibility in terms of the clients of interest we might be interested in performing in, for example. So, for instance, we can use linear functionals of this form to find aggregated bin counts, or we can just pluck a single element out of our vector by just making one at the position of interest and zero elsewhere. So, to state this again, maybe even more explicitly, our uncertainty quantification goal is to find. Our uncertainty quantification goal is to find a random interval with a coverage guarantee, which is to say that for any alpha, we want to find an interval, i sub alpha, which is a function of our random y, such that the probability that our true quantity of interest is contained in our interval is at least one minus alpha. Okay, so hopefully we're all on the same page now in terms of notation. We're all on the same page now in terms of notation and whatnot. So, what are these sources of systematic uncertainty? After all, that's why we're here. So, as I mentioned, I'll primarily talk about two. The first is the so-called regularization bias. The second is the wide bin bias. The third, which I won't talk about, is missing auxiliary variables. And the fourth is uncertainty in this response kernel K. I will talk about these two, as I mentioned, and Richard, who's coming after me, is going to talk about number four. So you have some things to look forward to. Okay, so okay, so where exactly then are we getting systematic uncertainty when it comes to these first two in my list? So it turns out that we have this Monte Carlo unsat that we need to use in place of our true intensity function f, as I described earlier. As I described in earlier slides. Namely, computing our matrix K involves knowing F, which of course we don't know. But we can use this Monte Carlo Nansot, which I'll call FMC, to approximate two key quantities. The first is, of course, if we have this FMC, we can find basically the corresponding mean vector for our Monte Carlo intensity function, which I'll call lambda. Function, which I'll call lambda mc. And it turns out, and I'll explain this in a second, this affects, this is a key part of the regularization bias. But more importantly, and this will deal more with the wide bin bias, is that to compute k to compute k, we also need f, which again, we don't know. So a natural thing to do here is to substitute in our want to correlate intensity function. In our Monte Carlo intensity function. And we hope, of course, that these things are close to each other. But that might not be the case, and it could hurt us in some way. So let's go through these one by one. First, I'll talk about the regularization bias. Okay, so let's imagine a scenario where the number of true bins, which again I'm using n, is large. That corresponds to a case where the smearing matrix K is going. Where a smearing matrix K is going to be severely ill-conditioned, which is going to mean that if, for example, we were using a least squares estimator, this would be very sensitive to noise. So a natural thing that we might do is reduce the variance by increasing the bias of our estimator. And so some common solutions here are things like technology or doing EM iterations with early stopping. With early stopping. So these are, you know, I think very sensible things to do given kind of this particular problem. But unfortunately, both of these approaches bias our solution towards our Monte Carlo prediction, lambda MC. And again, depending on how good your FMC is, this can maybe be bad news. These bad news. And this is extensively discussed. And again, Mikhail's dissertation has several excellent pages on this. But just to give you a little bit of taste of what some of the plots he has in his dissertation look like, here we can look at basically the results of simultaneous coverage as a function of both regularization strength in terms of ticking off regularization, and again, simultaneous coverage as a function of number of iterations. As a function of number of iterations and the EM would early stop. And you can see here that, for example, in the Tikhanov regularization case, as we increase our regularization strength, our coverage basically is destroyed. So intervals that we might want to make for our bin counts basically lose their coverage properties as we increase regularization strength. So the takeaway here is that not So the takeaway here is that non-zero bias, which in this case is incurred by regularization, means that coverage is not one by itself. Now, a naive question is, well, can we just not regularize? Maybe that's a sensible thing to do. And I'll get to that in a second. But first, I'll talk a little bit about what I'm going to call Systematic 2, which is this wide-band bias. Okay, so we've already seen that there's maybe a few issues. Already seen that there's maybe a few issues that will arise if we explicitly regularize. So, what if we implicitly regularize by using fewer or wider bins in the unfolding stuff? Well, that seems like a good idea. Intuitively, we might want to choose the number of bins to be kind of on the same order as the detector resolution. It seems like a sensible thing to do. However, again, we get exposed to the misspecification of Misspecification of FMC, which again comes through in our K matrix. So the wider bins we make, basically, the more we're exposing ourselves to misspecification of that Monte Carlo element. Okay, so this seems dire, but don't worry, we'll present a solution in a moment. But before I present a solution, let me just tell you a little bit about simulations that we're using. We haven't, unfortunately, We haven't, unfortunately, looked at this on real data yet, but we have kind of a realistic simulation study that illustrates the nature of the solution. We're going to basically look at a Gaussian mixture model, which in this case has been scaled to make it some point process intensity function. And so we've defined a true intensity and a Monte Carlo intensity, and you can see here that we've defined it in such a way such that there is actually kind of a specification between these two intensity functions. Between student tensity functions. And that will allow us to kind of systematically look at how this particular Monte Carlo misspecification affects the coverage of our resulting controls. Okay, so let's talk about why bin bias. So we propose that you can address this wide bin bias issue by using fine bins. The high-level idea is that we can reduce the dependency of our K-matrix on our Monte Carlo one sorts by infinitely. Our multicolored one sorts by unfolding with a higher number of fine bins and then aggregating afterward to whatever our desired wide bin granularity is. And to give you a sense of why that might work, I'm going to again show kind of a heat map of matrices under kind of different discretization schemes. And you can see here that in the left-hand side, if we unfold directly to 10 wide bands, the difference between Things, the difference between our original K matrix and our Monte Carlo matrix has some kind of hot spots to it. But if we increase the resolution of the true space, we can see that those go away not entirely, but by and large, they go away. So, our general recipe here, to which you could likely apply many different types of unfolding procedures. Many different types of unfolding procedures is to unfold first with fine bins. And the key thing here is no regularization, which will address the first systematic. And the second is that we can aggregate to wide bins, keeping track of correlations for error propagation. So I'll try to illustrate how this recipe works kind of step by step. So we'll just take a very simple estimator, least squares, and construct. Squares and construct the corresponding intervals from this estimator and look at how this works for coverage. So here we're unfolding directly to 10 bins. And using just our basic least squares intervals, we can see that in many bins, we suffer from pretty dramatic undercoverage. So here we're trying to get 95% confidence intervals, and that clearly does not happen. So, the first step in a recipe is to unfold to wide bin. So, if we do that in this case, you can see that we're nearly in every bin basically now achieving the desired level of coverage, but we've paid a dear price. We now have these massive interrolls. And maybe that's not great, but at least we've solved the problem, right? But of course, this is a two-step recipe. step recipe. So the second step here is that we can aggregate adjacent things together and hopefully, you know, well it turns out, not hopefully, that this actually still we still get the right coverage and you can see here that the size of these intervals is dramatically reduced. But it turns out that we can do even better. So again, I've Even better. So, again, I've just talked about kind of maybe the most naive thing that one could think of doing, which I think illustrates well the recipe, but we can do even better. So there's some limitations of using least squares. For one, it's not exactly obvious how we can build in any physical constraints we might have. And two, the fact that we need to have k transpose k be invertible means that there's only so much, or there's only so many. Much, or there's only so many true bins that we can use in the discretization before things start to get messy. So, we propose two solutions in our paper here, which allow us to use a rank coefficient k, which again can help you drive down the mispecification error of the k matrix, and allow easy incorporation of non-negativity constraints, which in this case is a good idea because, hey, these are bin counts, we can't have negative bin counts. Can't have negative incomes. So, the two methods that we propose are called one-at-a-time strict bounds intervals. And these are basically a modified version of simultaneous strict bounds intervals from this paper by Philip Stark and also a nice paper by Christ McLeary. And these produce intervals that have bin-wise coverage. And based on these intervals, so these intervals are basically constructed as two endpoint optimizations. Optimizations. If we look at the dual of those optimizations and do a little bit of magic, we get these so-called prior optimized intervals, which are also kind of a decision-theoretic element where we can use a prior basically to optimize intervals over a set of decisions that are intervals that are guaranteed to have the right minimized coverage. And the important point here is that although we're using a prior, prior is Using a prior prior misspecification in this case does not affect coverage because we're only choosing between those intervals that are guaranteed to have coverage. Okay, so I'm unfortunately not going to have time to show you the actual math. Maybe we can get to that in Q ⁇ A. But just to show you what the results look like for our simulation example here, we can see here a comparison between least squares intervals, our one-at-a-time strict bound intervals, and the prior optimized intervals. And you can see here that where the You can see here that where the least squares intervals for souls suffering some oversized problems, we're doing substantially better with both the one at a time and the prior optimized intervals. And even when we kind of do simulation studies here, we can look at the average interval lengths, and we can see that we're consistently doing much better than least squares. And importantly, we are. And importantly, we are actually overcovering in most of these bins, but the important thing is that we have at least one minus alpha coverage, bin-wise coverage. Okay, so just some quick conclusions and next steps. We've identified four sources of systematics and unfolding. I spoke about regularization bias and wide end bias, and again, we're going to talk about number four soon. And we can potentially address these two, but in Address these two by unfolding defined bins and then aggregating. And we provided two methods to do this that allow both physical constraint inclusion and the use of rank deficient k's, which allows you to address this missification. And as a local next step, Richard's going to be talking about number four next. And as a non-local next step, given all the discussion about likelihood ratios and all this, I just wanted to mention that a good part of my PhD work is thinking about. Part of my PhD work is thinking about likelihood ratio tests and incorporating constraints into likelihood ratio tests, and in particular, looking at these particular strict bounds intervals as vertical likelihood ratio tests. So I'll just say, if you want to stay tuned for that, hopefully we'll have some more interesting and useful results for your community. And thank you so much for listening. Great, thank you, Mike. The next talk is very thematically aligned. So, is this really specific on this talk? Yeah, I think so. Okay, yes. So, why do you prefer to aggregate your fine bins into wider bins instead of just doing principal performance analysis and presenting your result as a decomposition into principle? Okay, so in this case, are you saying like we could do principal component analysis on Principle component analysis on on on what exactly? Well, you get your uh covariance matrix for five bits, then you do principle component analysis, you get a bunch of eigenvectors, then you decompose your result in terms of those eigenvectors. And that's, I think, a proper way to present your result because you will not get any covariance matrix remaining after that. Your covariance matrix for those components is going to be diagonal. So, can you maybe clarify what your concern is here? So, are you concerned about that? I'm just trying to understand, you know, you are trying to make wider bins, but you still remain with a covariance matrix for those bins, if you want to publish the results. So, why not instead publish the principal components, you can position of your results in terms of those components and uncertainties? Well, the thing is that when we obtain these intervals, the coverage guarantee. These intervals, that the coverage guarantee is for each bit, for each interval. Yeah, you get the coverage guarantee for each principal component if your original coordinates means it was correct. Specifically on this talk. No, no, it is on this talk. On this talk, yeah. Okay, all right. Thank you for like following. So, as maybe for Mikael, so folk are global. A lot of work on that closing, no, I know. I know he was a big fan of using dramatically different number of bins and aggregating afterwards and stuff. So I wonder if Mikhail knows if this is what Coker was doing or a different flavor? I'm not actually entirely sure. I know that there has been a lot of discussion of should you be using more Bins TrueSpace versus the smear space? And I think Lobera and Zach was the other kind of long-term person during this campaign. I think he had different recommendations. This campaign that we have like different recommendations on that. But I haven't seen people doing what we are doing here, which is that you first actually basically do as many findings as you can to kind of reduce the systematic from the Monte Carlo and the risk matrix, and then you aggregate. It's mostly, the aggregation part is mostly kind of a communication part. In principle, there will be other ways we summarize these results. I mean, you know, people like to communicate this using histograms, so that's why we are doing it with histograms. But at the end of the day, you don't have to find out. At the end of the day, the fundamental solution in some sense is the thing that blows up and looks terrible if you look at it as a histogram. So putting it into white pins is maybe a better way of communicating that. But yeah, I don't remember much about the presentation. I sense we're going into a discussion. The two talks are quite, I think, thematically aligned. Are quite, I think, thematically aligned. And I think it would be profitable by default first. And then we can come back and discuss. Can I just add something on this on a particular part? Because so when you show these intervals, and you mentioned they are intervals, are essential intervals, and I can clean up about using the middle of the interval, like slide 17. Well, the method actually, what's interesting about it is that we find the endpoints. There's no, it's not like we're finding a middle and then finding a plus or minus around. So, I mean, you could always just take the midpoint and say that's a point estimator, right? I mean, I don't know exactly what that means. But that's why you could do that. Yes, yes. Okay. 