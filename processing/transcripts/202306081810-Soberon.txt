Okay, just the size of. Before we begin, I want to thank all of you for being here. Really enjoy the workshop so far. And the other announcement I have is to just give you a reminder that we do collect some tips for the waiters in the restaurants. If you're inclined to do so and you haven't done so yet, If you're inclined to do so and you haven't done so yet, you can ask Deborah about it and just keep that in mind. Okay, so I want to talk about the central transversal theorem. Revisited to tell you something a bit different from this, sort of in line with the title of the workshop about new trends of classic theorems. And this is joint work with Michael Manta. Michael Manta is now. Michael Manta is now an undergraduate student at Caltech. And of course, I'm telling you the story in the reverse order. We didn't start working on the central transfer theorem, but as we were working on Yao Yao partitions, which I'll mention throughout the end of the talk, we stumbled on these things. Okay. So before I actually start with the big thing in the talk, let me give you a bit of motivation for that, sort of tell you my perspective. Sort of tell you my perspective on how one operates in topological kinetics. Okay, so let's start with an example of the Ham-Sanditz theorem. I will avoid any precise conditions on the measures, but the main idea is that given D measures in RD. In RD, there exists a hyperplane halving all of them. And the obligatory drawing is: I give you two measures in the plane, and then there's going to be a single line which cuts exactly half on each side. And as I increase the dimension. Side, right? And as I increase the dimension, I increase the number of regions. So if we want to prove something like this, we say, okay, I'm talking about partitions of R D with hyperplanes. I'm going to try to parameterize the space of such partitions. And once we do that, we get to a topological tool, right? We get here to the Borsukulan theorem. Which says that. Which says that any odd continuous function from a d-dimensional sphere to a d-dimensional real space pass a zero, at least one zero. Okay? And of course, by an odd function, I just mean that f of minus x is minus f of x for every. Is minus f of x for every point in the sphere. Okay, so as you want to prove variations of the Hans-Sandrich theorem and other results which happen to be in topological combinatorics, you're doing these parametrizations, reducing your statements to statements on topology. And then you have to do one of two things, right? One is to learn the tools to prove results of this type. So I change the spaces, I change the type of partitions, you get some root statement here, you have to know the tools to prove this. Know the tools to prove this. Or one other convenient thing to do is you start building a dictionary of really good results of this type, which happen to, you can pick off the shelf and see if it works, right? And this is a very productive way of working, because if you notice, the Borsukoulam theorem by itself is just quite in a bit of result, okay? So one of the things I want to convince you in this talk is about a variation, an extension of the Borsukoulam theorem that hopefully The Brussels coulom theorem that hopefully I can convince you to put into your collection of useful and good results here. Okay, and instead of showing you something completely new to prove with that result, I want to convince you that the central transversal theorem and some of its extensions we can prove with that. Okay, so this is the main objective of the talk, and you can judge based on that if it was successful or not. So before I want to write the big term on that board, so I will use the blackboard for some definitions that we'll have here. That we'll have here. And so, the space that we'll work with, instead of working with spheres, we're going to work with Stifl manifolds. Okay? And I'm going to denote by VK of RD, this is the Stiefel manifold of orthonormal K-frames in RD. Okay, so this is just the set of K-top. is just the set of k tuples v1 up to vk such that this so vi is in or d for every i and they are orthonormal every vi is a unit vector any two vi and vj they are perpendicular to each other okay so a picture of a point in v2 of r3 would be to have v1 in the unit sphere V1 in the unit sphere and V2 somewhere along the unit sphere as well. Okay. So if, for example, k is equal to one, this is just a unit vector. We're talking about s d minus one. I'll just adjust the dimension so that things match a bit. And we want to study Borsokoulum type theorems with this space. Now, here we're talking about odd functions. We really care about symmetry in our problems. And here we also have And here we also have action subgroups. I like to think of Z2 as plus one minus one with multiplication. And here we have an action of Z2 to the K. Okay, so if I give you, maybe not lambda, alpha one, alpha two, up to alpha K A and L. To alpha k an element of z2 to the k, and I want to apply this element of this group to this element here. I just multiply v1 by alpha 1, v2 by alpha2, v3 by alpha3, and so on. And if this was an orthonormal frame, when I do this, unit vectors are still unit vectors, perpendicular vectors will still be perpendicular vectors, okay? So we have a nice free action of this group on this space here. The other thing we can look at. The other thing we can look at is the dimension of the space. And the dimension is d minus 1 plus d minus 2 all the way plus d minus k. Because if I want to choose v1, it has to be on the unit sphere in Rd, so it has d minus one dimensions to choose from. If I want to choose V2, it also has to be on the unit sphere, but it has to be perpendicular to V1. So it has a sphere of. To v1, so it has a sphere of dimension d minus 2 to choose from. So, this is the dimension once I choose v1 and v2, and you keep adding until you get to the end. Okay, another space that behaves this way is or d minus 1 times r d minus 2 times or d minus k, right? The dimension of this space is precisely this. And this space also has an action of z to the. And this space also has an action of z2 to the k, doing exactly the same thing. I just take whatever entry I have in the first real space, I multiply it by alpha one. Whatever vector I have in here, I multiply by alpha two, and so on. Okay, and if k is equal to one, it's just or d minus one. Okay, so this is the Portsukou Lamb theorem that I'm going to advertise here. And we found that we needed this result. We found that we needed the result, we proved it, and then we figured out that this was already proven by Chan, Chen, Frick, and Hall 2020. How do we write this? Any Z2 to the K equivariant Continuous function F from VK of R D to the space pass As a zero. Okay? So this is the result I want to convince you to have in your collection of good terms. A few things here. If k is equal to one, this is Borsukula. Okay? And I want to continue three things. First, so the reasons why I like this is there are simple proofs of this result. It has good consequences and useful things we can do. Good consequences and useful things we can do with it, and it's at first sight, at least for me, a bit unexpected in what sense. So, one of the first or the most useful ways I found to extend Porsocculum theorems is using this really nice result by Dolt. So, if I want to check if a space, if a function from two G spaces, if there's a function, I can give you a G space, another G space for some from G. I want to know if you have an. For some group g, I want to know if you have an accurate function of between these two spaces. One thing you can check is the connectedness of x and the dimension of y. And if this is highly connected and this is low dimensional, there won't be an equivariant function and you get stuff precisely like this. Okay. So the go-to tool that I try to use here, it doesn't work for this. Even if you think of the case, like say V. Like, say, V2 of R3, this is SO3, which is nowhere near as connected as you need to use those two. I think this would need to be two connected, and I don't think it's one connected. It misses on the connectedness that you need for using those tools. Despite that, almost anything else you throw at Atherem proves it. So, let me convince you of how. Me convince you of how one would prove this and give you an idea of what other tools you might want to use. Okay. And what is the proof idea behind proving this? And I will hand wave a couple of things. But the main thing is if I want to show that this has a zero, the first thing I will find is a particular function g. Let me call this big thing just or. I want to find a particular function here, so somewhere where I can tell you something interesting about the zeros. And what this function is going to do. So the way I like to think about the image is I'll have first a vector in order minus one, then a vector in order minus two. Then a vector in Rd minus two, a vector here, up to a vector in Rd minus K. Okay, so what I'm going to record here in Rd minus one is the last D minus one coordinates of V1. Okay, so last coordinates of V1. In the second one, I will record the last D minus two coordinates of V2. d minus two coordinates of v2, the last d minus three coordinates of v3, and so on. Okay, so let's look at the zeros of this function. If I have zeros here, so that means that v1, the non-zero entry could be just the first one, it has to be a unit vector, so it must have a plus minus one on the first entry, okay? For v2, it must have zero coordinates here. It must be perpendicular to v1, so it must have a zero here. To B1, so it must have a 0 here. And so it can only have plus or minus 1 here. V3 must have zeros here, 0 here, and 0 here. It can only be plus minus E3. Okay? So there are exactly two to the k zeros. Okay? Okay, and of course, if I bias that zero, the whole orbit under z2 to the k of that zero is also going to give me a zero. So that's actually, there's only one orbit of zeros. And one is an odd number. Okay. And so then one remembers, for example, Imre Barron's proof of the Borsukulam theorem from the 80s. There's a very nice. Theorem from the 80s. There's a very nice paper by Oleg Mossin from 2012, which really expands on the method. But what you do is you make a new function. Okay, there we go. H from Vk of Rd times 0, 1 to the space, the same image here. So I'm increasing by one entry. And what I do is just h of x. just h of x lambda is going to be lambda f of x plus one minus lambda g of x okay so here i have my stifl manifold my stifle manifold i'm taking the product of zero one so i get like the cylinder and when lambda is equal to zero we just get g so here the function behaves like g if lambda is equal to like g if lambda is equal to one we just get f so here it behaves like f and then if we look at the orbits of zeros if this is a generic enough function by counting the dimensions it should be a one manifold meaning it should be a bunch of cycles which i don't care about or a bunch of intervals with a boundary in the extremes maybe both endpoints on the same size or one endpoint here and one endpoint here okay Here, okay. And the endpoints they must go to different orbits if you look at this for a while. And so the orbits on the left side must either cancel by pairs or cancel with orbits with orbits of zeros on the right side. And since I had exactly one orbit of zero here, it must cancel with an orbit of zeros here, and that's the zeros on f, right? Now, what if you tell me, Pablo, I don't like this proof, I don't like this method. I don't like this proof. I don't like this method. What else can you do? So, one thing is the Chan, Chen, Trick, and Hoolproof. What they do is they construct a different topological invariant. They use the same function g. And so on function G, it's something. On functions which don't have zeros, it's something else. So you must have a zero. It's basically a different invariant, but it's the same clever function that tells you this. Same clever function that tells you this. You have this proof with the methods. So, if you like the sort of things, just read Alex's paper from 2012. That's the best reference for it. Or maybe you say, you know what? I just talked with Pablo Bergovich and I just want to use the Fadel-Hossini index. So if you like this sort of things and you say, I really want to dive into the topology, you don't actually need to do anything. You pick this, the original paper. This, the original paper of Fadella Rossini from 1988, and you realize that they computed the index, enough of the index of this spaces, enough of the index of this spaces minus the zero to just say, okay, I just look at the index here, the index here, one doesn't fit into the other, and we're done. Actually, Fader and Jusini, they prove exactly this theorem, but they didn't like the difference here. They just have d minus k, d minus k, d. just have d minus k, d minus k, d minus k, d minus k, and they prove that. You can prove that one by doing this one and completing with zeros on the top, but you'd have the different subdimensions, so you cannot do that trick. Okay. Okay, so hopefully you believe this theorem. So let's go and try to prove something clever with it so that I can convince you that it's useful. Okay. Any questions so far? Oh, yeah, great. I I could find some examples. I was thinking about So, the big use of thing as compared with the Grassmannian is that here I will have an orthonormal basis of the span of those. And that's exactly what solves all the issues here. So, let's just start with some notation. What do I mean by the central transfer theorem? Maybe this is an audience in which I don't have to give so much background, but what we have is a definition. A definition. So we're going to take μ a measure in order and L a lambda dimensional affine space. Okay, and what we're going to say is that L is a lambda transversal to mu. If every half space have space H plus such that L is a subset of H. So every hash space that contains your L satisfies that mu of H is at least one over B minus lambda plus one, where I'm assuming that the measure. Where I'm assuming that the measure is a probability measure just by scaling. So that I should do the obligatory drawing. So if I give you a mass in R3, a line is going to be a central line to it if every half plane that contains it has. Every half plane that contains it has at least this much mass, right? So if lambda is equal to one and d is equal to three, this would be at least one-third of the total mass here, okay? If lambda is equal to zero, this is just a center point. If lambda is equal to d minus one, this is a Haling hyperplane. And between those, it's whatever is between those. Okay. So the big theorem. By Shivalchevich and Verechika from 1990, and also by Donnikov from 1992, is that any lambda plus one measures in R D share a Lambda transversal. Transversal. Okay. So this is really cool because in the statement, like we have lambda plus one and lambda, it seems almost that the dimension doesn't matter, but the dimension, of course, is hidden in our definition here. Okay. So lambda is equal to zero. This is the existence of a center point, which we actually use to prove the peak results. So, and for lambda equals d minus one, then this is just the port. Then, this is just the ham sandwich there. Okay. So, let's go ahead and prove this using that result. And for that result, we're going to use k equals d minus lambda. Okay? Because what happens if I give you an orthonormal k-frame v1 up to v minus lambda? D minus lambda. I can consider m the span of d1 to v d minus lambda. So what are we doing here? Let me make a drawing. I'm going to assume that you have learned this by heart by now. So what we're going to do is, so here we have So here we have V1, V2, and so on. And here this is the span of those things. Okay. So we're going to take the masses, each mass in RD, and we're going to project it orthogonally to L. Okay, so this is going to give us a mass in here. And the set of center points of a mass is a convex set. So we can pick. convex set so we can pick say the bary center of the center points if this was mu i we get a point p i here okay so p i is the center point of the projection of mu i okay and so i give you we started with lambda plus one masses so we'll have p0 So we'll have P0, P1, up to P lambda our center points. If all of those points coincide, we're basically done. Because if we take the inverse of Pi, since this was a D minus Lambda space, the inverse of the rejection is going to be an affine Lambda space. And the condition on the center point in this dimension will imply that it's a central transversal with the corresponding dimension. Okay, so I just want to figure out. Okay, so I just want to figure out: is there a set of vectors v1 up to vd minus lambda so that these projections coincide? And so we're going to be making a function from our Stiefel manifold to some space that's going to be checking those things, okay? Checking that those things are equal is basically checking. Well, think of the vectors p1 minus p0, p2 minus p0, up to p. zero up to p lambda minus p zero okay if all these vectors are the zero vector they coincide and we're happy if not then they don't coincide and we're certainly not happy okay so those are vectors in n and this This is an orthonormal basis of m. So we can write each of these vectors using this basis. Okay? So what do we want? We want to make a function from Vk of Rd to Rd minus 1 times Rd minus 2 all the way to Rd minus K, which is R lambda, right? Right? E minus one or e minus two. And so, what I'm going to do is on the first set of entries here, I'm going to write p1 minus p0 with the coefficients that go into that basis. If you want to be more precise than that, so what is the entry that goes here, for example? Here it would be v1 dot be v1 dot p1 minus p0 right then v2 dot p1 minus p0 and so on and then you do this the second row you'll have p2 minus p0 all the way up to p lambda minus p0 we just get exactly the right space we want okay so we have this uh so what happens if i flip the sign of v1 let's check that function okay if i flip the Let's check that function, okay? If I flip the final v1, m doesn't change, so p1 doesn't change, p0 doesn't change, p1 minus p0 doesn't change, but v1 changes sign, so here we have a change of sign. And so every entry here changes sign, and none of the others change a sign. That's exactly the type of action we wanted. Now you might be thinking, oh, what about the rest? Fill it with zeros, okay? So, if I do this function, it's doing exactly what I want. It's an equivariant function between these two spaces. So, it must have a zero. So, all these vectors must be zero. They coincide. We have the central transitional time. Okay. Am I with time? I have 10, 12 more minutes. Oh, no. 30 minutes. Okay, good. More than enough time to show you interesting things. So, first, you might notice, okay, this is a This is what I would call a simple proof of the central top theorem, especially considering that you don't need that much technique to prove the topological stuff, right? It's not like I'm brushing things under the rug. What about these zeros here? We are sort of wasting a bit of information, right? If we actually want to use these zeros, we can get extensions of the central transfer theorem. So let me show you exactly what we get. So this is a new. Okay, so this is a new theorem. So what is a thing that I think is a bit disappointing here is that for the ham sandwich, you're dealing with D measures, but all the other cases, you're getting fewer and fewer measures. And I really like working with D measures in order. So what I'll do is I'll give you always D measures. Always D measures. Okay? And I'm going to give you a number lambda. We want a D here. I think it might not want. Okay. And so what I'm going to find is there exist. There exists L lambda containing L lambda plus one containing L lambda plus two contained in L D minus one. So this is an affine space of dimension lambda, an affine space of dimension lambda plus one, an affine space of dimension lambda plus two, and so on, so that they satisfy the following. The first thing is that L lambda is a central transversal. A central transversal to mu zero, mu one up to mu lambda. Okay, so the first lambda plus one masses, this is going to be a central transverse form. This is already the central transversal theorem, okay? But what about the other spaces? L plus one, lambda plus one is a central, or it's a lambda plus one transversal. Transversal to mu lambda plus one. So for the next one, this space is doing what you want. Okay. And oh, lambda, yeah. All the way up to L D minus one is a D minus one transversal. So this is a hyperplane. For the last measure, mu d minus one, right? So I started with mu zero, so I end up with mu d minus one. So this contains the previous result. Now, if I just gave you this theorem and asked you to prove it, and you don't know about stiffer manifolds and all that nonsense, and you know about this, you can bootstrap this theorem to prove this. You find a central traversal that does this, and then you use the That does this, and then you use the central transversal theorem to see how you can extend that result and get the next space and the next space. But the key thing is that I don't know how you would come up with something like this without a motivation like this one, right? In this case, it's very clear that we have these zeros, and if you see how to use them exactly as I told you, that's exactly what you get, okay? And this one, even the case lambda equals zero is sort of interesting. Here, the case lambda equals zero is the existence of center points. Lambda equals zero is the existence of center points. But this is telling me if I give you d measures, I'll find a point that's a center point for the first measure, and a line containing that point that's a central line for the second measure, and a plane containing that line that's a central plane for the third measure, and so on and so on until you have one for each. Okay? Good. One more thing. Let's I mentioned examples for I mentioned examples for the two of R3, so let's try to work with that. Let's see what we can do. So, what is the other thing where we're missing information? When we were doing these projections onto the span of the spaces, and I gave you an orthonormal basis of M, the center point doesn't really care about that orthonormal basis. It was an artifact of the proof that was really useful. But if the point P1. Really useful. But if the points p1 up to p lambda they depend on the orthogonal basis, not just on the space, we would still get stuff like this. So, what sort of parameters could I get from the measures that depend on this? Well, one thing you can do, let's think about R2. If I give you a mass in R2. I can find the center point of the mass, right? Any half plane containing this point has at least one-third of the mass. The other thing I can do is if I give you v1 and v2 an orthonormal basis, I can take a line parallel to v1 that splits it by half. I call this the ham, I call this the cheese, I apply the ham-sandwich theorem. I call this the cheese. I apply the Ham Sandwich theorem. I find a second line that splits both by half. This is a partition with two lines that splits this into four pieces of the same set. If I fix this first line, this point actually is uniquely determined. And if I move the mass continuously, it will actually move continuously. And it depends on the basis because it depends on the. Dependent on the work, right? So, this center, I'll call it a Yao-Yao center for the moment. Because when you try to do these things in higher dimension, that's the sort of partitions you'll need. We'll call this the Yao-Yao center of a partition. Any half plane containing the Yao-Yao center might not give me at least one third of the total mass, but it has at least one fourth because it contains one of those four quadrants. One of those four quadrants, and it's easier to verify that this is actually doing something like this. Okay, so if I do this with say d equals three, lambda equals one in the proof I told you before. So I have mu zero and mu one, two measures in O3. For one of them, I could take the yellow. For one of them, I could take the Yao Yo center. For the other one, I could take the center point, or I could take the two Yao Yao centers. How does that compare with previous results? So let's think about two masses in R3. Okay, so let's see some results about two measures in R3. So let's pass here a measure. measures in all three. So let's have period measure. So the center central traversal theorem tells you, so let's call them mu1 and mu2. There is a common one transversal to both. Right? This is Shivajevich Chambrechika from 1990. This is Dolnika from 1990. From 1990, this is Dolnica from 1992. We get exactly this. The other thing I can find is there are two planes, H1 and H2, that's split blows into four pieces of equal size. So I can find two planes so that each piece has one fourth of each measure. This is Hadwicker from 1966. This is how we prove that for any mass, there's three planes that cut it into four pieces of equal size. You just take a halving plane and then you. And then you call one side mu1 because mu2 and you apply this. Okay. And there's a simple, a newer proof by Lagochevich, Frick, Hase, and Siegler, I think, that uses some nice degree arguments. So what we get is an intermediary between these two. What we get with this result is there are two planes, H1, H2, so that the first thing is that they split. The first thing is that they split μ1 into four parts of equal size, like this. And the intersection of the two planes is a central line for the other measure. H1 intersection H2 is a central line. For mu2. Now, why do I like this very much? Suppose that you want to prove this and you say, okay, the first case I'll try to prove is when mu1 is equal to mu2. This result is trivial. Project into any direction, find the center point, take the inverse. This result is trivial. Project into any direction, do this, project back. This result, I have no idea how to prove it with a simpler way, even if mu1 is equal to mu2. Even if mu1 is equal to mu2, which is something I would like to add. And there are versions of this in. So, why don't I think the only downside is with these techniques, I cannot prove exactly this result. Because if I give you the same Yao Yao center for two measures, I don't know that the extra line I have here will be the same for both. Okay. So there is still some wiggle room where the methods that you use here, and they're often a bit more sensitive. They're often a bit more sensitive to what's happening in R3, they tell you a bit more, right? But yeah, rather than showing more applications, I have a few more applications that I've worked with Axel Broadfree than with Takahashi on variations of mass partition problems, where the key thing is that this Borsupu Langtap result for stiffled manifolds gives you an easy way to parameterize many spaces, and it's powerful enough to prove some clever things like this. Uh, clever things like this, okay. If you want things like this in higher dimensions, I didn't mention what Yao Yo partitions are, maybe I should. Uh, so one of the things we've heard about quite, so a couple of times in different talks is about the Grumbaum Hadbiger-Bramos problem, right? If I give you one measure in RD, ideally I would want to find D hyperplanes that split it into two to the D pieces of the same size. And that's hopeless. And that's hopeless. And the good thing about hopeless problems is that when you give it to someone who does computational geometry, they often find a way to fix it. And so this theorem by Yao-Yao is that for so given mu a measure in Rd, there exists a partition. There exists a partition of R D into two to the D convex pieces of the same size one over two to the D of mu such that no hyperplane Which is what you would expect to happen if you were able to split with the hyperplanes. Here they are just cones with a common center, which we call the Yao Yao center, that do almost what you wanted. Okay. And the Yao Yao center depends on an orthomal basis. So it goes into this. So it goes into this proof method quite nicely, and you can do many things like what we get. Okay? But I think that's as far as I want to get in your summaries. What you can result that you can prove you mentioned several results that you already proved, but what is new? How many? Now you can. How many mountains? How many? So, just peaceful, right? The ones that I presented here, those two. But if you, for example, the talk from Pavle Blakovich on mass assignments, there are many results on mass assignments that you can get very cleanly with this without having to go through a lot of topological tools. And the results with Yuki Takahashi, they are related to ham sandwich cuts, but instead of having one hyperplane, you have two polar handplanes. Instead of having one hyperplane, you have two parallel hyperplanes, and you can do more measures. Something like that. So I know that. Yeah, so actually, for example, Shivaljevich and Vredchika, they used tools. They were, I think, very close to getting a proof exactly like this one because Fadel and Hossini, they proved this Porsukulan type. They proved this Porsukulam type thing for a Stefan manifolds, but with just the same dimension on top, and that's exactly what you need if you just want to prove the central transfer theorem. But they basically saw that they reuse the tools on the Grassmannian, and then they got a conclusion. Yeah, so I would encourage you to try this in any place where you can use Grassmannians in that way. Try to see if Stifl manifold simplifies. Try to see if stiff manifold simplifies. So, one of the things you gain from doing this is that you gain this action of Z2 to the K, which sort of was hidden when you factor into the Grassmannian thesis. Why don't you change the action there? You can go from one normal race to another one in photo rates. Yeah, so I think the key thing is the key thing was finding a function with an odd number of orbits that went to zero. If your new action still has a function that does this, then you're basically good to go and prove this results of that. The point is if you if you know at least one example, good examples, then you can prove for any other. For any other because we have a complete solum theorem has quite numerous combinatorial consequences. So do you think that you can get something interesting also in some other direction? Like I don't know, color recognizer graphs like the original. That's a good question. I don't have anything clever to say about it. But potentially, yes. But potentially, yes. But what all you need to really prove is that if you have one system of line, which means one line in every direction in Rd, then two of these systems share of line. When you have, instead of line hyperferences, the vorticulum, and you have all the all the variations in the is this related with your Related with your, can you prove it using these your ideas of different width and manifold? Yes, because in any direction you take the complement, you take the intersection of those things. And if you impose any basis, you will force this action when you take the dot products, as I mentioned when I had the columns here. So you recover exactly that. Very nice. So here you model the problem by DK and you map it and see if there is a zero, but you have a zero existence result. You have a problem which is modeled by the same, but you cannot use it because you exist a map to the sphere. Something that you try to prove that you modeled by the variance, but the map exists. I don't have one word on map exists, but when I Have one word of map XS, but when I try to construct functions with one zero, I can't get it to work nicely. So I do have things where things work almost exactly right, but it's a construction of a special function that fails by small details. Yeah. Thank you, speaker, again. I have to refresh tomorrow. I have a breakfast tomorrow, but putting the others.