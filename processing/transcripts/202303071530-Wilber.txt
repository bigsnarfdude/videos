Alright, let's start. So first up, we have Heather Wilder, who is at the University of Texas, at least for another few months, before she goes off to greater things. So fantastic to give this talk. Please go ahead. Yeah, thank you. So thank you so much to the organizers and to all of y'all on Zoom. Please interrupt me with questions if you have them. And I'm going to talk today about some really interesting rational approximation methods. Interesting rational approximation methods. These are called sometimes data-driven or adaptive approximation methods. And I think there's two ways in which these approximation methods are interesting in the context of the matrix computation. So the first way is that there's like fundamental core matrix computations that come up in the design of these methods. And so I'll try to highlight what those core computations are just to sort of like give you some interesting application areas. Interesting application areas where some things you're working on might have some context. And then, of course, you can think of it the other way around, right? If we can do these adaptive rational approximations, they can be useful for helping us design other matrix computation methods. So I think a reasonable place, okay, the clicker broke. That's okay. So I think a reasonable place to start is to ask the question. is to ask the question, when should I use rational approximation? When is this valuable? And if you ask people that who are sort of like broadly belong to computational math world, you'll usually get at least one of these two answers, if not both. So the first answer you hear, right, is that rationals appear in fundamental things we do in numerical linear algebra. So when I think about designing, you know, some algorithm, possibly an iterative algorithm, to do things like evaluate the function of a matrix or do Evaluate the function of a matrix or do eigen decompositions or solve one of your systems. Well, at the end of the day, the core things that I can do with a computer is I can do plus minus times, and if I'm lucky, sometimes I get to do invert or divide, right? And what kinds of functions can you build with plus minus times? You can build polynomials. And if you get the divide, then you can build rationals. So often when you peel away a lot of these numerical methods, which you find at the core is some rational approximation problem, a polynomial. Rational approximation problem or polynomial approximation problem, iterative methods can be thought of as building rational or polynomial approximations and then evaluating them in a matrix. So, this is one reason that we care. The other thing that people will tell you that's, you know, sort of maybe the thing rationals are most known for, right, is that they have excellent approximation power near singularities. So if you have some function that has a singularity and you want to resolve or you're interested in what's going on near or around that singularity, polynomials don't do that well. You need to reach for some nonlinear approximation. You need a reach for some non-linear approximation, and rationals are one choice. Okay, so what I want to focus on in this talk are not those two properties, we'll be using them, but there are many other sort of less heralded, interesting things that rational functions do. There are many benefits that I think are not as widely known. So, what I want to do in this talk is illustrate some of those benefits to you in the context of some software development that myself. Some software development that myself and my collaborators have been doing for single-tosset problems. So, just this first bullet point is really just a rehash of saying rationals are good at handling singularities. But the rest of the bullet points are interesting. They're different. One of the things that's interesting is that if you have a rational approximation to some function, you can then take the Fourier transform of that rational approximation and you will get a sparse representation of that. Of that function in frequency domains. You can have sparse representations in both domains. Rational functions can be used to filter noise in interesting ways. You can filter out Gaussian noise and still preserve some of these high-resolution, high-frequency features that you're interested in. So we'll talk about that. Rational functions have really good analytic continuation properties, and that makes them good for doing things like imputing missing data or extrapolation. And then there's like this interesting area where we're going to spend some time with, where We're going to spend some time with where you can use rational functions to understand and identify or locate singularities that maybe you didn't know where they are a priori. So I'm going to illustrate this with a little example. So what I've plotted here is a rational approximation, a trigonometric rational approximation, we'll get there, that I constructed using our software to a cubic spline. Okay? And so this cubic spline, it's constructed in pieces. It's constructed in pieces, and where the pieces are connected, those are called knots in the spine. And this function, if you look at it globally, right, it has like a couple continuous derivatives, and then it breaks down. The continuous derivatives break down. So there's weak singularities at these points. But if you're looking at it to the naked eye, you can't see these singularities, right? And so my question for you is how do you find them if you don't know their? How do you find them if you don't know they're there? Right? This weak kind of singularity. Could you find them? Is there a way to find them? And one way that you can do it is you can construct a rational approximation to the thing and look at what the poles do. The reason that this works is that the poles cluster in areas where the singularities exist. So what I'm going to do is like a small transformation because I like working on the unit circle. But all I'm going to do here is I'm going to map 0 to 1. 0 to 1, I'm going to wrap it around the unit circle. So the location of these nodes is now being mapped to these points on the unit circle. And then I'm going to plot the poles for this because it's real-valued. They come in conjugate pairs. And the ones with the negative real part correspond to poles inside the unit circle. And I'm going to plot those. And so you can see the poles line up really nicely with where those singularities are. So one way to Are. So, one way to find singularities is to build some approximation that has these global features that you can then use to explain them. So, and people use these kinds of things in practice. So, this is an example of an ECG signal. We reconstructed the signal using rational approximation from our software. And what people do with these signals is they take the poles, the patterns, the clustering patterns of the poles, and they train classifiers on those patterns. On those patterns, and then they use those classifiers to classify signals and tell you, oh, your heart's good, your heart's bad, whatever. I don't know what they do. But the point is, is there's like a lot of application areas where you run into this kind of situation where you've got some sort of underlying signal or function that you'd like to reconstruct. It has high-resolution information, right? So maybe like fast change points or sharp features. Sharp features that you'd like to preserve, right? So, polynomial methods are kind of out for you. And one way you can do it, and one way that people have done it in various applications, not all the ones listed here, but definitely just ECG one, is you can sit down and you can figure out, given the parameters of my problem, the kinds of signals I'm looking at, what are some like special collections of basis functions I can use to build a good non-linear approximation? And so those might. Approximation. And so those might be wavelets, RBFs, lines, they might be rational functions. They can be all kinds of things. But in order to do that, you have to know a priori what kind of signal you're working with. You have to know what kind of singularities you want to result, where they're going to appear, and with what frequency they appear. And a really active area that's become of interest to people is what happens if you don't know? What if the goal of your problem is to find out? Can you build a black box? You know, a black box, right, where you put in a bunch of samples because that's all you have from the signal. And those samples might be corrupt or, you know, missing or have various problems with them. And what that black box fits out is a reconstruction of your signal. And so what we wanted to do, so there's parallel work that's been going on very recently in the model order reduction community, but what we wanted to do is build a software package that allows you to do this sort of thing. That allows you to do this sort of thing and kind of put it to the test and see what's possible, right? So, to make our lives easier, to make the problem a little simpler, we assume that the signals that we're working with are band-limited, which means we're dealing with, you know, you can think of it as dealing with periodic functions. And the analog to rational functions in the periodic setting is trigonometric rational functions. But everything I'm talking about. Functions. But everything I'm talking about, you can extend it to non-periodic settings in various ways. So, our goal was to develop some software tools that allow you to work adaptively with rational approximations to these kinds of functions, these periodic functions. And we had a wish list of what we would like to be possible. We wanted the rational approximations to be near optimal. And what I mean by that is that given some accuracy, you want to use as few poles as possible, or close to as few poles as possible, to represent that thing. To represent that thing, we want them to be data-driven. No need for the user to input tuning parameters or choose basis functions or know something about singularity beforehand. We felt it was important to see what kind of conditions we could handle various problems with data. And then we wanted to develop a basic tool set. What are some algebraic operations that, given these rational representations, we can wrap up and do efficiently to present a software package that we're To present a software package that works pretty adaptively and automatically for users, right? And when we surveyed the landscape to see what was out there, we found an interesting paradigm. So there's two data-driven rational approximation methods that work without tuning parameters and do a good job, a near-optimal rational approximation. And what we found was interesting is one of them, the AAA algorithm, is one that you apply directly to the samples on your sigma. Directly to the samples on your sigma. So we think about applying that in the time domain. The other one is an old method, it's called regularized crony's method. And the idea here is you take the Fourier transform or you take the Fourier coefficients of the signal you're trying to fit a rational function to, and you find a representation in the Fourier domain of the Fourier transform of a rational function. And then that gives you a rational in the time domain. So what we found interesting about this is that What we found interesting about this is that it kind of parallels what you're often doing in signal processing, which is moving between the time and frequency domain. And at the same time, if we were able to take these things, which have different advantages and disadvantages, and put them together and build a bridge that makes it easy to walk from one representation to another in a stable way, then we could make everything on our wish list work out. So, our goal was to figure this out and build this bridge, but also try to. This out and build this bridge, but also try to understand the ways in which these methods can help us with these kinds of problems. Right? So, what I want to do is show you what you can do with some of these methods, both independently, and then if we have time, I'll talk about this part, which is like, you know, the work we did building this bridge, but at least show you kind of what happens when you can, what can happen when you combine them. So, okay. Okay. Sorry, I'm like way moving way too slowly. So I want to just show you the model that we build. So we want to build a rational, a trigonometric rational function, which you can think of as a quotient of two trigonometric polynomials. Okay, and we say that this trigonometric rational function has two m simple poles. We want it to be real-valued. It to be real-valued, so the poles appear in conjugate pairs. If we start in Fourier space, we apply a method called Crony's method. And the key thing that we're trying to do, the key thing we're taking advantage of, is that if you take a rational function and you look at its Fourier coefficients, that's these guys here, those Fourier coefficients satisfy a length and difference equation. And you can use that to write down a sum of complex exponentials. Of complex exponentials. So here omega j is a complex valued weight and lambda j is some complex exponential term, where when you plug in k, the index of the Fourier coefficient, what it spits out is the Fourier coefficient, the k Fourier coefficient of the rational. And this is exact. In the 1700s, Crony developed a method, Crony's method, that solves the problem exactly with only two n samples. And then there's a regularization that you can apply to solve this problem in a numerically stable way, because this is. In a numerically stable way, because this is numerically not stable. I won't talk about this, I don't have time, but the key algorithmic thing you're doing in this problem is you have some Henkel matrix and you want to approximate that Henkel matrix with a low-rank Henkel matrix. So you're trying to find a low-rank approximation that preserves structure. So, using that, one of the cool things, using that, you can get a representation in Fourier space of this. In Fourier space of the Fourier coefficients of this function in a really efficient way. Another thing that's really cool about this method is you have automatic denoising. So one of the things that you can show is that if you've got some, this is an example where we're extracting pulses from a Pacific Blue Well song. So I guess people are interested in the pulses. You can see it's very noisy. And you can apply this thing automatically. There's an automatic way to detect. There's an automatic way to detect where the denoising level should be. And you can just pull out the pulses. So it keeps the high-frequency information in those pulses, but it erases the Gaussian noise. And this has to do with finite differences and null spaces of Hengel matrices, things like that. Another advantage is that there's a lot of theoretical results that show you how to combine these things. So if you have two exponential sums and you add them together, at most you have n plus l terms. At most, you have n plus l terms in the sum, but you can often reduce that, you can compress it, and there's some beautiful theory as to how you can do that that we take advantage of. And there are some other things that just come naturally from working in Fourier space, like you can evaluate derivatives quickly, things like that. All right, so I want to turn. Am I out of time already? But maybe we can do questions later, so if you go another three minutes. Okay. Okay, so I'll really quickly mention in Really quickly mention: in the time domain, we use the AAA method. So, the AAA method is an algorithm, we use a variation of it, but at the end of the day, it's very similar to the AAA method, which is a method that was introduced by Tripeta, Nakats, Pasa, and Olivier Set. And the key idea here is to build a special type of rational interpolation. And this rational interpolation, you can write it down. It looks like this. The parameters you need to find are these nodes and weights. Parameters you need to find are these nodes and weights, okay? And there's, I won't go into the details about this algorithm. If you're interested, I'm happy to talk about it. But one of the things I'll show you is that one of the nice things about this is that it has really nice L1 error minimization, and you can use that to do things like recover big chunks where your data has been erased. So this is an example of a function that has these steep wells, and we just erased pieces of the data. And we just erased pieces of the data. And you can see that if you apply AAA to recover this thing, the error is pretty good. It doesn't jump too terribly high, and it stays very localized in those missing pieces. So this is a nice advantage to using these kinds of methods. And then let me just summarize, even though we didn't get very far, let me just summarize. The key thing that we did is we figured out a stable way to compute Fourier and To compute Fourier and inverse Fourier transforms that allow you to move between these representations. The representations have different strengths, right? So in the barycentric form, in addition to sort of the strength you get from the approximation method, there are all kinds of efficient ways that you can compute with things in this form, right? And then the same is true with exponential sums. Because we have these stable transforms, we can move between them. So we can build computational objects, move between them, and do all kinds of interesting computations. And do all kinds of interesting computations with them. And then there are all kinds of advantages you have to combine all these things as well. So if you're interested in that, I would love to talk about it. But I will just stop because it is a workshop. I'll mention there's a lot of open directions in these data-driven adaptive methods that I think are interesting. So a lot of people have asked me about: you know, can you run something like this AAA, which is an interpolation method? Which is an interpolation method, and construct rational approximations that are guaranteed to always be possible at any certain interval. You have good representations in time domain and frequency domain. What can you do in terms of things like time frequency analysis? Can you use that to build multi-scale tools or mixed models? And then I think there's a lot of interesting questions around structured low-rank approximations. And then I'll leave this point. But thank you. But um thank you. So we can take questions while the next speaker sets up. We're done. Yes. So if you have a representation in both domains, presumably you have a norm in both domains to measure how far away two things are. Does your work give a kind of natural norm that takes both of those to norm that takes both of those two norms? I don't know if that question makes sense, but not yeah, so I think the natural norm to work in for the setting that we're working in in both domains is L2. So L2 over the data. So is the L2 norm, so Fourier transforms preserve L2 norms. So then it's just the same over the data. Right, right, yeah, yeah. So you mentioned in the conclusions this structure are possibility. Conclusions with structured approximation. There was a recent result, I think, in the TCS community, by Dr. Kartlov for no rank turtless approximation. You can get the same accuracy, but you need the poly K rank. You need to increase it. So you have a good rank approximation for turtledist matrix to preserve the structure. You have to use poly K rank. But you have to use poly K rank. But I don't know whether we don't have a lower bound, it's the upper bound. So for Hankel it's easier because you have this connection progression. It's easier because you have this connection to rational functions in this way. Yeah, Heinkel, there's like a direct connection with the symbol. With tablets, there's a connection as well, but it's not as straightforward. Anything else?