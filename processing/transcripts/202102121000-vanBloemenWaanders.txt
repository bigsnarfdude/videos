The air national lab is going to talk about hyper differential sensitivity analysis. And yeah, Bart, take it away. And we're looking forward to your talk. Thanks, Lauris. Appreciate it. And thanks to all the organizers for the invitation and putting this together. It's been very interesting. Let me just first say that this talk will be slightly different from talks we've seen this week in that our technology That our technology is entirely deterministic. And I'll explain why that is. I'll just hint at it. It's for computational reasons. But before I start, I will also, let me just acknowledge Joey Hart. Really, the energy behind all this and the brains and the creativity comes from Joey. Lisa and Julie have been in. Lisa and Julie have been instrumental in the aerospace application I'll show at the very end. Okay, so a lot of this work is motivated in our desire to tackle complex applications. And here are some examples from engineering, additive manufacturing, and our fusion experimental facility called ZPench. But also in the geoscience area of climate, of course. Area of climate, of course, and seismic activities. And we face complicated decision-making, and I define decision-making as inverse problems, control, and design problems. And the underlying feature that makes this complicated, of course, is a range of multiphysics at multiple spatial and temporal scales, in addition to a large number of optimization variables. And on top of that, we face And on top of that, we face many sources of uncertainty that have to do with model parameters in the dynamics or unknown boundary conditions, perhaps bad data, what have you. There's just a whole host of uncertainties that arise in these applications that have yet to be tackled. So our thrust has been to leverage our rather efficient Rather, efficient computational methods impeding strain optimization and tackle uncertainties in an equally efficient way. So as I said, not in a stochastic way, but more in a deterministic way. And so we don't try to quantify the uncertainties, but we try to characterize them in the context of the optimization problem. And this is how this term hyperdifferential sensitivity analysis came about. Analysis came about. And it's not a new idea. It's been around in post-optimality analysis and linear programming, for instance. Also, some work done by Fiaco and Gami in 62 and Bonas and Shapiro later on in 98 for nonlinear problems. And then a key development was done by Roland Greasy, now known as Roland Herzog. And we collaborated on our first paper in this. On our first paper in this area and coined it hyper-differential sensitivity analysis. The hyper means hyperparameters in the optimization formulation. Now, Roland devoted the idea to stability of systems, where the sensitivity of the post-optimality or the post-optimality sensitivities were used to determine the stability of the system of the control problem. We extended that to. We extended that to handle uncertainty and parameters to be able to prioritize the uncertainty, a variety of different conditions, perhaps material properties in the PDEs or some boundary conditions that are unknown. We wanted to know what was the variation, what was the effect of those uncertainties on the control solution. So let me. So, let me explain now mathematically what is going on here and how we solve these problems. So, this is a fairly standard P constraint optimization formulation. What's different here is that in addition to the state and the optimization variables, we also have a theta parameter that is the auxiliary or the uncertain parameters that might live either in data or in C here, set of nonlinear equations. Here and set of nonlinear equations. But everything is fairly standard up to this point, except for that data parameter. So the standard solution techniques are used here. We form a Lagrangian, take variations, we get a solution. But the implicit function theorem tells us that there is a function, differentiable function operator f that maps the neighborhood of phi naught to neighborhood. Of phi naught to the neighborhood of that of the solution, of the original solution, such that there is a derivative that's set to zero for any theta, a theta in the neighborhood of theta naught. And then through some chain rule arguments, you have a derivative of f prime that can that equals minus k inverse b, where k is the Hessian of L and B is the Frechet derivative of L prime with. the Frechet derivative of L prime with respect to theta, both evaluated at the solution. So this is in a nutshell what HTSA is all about. As I said, we extended this to uncertain parameters, and our desire was to be able to compare the impact of various parameters on equal footing. And so we And so we added the scaling and coined this S as a sensitivity index. And this kind of allows us to compare different parameters that might be different in magnitude or just different types altogether. For instance, boundary conditions versus a diffusion coefficient. What is the effect of that in the context of the solution, of the optimization solution? But the goal is, of course, to do this efficiently. So you can imagine. Efficiently. So you can imagine that the computation behind S is that we have to solve a PD-constrained optimization problem. Of course, that optimization problem is constrained by a set of PDEs that rely on finite element discrimination and linear solvers and non-linear solvers and linear solibs and so on. And then on top of that, we want to calculate this quantity for each uncertainty parameter. And if you have many of them, say thousands. And if you have many of them, say thousands, this is intractable computationally. So we leverage, we assume there is low-rank structure in the uncertainty space, and thereby we can exploit the singular value decomposition of those operators. And so that's what that formula basically says is the operators can be decomposed and the singular values will provide. The singular values will provide you with the most energetic mode of those operators, which is the sensitivity. Now, you can reformulate that as an eigenvalue problem. And computationally, we leverage randomized methods, randomized eigenvalue solvers to drive at these values. And the main reason is that we don't really need the accuracy of these singular values. singular values, we need a relative comparison between them. So we can afford some inaccuracy in those calculations, but it's nearly embarrassingly parallel. So it's easy to implement. And on top of the parallelism that we already have in our p-in strain optimization calculations in linear algebra, we now have a second level of parallelism to calculate the sensitivity index, provided there's a low-rank structure in the space. Low-ranked structure in the space. Now, this is a location at theta naught. So, have a third layer of parallelism where we randomly sample to kind of extend beyond that neighborhood. And we can't call it global sensitivities, but what we've seen so far is that when we do this sampling, it seems to be fairly consistent. So, this work started. This work started in the PDE context, and I wanted to just give you a few quick examples before I get to my aerospace example. And the first example is a control of a thermal fluid problem where you inject a fluid in the top boundary. It's denoted gamma i here, and the goal is to heat the boundaries of this. The goal is to heat the boundaries of this vessel so that you minimize the vorticity near the substrate on the bottom, gamma B. And the idea is that this is you might have like a C V D reactor, a chemical vapor deposition reactor where you want to keep the substrate deposition as even, as uniform as possible. And so this is a very high level prototype of that. Prototype of that. And when you solve the optimal control problem, you see that you can control the boundaries sufficiently to minimize those fronticities. The question we ask, though, is that if you have uncertainties in the boundaries, and we've parametrized the boundaries with uncertainties, that the left, the right, and the bottom are instrumented with certain Certain sinusoidal functions that represent the uncertainty on those boundaries. We want to know what those uncertainties, how those uncertainties impact the solution of this. In addition, we want to know if major fluid flow parameters like Grassoff from Prendel and Reynolds number have an impact on the solution of the optimization problem. And that's what the right figure shows: it's the sensitivities plot and Sensitivities plotted and sampled by denoted in color for the different parameters. And what's striking is that there are two parameters here that are most influential in the optimization solution. And it turns out that it's the first and second coefficient of the sinusoidal expansion of the uncertainty on the bottom boundary, which perhaps makes some sense. Perhaps to make some sense. But a priori, I probably would have guessed that the fluid flow parameters were extremely important: Reynolds number, Prandtl, and Gressov, but it turned out to be the least influential in this setting. And so this kind of highlights what hyperdifferential sensitivity analysis does for you. It gives you the post-optimality sensitivities versus the standard sensitivity parameter where you... Standard sensitivity parameter where you perturb some forward prediction for a point of interest, and the intuition is different. So that's a control problem. We also applied this in the inverse setting. And in this case, the goal is to recover a log permeability field in the subsurface. And we do that by combining convection diffusion. Combining convection diffusion and Darcy equation, and we inject concentration, some chemical into the subsurface at sparse locations and then extract concentrations at certain sensor points in addition to pressure measurements. This is a common practice in the oil field in groundwater. You use kind of a chemical that's called a tracer test, and it enhances. And it enhances the information content in an attempt to recover the permeability field. And so here we show that with sparse data and with noise and initial guess that's far away, we can recover the true permeability field. Not exactly as you expect, because it's an ill-posed problem and you have noise in the data, and so we can recover it. We can recover it fairly well. But the question is: what is the effect of remaining uncertain parameters? And in this case, we're looking at the uncertainty in the pressure and the concentration sensors in addition to the source term, in addition to diffusion coefficient and boundary conditions. So that's a pretty broad question, right? Right, that is not easily characterized with other methods and certainly not through standard sensitivity analysis. So, in this case, we can calculate the sensitivity index across all these different parameters and compare them, again, on an equal footing. And it turns out the left boundary is probably most important in this inversion problem now. Now, this is a time-dependent problem as opposed to the first one was a steady state. So, in time now, we can also evaluate the effect of the sensitivities for different parameters. So, in the bottom figure, I show a time slice of concentration. That's the bright color. And the sensor points are mapped onto this domain. The colors indicate the sensitivities. The colors indicate the sensitivity. So, what you can see is that the sensitivity for the concentration sensors changes over time. And it's not surprising, but it's still not intuitive, entirely intuitive, that that would happen. The right figure is a rectangle representing each sensor point in space and the red line. And the red line shows the change in the sensitivity index over time. So you can quickly see that the dynamics really happens on the left side of the domain in the center. Again, makes sense. That's where the action happens and the flow moves from right to left. So this is just a quick snapshot of some examples. We have many more examples that have provided us insight in these complex systems. Complex systems. The question remains, however, what exactly do you do with these sensitivities? So you have this prioritization, and then what, right? So one possibility in the context of the thermal fluid problem might be that the parameterization might be wrong. So that's interesting. And we might change that parameterization if we determine through expert knowledge that that. Through expert knowledge, that boundary condition is correct and that the representation is correct, then perhaps we need to run experiments to better characterize that boundary, make sure that we minimize the uncertainty in that boundary condition. And perhaps that then can be used for a robust type optimization setting using risk averse methods or min-max systems. Methods or min-max systems formulations and the like. And the same is true for the inverse problem. In this case, you might want to place additional sensors and you might combine with traditional OED methods to determine where you might place the sensors and which type. Now, traditional OED methods are not really set up to evaluate different sensor types. Different sensor types. And so this kind of analysis might be a nice support for OED methods. Okay, so now let's switch to aerospace vehicles. And this is slightly different in that, well, first, this is a set of ODEs to do trajectory planning. But what's common to the previous examples is that To the previous examples, have a large number of optimization variables as well as many uncertainties. The uncertainties come in the form of the surrounding environment, perhaps unknown density in the atmosphere. We might have a moving target or a target that changes. And there are lots of uncertainties in the aerodynamics. For instance, at certain Mach numbers, you might experience thermal ablation. Experience thermal ablation that changes the profile of the vehicle, which in then turn changes the aerodynamics. Then, there is also the lack of sensing for these systems, and that requires then that these systems are handled in sort of an autonomous way. So, our goal here is to come up with a way to make this more robust. Make this more robust. And the way the trajectory planning process works is that you solve offline an open loop problem. And once you have that solution, you let the people fly and you rely on an onboard autopilot or a feedback controller like A feedback controller like LQR or MREC or MPC to keep the vehicle on track of your offline reference. But what happens though is that the model that you use for the aerodynamics is not close enough to reality. There will be a discrepancy there. And the issue is that we're asking too much of the feedback controller. Too much of the feedback controller. And that is at the heart of the implementation I'm going to show you next: is that we want to use sensitivity analysis, post-optimality sensitivity analysis, to help determine how to sample the higher fidelity model, which might be data from flights, or it might be wind tunnels, or it might be high-fidelity CFD. CFD to kind of augment the optimal control problem that you solve offline. Okay, so the open loop formulation for trajectory planning, the standard notation is that you have a running or a tracking objective function and a final destination objective function. We are not going to. We are not going to, in our examples, look at the final destination. It's mostly going to be a tracking formulation, but I thought I'd be complete here. And that's constrained by the set of ODEs, initial condition, and then various path constraints. The path constraints have to do with a variety of limitations on the vehicle itself, and I'll explain those later. What's important in this scenario is that we have implemented a pseudo-spectral discretization that consists of Lagrange polynomials and Gauss-Labato quadrature in addition to time adaptivity. And this is a field that has been developed by Ross and Farou in the 2002 era and further developed through Anil Rao in 2014. With this work, With this work on time activity. So, there's a lot of technology that goes on that's been developed. This is not the only discretization one can consider, but it seems to be the most advanced. And the reason for the pseudo-spectral terminology here is that it achieves near-spectral convergence as a result of the Lagrange polynomials. It's not quite spectral. So, what's different about this formulation, of course, is that we want to evaluate the uncertainty. The uncertainty comes in the form of G. So G represents the aerodynamic characteristics of a vehicle. And there are two forms. The first is a true model. And I apologize for any background noise. Sometimes my dogs get a little. Background noise. Sometimes my dogs get a little excited, but the G star represents the true model, and this is what the vehicle experiences in real life. Then there is G bar, this approximation of G star, but that's the model that we end up using in the offline computation to develop our reference tracking trajectory. So, OL, the The optimal control formulation we call OL as the open loop or reference solution. And then U-bar comes out of that. That's the controller that's the solution of OL. Oh, sorry. Sorry, come back here. So just to put up a cartoon of what I mean by G star and G bar, and I'll use this example later in one of the navigation examples. one of the navigation examples but but here g star is some rep some functional represent representation a function of of x1 in this case and it it is it's the truth we we we basically assume that's the truth model and then g bar is what we use in this um in this calculation in the open loop optimal control problem and you can see that g bar will have a big effect right it the reference solution will The reference solution will not match what the vehicle will actually do in real life. And so, on top of that, then what we want is we want to make use of the closed loop trajectory, meaning that we're going to rely on some kind of a feedback controller to keep us close to the reference trajectory. And that's what this cartoon basically shows. This is a trajectory from This is a trajectory from zero to 1.2, roughly. The reference solution is in blue. The open loop is in red. And that is what the vehicle sees. And the difference between the reference and the open loop is the fact that we cannot represent G-star properly. Now, the hope is that the closed loop can come close, but it comes closer, but not quite close enough. Close enough. So, just a table of various notations that I'll use later. It's a reference solution, open loop, and a closed loop, and that's sort of represented here, and the various parameters that we'll be talking about. Okay, so our goal then is to come up with a formulation that sort of is Sensitive to this idea of coming with a reference trajectory that avoids saturating the autopilot or the feedback controller, which operates in a linear regime. So our goal is now to reduce that uncertainty and the uncertainty is manifested in the function g. In the function g, where we, in the offline computation, it's an approximation, and in real life, it's this g star, this truth model. And our goal is to come up with a way to sample the truth model in an intelligent and strategic way. Because that truth model might be an actual experiment on a Experiment on a vehicle, or it might be a wind tunnel test, or it might be high-fidelity CFD. And in any of those three cases, it's going to be expensive. So we want to be judicial about it. We might only have a small budget to do this. So we are going to formulate an additional problem that is encapsulated. Encapsulated in this variable e and its deviation between the state and the reference trajectory x-bar. So if we take a derivative through e, we get the ODE that you see there in the top, the constraint. And v is now the augmentation of the controller. Now in the previous slide, I showed u plus delta u as a function of x. Here we're going to just look at an augmentation in time. An augmentation in time. And that's why the difference in notation. So V now is this ideal closed-loop controller that is being solved as the offline trajectory planning process. And again, our goal now is to try to come up with a way to sample the truth model. So again, similar to what I showed earlier, there's Similar to what I showed earlier, there is a derivative of f that equal to a Hessian, an inverse of a Hessian times B. In this case, the Hessian is again the original KKT operator. B is a cross term now between V and G, a second derivative of G with respect to V and G, both evaluate at the nominal V and G. The nominal VNG. The sensitivity index is slightly different. It's scaled with respect to the nominal controller, U-bar, and it's perturbed in the direction of G. G again is the aerodynamic model. So the sense index is now the relative change of the optimal controller V augmenting U bar when G is perturbed. When G is perturbed in the direction of delta G. So the goal here is to reduce the effort on the feedback controller. And this slide kind of repeats what I showed earlier, but it shows you how we derived at the sense of the At the Sensity Index, and how we got to the different scaling idea. And it comes from the fact that F at G star, and that's a little typo that should not be scaled by U star because we'll never have the true controller. It's U bar actually. So that's a typo. But it's the ratio of what the vehicle experiences versus the Vehicle experiences versus the not the controller. And so it implies that delta G is a direction in which S is the largest, where errors in G will require the greatest feedback effort. That's what this sensitivity represents. And ideally, then if we can sample at those points to G star, we could reduce some of that uncertainty. We could reduce some of that uncertainty. Now, if we do do that blindly, if we just blindly pick the highest sensitivity indices and sample G star there, and we have a budget, say, of a few points, it's going to gravitate entirely to that point, and that's not useful. What we really have to do is we have to be sensitive to the correlation length of the parameters that we're interested in, and so we augment. That we're interested in. And so we augment our sensitive index with two components. One is we augmented with this estimation error that can be provided by the user, or we could at least we could think about the estimation error, this matrix that provides you with a weighting of how the estimation error is incorporated. Is incorporated in the sense to the index. For example, we might not want to run experiments at really low Mach numbers. That might not be very insightful. We might place more weight. We might increase the weight of this when we have flight conditions where thermal ablation or chemistry comes into play, and that might be more insightful. So, there's more sensitivity there. B, B is another matrix that's required, and you can think of sort of as a information gain. B is an information gain where R is the relative reduction. Again, as specified by the user, somehow we have to embed a correlation length into this into the sensitivity index for the sampling to eventually work. Sampling to eventually work. And W here is a binary choice of where to choose these experiments. So an example form of R could be this exponential that depends on alpha and the difference of x1, this variable x1, at different nodes in the sampling space. It's a form, but what's form, but what's nice about this is that we can inject information from that's that's depend that's relevant to the particular application that we're interested in. In this case, R works for the numerical example that I'll show you, the two numerical examples. And some work went into how to actually establish alpha i, having to do with the gradient and so on, but that's all beyond the scope of this. That's a little beyond the scope of this talk. All right, so as I said, we want to sample G star to try to reduce that uncertainty in the offline computation. And we're going to appeal here to optimal experimental design. And from optimal experimental design, we know that we can consider formulations like A optimality, where you take the trace of the covariance, but the covariance is the inverse of the Hessian related to the original optimization problem. Optimization problem. So, in that same spirit, here we're trying to do the same thing. We want to minimize the trace of the sensitivities constrained by some budget, a number of experiments that we can run. And then we do some algebraic manipulation to get it in this particular form, and that's kind of nice because you can see now that the sensitivity is scaled by the uncertainty level E as well as the expected information gain B. The expected information gain B. And I'm using the expected information gain, you know, with in quotations simply because, you know, that comes from information theory that's typically stochastic here. We're using it more in the terms of reduction and error. And then we make a quick, we do a little manipulation here. I don't know if you caught the plus function in the original, but there's some slag variables that we introduced so that we can put. Variables that we introduced so that we can put it in a convenient form to solve a mixed energy quadratic when you program with Jorobium. Okay, so I'm going to go into some examples that demonstrate this procedure, but maybe I can first highlight the overall process, the overall algorithm to generate this. So, first we solve the trajectory planning problem to generate a reference solution x-bar, u-bar, and t-bar. X bar, U bar, and T bar. Now, actually, I should mention, I have not mentioned T bar in that most of these optimal control problems don't fix final time. T becomes an additional optimization variable. And so that's the reason for the adaptivity. It's part of the reasons why we have pseudo-spectral discretizations. So anyways, you solve the trajectory planning. The trajectory planning, you get U-bar, and then we go through our hyper-differential sensitivity analysis calculation. In this case, the coupling of the open loop and closed loop ideas coupled with the with the sensitivity index indices. And we go through an optimal experimental design to sample G star that depends. That depends on the HDSA index, the sensitivity index. And then we evaluate G star at the design points and use the data to fit G tilde. Now, I haven't said anything about G tilde, but G tilde now is a modification of G bar. G bar, if you remember, it was the approximation to G star, the truth model. G bar is now updated, and that is what we're going to use to try to improve. What we're going to use to try to improve on the offline trajectory plan. So then we solve the trajectory planning problem with G tilde and get an improved reference solution. And then we're going to see how the feedback controller is able to track x tilde, which is now the new trajectory path. Okay, so let's start with a simple numerical example called Zermelo. Simple numerical example called Zermelo. This is a very simple navigation problem where you want to maximize the distance a vessel travels in the vertical direction. And the ODE for x1, x2 are trigonometric functions. You can think of it as the mapping of the force on this vehicle in relation to the current. And G represents the. And G represents this unknown current. So we have a stream, say, for example, where we don't really know the current. We have an approximation. We have some initial conditions. The initial conditions say that the vessel starts at the bank, it moves out into the stream, and then the final condition says we want it to arrive back at the bank, but we want to maximize that distance. Here's what happens. Here's what happens. So I showed this approximation g star and g bar before. This is the functional representation of some complicated sinusoidal exponential concatenation. And we've perturbed G bar to create G star, of course. But the point is, there's a difference, and we want to reconcile that difference. Want to reconcile that difference. So, what happens is the closed loop, the feedback controller, the LQR in this case, cannot achieve the reference tracking path. And so that's problematic. It might seem fairly close, but this is problematic introductory analysis, the discrepancy between the two. And I should mention LQR. I should mention LQR. We chose LQR because it's fairly robust. It uses this Riccati equation, algebraic Riccati equation, to achieve the stability of the system. But it can be replaced by other methods. But that's not the point. What's happening here is that the LQR feedback controller is being saturated. We can't really achieve the reference solution. Can't really achieve the reference solution. If we go through our HDSA analysis, what we show now is that the sensitivity has these two peaks for X1. And if we map the sensitivity on the phase space between X1 and X2 and use the colors to show the magnitude, you can see that the sensitivity happens when it wants to accelerate back to the bank. To the bank very fast, and you can see the velocity of X2 being very high there. It has a minimum there at 0.85 roughly. And if you remember the original G, it's pretty steep in magnitude. So it's having a hard time with it, but that's where the sensitivity is. So it means that this is where we want to reduce the error potentially. And when we run through our optimal experimental design routine, Optimal experimental design routine. The left figure. I'm sorry. There. Sorry. So the left figure is a little bit of a complicated figure, but what it shows is the original G star as in red, and it shows the approximation G bar in a blue dotted line. And the dots show the sampling that we And the dots show the sampling that we choose. This sampling is near those peaks, but it's not quite near those peaks because of the selection of R, the information gain that we've specified in terms of that exponential with the correlation length. But the interesting thing is that when we do a regression between g star and g bar and get g tilde, the dotted line, the trajectory now is much better. That's the middle. Now is much better. That's the middle graph. You can see that the reference is much closer to the open loop, and the closed loop is quite good at tracking the reference solution. Now, the question is, is this optimal? And the answer is probably no, but it is pretty darn good. So the right figure shows that a random sampling of different designs, different sampling points on G star, and you can see. G star, and you can see that our dot is the best relative tracking error of many of the samples. But there are designs that are a little better. We could afford to do this, of course, because the computational time is very low. And normally, in an example like the next one, we cannot afford to do the sampling across all the input space that's intractable. So we have to. That's intractable. So we have to rely on the OED, HDSA-endowed OED process. So the next thing I want to show is a more complex situation where we want to fly an aerospace vehicle from point A to point B, the terminal condition, and we have to negotiate all the forces and aerodynamic coefficients and Dynamic coefficients and the different coordinate systems that come into play. The ODE, the optimization problem with the ODE constraints are much more complicated now, and largely because we have to map the forces on a vehicle properly. So there's a coordinate system that's relevant to the vehicle, but then there's also conditions that come from the velocity trajectory with respect to. Trajectory with respect to the Earth, in this case, a non-rotating, rotating spherical planet situation. So you have two coordinates you have to negotiate and map the forces appropriately back to the vehicle. And that's where all the trigonometry comes in. And that's what you see here. So here we have a six degree of freedom state space. And to the space space, you have three degrees of freedom to control this vehicle through an angle of attack. Vehicle through an angle of attack, a side-slip angle, and a roll angle. So that's the problem. And we're going to use OED again for the same reason in that in the offline computation, we really don't know much, we don't know enough about the aerodynamics. And we want to sample high-fidelity CFD wind tunnels for actual different flight coefficients to augment the system to get a better reference. To get a better reference trajectory that avoids saturating the feedback controller. So, this is the formulation, complete formulation. And what I've added here are the path constraints. There is a heating rate limitation on the system. There is a dynamic pressure constraint, Q max. There is a load factor that comes from the forces from lift and drag that needs to be bounded. And then we impose. bounded and then we impose also a no-fly zone that's represented by this by x minus cx and y minus cy then we have bounds on the three controllers a little more inside in in the aerodynamics we have a drag lift and side force that we have to calculate on the on the vehicle that comes from uh normal Normal, actual, and yaw forces, again, using trigonometry to try to map that from a coordinate system to the vehicle. And in those forces, the normal, the actual Yaw forces, we have aerodynamic coefficients C N, C A, C, Y. In this example, these come from a real aerodynamic system that have been mapped to a surrogate, a cubic polynomial surrogate. A cubic polynomial circuit for each of these, and that represents g bar, and then g star is a perturbation of that. The row here is to emulate the atmospheric density. That's actually a very important component. And as I said, the aerodynamic coefficients are now represented as the uncertainty parameters as g, as the function g. So, here's what happens. So here's what happens. The reference solution is in blue again. It's this cylinder in the middle of the picture that is a no-fly zone. And the open loop is now the controller that is exposed to G star, the truth, the real environment. And what you can see is that when it hits enough density, When it hits enough density, enough atmosphere, the vehicle tries to increase its lift and it deviates significantly from this reference trajectory. The open loop can come back to the reference, but not quite good enough. So the bottom three is the planes in X, Y, Y, Z, and X, X, D. Okay, more importantly, the controllers go above and beyond the Go above and beyond the bounds, and that's not good because it either causes major aerodynamic problems or causes failure. So, this is what you saw in the controllers, the angle of attack is exceeded on the negative side, and then the side slip angle exceeds the upper bound. And so, this is not a good situation. What's interesting is that this provides us now insight of what's happening. Provides us now insight of what's happening with the normal actual Nya forces. And this plot is a bit detailed, but it shows that the largest sensitivity happens on the trajectory path where big changes are happening. And what I've superimposed here are plots, our dots in Cyan. These are the new sampling points to try to sample. To try to sample from wind tunnels or high CFD to try to reduce that uncertainty. But this is very insightful, not intuitive at all. And we're using the sensitivity now to calculate a new controller. And this is the new reference solution that now the open closed loop is better in tracking. You can see Tracking, you can see there's some improvement there on this figure, but really, what's most important here is this controller comparison plot where the original controller I showed earlier, and now it's much improved. Now we're staying within the bounds. The LQR controller can maintain the reference, which is the same as the open loop. All right, so that concludes my presentation. This kind of summarizes the thing. This kind of summarizes the things that I discussed, and I'll just mention the next step for us is to go actual sample Hyphid L E C of D. And that's a very large in-scope endeavor, but that's where we'd like to go. And I'll just leave you with some references on HDSA and a paper that will hopefully be in archive in the next couple of weeks that discuss a lot of the aerodynamic. That discuss a lot of the aerodynamic vehicle problems. Thank you. Well, thank you, Bart. That was an awesome talk. I'm sure there are a few questions. Thomas? Yeah, I have a question. Hi, Bart. Thanks for the thought-provoking talk. So, I mean, my first question would be: you base a lot of the sensitivity analysis on the assumption that you can apply the implicit function theorem to the first order condition. Function theorem to the first order conditions of the optimal control problem, which is fine if you don't have any quality constraints. But in the end, you did have any quality constraints. So, I mean, you know, are you sure that the sensitivity analysis still technically works for those inequality constraint problems? Because the only way that you would still have a derivative of the implicit function is if you have strict complementarity everywhere for every element in the parameter space and a strong second order condition everywhere in the parameter space. Second order condition everywhere in the parameter space. You're absolutely right. This is still an open question. We have some ideas how to handle that, but it's an open question. So admittedly, we had to make sure that we never hit those bounds. Okay, so they're never active. Never active, right? And so that's what I showed in the last plot: we don't hit those bounds. The controllers never hit those bounds. So I mean, out of curiosity, did you try and see? So I mean out of curiosity, did you try and see what happens if it does hit the bounds? Does the sensitivity analysis still give you reasonable statements? Or is it then no longer, or does it no longer appear to be giving you the right answer? Yeah, mathematically, right? When it hits those bounds, you're no longer differentiable. So what is it? Yeah, I know mathematically, but I mean, like, did you try any experiments to see what happens if it hits the bounds? Like, what happens to the sensitivity analysis? No, it's the next step, actually. Oh, okay. Step actually. We're about to try these things. Absolutely. Very good question. May I also ask a question? Sure. Okay. Can you hear me? Okay. Okay. Thank you, Bart, for a very interesting talk. And I was just wondering, because we're also doing which we call dual control. And the idea is that we also solve actually like an NMPC and the control, so feedback control and the control. The control and the control is taken into account not only optimization, but also the reduction in uncertainty in parameters. So, but based on this, on the trace of the covariance matrix of the unknown parameters and so on. So, I found it's actually very similar ideas. But what we also have noticed is that sometimes the capacity of the control is not enough. Of the control is not enough somehow to do both, to perform actually both aims. So, to optimize and also at the same time to reduce uncertainties. That sometimes all capacity are put on the reduction on the uncertainties, and there is no strength for optimization. And then actually, the control does not make sense. I mean, for the original problem, because at the end, you don't want to. The end, you don't want to identify your aerodynamic parameters, but you want actually to put your space shuttle to land it safely somehow. And did you see this? Can you comment on this or something? It's very interesting. We are working on another implementation where we use Use a so-called desensitized control, which kind of reminds me of your implementation. And we can talk offline. I'd be very interested to see some of your work and compare that to the desensitized control approach. We just submitted a paper in a conference, The Big Sky. Okay. Yeah, we'd be interested just to compare the approaches. We should definitely talk, absolutely. Yeah, because I find it very exciting that there is some to hear your talk. To hear your talk. Excellent. Great. Let's talk offline about. Yeah, let's talk. Yes. Wonderful. Are there more questions right now? I mean, I have one part. Maybe, sorry, maybe a really naive question. So you have the optimal policy or the optimal control that's G star, and then you have the G bar. And it seems that in the beginning, and it seems that in the beginning they agree very well but then the the g bar policy deteriorates um so they the trajectories really deviate from another so what's the reason there is that that you don't solve the control problem in the beginning accurately or is are there perturbations that happen to the state as you go um yeah so so i think you have them switched but but uh the concept of the same But the concept is the same. In the offline computation, we only have access to an approximation of the truth model. Okay. G-bar, right? And so that's where the uncertainty comes from. That's why there's deviation between the final trajectory and the reference trajectory and what the vehicle actually experiences, but we call open loop. And the closed loop then has still the incorrect model? The closed loop. The closed loop is exposed to the truth, the actual atmosphere or the actual conditions, but it uses that mail QR feedback controller to try to bring it back to the reference, right? But it's oversaturated. The difference is too large, so it can't do it. And so the concept is we're going to improve on our reference trajectory, the offline computation, by Computation by providing it with more information, learning the sampling on G star efficiently.