So, and then it could fluctuate up and down in large magnitude because it will be compensated by the responding fluctuations in the background. And this method doesn't take this into account, right? It's pure field. I mean, not exactly, no, it doesn't exactly take that into account. But generally, it overestimates the allowed additional. Allowed an additional amount of signal that in your background model by so much that we don't worry about this. So it sounds a bit strange, but for example, we take quite a wide signal region and we choose the maximum number over the whole region, anywhere that could happen, instead of like in the middle of the peak. So of course we have a signal peak where we expect where we know where the center of the peak is going to be in this case for the weeks at least. At least. And we don't actually take that into account. So if we have a bad modeling of our signal model, we still allow that difference in the tails of the signal model to be covered by this current signal test. So indirectly, we take it into account. But it's a bit of a... I mean, there is quite a lot of discussion between Atlas and CMS about what is the correct way to do it. And I think in the end we agreed that we don't agree and that we're both wrong. And that we're both wrong? And that we're aware of that, but which one is more wrong than the clientable one? Yes, there are only some kind of fundamental phenotypical differences between others and CMS. I mean, the screw signal technique is one example where you rely more on the Modocallo, because you know if you use the Modocalo as a starting point for your uh assessment, not the data or this um this game profiling is studied based on the data. Profiling is studied based on the data. And this just reminds me: there are other examples like that for model system matics, for instance, ATLAS, where you're using FASTIM and using model systematics comparing one FASTIM model power with another one, and not involving the data. So like this is the project or the production measurements I've seen. While at CMS, it's more based on, okay, maybe you are too stupid. Yeah, it couldn't simulate faster. Couldn't simulate faster, we didn't manage, but we we are based basing the systematic uncertainty, estimation always points the data directly, you know. Just uncoding the model as data with different knowledge values. While in others you use, I mean this fast mo of course has a huge advantage, get rid of all statistical fluctuations, so it has a has an advantage. But if if the Monte Carlo is very bad, if if the Monte Carlo is out without these private data, I think that this technique is also not really Technique is also not very not so trustable, maybe. Which is which analysis was that? That was under the top production measurements from the Spoof 160 data. So I think, Lydia, did you want to say anything? And if not, we'll go to Andre. Yeah, I just wanted to reply to that. So I agree with you that there is always a danger if you're basing your uncertainty on a Monte Carlo sample. Monte Carlo sample, the official response from Atlas, and I'm not going to say this is my opinion, but is that there's also a risk in basing it on data because you might bias yourself to an unknown physics that you're just not realizing that you are seeing the data. So it both has its advantages and disadvantages and in the end, like you say, there's a sort of a philosophical discussion going on always on how we treat this. Let's go to Andre. I wanted to follow up on this. I wanted to follow up on this, and I understand we haven't started the discussion yet because the button has not been pressed and all of that. No, no, no. Oh, brilliant. Sorry. I did it very quickly. So I'm going to ask this question for the benefit of others. It's not necessarily that I don't know this discussion has been raging on for a long time. But it's, you know, following on what Miguel was asking. So starting from the Monte Carlo, I don't think there's a big problem with that. But there are regions, I mean, particularly. But there are regions. I mean, in particular, Nicola showed very clearly. We try to go into particular regions. Can you go back to the slide on the template cross-sections? Right, so there are two goals here. One of them is to split things in a way that we become sort of theory-independent, and then we can reapply the theory and certainties afterwards. And there are some of them which are isolating very high. Isolating very high energy transfer, let's put it like that, where interesting stuff might happen. And those are typically very low statistics. So I don't know, and here starts my real ignorance, I don't know how you go about doing the Monte Carlo for those particular low statistics, rare face-space regions, and then say, oh, I have a spurious bias estimation that is really something I truly believe in. I would not like to put my hand on fire for. Does anyone want to choose? Well, I think the rationale is that even if the Monte Carlo is not perfect, at least it shows the tension between what could happen in terms of In terms of possible kinematic shapes in the background from a Monte Carlo where you have accurate kinematics. But do you even have enough Monte Carlo vents? Or the shape to make... I guess, yes. Well, Cassini, you asked me. Maybe one thing we should say is actually we suffer from a We suffer from quantum statistics pretty much everywhere. So, in the high, if we have a lot of data, of course, additional, yeah, this now hurts us more. If we are measuring in a region where we don't have a lot of data anyways, the system I think doesn't matter, I think. So what you're saying is that the period bias that comes out of that might be large, but then the statistical uncertainty or the counts uncertainty is also very large. Counts uncertainty is also very large. Exactly. Okay. So, so, of course, that's not nice, but it doesn't hurt us too much. No, but that's not very different from, you know, Bob mentioned the criteria we use in CMS earlier, which is we try to make sure that the systematic is not much smaller than the statistical uncertainty that you expect. So that ends up being the gauge, it's the statistics. I think we're delving quite deep into analysts. I'll maybe ask any of the statisticians in the I'll maybe ask: do any of the statisticians in the room have any questions or comments? I think the density of jargon's probably just got quite high. It's over without that. I think I have one question. What do you mean by fast and balanced? Okay, uh that very much depends on the cases. But uh uh clearly we are talking about like vector boson plus jets. Plus jets next to linear order. I think we are at the billion level, something like that. One billion levels. What does that mean in terms of time? A million could be a lot. We have a little computing time. It's like a minimum. A billion stuff, but what does it mean in terms of computing? It's enough for CMS to complain that that was hogs all the resources. Frank, did you want to come in on this? About 20 seconds. So, two units are lost. Okay. At least a few minutes. Billion is a lot, maybe, but I mean, I can also question is, what's the actual limitation? Is it a simulation or is it a general error? Because I would naively think, I mean, maybe B plus lots of jets, where you need to add a node to whatever C B plus three, four jets, that's okay, crazy, whatever, but fully. For example, if you also have the H2B or the HBB, that essentially is V plus one jet, and then FBB in the final state. And I would imagine that if you have, you know, if you generate events in the phase space that you care about, it shouldn't be that expensive. What is the thing which is actually expensive? So it it really depends on the generators and on the cases. Typically Typically, Sherpa and the version of Sherpa that we had at that time was extremely costly for the HBD because you are using particular parts of the phase space. And so especially to enhance in digits, actually you have to generate a lot and the generator filters out, but does not matter. Uh and and and yeah And and and yeah, these are extremely costly. Uh like that plus BB at very high PT, like it's 10 minutes per event or something like that in the worst case. I know that the Sharpa has improved a lot in the recent versions, so it's it's quite a bit better, but clearly we are at a stage where generation and full simulation at more or less of the same level. Less of the same level. So I think other ones exist. Yeah, I think the other thing is more. Globally, this is actually a sort of time-dependent and experiment-dependent question. It's actually a little bit different for Atlas and CMS, it's also different now. Okay, it's how it's going to be in time of CMS. So right now, for example, for Atlas, robotically, it's simulation. The post-mary takes a lot of time. And BTCMS, it's the reconstruction of what takes the the largest piece of time. I used piece of time. However, of course, as soon as something takes a whole bunch of time, you focus on it and you reduce it, right? Post simulation techniques, et cetera, et cetera. ICLAS is looking at the generation. That will likely be the protocol atlas in the future and tidy plus and TMHC. At the same time, the reconstruction, which is when you process the data figured out, that's also going to get a a lot slower. Um so yeah, it it's not a simple answer, um but that's something that I sketch. The person with that sketch. And definitely you are right that, well, as usual, if you look at your particular case, you find what your bottleneck is, and you try to work on your bottleneck to improve it. Jack, are there any questions online? Perfect. Okay, other points, questions, requests for clarification of jargon? Maybe to help people understand a skill of computing. People understand the skill of computing. We say how many cores we have in the world and what fraction of them we have loaded to my call simulations. A lot of so yeah, we just don't know my notes. So the question is, how many CPUs do we have and how much of them is rated to simulation? Oh, speakers. I mean, like roughly the computing capacity of the word is a million cores, and they're running continuously. And so that is maybe more over fifty percent. How long would it take to double the Monte Carlo that we have statistic? Sorry, could you just repeat that question so everyone can hear? How long would it take to double the Monte Carlo that we have right now? So the lead time, the lead time for very big background models, like Essentially, you limit it to what you got for practical purposes. Let me get it back into the corner. Just to say, it's a little bit more complicated. It's not like we have just one monthly auto set. You can use it for everything, right? It depends on the year, depends on the latest software and things like that. So typically, we do one big campaign, say, every year or every couple of years. So it's not just extending it, you actually have to redo it quite often. It's just what limits, how much we can have. For practical purposes of a shirt turn, you got the multi-colour, but that's it. You have a set, and then you have a new campaign, and then you make a new set. You can add a little bit. Well, if you find very specific issues, like you need more statistics in one particular background or something like that, you can always uh ask for extensions and and new requests that can come in fairly quickly whenever needed. Quickly, whenever needed. But for the bulk of the Monte Carlo, yeah, you have something fixed for a couple of years. I mean, if there's like a good argument why twice the size would be beneficial, I think there's a lot of fundamental patients getting it. I mean, we will and we have ways to prioritize. Okay, um other questions, comments, provocative statements? We have a provocative statement. So you mentioned, Nicola, at some point, using new techniques that would get around the Monte Carlo, and you talked about the Gaussian processes and stuff. Can you just say a few words about maybe this whole problem with the Monte Carlo can be sidestepped by being clever and using one of these things that you mentioned on your slide number 12? Yeah, yeah. So yeah, I mean, Gaussian processes are like one like non-paramistic approach. One non-parametric approach to try to fit a distribution and have quite meaningful uncertainties on that distribution. So clearly that's the kind of thing that receives a lot of attention nowadays in multiple analysis. Unfortunately, um I I guess this will stay limited to cases with let's say fairly simple background distributions like Background distributions, like steeply falling background distributions, something like that, for many, many analysis where you have complex background shapes with multiple backgrounds that have vastly different kinematics and that end up in your signal region. It seems still nowadays quite difficult to be able to use those new techniques and style step completely on the cloud. Wouldn't it be possible to take a very simplified Monte Carlo simulation that includes all these complications and different components and modulate that with a Gaussian process? And bumps would be absorbed into the Gaussian process. Well, the thing is, um either you do that on your NLO, Monte Carlo, and you have generated your Monte Carlo already. Monte Carlo already. Either you don't, and you have used fast Monte Carlo, but in that case, maybe you are missing very important differences. In this case, V plus Jet, the difference between LO and NLO is really very large. And it's not something that a Gaussian process could accommodate for. I think, Frank, what I would argue that the anal owners, those TL owners here, has almost nothing. owners here has almost nothing to do with your desire to model the DB bar. Sometimes there's this notion of NLO is better, so let's just do it. And then we wait years and years to get 10,000 benefits. So I think there is something to be said. If you really care about very detailed, I mean you know maybe there's better way of doing this. I am not you know I'm a theorist, I've always wanted the highest for the calculation ever. I always wanted the highest for the calculation ever, right? So, of course. But, you know, NNLO event generators, right, which are going to hit the market in a few years, I mean, they will be in a tech door, I don't know, 10 slower than this. So you're not going to run those. 10 is okay, I'm going to give you something which is 100 times slower and even better, right? So I think at some level you have to, you know. Yeah, no, you have to Definitely and relevant. Yeah, and well, and true, there are some ideas on the market. Well, a bit in the sense of what I've shown here that to evaluate the difference between two Monte Carlos, we cannot have enough Monte Carlo stats in one of them. And so we use machine learning techniques, well, optimal transport and things like that. So you could imagine, for instance, So you could imagine, for instance, having like high stats NLO, low stats NNLO, and try to do multi-dimensional reweighting of your NLO to your NNLO, something like that. Maybe you ought to explain LO and NLO and NN and LO to statisticians. Well, yeah, sure. So let's say less precise versus more precise. Simple configuration versus IR job connections. It's more accuracy, right? It's not precision. So true order means fewer effects are taken into account. Next speeding order takes smaller effects also into account, and so on. Yeah, so yeah, indeed, I got it. But the number of terms in your perturbative fraction that you're eventually going to add in, you're not adding in particular. I think I praised this for Fasten, and we also need an explanation. Someone, I guess, like maybe you guys are coming there at Fasten. I think it's a topic where we have no specific discussion at the workshop. Who wants to provide a short description of Fastenpot? Okay, well going back to this, okay. Going back to this, okay. Uh so going from your theory parameters to uh actual observables you have okay what is like Monte Carlo generation like from physics principles, Feynman diagrams, plus phenological models. Then you have how to model the effect of your detector. The effect of your detector. Because our detectors are super complex objects. And this part, usually what we call full simulation, which is very detailed simulation of the interaction of particles in matter, and in that case in our detector, and the response of our detectors. And given the size and complexity of our detectors, it takes several tens of seconds per event. Several tens of seconds per event, even a few minutes per event. And so the idea is to try to replace this full simulation of the data with simplified ones, more parametric or using machine learning models, generative models for that, to have realistic description of the observables given the matrix of the samples. So that's what we call using What we call using fast SIM compared to full simulation. So, with the hope to be much faster and so be able to, in the end, get more multi-carbons in the future. So, a question about the extrapolatability and transferability of systematic uncertainties. So, say you've got a leading order in the Nanolo of Monte Carlo, and you've got a detector simulation that you want to run that's slow, and you've got a fast one that's much faster. Much faster. And you'd like to transfer the uncertainty of comparing the generators to a full simulation, but you've only run the fast simulation. But maybe there's something about the physics that is hard to simulate. So say your NLL Monte Carlo predicts one of gluon jets. Gluon jets are harder to measure because they've got more neutral particles. How do you do one parameter at a time? So you've got your detector part and you've got your physics part and you would like to separate those. Part and we would like to separate those for computational reasons, but we'd really like to do them together. How do you address them? Did someone, do you want to comment on that? This is sort of what I was saying earlier, right? This is somehow I think this is a little bit of a misconception, right? Because the development depth is at the end of the hardware level thing, right, which then goes into the generator, right? It has like hundreds of neurons, and not even blunts like it has all pioneered stuff, like this is hyperlice. High on stuff, like this is haplined. So that's for one of them before the hydronization, right? You already have like hundreds of bluard points, which then get hypnotized. But the NLO does, it changes the weight of the first gluon. So it gives you like to some approximation, it has nothing to do at the end of the day with the distribution of all the pions that you then go through in the text plus the hard gluon. But that one hard gluon, yes, it will maybe generate one additional. Yes, it will maybe generate one additional harbjet, but I'm sure you have enough events at leading all of all the harbjets which look exactly like this. So it's just, you know, at the very, very, very beginning, a sort of shifting of weights for some events. So I, you know, if, let's put it this way, if you are sensitive at the level of the simulation to L O versus L O, you would have huge problems. If that was the case, I would not trust anything you ever measured. Anything you ever measure. So this cannot be a problem. Those extra jets may go down the weed pipe preference. That's a hard, harder to do. But all of that happens leading all of just as far. So I think Rikas wants to come in. Raise your hand if you do want to come in. I'm just trying to understand the same as so are you saying like all types of events are already present in uh leading order uh and so it's just like like uh the NLO distribution kinda like a re-weighting? And a low distribution kind of like a re-weighting of the at the level of the stuff that goes in the detector simulation, I would be extremely surprised if there's something in there, an envelope which is not only in a deleting model. I mean, model, you know, whatever. I don't know, like, the leading model doesn't have a certain channel that you're really interested in, right? Like, if somehow the leading model doesn't have a half feature, that you somehow get it next to the leading model. But apart from these special cases, good. So far, we move down in the chain of things that happening. But I mean, maybe to say a little bit more, I mean, we've spent a lot of time trying to work out what we think the FASCID does well and badly. And I think, because really, the big differences in the calorimeter simulation, I think, certainly earlier on people were much more worried about, for example, using festive things like when you try and reconstruct large radius jets for boosted bosons, whereas I think. Bosons, whereas I think, um, well, I guess two things. I think now we understand a lot better, and there have been lots of improvements that have gone to the fast simulation. Um, I guess the other thing is like if we're ultimately unfolding things and we're confident we've done our unfolding checks and that's doing it well enough, then you can at least factorise out some of the detector. Any other questions or comments? We are approaching nominal lunchtime, but I think we do technically have. We do technically have five minutes, so if we have any remaining questions, maybe let's not open too many boxes just before that. But if there are any remaining points and clarifications or questions or topics that we haven't discussed, then we continue to. So maybe you could say something about the role of important sampling in producing Monte Carlo in regions of sort of rare events. Yeah, what can I say? That definitely's, from what I know, something that has been a lot of development in the past years. Yeah, so okay, that's something that I mentioned a little bit when talking about T keyboard that you have You have background processes which have huge cross-sections, and for a given analysis, you are interested in them in only specific corners of the phase space. So, let's say the simple traditional approach that has been mostly used so far is to just be able to filter events very early in the generation process or Process or apply selection, apply filter. You can have slicing strategies because multi-carol generators allow you to select some parameters for generation. That has been used with quite a bit of success so far. From what I know, indeed, there is a move towards having more control about how your generator is going. How your generator is going to weight events. And so, typically, yeah, you want, like in tails of distribution, usually you want more events with respect to what you would naturally have. And this is something I think which is coming to more and more generators and that we start to use more and more. Actually, a more recent version of Sherpa, for instance, have that. How's that? I actually had kind of a thought as you were saying earlier. So, had, for example, the SDSX bins been defined, had they been defined before or after the Monte Carlo pins were constructed? Like, how would you, you know, how does the feedback loop work? And maybe you're not the person to ask this, but I mean, I think for it hasn't been a problem because we have really enough multi-computer. Really, enough multi-calorie events of a seamless. Did you want to make a comment? No, no. Oh, okay. I can say over large bits. Did you have a comment? But another topic. Yeah, please. One thing I think is presenting all this LHG stuff is always overwhelming. There are so many details. But one thing which strikes me in general is that it seems to me that for every That it seems to me that for every single animal, we are profiling Mussel's parameters, and often we are worrying with the discussion yesterday by David, if we can trust these constraint various talks. But what we are lacking a bit is, I think, to correlate the use of parameters from one annual use to another one. And maybe we should do this more in the future. I mean, my view we are just trying for every single announcement to have everything out to the world. To everything optimal, but you know, if we find a shift of the energy scale or if we find the shift of the theory parameter, we should see if this matches with another analogy. This thing is something which has to be developed. So, this is something which is looked into a lot in the context of Higgs communications. But uh you know that we have you know maybe twenty different uh Higgs channels which are all combined together and All combined together. And in the context of those combinations, these things are looked into quite a bit. So far, with a very pragmatic approach, knowing that for many theory-based parameters, when you start to pull or constrain one of them in two different analysis, which are looking at quite different corners of the phase space, usually it's Usually, it's maybe not justified to actually correlate that. So, most of the time, things become stable uncorrelated. But there are a few cases where we indeed correlate non-pharmaceutical between analysis because we really trust that, yes, actually, this is telling us our data is telling us something of the underlying model, and we want to manage it. So, I think then you wanted to say something? I don't think we would like to say very quick comment. Very quick comment. It would be interesting to follow up what you just said, Olaf, in the context of two-point systematics. So, for example, you've got two different measurements, and I do both of them with Pithia-based corrections, and both of them have period-based corrections. And so, maybe we're moving in some sort of coherent way, which would facilitate combination. I think Frank wanted to make his comment, and then we'll end up with it. I mean, as f as far as the Argonauts workers, the pre-order and so on, this is the whole reason for the SGXS spins, right? For the STX Spins, right? Because by, I mean, Nick said it, but moving the theory systematics out of the measurement and into the interpretation, right, is a huge step forward because then we can deal with it after that. So this is part of the whole point why we start doing all of this. Okay, um, so I think we should now go for lunch, uh, but there's lots of time for people to ask a lot more questions. Um thanks very much, and thanks for watching. And take the next one.