Multipliers in mixture modeling. So, specifically clustering. And this one, I'll focus on matrix variant normal mixtures. Okay, so first I'll just do a very quick overview of what classification and clustering are because it's probably very familiar for everybody here. And then I'll get into the multivariate normal version of the O-Class algorithm and then. Version of the O-Class algorithm, and then how we've extended that to matrix variant normal data. So, classification we can kind of separate into three categories. We have completely supervised, so we'll have K-known observations. We'll use those to predict the unknown labels. In semi-supervised classification, we actually use all of the data to classify our unknown labels. And then clustering is we know none of the labels, and we need to try to find groups in the data. And we need to try to find groups in the data. And that's where I focus is in clustering. So, in supervised classification, there are many methods. You can do this in a mixture modeling approach, but there are very good ones out there that, because of the supervised nature, perform very well. So, some examples would be regression trees, random forests, gradient boosting, deep learning, those kinds of things. Those kinds of things, and we can evaluate the performance because we know the labels is using a test set, cross-validation, stuff like that. So, there are some statistical approaches for supervised classification, but admittedly, the other ones are usually just as effective, or if not more effective. So, this model-based approach is very useful for clustering, classification. You can use it, but you may have better results. Can use it, but you may have better results with some of the other methods. So, clustering is quite different because we have no labeled observations. So, there are consequences for which model we choose. It can increase the number of parameters, multiple solutions, stuff like that. So, there are main realms in clustering. So, you know, you have k-means clustering, hierarchical clustering, and then where I focus is model-based clustering. So, fitting a mixture model to our data set. To our data set. And so I like to focus today on the statistical model-based approaches, and these provide distinct advantages, having such things as likelihoods. And we can give, for example, the degree of certainty of where it is in the cluster. So we can say that, you know, a point is very certain in the center of the cluster is very likely to be in that cluster. But if it's along the edges, maybe that it's 60% in cluster A. It's 60% in cluster A and 40% cluster B. We have a probability of it being in each cluster. So there are some advantages, distinct advantages, in model-based approaches. So I'll just give a quick example of these different methods. So this is the X2 data set, and I realize it's very light. Can we maybe turn off the light? Ah, thank you. Okay, so this is the X2 data set. It's a simulated data set. In R, it's pretty famous for testing approaches in clustering. And so we have three clusters we can see. Two of them are more elliptical and one as a circle here. So in k-means clustering, we know we just, honestly, I'll probably explain it just using an animation. We have, we randomly select three points as the mean in this case. Select three points as the mean in this case. We classify our data, move the mean, reclassify, and just keep going until things no longer change. So, this is the k-means clustering solution. We can see that this purple cluster, we end up classifying some points in the green one because it's actually like spatially closer to that mean. So, that's why k-means for this type of data doesn't necessarily perform as well as if we were using. Perform as well as if we were using Gaussian distributions. This is Gaussian simulated. So I focus on model-based clustering, which is a parametric statistical approach. We have each cluster has a specific density, and then the density of the entire mixture is given by, this is like a weighted average of them. So each component is usually taken to be a cluster. And then our distribution for each cluster can take almost any distribution. Any distribution. So you can have skewed distributions. The earliest ones were, of course, Gaussian, but we have very much moved on from that now. But I will focus on multivariate and matrix variant normal distributions for this. Okay, so there are ways to estimate these. Most commonly is the EM algorithm, but you can use other things like evolutionary algorithms. So we alternate between So we alternate between expectation and maximization steps. So we will first just randomly assign the points or use another solution like k-means to start off. And then we will just alternate between classifying the points and finding our parameter estimates until we converge, our log likelihood converges. So we might try this for several different models and pick our one based on a criterion, such as the Bayesian information criterion. So that one actually penalizes. So, that one actually penalizes how many parameters we have, which helps you from over-parametrizing the model. Okay, so then here's a very quick animation of the EM algorithm. I just started with random assignments, and then it eventually converges to our clustering solution. So, I've overlaid the density of what it predicts, and this pretty much outputs exactly what these clusters were. But this was Gaussian simulated data and Gaussian distribution. So it is likely to get the solution we want. Okay, so now I want to introduce the O-Clust algorithm, which is our way of trying to address when we have outliers in these types of models. So outliers can really affect our mixture models if we don't account for them. So they may be easy to see. In the data set I had, it was just two dimensions. In the data set I had, it was just two-dimensional. If I had a point way off to the side, I could easily say that's an outlier, right? But when we go into high dimensions or matrix variant distributions, it's hard to visually assess what they are. And if we don't do anything about them, they can negatively affect our model by moving our mean. So it may draw them toward the outliers, inflate our variance, and that can affect our parameter estimates and then our clustering results. Clustering results. And then, alternatively, if we had specified that there are three clusters, let's say, if some belong in a group, if some end up hanging out together, we may make that a group and force unrelated groups together. So outliers, if we don't account for them, can really affect our mixture model. So I'll just do a quick example here. This is a different data set, but we have still three clusters. And I have simulated 10% uniform outliers. Now, I did reject. form outliers. Now I did reject if they were close enough to the other clusters just for, I use this for another one to just output the number of outliers. And if I hadn't rejected them, a lot of them would have resided in over top of clusters. But in any case, this is uniform outliers. And when I just apply a normal, yeah. They are. So the question was: Are these outliers to the mixture model as a whole or specific clusters? The way I rejected it was if they were too close to specific clusters. The actual rejection for this doesn't matter that much. It's just when I was applying this model, it was, I needed to, I wanted my algorithm to output how many outliers there were. And because it's so close to the actual data, a lot of them would have been right on top, where you would expect. Right on top, where you would expect them to be normally classified. Just like, for example, you can see in the Navy one, we actually have some light blue points over top. No algorithm is going to give you that to the light, the lighter blue. So that just is going to happen. You're never going to have perfect classification in a sense like this because those middle blue points are actually well within the Navy cluster. So, if I apply just the general EM algorithm, I tried it over a range of possible number of clusters and picked the one that the BIC selects, we get seven. So we can see that this one cluster gets classified into two, and then our outliers kind of belong in three different clusters. So, this is not getting us the result we want because we haven't done anything to address the outliers. Okay, so now I'd like to get into. Okay, so now I'd like to get into the motivation of the O-Clus algorithm. So let's consider our original data set, X2, and let's add one outlier. This one is very obviously an outlier, the red point there. Now, we could visually say that's an outlier, but let's see what it does when we look at the log likelihood of the model with and without the outlier. So we can get the log likelihood of the mixture model as such. And if we find our log likelihood with all of the Find our log likelihood with all of the points, including that bad point, we get negative 998. If we remove that point, we get negative 952. So the log likelihood goes up by removing that point, but we would expect that if you remove pretty much any point from that model, the log likelihood is going to go up because the density at each point is less than one. So the question is, then, is this increase normal or is it out of the ordinary? So here is a graph. So here is a graph of I tried removing one point. So I took any point out, took its log likelihood, put it back in. Took out a next point, found the log likelihood, put it back in. So here's all 300 points, what that looks like. I call these subset log likelihoods. So they're log likelihoods of the n subsets of the data, each with just one point removed. This one is the bad point that I added. So we can see if those are all of our good ones. Those are all of our good ones. This one really is out of the ordinary. If we do this again, after we remove that outlier, we get this. So the log likelihoods are different because I'm not just cutting off the graph, like I'm remodeling this after the outliers are gone. Well, the one outlier is gone, but we can see that that looks like a distribution, right? And that was my first thought when I saw this: is this a distribution? And And it actually is. So, in like our 2023, it's currently an archive preprint, submitted to the journal of classification. We have, we show that the distribution of the difference. So if we look at how much does the log likelihood go up when we remove a point, it has a distribution, and it's a shifted and scaled beta distribution. The derivation of this is in that paper, but. Of this is in that paper, but I'm going to go through it for the matrix variate version so we can see it's the same idea. So to detect outliers, we can leverage this distribution. So in the one slide that we saw, it followed the distribution very well without the outlier. We might say, well, that is following our assumptions. Yeah. I don't know. So the question was, is there a similar result for the ring? Is there a similar result for the ratio of the log likelihoods? I don't know. That is something that I should look into. Yeah. Well, it's because the log likelihood, like, I would expect the likelihoods, yes, but the log likelihoods, we're summing each observation. So I think that it would be a similar result for the ratio of likelihoods, but the distribution might be different. I'd have to do the derivation of that. So we could see that the Could see that the subset log likelihoods followed a nice distribution when there were no outliers present, but it kind of got messed up when we added the outlier. So, this is how we're going to identify outliers. We're going to remove one by one until we get the distribution we want. We're actually going to overshoot it and see which one fits the best. And we can measure how well it fits using KL divergence. So, that is the multivariate normal version. Just a quick overview. I'll get more details of the OCLUS algorithm. Details of the OCLAS algorithm in the matrix variate sense. So now we're going to switch from multivariate data to matrix variant data. So we might be used to, well, actually, this audience probably does matrix variant data a lot, but usually I talk about we have n vectors when we're doing multivariate data. But in matrix variant, each observation is a matrix. So you might consider our columns. Our columns to be different variables that we're measuring, and our rows could be different time points. So, an example in Robert's talk was the mice gene expression. We could, for example, have along our columns, which genes, and the rows would be over time. And then each mouse would have its own matrix as an observation. So, the most well-known matrix variant distribution is the matrix variant normal. We can see this is just an example of matrix. You can see this is just an example of matrix variate data, as I was saying, variables and time points, or another example is grayscale images. So, this case, we have our columns and rows to be actual pixels, and the values in each one is the intensity of the grayscale. So, now we'll get into the derivation of this. We have our density of the matrix variant normal distribution. This is just Variant normal distribution. This is just for no clusters. This is just the matrix variant normal distribution. And we have an equivalence that if we vectorize our data, we end up getting a multivariate. So that's how the relationship between matrix variant and multivariate normal data is. And the distribution that we get actually boils down to the Mahalanobis distance. So that's what the derivation was in the multivariate version. It ends up being when you take the difference in the log likelihoods, we end up basically getting a multiple of Getting a multiple of the mahalonous distance. So, in the multivariate case, we have this, but in matrix variate paper that I did with two lab mates, we show that this is the matrix variate version of Mahalanobis distance. So this is kind of like accounting for variance, how far is each point from the cluster center. And then we show that if a chronic product structure exists for sigma, then we have our mahalon. Sigma, then we have our mahalanobis distance with our estimated parameters is going to converge in probability to what they actually are. So that is our log likelihood of our matrix variant normal mixture. Then we show here that we have an approximation for the likelihood. So we can say that points that are in a specific cluster, the density is really dominated by the cluster that it's in. By the cluster that it's in. So the density coming from other clusters contributes very little to the actual log likelihood. So if we just consider the density to be from the cluster it belongs, and you can simplify it to this, and when we take the difference, we end up with a whole bunch of constants and this half tau j, which is the mahalanobis distance. So we know the distribution of mahalanobis distance, it's gamma. Of a halonobus distance, it's gamma. Well, it's uh, it's chi-squared, which is gamma p over 2, 2. I set it as gamma so that when we take half, we can use the scaling property of the gamma distribution. But we can see that the difference there is a shifted gamma distribution, but we don't really know what the population. This is clustering, right? We don't have information about the data ahead of time. We have to estimate everything. So, this is how we. Estimate everything. So, this is how we can estimate our M, U, and V. I just realized I didn't explain what U and V are. They are: U is the row covariance matrix, and V is the column covariance matrix. So if we replace this with our estimates, we also get a constant, but now a half Tj, and I'm saying that Tj is the sample version of our Mahalanoa's distance. So we know that it. So we know that it converges in probability to tau, which was in the previous paper. And so now we know that it converges in probability to our gamma. So all in all, we get a distribution for each point in each cluster. We have gamma. If we want to do the density of the shifted of the subset log likelihoods for the entire mixture model, we can turn that into the mixture model itself. Okay, so here's an example. Okay, so here's an example of simulated matrix variant normal data. These are the subset log likelihoods and the density we predict. This is for 300 data points. It's okay. But here it is for 3000. So we can see the more data points we have, the closer it gets to what we'd expect. You'd expect that with any sample, right? The sample is going to follow our reference distribution, the more points we have. And this is what happens when I added outliers. So, again, we can see that it's keeping us from getting the distribution we want when we have outliers. So, what we'll do is remove points one by one. And the point we remove is the one we think is most likely to be an outlier. So, remember in the histogram I had of our subset La Latlihoods, we had the outlier way at the end. That one is the outlier. It has the biggest subset law likelihoodless. Outlier, it has the biggest subset log likelihood. And that's what we're defining as our candidate outlier. That's meaning that the model is improving the most in its absence. So that's what we're going to consider a candidate outlier. And so if our log like subset log likelihoods follow our distribution, then most likely we don't have outliers. If they don't, then some assumption is violated. In this case, we'll assume it's that they're outliers. So the OCLS algorithm. So, the O closed algorithm is basically: we cluster it once, get our parameter estimates, and then cluster it n times, each with one point removed, calculate how close we follow the distribution using KL divergence, and then take away the point we expect to be the outlier and continue. And we will go, we will continue this many times well past what we think there might be number of outliers, because we don't, there's no point where you can say this fits. There's no point where you can say this fits. It's basically we keep going and say which one fits the best situation. Okay, that skipped a whole bunch. Okay, so let's get into a simulation study. Guess what? So if I give you a strength of variance in saying fast group, cross-scale systems are fine. So if you allow the variance approach, I don't know how that modeling traces structure of covariance. Okay, so the question was: if we constrain covariance matrices, things might show up as outliers, right? That wouldn't be if we didn't constrain. This model is actually. This model is actually not constraining our variances in any bit. So each. Yeah, in this case, this same with the simulation I had, or the example I had, I had 300 points and 3,000 points. I had 300 points and 3000 points. The 3000 points perform a lot better. So, this is very much more useful when we have more data points than parameters. So, we don't really run into the issue of that. Good to move on. Okay, so this is, I'm just mirroring a simulation study by Tamarque Otto. This is for contaminated matrix variant normal clustering. So, we generate 150. So, we generate 150 observations, two by four matrices in two groups, and we replicate this a thousand times. And 10% of those matrices are going to be replaced by a noisy matrix. So we have 10% outliers here. So, this is our composite KL graph of all thousand repetitions. We can see that our divergence goes up as we start removing some outliers, and then it goes down to a minimum. And then it goes down to a minimum. The line is 15. So that's where we are estimating how many outliers there are. And this is, those two graphs are showing the same thing, but this is how many outliers we are predicting each time in each of our thousand repetitions. So we're predicting the most common one is 15, but there are sometimes where it's 16 or 14, and we do sometimes. We do sometimes have outliers for our outliers, but our sensitivity is 89.4%, and our specificity is 99.3%. So we rarely actually identify non-outlying points as outliers, just sometimes we miss some of the outliers, just one usually. So our subset, all likelihoods, follow a distribution. If it fails to follow the distribution, we probably have outliers and we'll remove them one by one until we get the We'll remove them one by one until we get the closest we can to the distribution we're looking for. And so, next steps for this would be to evaluate its performance against some other outlier models for matrix variant normal data, such as contamination. We would like to release a software package. There already is actually the multivariate normal version in R. It's called O plus, and then extend this to SKU data. Thank you very much, Kat. I will start with a question this time. So my question is about the outliers, like you generated them in a very kind of unique way, but what happened in real data analysis, you may have the outliers even clustering together as well, and maybe. As well, and maybe only a few so that you don't want to make them a new cluster. But then removing just one point may not make that likelihood to decrease enough to detect the one that you took out. Have you explored that? Yes, so that ended up happening when we were doing some benchmark data sets in the multivariate normal version that some of them would be together. So we actually had to, in this case, in the algorithm. In the algorithm, I have an optional step. Remove. Yeah, I don't know. It's okay. It's okay. So the optional step is remove gross outliers first. So it's way faster as well. If you consider the fact that we have to cluster it n times for each iteration, that does take a lot of computational time. So it's actually faster if we can eliminate obvious ones first using other methods, using like db scan, let's say, or things like that. So if we can. Okay, things like that. So, if we can remove ones that are far apart, other methods pick up first, that that can get us to a closer to our solution faster without having to unnecessarily cluster it so many times. Okay. Okay. Thank you. Other questions? Yes. You have to sit in the front row. Okay. So there's like a lot of excitement about k-means and some, some, not all, model-based. And some, some, not all, model-based clusterings mapping to these semi-definite relaxations and generally being less convex or less non-convex than we think. Does any of that sort of map into this space? Help you sort of put the search for outliers in a cleaner format? And then the alternate to this question is, have you seen all these semi-definite relaxation k-means papers? And if not, I'll email them to you. Yes, please. Okay. All right. So I can't really answer that question until, so thank you. I would like. Question until so, thank you. I would like to look into that. I'm biased because that's a big NYU thing. Yeah, thank you. Okay, everyone. Yeah, nice talk. Sort of two questions. So one is a bit like riches. I have recently become reasonably enamored with isolation forests. And I don't know if you know what they are or had a chance to look at them, but I would, again, I'll, it's pretty easy to search for them. They're a random forest type. Search for them. They're a random forest type approach to finding outliers in very high-dimensional data, and it's pretty fast and seems to work remarkably well. But then when you move into this sort of matrix outlier stuff, I'm just like the size of an image that I would work on, the size of a data set that I would work on, the sort of repeated clustering and right is just, it's probably not feasible, right? It's probably not feasible, right? So, how do you like? Have you got thoughts on how to do something that's going to be a lot faster? Yes. So the luckily with using the matrix variant normal assumption, it's actually ended up being faster than just doing the multivariate version because we have nice properties. We have fewer parameters when we're doing that. So that does speed it up. I am doing this in parallel. I am doing this in parallel because each of the subsets is independent. So I am doing that in parallel to speed it up. But yes, it does, considering if it's a very large data set, it is quite slow. So I am definitely working at trying to speed it up. Using, for example, early iterations of the algorithm, we don't necessarily need a very precise log likelihood. For example, that one outlier, it was way ahead. So if we don't let it take a thousand iterations to converge the EM, we could. Durations to converge the EM. We could stop it shorter and things like that to speed it up. At least initially, if we know that we have somewhere between 50 and 100 outliers, we could do an approximate approach for the first 40 and then try to get more detail. But yeah, we definitely need to find ways to speed this up. I think that's it. Yeah. Here, most of the approach is to remove the outliers, but sometimes. But sometimes the atypical or at large are the interesting ones. So, is your method able to identify those atypical that maybe are the ones that are suffering some, for example, in case of medical research, some atypical that are the ones that are you are interested in to treat different nodes and things like that, or it's just identifying the outliers. Identifying the outliers to be removed. So, yeah, this is a trimming method. So, we are removing the outliers to improve our model, but that could just help us find what our parameters are, getting a better estimate, and then putting them back in to see. So, rather than letting the outliers influence how our model is being fit, we can remove them, fit the model, and then we could put them back in to see how they get classified based on what our parameters are. Also, I do keep track of which ones. Also, is I do keep track of which ones are outliers as I go. So I could tell you exactly which point I removed each time if you would like to look at what about this point in particular makes it an outlier. Yeah. And just a thought on there might be ways of trying to approximate the difference in likelihoods when you remove a point. This reminds me a lot of the cross-validation techniques for leave one-out estimates when you're doing kind of smoothing. Estimates when you're doing kind of smoothing models. I was wondering if there's a way of kind of from the full model fit, try to approximate the difference of removing each of the points directly from those parameters. Yeah, so the method, the distribution really boils down to mahalonomous distance, right? The difference in log likelihood, that's where we're getting the distribution. So at least initially, we could just use mahalonomous distance. But the thing with this is it's usually more pronounced. Usually more pronounced the difference versus just the mahalanobus distance, because the mahalanobis distance is using the outlier to find what the variance is. So when we remove that outlier, the variance usually goes down and it makes the difference more pronounced. But at least initially, we could gross outliers use just Mahalanobis distance to see which ones are far away. Well, one more and then we go for coffee. Yeah, the first one's similar to the last comment. It's just like, so there's a lot of great sort of resampling methods and theory around that, right? In particular, the leave one out. It's like, if you do something called jackknifing, it's like you're trying to estimate like the simplest cases, you want the mean, but now you have a distribution over them by leaving one out. And there's a lot of theory for that. So I think if you can pass this or find relationships with that framework, like what is the quantity I'm trying to estimate? Like, what is the quantity I'm trying to estimate via this resampling? That could be nice. The other comment was: I think what you did where you do this a lot and you do it for more than you need, like you said, and then you look at the curve and see where you should stop and call them outliers. That's a very natural thing that you would definitely do in these very non-parametric methods. Like, if you're doing K means, you're selecting K, you do something similar. Yeah. But you have this nice, you know, distributional stuff, right? You have matrix variates and things like that. So, is there like a way to So is there like a way to, you know, have some kind of information theoretic criterion, whether it's related to a likelihood ratio or something that'll tell you when to stop so that you can save time? Yes. So I have been working on turning this into an approximate p-value. So I do have that going for the multivariate normal version. This one, I just finished up the simulation. So I'm still working on a way to do that. But what I did for the multivariate normal version was find our distribution and see how well. Distribution and see how well it fits using like the CDFs, empirical CDF to Kuyper's test, basically. And then I simulate from the data set and see from simulated how close is it and is it out of the ordinary from simulation. So I was doing that in multivariate normal. It was working pretty well. But matrix variate, I still haven't ironed out the kinks for that yet. But that was making it faster to stop. If we have some threshold for p-value, we could stop early. P-value, we could stop early. Perfect, thanks so much. Okay, for the break.