I would like to thank the organizers for this opportunity to give the talk. And I will talk about the Hardy-Littlewood theorem. And let us start with the general setting. So if we have some p from one to infinity, and we have a function with the following Fourier series, and I'll say from the beginning that I will write always sine. Always sine series. But we have to keep in mind that most of the results I was talking about are also valid for cosine series and in the multi-dimensional setting for the mixed sine and cosine series and everything. Okay, and then the Hardy-Littlewood relation will be like this. So it is an equivalence between L P norm of a function and this, say, weighted L P norm of its coefficient. A p-norm of its coefficients. And the famous Perley theorem says that if we have p from one to two, then once we have a function from Lp, then its Fourier coefficient must satisfy this inequality, must be bounded by this L P norm of the function. And the second part of Pelle's theorem says that if P is greater than 2, then vice versa, if we have some coefficients. Vice versa, if we have some coefficients, some sequence such that this weighted L P norm is finite, then this is a sequence of Fourier coefficients of some function from L P and L P norm of this function will be bounded by this expression. Well, it's worth noting that 4p equals 2 is just the partial equality, right? Well, the thing is that the range Well, the thing is that the ranges here are sharp. So, if we want to have these two-sided inequalities, this equivalence, and we do want to have it, because well, if we have this equivalence, then we can reduce problems concerning functions to problems concerning coefficients, and we can go from functional spaces to space of coefficient and back without losing anything. And this is very important for. And this is very important for applications. So, yeah, we want this, and then we have to sacrifice some generality and we have to impose some additional conditions. So, the classical result of Hardy Littlewood says that for monotone sequences, everything is okay. So, there is this two-sided inequality for monotone sequence Cn. The monotonicity condition is quite restrictive. Quite restrictive. So there is a research line which tries to weaken this requirement. And the most wide classes that turns out to obey these Hardy-Little relations are these general monotone sequences introduced by Tichen. And well, let's note that first of all, we don't have to. We don't have to require positivity for such sequences. And they also have some nice features: like if we have a trigonometric series with such coefficients, then this property is inherited by a derivative. So, and it's also important for applications. And we can also see that, for example, for this class, we cannot have zeros. And this problem was also settled by this more general class. More general class which substitutes this Cn by some mean value. So we can have positive, negative values, and zeros, and that's a way wider class than the monotone one. And I will also remark that there are generalizations of Hardy-Little Ethereum, also for weighted Lebesgue spaces and for Lorentz spaces, and for this general class of general monotone sequences. Monoton sequences. This is also true in these weighted spaces. So, in one-dimensional case, the picture is quite complete. But if we want to think about the multi-dimensional case, we should first understand what we should mean by monotonicity in this case. So, well, let's see the one-dimensional definition and play around a little with it. So, first of all, we can say that, well, monotonicity is when C. Well, monotonicity is when Cn is greater than Cm, whenever M is greater than M. Okay, that's correct. And but there is another way, like we can say, let's define this differences and say that all the differences must be positive. This is also a correct way to define monotonicity. And these two points of view help us to generalize this concept to multi-dimensional case. This concept to multi-dimensional case. Well, the straightforward one is to require monotonicity in each direction. So it's just a straightforward generalization of the first definition in one dimension. Although it seems to be a natural way to generalize, looking ahead, I would say that it is not an efficient way. Because, well, in one dimension, we have like Like a very rigorous control over the sequence. But when we require this coordinate-wise monotonicity in, say, two dimensions, we almost lose totally this control. Okay, let's, for simplicity, let's denote this class by CM, coordinate-wise monotone sequences. And to gain this control, that I said, we need to introduce this. To introduce these differences, one-dimensional differences and this delta one-one difference. And it seems reasonable to use them to define monotonicity in, say, two dimensions. But well, the question is, which is the right way to combine them to what we have to require, actually, maybe delta one, one greater than zero or something else. Well, let's see the results. Well, let's see the results. And the first one is by Dychenka about coordinate-wise monotone sequences. And it says that if P is greater than four-thirds, that everything is okay and we have the two-sided inequality. But it turns out that this four-thirds is the critical value. Because if we are below this value, then we can find. This value, then we can find a sequence which is, yeah. Well, I'm sorry, I didn't mention that, of course, we should require this vanishing at infinity. And yeah, if p is less than four-thirds, then there is this exist sequence with that vanishes at infinity, and this weighted p-norm is finite, and everything is good. Finite and everything is good, except for this sine series diverges by squares almost everywhere. And this means that B part will be broken for these values of P, because if this sine series was a Fourier series of a Hilkin function, then it must converge by squares almost everywhere. So we understand that for this problem, this concept of monotonicity. This concept of monotonicity doesn't work very well. And the next result is by Muritz. So, if we require this delta 1, 1 to be positive, and of course we should vanish at infinity, then everything is once again all right. So, we have to set inequality for all p and well, let's stop for a second and understand. Let's stop for a second and understand that if this delta one ones are positive and if we vanish at infinity, this means that all the terms are positive and all one-dimensional differences are also positive. Okay, but once again, if we want to weaken this requirement, we can see the following result by Dichen Kentikhano that says that, well, we can generalize. That well, we can generalize somehow this Moritz result and let's see why it is really a generalization. So if we suppose that this delta one one is positive, then this left-hand side is just CKL. So this is really a generalization of the Moritz result. And it turns out that also we can just erase this three term and write here. Is this three term and write here some lambda? Some lambda which is greater than one, and this class will also obey the hardy-little relation. So, well, but there is one problem that we still need the positivity. And to get rid of it, we have to introduce some new classes. So, we say that CMN belongs to this GMC1 class. Class, if once again we vanish at infinity, and also there is this condition. So if we just draw a picture, k to k and l to l sorry, I will draw it here. K to k, L to L, and we get this corner, and we sum over this corner. And we sum over this corner all these absolute values of delta one one. And this sum must be measurized by this guy here. So this is the definition of GMC1. And the next class is GMC2, which is the same definition. The only difference is that the meteorizing guide now is not symmetric. Symmetric kind of weird condition. So now the sum of this delta one ones is bounded by this term here. And well, what we have for these sequences, we have that actually everything works. So for this GMC1 and GMC2, both left-hand side and right-hand side relations are Hair side relations are valid. So now we drop this monotonicity, sorry, positivity requirement. And I just remark that this result also holds for weighted spaces. So what can we say now about the sharpness of this result? Well, there is the following theorem that says that for genius. That says that for GMC2 sequences, if we replace, well, recall that there was this condition about the corner, but if we substitute the corner by just this rectangle, then the first part of the zero one fails. And this also holds for weighted spaces with the corresponding changes. So I don't really. So, I don't really have time to go into details about theorem one, but I would like to give some idea about the construction of the example which underlies this theorem two. So, well, now we see something ugly on the screen, actually, but let's try to calmly find out what it means. So, we have we'll introduce some sequence which is. Introduce some sequence, which is some guy multiplied by another sequence, and let us first understand this factor. So, this minus one to the power is just the m sine of Rudenshof-Bird sequence. What does it mean? It means if we, for any n, if we just sum up this minus one to the power delta k, just exponent. Just exponent, we'll get something less than square root of n. And this will help us in our further arguments. Okay, and this guy here at the bottom will help us also, but what we need to understand about it now is that on an interval from m to 2m, it almost doesn't change at most in two times, right? Right, so well, let's understand now BM, concentrate on it. We will need some notations. So let Aj be this kind of dyadic interval. And we also introduce the following matrix, if you wish, or just a triangle. And we'll denote for any J. For any j, for any natural number j, we'll assign a number i of j in the following way. So we look at this triangle and we find here j. For example, if j equals 5, we find it here and it's the second column. This means that I of 5 is just 2 and in the same way I of 6 will be 3. Will be three, i of ten will be four, and so on and so forth. Okay, uh, now I'll like to draw something here to make it clear. So if I try to draw my sequence bmnth, if I fix some m, just fix, then from the beginning, bm. And from the beginning, BM will be just this thing here. And with some sign, but now I'll not talk about it. And let's call this function just a pattern function. Okay. And it will go like this with this pattern till something happens. And what exactly happens? I will write once again the triangle. And so on. So, what must happen? I will change the color. A till n enters for the first time to this interval i j1. What is j1? Well, j1 is when i of j1 is equal to m for the first time, for the first time. Time. So, well, let's see what it is. If m equals 2, so let's find the number which falls to the second column first. And this is 3, right? So for m equals 2, j1 is just 3, right? And J two will be 5 and J three will be 8 and so on. Be eight and so on. So, this means that if m for the first time enters this interval ij1, our pattern changes dramatically. So we will have here 2 to the power minus m squared minus 3m. Okay, and it will go exactly like this, the same value till m enters the next ij2 and then. j two and then the value will be two to the power minus m square sorry m plus one squared minus three m plus one and so on so if we try to i'll clear it up and if we try to represent the whole picture like m equals one m equals two m equals three and so on once again i will draw the triangle Once again, I will draw the triangle. Okay, then let's see what happens. When n enters the first interval, I1. So one is in the first column, right? In the first column. This means that we will change just the first column to the following quantity. Okay, and the rest ones will be just. Rest ones will be just following the pattern. Okay, and goes like this till n enters i2. We see that 2 is also in the first column, so we just change the first column to this value and we don't touch the others. They're following the pattern once more. Okay, when m going to i3, we see that 3 is in the second. We see that three is in the second column. This means that we must have the first column the same, right? And we will change the second column to what? To 2 to the power minus m squared, which is 2 squared minus 3 times m. Okay, and from 3 we have the pattern again. Fine, and so on and so forth. For example, when And so force, for example, when n enters I6, we have the first column equal to this guy here. The second is the same. And the third column changed for the first time from the pattern to exactly the same value. And after that, we have the pattern. Well, I hope that some idea now we have some idea about the construction. Now we have some idea about the construction and what can we say about it? Well, the first thing that we have to note is that the left-hand side guys are changing before the right-hand side ones. So we have a kind of monotonicity here. So B Mn is almost increasing in M almost by almost I mean there is some constant. I mean, there is some constant, and up to some constant, BMN is increasing in M and it is clear that BMN is decreasing in N. What does this mean? That if we look at a rectangle which goes from Mn to M to N, then after. Then, up to the constant, the maximal guy here lives at the right bottom, right? And one more fact that we can see from this construction is that these dramatic changes, they happen only once in a dyadic interval. So, in this rectangle, we have at most one dramatic change. And what does it mean? And what does it mean? Actually, this leads to the fact that the sum of delta 1 ones is comparable to the maximal guy in this rectangle. And the maximal guy is this right at the bottom. So what we have, we have this modified GMC two modified GMC2. GMC2 condition, this one. And now we just have to verify that the hardy little relation really fails. And to do this, we have like a direct computation which shows that this norm is really infinite. And now we have to deal just with the function. So we have to estimate. So we have to estimate the Lp norm of the function where Pm is Rudin Shapiro polynomial and Dn is the usual geographic kernel. So we have this expression at the right hand side and recall that from the general Hardy Littlewood we can estimate actually when we know the behavior of decay. Of dk, it's just k to the power p minus one, and we know the estimate for this Rudin-Shakira. And then, if we just apply this and make the calculations, we'll see that this LP norm is finite just using this estimate for Pm. This estimate for PM and well from this expression of amn that we constructed. So actually this means that our function has a finite norm and our coefficient has an infinite norm that breaks the Hardy-Littlewood theorem. So yeah, I would like to stop here. Thank you very much for your attention. For your attention, any questions? One question. Is it connected four in your last result with four over three? No, actually, this four comes because it's a p prime. No, no, it's well, if we see the weighted version. Well, if we see the weighted version, we'll see that this prime of four over three is equal to four to four. Yeah, but actually, to be honest, I think that this four has nothing to do with the real situation. It's just that it comes from roots of Europolynomials. So it's the best I could do, but I think that's not the best threshold. Threshold, thank you. Any other questions? Okay, so thank you again. Thank you. And this is it for