So our next speaker is Lydia Liu, and she's going to talk about social dynamics of machines. How could you learn it for precision? Looks like the screen, yes. Just like it's great, yes. Yeah, thank you so much, Rachel, and thank you to all the organizers for this fantastic retro. Very excited to be here. And yeah, I am Lydia. I just graduated from Berkeley last week, as I was telling some of you. Whereas advice by Mart Hart and Mike Jordan. And in this summer, I'm going to start at Cornell doing a post-doc with John Kleinberg. So today I'm going to be talking about a few topics in what I call the social dynamics of machine learning or decision. Of machine learning or decision. So, in my PhD, I've been interested in the use of data-driven algorithms to make consequential decisions in society. So, what kind of decisions am I thinking about? I'm thinking about decisions in education, to make admissions decisions with algorithms, in hiring, and also even in lending. And algorithms have been used in these domains, but have often led to controversial outcomes that have not been. Outcomes that have not been beneficial to the stakeholders. So, although machine learning methods have succeeded in many challenging tasks here, like playing Go and recognizing images, transcribing speech, these tasks tend to have very clearly defined boundaries. Like playing Go is a very good example. The decisions that are made by the algorithm only impacts one player, the other player, and only in the span of one game. And that's a very bounded setting. Whereas in long-term decision making, In long-term decision making. So, for long-term decision-making in societal systems, this is not true. Like, when an algorithm makes a decision or is used to make a decision in hiring, lending, or admissions, for example, this is often going to impact multiple stakeholders and have longer-term consequences in society at large. And the tools that we talked about in statistical tools and optimization tools are. Optimization tools are, you know, they really form the backbone of solving these bounded tasks, but in the social settings, they don't really account for broader societal dynamics or that we know of. So in this talk, I want to examine algorithms within a broader societal context. And the first layer on top of the algorithms is considering how populations might interact with the algorithms. And this is going to require some behavioral and economic modeling. Modeling. And the next layer ideally would go beyond mathematical modeling to think about how the society, how in society, what are the values and ethics and norms that are promoted by algorithmic systems. And these increasing layers of context are also dynamic and can evolve over time. So I'll also be thinking about time as a form of context, as in time as in long-term versus shorter-term decision-making. So, in this talk, I will discuss two lines of work on different aspects of algorithmic decision making. And the first part is on the long-term fairness of algorithmic decisions, and I'll be focusing on domains such as lending and hiring. And the second part is on the long-term stability in matching markets, where there are multiple agents who are learning and also making decisions simultaneously. And this first part is based on joint And this first part is based on joint work with my co-authors, Sarah Dean, Asterooth, Maxine Kuitz, Moritz Hart, Aisha Wilson, Nika Hattalov, Adam Clive, Christian Borks, and Jennifer Chase. So the question that I have is, can algorithms help humans make fair decisions? And this is a very high-level question. We know that non-discrimination is a very important criteria in decision-making, especially for high-stakes contexts such as For high-stakes contexts, such as employment. And in US and Canada law, there is protection for social groups from discrimination in hiring, especially social groups that are based on race and sex. But the use of algorithms in these settings has been found to be discriminatory, especially against minority groups, such as women and tech. And that has been a very significant and productive line of work. Significant and productive line of work addressing how to formalize and operationalize different notions of fairness to be used for algorithms and to make algorithms less or non-discriminatory. And I will give examples of these formal notions later in the talk. There's some tension here, however, because as this quote from a sociologist and race scholar Ruha Benjamin suggests, narrow ways of operationalizing fairness in technology, Of operationalizing fairness in technical systems would still reproduce certain forms of discrimination. And the concern here is that that might cause harm still to the populations that those operationalized fairness criteria are intended to protect. However, operationalizing these fairness, the idea of fairness in technical systems is a challenge we cannot avoid because if we wanted to ensure that the benefits from data-driven decisions are Data-driven decisions are equitable, then we have to make some of these choices. So, how do we assess what mathematical definitions of fairness are reasonable? And also, you know, it's unclear how to evaluate them, even though a number of different criteria have been proposed. So, I'm going to talk about one form of evaluation that's based on the idea of time. So, it's to consider time as a context of decision making. And one of the Decision making. And one of the reasons why algorithmic fairness definitions might not be helpful and might reproduce discrimination is because they are very static and in the sense that, first of all, these definitions of algorithmic fairness do not take into account longer-term impact. And specifically, we have found that decisions that are fair in an algorithmic sense, they can actually have harmful longer-term outcomes for some groups of stakeholders. For some groups of stakeholders. And this is the theoretical result that I'll talk more about. So the question comes: how do we actually model long-term impact formally? And now I'm going to delve into that in this lending setting. So in the US, we know that credit scores are used by banks to make decisions such as approving or rejecting a loan. Or rejecting a loan application. So let's suppose there are two groups here, and you can think of them as protected groups. And these are the histogram of credit scores in each group. And at higher credit scores, a greater proportion of people will repay their loans. And you can see that I've separated the circles into empty circles and solid circles. So the solid circles represent people in that group who would actually be. In that group, who would actually repay their loans. But you cannot tell based on the credit score whether they end up repaying or defaulting. And so, here in this example, the blue group has lower scores on average than the orange group. And this is often the case for historically marginalized protected groups. And now, if I'm the bank, I'm going to think about how to make lending decisions for these two groups, but maybe with fairness and consideration. So I want to avoid discriminating. Discriminating against the blue group in some way. And one of the approaches that has been studied is to apply a so-called algorithmic fairness criteria. And this notion called demographic parity says that whatever the bank should, whatever the bank does, they should accept the same fraction of loan applicants in each group. So if they approved 40% of the loans in the blue group, then they must do the same. In the blue group, then they must do the same in the orange group. So, in this case, the rational profit maximizing bank will pick the loan approval rate in each group that still maximizes their profit subject to this demographic parity constraint. And a variation on this theme is this other criteria that equalizes the conditional loan approval rates. So, now I just look, this criteria has been called equal opportunity. Equal opportunity in the literature, and Nadi a paper working on that. And it says that conditioning on repaying applicants only, the loan approval rate should be the same for both groups. So I only look at the people who are repaying, and I make sure that I'm selecting the same fraction in both groups. And this can be interpreted as equalizing the true positive rates. And this is a slightly different constraint, but of course the bank could also choose to use. Could also choose to use one of these constraints or choose to just maximize their profit without considering any of the constraints. So there are three forms of deciding on a policy. So we ask the simple question: Is it a good idea to use these algorithmic fairness constraints in this problem setting? So suppose the bank uses a So suppose the bank uses a fairness criteria, the fairness criteria called demographic parity, and means that in each group we must approve the same fraction of loans. So we actually approve all the loans that are above a certain threshold score, and that threshold happens to be group dependent. And now after the individual gets a loan, there's an outcome. We observe the outcome is whether they repay it or not. And their credit scores will actually change as a result of that. Will actually change as a result of that. So, in the orange group, all the people who got the loans actually repaid, so on average, their credit scores have moved up. Whereas in the blue group, because many of the people who got the loans actually were people who could not repay, so more people defaulted. And the scores, if I got worse on average. And this is arguably harmful to the blue group, because that means on average they have more debt and less access to credit in the future. Less access to credit in the future. So we call this population change in the score of each group the delayed impact. And this gives us one way to quantify how each group's aggregate welfare is affected as a result of this lending policy. Now, the very simple technical insight we have here is that under pretty mild assumptions, the delayed impact can be written as a concave function of the acceptance rate of the policy. So the loan. Of the policy. So basically, the loan approval rate of the policy. So the loan approval rate goes from 0 to 1, and this curve shows how the delayed impact varies. And so the loan approval rate for one group in this case. And banks that only cares about maximizing profit will achieve an intermediate level of delayed impact. So we mark it here. So this is the delayed impact achieved by a profit maximizing policy. Profit maximizing policy. And if the policy chosen by the bank actually has a better delay impact than this max profit point, then we call that relative improvement because you're improving the credit scores beyond just a profit maximizing policy. And if it does work, we call it relative harm. Yes? So what's the y-axis here? Sorry? What is the y-axis here? The y-axis is the average change, expected change in the score for that group. A change in the score for that group given a particular loan approval rate. Yeah, and that would depend on your model of how much the scores change with a repayment or default event. Why is the yellow relative part? Just scores. Oh, yeah, so the scores improve less than under max profit. Okay, so that line is the max profit. Okay, so that that line is the max profit. Yeah, this this line is the max profit kind of loan loan policy But I'm just going to oh this is a result so the result is that the delayed impact that quantity we define is a concave function of the acceptance rate of the What's the model for it then? Or I mean how do you determine how this the How does the how is the what determines how the score is changing? Yes, yeah, so we have a model for that. So I'm abstracting some details, but for example, so basically if someone repays the loan, then we say that your credit score increases by a fixed amount. And if they default on the loan, your credit score will go down by a fixed amount. And the probability of the repayment versus default will also Repayment versus default will also figure into that model. And that's what determines the shape of this code. And then are you saying you have a one-off scrolls for each of the different groups? Yes, yes. So this is for like a single population of interest. Yes. So the worst case is when the delayed impact of the lending policy is negative. So that's what we Negative. So that's what we saw in the previous slide where the credit scores actually went down on average. So we call this regime of interest like this active harm regime. So your delay impact is negative, meaning your credit scores are actually going down. So max welfare is that just defined as the peak of this? Or what is max welfare? Max welfare is like, so we can think of the delayed impact as Neil Trump welfare. So that's why we're going to be able to. Is there no if the max max welfare is just decrypted impact? But I guess there's also instantaneous welfare that comes from someone getting a loan at this round that they wouldn't have gotten otherwise. Yes. But that's not on this picture. That's not. Yeah, that's a good point. It's like that's not something we actually consider. But yeah, I think that's an interesting question and can say something about that later. So um so we define this model and have um some tools to analyze like what happens when you use different lending policies. So the first statement we can say is that all the outcome regimes, the three outcome regimes that we defined on the previous slide, they're possible under fairness criteria. So in general, no matter which of the two fairness criteria we use, so there's equal Criteria we use. So there's equal opportunity and demographic parity, all of them. We can find some settings where they cause either relative improvement of the delayed impact or relative harm or active harm. And in contrast, the unconstrained profit maximization never causes active harm. So it never causes the delay impact to be negative. And the intuition for that is because we took it. One of the assumptions is that the bank is risk-averse. Risk ever. So, whatever loss they suffer from a defaulted loan is greater than the loss and credit scores that are suffered by the applicant. Or it's a particular ratio that is different. So it depends on the ratio of how much the bank values a repaid loan versus how much they do not want a loan to be defaulted. And so by assuming that the bank is more risk. More risk adverse, then we get this effect that the unconstrained profit maximization never causes active harm. Profit maximization. So it's choosing a fixed data and not adapting it over time, even though the impact is measured over time? Yeah, so right now we're just considering one time step. So the profit maximization is just over a single time step. And then we're looking at the And we're looking at the outcome that is revealed after this lending decision has been made. We're just looking at the two time steps. But there isn't a follow-up decision that we're looking at. And in fact, what's interesting here is that we can also see different choices of fairness criteria can lead to different kinds of harm in different settings. In different settings. So, one insight here is that if we use demographic parity, it may cause active or relative harm by over-accepting. So, accepting more loans than is actually likely to be repaid. And that's something that this fairness criteria would do, whereas in the same setting, equal opportunity would not have that kind of effect. And we also find that equal opportunity may cause a different kind of harm by Kind of harm by under-accepting. So there are certain distributions that we find where using equal opportunity will cause less loans to be given out. And demographic parity, on the other hand, will not underestimate them. So the choice of fairness criteria also has a very clear impact on how the press source evolves in the next time step. Well, opportunity can never cause active harm, is that right? Never cause active model, is that right? Yes. Yeah, that's right. Yeah. So the question that John brought up is interesting because there are potentially many ways you can measure what welfare is, including the cash transfer that you get. And that's not something this model captures. It just captures one single score that measures your credit health, which is the credit score. And we think that this model is particularly relevant to the setting where there is a single score. Relevant to the setting where there is a single dimensional score that does change over time. And going beyond this, we might also want to examine other models of dynamics. And one model that we looked at is one that was motivated by hiring decisions. So in this model, we consider how humans and strategic individuals might respond to algorithms, and in particular, how they might change the future distribution of data. Future distribution of data as the model training is repeated over time. But there has been a very long line of work considering how strategic individuals react to algorithmic decisions. And I'll just talk about a few themes here. So on the problematic side of things, people might strategically change their features to game the algorithm. And this is in a setting where we think of the people as having many different actions that they can take. Many different actions that they can take to look different. Like, for example, changing the way emails are written to avoid spam filters. And this makes it harder to design a classification algorithm that can distinguish well between positive and negative items. And on the good side, algorithms can also incentivize humans to take improving actions over gaming actions. If the weight Actions. If the weights over certain scores are done correctly, that can incentivize good actions. And related to that is this idea of fair reward. So if an algorithm rewards individuals fairly and they have a choice whether to pursue some particular positive qualification or not, the algorithm's decisions can encourage them to pursue those positive qualifications. And in this, I'm thinking about things like acquiring job skills or preparing for Like acquiring job skills or preparing for college, which are costly investments. And this is a model, this first came out in a model from 93 by labor economists Koo and Laurie, where they looked at a single dimensional problem, but there's this very clear model of incentivizing whether to become qualified for a job or not. And on the other side, when the algorithms fail to reward certain groups, that can discourage them from pursuing positive qualifications. So the qualification rate. So, the qualification rate in that group also becomes lower. And in this work that I'm currently going to talk about, we also examine how when there's heterogeneity across groups, so having heterogeneous feature distribution across groups, that can lead to different responses from the groups, which exacerbates the problem. So, now I'll talk about what exactly this model of dynamics is. So, we on the one side. So, on the one side, we have the player which is the company, and the company has access to data points of job applicants. So, job applicants here are either skilled or not skilled. And the problem for the company is to come up with a classification algorithm that tells them who to give job offers to. So, for example, now this hiring policy maximizes their profit. Profit. So then, and the profit comes from, you know, if I hired a qualified job applicant, I get a certain amount of benefit. So that's how the profit is calculated. And there are actually two groups of job applicants again, and we're concerned with not discriminating against one of the groups. So the job offers are being given out to each group, and now each group is made up of strategic individuals. Of strategic individuals. And by looking at how the job offers are given and which individuals are rewarded, then they will make decisions in the next round, in the next batch of job applicants, whether they're going to acquire the relevant job skills or not through this costly investment. And this in turn creates more data points that the company might take into account the next step and update how they're going to hire. So to summarize this, So to summarize this in words, we assume that the institution has a rational way of decision making, which is to maximize their profit, and this profit depends on the relative qualification rate in each group. The individuals also have a rational model to decide whether to invest in acquiring job skills. So if they acquire job skills, their label becomes y equals 1. And this decision is also made by Maxim. Also made by maximizing their gains. So they only do that if the cost of doing that is outweighed by the expected gain from either getting hired or not. And one feature that we introduce into this model is that the features of the applicant can depend on the group. And this means some of the so-called measures of quality that the company has access to could themselves favor certain groups over others. Groups over others, even if both individuals have acquired the skills and are equally qualified. So we introduced this kind of disparity in comparison to the Archimole model from Hope and Lurie. And the individual level decisions of whether to acquire the job skills or not will determine the overall qualification rate in each group. Now, so the institution sets the hiring policy and then the individuals respond to. And then the individuals respond. So there's this back and forth. And as time goes on, so when we look at the long-term outcome, eventually the qualification rates stabilize and you have reached an equilibrium. So we're interested in what happens at a very long time horizon, just what happens at equilibrium. Sorry, can I ask a question about the model? So on our company side, so okay. So, okay, they don't observe the type of, like the type in terms of skill, not skills. Do they observe that for the applicants or no? They only observe that from their own data. So they have like an observation of, so they have the distribution. Yeah, so we assume that they are able to optimize over the distribution of applicants, current applicants. Okay, so they understand the, and they are, that they're not constrained to use a hyperplane separator or something like that. Use a hyperplane separator or something like that, like the end of a picture of the whatever they yeah, so they have a certain model class. Yeah, but so I think if we, some of the results that we have do assume a particular model class, such as a linear separator or something, we make stronger assumptions to say some of the things we do say. But in the general case, like they choose some arbitrary models. And on the applicant side, they don't change their features. They don't change their features, they just change this one bit whether they have job skills or not. Yeah, and that sort of gives you a distribution where they then sample their features from. So yeah, so it's like the model would work like, okay, one person decides to go and invest in job skills, so they might do a bunch of things, but it's like stochastic what their resume ends up looking like. So it would affect the So it would affect the if you don't do that, then you sample your features from a different distribution. So the individuals have new features that you track, conditional and error, scale. Yeah, you can think of them as a new batch of individuals who are observing this process. And the features observed? Yes. They observed the features, but not the graph truth label. So, the first thing that we observe is that the difficulty really arises when the decisions are error-prone. So, if the decision, if it's such that there exists a zero-error hiring policy in the model class, and the problem is realizable, or even approximately realizable, then there is always a unique non-trivial equilibrium. And this is nice. And this is nice because if we knew that we could perfectly distinguish between who is good qualified for the job, then all the groups will have the same qualification rate at equilibrium, and this is also, in a sense, the optimal qualification rate. And then if we deviate from the realizable or almost realizable setting, we start to face some difficulties. So one setting is when the groups are heterogeneous in the sense that for each group, In the sense that for each group, the best model was different. So there exists a zero-error model for each group separately, but it's not the same model. And in this setting, we can show under some somewhat strong assumptions on the distribution that you have this effect. You have multiple equilibria, but they are all either unbalanced or unstable. So what do I mean? Like, I have two classifiers here. Classifiers here. These two classifiers are stable equilibria. But they're unbalanced because they only classify one group correctly, but not the other. And those are the stable equilibria for this problem. There is also another equilibrium, which is this black classifier over here. And this one is balanced because both groups have the same qualification rate at this equilibrium, but it's unstable. So basically, we even But it's unstable. So basically, even if we initialize very close to this, we wouldn't average to this particular classifier. And if we made the problem more difficult and have no realizability in even one group, there can be potentially infinite number of equilibria as well. So really you can see that this having error-prone decisions causes problems, causes disparity in long-term outcomes. Causes disparity in long-term outcomes. We also learned that. Yes, question. Sorry? What were the distributional assumptions? So we looked at a number of different distributional assumptions. So one of them is this uniform in one dimension distribution, and another one is like a Gaussian with using like a separating hyperplane as the classifier. So, another thing that is of interest is what kind of interventions can actually help us get to a better long-term outcome. So, we've learned that some of the short-term interventions that are good in a one-step setting can actually hurt in the long run. So, specifically, there is this idea called decoupling of The coupling a classifier, and that means that we use a different classifier for each group separately so that we can improve accuracy and also fairness in some sense to have higher accuracy for all of the groups and potentially the same accuracy. And this is known to help in the static setting as shown by previous work. But in this problem, when we have a dynamic setting, this is not necessarily the case. And specifically, when the error rates are in the When the error rates are inherently high, decoupling could lead to a bad equilibrium for a group where that group is stuck in a bad, a low qualification rate that reinforces a hiring policy that doesn't work well for them. So this equilibrium could potentially be worse than under a joint hiring policy if you just use the same policy for all the groups without trying to tailor to each group in a sense. So this could be an argument against sort of separating. Argument against sort of separating your hiring or admissions policy for different social groups. And so some of these results that we have reinforced us the need to go beyond shorter-term definitions of universities. And some of the further questions that we're interested in include our modeling choices. So now we're talking about positions, but a lot of, including previous work, have modeled. Including previous work, we have modeled these decisions as a classification problem, and it doesn't take into account that in many real life settings we are actually looking at relative quality. Like we want to admit, say, the top 20% of students or accept the top 10% of the papers or whatever that is. And there is a competition factor that is not modeled in classification alone. We're not simply giving out labels, but we're trying to select some top fraction. Fraction, right? So, one initial step we might take is to look at the problem of ranking, but how strategic behavior might feature into ranking. And in this one model that we studied, we can show a certain equilibrium behavior that's the result of competition between applicants. And another thing that I find that comes up again and again is that this long-term impact we can define is a useful lens for evaluating. Defined is a useful lens for evaluating algorithmic fairness, but it's not an alternative per se because it's not really a straightforward objective itself to optimize. Like the lending problem I've talked about previously, that requires some knowledge of the credit domain and to establish those so-called delayed impact models, that's actually more difficult. And in theory, we can say, oh, suppose we had this model, but in practice, it's hard to. Model, but in practice, it's hard to measure that or to define it. So there's still pretty open what is the right alternative for making these decisions. So I will now switch gears and move on to talk about different kind of social dynamics in the context of making recommendations in matching markets. So, folks here The folks here, I think everyone is very familiar with the use of machine learning algorithms to make recommendations. And learning is very useful in making recommendations. And we can illustrate it with a simple problem here. Suppose we started a new gym and we wanted to recommend classes to one of our members. So let's call this one, this person, player one. And each class has an unknown reward distribution for that player. And you can already see what I'm getting at, which is the multi- See what I'm getting at, which is the multi-armed vendor problem. So, without knowing anything a priori, we might want to recommend sailing to this applicant, this player, and then they get a reward and they rate this experience as two stars. But because I know that these ratings are noisy, so the next day as the recommendation system, I might not want to recommend the same activity. I might recommend them to explore other activities. And over time, we can learn what. And over time, we can learn what the individual player likes on average based on their pass ratings. And we know how to solve this problem pretty well in the single-player setting using stochastic vendetta algorithms. However, in this gym, we have more than one player, right? So we might have other, potentially many people that we need to give these recommendations to. And if we happen To. And if we happen to give all of them the same recommendation, they will try to get into the same class, but maybe the spots in the class will fill out. And in this case, not all of the users who have recommended something will get to participate. And also, we lose the opportunity to get feedback on what they really like and to learn about their preferences. So there is this element of competition that between the players that we need to take into account when giving recommendations. When giving recommendations. And going beyond this current example, this Twy example, I'm thinking about platforms in the gig economy where there's competition among the players on one side or both sides of the market. So how do we model this competition between users and also this online preference learning problem? We can model this as a two-sided matching problem with online learning. So on one side, Learning. So, on one side, we have the players whose preferences are unknown, so they have certain preferences for the spots in the gym classes. And now I'm going to abstractly call the spots arms after the arms in Multi-Amendance. So, on one side, we have the players, and on the other side, we have the arms whose preferences are assumed to be known and fixed. And as mentioned before, the players get a stochastic reward when they pull an arm successfully. Successfully, and I'll talk about why it might be unsuccessful. And different players have different reward distributions, so they might value different arms differently depending on their individual likes. And I said the arms have no preferences. And there's a competition aspect. So when multiple players pull the same arm, only the most preferred player will get a reward. And the other players in that round are deemed to be as successful. And this is the part. And this is a departure from there's a lot of work in multiplayer multi-ampites, which are motivated by a different setting. And in those cases, they assume that the players have a common reward and that they are actually cooperating. The challenge now is to design a learning algorithm for not just one but all of the players that balances the learning of each player's preferences with this social objective of. This social objective of reaching a stable matching. And the stable matching means that it's a matching of the players to the arms, where no two pairs of players in arms have an incentive to switch. And this is this economic notion of equilibrium in matching. So in online learning, a algorithm has low regret, but in this multiplayer setting, we have to define the appropriate notion of regret that captures both learning. That captures both learning and stability. And to intuitively see, so this is the definition of the stable, we introduced this definition of stable regret, which is the difference between the t, where t is the horizon, t times the mean reward of the stable match for that player. So for player I, it's t times the mean reward of the stable match for player I minus the reward that's collected by player I up to the player. Reward that's collected by player I up to time T. And we can ask ourselves to see that this is reasonable, we can ask ourselves what is the best arm in hindsight for each player. And as it turns out, if all the players have perfect information, no player can continuously do better than the stable matching. So in this sense, this tells us that it's appropriate to compare to the stable match. And in some cases, if the stable match is not unique, we can look at Stable match is not unique. We can look at two notions of stable regret. So, the stable regret relative to the best stable match, and the stable regret relative to the worst stable match. But this definition is well defined. This definition is proper when there is a unique stable match. And now, if the stable regret is sub-linear in the horizon T, that means the market as a whole is learning and that all the players are performing well relative to. Are performing well relative to their stable match. So here I can give you an algorithm with low stable regret that makes use of a matching platform. And what happens here is that at each time step, the players will look at their past rewards and compute the upward confidence bounds of their current empirical reward distribution. So they use UCB to rank the arms. And now, And now both players in ARMS will send their rankings to the platform. So the players will send their rankings there based on UCB. And the platform computes a stable matching based on the submitted rankings. Oh, is this my time? Or I'll wrap up soon. Okay. And so they can compute a stable matching by a very celebrated different acceptance. A celebrated deferred acceptance algorithm, and then the players will get rewards from the matches. And after T rounds of running this algorithm, we actually get the following regret guarantee, which is optimal in terms of the horizon. And there is this curious dependence here on the minimum depth of the arms rewards for all players. So, for every player, in the worst case, they will have to incur They will have to incur a cost that depends on the smallest gap for any player. And this is because if there's one player in the market that has to explore more, another player will, in the worst case, all other players can incur a higher stable regret. So there's this interdependence in this market setting for learning. So in some sense, the sample complexity of preference learning is higher when there's this added social objective of reaching a stable magic. Chance to watch it. Yeah, and so I will skip over some of so there are also other interesting things we can look at in this setting about incentive compatibility, like whether the players are incentivized to rank truthfully. If they you know, i is there anything smart they can do? And the answer is not significantly. They can't really do anything significant to improve their single regret. Can to improve their civil regret. And there are also some results we can show in a decentralized setting where there's no such platform, and instead we can use randomized conflict avoidance strategies. So as promised, I've discussed at a pretty high level some topics about the relevance of social dynamics and how these broader social contexts may be modeled mathematically and we can gain some insight into basic trade-offs. And I think in the long term, like answering these questions that I brought, Like answering these questions that I brought up requires a lot of interdisciplinary work, so that's something I'm working on right now as well. Thank you.