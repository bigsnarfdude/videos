I'm at you tutor Zierik on permut. This is it's hard to pronounce C, let's see, P permutilib polyhedral tools for polyhedral computation. Thank you. So what I did, so first I will explain my motivation. Can you turn your volume up, Matthew? Excuse me. Could you speak louder or volume louder? Could you speak louder or volume louder? Okay, I will try. So, over many years, I have worked on polyederal computation using symmetries. So, I have done many works on with lattice theory, optimization, topology, group theory. And most of the computation were done using GAP. But GAP has several limitations: slowness, large memory usage, and training limitations. So, this I decided to rewrite most of what I did in CS Bus. So, in contrary to my previous work, everything is completely available on GitHub and I contribute daily to it. And it is open source and anybody can contribute. And the last point, because compilation problem can be an issue, now there is something named Docker file which allows you to make your code directly accessible and it can be And it can be you can put on a session on AWS or on your PC or your one. So, here below you can see the GitHub link and the link also to the Docker instance where you can download it and install it on your system. So, I will present basically it's some computer science talk anyway, but what I want to share is all the techniques that can be used in your own work. And I'm open to And I'm open to collaborations. And I'm welcome collaborations. And the goal is to get a better algorithm and a better large-scale computing being done with the best performance. Okay, so now why is this done in C ⁇? So because C ⁇  is fairly well suited for mathematical parallel computation. Matthias, sorry, sorry, could you make your screen larger? It's very small and you're very quiet. Sorry to interrupt. But I don't think I can, honestly. Okay. We only see we see two of your slides at once. Could you zoom in and make it like full screen? Because I can see. Because I can see, I see only one on my system. Yeah, I'm not sure why. Oh, this happens when people go full screen before sharing their screen or the other way around. Okay, so let me tie. So you can ask the pay. Now I go to screen. Share. And now? Looks the same. Shift, please. Yeah, we don't see the full screen. I don't know. If you have some idea what to do, what can do, but no, I don't know what to do, honestly. Zoom in? We can tell it. But we can tell it. Definitely getting bigger for us. Does it look the same for you or different? Perfect, perfect. Yeah, that's much better. It's good for us, but okay. Now what we need is the volume. Okay, that's the trickeries of Zoomer. Okay, I didn't use that. Sorry. And your volume can't be increased, no? Okay, let's try. I don't see that. I don't see that as functionality in Zoom. Probably need to do it on your own computer, like whatever the system settings for the input volume for the mic. Yeah, there you go. That's now we're talking. Now we're in business. Okay, sorry for that. Okay, so let's. Okay, so let's speak first about yes, YC. So my motivation for using it. So the first is my very I turned it down. It's too big. Yeah, your volume went back down again, Matthew. What happens is that Zoom tries to automatically adjust the volume. I don't know what to do. Okay, now it's got to stick. In the zoom settings, it may be automatically adjusting your volume. But it's just so when you raise it there, a zoom will like help you out by lowering it. Yeah. Sorry about that. In in Zoom there's an audio there's a checkbox to turn off the audio adjustment. Off the auto adjustment. Okay. And I would listen to anything such an attractive dog says. Okay. Is that good now? Yeah. Good now. Now it's nice and loud. Okay, good. Okay, so let's come back. So why are you supposed to press? Because it's. Make it a tiny bit quieter. I mean, it's distorted. Sorry. Sorry. I don't know how to clarify. I think you just want to just now they're asking to lower the volume a bit in that little pull-down menu that you have there. Yeah, maybe halfway or something. Do like a bisection search. Okay. Well, okay. So what do I mean by that? So the language has changed a lot since the previous standard. So, the Superfirst came in the 90s and was pretty awful, I mean, in many ways, but now it is much better. Many opinions are based on these previous versions. Every three years you have a new version. C ⁇  11, we have a strong push for making it simpler. So we have some construction from Python that you can see directly, I mean, very similar way in C. Another big thing is that C has always been known to be fast and you can make fast computations and you are not limited by garbage collecting and other issues. You can do full star programming. I mean not the ASQL or OCAML type because it's kind of unservable, but you can provide some functions to your system. To your system, and you can do full snap grammar in terms of any data type you can put. You can do many ways to parallelize. You can do fresh, multi-processing, distributed. So you can do make some computation on supercomputers with C ⁇  on those functionalities. Also, you have the templates that allows you to write abstract code and have type instructions that you can link with. You can link with that. You can, I mean, if you have some ring over some field, you can provide the field that you want in your system as a template parameter. So if you have, and if you really want to use in your property system, like say Python versus Julia Java, you have some foreign function interface system that you can do with it. Do that. Okay, so now the permutational group library part of my work. So for general groups, you don't have no algorithm because groups are so much varied that you cannot do. But when algorithm exists, when practical algorithms exist for computing, they depend inhibit. In almost all cases, on some finite group cases. So, finite groups, you can what you have is computation groups. On this, we can effectively decide and make computations. So, GAP has implemented most of such functionalities that you may need. So, what we want for this library is not all the GAAP functionalities. So, for example, GAP can determine what group the nature. What group the nature of some group classified in the amino decomposition and so on. What we want is for application, we need to be able to complete stabilizer of a set and the repertoire group, testing if two sets archival and repertoixtion groups on finite form and iterating over all group elements. So all of the above have been implemented as plus plus but on the GAAP code and the result is Gap code, or the result is 10 to 100 times faster than gap. So, this is what we want. I mean, so to be able to use AdLibory to make fast computation. So, people and GAP are not necessarily so much happy because it's a separate effort. But if I want to make the work that I did, there was simply no other way. So there are other codes by Christopher Jefferson that provide some functionality like that in host, but they also have better algorithms for computing normalizer and conjugation element. So one thing that is not completely satisfying is that you have bit splitting on double cost sets. So if you have some orbit xj for group j, what you often want is to split this orbit. is to split this orbit xj into some suborbit. So the standard techniques is to do a double cosine decomposition using the stabilizer of x and j. Then you have j equal to j1h j1h jph and then you can decompose the orbit xj as union of xjg and xjg become your representative for the subgroup. So what we need is that double cross-border what we need is that double cosine decomposition and we have something which is non-optimal by using canonical form which which works but what we want is that to have some something really based on the group structure but replementing the gap takes some time and it has not been done yet so there are many features to implement so center centralizer has been done normalizer conjugator article set assembly chains and you have also And you have also some heuristic questions to be considered for the practical computation. So, does it make sense to source double cos sets? So, what we're building the full orbit and sensitting it? So, why do we can store the double coset? Because if you have two stabilizer x which are conjugated under the group, then their double cosetic composition will be the same. Also, if you store them, then you can get some speed. But okay, this is some future problem. Okay, this is some future problem to be without. So, now graph algorithm and technical forms. So, the Partial backpack, which I think is known by most, but I think still useful to explain. So, given a graph G, the problem is to find the group of this graph. So, you have several software present backtrack that allow to make computation really fast. And the field really started with an OT by Bradan McKay with about 10,000 users, which is Noti, the first really fast. Not T is the first really fast program for such computation. Our no-grade software is now Traces, which is 10 times faster than Noti. So you can see in the plots in their paper, it's really quite startling. I mean, Traces is really much faster than Noti. And we also have in the Morphic system check. And why we focus on graph? Because we can reduce almost all computer structures to a graphical one. So edge-up graph, intergraph, scalar complex, all of this you can. graphs, claw complex, all of this you can reduce to a graph and apply the machinery. So technical form for graphs. So the partial backtrack actually such as TRC is also a complete clinical form of the graph. So in many cases, but not all, this allows to find a clinical form for the original object. So what I mean by that, if you have your edge-weighted graph, your repair graph, something like that, you reduce to a graph, but can you, when you find clinical form of view, reduce graph. When you find physical form of your radius graph, can you lift it to some similar complex and so on? And well, not always. So, this is useful for many reasons. This allows to dispense from invariants, which can be expensive business, what is the best invariant to find, what is a cheap invariant, expensive variant, and so on and so on. In the office, checks become very easy. And when you have the clinical form, actually, what you can compute, you can compute the form actually what you can compute you can compute the hash of an object you compute the hash of the canical form and from the hash we can get partitioning for the class and this is very important when you do parallel computing because if you have your set let's say of one to thousand lattices and you compute the canonical form when you have the hash and then you get you can assign directly to the processor from the residue modulo 10 of the hash also it's useful for you can use the standard data structure Or you can use the standard data structure, such as an order set, the hash table, which have a really good performance guarantee. And some tricks, I mean, it's best not to use the same hash function for the portioning as the one that you use for the unorder set, because otherwise you lose performance. So trace these do not provide shared marks and check, but instead provide clinical formats that can be then compared for equality or hash or whatever. Okay, so now let's go ahead. Okay, so now let's go to lift up the skylight form for polytops and lattices. So if you have a linear symmetry group of a polar code, so what I mean by that? So we take polar code, it takes a full-dimensional polar code generated by vector B1, actually it should be AI, from I equal 1 to N in R. And the linear symmetry group, Lince, is a group of transformation sigma in sim n such that there exists sim n such that there exists a in gel and air that a b equal to b sigma i so i need to precise here that there are um you can define other kind of linear symmetry of symmetry group on the polyproidal code i wrote some paper on that but the the one the other one are not as easy to compute as this one for example if you put some scalar here and then it becomes difficult but okay so what you Okay, so what you can define, you can define some quartic form, Q, which is sum or equal one to n of transpose by I. And since your code is assumed to be full-dimensional, this Q is a full rung. And then you define some H-color graph, EC and N-vertise, which vertex and H-color, CG, VIQ minus 1, VG transpose. And then you can prove, it's not really too hard, that the sub-group of the H-colored lab is this. Group of the H colon graph is this Lin group Lince. Okay, so you can get from that, you can get clinical form of the polycarcon, and I think it's worth to explain how it works. So by duplicating the color of VC, you can reduce the vertex and esch colour graph to classical graphs, say LO VC, on which you can apply traces. And then basically every you split your vertex set, you duplicate your vertex set, and then you add colours accordingly. And then you add colors accordingly, and then from that, you can compute the group of the polar code and to compute the empty semester. But then, from the canonical form, first from the list of canonical form, you can get chemical ordering of the vertices that we see. Then, from ordering of this vector or set of vectors, what we can get, you can get the standard basis. I mean, when you have a list of vectors, you take first vector around zero, you take it, second vector with this linearly. Second vector with this linearly dependent on the other, then you reject until even otherwise you take, and then you finish when you have some basis. Okay, when you have start basis B, you can express the vector Vi in start basis B, and this gets this clinical form of your system vector. What do you mean by clinical is that clinical of A V i is equal to the same clinical form of A V sigma i for sigma a permutation of the Permulation of the an element. And this kind of strategy works effectively for polytopes up to 330,000 vertices. I mean, you can work, it will work. But of course, you want to work more. And there are several ways you can accelerate the computation. So, first, if you have your set of vertex set, if you can determine some subset of the vertices that will be present, then you can compute some group of this subset and check if all Of this subset and check if all symmetries will preserve the full polydale code. So, another idea when you have this color CG that you can actually map them to some another set, of course, a smaller one. And again, check if some group of this radius will map will work for you. Another idea is that actually you accept that this will not you use strategy one and then you accept that you might get a bigger book. Might get a bigger book, but then actually, compute the stabilizer of the subset of the remaining points. So, what if you use the first strategy? Actually, you can lift yourself to 100,000 vertices. So, now it's not completely obvious, but I mean, with some work, all the strategies, you can also find a canonical form of your polytop, so which is nice. Okay, so now how do you work with canical form for lattices? Do you work with skeptical form for lattices? So, for a positive definite quadratic form, we define ax equals xax transpose. And for lambda greater than zero, we define mean lambda set of x in the dense z a of x lower than lambda. Okay. And then define lambda mean the small, the minimum lambda, the z minimum lambda is full dimensional and span the den. So again, you have your family of vector and you define some edge-weighted graph, mean lambda mean, with weight. With weight between two B's VI, A V G transpose. Then you apply the same strategy as for polytop. So you get another ring of the mean lambda, but then you need to do something to get some, you cannot apply the present argument for the basis. Because you can get something which is not only rash, some basis which does not span the den. And the idea is that you apply the hermit malform to optimical form form in London. form to obtain chemical form form in lambda on the use matrix A so then you can see that you have some tricks and let's say if you have other groups you might not have some such equivalent of these constructions so this works kind of beautifully I mean because you can get a cardical form of 12 dimensional lattices in I mean in less than seconds when you work with nine dimensional because we want to do Nine-dimensional because we want to do some work in that dimension, then it's in microseconds, and you cannot simply hope to have that for Microsoft reduction because Microski reduction, which is standard approach for chemical form, will really not give you that solution. Okay, now what I want to say also is integral symmetry group because typically when you have this lin set, Lin P, your set of linear transformation preservation equality. Set of linear transformation preserving the polytop. You also want to have the integral transformation preserving it. That is the following. If p is spanned by the i, which you assume to be in z n, then different l prime to be z v1, zvn. And if the i be in zm, then you have this sublativity, which is defined here. What you find is if you find this voice integer so that L prime with L, one over the L prime, then you compute the so. Then you compute the some group for this lattice. And then the group J becomes somebody in J ln Z D, and L becomes a subset of this Z Ln. And now what happens is the integral stabilizer becomes a problem of solving set stabilizer. So as it is volatile, it's a bit hardcore, let's say, in terms of computing this big set and so on. But you can with some relatively elementary tricks, you can really make it manageable and compute effectively. And compute effectively what you need. And similarly, you can get his own fizzle on clinical form. And actually, this could be interesting for this clinical form because here we have this condition that mid-lambda is fully motion at once by zn and you're able to drop this zn but i didn't program it yet okay so now we come to the problem of dual description of a polyterm and the first immersion which is uh basically where I started this field in this field. Where I started this field, in this field. So, here the polytop P, which is defined by vertices, what you want is you want to find facets. So, okay, so here you have a setup range which will define a cube, as will become clear. Then you have the facets altogether. So, the problem of going from the facet to the vertices is equivalent to this one by a duality. So, going from here to here is the same as going from here to here. And the other champrolet. The electron problem is useful for many different kind of computations, the length of perfect forms. Typically, for the pull cups of interest, so they are the ones with a large symmetry group. So, we do not want a full set of facets and we want just representatives because we want to find equivalence de la name up to equivalence and stuff like that. So what you have to be aware, I mean, this problem is not the complexity is not completely clarified, let's say. Clarify, let's say, but we can't expect a miracle. So we have n-dimensional virtue, so it has two n facets but two n-vertices. And Klamatar exchange is to be expected in general. So you can expect that some case will be unsolvable. So that is the idea for using symmetries. So something happens many times, but well, let's do it again. So input vertex set of operator P, only have a group G acting on P. And you have a group G acting on P. So the old P, the set O, the surbits of the facet of P. And then what you do is complete the initial facet bilinear programming and insert to the little cross of the orbit as undone. And for every undone orbit or facet, you can take an observative of this facet. And then you find the ridges contained in this facet. So that is the facet of the facet. So as it is, this is a draft scription computation. And for every ridge of this facet, then you can find scrolling adjacent facet. You can find the coding fades and facet F prime, so that L is equal to Fashion F prime. And when you have that, probably adjustment facet, you take the coding orbits that already present in O, if told, then you start as a done. And then you mark the orbit as done, and then you terminate the orbits as done. So it has been represented many times, and you can see I just saw today some people doing it again. So, okay. So we can do reversibly. We can do regressibly because, in most cases, the orbit of maximum incidence also has the highest symmetry, and that's most difficult to compute. Well, at least you can hope that because if you have maximum incidence on log symmetry, then things are bad. And the computation of the elections facet is the accurate computation. So, you add this, you apply this method recursively. And based on information symmetry groups, incidence and the depth, we decide if you could spam the adjustment method at equal level. At the per level. So we have the issues: the number of cases can grow dynamically. And if you want text have a level of fail, then the size of the groups may be too small. But actually, we can deal with that by computing the group of the polytop by the method of crossing section. One thing I did not mention is that you have always some degrees of a certainty because you have to choose some heuristics whether to do this or that. What I'm concerned. This or that. What I'm considering now is to do some machine learning approach in order to make better decisions at this level. So, but at some point when you go down, you are doing some direction computation, and then you have to use some code that I mean, R code into your system. So, template is so in Polar command, so there are several codes that are available. So there are several codes that are available. So you have LRS, which only requires your field of your, how can I say, your coefficients to be in a ring. And it arrays over all immensible basis of the simplex algorithm. So it is a research, no memory of limitation, well, except for storing the data if you want to store it. Storing the vertices obtained, I mean. Or is it parallelize? Or this is ideal if the pull-up has a lot of vertices. So you have CDD, we shake. So, you have CDD, which requires actually your coefficient to be in the field. And you take your polytop, you add an equality one by one, and maintain double description with the computation. So, all verses are stored. The mere limited are two parallels. It has good performance with the polytop as general disease. Now, what happens, and well, Philippe El Bas Vincent will get a comment on that, is that actually here this code is in C ⁇, so it was on translation of the original. So, it was on translation of the original. But it has not exactly the same performance as the original code in C, which are available externally from polling our command. Another code which you can access if you want is PD, which allows to, I mean, it's another technique where if you compute the dark crypto, but the set of facets that you will find is lower than your initial set of vertices. That your initial set of vertices. In that case, you have some techniques that allow you to find some initial set, and then if you don't find it complete, then you can by end up programming, find a new one, and you read the right until you have completed. So it's a very specific technique. So bounding methods. So because you want to, when you do the recursive transcription, you need to store the dirt from the faces. And some divergence can happen several times. Some document can happen several times. And what we want is you want to store it. So, what we do is that we can close it and then we store it with another map or some other container because I mean, it's a plus plus. So, if you have you are not happy with the performance of this one, then you have so many different containers with the performance profile that you miss seek. So, still, that being said, it's not without problem. That being said, it's not with a problem because you need to research for when to decide to store a direction or not. So you need to research when to check the storage is temp or not because T-ball doing some kind of calization procedure, which is not cheap. Do we store the full symmetry group or for the group that we have encounter? Because that can be different. So all that being said, so we implement this either as a class where you can. This either as a class where you can access directly from the system, or on the clean server if you have a multi-processing setting. So it depends on your use case. So what is the performance that you will have? So let's discuss, I mean, numbers. So the Cut Polytops Cut N is a classic polytop in prompt optimization. So it has two n vertices, dimension n and minus one over two, and two n factorial symmetries. So what happened is that less than So, but the point is that the last instance that you can compute on your computer easily is code 8. So, with the old gap code that I wrote many years before, so it took two days to make the computation. With the gap code, using the canonicalization procedure, it takes 90 minutes. With the new SPS code, we are at 39 minutes. And then all this on the same computer. Then, if you go on a recent laptop, you have 19 minutes and expect further import. Minutes and expect further improvement with better heuristics and better termination criterion. Where you could, I think, you'll probably get to five minutes. And most of the time is in the group library, well, that it is on the dual description computation. Now, in the memory layout for the facet descriptions, so if you have some polytop, or let's say on red vertices, as we need for the perfect form, if you have two Perfect form. If you have 200 million facets, your memory expense was L Giga, which is reasonable, and which you simply cannot achieve if you have some gap or some other system. Okay, so now face lattice computation. So I had for some case in equation with close relac. I needed to compute that. So if you have a polytope dimension D given by any vertices, so what we are interested in is the faces of this polytope. In the faces of this polyta. So the vertices are the faces of dimension zero and the facets. Okay. But what you have to be aware, I mean, from the just when you start, is that for a denimial samplex, the number of faces of dimension i is d plus one choose i plus one. So it should be exponential in the middle. So if you are only interested in the small faces of small dimension k, sorry, one top three or something like that, then you can use linear programming. And essentially, you are Programming and essentially you have the limits contained in those dimensions, but you don't need to know the vertices, so that's nice. Now, if you want all dimensions, which you might need for one reason or another, then you this sequence the facets. So, actually, the first step is then to compute the facets. And then, when you have that by linear J-PARPA, it should be faster than linear programming from sending inside the facet. So, similar potency because you will get this exponential, but okay. Get this exponential, but okay. And this code is based, of course, on the clinical editions on another set for storing the data. Okay, so now let's see some use cases that demonstrate all the features of this content. So this is H domain dimension six. So I will explain very shortly. I mean, so notion of set type, so weakening the notion of L type, which is in the geometry of numbers. Top which is in the geometrical number. So, what happens is you can see a donate polytops on the structure of the domain poly of the actually of the vernal polytop, and you look only at the facets, not at the lower dimensional faces. So, what happened is that the iso edge domain defines some selection of the codes as end of positivity quadratic forms, and they are encoded by a family of two to a minus one vectors. Now, when you have this family. Now, when you have this familiar vector, you can define, you obtain the defining inquisitive redoubted domains. So I'm really gabbling it here. But the notion actually was fairly ancient, but not really well explained. And we wrote with Mario Kumer some, there's only so explained in a modern way this notion in some recent paper. Okay, so now let's see how we could. So, now let's see how we compute those in dimension 6. So, what we have is that we need to enumerate configuration of 63 vectors in dimension 6 and use the canonical form of the scroll. So, it's allowed to assign the usage domain to one of the 20 processor canonically. Because you have compute canonical forms, then you can do that. Or you can use hash tables, just as I did use, explain the beginning. Now, for the canonical form, then we select some genus ID. Instead of looking at using Instead of looking at using the weight w eg called vi a vg transpose, take the absolute value. And lose a graph, 2n mu s1 versus because here we had before we have 2 times 2n mu s 1. And now a priori, okay, you can find canonical form for this, but you don't know when it uh it's not correct when you lose something. But actually you can, because you can compute the term group of this graph, I mean, for this absolute value. Absolute value. And then you can check if the automation can be lifted into some monomorphism of the full graph originally. And if you can do that, then you know that you are in the right. And if not, then you have to use the big graph. And it's a bit more expensive. But actually, this never happened. And now, for so, this really allows you to scale your factor, and then you get. factor and then you get another trick was to when you when you have because what we have you have some possible ways to to switch from one configuration vector to another and Alexander McGillisinoff found a way to to find a non-trivial condition that really reduce number of cases to consider our CDD was used for elementing non-reding realities so we have the Clarkson method which is really the best way to do it on UNIDEZ actually we find And when it did that, actually, we find originally some performance problem. But then we find a bug actually in the implementation. Oh, no, it's really much faster. On seven days on 20 processor, we got 55 million different times. So this was a big success for this approach. Now, last one, I want to mention in this source code. So you have, I mean, I put inside everything that I do, so I try to be as current as nice as possible. So I tried to get Clarent as nice as possible, but it goes. And you have the code for compositive programming. So you have that I wrote with Sean Carnato and Schumann. This is some work I did in the last month on the Winback algorithm for printing fundamental domain of the IPA Black Oxter group, which I think now is the fastest in the world. So actually Daniel Alcock has some better algorithm, but well, he has not finished implementation. So I think I'm I think I am. The code, so for working with short vectors, vector configuration. So SRC short. Then you have, if you want to find space solution of linear system, which is something useful for IKE operators and stuff like that, then it is in RC space. And you also code for remoting the owner polytop in lattices as well as computing perfect forms. Now, a lot of this work is motivated by the achievement computing perfect form in the machine line. And I work with that with the Wessel on the Wareten, but this is still a work in progress. And this is the end of my talk. Thank you. Thank you, speaker. Do you have any questions for Matthew? The far toller worked? I don't know if you can. Oh, we can't hear you. Can you say more about? Hear you. Can you say more about how the sparse solver works? So, this was some actually, I can't really, because you have some method, the method approximating such passing, and I can send you the manuscript where they explain that. And it's some heuristic argument, essentially, which works ideally for graphs with artisans, we don't say. Without sec, um, it works approximatively if you have some cycles. I mean, it gets you a solution, but it's no guarantee of getting a solution. But if you have too large a system, then you will get some, can I say, it's no longer guaranteed to work. I mean, there is some point at which it's not going to work. So it's, yeah, I can't really explain on that, I must say. Did you put a link for the where the code is or yeah absolutely absolutely on the first page I just somehow missed that yeah sorry are there any and is there some kind of an interface to gap if you want to use orbit stabilizer match or something is it is it um uh yeah I mean so if you want to use that Yeah, I mean, so if you want to use that, I can you can provide it by the text uh interface because GAP doesn't have some foreign function interface. It doesn't have that as far as I know. So you write a text file, then you run the C ⁇  code, and then you read the text file after that. So you can do that. And what kind of groups are you dealing with there? Are you dealing with matrix group permutation groups? Yeah, permutation group, matrix group, and all those kind of things, yeah. The also kind of thing, yeah. So you have different types, okay. Yeah, questions? How enumerating perfect forms in dimension nine, how say it's in progress, but it's yeah, I mean, it's uh seems like a monumental task. Yeah, it is kind of, yeah, but uh. Yeah, it is kind of, yeah, but uh I mean you learn new stuff on the way, you find a better way to improve your program and it uh it's not limited to the work that we did, it's not limited to perfect form is useful for other people. So it's I don't know, can say, I mean any any predictions for when there will be or no, I don't want to give any prediction. Oh, come on. Oh, come on. Please. Yeah. Well, I don't have any deadline on that, so please don't put it on me. That's a huge computation. Okay, do we have any further questions? Okay, well, let's. Can I ask a question? I don't know this is uh, yeah. So, hi, Matthew. I'm not sure. So, hi, Matthew. I'm not sure if I should ask you this because, but I mean, you're experts in lattices, so maybe you, the one. So, I assume that I have a lattice in lower dimension, not much like two, three, four, five, for instance. And I want to find a vector in that lattice that I only know the Euclidean length or norm best, for instance, like the square, Euclid norm in 10, for instance. Yes, you can do that. Yes, yes, yes, yes, usually. Yes, yes, yes, yes. I do it follow. I know that I use Paris P, for instance, and I list all the vectors of length 10, and then I pick the one of length 10. But I will, my question is, is there any way faster to do it instead of just list all vector up to that length and then choose the right one? Not that I know. I mean, so there is some implementation I have for thinking post algorithm, but But I don't think it will be any faster than Paris because I don't have any insight that Parry does not have on Finky Post on the shortest vector computation. No, I don't mean the software. I mean, is there any better way to find that one instead of lists all vectors up to that length? Yeah, I mean, if you really. Yeah, I mean, if you really want only one vector, you can use my code. And then when you put the iterations, you can pull the Lambda function. That says, yeah, okay, you have to speak in this kind of language. And then when it finds one vector, then it takes it from the loop. And then it's you will get one vector without getting every one of them. So do you have some code doing that? Uh, doing that on somewhere in your website or somewhere? Did you file this? I mean, not really because I don't have that precise situation, but it's probably easy to do, yes, indeed. But I don't have miracle on that problem. I mean, but you can have communication after and email you later. Yes, absolutely, yeah. Yeah, thank you. Any more questions or comments? Okay, well, if not, let's thank the two again and zoom at nine forty-five Banff time. So we will see you then. We will see you then.