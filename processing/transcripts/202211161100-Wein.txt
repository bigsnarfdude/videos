Yeah, thanks a lot. And yeah, thanks to the organizers. It's been really great being here. So, right, so I want to talk about something called tensor decomposition, which is kind of a basic algorithmic primitive that's used in like all kinds of different machine learning tasks. So I guess I have to apologize that this talk is not directly about learning in networks, but certainly tensor decomposition. But certainly, tensor decomposition does have, is used for like community detection and also for some variants of it, like mixed membership or multi-layer networks and so on. But also like all sorts of other kind of things that come up in sort of machine learning and data science and so on. Okay, so what is a tensor? So you already know what an order two tensor is, even if you don't realize it. It's just like an n by n matrix. Matrix. But you can also have an order three tensor, which is like an n by n by n cube of numbers. The tensors I'll talk about today will always be sort of like square, like the same dimensions, and they'll also be like symmetric. So like Tijk is the same as Tikj and so on. And we know you can have like a rank one matrix, right? Like Vv transpose and its entries look like this. And the same way, you can also have a rank one. And the same way, you can also have a rank one tensor, which we write like this. And analogous to this case, those are the entries. Okay. And so here's actually the tensor decomposition problem that I want to talk about. So imagine we're given a tensor that's rank R in order three, right? So order three, remember, means it's an n by n by n cube full of numbers, and it's the sum of r. Numbers, and it's the sum of our different rank one parts. Right? So each AI is like a vector, and you form the associated rank one tensor and add these up. All is that I just give you this thing, and you want to break it down into its individual parts. So recover the actual components, you know, up to sort of reordering, right? You can't tell which one is called a one versus a two and so on, but you want to recover this. Recover this set of components. And to make our lives even easier, let's even assume that the components are random or just random Gaussian. Of course, this is kind of a strong assumption, but I'm mostly going to be talking about lower bounds today. And for lower bounds, you actually have a stronger result if you make more assumptions, right? Because if I can convince you that even this very idealized version of the problem is hard, then certainly you can't hope to succeed. And certainly, you can't hope to succeed in a more general setting. Okay. And like, so, and so, why is this problem so important? Why does it be used as a subroutine in all these different machine learning tasks? One main reason is because of the method of moments. So, if you are observing a whole bunch of vectors that are drawn from some unknown distribution, one natural thing you might want to do is try to use these to learn the low-order moments of the distribution. So, you could say, Distribution. So you could say, try to compute all the order three moments of this distribution using sort of the empirical moments of your observations. All those joint order three moments could be formed, you know, form a tensor. And then decomposing this tensor might kind of let you understand the structure of the original distribution. Is that okay? So you assume your tensor is symmetric, and then these AI, you're not doing AI across A. You're not doing AI cross AI. Three vectors, you're always one vector. So these obviously go together, but so any any symmetric three tensor you can write this way. Good. You're asking if any symmetric tensor has a symmetric decomposition. I think actually maybe the answer to this is no. I think there's like some weird examples of rank versus border rank. And I mean, it depends on what the rank is. On what the rank is, um, but I don't remember for sure. Does someone know? I think you can at least for maybe there's some issue with like the parody, the parody of the parody of the like whether it's even or odd or something. I think for this, this is true. And it's the same statement as saying that you can like equate these to homogeneous polynomials of degree three in this case. And this is saying that any homogeneous polynomial is some sum of. Polynomial is some sum of linear powers of linear polynomials. And I think this is you can do this with some abstract like algebraic geometry. Kind of argument, but the rank maybe would have to be very large. Well, you might like to express a given particular. But what you're saying, I think, is that the so the rank when you're allowed to. Rank when you're allowed to use different AI, tensor ADA, tensor AK types of components, that can be smaller than this rank at a minimum when you're and T is symmetric itself. Yes, that's the kind of surprising thing. Thanks, good. Okay. Yeah. So yeah, that's right. Like you can definitely consider these asymmetric tensors and other things. These asymmetric tensors and other things. Yeah, for my purposes, I'm kind of imagining there's like this one planted decomposition, and the planted decomposition has, you know, is of this symmetric form, which is the kind of the case that would come up in the method of moments typically, because there, the third moment, you know, would be a symmetric tensor. And all right, great. Is this problem clear? So, what's known about this already, you know, we have a You know, we have a bunch of algorithms now. Sorry if you can't read this because of the colors, but so a ton of algorithms. Some of the sort of state-of-the-art ones are based on either sum of squares or some kind of spectral method where you build some kind of matrix out of your tensor and then look at its eigenvectors. It's actually not very obvious at all what matrix to build. And so a lot kind of goes into that, but I won't get into it right now. And so, the main question I want to ask is: for what values of this rank are you able to decompose it? And so, what we know is that all polynomial time algorithms basically break down at this same, at this point, when the rank hits n to the three halves. So this threshold was first achieved by Ma, She and Stoyer, and we don't know any algorithm that goes beyond this, that goes to like, you know, n to the three halves plus epsilon. Epsilon. But information theoretically speaking, it is possible to decompose it up to a much higher rank, even when the rank is like n squared. That's because the decomposition is unique. And so in principle, if you're sort of willing to do some kind of brute force search over all decompositions, you could figure it out. But we don't know a polynomial time algorithm to do this. Yeah, that's right. Yeah, that's right. Yeah, actually, I'll talk about this later, but for three tensors, that is actually the best we know. Like, we don't know tensor decomposition under like weaker assumptions for three tensors. Yeah, if you allow the components to be truly worst case, I think you are kind of really out of luck. But a lot of people look at like sort of a smooth analysis type of thing. And for smooth analysis, for three tensors, we only. Analysis for three tensors, we only know it up to rank n. Yeah, for worst case, it's not identifiable, so it's just that, yeah. Yeah, I think right, those kinds of issues are a problem, yeah. Good, thanks. Um, so you know, we kind of have this like picture emerging where here I'm plotting R, right? So, as I increase the rank, the problems are becoming harder, right? It's super easy to decompose a rank one. It's super easy to decompose a rank one tensor, but once you have a lot of components, it's hard. And so, you know, it seems to have these three phases that we call easy, hard, and impossible. Right? Easy meaning we know a polynomial time algorithm that provably works. Impossible, meaning that, you know, even statistically speaking, there's nothing you can do because the decomposition is not unique. And so you can't hope to find the planted one. And then kind of in the middle, there's this more elusive hard regime where, in principle, it's possible, but we don't know a fast algorithm. Possible, but we don't know a fast algorithm. Right. And so the main question I want to ask today is like whether this hardness is inherent. Might there just be a better algorithm out there that works? If so, we'd like to find it. But if there's not, we'd actually, we'd also like to understand if this hardness is sort of inherent so that we know we can stop like wasting our time looking for a better algorithm. Okay, and so for context, you know, this is not like just the one-off. You know, this is not like just a one-off thing. This is an example of a statistical computational gap, which is really a ubiquitous phenomenon throughout all kinds of different high-dimensional statistic problems, you know, including the ones listed here and many others. Maybe the most canonical example is this planted clique problem, which has probably been mentioned a few times already at this workshop. So, this is a Gn one half graph. So, N vertices, each edge occurs. n vertices each edge occurs with probability one half but then i plant a k clique meaning i pick k of the existing vertices at random connect all of those to each other right so here's a graph on eight vertices where i've planted a four clique you can take a second to see if you can find it so here's the answer um and so this also has like a similar picture right where now we're sort of signal parameter is k so as the clique gets bigger it's easier to find Gets bigger, it's easier to find. And you have sort of similarly this quite large, mysterious hard region. Okay, and so you know, what do we do? We'd like to understand what makes these problems easy versus hard in sort of a principled way. We don't really have any average case complexity, right? Like if you imagine the classical tools like NP hardness, that's not going to say anything useful here, because those really are about worst case inputs. And here the input comes from a specific distribution. Input comes from a specific distribution. So the maximum clique problem is NP-hard, but that doesn't prevent you from solving planted clique for some input distributions. So instead, what do we do? Sometimes people do reductions from planted clique. So a lot of experts in the room here. If you're willing to take it as a conjecture that planted clique is hard, you can use this to formally prove other problems are hard. You can also prove lower bounds and restricted models of computation. Bounds and restricted models of computation, things like sum of squares or statistical query algorithms or low-degree polynomials, you know, prove that some large and powerful class of algorithms can't do it. And then finally, maybe looking at some kind of optimization landscape or structural properties of the solution space, solution geometry, and so on. Okay, and like this has been hugely successful in recent years. We now have a ton of these. Years, we now have a ton of these kinds of results, and we now sort of understand pretty well for like all of these problems and many others, kind of like what's making them hard and easy. And we have a lot of like lower bounds and all these kinds of frameworks giving us evidence for why things are hard. But despite this, we kind of don't know anything about tensor decomposition so far. And so I want to take a couple of minutes to like tell you why this problem seems more. Why this problem seems more challenging. So, I guess the first thing we should decide is what lower bounds framework do we want to use to try to prove this thing is hard. Maybe the first thing we should try is reductions. So, okay, there's lots of experts here. Maybe hopefully someone in the audience can prove me wrong here. Are you working on it? Oh, really? Okay. Oh, great. Oh, that's interesting. Great, oh, that's interesting. Nice. Okay, so maybe this is not as out of reach as I originally thought. But I mean, certainly, I think what's true is that this problem is quite different in structure from the ones where we previously have reductions. And so something new, I think, needs, you know, new ideas maybe are required. And it sounds like maybe you have them. That's seeing some gold problem as well. There's also no sample. Yeah. So if you take GL half, then there is some conjectured backup tool gap there also. There is actually a natural way to do this as a planted model with H0 and H1. But anyway, yeah. Okay. Yeah. I mean, that's a good point, though. I think there are a few different ways you could. That's a good point, though. I think there are a few different ways you could try to view this. You could try to view this as sort of like just like a random optimization problem with no planted signal potentially. But what I'm going to do instead is to try to think of it more as a planted problem, but by changing the problem a little bit. So yeah, we'll get there in a second. But yeah, okay, first let me tell you about some of these other lower bound frameworks that you might hope to try. So there's the statistical query model, but that one really is not applicable. But that one really is not applicable here, I think, because you don't have IID samples from some distribution. I mean, okay, maybe you could reformulate it as some kind of Gaussian mixture that's equivalent or something, actually, but okay, let me not get into that too much. Also, some of squares lower bounds are a popular thing. One thing to note about this is that some of squares lower bounds are really proving hardness of what's called a refutation problem. Of what's called a refutation problem, whereas decomposition is more like a recovery problem. And so, in general, hardness of refutation does not imply hardness of recovery. There's an example in this paper. Yeah, right. Part of the problem is that it's not like there's an obvious, there's not necessarily an obvious correspondence between a recovery problem and whatever. Recovery problem, and what is the associated certification problem, or something, or refutation. Yeah, so exactly. It's they just looked at pure noise or oh, so this paper, this is a paper with me and Tim and some other people. This is showing that like the planted coloring problem, if you think about recovering a planted coloring versus refuting that there is a K coloring in a random graph, that there's sort of an inherent. That there's sort of an inherent gap here. And depending like what degree the graph needs to have, there's like a factor of two gap between recovery and refutation. Does this make sense? Yeah, exactly. This is just like some other problem. This is a more general sort of high-level statement. Yeah, good. For tensory decomposition, I think there is a natural thing, like optimization problem, which is like the injective norm problem. The injective norm problem to maximize the correlation with one tensor with your observation for which, you know, kind of softly speaking, SOS lower bounds would give us. Well, so tensor, like tensor PCA, the tensor PCA objective function applies to this. You can think of it as like searching for a component, searching for one component. And so showing a certain, like even the value of that. Like, even the value of that, whether it, like, whether the optimal value is close to one and kind of achieved near the components versus just bigger by achieved by points than the generic ones, maybe tells you, you know, so I think that's a fair kind of proxy for hardness of this. But we don't know SOS lower bounds for even that, even though it's gotcha, good. Yeah, so right, there's definitely some related SOS questions. But yeah, like even if certifying the injective norm were If certifying the injective norm were hard, does that necessarily mean you should believe decomposition is hard? I'm not necessarily convinced. But yeah. Okay, and another related thing you could maybe look at is optimization landscape. So people have looked at some of these things. Try optimizing some function whose optima will kind of coincide with the components. But sort of a But, sort of, a possible caveat here is that you have to pick some function to optimize. And there's been a lot of recent work sort of suggesting that it's actually really quite important what function you pick. So, in other problems like PlantedClique or TensorPCA, you know, you have to pick actually sort of a clever function. And so, just because the sort of canonical optimization problem seems hard, that doesn't necessarily mean you should believe the problem is inherently hard. Okay, and so the framework I will talk about today is low-degree polynomials, which I think is sort of especially suited to really saying something about this decomposition problem. But of course, like really, you know, all of these things are sort of interesting and there's a lot of good questions about how they all relate and so on. And, you know, so all of these things sort of bring different and complementary perspectives to the picture. But yeah, today I'm going to tell you about. Yeah, today I'm going to tell you about sort of this framework based on low-degree polynomials, which I'll talk about soon. But let me also say a couple more things that make this problem sort of difficult compared to previous problems. So one is maybe what you were kind of getting at. Like in Planted Clique, there is one specific solution we're trying to find. Here, it's kind of like you have our different solutions. And I guess you're trying to find all of them. It would be convenient to sort of ask the algorithm just. Would be convenient to sort of ask the algorithm just to find one, but then, like, which one is it going to find? It needs to sort of break the symmetry somehow. Um, and then maybe the even you know, probably the most like serious difficulty in this problem, I would say, is that basically all the existing lower bounds we have in any of these frameworks, um, and also kind of reductions to some extent, what they're really doing is leveraging hardness of some underlying testing problem, where it's some planted distribution. Where it's some planted distribution versus a null distribution, where crucially the null distribution has independent entries. So, in the planted clique instance, we're thinking about just can you tell the difference between the graph that has a planted clique versus just a totally random Erdős-Renik graph, right? And so, when people have proved hardness for planted clique in this middle region, This middle region, what they're really doing is, you know, they're not directly showing it's hard to find the solution, they're showing that it's sort of hard even to detect if the solution is there, right? They're showing that it's hard even to tell the difference between these two. And somehow this is like either explicitly or implicitly used in essentially all the tools for these things, like pseudocalibration or statistical dimension or the low-degree likelihood ratio. Like all the standard tools kind of require you to have. Standard tools kind of require you to have a hard underlying testing problem. What about like OGP or something like that? Yeah, you're right. That's probably different and does not require this. Yeah, so good. Yeah, so this would fall into sort of this other category of studying some kind of optimization landscape. Yep. Yeah, so what I'm talking about applies mainly to these three frameworks. So, Alex, maybe it's a dumb question. I never worked on any other. It's a dumb question. I never worked on any of this, but if you try to return, like you're insisting here to return the decomposition with a fixed R, right? Could I have larger R? Like just decompose the tensor, you know, like proper versus improper. Is it make sense here? Like if I actually give you a rank R tensor and you're allowed to output something of higher rank that sort of is close. It becomes very easy. Is there such a fact here? What T said is correct, then you can always. What you said is correct, then you can always put it. There's always some presentation, but that's not an algorithm at all. It's just for any answer. So I'm just curious because that's the usual pattern, it seems here, right? That the proper learning is fine, but improper is very easy. Okay, interesting. Is there such a pattern? Yeah, I don't actually know, but that's a good question. Does anyone know about this? I mean, I think we kind of know that this, we know that this decomposition is unique in some sense. This decomposition is unique in some sense, so I'm not sure if you can find a different one. It's not a requirement. Well, it should be right, but the general position is probably one. Yes, I suspect any bigger R is going to be the tensor is going to be far in any reasonable norm. No, no, we skip star. Yeah, yes. Oh, you're saying if you add, if you allow larger r, it's no okay, because of course you can have zero components, but I don't know if will there be like a meaningful, another meaningful, yeah, I don't know. Uh, it's a good question, yeah. Um, okay, so what I want to point out is that, like, the situation for tensor decomposition is not as simple as this, right? Right, you know, this would suggest that we should try to turn it into a testing problem, right? Let's think about testing the rank R tensor versus just an IID Gaussian tensor or something. But that problem is actually quite easy. It turns out there's like some degree four polynomial that can tell the difference between these. You know, we're hoping to show that this is hard when r is bigger than n to the three halves. But if r is a little bit bigger than n to the three halves, like this testing problem is easy. And so we can't leverage hardness. So, we can't leverage the hardness of the testing problem to show hardness of decomposition. We have to do something else. Okay, so there was this like prior work with Celio Schramman and myself where we sort of overcame this for certain types of problems that were kind of of signal plus noise type, but this also doesn't fall into that framework either. Okay. So, okay, let me talk first talk about like. Okay, let me talk first talk about like what we're going to do about the symmetry issue. And so, what I'm actually going to do is just like change the problem and talk about a slightly different problem. So, I'm going to talk about something called largest component recovery, which just means like, let's take the tensor or like the tensor we're going to be given is going to be sort of like before, but now one component is a bit larger than the others, right? It has like a norm of one plus delta. So, delta think of as an arbitrarily small constant or something. Arbitrarily small constant or something. Right. And okay, this is just for sort of technical convenience that I'm going to now assume that the components are IID, like from the hypercube instead of the Gaussian. This is, I think, not, there's nothing really fundamental about this. Probably what I'm saying could also be adapted to the Gaussian case. But the goal is to just now there's a clear goal, right? The goal is to recover this. Right, the goal is to recover this specific big component, right? So the goal is to estimate the first one, and really by symmetry, it kind of suffices to think about estimating just the first coordinate of the first component, right? So that's just one scalar value. It's either a plus one or a minus one. Because if I know how to figure out that thing, I can, by symmetry, like do the same type of thing to figure out any coordinate I like. Okay, and so. Okay, and so this is going to be sort of the new problem I'm going to talk about for the rest of the talk, right? So, this is kind of now like a planted problem where here's the planted signal we're trying to find. We're thinking of the rest as kind of some kind of noise. But okay, what's exactly is the relation between this thing and the original decomposition problem? Well, there is kind of a nice relation because if it were the case that this, you know, if I'm able Were the case that this, you know, if I'm able to sort of convince you that this problem is hard, say for all polytime algorithms, that implies hardness of like a very, you know, slight generalization of the original decomposition problem, where you're still decomposing this tensor that has random components, but now those components have like arbitrary norms and say even within like some small range. Right? So this is some kind of like semi-random version. Yeah, question? Version, yeah, question is delta here an integer, otherwise it seems like you'd have all your elements in T would be an integer, except they'd have this plus or minus delta away from an integer. Okay, interesting. I was not thinking of delta as being an integer, but that's true that potentially this is, yeah, right. Is that giving away some like information? Yeah, that's a great point. Yeah, that's a great point. That's really interesting. So the yeah, so how can you you can like take the yeah, that's a great point actually. So really I should probably be thinking about like some kind of, you know, add like a small amount of noise or something for this to be like a more meaningful problem or something. Yeah, or just add a uniform zero, one or something to everything. Right. Or I can think of delta just being one, like maybe as a A norm. Yeah, maybe delta is one, and so this is an integer, and then that could still be an interesting problem. Yeah, good. I hadn't thought of this, though. That's a good point. Yeah, the threshold does it. I'm going to think of delta being any constant and it won't change the threshold. So delta can be a million, it can be 0.01. Yeah, it doesn't matter really. Yeah, good, thanks. Okay, so yeah, is this part clear that like if I could decompose this thing, then I could decompose, sorry, if I could decompose this, then I could solve this problem, right? I mean, it's sort of a trivial reduction. Like if I had an algorithm for this and I want to solve this problem, I just like do the decomposition and then output the largest component. I guess here decomposing means finding both the lambdas and the a's. know finding both the lambdas and the a's um is that okay so that's um yeah so that's just to say that you know for now from now on i want to focus on this problem but you know it has this connection that if i'm able to convince you this is hard then you should also believe that this very kind of natural decomposition problem is hard um okay and so someone tend Okay, and so someone, tensor PCA was mentioned as well. This problem sort of looks a little bit like tensor PCA, right? So tensor PCA means you have some rank one component. Normally, we imagine delta being quite large in that case. Plus, usually it would be like Gaussian noise. So you can kind of think of this as being a tensor PCA, but instead of Gaussian noise, you have like this low-rank noise. And so, yeah, so there's not sort of Yeah, so there's not sort of a direct relation. And yeah, it turns out the low-rank noise makes it part of the challenge for what makes it harder. Because tensor PCA already is quite well understood in all of these frameworks I've mentioned before, whereas tensor decomposition is like not well understood yet. Okay? Okay, so that was a lot of sort of buildup and philosophical stuff, but here's the actual result now. So I'm going to tell you the result and at the same time tell you like what I mean. And at the same time, tell you what I mean by this low-degree polynomial business. So, we're going to think about a particular restricted class of algorithms that could try to solve this problem. The algorithms are multivariate polynomials whose input variables are all the entries of this tensor. So, this is that tensor from the previous slide where one component is a little bit bigger. So, right, this has like n-cubed entries, and you're allowed to take any multivariate polynomial in all of those n-cubed variables, right? So, this is like a polynomial that takes in a tensor, outputs just a single number. Is that okay? And we're just going to think about how well can that polynomial f estimate the thing we wanted, right? We wanted, remember, we wanted to think about figuring out the first component of the first entry of the. Component of the first entry of the first tensor component. Okay, and we're just measuring this in mean squared error. So this is like the degree D minimum mean squared error. If I allow myself to use degree D polynomial estimators, what's the best possible mean squared error I can have? Okay, and so that's the result basically is saying that, you know, in this sort of, you know, in the easy regime where we know algorithms, the rank is smaller than n to the three halves, then low degree polynomial. Then low degree polynomials succeed, right? So you can have a polynomial of degree, of you know, low degree, meaning like log n that works well. That gets basically perfect mean squared error as n goes to infinity. Whereas on the other hand, if the rank is like bigger than this, then polynomials are not doing well, right? That even if I allow myself a fairly large degree, like n to some small power, I still get like trivial mean squared error. Like trivial mean squared error, right? So, this is the mean squared error you would get by just outputting zero all the time. Okay, so I'll tell you on the next slide a bit more about like how to interpret this or like why we're thinking about this class of algorithms, but is the mathematical statement of this omega of one instead of big O one? So, omega one means like a small constant. So So, yeah, I'm only showing that for n to a very small number. So, think of this omega one as being a small constant that might depend on this epsilon, right, the distance from the threshold, and might also depend on delta, delta being this delta here. Is that okay? Yeah, good. Other questions? So, Alexander, in all the previous discussions, when you do tensor decomposition, you always went up to quadratic error. But I just, again, I don't work on this, but when you guys talk about tensor decomposition, I thought you mean with zero error, but not suddenly with MMOC. Yeah, good point. So I think there shouldn't be a huge difference between, I mean, there should be some correspondence between these. Were you about to say something? Well, yeah, this is paper having brought that thing. This paper in Bronca thingy model that says if you if you initialize uh like GD close enough to the to the thing, then you can then you can converge. The hard part is getting like some any non-trivial inner product type of thing. So you can just start using something so that it will get cool. So somehow it's the locally strongly from that or something. Right. And so like I'm mostly thinking about the, you know, for me, this is like the main result is the hard part. I'm mostly talking about the you know hard. I'm mostly talking about the hardness part. And so, if you can't even do something good in mean squared error, you're certainly stuck. Yeah, the easy regime is mostly the purpose of this part is sort of to make this one more meaningful, right? Because if I only showed you this part, you would be asking me whether polynomials also work in the easy regime or not. This is like some power methods. O of login is not going to take quasi-polypones to implement? It will actually, as far as I know. It will actually, as far as I know. Yeah, I'm not sure if it, so I'll show you on the next slide roughly how it works. But yeah, I, I mean, we, yeah, it's not, I don't actually know how to take this and turn it into a polytime algorithm. I mean, I haven't tried too hard, but you know, naively, it only gives a quasi-polytime algorithm. You know, which is still a big gap between these. Like, this would be suggesting that you need like sub-exponential time. But of course, we already know like all these different algorithms that. Already know, like, all these different algorithms that do tensor decomposition in polytime, so that's yeah, not really the focus. Um, okay, good. Other, uh, so, okay, let me take one more slide to tell you like why, um, you know, what's like the point of this? Why are we looking at these low-degree polynomials and stuff? So, this is also not like a one-off thing. This low-degree polynomial framework is something that people have now been looking at for a while for all these different kinds of problems. Um, so one reason. So, one reason to study these things is because low-degree polynomials actually capture a lot of important algorithms that people have studied a lot, including all kinds of different spectral methods. So if you're doing some kind of power iteration, you could imagine writing this as a polynomial. Also, AMP is approximate message passing. Local algorithms on sparse graphs. Guy has a paper sort of showing that, like, if low-degree polynomials can't do it, then Degree polynomials can't do it, then statistical queries algorithms also can't do it in certain settings. And so, you know, one way to think of this is that these types of lower bounds are ruling out a bunch of certain known approaches to this problem. Right. And it turns out, you know, this class of approaches is actually quite powerful and is, in some sense, like the best thing we know how to do for these kinds of problems. Because at least, you know, for these kinds of high-dimensional statistics. These kinds of high-dimensional statistics problems I'm talking about. You know, we now have proven these low-degree lower bounds for like all these problems, like planted Klieg and sparse PCA and so on and so on. And like the low-degree threshold is always matching with the best algorithms we know. Okay, and so when a new problem comes along and we figure out the low-degree polynomial threshold for that, it gives us a good reason to think we shouldn't be able to go beyond that. Be able to go beyond that. I have a random question. So, do you mean exception like this low-debate or model predictions now? Yeah, so the main exceptions, there are some exceptions, some counterexamples, meaning situations where some other algorithm beats a low-degree polynomial. But these tend to be these very kind of algebraic algorithms that don't tolerate really any amount of noise. And so, I think the low-degree. And so I think the low-degree threshold is still saying something important in this case. In particular, if I restrict myself to problems that are like at least a little bit noisy, whatever this means. So maybe this means adding some noise so I can't just look at the small delta and stuff like that. If I restrict myself to those kinds of problems, then we don't really know these counterexamples anymore. I mean, I also have to be a bit careful about what constitutes a high-dimensional statistics type of problem. Statistics type of problem, like it should be kind of very symmetric with respect to all these coordinates and stuff. So, yeah, it's a bit of a vague statement, but one other question. Also, if the late propagation runs for many rounds, like results and how do you want to know, is that they both already end up? Yeah, that's a good point, too. Right. Yeah, so this broadcasting on trees, right? So, that problem is kind of a bit of a different flavor where you're, yeah, you have these trees and you see. Yeah, you have these trees and you see just the leaves, and yeah, so that's a that's another good counterexample where their low-degree polynomials coincide with this Kestenstigam bound, which somehow I think is kind of a meaningful thing, but yeah, it's also not the same as the true computational threshold. Yeah, so that's also kind of a puzzling example, yeah. So is that because this 3D congregation doesn't update, so it cannot be accurate to the process. Actually, to be approximated by some polynomials. Yeah, I mean, right. And somehow, because you have to run it for a bunch of rounds, and if you want to approximate by a polynomial, I guess the degree keeps blowing up somehow. Yeah. There's similar examples for if you look at like a spiked matrix model, but where the noise is not Gaussian, there's some sort of like prediction for what the threshold should be. And it's kind of a similar thing where you have to kind of. Of it's a similar thing where you have to kind of deal with you know the non-linearity that you're supposed to kind of pre-apply before you do like PCA, uh, that kind of forces you to sort of artificially inflate the degree of, I don't know, artificially or not, but it makes you take you know higher degree kind of entry-wise polynomial. And so that sort of like yeah, forces the cumulative degree to be higher. Good. Other questions? Okay. And so the final thing I wanted to mention about this framework is that the existing results on these low-degree polynomial stuff can roughly be split into three different categories. There's testing problems where you're thinking about distinguish this distribution from that distribution. There's estimation, meaning like find the planted. Like, find the planted thing. So, this paper I'm talking about today would be an example of estimation. And then there's also optimization, by which I mean like there's no planted thing, and you're just trying to find, yeah, optimize some function. So, imagine I just give you a totally random graph, find the largest independent set that you are able to find. And so, there's sort of different ways of sort of setting up the Lodiery, you know, set up in all. You know, set up in all three of these and somewhat different proof techniques used in each one. But yeah, all of these have been, I think, quite successful. Yeah. But for the optimization problem, do you still need to go through this OGP and then view the latest stable and then go down? Yeah, exactly. That's how the proof works for these kinds of things is using this overlap gap property and some stability of low degree policy. Stability of low-degree polynomials, yeah. Okay, okay, and then finally, there's you know, some connections to circuit complexity, which you know, maybe have not been fully explored, but in some cases, this can imply some circuit lower bounds. Um, okay, but I know I'm getting close on time and don't want to come between you and lunch, so let me maybe move on and try to tell you a bit about the proof. About the proof. So I'll try to tell you about the upper bound first: how to actually construct a polynomial that works well, if there are no other questions. Okay, so how to actually find a polynomial that does solve this problem in the easy regime. The idea roughly is kind of inspired by this line of work on building spectral methods that are based on tensor networks. Okay, so what's a tensor network? I'm not going to try to even define it formally, but you can multiply major. You know, you can multiply matrices together. We all know this. You can also multiply tensors together, but there's actually a lot of different ways you can multiply them together because tensors now have like these three different sort of legs and you can connect those together in all kinds of different diagrams. So these diagrams kind of specify a way of multiplying tensors together. And a lot of these existing works can sort of be thought of as like, you know, take your input tensor, multiply it by itself. Or multiply it by itself a bunch of times in some kind of network, flatten that whole thing into some kind of matrix, and then look at the eigenvectors of this matrix, basically. It's very much not obvious how to take the existing algorithms and immediately turn them into one single polynomial, I think. It doesn't seem to follow directly, but okay, instead, it turns out you can do something sort of similar, inspired by this, which is to take one big tensor network. Take one big tensor network of like roughly log n nodes, like a random three regular graph, and just connect it up like this. And this has one single output. So this means it outputs one vector, and you take that vector as your output, as your estimate for the A1. Okay, and so by analyzing this thing, you get the upper bound. Okay, there's some a bit of cheating where the polynomial. Cheating where the polynomial I use is not exactly a tensor network. You make the edge labels be distinct, if that means anything to anyone, but maybe I won't worry about that too much. And so, yeah, like I said before, this immediately gives you a quasi-polytime algorithm. Likely this can be turned into a polytime algorithm maybe by using a different polynomial or finding a smart way to evaluate this quickly. But yeah, that's sort of not the main focus here. Okay. So, the yeah, the main thing I want to get to is this lower bound. So, how do we prove that, like, all polynomials, no matter what they are, like, all of them have to fail, right? This is like seemingly more difficult task. And to tell you about this, I want to prove the lower bound for just like a sort of silly baby example. So, let's instead of, you know, forget about tensors, let's just say I observe a single scalar T. Of a single scalar t that's playing the role of our tensor. T is the sum of a bunch of these ai's. Okay? The AIs are plus minus one sign. So everything, these are all just scalars. And the goal is to estimate A1. Okay, so you might immediately notice this is like a silly problem that you can't even hope to solve because the algorithm has no way to distinguish like what was A1 versus A2 and so on. Like, what was A1 versus A2, and so on? So, the point of this is not that this is an interesting problem, but that I'm going to illustrate the proof technique on this example. Okay, so hopefully that's okay. Okay, so what do we want to show? Remember, we wanted to show that like no low-degree polynomial can have a good mean squared error for estimating A1. An equivalent thing that turns out to be a bit easier to work with is to think about. To work with is to think about this correlation. So, correlation means take the best degree d polynomial that's trying to achieve like a high correlation with A1, but then like suitably normalized so that if I multiply by polynomial by 100, I don't do any better. Okay, it turns out if I can show that this thing is small, that's equivalent to showing that the mean squared error is bad. Okay, so what do I do? You know, I have the supreme over polynomials. Let's sort of think about an arbitrary polynomial. You know, it has some expansion just in the usual basis, so as powers of t. And these f hats are whatever the coefficients are, right? So, a first thing we might attempt to do is just to directly, like explicitly figure out what this is. So, right, what is this numerator? Well, if I plug in the expansion of f. In the expansion of f, I will find that it's sort of a linear function of the f hats, right? So if I think of f hat as a vector and indexed by d with like these entries, this numerator is some vector c inner product with the f hat, right? And this the vector c has exactly these coefficients, you know, it's something I can explicitly compute. Similarly, the denominator is like some quadratic thing if I expand it all out. If I expand it all out, it's some quadratic form, right, with a certain matrix P whose entries are defined as something. And so if I want to figure out this value, well, I can write it, I can think of it now as supremum over the coefficients of exactly those things I just computed. And that has an explicit answer. So you would kind of think we're just like done. The problem is that P inverse. The problem is that P inverse is like a nasty thing. And it really doesn't seem tractable to directly compute this thing, as far as I can tell. So really, we're going to have to find some kind of like, we're not going to exactly compute this. We're going to find some kind of upper bound on this that is like tractable to work with, but also gives a good bound. And part of the issue, like why this is harder than sort of many. Is harder than sort of many of the existing results. It would be great if these were orthogonal polynomials. Orthogonal polynomials meaning that when I take this correlation, I just get one if d and d prime are the same and zero otherwise. That would be great because then p would be the identity matrix and I would know how to invert it. Is that okay? And so that's sort of like the root. And so that's sort of like the root of a lot of these problems. And so, what we are going to do instead, you know, there is, yeah. Maybe this will come up in like a more interesting way later, but the sort of truth strategy is not to say that we have this. Maybe this isn't even true here. So, like, do we have a similar kind of description of the optimal F as like an answer on degree truncation? Degree truncation of something like would it be, I guess, the like posterior or something, yeah. The ratio of two binomials, right? But even like more generally, it would be in this kind of like correlation setup, right? There's some description of the yeah, so it is it is the optimal polynomial is the projection of the posterior expectation onto the thing. Yeah, but this is the let's look at some general volume. General polynomial and then we'll bound it over all polynomials. Right. So, the way, like, if you wanted to go that route, you know, how are you going to compute the low-degree projection? Well, it would help a lot if we had an orthogonal basis of polynomials to project onto. Otherwise, it's not, we don't know how to do that calculation. So, I think we would end up kind of in the same place. Yeah. Great. And so, yeah, like it would be great. Great. And so, yeah, like it would be great if we sort of knew an orthogonal basis of polynomials for t, but this would, you know, I mean, an orthogonal basis of polynomials with respect to this weird distribution over tensors or whatever we have. And like, we basically only know how to find orthogonal basis of polynomials when you have like IID inputs, as far as I can tell. So, what we're going to do instead is try to, you know, there are some IID random variables here. Are some IID random variables here, right? The A's, the underlying A's. We're going to kind of try to leverage that orthogonal basis instead. So, you know, any polynomial in my tensor, I can also think of it as some underlying polynomial function of the underlying A's, right? Remember, T is just the sum of the A's. So, any polynomial in T can also be thought of as a polynomial in A's, now a multivariate polynomial. In this one, I do have sort of this nice expansion for it. I do have sort of this nice expansion for it, right? This is like the usual Fourier basis, right? Because these are just random plus minus one signs. So a to the u is like just some Fourier parody character thing. It's just the product of the, you know, u is some subset of these a's, and we're just multiplying those together. Right, and so this is like a standard thing from Boolean Fourier analysis that that basis is orthogonal. Is orthogonal. And so, what despises us is that that annoying thing on the denominator, the expectation of f squared, is just the same as expectation of g squared, right? G is the same as f, just with different inputs. And so, and that's just kind of by parsable, like the norm squared of these g coefficients, right? Is that okay? If I imagine computing, if I imagine computing this. Computing, if I imagine computing this explicitly, I'd have to expand this out and apply this property, you know, you get this identity. But in order to actually use make use of this, we need to kind of understand how the F hats are related to the G hats. And so, you know, you can kind of write this down and work it out. It turns out there's just some matrix that relates them, right? If I want, if I'm given the F hats, that determines like this polynomial, the associated polynomial in A's. Associated polynomial in A's, you know, has some coefficients that's a linear relation there. This is kind of the key thing. The key thing is that it suffices to construct an explicit left inverse for that matrix M. Left inverse meaning, you know, the thing times M gives you the identity. Okay, here's sort of the short proof of why that works, right? So the claim is that if I have such a left inverse, it allows me to get. Such a left inverse, it allows me to get a bound, get an upper bound on the original thing I wanted to bound. M is super tall. Yeah, right. M is not square. M will be tall. That's right. Yeah, so there's like lots of possible left inverses. And so, yeah, part of the game will be we need to also pick one that happens to give a good bound here at the end of the day, right? Okay, so what's like going on in this sequence of things? Of things. This is just kind of what we did in the last slide, right? This numerator is some linear function of the f's. The bottom, remember, is norm of g squared, but I want to write it in terms of f. So I'm writing m times f for g, right? That's the relation, okay? And okay, now next I've just taken a copy of the identity and sort of snuck it in here, right, using the left inverse property. It in here, right, using the left inverse property. And now, next, this is like the key step. You know, this the new g here is playing the role of m times f. But the key thing is that I'm taking this supremum over all vectors g, not just the ones in the image of m. So this is like an inequality that's, you know, potentially lossy, but turns out to be tight enough. And okay, why is that great? Well, now we have something we can. Great. Well, now we have something we can solve explicitly, right? Now we have just this norm of g on the bottom, right? So, you know, what's the answer to this problem? It's some vector inner product with g divided by the norm of g, right? You should just take g to be equal to like this vector. And then that has a very explicit solution. Is that okay? Yeah, I guess maybe confused about the weighing size somehow depends on M. And then the what you're telling is something should not depend on the problem. Yeah, so M is some fixed thing that comes from the problem state. Yeah, once you're given sort of the distribution and the problem setup, that gives you M. We do have a choice of which M plus to use. M plus to use, right? This is not unique. So we, every M, every left inverse gives a different bound. We want to pick one that gives a good bound, but also importantly, needs to be something we can explicitly write down the entries of in order to compute this final thing. So the second claim of an amateur both points of amateur hats there. So nm well uh does not depend on the particular choice of fairness. Yeah, this is true for any f hat. True for any f hat, you apply m and you get the associated g coefficients. Okay, so I know we're about out of time, so let me quickly try to finish. So what remains, I have to convince you how to actually construct this left inverse, right? Okay, why should I even be able to do this? I told you earlier that like inverting some big complicated matrix is like horrible, so why is this any easier? Horrible, so why is this any easier? Right, so what are we trying to do? Remember, M is the thing that takes you from the F coefficients to the G coefficients. So, what is the left inverse doing? It's something where like, if I, you know, if I take my f, put, apply m to it to get the associated g, m plus is just a thing that takes that g and recovers the original f. Right? But it somehow only has a guarantee for like actual valid g. For like actual valid g's that could have arisen this way. Like, if you feed it in some other vector g that was not produced from f, it like doesn't have to, there's no requirement on what it does for such vectors. Right, so the problem of constructing a left inverse is just the problem of given a valid g, recover the associated f. Here's like a small example illustrating, you know, why you might hope to do this. So let's say I gave you this at g, right? This is some function of the underlying a's. A function of the underlying A's, I'm claiming that it's equal to some function of T, T being the sum of the A's, right? Can you figure out what function it was? I'm also telling you it's only a degree three polynomial in T. You know, so I can kind of look at this one term here, like this has got three things in it. So, you know, what power of t could have possibly created this? Well, it has to be some t to the third, right? So let's figure out, and that's the only. So let's figure out, and that's the only monomial in t that can produce this term is t cubed. So I can use that to figure out what coefficient do I put in t cubed in order to get the term I wanted. But now that I've figured out what the coefficient of t cubed is, I can just take, you know, whatever this is, subtract that off from here and kind of keep going. And it turns out there's always like a way you can do that. You can kind of keep doing this in a methodical way. Yeah, exactly. So the M star. Yeah, exactly. So the M star is upper triangular. And same with the M as well. Yeah, that's kind of key. Okay, so we have like one minute left. So let me just say quickly, when you do this, so right, that was some baby example. When you carry this out for like the full problem with the tensors, you end up getting, you know, the thing we need to bound is some norm of some vector. This vector is indexed by some kind of hypergraphs and has some like, I don't know, complicated recursive definition. Like, I don't know, complicated recursive definition. So, like, you know, here's a hypergraph A, here's the same hypergraph A. Here's two different ways to collapse that to a smaller hypergraph by XORing some of the hyper edges together. And basically, this recurrence is something like, you know, if you want to compute VA, you have to think about all the ways to collapse it and take those smaller things and some recursive thing. So, the most actually like technically difficult part of the proof is analyzing this recurrence. And there's a lot of like subtle cancellations that happen. Subtle cancellations that happen and so on, but I'm not going to try to get into this. Okay, so let me just wrap up then. The point of this was that we gave the first concrete lower bound, suggesting that this random tensor decomposition problem is inherently hard in some regime of parameters. Everything I said actually applies basically to tensors of higher orders as well. There's some caveat for even k, but I won't get into unless you. K, but I won't get into unless you want to know more about that. Possible future directions would, you know, I suspect you could do the same thing for Gaussian components if you work a little bit harder. There's also a lot of interesting questions involving structured tensors, like coming from these, you know, orbit recovery problems where the tensors have components have some like algebraic relation between each other and stuff like this. And then finally, a problem I think is kind of very tantalizing that I mentioned earlier. Tantalizing that I mentioned earlier is about random versus generic. So, random components versus basically some kind of smooth analysis model. For three tensors, there's actually this gap here where like for generic tensors, we only know an algorithm up to root rank n, but random, we know it up to rank n to the three halves. So, the question is, like, is this regime, you know, is there a better algorithm for the smooth analysis model? Or is this actually fundamentally harder? And can we prove that in some sense? Can we prove that in some sense? Great. Yeah, thanks. That's all. Anything else? Questions? Anything about tensor completion at all this? Oh, tensor completion. Okay. Yeah, I haven't really thought about it. Is there like a conjectured hardness there and stuff? Yeah, I don't know very much about it. And stuff, yeah, I don't know very much about it. Very similar to Fencer PCA with different questions. You would give a single pencil with the close to the entry. Yeah, I know what I think there is some barrier between R. I don't know if this is being justified in any of these lethal financial frameworks. There's this thing by Anker and Boaz, right? Which like. Bohas, right? What's like, don't they show some connection to CSPs and prove it's hard? No, is that a different problem or no? Yeah, it's the same tensor equation. It's essentially the same as noisy for the 3xOR CSPs for 4xOR. So you can write it out those equations of the tensor equations of organic statues. So yeah, after I think there is even an SOS to work on it, but using the connectors and random CSPs you get an SOS. CSP is a good SWS. I think that sounds right, but yeah, I don't know for sure. There's no load of turning. I mean, given that this exists, I think you probably could, but I don't know of anyone looking into it. Yeah, I don't know. We'll say a little bit about this extension of Gauss, if we want to do the Gauss and stuff. The Gaussians, if you want to do the Gaussians, should I be worried about certain collaborations? Right. So, I mean, the main thing that would have to change is that now you have to use like the Hermit basis for the A's instead of this Fourier characters. But aside from that, I don't think there should be like a fundamental problem, but I haven't really tried it yet. Okay, I think people want lunch. Okay, I think people want lunch. Thanks.