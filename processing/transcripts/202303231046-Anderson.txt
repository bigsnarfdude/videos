The details and then see the applications. And for those of you who are DA experts, you can do it the opposite way. So, this figure is celebrating its 20th anniversary. It describes the way that we think about doing ensemble common filtering in the framework of the data assimilation research testbed software that my group develops here at the National Center for Atmospheric Research. So, this cartoon shows, and many of you have suffered through this before, sorry, at time T sub K, we have an ensemble of model state estimates represented by blue. Model state estimates represented by blue asterisks. We make forecasts forward to time t sub k plus one when we have one or more observations that we want to assimilate. We then assimilate the observations one at a time. Take the first observation and we compute forward operators from the state vector. And that gives us in this scalar space at the top of the figure an estimate of what the instrument actually would have observed, three estimates in this case. From the instrument itself, we get an From the instrument itself, we get an observation likelihood. So, for a normal distribution, that would be an observed value and a normal distribution, but it could be something different than that for where we're headed here in this talk. We then do some scalar Bayesian statistics to compute an updated analysis or posterior would be the terms people use, ensemble estimate of the observed thing, and that is the blue values here. And we get corresponding increments for our ensemble estimates shown as the blue vectors. As the blue vectors, we then proceed to update all of the model state vector elements independently by doing a bivariate linear regression using the prior ensemble statistics. And so we regress the increments for the observation to compute corresponding increments from the state variables. And then we repeat this process for any other observations that are available at this time. When we're finished with all the observations available at this time, then we move on advancing the model ensemble to the time of the next. Advancing the model ensemble to the time of the next observations are available. At this point, the first part of this talk, if we had more time, could be about the scalar Bayesian statistics at the top here. And so Dart now has a nearly general framework for doing solutions to this step. As an example, it can do the HODIS and all distributions that were mentioned in Yvonne's talk earlier. It can do gammas, it can do betas, it can do non-parametric distributions. Do betas, it can do non-parametric distributions, it can do the normal quite easily, it can do particle filters in the scalar problem, it can do basically anything you want. And that work has appeared in the literature in monthly weather review. So, for today's purposes, I'll just quickly review some of the things they can do here with an example that's specific for a tracer, a bounded quantity. So, hypothetically, let's suppose that I have an ensemble prior of ozone shown in the green asterisks up here, and I've got some observation. And I've got some observation of this ozone, wherever this is. Maybe it's ozone outside in Banff today. So my prior is shown here. Because this quantity is bounded, if I fit a normal to this, I would end up getting probability that went below zero, and that would be problematic. And there was a question after, I believe, Yvonne's talk that maybe Tiana also answered about the generation of negative values in the assimilation itself. And that can actually happen, even if your model keeps your tracers positive, if you have observations and you try to. If you have observations and you try to do it in a standard normal distribution space, you can get stuff that violates the bounds. But here we definitely want to avoid that. And so I'm just going to choose a gamma distribution. This is something Craig Bishop and others have talked about in the literature previously. And so the green continuous PDF here is simply a gamma fit to the sample statistics of this prior ensemble. For this particular example, to make it clean, I'm going to assume that the likelihood that comes from the instrument is also a gamma distribution. In other words, my instrument that measures over. In other words, my instrument that measures ozone is not going to tell me I have a negative amount. The algorithms here are general and powerful enough to use any distribution you want. The gamma likelihood with a gamma prior is nice. Turns out the product of a gamma and a gamma is a gamma in the same way that the product of a normal and a normal is a normal. That's what makes the common filter. So if I take the product of my prior and likelihood continuous distributions, I get this analysis continuous distribution in blue. The algorithms we developed then tell you how to resample that. And in this case, you get a post. Sample that, and in this case, you get a posterior estimate, ensemble estimate of ozone that's shown by the blue asterisks. It's guaranteed to be non-negative. It shares many other characteristics of the distribution. I won't go into more detail here, but this is a very, very general solution. Now, the problem is, as soon as we go back to this cartoon and we take increments that were computed in a very nice way for ozone in observation space and we linearly regress them onto state variables, all bets are off for the statistics. All bets are off for the statistics of the state variables by using that standard linear regression. So, what I'm going to talk about the rest of the time today are methods to improve the behavior of this regression step when the relationships are non-linear and when the distributions for the state vector elements are non-Gaussian. So, the example I'll start out with will continue that use of an ozone hypothetical distribution. So, this is a bivariate prior distribution. It is Distribution. It is artificially generated, but it could just as well have come from a model. So it's showing on the horizontal axis a temperature variable, and that temperature variable is observed. And again, maybe it's temperature outside in Banff today. And the unobserved variable is ozone concentration. And in this hypothetical example, as the temperature goes up, the ozone concentration is going up. Now, ozone, of course, as we said, is bounded below. And so it's not got a normal distribution. And if you generate a distribution like this, you might get a joint. Distribution like this, you might get a joint gamma normal that looks something like this. And so the shading is an analytically very expensive MCMC computation to get the right answers here. And the 100 green asterisks are a prior ensemble from this bivariate distribution. As I've mentioned, I'm going to observe the temperature variable and the likelihood of that observation is shown here. So the temperature, I'm going to basically assume a normal distribution. And if I follow through and do what I described before, I do a standard. Before I do a standard common filter update for the observed temperature and I regress that onto these bivariate statistics, I get a posterior that looks like this. First of all, the shaded is an analytic value of the posterior. It shows what we should get. And the blue asterisks are what we get from our 100 ensemble members. And they're a mess. So there's a number of negative values. This demonstrates how you can generate negatives in the data assimilation step. There's a number of larger values that are really outside the range of the probability. The range of the probability. So, this just isn't a good solution. So, we want to try and find a way to improve this linear regression step. This just shows you what's going wrong. So, this picture, these are the same 100 green asterisks that are shown in the left panel here. And so, these 100 green asterisks are my prior. And the way that linear regression works, which is, by the way, just what the Kalman filter would do, is it fits a least squares line thrown in the black dashed here through the bivariate distribution. Through the bivariate distribution, we computed the increments in the observed variable space for the temperature, and then we would just project those increments onto each ensemble member along the slope of the least squares. And you can see nine example ensemble members here out of the 100 showing that regression. And a lot of them end up going negative here because there's no constraints on this. So, what we're going to do is transform the variables in this bivariate distribution to a place where the regression works better. And that transformation is going to take. And that transformation is going to take two steps. The first one is that we're going to find an appropriate probability distribution that represents the marginals of both of these. In this case, we're going to choose a normal because that's a pretty good representation of free tropospheric temperature. And we're going to choose a gamma distribution for the unobserved variable, the ozone, because it is bounded below at zero. We're going to do something that's called the probability integral transform. Dorit's still there, so I can try and pretend like I'm a statistician. I'm a statistician. That's basically just applying the CDF, the cumulative distribution function, for the normal to the temperature ensemble members and for the gamma to the ozone temperature members. And it gives you a transformation like this. If we've chosen the right cumulative distribution function, this is actually going to be a uniform marginal, so uniform zero to one for both of these variables. But regression is still a problem in this space. And so we're going to do an additional transform then called the probit transform. The probit function. Transform. The probit function is simply the quantile function, the inverse of the CDF of the standard normal. And it transforms a uniform 0, 1 to an unbounded normal 0, 1, a standard normal distribution. And this is what the probit function looks like here. So it converts 0, 1 to an unbounded. When we go ahead and do that transformation, we now have hypothetically normal marginals between both of these variables, and we can go ahead and do a standard linear regression, which will give us. Do a standard linear regression, which will give us the best linear unbiased estimator in this space. We can compute increments for the ozone variable in this transformed space and then transform back to the original space. And this is what we get if we do this. You remember that before when we used standard linear regression, we had dark blue asterisks that were negative. They were above the distribution. The cyan asterisks are what happens here if you instead do the regression in this transformed space. In this particular instance, we can basically prove that these. Instance, we can basically prove that these are a sample of the correct distribution, but I cannot do that in general, so I don't want to make claims that are too strong. But the bounds are enforced and the nonlinear aspects are respected for the state variable, the unobserved one, in this case. I don't want to spend too much time with limited time, but this transform is really quite simple, and we can show it in mathematical terms. So if I have subscript n here being an ensemble of size capital N. Of size capital N. Y is the prior ensemble, YP is the prior ensemble for my observed quantity, temperature in the example I just gave you. YA is the analysis quantity for temperature, and X is the prior distribution of my ozone variable. I then need to select, as I said, an appropriate distribution, prior distribution, continuous prior distribution. And these Fs are the C D F of the chosen functions. So the F I chose for X was a normal distribution with the For X was a normal distribution with the sample statistics of my temperature ensemble. The distribution I chose for Y, sorry, I switched that. The X is the state variable and it is gamma. The Y is the observed temperature variable and it was chosen to be normal. Phi of Z here is the C D F of the standard normal. And so phi inverse is the probit function. And I end up doing this parent transformation. I first do the probability integral transform and then the probit. I do that for the Probe it. I do that for the unobserved variable x ensemble, and I do it for the prior and analysis ensembles for the observation using the same prior CDF for those two transformations. In the transform space, then I didn't just do my normal thing. I compute the increments in that transform space by subtracting the prior from the analysis for the observed quantities. I then do the standard regression. This is just the same equation that we've been using for 20 years. Just the same equation that we've been using for 20 years to do this to compute the increments for the state variable. I compute the updated values of the state variable in the transform space. That's X analysis. And then I convert back by simply inverting. So that's applying the CDF of the standard normal and the inverse of the CDF or the quantile function for the distribution that I chose. Okay, so I'm going to show a few other examples here before I move into tracers. This example here, I think, is also. This example here, I think, is also relevant to the topics of this workshop. In this particular case, the observed variable is ozone. So, maybe I have some instrument outside that's measuring ozone concentration, and I want to use that to update my estimates of the temperature. So, this distribution now is gamma in the horizontal and normal distribution for temperature in the vertical. And again, I have a 100-member ensemble. If I do a, and the observation here is a gamma, like the example I showed before, so the likelihood. The example I showed before, so the likelihood is gamma. So, if I compute the increments in for gamma distributions and then regress those under the temperature, if I do that in a normal space, I get the blue asterisks. And again, there's no bounds to violate for temperature here, but I still clearly have problems. You can see the residual curvature from the prior. You can see that there are ensemble members that are outside of the likely range for temperature. But if I do the regression in this transform space, I get the cyan once again, and they accurately represent the. Again, and they accurately represent the distribution. So they avoid the curvature problems, they don't extend out of the high probability regions. So, another concern here, however, is that we may not really know the right distribution family, or we may not know the parameters of that distribution. So, fitting a gamma to our ensemble of ozone, fitting a normal to our temperature, fitting something to say a precipitation or a cloud concentration distribution, these can all be tricky things to do. Can all be tricky things to do. And so, in fact, there's ways to easily extend this to non-parametric continuous priors. I'm going to show one specific example of a non-parametric continuous prior related to the rank histogram filtering method, which we started using about 15 years ago. It is the same prior that is used in observation space for that rank histogram filter. And it works like this. We're now again working with just a scalar variable because we're going to do this marginal transformation. Going to do this marginal transformation. I have a prior ensemble for illustrative purposes, just five asterisks here. Now I want to come up with a PDF that accurately represents the distribution that these ensemble members were drawn from. And what I'm going to do here is just assume that these things are uniformly drawn from the right distribution. And if you do that, the same reasoning that leads to the rank histogram methods for validating. Methods for validating probability distributions can be used to generate a prior. And so, basically, how this works is: I have five ensemble members, they partition the real line into six regions, and I'm going to stuff one-sixth of my probability into each one. So I'm going to say there's a one-sixth probability that a unknown ensemble member would be bigger than the smallest and smaller than the second smallest, and so on. And so, in the interior regions, I don't know anything else about these distributions except that I need. Else about these distributions, except that I need that much mass there. And so I'm just going to put in a uniform PDF distribution there that represents one-sixth of the mass. I've got to do something on the tails, and the tails are tricky. I'll comment on that in a minute. But in this particular case, I'm going to do the same thing we've been doing with rank histogram filters, and that's put one-sixth of the probability as a normal tail outside of the extreme ensemble numbers. By the way, these transformations, I failed to mention it yet. I failed to mention it yet, are a form of Gaussian anamorphosis, which has been around for a quarter of a century in this field. I'm not doing anything particularly novel there aside from using this non-parametric form of the rank histogram and the fact that I'm doing the Gaussian anamorphosis only for the regression, not for the computation of the observation increments. So, back to the chase here. I'm showing you a continuous prior PDF that has been defined for these five ensemble members. I didn't have to know any. Ensemble members. I didn't have to know anything about them. And the top frame on this panel is that same PDF, and the bottom one is the corresponding CDF. So that's just the integral from negative infinity to x of this particular distribution. And so this is the CDF, which is what I would use to convert my ensemble to the first step of this space. And because of the way these ensemble members are defined, with one sixth of the probability between each ensemble member, these are a better than uniform drawing. These are a better than uniform draw in this space. They're an exact uniform draw from this distribution. Their quantiles are 1, 6, 2, 6, 3, 6, 4, 6, and 5, 6. Now, I also may want to use information about bounds on these distributions, especially if I'm working with a bounded quantity like ozone. So in that case, I can modify this ranked histogram distribution to say none of these values can be below zero. And so in that case, I want to fit one-sixth of my probability. Case, I want to fit one-sixth of my probability between my smallest ensemble member and the lower bound. And there's a variety of ways that can be done. But for here, I'm basically going to just shove a little bit of a normal in between the bound and the smallest ensemble member. And again, the CDF is shown on the bottom here. That's what I'm going to use for the transformation. Okay, so to show that this works, this is the same example on the left that I showed a moment ago, normal observed, gamma unobserved. But instead of using that information, I'm simply going to fit a rank histogram prior. I'm simply going to fit a rank histogram prior to this stuff. So I'm not explicitly using any information about the distributions except the ensemble and the fact that I know that ozone is bounded below at zero. And I continue to get very nice posterior distributions. They are by definition non-negative and they do a fairly good job of representing the curvature. Given the time I have, I will quickly go through the next two issues. I know there are still some data assimilation experts online and in the room. Room, even if you do the regression in this transform space and life is good, a lot of times you have to do localization for the types of problems that we're doing here. If you apply localization after you've computed these things, once again, you can violate the constraints of your distribution. It turns out that that's not a particularly bad problem for things like concentration, which are just bounded below, can be an enormous problem for things that are, say, bimodal. And this particular picture just shows what happens if you do. Picture just shows what happens if you do localization for a bimodal variable that has been updated in this transform space. So, in both of these figures, the ensemble members on the bottom are the same. They are the priors for a particular quantity. The ensemble members at the top are the same. They are the posterior that was obtained by doing this type of transformed space regression for an observation. If I come back and do standard increment localization, and the value of the localization is shown on the vertical axis. On the vertical axis. In the standard space, what I get is that as I apply more and more localization, my particles just get shifted from the posterior back towards the prior. And they end up in this space in the middle, which my prior says, there's no chance that ensemble members can be there. So why are you getting things from the localization? But if you localize in this transformed space, it knows things about the probability distribution. And so when you apply localization there, the particles, the ensemble members. Particles, the ensemble members jump across the low probability region in the middle. So you can do, you can maintain all the advantages of this transform space for localization. Similarly, inflation, which is much more relevant to tracer variables, is also a problem. If I do my inflation, say a linear multiplicative inflation in a standard space, then I get something like this. In this figure, the prior ensemble from a gamma distribution is shown on the bottom here. The bottom here. As I apply increasing multiplicative inflation going up, things spread out, but you can see that I end up with negative values which are inconsistent with the distribution. If I instead apply my inflation in the transformed space, this shows applying it in the gamma space, this shows it applying it in this rank histogram space, then my bounds are obeyed. I don't violate qualities of the distribution for the unobserved variable when I inflate it. When I inflate it. Okay, so I want to jump to the chase now on an example. So, those of you who are not interested in the details of data assimilation, if you want to wake up again for just five minutes here, I think you may find this interesting. This is a low-order idealized model specifically designed to test PACER transport data assimilation. It uses winds from the standard 40-variable simple Lorenz 96 model. And plotted on the left here, this, of course, is a one-dimensional model. This, of course, is a one-dimensional model. So the horizontal axis of the 40 variable locations, and time is the vertical location. And so this is just stuff from a standard F equals 8 value for Lorenz 96. Now, the novel part of this model is we're going to use those values from Lorenz 96 as wins, and we're going to use those wins to invect a tracer on that same Lorenz 96 domain. In this example, that tracer is coming from a constant source at grid point one, and you can see what happens. The values of Lorenz 96 are The values of Lorenz 96 are more commonly positive than negative. And so, in those times, tracer from this plume here gets blown off to the east across the domain. You can see a variety of plumes blowing off. Occasionally, the winds reverse, and so occasionally you get plumes blowing back towards the west from this distribution. What I'm going to do now is show examples of doing data assimilation where I'm going to observe tracer concentration infrequently with fairly noisy observations. I'm going to observe the Lorenz 96 state. The Lorenz 96 state with even noisier observations. And I'll just show you results for the tracer concentration at two grid points: grid point 14 and grid point 36. While I noticed that in addition to a source here, there's a diffusion and rainout of this particular tracer. And so for much of the domain, there's actually zero concentration of the tracer for most of the times. In particular, you can see that there will be times of zero tracer concentration at both of the points I'm interested in. So if I go ahead and observe. So, if I go ahead and observe the state and concentrations, as I said, these two time series show what's happening at those two grid points. So, the top one is at the grid point that's seen plumes come in from the left. The true concentration is shown in blue here. It's a function of time. So, you can see individual plumes coming through and then times in between plumes where there's little or no tracer. And if you apply a standard ensemble column and filter, problems like this were actually highlighted in some of the earlier talks. You get a green ensemble here. You get a green ensemble here with ensemble mean in red. It's heavily biased away from the lower bound values. It can go to all zeros. I mean, it cannot go to all zeros. It has a variety of other problems. In particular, down here at grid point 36, where you get stuff blowing from the east, you can see occasional happenings of negative concentration values. You can see again the bias. If instead we apply a rank histogram thing, so A rank histogram thing, so no prior assumptions here for both the observation space of the concentration observations and for doing the regression onto the concentration variables. These are the results. So the truth is the same time series. These are now ensemble members from this methodology, the red is the ensemble meme. So things are a lot better. It is nearly unbiased, even though the concentration distribution is strongly non-uniform. It is able to go to all zeros. It is able to go to all zeros. You can see that in particular in some of these cases in group 0.36. It is able to see plumes as they come in and then see when the plumes disappear. So, no negative values can go to all zeros, lots of other advantages. And just a quick note is that people have talked about many other types of transforms in the Gaussian anamorphosis context or in other contexts in ensemble data simulation. Craig's work on gammas and inverse, Craig Bishop's work on gammas and inverse gammas, lots of works on log normals. It turns out that none of It turns out that none of those transformations are able to do this because they either cannot represent probability at zero or they're unable to represent likelihoods that can move towards density at zero. And so there's really some unique capabilities by using this particular distribution. But if you like gammas, if you like inverse gammas, if you like log normals, you can apply them in this same context. We've implemented this in Dart. The implementation in Dart is completely trivial aside from Is completely trivial aside from having to write all the probability distributions and stuff. Simply have to convert to joint state space variables to probe it space once and apply the standard algorithms in Dart. The code is unchanged. We do all the inflation, localization, sampling error correction, all of our other bells and whistles all work. Our parallelization is completely unchanged. We've made a release. It is not fully supported yet, but it will be supported in a few months. And just quickly, efficiency is always a concern for these things. However, these algorithms are However, these algorithms are remarkably efficient. We have run initial tests doing basically global NWP low resolution for the modern era reanalyses. So one degree in a global atmospheric model from NCAR with a couple million state variables using these bounded when appropriate rank histogram distributions. We're assimilating moisture. We're updating all the cloud quantities and everything else. Our results are slightly better than using standard ensemble filters and the total time. Filters and the total time penalty is around 1%, so 1% more costly. And that's because we only have to do these transformations once for assimilation time for each state variable. I'll stop there with a little bit of time for questions. There is a pointer, and as Arthur said earlier, I will upload the talk if you want to see this. So the code is out there if you want to take a peek at it. The manuscript that describes this stuff too is also available on request. It's submitted to monthly weather review, and shortly it'll be going. Monthly weather review, and shortly it'll be going up on our website. A bunch of people have helped me with the discussion of these notes here, and thanks very much for your attention. Nice call. Audience here, Brian. So, Jeff, thank you for your talk. Yvonne here. On here. Back when I was starting doing parameter estimation in 2018, I was looking for cool algorithms that can handle non-Gaussianity for parameter estimation. And I also looked into the Craig Beshop's Giga filter. But then I concluded that it would not do much for me because, indeed, to go back from observation space to model space, it would do this linear regression. So it would not do anything. Regression. So it would not do anything for observed variables. So for unobserved variables, so parameters in this case. So using this probit function, does this solve this problem? So yes, it does, Yvonne. So now basically you will get all the same benefits you got from Craig's gig stuff for the observed variables for any variable that you want to say is gamma distributed. So the remaining, the vast power. The remaining vast power here is you can specify any distribution, including, as I showed, these non-parametric distributions that just use your ensemble. You can specify any distribution. As example of other ones I didn't show in this talk here, but you can do, this is a beta distribution, which Craig's students are currently working on. These are bi-normal distributions, which are relevant for clouds and other stuff. So, whatever distribution you want to enforce for your state variables, you can enforce by this methodology. Well, Jeff, since I'm hearing silence, this is more of a pedestrian question, but given that the infrastructure is all the same, you just do this transform before applying the infrastructure, does this mean that those of us that are historical users of Dart, we can? Historical users of Dart, we can easily convert to this system. Yeah, so there's an easy answer and a hard answer. The easy answer, Arthur, is that you could apply this in your problems and change all your tracers to rank histogram-bounded things in about 20 minutes. The caveat is that there's so much flexibility here that we are being challenged. Here, that we are being challenged in a really lean way to software engineer that degree of control. And so it is not as straightforward for now as controlling the rest of Dart. It requires actually editing Fortran 90 modules, which we don't like, but we're working on. But yes, for your case, by the end of before lunchtime, we could have you running with these. Okay. So we can discuss it in detail later, but what you're saying is the interface is. Saying is the interface is not all on name lists yet, but it involves getting in and changing the code itself. That is correct. Okay. All right. Well, that's trivial. I mean, that's. It's contained. So it's one module, which basically says, this is the module that says what the heck I want to do. Okay. Basically, you have to say for every type of observation and every type of state variable, you have to say, this is the distribution I want you to use. Okay. Distribution: I want you to use. Okay. Oh, wait, I thought you could just default to the rank histogram approach. You can. And so you would say for every state variable and every observation variable, use the rank histogram. And that's in fact, except you have to specify the bounds. Right. And one, sorry, I know others like Ave want to ask a question. How sensitive is it to the way you've closed the tails? Yeah, so for bounded quantities, it is remarkably unsensitive because normally, you know, not a lot is going on there. There is no doubt, and this goes back through the history of the Gaussian anamorphosis literature. There is no doubt that in cases where you have biased priors, which many of us deal with significantly biased priors, that the details of the tails can be problematic. And I did not discuss it here. And I did not discuss it here. There has been lots of research on what to do with tails, and there are options to do other things for the tails besides the normal distribution here. For instance, you can do koshis on the tails, various other long fat-tailed distributions to help you deal with bias. Those are, if those became relevant, I can code one of those in an hour. That's the beauty of this thing: new distributions can get coded up in an hour, but we haven't tested it. Okay, well, thank you. Very, very interesting. Well thank you. Very, very interesting and very good talk. So somebody may have to defeat my question because I'm sitting back. So Jeff, thank you very much. So do you think that there are some disadvantages of using say for example serial assimilation of observation? Because the EPA imports for example locally If you impose, for example, localization, the order of observation can assimilate matters. And then, you know, if we are looking at the impact of observing system, it affects the results. So if you do this organic history, I know you discuss with them. I'm not sure if you can preserve the continuity from one week to next among the sample members. So I was wondering. So I was mandating, so this is great. There are lots of advantages for your method. Do you see any disadvantage or challenges in this proposed method? So first of all, you know, our algorithms have been sequential in this way since 2003. So every result you've ever seen from Dart is sequential. There are some There are some pros and cons to that. In general, we have not ever been able to document cases where there is a systematic disadvantage of doing things sequentially. There is no doubt that the answers are quantitatively different when you're using localization, when you're doing the sequential from doing the other ways. There's been a number of people that have hypothesized that the order in which you do observations can become important. Observations can become important. I have never been convinced. Sorry, there's maybe a couple of authors in the audience, which I will apologize. You can convince me that there's any systematic degradation or improvement in particular choice of the order. So I don't think that the sequential thing has any significant disadvantages that I know of. There's no doubt that the less information you give, Information you give your algorithm about what the distribution is, the larger the errors will be. So, as an example, if you really know that free tropospheric temperature is your error distribution is pretty much normal, then you should just say this is pretty much normal instead of using the rank histogram. If you really know that your ozone when it's close to the bounds is pretty much gamma, and there are some problems with gamma, but if you really know that, you should just use the gamma. So, there is additional sampling. So, there are additional sampling errors when you start doing these non-parametric things because ensembles are noisy, and so that noise will percolate not only into the regression itself but also into the definition of the transforming CDFs. Despite that, I will admit, I was pretty surprised when we threw this thing into the NWP application, and it doesn't get worse than assuming. Doesn't get worse than assuming normals for all the free tropospheric stuff. And it may be that that's being counterbalanced by the fact that we're getting more information from assimilating the moisture observations and the moisture distribution itself is better. But I was surprised by that. So I'm sure there's other disadvantages. You know, all of these things are heuristic. And so every heuristic has its own disadvantages. But at this point, we're pretty enthusiastic about how this is working for a lot of applications. About how this is working for a lot of applications. Thanks, Jim. And for the sake of time, I think we should see the talk and we can continue this discussion. Thanks, everyone.