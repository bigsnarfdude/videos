Thanks for the speakers before me who have given beautiful talks. I've learned a lot in this. Thank you. Second, unlike the other talks, I think I'm not going to be talking about some recent work. So I hope, in fact, I'll be giving a talk with some of you who have seen the thing I'm talking about, and I'll apologize for that. But hopefully, it's old enough that you've forgotten that. So, I'm going to talk about this method in approximation algorithm design. Method in approximation algorithm design. Again, I've given this talk before not to approx algorithm community, so this is a little bit of a, many of you are more experts on this than I am. But still, I feel that this is an approximation algorithm design technique, which is not seen as much use as it could have. So I'm going to try to describe this technique for some recent results on clustering problem. When I say recent results, I'll probably talk about only one result, but then I'm going to say where all the problems are. Say we're all there. Okay, so let's maybe begin. So, this is just the usual thing. We start off our problem, we cast it as an integer programming formulation. So, here you should think of P as a polynomial which is simple in the sense it has you know polynomially many constraints, say. And Zn, the integer constraints make the problem R. So, the usual, the natural relaxation is to remove the integer constraints, and you can solve that since P is easy, you get L P. Easy to get L P and then you have a rounding algorithm. So I would like to make you think, have you think of this rounding algorithm as this abstract thing which takes any point, any n-dimensional point, and lands into a point in the feasible region. So this x may not be in p. It's an algorithm which takes you to the point, a set of solutions. And of course the quality of a solution is the alpha approximation. If the cost is within alpha times the LP, that's The cost is within alpha times the LP, that's the usual way we try to bump the optimum factor. And the quality of this relaxation or is this integrality gap, which looks at the maximum ratio between the LP and the complex. So again, these P's should be actually P i's because as the instance changes, the positive changes. So that's the sort of usual thing. I should have mentioned anytime there is a question, please just interrupt me. I would prefer that. I would prefer that other than other okay. So there are many problems. We write the natural L P relaxation, the integrality gap turns to be very bad. So the next thing we try to do is strengthen the L P. So what does strengthening mean? We want to find this Q, which is another polytope of context, which sandwich between the integer points and this problem P. So I'm going to try to minimize that. And since it sandwiched the value of this The value of this L P is going to be long direction. Yeah, sorry for that. So it's going to be better. Okay, it's going to be better. And so the integral gap is going to be hopefully smaller. So where do we get this Q? So one way is to sort of, you know, you can write, you can add more variables and you can project this where the hierarchies come from. That's one way you can get Q and you can try to solve this. But sometimes All this. But sometimes the Q itself may not be a compact formulation. It might have exponentially many constraints. But even then, you can maybe solve the LP by resorting to this beautiful algorithm that we all know and love, the ellipsoid algorithm, which says if you can separate over the constraints in Q, then you can solve it. So I'm going to sort of again say what separate is because this is going to be the key thing. Given a point x, there is a way whether I can certify whether x is in q or I can actually certify x is not in q by. Or I can actually certify it's not in Q by But sometimes the router cut method is actually goes one step further is sometimes we actually don't have separation oracles function you might not have and even then use the ellipsoid algorithm so. So, the skeleton of this is this and then I'm going to so this is going to be very generic. So, let opt be this guess and right. So, this is I am just And right, so this is uh I'm just sort of retasting it. So Cx less than op x isn't q q is sort of the positive which might be hard to optimize over. So this is non-empty for a correct guess, if my opt is actually the correct guess. So the paradigm starts with this as follows. So you start with a with a point, x0, and I say any point, but it should be sort of not too crazy, not too far away from the optimum. And then you try to round it. So this is this rounding algorithm. I mean you round it and you get a point in the feasible region. In the feasible region. And if that feasible region's cost is already within some alpha times of, and alpha is the desired approximation factor for, well, then it's great, I'm happy. What does happy mean? It means that if the guesses was too big, then I'm going to have the guess, make the guess smaller. But if that guess is correct, I've got an alpha approximation. But if the thing is not correct, then I would like to cut this point x, not g, cut this point x from this point. point x from this point q. Again what does cut mean? I want to find a separating hyperbolium which separates q from x. And this cutting is sort of inherently connected with the rounding because I am going to use the fact that my rounding algorithm failed in the cutting. Because this round and cut are integrally tied together. If we are able to do that then I can feed this separating hyperfluent into ellipsoid. Ellipsoid will give me a new x completely different from the other x whatever. Completely different from the other x's, whatever it does, very far away, and this procedure continues. What happens? The ellipsoid algorithm proof shows that in as long as your x0 is not super crazy, it's not too far away, in polynomial many iterations, either I will reach this happy place or I will prove that this polytope is actually empty, which means your guess was too small to introduce. So, that's the structure. So that's the structure. Any questions about this? If this was too high level, I'm basically going to try to show this for the clustering problems. One thing that I want to make you remember is that this Q sitting between P intersect Z and P, it could in fact be P intersect Z. It could be the integer hub of the points, which is NP hard to optimize over, of course. And we'll see that the problem that you look at, indeed, it would be. The problem that you look at, indeed, it would be that. Okay, so let's talk about the clustering problems. Again, I think Yarek gave a talk on facility location, so we sort of again know all these kind of problems. So these are center-based clustering problems. So the input is going to be there are two sets, client C and facilities F, they both lie in a metric space. And the output space would be to open a collection of centers. And as soon as you open the centers, And as soon as you open the centers, for this today's talk at least, every client is going to go to the nearest center and there will be a cost D V of S that we pay. So I'm just saying that we're not forming an assignment as well. So once you open the center, every client goes to the nearest center. And then the objective function that you want to sort of optimize is some function of these things. So the problems we're going to look at today, we're going to look at the center objective. So we're going to look at Objective. So, we are going to look at the case center problem. So, I print asterisk because it is actually the case supplier problem, but forgive me for not calling it supplier. The constraint for this is you have to open at most case centers. And the knapsack center problem is the generalization where every facility actually has a weight on it. The weight is not all one, and the constraint is that the total weight of the facility is over three o'clock. So, all the weights are one, then this is a case, yes. So, that's those problems are very well studied. And the objective is that I want to minimize the maximum. That's the center of the. So, I want to open this center so I want to minimize the maximum of any distance that the clients travel. So, that's the quota the classic or vanilla versions, but we'll be more interested in the outlier versions where basically you know the center, the set of plants could have points that are far away. Plants could have points that are far away, but this objective might become too large. It's sort of to bypass that hard difficulty. We want the outlier version. So the outlier version of these problems have an extra parameter, a parameter M, which says this is the number of inliers, number of people you want to cover. So not only do you want to open the set of centers, you also want to figure out the M clients that you will cover. So that's the set of T. So your solution has to give both of them. And you want to, the objective now is. The objective now is maximum only over the search making times. Okay. Any questions? These are all classic problems. Many of you have seen this and many work companies. So there are four problems, right? K-center and abstract center, without outliers, with outliers. So the without outliers, we have known a 3-approximation from 80s, and in fact, it was 80s and in fact, it was David's PhD thesis, yes. And in fact, it's as hard as three. With outliers, the problem becomes interesting. And sort of the maybe the one theme or one question to take away is, are there problems for which if you add the outlier constraints, the complexity does it drastically change or not? And for these two problems, the answer is no. But it took some time to get there, right? So the K-center with outliers, The K-center with outliers, there is also a three-approximation. And the first is a beautiful combinatorial algorithm. So these are both combinatorial. We'll see these algorithms in it. The first three approximation algorithm we'll not see today is the one by Chariker, Kuller, Mount and Varsiman. It's a greedy algorithm which gets a three approximation. But then we also, so this we work with Prachi Girl and Ravishankar Krishna Swami, we actually give an L T based algorithm and that's the one that we will talk about because it leads us towards the rounder cut frame model. Us towards the round or cut frame model. And again, for the experts in the room, when you really have the real center problem, the set of facilities and the clients are the same. So Charekar et al. still gave a three approximation, but this paper actually got a two approximation. So there was some benefit to it looking at it from an LP perspective. But this problem is the one that is the rounder cut method I would like to exemplify on. That took a little more work. That took a little more work. This is a work with my student, former student Mariam Nagabani. And this is where all LPs that I know of have time gaps. And in fact, I don't know of a combinatorial algorithm to get this. Three approximation, in fact, any constant approximation. Might exist, but that's something to think about. Anything that bypasses this helps right out. Okay, so the plan for the next half an hour I have is to actually go over these these three algorithms. So K center with outliers, why can't you think of it as K plus M, okay? I mean increase the number of... So the K is on the facilities and the M is on the clients. So it's not, I cannot increase the number of facilities and get away with it. I have to decide which client is smart. Other questions? Okay, and and uh connecting with the uh where I guess the oct throughout the talk I am going to assume that I know the guess of the optimum which is the the min max right. So, what is the maximum distance? So, I am going to call it r and all we are going to sort of say is we are going to either find a solution with radius at most 3r, so everyone is with an atmosphere. At most 3R, so everyone is at most 3R from the open solution, or I will prove that R is a bad guess. That is going to be the strategy. After that, binary search will solve the problem. So, we assume that we know the optimum solution. So, this is the same as the guessing. So, here's the algorithm of Dorit and David. This is case center, vanilla case center, long players, nothing. I pick an arbitrary center. I pick an arbitrary center and I call it a representative. I put in a representative set. I open a ball of radius R, R is the guess. I open a ball of radius R. Now this better contain a facility. The optimum solution has a facility with them. I open an arbitrary facility. I just open it. I draw a ball of radius 2R around that representative, and all the clients that are there in that ball, There in that ball, they become the representatives' children. So, the representative is going to take care of this children and I remove them and I repeat this procedure. The set of clients are coming. So, I am going to pick another arbitrary client as my representative, radius R, open a facility, radius 2R, each of clients. The algorithm clear? This is the Hoffcombe Schmidt algorithm. From two observations, every client is within three hour distance from some open facility. Why? Well, it's within two hour distance from its representative. Representative is our distance from an open facility. Triangle may always be that. That's the three that is going to be there for all of this. So why am I open why am I open only at most case centers? That's the problem. Well, any two representatives are strictly more than two hour apart. Are strictly more than 2 hour apart. That's by design because representatives open a ball of rig is 2 hours in need. So, if there were more than k centers opened, then I would have more than k plus 1 clients which are pairwise 2 h apart, but then no solution can cover them with only k open centers. So, that's then my guess is wrong. So, that's the that's the argument. So, that's the so this three So, this 3, this 3r will be 3 for all the algorithms. This is the argument which becomes more and more complicated. Okay, good. So, I want to point out that I have picked an arbitrary center here. This is arbitrary, this is arbitrary. If you work for the knapsack center without outliers, all you do is you don't pick the arbitrary client, you do it uh you don't pick an arbitrary f, you pick the f with the smallest weight. You pick the F with the smallest weight. Okay. Good. So I'm now move going to move to A center with alpha, not an abstract center. I am bringing the L P str. Okay. So what is an L P? So L P is this natural L P to write. Well you decide you decide which facilities to open and you decide which clients to cover as in client. So every client will have a cover. So, every client will have a coverage. And what are the constraints? Well, there are two natural constraints: I have to open case centers, I have to cover at least n people. And the thing connecting X's and the curves are the following, that for every client, if I look at a ball of radius R around it, so if it is covered, there must be someone open around in that ball. So, the coverage of a client cannot be bigger than the amount of facilities. Amount of facilities opened in any distance are. That's it. So that's a natural LD. So if my guess is correct, then this must have a feasible solution or other if this doesn't have a feasible solution, then I know my guess is too small. So we want to now round this. So we are going to actually use the same hop-functionalized algorithm, but it will be guided. Algorithm, but it will be guided by the LP. And so, here was the, so I'm going to sort of use this tool. So, instead of arbitrary, we start by picking the client with the largest coverage. We pick a client who is supposed to be an inlier. We draw a ball of radius R, but instead of committing and opening that facility, we select an arbitrary facility and just keep it. We tentatively keep it in our bag. Keep it in a bag. And we continue. We follow radius 2R, meet all there, collect children. I am responsible for my children. I continue. So, next, this is the one with the largest amount remaining. I select this. I pick some children. Children. Remember, this one has no one, but it still has some children. So that's So that's every set of representatives, each representative have some number of children allocated to them, and some of them have a facility also selected. Just to see everyone, what will be the curve of this fellow if there is no facility here? Zero. Because the curve can be at most the total stuff open if there is nothing open, this will be zero. But I'm still good with working. Will be zero, but I am still good with it. Okay, now the algorithm changes a bit. I have not opened these facilities. So each representative has some number of children. I put that weight, the number of children on their head. So this is 5, 2, 2, and this one also has 1. And now we open the, so we look at the largest K numbers and if they have selected facility, I just open those facility. So I'm going to open gates. So, I'm going to open kH centers by design. That's the algorithm. This is an important point to note. Although I solve my L feed which has xfs, this algorithm only uses the tubs. So, this is a rounding algorithm which is only using the coverage, the cover fees. Would it make sense here to look at the sum of the coverages of the children you have rather than the number of children? Have rather than the number of children? Good. The sum of the coverages of these all the children here will have coverage less than the representative, and that's all that you need. You'll see that in a bit. So just the number of children is good. Good. Okay, so what can we say about this algorithm? Again, every okay, I never said, so sorry, so I opened the So, I open the K largest facilities and I cover the children, and every other children there aren't covered. So, that's my set of inliers. So, every inlier, every person have decided to cover is within three hours from an open center. Same reason as before. Of course, I opened case centers by design. So, what is not clear is why have I covered at least M clients? That is not clear. So, let's see why it is. So, what do I know about the sum of these coverages? So what do I know about the sum of these coverages? That's at least m, because fractionally I'm covering at least m fellows. I'm now going to divide this as you know I sum over all the representatives, sum over their children, their cups. Because the children have partitioned the clients. And now I'm going to use this greedy rule. Each child's coverage is at most the coverage of its representative. So I get this inequality that this sum Quality that this sum here is at most the coverage of the representative, which is V, times the number of children, just the number of children, which was the thing on their head. So what do I get? I get that the weighted sum of these numbers weighted by the covert is at least m. So in this example, I get 5 times covered plus 2 times covered. So it's this linear equation I get. That's 1. 2. What can I say about the sum of the coverages? What can I say about the sum of the coverages of the representatives themselves? Well, so this is the thing. So note that this coverage is at most the total X mass in this yellow ball. That's the L P constraint. So therefore these sums are at most the total X mass in all the yellow balls. But the yellow balls are disjoint by design. So that is the total mass, total X mass. Is the total mass, total x mass altogether, it is at most k. So, I have two equations here, okay. So, fractional solutions which satisfy these two equations and therefore if you take the top k among them, among these coefficients, they must sum to at least n. That's it. So, we are taking the top k things and taking the children, and those children are definitely covered. So, I am covering at least. So, that is the So that's the this is this leads to three questions sort of in your I need so if I if I find clients with if I find any set of representatives or whatever with these constraints then I can get uh that number. Of course. The number, of course, where five means number of children. So the partition of the clients in this table. So no client should be overcome. Can you slash the P of B? Can you slash? Can you show the PM? Sure, I can show that, but I can also describe to you in words, which is right. Isn't the LP is just saying that the coverage of the client, if I draw a ball of radius R, there should be enough X mass. You have a follow-up question? No, I was wondering why can't you eliminate the X-ray, maybe related to yeah, of course you can eliminate them. That's what I say. Yeah, so it's so you can eliminate them and you can only work with the cove and that's the that's important when you come to the cove world is also. Important ones, the cover is all that. Good, other questions? So let's move to the interesting part. Okay, that's exactly right. So of course you can write the natural L P, right? You can imagine what the natural L P is the only change is you should have gone forward and come back. You don't have foresight. Do not have foresight. So, the natural L P is instead of saying that summation xf is at most k, I just said the weighted summation k. And if you work with NAPSA, you can see that this L P is not going through. So, let us look at a concrete integrality gap. So, it is a very bad integrality gap. So, let us look at this example, Sito example. So, here is an example. There are two facilities A and B. They have six clients around them, around this radius one or r or whatever. But their weights are different. So, the weight of this facility is 6 and the weight of this facility is 1 and the budget is 6. So, you cannot open both the facilities. But, m, the number of inlayers that is wanted to be counted is 11. So, clearly there is no solution, right? But how does the L P cheat? The L P cheats, how L P cheat cheat. Well, LP cheats how LPs cheat often by you know opening this facility only to a 5/6 amount and opening this facility or now since it's 5 sixths it only pays total weight of 5 plus 1 6. So it is fine. I have opened yes coming. Is that oh so just wanted to write? Okay. Can I move on? So good. So what happens if you open it to 5-6? Well, the only thing that happens is these coverages of these clients are only 5-6. But you know, this 11 was chosen for a reason. So the 5-6 covers 5 clients here. These coverages are 1. So total coverage is at least. So so total cover is at least m. So this is a feasible solution for r equal to whatever this r is, r equal to 1, but of course integrally answers this. Okay. But we have a rounding algorithm. We had a rounding algorithm. So why doesn't that rounding algorithm work? Again, going back to the rounder cut, you have a rounding algorithm and you ask why it doesn't work and if you can use that fact that it doesn't work to cut off this bad Work to cut off this bad healthy solution, then download cuts as you win. So it doesn't work because if you try the same thing, this is going to be the highest tough, they eat up everything. So this person will be the coverage with six children. And there's one, so I have to change the algorithm slightly, as in just like in the knapsack center, when you open a ball of radius art, there can be many facilities. Who do you select? You select the one with the smallest. Select the one with the smallest weight, but in this case, of course, there is only one facility here, so it just selects that to open. So it has 6, and you can do the same thing here. So the two inequalities that we had, well we still have 6 times cuff p plus 6 times cuff q is at least 100. That was one of the constraints. But the weight, we do not anymore have cuf p plus cuff q at most you know 6. You have 6 times cuff p because the weight of this a is 6, you have the weight of this. A s6, you have the weight of this nearest facility selected facility times the curve plus 1 times the distance. So at this point, there are two things, right? So the rounding algorithm doesn't stop here, it tries to find a solution for this set of, 0 1 solution for this set of inequalities. So, but this is an easy question in the sense I want to maximize this with subject to this constraint. So, that's just a knapsack. Constraint. So that's just a knapsack problem. But wait a second, knapsack and NP hard. So in general, how do I even find the maximum value of this subject to this constraint in polynomial time? See if I'm weighted, this is all the numbers are small. It's not unweighted, the numbers are small, right? So what is small, not the weights, because these weights can be arbitrary, but these numbers, the curves, are the numbers of smaller. These numbers, the calves, are the number of children, which is at most n. So it's a polybounded nap. So this is a problem that you can solve. So you can solve the maximum value of this subject to this constraint. And of course, this is not 11. So this does not have a 0. But you basically say, you use this, and this is the part that I'm going to talk about more in the principled way next, but you can look at this fact that this does not work to say that this is actually, this inequality is actually a valid inequality. So you add that to your constraint. So, you add that to your constraint and then this curve, this x and this curve is killed in your constraint. So, this part is very tailored to this example if you are wondering. So, but the question is is there a principle way to get these things? Okay. So, again, so we have this rounding algorithm, in this case, this LP-guided half function was. LP-guided how function works. And we want to see whenever it fails on these curves, it only uses these curves. Can I find a hyperplane to slice this curve away? So to understand that, what is the set of value inequalities that we can have? So actually, in this case, for this problem, we are going to look at the integer hull. So the curves, which are the fractional curves of there, they what is the integer hull of this? Well, I can go a little faster. Can go a little faster. Basically, you look at all possible solutions. So, this is a solution where this weight plus this weight is at most k. I look at all the elements that are covered. So, in this case, these have tubes 1, everyone else has a tub 0. I look at another solution where this weight plus this weight is at most k. Look at once for these blue guys, the 0 for the rest. I take a convex combination, that's a valid curve. So, that's how you get valid curves. So, again, these slides are, I can go faster for this audience, but I can go faster for this audience, but let me just get there. So, the set of valid terms is just this: I have a variable for every feasible solution. Feasible solution is a solution whose total weight is at most k. It's a convex combination. And cove V is, I look at all the sets where the set S covers V, which means that the distance from V to S is at most R. Most R. This is just a, you know, I'm just writing the integer hub of all the possible feasible things. That's this object. Let's see. So, in some sense, this is sort of the best LP relaxation, right? So, I want, I'm given these curves, there should be at least m, but they also should be arising out of the integer combination of a complex combination. integer combination of a complex combination of integer solutions. So clearly we cannot check the feasibility of this but we can run the round or cut framework. So let's again revisit the round or cut framework. What is it going to be? We're going to start with some curves okay. Maybe there's curves which from the natural LP. Why do you say you can't be this seems like the configuration LP, right? So why can't you solve this LP? If you solve this then you will find a Solve this, then you will find a problem in P integer half because then you will solve it in P hard of it. It's too much, two configurations are too many configurations. So again, so we will start with a taught which is supposed to be in the integer hub, right? I would like, give me something in the integer hub. I run the LP guided Hogbombschmois, get an abstract problem which I can solve because the numbers are small. And once, if the numbers are small, Small. And once, if the numbers are small, and I get the total value is at least m, I am happy, right? I am happy and I will have my 3 approximation. Otherwise, I want otherwise I want to find a separating hyperplane. We need to start getting boots. But how do I get this? That's the thing. Okay. This is the again, so the key takeaway is that you find the valid inequalities via Parkashi, by the way. So you have this set of. So, you have this set of cubs. So, you can sort of see if curve is not in this, if there's this is not feasible, well then something else is feasible. So, you sit and write what it is. So, you look at the dual of this. And the dual of this will look like as follows. So, if there is a, so let me state it this way: if the curves were indeed feasible, then the following statement is true. Let lamp. Let lambda v's be real numbers, could be negative as well for every client, which satisfies the following property: that for any subset of f which is feasible, so total weight is at most k. If I look at the clients covered by this set, their number is strictly n. So there's a mouthful, so let me go over it. So lambda v's are real numbers on these clients with the following property. With the following property, which basically says if I look at any feasible solution and look at the lambda value of the clients covered, that is at most 1. So, if the lambda is equal to 1, then if this was true, then there is no way I can cover m. But lambdas are not 1, lambda can be hidden. This is a hard thing to check. But if I find lambdas, if one finds lambdas that does satisfy the property, then if cubs are feasible. Then, if cubs are feasible, it must be the case that lambda v cup v is also must. Again, if one, if I think of lambda v as 1, which means that I cannot cover m clients, then you cannot have summation cup v at all. This is the generalization. This just comes from the dual. This is a mechanical process. So, believe me for this. But, what is at all not clear where will I get this lamps from? So, here is where the round and the cut procedure. Is where the round and the cut procedure come together. That if I am not able to get the three approximation, if my knapsack problem that I had set up does not have a solution, then the knapsack problem, the children, the values there, they act as a certificate as the lamb does to cut this thing off. So, I will so if so, I set up this problem, so I get a knapsack problem, and if the resulting knapsack problem And if the resulting Napsak problem fails to cover M clients, well that means that no matter where you picked K things, so that's so the S is the set of M. I'm looking for an abstract problem, I'm looking for a set S with W as equals K. But it cannot give profit of M. If it did, I would have found it and I would have won. But it does not have a profit of M. So those lambdas precisely. So lambda V equal to children V, to be honest. I don't have written, but lambda V equal to child V for all V in the representative. Child V for all mean the representative lambda v equals 0 everywhere else, they act as a certificate to show that that curve that you started with is not in. So, you then feed it back to ellipsoid and run the round of that procedure and at some point you will either guess the guess of radius is too small or if it is not you will get something. This one? Yeah. This is a type 1. This should not be. It should lambda be curved over all of the values. Thanks. Just sum of V over C. So, yeah, so lambda V summed over all of these at most time in the validating mode. So this gives you a kind of lifting that is not as tight as the pole. I mean, if I think about it, I'm going to. As tight as the full. I mean, if I think about the full potential set of inequalities, whether it's generated by the algorithm or not, it gives you a characterization of a set of inequalities that now lies in between your integral LP relaxation and your compact cell. When you go about the integral LPA, that LP. It's three. It is to you there. Yeah, you cannot do better than three because your hardness is parallel, yes. But I should point out, so maybe I'll, because I was going to stay. So maybe I'll because I was going to state this what you said. So the integer hell is too hard for me to optimize. But what this procedure says is there is a poly for this instance for every instance there is a polynomial set of inequalities that is enough to give a 3 approximation just LP in the idea of 3. But I don't think you can find it a priori. Of course all the all the separate all the strong and strength winnings depends on the instance but this sort of Depends on the instance, but this sort of depends also on the in some sense the cost function. So, cost function is the maximum distance, it sort of is very instance-oriented. So, the rounder cut method finds these hyperplanes which are enough, which suffice a posteriorly. It's hard to find an a prime. So, I'm going to push back on your answer because I mean I was describing, thinking about not just a polynomial that could actually get generated by this, but there's an exponential design. This, but there's an exponential description. And because, but it may be a P, not a NP heart rate, a co-NBA intersect NP kind of issue, because I might have this polytone which I don't have the ability to extract constraints from there, but I would still have a hardness with respect to it. I mean, I would end up with that B equals that B intersect 2. Yeah, so good. Yeah. Any other questions? Maybe to follow up on that. So this basically proves that this smaller polytrope you find, relaxation you find as integral 3 but not the original one. The original one has a bad integer, yeah? But I also find the compact representation, running it online, and that final thing will have a yeah exactly. But it's what I was trying to say that it's a priori hard to. What I was trying to say is that it's a priori hard to find. Right. We only have to find it via. So, for every instance, there is a relaxation. And that's a little bit like what Table is saying. That if somehow you've got that quality of them, you can then be checking. Good. So, let me sort of maybe try to elaborate the difference between this and the usual strengthening. The usual strengthening, you strengthen, you find valid inequalities that should be true, solve it, and then you run. While here, you find this polynomial many constraints after the fact. And you know, it's a very And you know, it's a way of thinking about the whole thing slightly. So it depends on the variation of some LT solution. For this LT solution, maybe this set of polymorphonal transparency matters, for this a different LT solution, it's a different set of polycastra. But in the end, you do get a contact. Yes, yes. That's what it is. And this is a volume of social capital, I suppose. Other questions? Other questions? Yes. I'm going to ask another recent thing. Would you actually have a concrete example that another deep logic one? Okay. Good. I actually don't have much more. So this was actually work with Miriam done, as I said, 2018. So after that, she and her thesis was basically using this technique to solve a bunch of clustering problems. So these are clustering Clustering problems. So these are clustering problems which are clustering plus some extra constraint like outliers. But recently there are many other constraints like I think the outliers problem is what David mentioned in his joint replacement, the punishment problem and fairness is another set of constraints. So there has been a lot of work on like fair case center clustering. There's a problem called non-uniform case center which I can tell you afterwards. But I want to maybe how much time we have? There's a parallax error. There's a parallax error here. Okay, this will ending up. So, I want to tell a little bit about this problem. So, this is a continuous question. So, I'll just tell you, I don't have a slide on it. So, your K-median problem, right? So, K-median, as I said, you have clients, you have facilities, and the facilities are given to you. So, you can run polynomial time facilities. But imagine the situation that the set of clients lives in a metric space, an abstract metric space, which is much much larger than C. So, concretely, suppose your metric space is R to the D. Suppose your metric space is R to the D and your distance function is say L2 or L infinity. Let's say L infinity for a reason I'll talk about. And the set of plants is now C, but the set of facilities is potentially any point in R to the T. But now you cannot even write an L P which has a X F for every facility. So what can you do? So this is a continuous K-median problem is given points in this abstract metric space where the facility can be open anywhere in the Where the facility can be opened anywhere in the set X, but X is much, much larger than C, so you cannot really find it. How can you get an approximation? So, one way is: I'm just going to say I'll open facilities only on the clients and that gets a factor to hit. That's easy to show by client for K-median, a factor to hit. So, whatever your best K-median algorithm is alpha, you have a two-alpha approximation. Alpha keeps growing down and down. But what's this two hit needed? Needed. So, a beautiful paper of Jung and Gohanadhar and Karthik shows that the following problem is hard to approximate to better than 2. What is the problem? R to the D points in R to the D, L infinity metric, K is equal to 4. So, only 4 centers. So, the discrete problem is trivial. I enumerate over all centers. But continuous problem K equal to 4, you cannot do better than 2. NP hard, I think, UGC, you get K equal to 2. I think UGC, you get K pool 2 is now. So, this proxy, so that sort of got us thinking. So, what can you do? So, what we could do is that the old algorithm for k-median of David, Heva, Moses, and Studito, which gets 6 and 2 third, that we could port in to work even in the continuous case. So, you cannot write an L P, but the algorithm of you know CGST algorithm only. CGST algorithm only uses this fractional costs and rounds it. So, you can think of working the rounding algorithm as a fractional cost algorithm, and as long as it satisfies certain properties, you are fine. And if it does not satisfy, then you can cut it off from this huge humorous thing. So, that is what this one is. So, again, this is very fast, but I sort of wanted to say that this round of cut algorithm has, but it does not get better than the 2 alpha. Current best, alpha best is less than alpha. best is less than alpha C G S T by 2 unfortunately unfortunately good good news. So okay so that's that's all I had to say. To add to this please those capacities where I thought it's probably quite yes yes this is no longer nowhere comprehensive list. In fact I maybe I In fact, I maybe I'll say this before pressing the RAM button. I did a very short job of giving any history of this problem because I would miss many names. So I did not even say the first paper which introduced the rounder-pot method. But that's a paper that is by Carl Flischer, Phillips, and Hume? Hume. But I was actually looking at that paper yesterday. They don't mention Roundbroker anywhere, of course. So the name, I don't know where it came from. So the name, I don't know where it came from, and it was too late. Maybe David. But one historical comment: that this shift from K-Center to K-supplier, we really should give the credit of the hardness in that to Karloff. To Karloff? Yeah, so we cited in our paper that the hardness of three I could only get a factor of three, and I was trusted. Three, and then I was frustrated, and I complained to him. He immediately came back and said, Well, you know, you take the Schoenylhauser piece and just lift it to the five parts and sit. Okay, good. So we give him credit as an acknowledgement, but he didn't want me to think it was appropriate for co-authorship. But if you're going to cite an artist or something, okay. Thank you. I'm done. 