I will start with David Cove Castro, who is in the University Corporate Marine and has been for two years in October doing things related to aggregation and diffuse. Please, Larry. Thank you, Juluis. And let me thank the organizers for the very kind invitation. It's always nice being in Granada. It's my third time. It's my third time this year already. My department started to get a little keys. So, why do you also mess? And so, I'm happy to talk about this series of two works that I've done with Juan Luis and Jose Antonio. So, the end of the talk is going to be understanding this equation, which if you want to see it as a does this. Does this have a laser? It it does not. Okay, but so this equation that um I can try it like this. Maybe this one does offer laser. Ah, this one has a laser. Okay, so this equation that if you want to invert Laplacian, you can just put Laplacian, minus Laplacian minus one of rho on top. I will give the motivation for this, but uh, just to start with the equation that we want to study. So, this has been. Want to study. So, this has been a series of two works for the cases alpha between zero and one and alpha greater than one. So, the first one is called the fast regularization. This already spoils sort of the result of the Newtonian vortex equation. And the second one, for alpha greater than one, that also appeared last year, is a vortex formation, which also spoils sort of the surprise of the result. And there have been two works I've done with Jalruz and Concentonio. So, you know, okay, I told you I'm going to talk about this PD, but why do we care? So there's two arguments of why we care about this equation. So there's a famous paper by John Chapman, Rubinstein, and Salman that says that you can get to something like this as the limit of the Ginfut Landau equations. And you have a V here, so this paper is from 96. So, this paper is from 96. You can do it in R D or in a bounded domain with boundary conditions. And there was a later paper from 99 that said that in fact you can remove the first order term from that, then you end up with this. And so, this equation was studied by many authors, just three of the main papers on this. And then, you know, these are the in the aggregation diffusion family. You take the Newtonian potential, and then you end up with something that looks like this, right? And so, I don't need to talk much about Pasterstein gradient flows and a lot, because a lot of people have done this for me. So, essentially, you have this, sorry, this gradient flow structure for those problems, which corresponds to alpha equals one in our settings. And okay, this is what's usually. Okay, this is what's usually called the linear mobility case, but in fact, you can write M of ρ here, and you can think about the adapted Passenger distances, which are only distances if M is concave. There is this really nice paper by Carrillo Lizini, Sabari and Slebcek. So beauty is that you have many of the authors, so you can ask them here. And they talk about non-linear mobility equations. So the philosophy that you still have. Use the philosophy that you still have the gradient flow of the same energy, but you're doing the gradient flow in a modified metric for the spatial probability distributions. And so, in fact, you can just take the mobility rho to the alpha, and this is the kind of equation that we have. So, this will be our equation for the day. Well, at least this is the equation for the density. But the equation for the density is too high. So, you look at the equation and you were in Paluis's talk. You were in Paluis's talk, so you always try to look for nice solutions. Two kinds of nice solutions are constant in space solutions, and then you know you solve the ODA and you get precisely this. Or you can let this go to infinity and you get the friendly giant, which is a solution, constant solution coming from infinity and pushing everything down. So everything has to decay like e to the minus one over alpha or faster. So these are nice solutions if you would have a comparison principle. And you can also Have a comparison principle, and you can also do circimar analysis. And if you do self-similar analysis, then what you get is that it depends on alpha. So, if you have alpha between zero and one, you have this nice self-imminer profile that you can rearrange so that the mass is one. And if the formula works for alpha greater than one, but then you get solutions that are not, the similar things will not have finite mass, they will go to infinity on either side. infinity on either side. And there's alpha equals one, you have this vortex. And the vortex will be surprising because it will be, so the picture is never this one, it's always this one. But the thing is that this will work also for alpha error equal than one. And this is the surprise. The case alpha equals one had already been covered by Serfati and Basque, and we wanted to do the other things. So since doing a theory in row is very difficult, you look only for you're humble and you look to the If you're humble and you look to the case dimension one and you take the mass variable, the mass variable is nice. You integrate the density and you get the mass. You look at what equation you have, you integrate this, and you get a nice equation. And in fact, you realize that you get like a nice first order equation for the mass, which is a rather surprising guy. Notice that I had like the derivative of something, so this is the rho to the alpha, and this is the other guy, which in the mass equation looks supernatural. Would supernatural, and this is you know a type of Hamilton-Jacobi equation, which for alpha equals one is precisely Burger's equation. So, alpha equals one somehow is already understood in this sense, for alpha equipment one is not, right? So, Burger's equation for alpha equals one. And when you get to Burger's equation, everybody's supposed to know everything, you can leave, but you can do alpha different than one, and this is new. And in dimension greater or equal than one, what you do is you say, Okay, can I do this? Well, Can I do this? Well, in dimensions greater greater than one, we do only radial solutions, dimension greater than one, and we integrate in balls, but we don't integrate, we don't set I as the variable for the mass, because the equation becomes annoying. You set the volume. So you integrate m of t, v is the integral over the ball of volume v. So v is the volume of the ball. And then you work a little bit and you end up with a nice equation. Little bit, and you end up with a nice equation. The same equation, in fact, so it's dimension independent, which is a rather nice surprise. You're happy, and then, of course, at time equals zero, you give the initial condition. At volume equals zero, you're integrating at zero, so you get zero. And then, you know, at infinity, you hope for mass conservation, so you can set the mass conservation, but you can decide not to put this on the equation with conservative. It's fine. And then, you know, once you get one of these equations, you want to see how it goes. Equations, you want to see how it goes. And then you pick a numerical scheme. Beauty about numerical schemes, you can pick them however you like. And we've seen explicit schemes, or explicit, erased, but you will turn someone, implicit schemes, but you can also pick every term in different way. So we picked the mass, so the derivative, the non-linear part explicitly and the other part implicitly, because the more implicit your scheme is, the more stable it will be. More stable it will be, it will have better properties, but the more explicit it will be, the easier it will be to compute. So, this is a nice balance because you can solve. And you solve, and you find that the next step is just computed from the at the previous step, the one location and the previous location. This is the upwinded scheme because the information is moving to the right, and we'll see that. And where g is this nice function, so you can make it explicit. You can make it explicit. And your h is s to the plus. So I just take this and solve for mj n plus one, and my life is like. Of course, you said that at j equals zero, you said it's to zero because the mass of zero was zero. So for the scheme to be monotone, to preserve the, you know, you have initial conditions which are ordered and you want them to remain ordered as you solve the scheme, you need G to be monotone in each variable. If G is monotone in each variable, Each variable. If G is monotonous in each variable, if you have two initial data that are ordered, they will remain ordered, which is very reasonable and happens always in these kinds of things. So if you are doing the CFL and what you need for that, you end up with a CFL condition, which relates your discretization in space and time with something else. So if alpha is greater than one, we have this decay of the density, and in fact, we have a nice DFL condition. Have a nice DFL condition. And where alpha is less than one, you have like a square root singularity, you have to sort of solve it by a little truncation. So people in numerical methods know this much. You truncate this line. And now you have a scheme. Of course, this one has a delta, which you have to scale with the space screenization and the time spectration, but it's fine. You can work with it. And now you have a CFL condition for alpha FN1, which is once you pick delta and you pick HV, you need to. Delta, and you pick HV, you need this condition too. And you know, before we continue, we see what they do. We just plot them. This is alpha less than one. Solutions become, appear to become positive, not a problem. Alpha greater than one, so it is in 2.0 in this case. It does something weird and it sort of wants to look like it wants to do like vortex. Okay, the numerics are fun, but then you need to prosthum. Merics are fun, but then you need to consider it. So, otherwise, you know, this takes like maybe a day or two. You're not getting the paper published with that. So, you do some extra work. So, you can look at the first order problem. You go to the big bosses, you ask Evans this book, and then you say, I want to solve this guy. And they say, You can do characteristics. So, you can solve along lines and you can decouple some certain curves. To decouple the curves, To decouple the curves, you need to solve the system of three ODEs, which gives you the information about the characteristic. And so you're solving the characteristic and you see what's what. So now you say, okay, it takes me another two days, and I solve explicitly the characteristics. It's funny because the characteristic lines are straight lines, and then you have explicit mass and density along these characteristics. So if you know the characteristic at one point, then you see it all through a line. Through lines, and then there's an alpha minus one, which means alpha greater than one and alpha less than one will be different. In fact, if rho zero is decreasing and alpha is less than one, then your characteristics are nicely ordered. So, in fact, you can construct explicit solutions if alpha is less than one and rho is smooth and non-increasing, you get classical solutions. And in the rest of the cases, your characteristics will cross and you're two. And you're doing okay, so but at least you know, when my solutions are decaying, life is nice. So, one of the things that you do is like you say, okay, so whoops. So, let me try and do like what we people in PDs like, which is let's try and give this as an initial condition just to mess with the equation. And you say, okay, I can approximate this by nicer things that. Nicer things that look sort of like this. And these ones are smooth enough so that I get a solution by characteristics. And you look at the characteristics and what they do. The characteristics are in blue. In yellow, you have the initial datum, yellowish in the screen. And in green, you have what happens after time equals 0.5. So I'm going to do epsilon 10 to 0. This is all an explicit construction. And in fact, you Explicit construction. And in fact, you see how, as you approach this jam, we have constructed a solution which is strictly positive. This is alpha less than one, 0.5 in this case, and the solutions have become positive. And this is what's usually called a rarefaction fun thing. And you can work a little bit and you can write it explicitly. So this is one solution. And it's a reasonable enough solution. It's the limit of classical solutions. So hopefully it works. So, hopefully, it works. But we are doing conservation laws that apparently allow for, you might allow for shocks. So, you also want to construct this to paste two solutions through a shock. And you can do it. The condition for this is usually called the Rankin-Worn condition. And it basically just means that you paste the solutions where the mass pastes. And you get an ODF. So, working a little bit, you can. Working a little bit, you can actually find explicit solutions that are constant and zero, and then you solve for the S, and you find two solutions starting from the vortex for alpha less than one, you have these two guys. You have the vortex and you have a smooth one. This creates a a problem and an opportunity. A problem and an opportunity. Now you have two solutions, you have to pick one. How do you pick one? Well, this is what mathematicians like. You need to make a theory that works. So the equation for the mass is a Hamilton-Jacobi-type equation. Okay, two viscosity solutions. This is, you know, the natural idea. So you go back to the, this jumps equations like a madman. That's fine. So you go back to the So you go back to the Crandellon's paper for viscosity solutions, the original one, the one without epsilons and things, and you see that there is a reasonable way of describing viscosity solutions. If someone doesn't know the definition for this or why this is defined, I guess I have backup slides, so ask at the end. But yeah, time is short. And I want to get to the pictures. Real quick, I get to show pictures, otherwise I go quick. Otherwise, I talk. So, existence, well, you do it. You regularize the age, the h that was the thing in the middle of the equation. You pass to the limit by the continuity, and viscosity solutions are stable, and life is nice. If you want to read some more about discarsity solutions, I can recommend this book by Katsurakis, where you can read all about this wonderful world of discursive solutions. Okay, but in Okay, but in this sense, viscosity solutions are well ordered, so this gives you uniqueness and a comparison principle. So you start with ordered masses and you start with a sub and super solution and they are ordered. So you can only get one viscosity solution. Fine, it's able to tell apart these two weird guys for alpha less than one. For alpha greater than one, you don't really get to construct the solutions, but this is fine. So there is exactly one because it is. So, there is exactly one viscosity solution. There is a way of picking a reasonable solution for the initial problem that is usually consistent with the vanishing viscosity. If you put an epsilon appliance, it passes. For alpha less than one, it is a running function fun one, because I already constructed it as a limit. So, biostability is supposed to be the good one. The other one is sort of spurious. Funny thing is that for alpha. Funny thing is that for alpha greater or equal than one, the vortex, the other one, is the viscosity theory. The thing is that for alpha less than one, you get two. For alpha greater than one, you only get the vortex, and the viscosity theory is able to say, okay, there's only one, just keep it. It's the physical one, sort of like a bifurcation with alpha. And the stable one here is the rarefaction fun solution, so the one with the tail, and here you only got that. And here you only got like the vertex one. So the one that is just constant and zero. In mass, this means that you're linear and then constant. So yeah, you check that they satisfy the definition. It's a little messy, but you know, there's one paper for that and one paper for that. So it's fine. The numerical scheme works, it converges. For alpha between zero and one, you have to be a little bit careful because you had to. To sort of regularize your singularity with the parameter delta. So it's natural to pick the space expectation depending on delta and the time expectization depending on delta. And you get convergence, uniform convergence with an error that depends on delta. You do the same thing for alpha ratio or equal to one with the CFL condition, and you get a similar thing. Takes a while to prove, but the proof is by variable doubling. To prove, but the proof is by variable doubling, it's fairly standard for people that do these kinds of things, still takes a couple of pages. So, um, sorry, yeah, I'm going to talk about this waiting time. But this waiting time, so when we saw the video, you may have noticed that it took the solution a while to start changing the support. The support sort of stays stationary for a while until it starts moving. The time it takes for the solution, the support, to move is called. Solution, the support to move is called the waiting time. So, how do you prove that things have waiting time? You construct some solutions. And one day, Juan Luis comes into the office and brings this monster. Where did it come from? You don't understand. It takes a while. Then he tells you there was a change of variables with the pressure and something, and then you explain. It takes two pages to see how you ended up with this guy. This is a nice guy, which happens to be a subsolution defined between zero and one. Uh, defined between zero and one, and it goes like this as time moves. It's a really nice guy, it forces your solution in mass to stay at value one after this initial value, and it takes a time for it to start moving. So, you prove that this is a solution to convince you that this is true. This is the numerical result in blue and the explicit subsolution in red. And you will see that it's able to. And you will see that it's able to hold the solution in place for a while until they start moving. You say, okay, things that are on top of this kind of answer are nice. Now you're, yeah. Do you have conservation of mass or you lose mass go down with your mass? So you have conservation of mass. This is the mass variable. So this is the total mass. But this is the raw is the density up there. So you see in density and this is actually a see intensity and this is actually uh so you have one place it's like when the density goes down and another is yeah yeah so this is mostly the same yeah you can you can see it here right as the the on top you have the density variable and you see it i see you see it move but in fact the scheme is done in mass and in mass you're seeing that the total mass is the right one and it up there i just plot the numerical derivative of the mass which is Derivative of the mass, which is done for the scheme. So the scheme is in mass, so you already know the total mass is fine because, in fact, you're just transporting the values and the density is spreading, it's coming from that part to that part. And it's not until the raw becomes a little bit singular, so there is sufficiently much pushing, that it starts moving. So it's a degenerate kind of diffusion. So I was saying that. I was saying that. So you see this picture, and then someone asks, well, can you say precisely when you have this waiting time? And you have waiting time precisely if and only if the mass, the initial mass, arrives at this top point with a certain singularity. And the precise singularity, if your limb swoop is infinite, then you move. If it's finite, then you wait for a while until you get this correct singularity. Get this correct immunity. Well, yeah, a certain holder rate at the top. So you have to get to the right holder rate at the top before you can start moving. And then, you know, once you hit the corner in the right sense, basically you see it in row, then you can start moving, but not before. And it's funny that the condition is in mass, not really in density. But then again, the model. But then again, they're mostly equivalent. Okay, so I finish because if you do a paper with Juan Luis, you don't finish until you plan the asymptotic behavior. Jualuis' former student and colleague know. So you have to see what happens. So first you say, okay, let's try to do this for solutions that are defined by characteristics. Then you prove the beautiful result. Initial data decreasing between zero and one, you get This in between 0 and 1, you get convergence to the profile for alpha less than 1, there was a profile, and you converge to the profile of the right mass as t goes to infinity, supreme, normal, in relative terms. Nice. You say, yeah, but this is a very restrictive range of initial data. So you want to do something more. And to do something more, we pass to the mass variable, which is the one that we know. And for alpha between 0 and 1, you take the mass of the profile and then Of the profile, and then your rescue mass goes to the mass of the profile in relative error outside of a small ball in rescale variables, which is the result that you usually get. And this is one would expect. For alpha greater than one, you take the profile, the mass of the vortex, and then you convert to the mass of the vortex in you convert the mass variable to the to the vortex. It's a really striking result. It's a really striking result. It's like the vortex is attractive for alpha greater than one. Also, you can get information on the support, but this is not enough. So I do now a simulation for alpha 0.5. Sorry, on the left you have original variable. On the right, you have rescaled variable and rescale density, rescale variable in space, and the mass you don't touch because the mass is the mass. So you see it evolving time, and then you see how in the scale variable you are going to the You are going to the profile, even for things that have a jump. And of course, this jump is what makes you have to remove a little bit of next to zero in the scale variable. And of course, in this variable at time passes, you see na. Okay, so this is alpha less than one. Whoops. And this is alpha greater than one, which is somehow more surprising. More surprising. You see, it also forms and it goes towards the vortex. And in fact, it seems to reach the vortex in finite time because the way the characteristics hold, and you can find this solution explicitly doing shocks. You need a shock at every jump. It takes a while, but you do it. When you're a student, you have time. And so, this is the asymptotic behavior as C. Symptotic behavior as seen through the numerical scheme. So we proved convergence in mass in relative entropy, in relative error, but the numerical results suggest that you also have it in row. It is that proving things in row is much more difficult. Okay, so let me finish with some take-home messages. We were studying this equation. We have studied the equation for the mass of radial solutions, which is nice in the sense. Which is nice in the sense of viscosity solutions. It has a nice finite difference scheme. And for alpha between zero and one, you have multiplicity of weak solutions. For alpha greater than one, only vortexes. For alpha between zero and one, positive solutions, so like passive different equation. For alpha greater than one, the compact support is preserved for positive times. There is waiting time that could happen. And I finished with the asymptotics, alpha between zero and one, you have a similar profile and for alpha greater than one. File and for alpha greater than one, you have vertices. So, yeah, thank you all for your attention. These are the list of references. I'm happy to take questions. Are there any questions? So, uh you mentioned your result in uh resume, but you have any questions? But do you have the radio convergence in it? You mean rate of convergence? No, just the conversion and the rate of conversion if you can reverse this. So this is L infinity convergence in relative error, which for any for any small pylon, which means that you can do L1 or Psychof here in compact domains. But on our key on interstate. Yeah, you you could do L1 log. Yeah, you could do L1 log. We haven't tried to do the L1. So, because the L1 will be natural in row, but the mass is not infinity convergence in mass. So, in this one, is easy because you have compact support. So, if you have compact support data, you know where the support is going. You know where the support is going. In fact, we have like explicit information for alpha ratio than one where the support is, so L infinity and L1 are equivalent. In this one, you will have to control the tails, but you have very sharp information in the tails because you can make, so you have comparison principles. So you can make like a tube for the mass. Remember, the mass on the right-hand side is going to fix value. So you can make sort of a cone and keep it there. And you could convert the L infinity to some kind of. Convert the L infinity to some kind of L1. So, what about gradient-flowed solutions? Well, we don't have potential problems, but there are things getting better in this radio setting. And then you can comment if your discussion solutions to agree with kind of the steep descent of the energy. Why would discuss these solutions be the right choice? Okay, so the viscosity solutions in mass correspond to the vanishing viscosity limit. So in this sense, you could go back to your original equation and do vanishing viscosity at the level of the density. And then you would hope for classical solutions. This will have as the mass the mass of the vanishing viscosity thing. And then when you pass to the limit, they should agree because they are the limit of the same thing. thing but i'm not sure that they give you a lot of information on the well there's a there's a second argument that is the the following so the scheme that we have done the scheme is somewhere it'll be somewhere no it's further further along yeah so this this scheme is actually the if you pass to the mass variable this is the finite volume scheme Mass variable: this is the finite volume scheme for the density with upwinding. The beauty is that the velocity is always only pointing one way, so you don't need to upwind much. And in that sense, these kind of schemes are also the finite volume schemes are the one that if you take them all implicitly, I did explicit implicit, but it's fine, you can use everything implicitly if you're going to do that, then they also decay. Then they also decay the free energy. So they preserve the free energy dissipation in some sense. They're not like the minimizing curves, but they preserve the minimization of the free energy. This is a series of papers that José Antonio has done. So in this sense, since we know that this scheme converges, you hopefully have the same thing. Also, there's a series of papers that work with the They write it like this, because for me, and they do minus s. Our paper is s equals equals one, which is somehow more difficult in this setting. So, this is still the gradient flow in non-minimum mobility. And for that, they prove at least in dimension one, they do the same scheme, but you have to upwind it because it's more difficult in bus. So, they prove business. So they prove bassistic convergence to the solution of the gradient flow, which is actually a decent solution for s less than one. This has nicer solutions sorted in the Caffari path recently. But the scheme is the same. The thing is that what I put here after the power alpha, of course, is a fractional operation of negative time. Any more questions? Okay, thank you. Next speaker is Maguire Vadition.