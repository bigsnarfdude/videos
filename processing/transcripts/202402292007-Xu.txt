Question. This is based on joint work with a wonderful undergraduate student at Leon and my wonderful collaborator Lefan Lu, also from Yale. So I picked this topic because I feel like there are actually many interesting open problems for this problem. So especially like this problem, even though it's kind of age-old question, we're going to see, but it actually has not been really extensively studied from modern statistical and a computational perspective. A computational perspective. So that's why I want to kind of share this problem with you. So let me start with the problem setup. Okay, so basically, as the name suggests, it starts with a classical linear regression problem. So we have these responses, yi, and then the covariates xi. They are linearly dependent. And then the classical linear regression is trying to estimate this regression coefficient. Coefficient. And this shuffled meaning regression basically adds another twist. As the name suggests, it's basically shovel the labels like the yi's. And then you basically lose the correspondence between the response yi's and the covariance x i's. Or more specifically, you can think about this is a problem where yi is equal to the inner product of x pi star i. So pi star represents this underline. star represents this underlying shuffleing process. It's an underlying commutation. And then take inner product with this unknown regression coefficient beta star and then plus some additive points. And now the question then is that given this data x set of x and instead of y's, you want to estimate this unknown permutation, like unshuffled them, and also estimate this regression coefficient. I guess like in this model, something. I guess like in this model something that's a little bit different than the regular linear regression is that like if I get I mean I can't like I can't ask for like n more samples because then it's like the permutation you know what I mean like it's something a little weird like how do I think about like the growing number of yeah so the way that I think is that fix the n so you fix the samples and then you let the dimension grow maybe that makes slightly more sense like d grows so beta stars Like D growth. So beta star lies in the D-dimensional space. Yeah, but you will see that more clearly. So in the more compact matrix form, you can write this as y equals this pi star times x times beta star plus w. So beta star is this unknown regression coefficient. It lies in this d-dimensional space. And then x is this covariate matrix. It's n times b, so it has n rows. And then each row is like a d-dimension. Each row is like a D-dimension. And then here, actually, I put a very simple distribution or assumption. Basically, XIJ are just ID standard normal entries. You could possibly consider a more general distribution, but here for this talk, I'm going to focus on this very simple setting. And then the W here is an n-dimensional vector that is the additive Gaussian noise. Actually, every component is ID Gaussian with variance sigma squared. Okay, and then this pi star is this m by n permutation. Is this m by n permutation matrix? Okay, and then your data, as I highlight here, is this y and x, and then the unknown variable is pi star and beta, and then you have some additive noise double. Is that clear? So the variance of the noise is known? Yeah. Okay. Yeah, I kind of assume that all the parameters are known. So you know the sigma squared. Yeah. Okay. Like without that, it's hard to see how to make any progress. Oh, you mean like you do not know the variance? Well, like if y equals. Well, like if y equals x and x is normally distributed, and then I randomly scramble y, I might think that there's no correlation at all, and it's just that y's are just the modus. Oh, okay. In fact, not exactly. For example, like think about in the simple setting, like here, the dimension d, let's say, is what. So these little x are just numbers, and the y's are also numbers. As you can see here, As you can see here, if you shuffle them, in fact, the one way that you can possibly unshuffle the data is by sorting its x and y's. So that's how you can possibly figure out the computation. Sorting with rehearsing equally. Yeah, so then you can always, like either is, yeah, so so there's something ambiguity, but you can just check that. If pi star is cyclic, then is this like this? And is this like this kind of multi-reference alignment type of problem that you have looked at specifically? I don't think so. My understanding is that that's more about rotate X, sorry, rotate beta, if my understanding is correct. Oh, okay. Here, here is shuffle the samples. I think it's different. Okay, three. Yeah. Okay, so that's. Okay, so that's the problem. In fact, it's actually quite interesting from both practical and then theoretical point of view. So this problem actually has a long history. Sorry for about my bad writing. So it actually, the history kind of go back to 70s. It actually has a very interesting name called broken sample problem because you can think about the sample where the covariance and the response are broken. So then you want to kind of figure out the correspondence. It actually also has many other names, for example. Has many other names. For example, it is known as unlabeled sensing. It is also called linear regression with premuted data or mismatched data, or like a linear regression without correspondence based on. It also has wide applications in different fields. I'm not actually going to talk a lot about this, but let me just kind of quickly mention fields. So, one application is in computer vision. So, here actually, I stole this figure from this Berkeley. This figure from this Berkeley group researchers. So basically, there's a, yeah, it's basically think about this 3D object, that's the Berkeley tower, and then this camera represents this unknown linear transformation, and then you get some low-dimensional image, like 2D. And then also those important points, the correspondence is actually lost. So that represents the underlying permutation. This problem also arises in data. This problem also arises in data linkage or data de-anonymization because you can think about, in practice, because you want to protect the privacy, you remove these record labels so that you do not know actually which coverage corresponds to which response. This loss of correspondence also arises in some physical systems. For example, in biology or single processing, there are some situations where you actually actually lose this correspondence. Actually, lose this correspondence information. But if you're interested, you can check those references here and there to have a more detailed discussion about these applications. For me, I'm more interested in, from a theoretical perspective, I feel like this is the problem, kind of simple, but also offer rich value for studying computation on the statistical limits. It actually has been already studied extensively in the past decade. There are lots of references I actually didn't put here. It also has many variables. It also has many variables. If you think about that, for example, one natural variation is to consider a case where maybe in practice you do know some partial information about the permutation. So then the question becomes how you can leverage this partial information to kind of unshovel the entire data. So this you can think about as like the CD version of this problem. There are also other variations. For example, here I do not assume this regression coefficient is sparse. You could possibly assume if it's sparse. You could possibly assume if it's false vector, and et cetera. And also, I want to say this problem actually has a connection, I mean, fall under the broad umbrella of this problem called learning from shuffle data. For example, it has connection to graph matching, database alignment, particle checking, uncoupled, isotonic regression. I think Jonathan worked on that before, and data serialization. And the list continues to grow. Yes? Just so I understand the problem, can I ask? So, I understand the problem. Can I ask another toy example? Yes. So, let's suppose we have a one-dimensional situation that x is normally distributed, y is normally distributed, and they're actually independent. Actually independent. So, if I then, but if I, in the effort to find a correlation, if I sort y from smallest to largest, then I'll probably have a pretty good looking fit because of the order statistics. Because of the order statistics of the normal distribution. Yep. So is that, I mean, is there some sort of overfitting or? No, so I think, yeah, you're right. So basically, like, you can consider a hypostasing version where the one that you consider is a null model. And indeed, if you have two independent samples, if you want to actually sort them, this is related to the watches and the distance. And then the distance can be very small. Like in the one-dimensional case, it's kind of like 1 over n squared. So it can be really small. Small. So then that means if you want to detect this correlation, then you have like, so the signal-to-noise ratio, as you're going to see, needs to be large. So your noise cannot be too large. Otherwise, it's going to destroy this kind of correlation in the problem. Yeah. Does it make sense? Yeah. Okay, so now the question that I'm going to focus on this talk is about statistical and computational limits. Statistical and computation limits. The first question, as we're kind of all familiar with, is basically trying to ask the following question. Like, I define this signal-to-noise ratio, which basically is the squared L2 norm of this unknown regression coefficient beta star divided by the variance of the noise. So, as you can imagine, if the signal noise ratio is large, then this reconstruction of pi star is going to be easy. But then you want to understand what is the minimum signals ratio that is required for reconstruction of pi star. For reconstruction of PyStar. And then also, I want to say in this talk, I'm going to mostly focus on recovering PyStar. So then, to make it more formal, I can define a particular metric that measures the overlap between the estimated permutation and with the ground truth. So basically, this formula is just computing the fraction of correctly unshuffled covariance. It's like basically compare a fraction of coordinates where these two computations coincide. And then you can naturally define this. And then you can actually define these two kinds of recovery guarantees. One is called the exact recovery. Basically, you just want with high probability, your estimator coincides with the pi-star. Or you can look at almost exact recovery where the estimator almost coincides with the entries up to little n fraction. And then you can also consider partial recovery whether you want to overlap to be bounded away from zero. But in this chart, I'm going to focus on these two, exact and almost exact recovery. And almost exact recovery. So that's the first question. And then the second question that you might be interested in is that, okay, then what is the computational limit? Like, if I want the reconstruction to be efficient, in particular in poly and D time, then what's the minimum signal observation limit? Okay. Yes. I would have thought that the problem would be to recover beta star, but are you saying that they are really the same problem? Not exactly. So in the exact recovery, it's In the exact recovery news, yeah, sure. So, if you can exactly recover PyScar, then definitely you can plug in this in and then you can recover the unknown regression coefficient. But if you get almost exact or partial recovery, then not necessarily you can recover the unknown regression coefficient. And also, in fact, in some cases, you do not necessarily need to recover the permutation to be able to recover the unknown regression coefficient as well. So, in this talk, I focus on this reconstruction of permutation. This reconstruction of permutation, but the reconstruction of the unknown regression coefficient, I mean, the problem itself is also very interesting. I just don't have results to show here, so I'm going to focus on this one. But I'm going to mention in the end, it's also one of the open problems. So the regression coefficients are known in your model, right? No, unknown. Unknown. Okay, but also not random? Okay, in my current problem formulation, I kind of assume that it's. Problem formulation, I kind of assume that it's kind of fixed and unknown, but if you want, you can put some plier on that. But how do you set things up to make it fixed but unknown? How do you exclude an algorithm from having it built in? Well, because you want guarantees over for all beta. Yeah, for all beta is not. Already the X's are random, so yeah, it doesn't really matter. Yeah, it doesn't really matter if you fix the norm. Matter if you fix the norm. Yeah, if it helps you, you can also zoom in as a prior on that, too. That's fine. He was just saying it's like worst case video starter. Yeah, you can also. Okay, so, oh, sorry, I dropped my head. So basically, I'm going to focus on the statistical limits in this talk. And towards the end, I'm going to tell you actually some open questions. Tell you actually some open questions, some like existing results about computation limits and open questions. So, here is basically the state of art for statistical limits before our work. So, this is actually this wonderful work back in 2017. So, they basically show that the required signal-to-noise ratio for exact recovery is between like n squared and the n to the c, where c is some unspecified absolute constant. Okay, and so they actually first. So, they actually further run simulation to actually numerically verify actually if a signal-to-oise ratio is needed. In particular, when dimension equal to one, you can actually efficiently run the simulation, and then they have this kind of plot. So the y-axis is actually the empirical success probability where you get the correct permutation. And then the x-axis is actually the exponent in terms of n, so the signal ratio. And then these different curves. And then this different curve actually corresponds to different samples that you have, the little n. And you can see that after proper scaling, all these curves collapse roughly. And then you can see roughly there's kind of a sharp drop from like, I mean, the threshold suggests the constants between 3 and 5. So that kind of caught our attention because this is actually something that suggests that maybe there is a sharp threshold ongoing here. Here and then basically our main results the sharp threshold for what the exponent is? Yeah, for the exponent. So it really is like take the log of S and R divide the log of M. So that's what they're joint. Okay, so here's our main result. Basically, we are able to determine this exponent, like the constant. For the exact recovery, it turns out that the single force ratio is actually n to the 4. And then this actually holds not only for And then this actually holds not only for dimension equal to 1, it actually holds all the way up to dimension D is a little over OM. Okay, and then along the way, we also identify the almost exact recovery threshold. The constant just drops from 4 to 2. And also holds for all sublinear dimension. But one quick comment that I'm going to tell you more later is that this result actually did not extend to the case where D is linear in M. So in particular, the cost. Yeah, so in particular, the colours are going to change. So it's out of this anyway. Okay, so now let me tell you a little bit about how we get this result. Okay, and also this also helps me to tell you more about the computational limits for this problem. So as we all know, in this kind of statistic inference problem, one natural estimator to kind of start with is always right down the maximum effort estimator. The maximum output estimator. In this problem, we have two unknowns. One is the permutation, and the other is this regression coefficient beta. So basically, I can try to minimize over these two objects and then to kind of basically search over all perpetuation pi and then on a regression beta to best fit the observed data. To minimize the L2 norm, squared L2 norm of y minus pi x beta. And then, in fact, for this problem, you can do a little bit more derivation. For example, if you fix pi, okay, and then you can actually solve for optimal beta for that fixed pi. This basically is like if you fix pi, then this problem is nothing but figuring out by projecting the y onto this column space, like space spanned by the columns of pi x. And then for this one, you can quickly write out basically this going to be given by the projection. Going to be given by the projection of y to this column space. And then this projection matrix you can write out more explicitly in terms of the x and then the permutation pi. And then you can try to plug in this optimal beta back to my original objective function. And then when you plug in, basically it's like you minimize this projection error. And this projection error is given by y projected to the space that is orthogonal to the columns space of pi x. To the columns space of pi x. Or equivalently, you want to maximize the length of the projection. And then for this one, if you kind of unroll the square, expand it out, you can write as an inner product of two matrices. One is this yy transpose, and the other is this p pi x projection matrix squared. If you write out, you're going to have pi times projection matrix of px and pi transpose. And then for me, this is one. And for me, when I see this, I immediately see that that is a quadratic assignment problem because we encounter this object very often in graph matching problems. So we know that this is a quadratic assignment problem. And then we know that the quadratic assignment problem, in the worst case, is kind of the empty heart. So that actually brings some computational issue for this problem. Okay, make sense? Okay, so now that is the maximal effort estimator. Max Lagrange estimator, basically, we're able to calculate the statistical guarantees of this maximum likelihood estimator. So, this is a more formal statement of the result I just showed you before. Basically, as I said, assuming this dimension D is sublinear in n, then for the exact recovery, this MLE achieves once the signal-square ratio passes like n to the power of 4 plus epsilon. And then for almost exact recovery, the exponent is 2 plus epsilon, as I said. Okay, and now you may wonder whether this maximal estimator is optimal or not. So it turns out that you can actually prove a matching, like a lower bound, very easily for this problem. So it turns out that it's actually really straightforward. You basically assume a Gini AD scenario where Gini tells you this unknown regression coefficient beta star. So it kind of revealed to you, kind of helps you to estimate this. And then the thing is that once you know beta star, And then the thing is that once you know beta star, then this problem is greatly simplified. Because once you know the beta star, then you can call this x beta star as a little vector little x. And then basically this boils down to this problem where you have a little x, and then you permute them, you get a little y. So this is actually called linear assignment problem, which actually has been studied before, including Tim and Jonathan. So then in this case, you can And in this case, you can actually call some existing result about your assignment problem, in particular, even so you will get that the exact recovery is IT impossible if SMR is smaller than, much smaller than n to the minus 4, and almost the exact recovery is possible if it is n to the minus 2. Okay, so that basically showed that the maximum alpha estimator in terms of at least in terms of the exponent is kind of optimal. It's kind of unoptimal. Okay. So you're saying well, you're saying the fact that beta star is unknown is not super helpful. Yeah. Yeah. So the reason, in fact, I'm going to, yeah, it's actually related to my comment next, because we're assuming that this dimension D is a little of O n. So that actually turns out to be crucial because as you can imagine, when dimension grows, this beta star, unknown of beta star is going to affect. Know of beta star is going to affect more about the hardness of the recovery. It turns out that in these little ones, it's not that kind of significant. So, in fact, there's a very good comment because that exactly leads to the first open question that I want to mention, is to calculate the statistical limits when D is linear in N. Sorry, Jimmy. I missed what the previous setting was. D was what within? Little lower. Just hold as long as D is a little lower than this. Okay. Because then intuitively you're not giving up much by knowing data star. Yeah, yeah. So that's why this kind of simple lower bound is tight. And the upper bounds also need D to be a little ON because you need the boundary. Yeah, yeah, for our analysis we need a D to be later ON. Yeah, I'm going to show you later why we need that. Okay, so here is actually the open question about specificity limits when D. Open question about statistical limits when D is linear in N. And I just want to mention that the purpose work actually indeed approves some upper bound for the next Liberal estimator. So if you assume D is like a constant rho times N, then they kind of show the exponent is some constant divided by 1 minus rho. So you can see that with rho converging to 1, this exponent kind of grows as 1 over 1 minus rho. So we do not know whether this is sharp or not. Because one clear thing is that we cannot use our privacy lower band anymore. Our privacy lower bound anymore because this lower bound does not depend on dimension at all. So, definitely, we need to sharpen the lower bound. We also possibly need to sharpen the upper bound. So, that's kind of the first open question I want to mention. Now, let me talk a little bit about another open question. I think it's also very interesting. It's about computational limits for this problem. It turns out that the computational limits for this problem, when D grows, is actually widely open. Open. Okay. So when D equals 1, it's very simple. Because as I show you, the maximum effort estimator can be reduced to the quadratic segment problem. And when D is equal to 1, this can be exactly solved by just sorting X and Y's. Okay, so in the dimension 1 case, because your covariates are just some numbers. So you can easily solve that. When D is a constant, but does not grow, you can try to approximately solve the maximum estimator by this quotation. Estimator by discretizing the beta. Basically, put some delta net on this minimization, and then you just need to properly choose this delta, the resolution. For example, you can choose to be 1 over poly log n, sorry, polyn. And then you can solve this problem efficiently because the inner minimization, you can solve it by exotic search over beta. And that's going to take n to the big old d. And d is a constant, so it's in principle still polynomial time. The outer minimization, The outer minimization is just a linear assignment problem, which can be also solved efficiently. Okay, but then when D grows, we know almost nothing. The only thing that we know, a little bit, is in this very special case where there's no noise at all. So in other words, the synthesis ratio is infinity. In this case, it turns out that there's an efficient so-called lattice algorithm based on reduction to subset some problem. So, but as we heard a lot about this lattice algorithm, But as we heard a lot about this lattice algorithm, it turns out to be very brittle to noise. So that's why it's very unclear how to actually get an efficient algorithm in the noisy setting. So would you believe that for N is, you know, for SNR, that's polym n for some large polynn, that there's a polytheum algorithm for MED? Yeah, I would believe that. Yeah. Yeah. But whether you can achieve this sharp constant like n to the four n to the normal. Sharp counter like N2O4, N22, or is on top of it? Do you think that alternating minimization is a reasonable, like alternating pi and beta, since they're easily rate? That's like a natural approach. Yeah, it is a natural approach because you have this double minimization you can try to EDM or something. Yeah, but the issue is that in theory, like how you can prove rigorously the convergence, so that's a misunderstanding. And also, even you want that, I think it really highly depends on the initial point. For example, if you, because you do not know anything about beta and pi, so for example, especially for pi, if you don't know anything, if you random, start with a random permutation, this does not go to converge. But this is indeed the idea where in the CD version, for example, if you already know pretty good estimate about pi, then you can use this autoization and show you convergence. I show the convergence. In fact, I'm happy to tell more. So I think, for example, definitely there is a possibility of trying low degree method on this problem. So I think that's the first thing that I can possibly try. Okay, um, how much time I have? Um I actually didn't track the time. Yeah, so a couple of minutes right now. Maybe. Maybe five minutes? Okay, so I will try to tell you a little bit about the proof idea, about the statistical members. So basically, it's kind of, I mean, the idea is very standard. You just want to analyze this maximal estimator. Here, the kind of the tricky part is that you have two unknowns. One is the permutation pi, and the other is the beta. Okay? And then, as usual, the kind of a starting point, for example, for the sake of analysis, you can always assume the You can always assume the ground truth permutation is just identity. And then you can give me any candidate solution, like pi and a beta, and I can compare the cost or the energy. Compare that with the ground truth. And then if you plug in the ground truth, the cost will reduce to basically the magnitude of the noise. And then if you plug in any candidate solution, you're going to have this particular expression. And if you expand that out, do some little bit of algebra, you're going to see this is equivalent to. See, this is equivalent to this inequality. And then, here, remember, W is a Gaussian, so this quantity is a Gaussian random variable. You want this to be smaller than some very negative kind of quantity, so then you can just use the Gaussian tail pro, Gaussian-tail bound, and this is going to involve bounding this kind of like a homogeneous function of this object, like x beta star minus pi x beta. So, roughly, this captures how different like pi and beta compare. Like pi and beta compared to other shoes. And then you need to study this object, basically. So that's also roughly the idea in the purpose work. But the issue is that the purpose work somehow they couldn't really figure out a tidy bound for this. In particular, the bound only depends on the number of fixed points for this permutation. But you can see that this object possibly can depend on the entire cycle decomposition of pi. So that's where we kind of can sharpen the analysis. Analysis. Okay, so basically we derive a general identity for moment-generating function of this form, and it depends on this quantity called nk pi. That is actually the number of k cycles in the cycle decomposition of pi. And then this p and q, they just depend on beta, beta star. And then how to derive this? Basically, the idea is that we're going to use the cycle decomposition, factorize the MGF according to the cycle decomposition, and then you have a nice Decomposition, and then you have a nice independence across different cycles. And then within each cycle, if you do a little bit of algebra, you're going to see that this is a quadratic form of a bulk Gaussian. So you can just directly use MGF for quadratic form of Gaussian. So then that's roughly the idea of how you can get this. Okay, so that's kind of the first challenge. Another challenge is actually how to take a union bound over beta. Because eventually we want to kind of take a union bound over all the possible part of beta, and beta is kind of. Possible pi and a beta, and beta is a kind of continuous object. But here the idea is also very straightforward. You want to do some discretization. But the only issue is that you want the approximation error to be small, so your delta, I mean the delta net, the delta has to be small. And that's going to introduce a large entropy turn when you take the mean bound over beta, and that's going to overwhelm the tail probability. Especially when pi is close to pi star, because if pi is close to pi star, your tail probability is. Your tail probability is not going to be very small. But that also roughly suggests what's the natural way to overcome this. It's basically kind of divide and conquer. So you separate the analysis into a large error regime. And for this large error regime, you have a small tail probability. And then you can take a union bound over beta very easily. And then for the small error regime, then you need to do a little bit more careful analysis. So those are the permutation pi that is very close to pi star. That is very close to pi star. And then for this one, the good thing is that there are not many such pi because they are close. So you can leverage the fact that the entropy is small. And then I'm not going to the details, but basically you leverage that fact, and then you can control the error eventually for the small error region. Okay, I think that's basically what I want to say. So let me just summarize the talk. So in this talk, I show you this sharp statistical limit for exact recovery. Limits for exact recovery and almost exact recovery in the sublinear regime. So there are lots of open problems. For example, the shock limits for the linear regime, the computation limits when D grows. And also there are some extension. For example, you can consider multivariate response where y is not a kind of. So y actually is like yi is not a number, it's actually an m-dimensional vector. And then there's a natural kind of And then there's a natural conjecture to choose for the next for this. And then also, you can also study the recovery problem for regression coefficients as well. So that's another part that we haven't done. Yeah, I think that's all. Thanks. Very cool. Thanks, Jamie. So let's maybe start the switch and we can have a quick question while we do the slot. So for the multivariate case, I guess those lower bounds are straightforward. I guess those lower bounds are straightforward. And what do you see what goes wrong with what people talk about? Try to make the lower theory face for it. Okay, that's... Actually, so I can now remember exactly. I don't think there's a fundamental difficulty. Just like you just need to do more analysis. Yeah, I don't think there's. the quadratic assignment problem that you that you end up having to deal with is is pretty special right like one of the matrixes is rank one the other one is also PSD and also ranking this but like do you have a sense could that be helpful especially when one of them is ranked one seems like you know if I had a general matrix of the other side maybe I could take like the top five On the other side, maybe I could take like the top eigenvector and try to align the entries or some tricky thing like that. So, could anything like that be useful?