Before that he was at Berkeley. He's won a number of things, importantly the best paper award at Nurix a few years back for his work for his work on Calvin substance selection. So he's going to talk about random APAC and how this affects the next generation of randomized neural algebra theory. Thank you, Bristol. Thank you. Thank you, Petros, for this very nice introduction, and thank you for the organizers. Okay, so. Okay, so as you can probably tell, this was until recently going to be a joint presentation between me and Michael, but I believe he's on a shuttle to the airport right now, so barely, so I'll do my best to fill in. And also, I should say, this is joint work with many people, particularly Riley Murray, and as well as many others, including Jim and Laura, who Jim and Laura, who might be on Zoom. So I'm happy to defer any tough software questions to them. So, okay. So what are we going to do? Well, it's quite simple. Yesterday we had two tutorials: one on sketching, one on software. We're going to take these two things, put them together. There's another tutorial. So basically, that's So basically that's the plan. Yeah, so in the sort of first part of the talk I will discuss our efforts towards putting together a sort of standard for introducing randomness into LAPAC, which we're calling round LAPAC and RAND Blast. I had several comments in the chat. I wonder if it's worth just taking a look what's up. I don't have a laptop. Taking a look what's up. I don't have a laptop. Oh, good point. Good point. There's species. Yes, the upside-down video, yes. We were aware of that problem and extremely fine. Yeah, I just need yes. So nothing we're going to do about that at this point. Okay. So, and then later on, in the later part of Later part of the talk, since I'm more of a theory guy than a software guy, I'll talk a little bit about kind of maybe the lessons that we can learn and kind of the theoretical motivations we can have from that might help in bringing random light back into reality and make it all that it can be. Okay, so let's get started. Get started this thing. Okay. Find a good support for this guy. Probably not. You should go behind it. If you want to make sure more, video panel? Sure. Hide. Sure, high floating controls. Yes. Okay, so some backgrounds. I don't really need to cover this in any detail at this point, but our starting off point, of course, is the standard libraries for normal linear algebra, the basic linear algebra sub-programs PLAS, and the linear algebra package LAPAC. LAPAC. As we know, so we have there's some kind of existing standardization, and we're talking about a little bit now because we're going to try to fit into that framework to a certain extent. So we have the three levels of brass, vector-vector operations, matrix vector operations, matrix matrix operations. And we have the LA bug, which consists of kind of lower level computational routines. Kind of lower level computational routines such as the QR decomposition and some higher level maybe drivers implementing kind of standard algorithms for standard numerical and algebra tasks such as least squares. Okay, so obviously, you know, there's a whole wide There's a whole wide range of communities using this software, including scientific computing, ML, of course, as we've established. And of course, the problems are getting larger and larger. We have to in some sense, our efforts in NLA have to keep up with the kind of the needs of these communities. The needs of these communities. And we've been able to do it with various innovations, both in hardware and software. But one could argue that maybe both on the hardware and the software and the algorithm side, we're reaching some level of maturity where such kind of leaps or innovations are maybe harder to come by. And, well, basically we believe that. Basically, we believe that run LA is such an innovation, and maybe we haven't tapped into it fully yet, right? So, that's what this is all about. So, randomized numerical in algebra. Again, I think at this point of the workshop, I don't really have to introduce it, right? We had the tutorials on sketching. And but, so I'll do it briefly. Okay, so at a high level, of course. Okay, so at a high level, of course, what the name means is just we're using randomized algorithms for solving deterministic problems, but in the practice of the field, there's a certain set of paradigms that are typically used in a lot of random array algorithms, and they often consist of sort of two key components. On one side, you have random sketching or making. Random sketching or matrix sketching as Chris called it. So, which basically is, you can think of it as a kind of a low-level operation, a blast-level operation, right, which takes a large matrix, applies a, you know, multiplies it by a random matrix, the sketching matrix S, and produces a smaller sketch, right, which reduces one of the dimensions of the large matrix. And so, as an example, here we're looking at, let's say, sketching the least squares problem with matrix A and vector B, the over-determined E-squares problem, so that's why the A matrix is so tall. This is one of the classical applications. And, okay, so that's one component of it. So, we know, you know, we have this kind of methodology to, you know. Take a large NLA problem, produce a smaller NLA problem, and what do we do with it? Well, we can just solve it with existing deterministic NLA drivers, right? So that's kind of driver side of it. And so in this simple example, there's a clear separation between the random sketching part and the NLA part. That's not necessarily always the case, you'd see. But so this part I'm, you can But you know, so this paradigm you can call it sketch and solve, but there's other ones. But basically, clearly, we have to have a close interplay between the randomized kind of sketching part and the deterministic analytic part that have to work together. Okay, so just for some background, right, so run LA has also itself reached a certain level of maturity, at least on the theoretical research. At least on the theoretical research side. So there's many tutorials, lecture notes, books, reviews, some of them more specific than others. So here is some number of references, possibly incomplete. So okay, so what does randomization buy us? So what does randomization buy us? Again, I don't have spent too much time on this, but in short, so we kind of have three bullets here, right? So we can produce efficient algorithms for computing approximate solutions where we're willing to accept, we want to solve an analytic problem, but we're willing to accept a certain level of approximation. That's probably independent of what level of approximation we want to What level of approximation we want to obtain so that we can reduce the computational cost of the problem of using one of the deterministic algorithms. And then another case is we can use randomness to develop efficient algorithms that actually reach machine precision solutions, right? But they use randomness to do it fast. And then we And then we can also mention algorithms, randomized algorithms for maybe certain variants of the analytic problems, which can be used as sort of combinatorial or computationally intractable, where you can still produce reasonably acceptable maybe approximations in various situations. Okay, so. Okay, so so more generally, what that sort of buys us is as well, it buys us on the communication side, gives us kind of lots of opportunities to reduce the communication just because we're reducing the size of the matrices we're working with. And also, something that we talked about, or that people here talked about, is About is finite precision arithmetic as we're kind of making the precision kind of lower and lower for certain applications. So, here this is a natural kind of fit in some sense with the randomization. If we have if we're using randomized algorithms and we're willing to accept a certain level of approximation, then Then we're at the same time can also allow for a certain level of approximation in the finite precision arithmetic that we're going to use. So there's kind of a natural connection there. Okay, so as the big sort of effort that I'm here to talk about, as part of that big effort, we have put out this Randall Leip book. Know, run the LEPAC book, as you might call it. So, this is, you can find this on archive. Like I mentioned, this is a joint effort with many, many people. And so, what is this? Well, besides it kind of being also essentially an overview of the field of random isomer collinear algebra, it aims to provide certain standards and taxonomy for and some kind of guidance for how. You know, some kind of guidance for how we might go about implementing a randomized version of the LAPAC and a randomized version of BLAS, and also includes some implementations that are available, that have been tested, and there's proof of concept implementations that show that doing this actually makes sense and can provide some substantial improvements over. Improvements over what's there in the late now. Okay, so you can see what's in the table of contents. Maybe let me go to a more structured version of that. So what is round the layback? Well, as we've said, it's a library that concerns algorithms that solve traditional linear algebra problems. Solve traditional linear algebra problems and use as part of them kind of randomized sketching functionality. And also, the goal here, of course, is to provide high-performance, optimized implementations rewritten in C ⁇, building upon LABAC ⁇. LABAC and we're thinking of it as having somewhat comparable taxonomy to the LA pack. So it will also be, to a certain extent, divided into drivers and computational routines. So drivers are sort of more higher-level algorithms that implement tasks such as v-squares and Lauren approximation. And there's certain computational routes. Computational routines that they use, which are specific to randomized algorithms, and which are using these kind of sketching tools. Okay, so paired with this is Rand Blas, right? So Rand Blas, well, the idea of it is simple, right? So what is basic sketching? It really is just matrix multiplication, or I mean, at least in its abstraction, I mean, at least in its abstraction, it's matrix multiplication. The specifics are a bit more complicated, and that's why we need it. But essentially, at core level, it's sort of like last level three type operations, right? And hopefully, it can be used beyond the specific drivers that are implemented by Rundelebrick, right? And again, here the goal is to provide a Again, here the goal is to provide a reference implementation in C. Here we're focusing on sketching dense data matrices, but of course as we will discuss, you know, the sketching matrices themselves might not be dense matrices, they might not be strictly matrices that we construct at all, right? So on that side, it's kind of more complicated there. But of course, the hope is that, you know. But of course, the hope is that it grows to be a community standard and that we can have this API see wider adoption. And to do that, we really aim to focus on a sort of a narrow core set of operations, of sketching operations that can be well maintained and well optimized. So I'll talk a little bit about that first in this next part. I'll talk about sketching and the run plus. And the run plus. And so, throughout this part of the talk, basically, I will mention various kinds of examples. This will not be like a complete overview of all the different functionality of run a layback or run blast that is envisioned, but rather just kind of success stories and examples that we can sort of learn from. Okay, so let's start with sketching in the run glass. So, here we can So, here we can kind of again a little more taxonomizing. We can divide this into two kinds of situations, which in the book are called sampling and embedding. So, basically, sampling means we're sketching down to a really small matrix. So, you can think of it as: so, we're sketching this tall matrix A, but the matrix might not necessarily be tall, but let's say it is for here. Let's say it is for here. So, in the sampling setting, basically, the sketch A hat will be so small, right, that it will actually have smaller rank than the original matrix A. So, that's an important distinction. And the other case here is what we're calling embedding, which is basically, you know, we're still sketching one of the dimensions of the matrix down to a smaller size. Of the matrix down to a smaller size, but we're preserving certain important properties, such as, for example, the rank of the original matrix, right? And that has certain different sorts of applications, different set of considerations. Well, in particular, obviously, it requires the larger size of the sketch, of the sketching operator. And so, in particular, we see these viable distributions. So, this is kind of a list of catching operators that we're most concerned with and that we want to see implemented in run block. See implemented in run plus. The list is longer for the sampling methods because, well, the logic is if the sketching matrix is smaller, the cost of the sketching operation is substantially less. So, you know, for example, with IID Gaussian sketches, I'm sure you're to certain extent familiar with these sketching methods from yesterday, but I'll talk more about them a little bit later. But I'll talk more about them a little bit later. But there's different costs associated with actually performing the sketching operation. We can afford to have higher sketching cost or more expensive sketching matrices when, such as the dense Gaussian sketching matrices in situations where the sketch size is small. Whereas when the sketch size is large, we probably want to restrict ourselves to certain kind of more To certain kind of more carefully designed sketching operators, such as structured sparse sketches, which I'll talk a lot more about, and certain sub-sampled FFT or DCT, RHT type. So, randomized panama transforms, Fourier transforms, these kind of matrices, or these kind of sketching operators, which also, you know. which also offer efficient algorithms to perform this sketching modification. Okay, so that's what we aim for basically, right? For Runblast to provide all these operations. Of course, it doesn't really matter that we're showing sketching from the left, we could be sketching from the right. But just for consistency, that's what we will show. Okay, so now as an example, Okay, so now as an example, and as a sort of example of what can really work well in practice, we will talk about some sparse sketching operators that we think should be included in such a standard. So we have, again, our own little custom taxonomy here. So we have these things called short axis sparse sketching operators which What does that mean? So basically, that means we have, maybe it's useful. I don't know if I can use the whiteboarder. I guess not so much with the upside-down video situation. But so, I mean, the idea here is we have the sketching matrix, right? So we want to construct a really sparse matrix. So that means most of the entries of the matrix. So, that means most of the entries of the matrix are zero. We're going to start in a sparse matrix format. And so, how do we choose the non-zero entries? What do we make then? So, the simple answer for what do we use as non-zero entries is plus minus one entries, which is, I mean, established that works pretty well. And then the question is: you know, how many Question is: How many non-zero entries do we want, or how do we put them? So, one of the kind of well-motivated by the theory approaches is to take each column of the sketching matrix and put some number of non-zero entries, let's fix that number as L, put L non-zero entries uniformly at random in each column of the sketching matrix, and that's what we can do. So, naturally, the cost of this operation should scale with the Should scale with the number of, you know, with how many non-zero entries per column we decide to use. So that's essentially what it says here. You know, for some historical context, you know, this has been motivated by theoretical literature, particularly in TCS literature, theoretical computer science, with methods such as Count Sketch, SGLT, all SNAP. I'm not going to get into these specific terms. We have our own term, which is. We have our own term, which is the SASL. And so, I mean, we actually have our own sort of implementation that is available. And what we've realized as we worked on this is Worked on this is that there's really a lot of room for improvement, particularly for performing sparse sketching operations efficiently compared to what the existing packages provide. So here, on certain examples, we can get very substantial speed up in how efficiently we can perform these sketching operations, particularly in the Operations, particularly in the setting where these sketching operators are extremely sparse, which is really the most relevant case in practice at least. We will also consider a slightly different type of sparse sketching operator, and we will see, you know, we will motivate that. So, which are called long-axis sparse sketching operators. So, why did I call Sketching operators. So, why did I call, where do my names come from? So, the short axis here is the columns, right? I mean, if we flipped everything around and we were multiplying from the right, things would be reversed. But the point is, we're kind of sparsifying along the short axis, specifying the number of non-zeros along the short axis, and in the LASOs, we're specifying the numbers of non-zeros along the long axis. Also, this actually makes a difference in the sense that it allows us to actually produce even sparser sketching operations. So, if you think about what the sketching matrix is, if we just use one non-zero entry per row of the sketching, it just means that it selects one row of the data matrix A per row of the sketching matrix. And so basically, it's just subsumpling the data, right? So, that's the Data, right? So that's the extreme case of it. And to provide some sort of functionality for various important sampling methods that have been shown to be effective, such as leverage square sampling, you know, these LASOs support that functionality. They support essentially choosing this non-zero entry in a non-uniform way, if that's useful for an application. And in particular, they go out once. But in particular, they go one step further and allow you to select multiple non-zeros per row. And so now we're in the realm of not just subsampling rows, but also some form of sketching, but that can potentially be much, much faster than even the sparsest possible SASOs. And so we decided on this actually after some implementation and some empirical analysis that we will see. Yes? Yes. So do you envision a implementing this for a n's matrix, A? The LASSO? Yes. That doesn't make sense to me at all, because LASSO basically is about as expensive as reading all the entries. And SASO is reliable, whereas LASO is not, without assumptions in the matrix. Right, so yes, so in some sense, worse, so and we will see exactly the So, and we will see exactly this in a few slides. So, basically, the point is: LASOs are more kind of designed for robustness, and LASOs are designed for, you know, if you optimize the parameters for, you know, a problem or domain, you will get better performance. And that's what we will see. So, I mean, yeah, so there's a trade-off here. There's a, yeah. Just seems to me that the traditional gain is speed. Additional gain in speed is, I can't think of an environment where that is worth it to give out. Well, I mean, I think I can only point to the fact that most data matrices are just literally uniformly subsampled. I mean, you can think of it that way, right? And so then, in a sense, Lasso gives you some more robustness on top of just sub-something. On top of just sub-sampling a matrix, which is L equal 1 is interpretability, basically, right? Because you have something in rows or whatever from the original matrix for L equal 1, but for L greater than 1, I also have the same concern. Are you losing the productivity? Will you gain anything? Just a comment. I mean, yeah, if you would let's see the results and we can talk about more. Yeah, sure. Okay, so. Okay, so first, the point we can make is: well, so we have implementations that, so the point is we kind of need good sparse matrix multiply implementations for really to kind of unlock the sort of efficiency of sparse sketching. And so we have some proof of concept implementations that Implementations that beat Intel MKL implementations by, I guess, in some cases, close to a factor of two. And okay, so... I'm sorry, and the accuracy? There's no accuracy because we're just talking about spice metric complications. That's it. So right now we haven't. And this is not specific to SAS or LASSA, it's just just. Specific to SASA or LASA, it's just, you know, you could choose either of the options. And I think this was the SASA one. But yeah, I mean, the main point here is that we want to provide this optimized sparse matrix multiply for using sparse sketch. Okay, so So let's look at an example of preconditioned least squares. So I think we've also seen that. So we have a least squares problem with A and B and by n matrix A. And so, I mean, the procedure here, I mean, we've seen we're basically going to use the sketch. To use the sketching operator S to construct the preconditioner for our iterative solve. And then, of course, the quality of this preconditioner, one way we can measure it, so we will look at how that works out. So we can look at the condition number of. The condition number of the precondition matrix to kind of see or to have some sort of an idea of how well the iterative solver will perform. So, this is closely related to the subspace embedding kind of conditions or guarantees that randomly theory has provided. So, just as an example, how good of a precondition can be? How good of a precondition can we get? Well, so here we're looking at the SASO sketches, so those ones that are more robust, as we mentioned. So we can consider two kind of extreme cases of input matrix. On one hand, there's an input matrix that's Gaussian, so that's kind of easy to To, well, it's an easy instance of the problem, harder instance is something called high-coherence matrix. And so that essentially contains some rows that are very large, some rows that are very small. It's kind of designed that way. And so we can, so here what we're plotting is the number of non-zeros per column of S, right? So how dense the sketching matrix is, versus the condition number of the preconditioned. Condition number of the precondition matrix. And so basically, what we see is: okay, if the number of non-zeros is very small, very close to one, the preconditioner maybe is not very good. But even after bringing in a few non-zero entries per column, very quickly we get roughly a constant condition number. And this is true for both these kind of hard instances and the easy instances. Hard instances and the easy instances. Okay, and so we can translate this into an implementation and we can see the performance gains. And you can see this even for this Python implementation. So it's clearly not an optimized plus implementation, but already we can see that it's faster than directly solving for this particular problem. Directly solving for this particular problem. This is a very tall problem for which we would expect this to work well. So we can see, you know, kind of 1.8 times faster. And clearly, this is an ill-conditioned problem to begin with. So the preconditioning would be neat. And so we can break down the. You're just plotting the absolute error, aren't you? error are you it's just the absolute error and it's not even of the true residual life yes so that's that's a good point and that's actually I mean that's so the basically looking at the the kind of the the error that you know that is you know kind of used by the solver to kind of get the stopping criterion. Get the stopping criterion for get a stopping criterion. And so, okay, so then the third comparison would be to compare to the error that does not involve the randomness of the sketch conditioner. And we will actually, so I can mention, I mean, this is indeed somewhat of a kind of a challenge because the quality of the precondition to a certain extent affects. Affects how we should choose the tolerance for the solver. So I will actually have further kind of experimental plots that basically where we basically optimized for choosing the right tolerance so that the proper relative error is comparable. Actually, I think Anne Ripo is there, right? And she has, I think she was one of the ones who pointed to a maximal attainable accuracy. So that is something that you may want to think about. Okay, well, yeah. Yeah, thanks for pointing us out. So, okay, we have kind of a breakdown of the operations here, right? We have the sketching cost, we have the factoring cost. Cost, we have the factoring cost, right? So constructing a preconditioner, and then the iteration cost, right? So essentially, you know, those are, you know, those are the kind of key costs. And depending on the parametrization of how we parameterize the sketch, the shape of the problem, you know, different one of these operations might be a bottleneck, right? So in this case, the kind of the iterations are a bottleneck, but it might be the case that the factoring is a bottleneck as well. Okay, so what does the performance? So, what does the performance landscape of randomized kind of least squares with preconditioning look like? So, that kind of goes towards some of these questions. So, basically, well, there is some parameters to tune, both on the deterministic solver side and on the sketch side, that really affect the performance. And so, that is, you know, that I mean, that is a challenge, that is something we have to kind of Is something we have to kind of figure out. And so, as an example, here, we can look at, I mean, we can compare several solvers. The thing that we mentioned is the stopping criterion, that also is effectively a parameter we have to decide because the randomness affects the stopping criterion, so we might have to adjust the tolerance. And we've seen that that matters in certain corner cases. It usually doesn't matter, but in certain It usually doesn't matter, but in certain corner cases, it does matter. And then there's the choice of the sketch, right? So we could choose either of the sparse sketching operations. We could choose Gaussian, we could choose some sample Fourier transforms and so forth. And then there's always the oversomething factor, which just says what is the sketch size that we're going to use, which will affect the quality of the conditioner. The other thing for sparse sketches in particular. thing for sparse sketches in particular is this the sparsity parameter L, which the number of non-zeros per row or column, which also potentially significantly affects the quality of the precondition. So, okay, so here's an example of the landscape. So let me kind of walk through this a little bit. There's a, I guess there's three. I guess there are three input matrices, right? So there's low coherence, medium coherence, high coherence. So these are synthetic matrices, kind of synthetic random matrices, but they're designed to be kind of easy, medium, and hard. So if you're familiar, coherence basically means how non-uniform, it measures how non-uniform the distribution of the leverage scores of a matrix is, which kind of is known from theory to have an impact on how these things work. Have an impact on how these things perform. And then for each one of the inputs, we have six kind of categorical choices between the two types of sketch, which we're looking at here, SASA and LASA, and the three types of iterative solvers. Here, the third one is sort of a more general purpose optimization solver, just shown here for the sake of comparison. Okay, so there's a lot going on here. Clearly, there's more going on in the hard matrices case. And well, there's a few takeaways that we can make, which we partly correct. So some problem solvers for But you know, when we're dealing with specifically squares, that's not would not be our first choice. Number one, that if we want to get a parametrization that provides a best performance, then what we really want to use, and this is regardless of the coherence of the matrix, as it turns out, is we want extremely sparse sketches, right? So these kind of last type sketches. So these kind of lasso type sketches, that's what turns out to win over. And by non-negligible margin, by a factor of maybe 2.5 up to 3. But of course, we can see what happens as the kind of problem gets harder, which is that we have to adjust the parameter, the sparsity parameter more substantially in the case of the extremely sparse of the parameter. Of the extremely sparse of the more sparse schedules compared to the SASO ones, which are kind of more, like we said, more robust to the hard instances. Okay, so... Sorry, I'm not sure. Could you just explain the two axes for like one particular example? Sure, sorry. Over something factor, meaning sketch size. Sketch size. I mean, this is like the dimension of the problem times the sketch size. And the other one is the density. It's basically the density, but the density is measured differently here than here because it's, you know, row-wise versus column-wise. So, like, the 10 here is much sparser than the 10 here. And what is the algorithm? Sorry, I'm just a little on the lasso side. It's literally, it's what we talked about just basically if you uniformly sampled. Basically, if you uniformly sampled the non-zero entries per row, and then those are plus minus ones. That's basically the output. And then you have a preconditioner based on the. And then this is the preconditioning out. So then you just implement that, right? So this is your S, so the choice of S, right, is what we're talking about here. But after that, the algorithm falls, and then the iterative solver is also. Yeah, the editor of solver is also chosen, right? The tolerance, the tolerance is chosen under the hood. I didn't show that parameter. We already optimized for that before we showed. Can I ask a question? Sure. Sorry if you already mentioned this. So when your solvers is blended pick, like, what does that mean? So does that mean you're actually because blend and pick also does mean if I'm sampling the preconditioning? So are you actually preconditioning? Like, are you double preconditioning or something? Or there might be misinterpreting a Demon misinterpreting that? No, it literally just means that we're using this case, it literally just means we're using an LSQR solver. So actually, basically, what you're seeing here would be what we would do in the BlendenPIC case. The LSRM case is SVD for getting. Use this SVD for for getting the preconditioner and and then um I I think it's not a double preconditioner. One is one the first set of steps produces an initial guess for LF law. So numbers three and four produce the initial guess and five computes the preconditional And five computes the preconditional for blended pig. Yeah, I think I get it. No, I think so. You're saying that I think this entire algorithm you're calling blended. It's not that just step six is implemented by blending pig. I thought what you were saying was step six was implemented by blending pig, so I was confused. Yeah, so you're right. So the naming is just blended. Sorry, say again. Sorry, say again? You're using the sketcher installed, has that zero for blending thing? Yes, that's the z0, yes. So that's important to do this to remove the dependency on the dynamic range of the problem. This actually, people call this one to pick, but this is rock one and tiger two times eight. It really is, yes, that's that's absolutely true. That's absolutely. And they also observed that you need a reasonable starting guess, and the sketch and solve does well enough to remove some dependency on scaling. Right, so it is the right-hand side versus the error of the solution. So we start with an approximation, a coarse approximation, but it has some sort of a guarantee and then the iterative solver gets a high precision guarantee. Come back to my question. Can you guarantee that the technology will be good enough that be good enough that your iteration six will converge uh your so much maximum number of iterations you're asking your ass I can't guarantee the problem if you want if you if you sketch if you sketch the matrix by applying a randomized transform and then you do uniform something then that thing does pretty well actually. Yeah so I mean I mean basically you know it's it's it's all randomized algorithms so the point It's all randomized algorithms, so the point is: you know, if you choose the parameters right with high probability, you get a good precondition. There's always a failure probability, and I mean, that's... But you have to pre-multiply by randomized transform before you do the sampling, right, to increase the coherence. Well, whatever, increase the coherence. And then it does well. Yeah, so I mean, so I mean, uh, if you use a you know, a sparse sketching matrix with the right choice of sparsity. With the right choice of sparsity, that also effectively addresses the high coherence issue of the matrix as well. So, those are just different strategies of getting a good precondition with good probability while not blowing up the cost of the sketching itself, which would be, I mean, the ideal thing would be to use a Gaussian sketching matrix, let's say, and then you would have a valid preconditioner basically with probability one. Basically, with probability one and a good precondition with extremely high probability, but we don't want to pay that cost. Well, what happens is that the precondition matrix has a really low condition number, right on the order of 10 or 100. And then LSQR converges really fast. Yes, exactly. Okay. So did that address your question? I think I have a question. So, does your least squares problem have an exact solution? I mean, is there I mean, is there an x such that A is actually equal to B? Because it looks like you got 10 to the minus 15 or so. Oh, no, we're looking at the error, kind of, there are a lot of error away from the, oh, you mean here, right? Yeah, those are absolute errors. Those are not relative errors. Yeah, so this, yes. Well, so here we're using relative errors. I promise you, here we're using relative errors. I promise you here we're using relative and so this is comparing. So there is no exact A A x equal B. There's an X star that minimizes it. And we're looking at I mean A of X hat minus X star normalized. So what's the error here? Then the A X. So the other plot is it's sort of the normal equation here. So it should go to zero. Can you show the formula? Can you show me the formula? Oh, sorry. Yeah, you have A transpose times Ax minus B, so that should go to zero. Oh, yeah, yeah. Okay. Alright, well there's a lively discussion, so I think I'll probably only get through the first part of the tutorial, which is the software part. That software part. Um okay, so and so just to comment on like the failure, right? So there's clear, there's the there's there's um some scary X's here. Those are the failures, basically. And those are you know in the cases where you know we would not not expect, well, I mean, certain cases that we would not expect these things to succeed. For example, when the oversampling factor is one. It's not really going to work out. But the x's kind of multiply once you reach kind of high coherence matrices, those kind of difficult instances. Okay, so no more questions for now. Okay, so then another thing we wanted to Another thing we wanted to highlight is, well, sort of the power of problem reformulations. So we in the book we actually write down essentially the sort of sketch and precondition algorithm, not just for least squares, but as sort of this kind of more general saddle point problem, which has Which has its dual version as well, and essentially allows us to be able to capture a range of problem settings, let's say, including ridge regression, and you'll see there's variants called kernel ridge regression, which are quadratic minimization kind of tasks, natural Linear algebra tasks that we can sort of reformulate as essentially a least squares problem so that we can give it to a basically to the LSQR solver. So that's something that we can do and that actually you can see that these reformulations really can significantly improve performance in the sense that we can go. Performance in the sense that we can go from essentially having to solve the normal equations using CG to kind of converting that into a version of the problem that can be handled with LSQR and that helps with convergence and numerical precision issues, particularly. Okay, and so there's another kind of example of this. We can have back a little bit more ML machine learning. A little bit more ML machine learning motivated version of the problem, which is kernel regression, and here you can also reduce that essentially to a normal equations type problem, which can be solved with this kind of reformulation setup. Okay, so I'll briefly go through kind of, so we have, we're kind of talking about, we sort of have We sort of have three sections, right? One is for least squares and optimization type problems. And so here, you know, we had sketch and solve, we had sketch and precondition. The other part is low-rank approximation. So approximate low-rank matrix decompositions where we can consider a range of algorithms. A range of algorithms. So, the obvious algorithm here that I'll just mention that we've all heard about a lot is randomized SVD and its various variants, power iteration, and so forth. And so here we can kind of consider, we can ask, how do we parametrize again these algorithms? So one way to think about it is we can ask, About is we can ask, okay, just produce a low-rank approximation using a particular method that is exactly rank k. That's one way to formulate. So here we're not really asking specifically how good is the accuracy of the low-rank approximation. Maybe that's something that the algorithm returns. But we want a specific rank approximation. And then another way to formulate. And then another way to formulate it is we want some Lauranque approximation, but we want to have an expectation of certain error at the end. So then the algorithm can accept the error epsilon and kind of uses that information to figure out what LoRank approximation for this particular setting we can use. Okay, and then we also can have And then we also can have full rank decompositions. So here I'll just highlight briefly you are with column pivoting. And so I think I'll not spend much time on this because I don't have much time and because Laura will talk more about this, I think, later today. And so of course, this is, we want to do QR for essentially a permutation of For essentially a permutation of the columns of the matrix A. So, of course, there's standard implementations in LA facts such as QP3, but the deterministic implementations are considerably slower than the unpoverted implementation. And so we hope to address this with using randomization to speed up the column pivoting. So, I mean, we essentially have taken existing well-developed implementations for this, which are essentially based on the idea of using the pivoting on the sketched version of the data matrix. So, this is an implementation that's essentially very closely. An implementation that's essentially very closely adapted from this reference. And we provide some empirical benchmarks that calling this QPR so that essentially what we can see is that, well, yes, the randomization does substantially accelerate the pivoting. So that just means that, so if we compare higher is better, basically in these plots, hires come faster. In these plots, higher is kind of faster. So we can compare the unpivoted, which is the fastest one no matter what, versus the pivoted deterministic pivoted randomized implementation, we get that the randomized implementation is much faster than the deterministic one. So there's two comparisons with one MKL and one with another library. We have up to five. Library, we have up to five times speed up. So clearly, there's good motivation to sort of have good implementations for this. Okay, so I am almost out of time, so I'll just literally show one slide. And so the question here is what kind of what kind of theoretical analysis should we be doing to kind of understand to To help develop run the laypark and to understand all these trade-offs. So we saw this parameter landscape to understand all these trade-offs in the performance of these algorithms. And so, I mean, essentially, kind of the bottom line here is, you know, maybe the Maybe a lot of the random lane theory, the foundational theory that we have developed, is perhaps more sort of worst case than would be applicable or perhaps useful from the perspective of Randall APAC, in some cases in terms of the sharpness of the error estimates, and in particular in terms of the parameter regimes that the theory can cover. Regimes that the theory can cover. So we've seen that maybe you want the right choice in practice for the sketch size for the preconditioner is twice the dimension or three times the dimension of the data matrix or the smaller dimension of the data matrix. Whereas the theory only starts once it's 10 times the dimension. And these are regimes that some reason. Regimes that some recent theory is beginning to be able to explore, and there's actually certain phenomena that occur in this regime that may not be observed by this kind of worst case theory. Okay, so I think, and then of course, you know, we want to also be able to essentially cover the kind of sketching operators that Of sketching operators that actually work well in practice. So, in particular, the various sparse sketching matrices as opposed to, let's say, dense Gaussian or plus minus 1 matrices. Okay, so I have a bunch of more slides, but I'm just going to leave it off here since we had a lively discussion and take any further questions. Thank you very much. Thank you. So I have two questions. The first one has to do with point two from the previous slide. So if you have a, if you take a user who has a list of response to SAL, they could either use the standard approach, like you have a back-end, or uh use one of these techniques. And the problem is that you get a lot of parameters here. That you get a lot of parameters tuned, and it's not necessarily even work for the problem. If you make it a high accuracy, or so the question is: how do we handle that? Or is it going to be done by a rent-lab or by the news? The second question has to do with this sparse case, with the sketching for the sparse case. There are alternative techniques which I claim may work better in that situation, and that is coarsening. You have coarsening, and we use your S comes from the coarsening part. Your S comes from the coercive part. And I don't know why it has not been tried against these techniques. In that situation, you're exploiting the data. You know the data and you're exploiting some knowledge. Whereas in the sketching techniques, basically the sketching matrix is not aware of the is just blind. So it's clear that for me that just from that perspective we'll get a gain by using horsemen. Again, by using coarsening. So, these are my parts. Okay, so to address them, so the second part, I'm may not exactly be familiar with the coarsening, but I agree that. So, I mean, the nice thing about many of the sketching methods developed is that they're, to a certain extent, black boxes, which can be sort of oblivious to the data matrix. But we actually saw even in these simple sparse kind of sketching operators, the stars versus lasso, actually, I mean, if your data weren't. Versus Arthur. So, actually, I mean, if your data were in some sense, so if you kind of, you know, if you don't expect to get the worst possible data matrix instance, then you can get considerable speedups. And we can see that. And I expect that you're right as well that carsoning might help when you know something about the data matrix, then definitely you should be able to to imp improve that kind of sketching performance. Yeah. And the first question was Yeah, how do you handle how does the user feel this function? How does the user feel this function? Well, that is a very good question. I mean, that's why we're kind of exploring this space of the parameters. And I mean, I think we don't have complete answers to that question at this point, I don't think. I think we would like to provide a lot of kind of flexibility so that some of these parameters can be. That some of these parameters can be chosen by the user, but ideally, you know, maybe some higher-level kind of driver-type algorithms can be developed that sort of do you know internally kind of figure out a good or a safe parameter regime. And that should definitely be an important part of solving. So that's a departure from standard flatback. Flatback, you get problems to get a solution, generally without failure. Generally, without favor. That's the yeah, well, that's I mean, but that's unavoidable. Yes. Thanks for your great presentation. Thank you. Well, what would be your opinion? Can I address one of you Seth's question? Oh, go ahead. Sure, oblivious versus non-oblivious sketching. The advantage of obliviousness is we can try to elicit the companies like Intel and NKL who build the blas, build the RAM blas, and hide all the details under the cover if they're. All the details under the cover if they're oblivious. So that's one of the reasons for having the RAND blocks, which are oblivious. Two questions. I mean, what would be the default value for the failure probability? The default value for the failure priorities from the user, let's say you have like an error estimator for the randomized SVD. It's a random, it's also a random estimator, like that's okay. A random estimator, like that's a failure probability. So, what would be the default value for the failure probability you would recommend? So, I don't, I don't, I wouldn't think of the failure. I mean, the default value would be, you know, small, let's hope, basically. I mean, we don't. How small? Is it like machine epsilon small? Like 10 to the minus 6p? I mean, it really depends on the sketch, on the sketch size. I mean, I would say, you know. The sketch size. I mean, I would say, you know, if you, I mean, if you use a dense Gaussian sketch, you know, you're definitely expecting until minus 16 failure probability, and you get a small condition number. And the sketch size is, so we have bounds that are much better that start before 9 times n. And sketch size is about 2n. If you have a really tall and skinny value, with a failure probability of 10 to the minus 16. I I mean, there are some problems where you just can check the residual, right? For least squares, you can check the residual and you can investigate our properties. But there are other problems, like if it will be randomized as a result of the problem. You have a least squares residual. So the least squares residual is not zero. And the least squares problem conditionally depends on the size of the least squares residual. Right. So there are okay, so we agree there are a number of problems where you cannot check a posterior d error. Apostle DL in a deterministic way. Or at least not without having to, you know, occur additional significant cost. Right, yes. So what would be the failure probability you would hard code these problems? So you can't really hard code. I mean the failure probability comes from the theory, and the theory, you know, is worst case. So you can't. But you cannot offer a randomized LTD out in the world. A randomized SPD out in the world, please everybody use this as a black box and not hardcore a default value for the failure probability, right? And what I was asking is: if you choose to make a sketch size 1.2 or something, something really stupid, then you will fail. There's a very high probability that the output is going to be used. That's what you're saying, right? No, for example, the randomized SVD is, I think, used in maps at a very deep level to solve the process system. It's deep down, right? It's deep down, right? You cannot kind of set the user parameters well, because it's just very deep in the software. You cannot kind of to solve as fast in your system. Please set the failure probability of a function that's used software very deep down. It's just not an option. You can set up the failure probability. That in turn determines the number of samples. So if you say, hey, I want my failure probability to be hard 10 to the minus 16, you can do that. 10 to the minus 16, you can do that, and that becomes the number of samples that are being used. Right, and my question would be: how would you, what would be your default choice? I think it's a good question if you want to use it as a black box. That's a fair question, but the problem is if I, in the first place, parametrize the problem as, or the algorithm as, you know, the user gives me failure probability and the algorithm figures out everything else, then what the algorithm has to do is the algorithm pretty much has to consider. The algorithm pretty much has to consider the worst case, and it will I mean it will way over kind of estimate the size of the sketches that it has to do. So that's problem number one. And then problem number two is, I guess, what failure probability should the user choose? I don't know about that. I think, yeah, you had a second question as well. No, no, no, that's fine. Just one. In the interest of time, maybe. The error estimator is much faster than the computation size. So you can set it to be very conservative without increasing the total computation size. I mean, still if you set it very conservative, you have to choose the probability at one point. Yeah, so it's 10 to the minus 10. Okay, so 10 to the minus 15. Okay, let's get to Joel because he's. I also have two questions. I mean, it's related to what we've been talking about. The first one is sort of a question about design criteria. You know, design criteria. So it seems like there are a lot of things that are here being called random blossoms or routines, which are kind of a collection of hammers and saws and things like this. I do kind of wonder whether at some level we're giving users a table saw with the instruction not to cut their fingers off. I mean, because some of these things are useful, but only right now in the hands of an expert. It is not reliable. So things like uniform sampling, you know, works if the matrix is low. Sampling, yeah, it works if the matrix is low coherence, otherwise, it's a disaster. Whereas, and so I can understand that perhaps the philosophy is we should give people lots of things that are well implemented so the experts can take advantage of this. You already mentioned, though, at the level of what you all call drivers, it feels to me like these things have to be bulletproof. That when you type backslash MATLAB, it should solve your linear system. It's the same here. If you type backslash and there's a randomized Type backslash and there's a randomized first algorithm inside, it should fail probably 10 to the minus 10 or something like this. And this should be almost invisible to the user. So how do I guess these questions about reliability factor into the design plan here? I mean, I think that's still kind of a lot of open questions, but Lot of open questions, but you can certainly say for certain problems like we were talking about, if you want a version of the algorithm, like the least squares algorithm, that has a certain guarantee, then maybe you're going to have to do this several times until you get it right. And then your failure probability is zero. But this is, again, a lot different from the way that people usually interact with software. People usually interact with software or with APIs. But anyway, the second question actually is also related to this, which is that this is a controversial statement. So to what extent is a priori theoretical analysis of randomized linear algebra informed? Because I have the sense that for a lot of these algorithms, we you know they are adaptive, they determine parameters, and then they do a posteriori error estimation to confirm that the solution is correct. That the solution is correct. You know, we spend a lot of time proving theorems about, like, oh, if this parameter is this, if this parameter is that, then this will happen. But I don't think that's how people probably should actually be interacting with these algorithms, especially because there is a chance of failure. It seems to me that the posterior stuff is underdeveloped and more important because, you know, in the end, it's like these things are reasonably predictable, but there's some sampling distribution. And, you know, in the end, you don't know what the matrix A is. All the results. You don't know what the matrix A is. All the results say: like, if the matrix A looks like this, then I can predict what the parameter should be. But if you knew that, you'd have already solved your problem. Yeah, so I mean, and so this is, there's, I think, a section in the Randolph book that basically talks about, you know, error estimation, right? So that's, but again, that's, you know, for certain problems, that is only error estimation, so that also has a failure probability. But yes, I mean, I mean, I mean, I agree that that's kind of an important part. I think my concern about a lot of the prior theory is that it involves unobservable parameters. And so I think it's interesting, but I also don't know whether it's actually algorithmically important for the design of software. Perhaps that was in the theoretical direction section to develop ones that observed quantities and more. I spent a lot of time proving theorems. In the interest of having. Well in the interest of having actually a coffee break, I'll wait for the speaker breakouts. Thank you, Jim.