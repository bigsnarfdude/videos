Thank you very much. So, well, first of all, I apologize that I couldn't make it in person. I would have loved to do that. At the moment, I'm still the head of the department, so it's very difficult for me to leave for a whole week. So, thank you for giving me the opportunity to give the online talk. The online talk. So, my talk is, I'm afraid, slightly not really central in the topic of this conference, so I hope it's going to be of interest to you in any case. And also, I would like to say that this is a very recent work that I'm doing with Ariela Biriani from the University of Tours and Itar Shiishi from Vazed and Sude University. From Vased and Sude University. And it is a completely new topic for me. And so I hope I'm not going to forget to mention important results of other people because I'm quite new to the subject. But it seemed to be opening a lot of very, very interesting questions. So I hope I will interest you with them. So let me start with the very The very classical result. This is a result due to Jack Hale and Genvi√®ve Rogel in thin domains. And this question was solved in beginning of the 90s, end of the 80s. Sorry, sorry. Okay. And so the this. Okay. So the problem here, you see, you have a domain. So the problem here, you see, you have a domain that is in Rn plus one dimensional domain, and you think of it as like in a box. So you have the omega that is in Rn and then in the n plus one dimension instead you have this function that is defined between zero and epsilon times g. And then the idea is that your function g should not touch, should never be zero, so the infimum is strictly So, the infimum is strictly greater than zero. And then this problem is very classical, you see. And we have a boundary condition that is a Neumann condition. And then you let epsilon go to zero. And what happened when epsilon goes to zero is that your Sohale and Roger proved that if G is C3, well, then the C3. the the the sequence of solution upper converge to u0 which is a solution of in so this is a function that lives in the omega so remember omega is the n-dimensional space so we have lost one dimension of course you see here y is zero but as you can see there is a lower order term that has appeared this term that is in red and you have this so this new term and so it's quite natural to And so it's quite natural to wonder where does the new term come from. So let me give you a sort of an explanation that is more or less a heuristic of what is done in the proof of Halle and Rogeri. So the idea is to go back to the equation in omega epsilon, but to straighten the domain in such a way that this becomes a box. Becomes a box. Okay, so we straighten the upper domain. So you make a change of variable, x doesn't change, but y, you take little y f and g x so that when little y is equal, so your new variable is x and y, so when little y is equal to one, you actually are on the boundary, okay? So for with this new variable, you have your domain doesn't shrink anymore because it stays like in a box, so I called it q, like a cube. Like a cube, and but you get a different equation, of course. And you see, in the equation, the only term that has something nasty about is that you divide by an epsilon square. Okay, so this is a divergence equation. We started with the Laplacian. So, of course, this is not surprising that you still get this sort of equation. So, okay, so this is what you get. Remember that we want to let epsilon go to zero. want to let epsilon go to zero so the term in front of dg square is not going to bother us but this is sort of annoying so anyway so you go ahead you write this in divergence form so you use a weak formulation and you see that in the weak formulation you have this term that you have this term that appears the term v epsilon grad v epsilon grad g that is multiplied by g that is multiplied by phi, not by the gradient of phi. So this is a first order term. And then it's easy to understand that since we have this term that was divided by one over epsilon squared, these things needs to be bounded. So in fact, what happened is that vy is going to go to zero. And so this can be proved using a priori bound. And then when you pass to the limit, well, Pass to the limit, well, what happens is that the big more goes to the identity, and so you see that this term has appeared here. So, this is a very simple computation, but very variation. This you see is very much related to the fact that we started with something that, you know, it seems to come from the equation, but from the divergence form of the equation. So, I heard a talk about About the consequences of this domain by a talk by Marcone Pereira, and who has worked a lot in this field, I will tell a little bit later on. But still, all the proofs seem to be very much related to this divergence form. So, the fact that one could somehow use this. So, I wondered if it was possible to see what. Possible to see what happened when you have instead a fully non-linear operation. So here, take whichever is your favorite operator. You can even think of the Laplacian if you prefer. But mainly what I care is that I don't want to think of this equation as an equation that has in divergence form. So I'm thinking of solutions, say, for example, viscosity solution. So this could be. Solution. So, this could be the Pucci operator if you're familiar with it. I'll go back to that for a moment. Or just take any trace of A of X V to U if you prefer. I will give some examples. And in fact, what I want, you see, it seems can look very sort of ugly, but this is a very natural condition. I'm just asking that it is elliptic in the sense that if you're sense that if your your second order, your Hessian matrices are ordered, then the operator have to be ordered. Let me at the moment insist that this f like minus the Laplacian because you see if y is less than x, I get that f of x is less than f of y. Okay, so what I am thinking in my mind that f is a monotone, but monotone like minus Laplace. This is a less or equal, this is ellipticity, this could be degenerate elliptic. Okay, so the condition is not strong, ellipticity is degenerate elliptic. Instead, I need, of course, for the operator to have some properness, I need that in the variable u, in the variable u, I need that some monotonicity, some strong monotonicity, so the difference of the operator computed. computed in u has to be alpha times r minus s. So in the previous example, this would be just the fact that instead of taking minus the Laplacian, we were taking minus the Laplacian plus U. Okay, so everything like here is this is very simple. But you see the difficulty is that now it's not clear. Okay, so we have our PDE problem, the domain omega epsilon is the one I draw at the beginning, so we just simply. So, we just simple equation, and we have a Neumann condition. Well, let me mention that here, the fact that there is the presence of corner in my domain when I'm talking about viscosity solution makes the game a little bit more complicated. But nonetheless, so this is the equation. And okay, and I will tell you, I tell you immediately that what we have found is that. We have found is that if this is your equation, your limit equation, so the function u epsilon will somehow, I will tell you why, how, will converge to this limit equation. So you see that now the Hessian term has changed in, okay, the function w now lives in omega, so in an n-dimensional set. And so this is a Hessian in. So, this is a Hessian in the x variable. And you see here, the term that has appeared in what was the second derivative in y you want is exactly the term that is grad G grad w. Sorry, this is as gone away. So, this is exactly what you would obtain if this was the Laplace and you take the trace of this matrix and then you get exactly the Laplace. And then you get exactly the Laplacian plus the lower order term. The rest you see stays naturally. So the same thing as before: that the derivative in y's goes to zero and your function goes to a function defined in x0. So, but now where does this come from? I cannot make it come from the operator. So let's see. You see, I simplify the equation. So suppose. I simplify the equation. So, suppose for a moment that what we have here is minus f of d2u. Remember, if you want, think of the Laplacian. And again, I do the change of variable as before. I fix this change of variable. And again, as before, I can imagine, so this is that using a priori band, I can similarly obtain that the second derivative yy has to be smaller. Has to be smaller than epsilon squared. Why is that? Because you see, when I do my change of variable, I have this term one over epsilon squared that appears, and in fact, it's in front of the second derivative. So, this is the natural hypothesis that I can make. And so, what is the answer that we have supposed? Well, you see, we said, okay, so we expect this function v epsilon to have a term that depends only on x, and then you have a term that goes like epsilon squared. That goes like epsilon squared, but you want that the second derivative goes like epsilon squared. So you may have something that is a function of x, which is what I called k of x. And instead, there is y squared over 2, because so the second derivative comes 1, and this c is somehow this k of x. And then I suppose that there are the lower order term. Well, if you do this, okay, and then you let epsilon go to zero, well, Let epsilon go to zero. Well, simply what you get is what you see here. This because of the change of variable, then you let epsilon go to zero. This term 1 over g squared come from the change of variable. So at the limit, what you obtain is this quantity. But remember, we still don't know what is k of x. Well, to find k of x, that's, of course, we still haven't used the Neumann boundary condition. Use the Neumann boundary condition. So we look at the boundary condition, but we look at the top. Okay, so the top boundary condition, in terms of my handsat, so this hypothesis that V is written in this way, like sum of w plus epsilon squared, k y square over 2, well becomes for the boundary condition, become what's written up here. What's written up here? Then you let epsilon go to zero, and you see that this gives you an information on k of x. K of x has to be equal to k over g has to be equal to grad g grad w. So k over g square has to be equal to grad g grad w g. So this is exactly what we obtain. So from the ansatz, we obtain exactly the equation that we expected. Okay. That we expect. Okay, so let me look at the time. So this is, of course, this is not a proof. Okay, this is just a way of understanding what's going on. But already I think this is, it sheds a completely different light on the term that is here, that is not appearing by some integration by part, but it's really appearing because you expect your function to have a certain behavior. So now Certain behavior. So now we have to prove this. Okay, I'm not going to go into detail, but let me just mention what is before telling you that. I would like to tell you what happened, for example, when you look at instead of the Laplacian, which is exactly the case that one already knows, but suppose we look at this Pucci operator. So, as probably all of you know, what I mean by Pucci operator is that you take the supremum among all leads. Among all linear operators that are bounded above and below by lambda small capital lambda i and capital lambda i, which also can be written as the sum of the eigenvalues, but in the hypothesis that when the eigenvalue are negative, I multiply by small lambda, while instead if the eigenvalue are negative positive, you multiply by capital lambda. Capital lambda is By capital lambda, capital lambda is larger than small lambda. So clearly, if small lambda is equal to capital lambda, this is n equal to one, this is just Laplacian. Instead, if they are not equal, this operator is highly non-linear, is a fully non-linear operator. And what is interesting here is that, of course, here I'm taking M plus, but if you take M minus, you just take inf instead of soup and small lambda, capital lambda exchange. Capital Lambda exchange roads. Okay, so now you see if you look at what was the limit equation, then I have to do the Pucci operator on this matrix. But you see, this is a diagonal matrix with respect to this term. So in fact, the eigenvalue of this matrix, the the Hessian matrix written here, are the eigenvalue of the Hessian matrix of D2W. matrix of d2w and then another eigenvalue is exactly dg dwg and so like this of course so the m plus of the limit equation is going to be the m plus of d2w of course here the matrix is n by n instead at the beginning this was n plus 1 times n plus 1 and here well this second value this term grad g grad w This term grad G grad W over G, this could be either positive or negative. So clearly, if it's positive, you multiply by capital lambda, and if it's negative, you multiply by small lambda, and the rest of the equation stays the same. Okay, but so this is an example of fully nonlinear, but still this one is uniformly elliptic, so close to the, in a certain sense, we don't require strong ellipticity. Strong electricity. So, let me show you a very, very simple example. So, now suppose that you again that your operator is very degenerate, and in fact, suppose that only has derivative in the y direction. Remember, the y direction is the vertical direction that was the direction that goes to zero. Okay, so your omega epsilon, your u at the end is not going to depend on y, the u epsilon, the limit of the u epsilon. U-epsion, the limit of the U-epsion. Well, in that case, so this is your starting equation, this equation satisfies all our hypotheses. So we know that there is going to be a solution is going to converge to a function that in fact is a solution of this first order equation. So you see that this term goes to zero, but something is left. And what is left is somehow, you know, the trace of what was happening on the Of what was happening on the boundary with the Neumann condition. And now take a very similar equation, but as degenerate as before, but this time we suppose instead that the diffusion is in the direction of the horizontal, so the direction that doesn't shrink, that doesn't become small. Well, in that case, well, you don't have this term. You don't have this term at all, and you get to this because your operator only acts on the first variable, so he doesn't see what happened to your matrix in the n term. And so you just get the limit equation that you would get naturally thinking that this shrink to the flat thing. So this is a way of understanding what's going on. And okay. And okay, so let me remind here the condition we have. So, the condition with our hypothesis were that, so in the hypothesis of Hale and Rogel, g is a function that was C3. Instead, here we only need C1. So, even in the Laplacian case, we get somehow a better result. But here, I recall that the only thing we require is that F is monotone. require is that f is monotone in the ession and that it is strictly monotone that is proper that in the dependence on the on the on the of the function and of course i will also need that omega is a c1 domain well in this hypothesis but the the last one hypothesis i don't really need it at the beginning so remember one two three okay so no i need it sorry so uh the first proposition is quite obvious The first proposition is quite obvious, but I tell you, since we have a corner, and since here this is not an operated, it's not, we cannot look at weak solution, we have to look at viscosity solution. The presence of the corner makes it a little bit more complicated. But in any case, the condition on H3, the condition on the zero alpha term, allows us to say that this value H minus 1 C0 and H minus 1 are. And H minus 1 are super and sub solution of the equation that you see here. So, as you know, the presence of the existence of sub and super solution allow you through Perron's method to prove the existence of a solution. So, under this hypothesis, we can prove an existence of solution. This is sort of standard. And furthermore, we have that the solution is uniformly bounded in epsilon. So, this is very good because then This is very good because then we can define, so this is the starting point, this allows us, this boundedness allows us to define the upper and lower relaxed limit, u plus and u minus. So here you see this is quite classical by now. So the idea is that you look at soup of u epsilon, looking at points that are around x, and when with epsilon. With epsilon that with this ratio r that goes to zero. So these limits are well defined, of course. And it's also obvious that u plus is greater than u minus. And what we can prove is that this upper, lower, and relaxed limit are in fact sub and super solution of the limit equation the way I have described. Limit equation, the way I have described it to you: this limit equation in which you have this lower order term. And I'm looking at the time to make sure. And then, okay, so as you see here, I'm not saying that we have a solution, we're saying that u epsilon converges to u plus and u minus in this way that are respectively sub and super solution. But of course, here Here, if I knew that I had the comparison principle, then I would know that subsolutions are below supersolution, but I already know that u plus is greater than u minus, but then this implies that they have to be equal. So, this is our limit operator. So, if I have one more condition, and the condition is just that, in fact, for the limit equation, the The limit equation: there is a comparison principle. And remember, here the situation is better because omega is smooth. So the presence of the Neumann condition on a smooth domain is not as bad as in the corner one. So with this further condition here, then what we obtain is that in fact the u epsilon converge uniformly to u0 and u0 is the unique viscosity solution. Need this causative solution of our limit problem. So, what we are saying is that this convergence is uniform and that u plus to u plus and u minus, which in fact coincide. Okay, so this is the first result we have obtained. So, let me mention that this has opened us to many other questions. And the first one that we have That we have wondered is the following. So, which is very typical of viscosity solution, again, for fully non-linear operator, in the sense that you see in this new domain, omega x, and look at the drawing, we have two boundaries that actually will flatten to go to zero. So, somehow there is this difference that we have a G plus, which is on the top of your domain, and G minus, which is on the bottom. minus which is on the bottom and they will all go to zero. We will ask g plus and g minus to be detached from one another. But the real novelty is not that. The real novelty is that the boundary condition on the top and on the bottom, so on the boundary that are going to collapse to one another, is not anymore a Neumann condition. It is an oblique Neumann condition. So we multiply the gradient Multiply the gradient of u on the boundary by a vector that is in fact, of course, you have to require that the product with nu is strictly positive, so your gamma points in the outside, but it need not be equal to the normal vector. And this is so in a So, in a certain sense now, we have to put a sort of something to understand what goes on. So, we suppose that this gamma here, plus or minus, that we have a term that is zero, which is what happened when epsilon goes to zero, of course. And then we have this term, k plus minus kxy. So, this is the first approximation. Remember, y is between epsilon g plus and epsilon g minus. G plus and epsilon g minus, so this y is going to go to zero, okay? And this term goes even faster to zero. So we have this development in y, okay, in the in the in this boundary condition. Yes, sorry, a couple of minutes, maybe. Okay, I'm done, practically done. Thank you. Sorry, okay. And so here the conditions are, so this was what I saw, and then we need we. And then we need some condition on the Neumann on this gamma on this vector. And the condition is that, in fact, at the limit, the vector has to have the same first, have to be somehow parallel. So this is the condition we have. Okay, I don't want to enter into the detail. I just want to tell you this. In this case, In this case, you see, I start with the same condition, so we have this non-oblique condition of the boundary, where the limit equation become really complicated in the sense that you see the Hessian is now replaced by the sum of these three matrices. And you see, in these three matrices, there is also a difference. I mean, not only. Difference, I mean, not only something is changing in the gradient term, which you see here, gradu, gradu, but also in this first order term and in the second order term. So this is very complicated. So let me tell you just what happened in case of the Laplacian. When in the case of the Laplacian, so not only you have a first order term appearing, but you also have a second order appearing. And these terms here depend on the fact that the greater The gradient, the Neumann condition is not the usual number condition, but this is an oblique condition. So you see, it really changes. You also get a new forcing term. So this is completely new. I have never seen this in the other condition. And okay, and further projects are on oscillating boundary. But I would like to just finish mentioning the word. Just finish mentioning the word the works of Marcone Pereira and Arieta, which have been really the starting point for our interest in this work. So it was important for me to recall their work. And of course, there is all another world of people working in scene structure, but this is completely different. And in that case, they use a gamma limit and it's limit of energy. So it's really another world, and I know nothing. Really, another world, and I know nothing about it. So, thank you for your attention. I am finished. Any questions? Peter? Hi, Isabel. This is Peter. The results of Hale and Roger that you mentioned, they go further and they study first the linearization of First, the linearization of the equilibria and their limits, and then the parabolic problem. We're looking at the factors, for example. Yeah. So in the fully non-inar case, at least in the non-degenerate case, do you think the results like that should be valid also? It's probably much harder without the variational structure. You mean if this. You mean if this could be done for the parabolic case? The first step would be to compare the linearizations at the equity, the solution to that for positive epsilon or the limit, whether they have the same Morse index, a number of positive values. Yeah, I don't know, it may be much more complicated. It's an interesting question. I'm pretty sure that we could do, I mean, I would imagine that for the parabolic, it wouldn't be very difficult, but I will look at the thing you mentioned of Hale and Rouge and see what can be done here. There seem to There seem to be some difficulties that didn't exist there and vice versa, some things are much easier in a certain sense. I think that the variational structure of place are all even bigger results on eigen. So if you don't, that's it. Yeah, yeah, okay. Yeah, yeah, thank you. Thank you for the suggestion. I think about it. The suggestion and think about it. Thank you. Okay, any questions from online audience? Okay, if not, let's thank Professor Brentini again. Thank you. Thank you. Now we take half our break. We come back at 10.30 Canadian West Time. Thank you. Thank you. Bye.