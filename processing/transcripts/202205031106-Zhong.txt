The floor is yours. You can share your slides and slide everywhere. Thank you. So I'm going to share my screen. Can everyone see my screen okay? Yeah, perfect. Thank you so much. Okay, so hello everyone. My name is Chudi Jong, and today I want to talk. Is Chudi Jong, and today I want to talk about sparse decision tree optimization. Excuse me. This is a joint work from myself, Hayden, Rattle, Elias, Jack, and Professor Since I Ruding from Duke University, and Professor Margo Serzer from University of British Columbia. I guess everyone here knows interpretability very well. So just to recap, in my understanding, the interpretable means the reasoning behind each prediction made by the model is understandable to humans. Understandable to humans. And compared to black box models, interpretable machine learning models are easier to troubleshoot, essential for high-stakes decision-making problems, and are more suitable for scientific knowledge discovery where fundamental rules are needed. For tabular data sets, logical models are interpretable since their statements are natural reasons for each prediction. And decision tree algorithms are definitely one of the most popular logical models, so I put a tree here. However, the Here. However, the main problem that has always plagued decision tree algorithms is their lack of optimality because they have historically been greedy algorithms like CART and C4.5. So these algorithms construct trees from the top downwards and then prune them back afterwards. The problem is that if a bad split is made at the top of the tree, then there is no way to undo it. So these greedy algorithms can produce sub-optimal trees, and we don't know how sub-optimal. Trees, and we don't know how suboptimal these trees are. So, it will be really great if we can improve over the grading methods so that we can get a single tree but has high performance. However, it is really hard. The full decision tree optimization is proved to be MP-complete, and there is a combinatorial explosion in the number of possible trees we could consider. For example, given an internal node that we want to split. Know that we want to split. There are so many different splitting conditions that we can use. For example, whether it is a rush hour, it's Friday, it's holiday, it's under construction. Suppose we use each of P features to split this particular node, ending up in P developing trees. Then for the next splitting node, there will be P minus 1 available splitting conditions. And it will be really expensive to search an optimal tree across such large space. Across such large space. So, how can we find an optimal tree efficiently? There are two major directions. The first direction is to use mathematical optimization. So, mathematical optimization solvers have been used since the 1990s. Many recent researchers formulate the decision tree optimization problem using the mixed integer programming, and the solvers are required. However, even with the recent development in With the recent development in sovers and smart formulations, algorithms in this group still tend to be slow to find optimal trees. Another branch of possible solution is to use the combination of branch and bound and dynamic programming to search for an optimal tree. Dynamic programming improves the computation reuse, and efficient bounds can help to cut the search space. So, together, they tend to find optimal trees more efficiently. And our previous work goes also for this group. And before moving on, I do want to spend a few more slides to explain how branch and bound dynamic programming work together, since it is the foundation of our new work. And I will use Ghost, our previous work, as an example. Also, Ghost aims to minimize a risk, which is a loss function regularized by the number of lifts in the tree. And we usually call it tree and we usually call it sparsity penalty term. The loss function can be for example misclassification loss that is 01 loss or 1 minus balanced accuracy and lambda is the regular riser that controls the sparsity and accuracy trade-offs. So in subring we want to find a sparse tree with small loss. I first want to introduce how dynamic programming works in Ghost. So let's start with the root problem defined by the four data Root problem defined by the full data set, and the label of this problem is predicted by majority class label. And then we split it into subsets using each feature. We continue doing this to achieve higher accuracy. And sometimes we find a sub-problem containing samples with the same label, and we don't need to split it anymore. So, this sub-problem is solved. We will also encounter some sub-problems that are identical to each other, and this means there is some. Other and this means there is some computation that can be reused. So the solution of one instance of the duplication can be used as a solution for another instance. So our formulation creates a dependency graph between problems and sub-problems, and the solution to each sub-problem yields the best feature to split on. And once all sub-problems are solved, we find the optimal tree from the dependency graph. And note that some sub-problems And note that some sub-problems can be proven to yield non-optimal solutions using branch and bound. And for these sub-problems, we can mark them as soft before fully soft. Next, I also want to briefly talk about how dynamic programming combines with branch and bound. And recall the goal's objective is to minimize the risk, which is the sum of the loss and the penalty term. So in the dynamic programming with bound, the goal can be represented as finding a solution that makes the upper Finding a solution that makes the upper and the lower bound of the root problem equal to each other. Suppose we have a problem P, which can be perfectly classified. So, ghost initialized the upper bound of this problem by the fraction of samples with the minority label plus lambda. And the lower bound is initialized by zero plus lambda. And lambda is included in both upper and lower bound because there is at least one leaf node, even we don't do any split. Split. And for example, let's split the root problem using a vertical cut. Now we have two sub-problems, p left and p right. And for each sub-problem, ghost needs to decide whether to bridge it or not. A bound can help. So if splitting the sub-problem will not bring enough accuracy, that is, the increased accuracy is not as large as regularizer lambda, then we set the lower bound of this sub-problem equal to the lower. The lower bound of this subproblem equal to its upper bound. And doing so, this subproblem is marked as solved. And once the child problem is updated, Ghost updates the parent problem P. So the upper bound of the problem P is the sum of the upper bounds of two sub-problems, and the lower bound of P is the sum of the lower bounds of two sub-problems. And Ghost recursively solves the sub-problems and updates their parents until the upper and the lower bound of the root problem. Until the upper and the lower bound of the root problem converge. As I mentioned before, sometimes bounds can tell the algorithm that some sub-problems cannot yield optimal solutions, so we don't need to solve it, actually. Then these sub-problems can be marked as solved. Here is an example. Suppose during training process, the upper bound of problem P is already updated and we call it current upper bound of P. And suppose feature JPREM is used to feature J prime is used to split this problem P, then we have two sub-problems. Then Ghost needs to determine if further branch is needed. Another bound in Ghost says if the sum of the lower bound of these two sub-problems is larger than the current upper bound of P, then the optimal solution of P doesn't depend on these two sub-problems, and we can get rid of them. So using dynamic programming with bounds of ghosts can reduce the number of sub-problems that need to be solved. Of sub-problems that need to be solved and reuse computation for identical sub-problems. So, together, they lead to a dramatic runtime improvement. Here is an example of CARTI and Ghost Tree on Monk2 dataset. So, CARTRI has 16 lifts and Ghost Tree with 9 lifts actually has higher training and test accuracy. And in this particular example, the training time of Ghost Tree on the right is actually within a few seconds. In few seconds. However, even though Ghost can deal with MUC2, which has around 200 samples and 15 binary features very well, Ghost and other branch and bound and dynamic programming-based optimal tree algorithms still encounter some challenges. For example, when fitting a real data set with continuous features, this algorithm suffers from long training time or large memory usage. Two factors cause this scalability issue. Scalability issue. So, first, all these algorithms use binary features as input. So, a pre-processing step is needed to take all thresholds from a continuous feature and transform it into a set of binary dummy variables. For example, blood pressure takes on value 102, 103, 104 all the way to 125. And in order to maintain the optimality, this continuous feature needs to be transformed to binary features like Features like blood pressure less than or equal to 102.5, less than or equal to 103.5, etc. Basically, we need to consider midpoints of all consecutive values. Therefore, a continuous feature can end up to a lot of binary features. And the best thing is the search space of optimal decision trees increase exponentially in terms of binary features. So it can be a nightmare. And the second reason is that lower bounds are often That lower bounds are often slow to converge. Algorithms may quickly find a good solution, but since unexplored space still has a small lower bound. So the algorithms may spend a lot of time to solve this sub-problem just to prove that the good solution is actually an optimal solution. Consider these two issues. We want to reduce the searching time of the current state-of-the-art algorithms by orders of magnitude, and we hope to find a near optimal sparse density. To find a near-optimal sparse density tree that competes in accuracy with a black box model. And in our new work, we propose three guessing strategies based on a good black box reference model. Specifically, we propose method to guess thresholds, lower bounds, and depths of the optimal tree. So the first guessing strategy is guessing thresholds. It aims to reduce the number of binary features introduced by continuous features. Recall the block. Features. Recall the blood pressure example I mentioned before. So we need to consider midpoints of all consecutive values. And in this example, we will have like 23 binary features after transformation. But not all 23 thresholds might be useful in practice, actually. And maybe, for example, three thresholds are enough. To do so, we need a black box. To do so, we need a black box reference model like Gradient Booster Tree and only select a subset of thresholds that are used by this reference model. Actually, two reasons motivate us. First, we assume not all thresholds in a continuous feature are important. Thresholds are highly correlated, and we think it is enough to maintain the prediction power of a continuous feature using some of its thresholds. And the second reason is in practice, we usually Reason is in practice, we usually just want a near-optimal tree that is simple but as accurate as a good black box model. And therefore, we assume if an optimal tree is trained using a subset of thresholds that are exactly used by a good black box model, then this optimal tree can have accuracy comparable with the black box model. So, long story short, we use the power of black box model to select a subset of thresholds to reduce Subset of thresholds to reduce the search space. Okay, so now I'm going to show how we guess the thresholds. So first we fit a reference model on the original data set and we choose the gradient boosted tree since it generates a very good performance. But there is no limitations on the type of reference model. And starting with the reference model, we extract all thresholds for all features. Extract all thresholds for all features. And then we do a backward elimination. That is, we order all extracted feature and threshold pairs by variable importance and remove the list importance threshold. And then we refit the boosted tree with the remaining features. And we continue this procedure until the training performance of the tree drops below a predefined threshold. And then we use the select. And then we use the selected substitut feature and threshold pairs as input for optimal tree algorithm. This gas can be used as a pre-processing step before fitting optimal decision trees, so it can be easily implemented for all optimal tree algorithms. And in our experiments, using threshold guessing often helps to reduce training time by orders of magnitude, which I will show later. But one thing we need to take into consideration. We need to take into consideration what happens if the gas is wrong and we call it bad gas. And we define the bad gas when optimal trees trained with threshold gassing don't have comparable accuracy with trees trained without threshold gassing. This may happen when the reference model is too simple. In other words, the guessing is too aggressive, so we exclude some important thresholds. Though in our experiences, bad gas rally. For instance, bad gas rarely happens, but if it does happen in practice, we can use a slightly more complicated reference model to include more thresholds. And the second guessing strategy is guessing lower bound. This guessing strategy aims to make lower bound converge more efficiently. It's also based on Black Box reference model. Suppose a problem. Suppose a problem can be perfectly classified. Instead of initializing the lower bound of a subproblem by zero plus lambda, we initialize it by the fraction of points misclassified by a black box reference model. We also have two reasons. First, we assume some sub-problems don't lead to optimal solution, and we implicitly assume interoperable model will have about the same accuracy on each sub-problem as a black. On each subproblem as a black box reference model. In other words, we assume a point misclassified by a good black box model can also be misclassified by an optimal tree. Therefore, we decide to guess the tidal lower bound for each problem. So given a problem P, we first fit a black box reference model. And suppose the reference model misclassifies some points in the raw squares. In the raw squares, the lower bound after guessing is defined by the fraction of points misclassified by the black box reference model plus lambda. And we no longer consider the original lower bound, which is zero plus lambda. And then we do tree optimization. Suppose we split the problem P horizontally, and we get two sub-problems. For the left sub-problem, since the upper bound is smaller than the gas lower bound, and we don't need to split. Lower bound, and we don't need to split it anymore. It's a good enough evidence that we should exclude this sub-problem. And compared with the original algorithm, we can save searching time by making this sub-problem much easier to be excluded. Similar to the threshold gas, we may also make bad gas for lower bound. That is, we train trees trained with lower bound gasing don't have comparable accuracy with trees trained without gassing. Guessing. And this might happen if the reference model is too simple, so that the initialized lower bound will be too high, and we may quickly exclude too many sub-problems, ending up in a simple but not accurate tree. This problem is also easy to be fixed. We can choose a reference model that slightly overfits the training set and then guess the lower bound. And the third guessing strategy is guessing depths. So, depth limit is commonly used as a hard constraint in optimal tree algorithms. In Ghost, we don't really consider this hard constraint, but in other optimal tree algorithms, they typically use the depth limit. Because without this limit, the size of the search space is increased tremendously, and it's much harder to find optimal trees. But we still want to mention that setting a hard constraint on depths is. Setting a hard constraint on depth is kind of a guessing the depth limit of the optimal tree. And in our new work, we propose how to guess a depth limit in case we want to have the similar prediction power to the black box model based on the VC dimension. This can give us an upper bound of the depth limit, but it might be too loose in practice. So in practice, we can still use that customized depth limit like depth limit 5, which is typically used in Used in numerous papers. Okay, so next I'm going to show some experimental results. I first summarize some algorithms used in our experiments. So GOAST and DL8.5 are two optimal tree algorithms. Both of them are based on dynamic programming and branch and bound. I believe everyone knows CART. It's a heuristic decision tree algorithm, and we use it as a Algorithm and we use it as a baseline. Boang Yen trees is used as another baseline. So it aims to produce a single decision tree from a random forest. And without loss of accuracy, it aims to minimize the complexity of the tree. We also use the gradient boosted tree as the black box reference model, and it also works as a baseline in our experiment. So the first So the first question we want to ask is how accurate I interpret models with guessing. Here we show the trade-off between accuracy and sparsity on the compass data set. And we compare the results of our methods with CARD and Boeing trees. As you can see from this plot, the guest models reach a good balance between training accuracy and sparsity compared with two. Sparsity compared with two baselines and usually form an outermost frontier. So, this means given the same accuracy, trees learned by our methods are more sparse than trees from baselines. And in other words, given trees with same number of lives, our trees have higher training accuracy. So, this other most frontier not only holds for the compass data set. Given two other data sets, Given two other data sets, Netherlands and FICO, this pattern still holds, as you can see from the red curves. Some of you may be interested in the test performance, looking at the test accuracy versus sparsity plot, similar patterns code. And also, trees learned by our methods can still beat card and Boeing trees and be very close or even beat the gradient boosted tree trained using 100 max steps three-weekly. Using 100 max steps three-week classifiers. Here we show the training accuracy versus training time on the compass data set with and without our guessing strategies given depth limit equals to three. So stars represent optimal tree algorithms with our guessing strategies and circles represent algorithms themselves without our guessing strategies. As you can see from this plot, See from this plot, our guessing strategies dramatically improve both ghost and DL8.5 runtimes by one to two orders of magnitude. Such speedup also happens when we have depth limit 5. And also in our experiment, we set the 30 minutes time limit for optimal tree algorithms. So the speed up can be even tremendous for timeout cases. For cases that algorithms For cases that algorithms can finish searching within 30 minutes time limit, our guessing can help to reduce the training time with almost no or very slightly change in training accuracy. But for time outcase, guessing strategies can not only reduce training time, but also provide a more valid result. And more interestingly, according to the test accuracy versus time flaws on the bottom, our guessing strategies in general. Our guessing strategies in general maintain or even improve the test accuracy of the optimal tree algorithms. The black dashed line in the bottom plot is a gradient-boosted tree that was trained using 100 MaxStep 3 weak classifiers. And the test accuracy of optimal tree algorithm with our guessing strategies sometimes even beats the black box. Though I use compass data set as an example here, these patterns hold for all seven datasets we used in our experiments. Experiments. And the second question we may want to ask is: What do optimal trees look like? Here we visualize trees trained with our guessing strategies or simply trained using baselines on a two-dimensional spiral data set. So dots are samples, and the splitted region in the background represents a decision tree. Compared to trees trained by ghosts with and without By ghosts with and without three guesses, the tree on the left captures the fundamental rule better than the tree on the right and has higher accuracy. So this means our guessing strategies can help to find a more accurate and valid tree. And compared to two trees trained by DL8.5 with and without gases, the left tree trained with gasing strategy is more sparse and accurate. In contrast, Accurate. In contrast, the right tree trained without guessing strategies is more complicated and doesn't capture the fundamental rules of this viral data set. And compare trees from baseline methods. Our trees are not only more accurate overall, but they also have fewer lifts and are therefore more integral and more sparse. Here we show two trees trained using ghost with gases on the compass data set. The depth limit is set to five. The depth limit is set to 5 and the regularizer is set to 0.05 and 0.01 respectively. Both trees are abated within 35 seconds. The tree on the left has higher regularizer, so it's more sparse. And here is a tree trained with gases on FICO dataset. This tree is abated within 10 seconds, and its training and test accuracy are close to the Training and test accuracy are close to the best black box models, but with only seven lives. So, in conclusion, in our new work, we introduced smart guessing strategies to find sparse density trees that compete in accuracy with a black box machine learning model while reducing the training time by orders of magnitude relative to training a fully optimized tree. Our guessing strategies can be applied to several existing optimal decision tree. Several existing optimal decision tree algorithms with only minor modifications. And with these guessing strategies, powerful decision tree algorithms can be used on larger real data sets and may increase the potential domain of application. And here is some reference and things. Thank you very much. Do you have any questions? Okay, let's start. Hi, Julie. Can you hear me? Yeah. Thank you for your talk. So as far as I know, like decision trees in general are quite unstable to train, not in terms of optimality, but more sort of like actual tree that you're obtaining. So you might have two, like if the data sets changes a little bit in terms of which samples are included, then Samples are included, then you would have very different decision trees because of the cutting point or the feature selected and stuff like this. I was wondering if, have you checked if the trees that you obtain are fairly robust? Yes, that's a really good question. So I agree that, for example, if a continuous feature, for example, we always select fewer thresholds. Example, we always select fewer thresholds instead of trying all of them, then the final tree will be slightly different. So I would say like at this point, if we use all these guessing strategies and use the optimal tree algorithms, the tree in general is very robust. So we tried one experiment. It's actually we train the data set using the random forest, for example, and we use the label. For example, and we use the label of the random forest as the predicted label in our case. And we then train the axe and also the predicted label from random forest and compare the test accuracy. And our trees are actually still has a very good test performance. This is one thing. And the second thing is that sometimes the threat, even though there could be a lot of different thresholds, but the samples changing. The samples changing between these thresholds might be very small. So, for example, if, for example, you have age smaller than 30, age smaller than 31, age smaller than 32, and we only pick age smaller than 30 and age smaller than 32, and we miss the age smaller than 31, but the sample difference between this splitting will be very small. And another thing that might be interesting is our current work on, we are also working on. On, we are also working on something like the rational set. And in rational set, we can list several different enumerate all the trees with very good accuracy. And in that case, you can also pick some trees that can satisfy, for example, robust requirements. And do you have any case where, for example, For example, a feature was selected in one tree, but like in a tree with a similar performance, it was not selected at all, or maybe it was in a different level of the tree. Oh, sorry, can you repeat? I cannot hear very clearly. Let's say that you train two trees with the same sparsity on the same data set. Did you have cases where you, if you check, I don't know, like did you have cases where the two trees were vastly different in the sense that maybe there was some feature that the sense that like maybe there was some feature that was not selected or a feature was uh at a different level one of the trees um actually in our experiments we try like four different um reference model configurations so it will end up in like different group of thresholds for example also like different groups of lower bound but still i think all these trees um at least in the training and task performance all of them perform very good All of them perform very good. And for specific trees, there might be some difference between the threshold you use. But in general, I would say the trees look very good. Thank you. Thanks for your talk. So how is can you compare the IPAPA and other speech configuration? Perfect speech configuration of your trees compared to traditional trees, much bigger or just exactly the same as so you mean like what is the difference between our trees and some traditional trees like cart? You have to find tun people parameters. Actually, not really. So because we have a very good, so in our setting, So, in our setting, we maybe I can just directly go here. So, in our setting, actually, we directly panelize the number of leaves. So, we prefer the tree that has fewer leaves. So, basically, we prefer the sparse trees. And for example, here is actually from the previous work: the left is card tree, and the right is ghost tree. And we this actually for this particular ghost tree, I think the lambda probably be something 0.01, something like that. But if you want to have even sparser tree, you can increase the regularizer to have a more sparse tree. Or if you want to have more complicated tree, then you can make the regularizer smaller. But in general, I would say like optimize the risk, which is the sum of. Risk, which is the sum of the loss and the lip penalty. Other trees are more sparse. And as you can see here, the card tree has 16 lives, and ghost tree only have nine lives. And the training and test accuracy of ghost tree are better than card tree. And also, one other thing you may interest in is like, oh, how about the training time? So, for this particular example, I think Carter is definitely very fast. And for this particular example, And for this particular example, the ghost tree only takes a few seconds, I believe less than five seconds for this particular tree, even without doing any guessing. And for trees, for trees something like here, then basically it takes like this one is 22 seconds, this one is 34 seconds. And card tree, they probably be they could be faster, but. They could be faster, but they might be like more complicated. I think this might be a good example. So, this is on the left side is actually the ghost tree with guessings, and the right tree is actually cart. So, in general, our trees are still better than card. And with all the guessing strategies, we can find the trees very fast. Thanks. Can you hear me? Yeah. Thank you for the great talk. And my question is about the lower bound guess. What is the sensitivity of vapor parameters of the black box model? Yes. Yeah, so actually for the, so as I mentioned before, in our experiments, Mentioned before, we in our experiments we actually try four different configurations of reference model. Like one configuration might be like 40 decision stumps, so it's a kind of a simple black box. And one configuration might be like 100 Depth 3 features, 100 Depth 3 gradient booster tree. So it might be slightly more complicated. So, in general, all these configurations still gives us a very valid lower bound. And if the lower bound is, if you use, for example, simple black box reference model, then the lower bound might be higher. So the tree might be too simple or not have very good accuracy. But actually, in our experiments, we don't see this kind of results. This kind of results. And if you see this kind of thing, you can slightly increase the complexity of the black box model. Thank you. I have just a small question. I've seen that you showed multiple data sets, but my understanding was that each of your trees was trained independently on each of them. And I don't know, maybe my question. And I don't know, maybe my question is a bit naive for your application, case, but I was wondering if you had a look at how generalization might work with the tree that you're proposing since the sparse. I would expect that the generalization helps. And also, I was thinking if you could use the cheating oracle, right? So you could use a, you could cheat when you're training your complex model for finding. Uh, your complex model for finding the splitting rules or the thresholds for guessing the thresholds. Um, yeah, so I was wondering if you had a look into that, and yeah, what do you take? Um, so I think so we try to do some evaluation study, like what kind of reference model can be enough for training, for guessing. Um, so in general, I think for raw data sets, even a very simple reference model can help us a lot. Can help us a lot and still give us a tree with very good performance. But for some simulated data set, you might need to use a slightly complicated reference model. But I think in real data set in general, all kind of reference model configuration can help. But that's a really good question. I think we can also try a few more experiments to show that. Actually, in our paper, we provide In our paper, we provide some kind of abolition study in appendix if everyone is interested. Very much. Thank you. So we have another speaker now.