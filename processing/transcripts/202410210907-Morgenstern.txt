Feel free to interrupt questions. I have probably more slides than I'll get to, especially if we're are we sticking to the normal schedule or do I get five extra minutes because we're starting with a couple of minutes? Okay. Yeah, I'm expecting that the last section of the talk I probably won't get to, which is fine. I think it's like a reasonably cohesive story of that. But yeah, so today I'm going to talk a little bit about what I have largely called prediction. What I have largely called predictive disparity or predictive equity in machine learning. And I'm going to draw some similarities and contrasts between what this looks like in sort of classic statistical settings compared to sort of more modern, like modern machine learning. And by that, I mean, you know, if we're thinking about either generative AI settings or things where we're using like large language models or large vision models, where a lot of the sources of differences in predictive accuracy across different demographics. Accuracy across different demographic groups. There are certainly some similarities, but also some differences in terms of where some of that performance difference comes from. Okay? Cool. So if you want one slide and to go back and take a nap, the primary thing that I'm going to talk a little bit about is sort of that the reason that we see differing accuracies for different populations are somewhat different. Are somewhat different in the context of thinking about either LLMs or computer vision models. Those are different and complex causes, and we can't necessarily draw all of the same conclusions that we could in lower-dimensional settings. And so, I'll sort of give more examples of that, and we can talk a little bit more about various pieces of that. So, the right approaches to addressing some of these inequities will also correspond to. Correspondingly vary. Okay, so I probably won't get to the last chunk of this, but largely what I'm planning to talk about today is sort of walk you through sort of some rough categorization of some of the things I think of as like classical sources of inequity if we're thinking about sort of statistical machine learning and the like you know pre-deep learning working. Pre-deep learning working era, talk about some of the things that are sort of the sort of in some degree, to some degree, in order of the frequency and magnitude of the sources of some of these inequities. I'll talk a bit about some work that I and awesome collaborators have done talking about how sampling and re-weighting can be used in many cases to mitigate some of those inequities. But in the case of thinking about these sort of low. Case of thinking about these sort of lower-dimensional, simpler model settings, we often face sort of hard trade-offs between accuracy for different populations. If I'm going to pick a single model, single linear model on 10 features, and I have two demographic groups, I often have to choose a model that, like, if I want to improve accuracy on one population, that often comes at the expense of accuracy on the other population, right? And that's like a fundamentally different structure than what we often see in some of these. Than what we often see in some of these more modern contexts. Then I'll move on to sort of contrasting that with some of the modern ML sources of inequity. I'll talk a little bit more about sampling and reweighting and some of the things we can say there versus the things that were, some of the theoretical statements we could say in the classical case don't necessarily carry over, but we can still say some nice things empirically there. It's also the case. It's also the case that when we start talking about some of these more modern applications, it's much more rare to have explicit access to demographic groups when you're doing prediction, right? So if you're doing facial recognition or object detection or using an LOM to complete sentences, you don't usually, it's sometimes not even obvious what you mean by demographics, right? But even if there is an obvious demographic, the race or gender of a person in an image. The race or gender of a person in an image that's not usually like written, codified as like feature three, right? And so that adds to the complexity of thinking about what we mean by, or you know, what we can even do to mitigate differences in performance. That's like a fundamentally different structural thing. So talk a little bit about that. And then sort of broadening out a bit, one of the things about thinking about something. out of it, one of the things about thinking about some of these modern settings is like, you know, thinking about like the black box setting of modern or of like classical machine learning where there's like a fixed data set, you use it to train some optimization, you use some optimization to train a fixed model, and then that like lives on in perpetuity and like there aren't other sort of bells and whistles and like things don't change all that much. That's like a modern machine learning applications have like many extra components. Have like many extra components to the pipeline and are usually much more dynamic. So, I probably won't have as much time to talk about more of the work that I've done in that space. But I think one way to think about it is just that modern ML pipelines have all these extra pieces. For example, how many of you have thought about data filtering? Is that even a phrase that any of you have really heard of? Okay, so a few people in the middle of the room. So a few people in the middle of the room, right? I mean, people who work at Apple, I'm not so surprised, right? But there are all of these additional components to like making like LLMs and fancy vision systems work that aren't part of sort of like our simple basic learning setup that we're used to. So it's worth like spending some time thinking about how some of these like models. Thinking about how some of these like modern applications work because I think there are some nice theoretical questions that are probably appropriate to be formulated based on some of those things. Okay, so I just want to, for purposes of grounding, sort of talk about like, you know, the standard machine learning setup that probably all of us are reasonably familiar with, right? I want to focus on thinking about this as like, you know, the distribution I care about working with. Let's say that for simplicity, That for simplicity, it's actually equal to like some nice mixture of several different distributions. We can think about these as different populations. Population I'm using very loosely, it could correspond to demographics, it could also correspond to just like fundamentally different populations for other reasons, right? Be they geographic or otherwise, right? So, a fairly standard thing is to think about us having a Us having a data set that's drawn from that distribution, and the goal is to pick an algorithm that, from a finite sample, learns a model which approximately minimizes the loss of the model on future trust from that same distribution. That's sort of like one of the more basic ML problems to think about. I'm also going to encourage us to think about picking models which Picking models which minimize the maximum loss for any particular population we're thinking about. So that's like a related problem, not exactly the same, but like especially when we're thinking about this mixture setup is like maybe a particularly nice one to think about. So rather than just thinking about minimizing my average loss, maybe what I care about is I have a bunch of different populations and I want to pick a model that does sort of as well as possible for the worst possible population. For the worst possible population. Worst meaning like my worst performing. Can you assume that the mailer does not know which group is that the hands? For you could you could do either way, right? Like, yeah. In this, what I've written here, correct. Yeah. Right. Yeah. So I want to talk a little bit, you know, I've sort of alluded to a few of these things, but I think it's useful to sort of do a bit more of a categorization. So, you know, what am I talking about when I'm You know, what am I talking about when I'm saying classical models? Largely speaking, what I'm referring to is tabular low-dimensional data sets, right? Tabular meaning like the features are sort of like structured. They might have different types. Lots of them are categorical. It's rarely possible to achieve zero loss on train or test for a lot of these problems, right? And there are a variety of reasons for that, but one of which is like the Reasons for that, but one of which is like the Bayes risk is often non-zero. It's often the case, too, in these settings to think about our sample size being like roughly polynomial in the number of dimensions, something like that. And we're probably all fairly familiar, and we've probably all both proved theorems and written some code to train low complexity models to predict labels from. Labels from our set of features in such settings. I like to also think about this as like the set of labels we're thinking about are either like a relatively small finite set or perhaps real-valued, right? So like the real value, you know, we're doing some kind of regression or something. Right? Okay. And as we're all probably again familiar with, right, this is a setting where like we get decent generalization, like the difference between train and test is just Between train and test is generally pretty small if our model complexity grows roughly proportionately to the number of samples we have. And then, sort of in order of importance of the sources of the inequity that we often see, like Bayes error rate often is like something that governs sort of how much difference we see in the performance across different demographic groups, right? Like it's often the case that the Bayes error rate for two different populations on a five Two different populations on a finite low-dimensional setting is just different, right? Maybe I can only get 70% accuracy for population A, whereas I can get like 90% accuracy for population B. And that's not a function of generalization. Maybe that's just like foundationally, the set of features we have are not as good at predicting y from x for different populations. Computation can also be a reason that we end up stuck, right, with, you know, with some differences in performance, right? Or, you know, Performance, right? Or, you know, maybe we can't find like the lowest loss, the lowest loss model for that reason. Representation, not necessarily Bayes error rate, but like the fact that we're learning a linear or relatively simple model might lead us to sort of force ourselves to have trade-offs between the different populations' error rates because we're not learning a sufficiently expressive model to learn different relationships between X and Y for different populations. X and Y for different populations, if in fact those relationships are different. Sample size can play a role here too, but it's sort of like on the lower order bits in a lot of cases that I've seen, like in terms of like the importance, the reason that we're seeing differences in performance across different populations in these settings. Sample size is like, it is a thing that can affect it, right? But like some of these other factors, Some of these other factors seem to matter more in a lot of these cases. I mean, sample size doesn't matter much. You mean the total sample size or the relative sample size between the groups? Yeah, yeah. If the group is very unrepresented in the sample size. Sure. So certainly if the minority population is really small, that's going to bump it up in this relative ordering, right? I do mean. I do mean, what I mean by that is, like, I do mean the size of the smallest population. That will sort of directly correspond to where sample size should exist in this list. Because if I only have a single sample from a population, it's probably going to be the governing factor. If I have a reasonable, you know, if it's a constant fraction, I'm generally not particularly worried. Like, if it's a constant, like, if basically any demographic group is a constant fraction of my overall population. Is a constant fraction of my overall population size, or like you know, a pretty sizable amount, like then these other things tend to be more critical. But yeah, it's true that like usually the relevant sample size when thinking about disparity is the size of the smallest population. So that's a great clarifying question. Yeah? Okay, I just see people doing stuff with their hands. Doing stuff with their hands. I also, my hearing is a little screwed up, so I'm trying to rely more heavily on my sit than I normally would for questions. Okay, so a couple of things about this setting. Disparities can be reduced by adding relevant features sometimes, right? That can reduce your phase risk, right? If you have sufficiently high number of samples, you could also possibly learn a more complex model. Learn a more complex model. There are any number of reasons this might work. This might be, you know, whether these features are demographic or otherwise, right, adding features can be helpful in these settings. And that's not, I think, something that like we as theorists think that much about, right? But like, I mean, I think we're intuitively, we would agree, right? Like, additional features that are related to why will probably help you, right? But it's something that. Help you, right? But it's something that I think there might be some nice spiritual questions to ask about. You know, when we might want to invest in gathering additional features for certain problems we're thinking about, demographic or otherwise, right, that can be helpful for mitigating disparities. It's particularly useful to add demographic features or things that are strongly correlated with those when Y conditioned on X varies. Y conditioned on X varies a lot by demographic. That's a particularly useful time to have some demographic information, especially if I have a model which is complex enough to basically learn a somewhat different relationship between Y and X for different populations. Okay. So when you say that... Oh, sorry, see what you're doing? Yeah, when you say that it's nice to add features, you mean at train time knowing where your XY came from, or also at test time, you're able to access those features? I meant both, right? So I'm thinking of a model. Right, so I'm thinking of a model that, yeah, yeah. So I'm thinking of like at unconditioned decisions, yeah. Yeah, yeah, yeah, exactly. So, like, I'm thinking about additional features both at training and test time because it'll allow us to learn a more complicated relationship between Y and X if we have sufficient samples to support such a thing. Yeah, right, because I guess the point I'm making in a lot of these cases is like the trade-off we're making in a lot of these lower-dimensional settings. In a lot of these lower-dimensional settings, it's not as based on generalization problems as it is on the signal strength we have in terms of our relationship between X and Y, and that that's not always the same for different populations, especially in these lower dimensional settings. And that it is reasonably clear that we're getting reasonably close to Bayes risk on, or you know, if we Or, you know, if we just train a model on one population ignoring the other populations, we're usually able to get to some reasonable risk. But those are not necessarily the same for different populations. Yeah. We have to get used to it. I'm going to ask those questions. But I don't think here that you assume here that the learner aims to be fair, but To be fair, but there could be a situation in which either the learner has its own biases and there I want to suppress the demographic membership. And or even that the prior knowledge, I mean it's not intentional, but the prior knowledge would make you select your class of models, has to do with some bias, and in that case, maybe having a variable that tells you more perfect move can help. Yeah, yeah, right. Yeah, yeah, right. It's not the case always that demographic like if you want to discriminate, that can be a problem. And there are certainly like both legal and there are any number of reasons you might not want to include demographic information, right? But like from a statistical perspective, it's often going to be helpful in a setting where we have some disparity. But I definitely agree, right? Like it's not always the case of throwing in, and in some cases, right? And in some cases, right, we do see that adding some of these variables doesn't help that much. That sometimes happens because the models we're using are not complex enough to express different relationships. There are any number of differences. But yeah, point taken that it's not the case that this is a panacea. Yeah, but models being complex enough to decouple decisions across populations is something that is sort of. Is something that is sort of critical for demographic information being helpful in making better predictive decisions here. Okay, there is some improvement that we see from gathering additional samples from smaller populations, right? Especially in the case where that is sort of like a particularly bottleneck. It does allow training versus test to shrink for eye. Shrink for I. And I'm going to talk about in a second that it sort of tacitly might encourage us to optimize in a way that focuses on DI, depending upon how things are working. And it, again, may allow us to explicitly or explicitly learn different relationships between X and Y for certain populations. Okay, so I'm going to talk. Oh, I don't know why that pieced in. Sorry about that. I am going to talk a little bit about that and give you a That and give you one theorem about sort of when we can prove something about adding additional training data from a population on which we are currently performing worse. This is joint work with a lot of excellent collaborators of mine. Claire in particular is a student of mine at UW and she'll probably be graduating over the next couple of years, so if you're looking for an awesome postdoc. Matt and Chris are both at Amazon. Konjil's at At Amazon, Conjuls at Google, and James and Georgia Tech. Everybody here is awesome. I think any of you know busted. But we were wondering, you know, and this is sort of going to say a little bit about something that Shai mentioned, right? There are some downsides, certainly from like a predictive equity perspective, for just like optimizing for average prediction, right? For one thing, that sort of prioritizes larger populations. Larger populations. And sometimes in ways that, like, you know, yes, it's better for average loss, but like, sometimes you might be able to, like, like 1% worse, but you might see like a 30% improvement on a pretty small population in terms of the performance you get. And, like, I guess, you know, it's in the eye of the holder, and it depends on the application you're thinking about, whether that's a trade-off you're willing to make. But, sort of, average case loss minimization sort of sweeps. Case loss minimization sort of sweeps that under the rug and sort of tacitly prefers models that work really well on large populations, even if like small degradations in performance on them could lead to huge performance improvements on larger populations. It also compounds some statistical, right? Like that can compound statistical issues you might have for smaller populations. One thing that, like, one, you know, the optimization I talked about on like the second or third. I talked about on the second or third slide that doesn't necessarily have this same problem is thinking about some maximum objectives. So, if you think about instead of just like finding a model which minimizes average loss, you find one that sort of minimizes the maximum loss any population sees, right? You're not necessarily going to have those problems, right? Like, if you can slightly reduce or slightly increase the error on one population that's huge, but like by doing so, really reduce the error on some populations. So, really reduce the error on some population where my performance is much worse. That's something that a maximum objective will take care of nicely. So, we were curious about both thinking about natural processes and procedures that might lead us towards finding solutions to maximum objective problems, but also thinking about this in the context where we might have some limited access to additional samples. So, I can tell you offline a bit of a I can tell you offline a bit of a story about one of the problems that led us to thinking about this problem. But I want you to just think about the following framework for sampling and retraining a model. So think about picking a hypothesis on a data set which minimizes some loss function g. Look at the value of g, like our loss function, of the model we chose on every one of our demographic groups. Demographic groups and sample an additional point from the group where my model is currently doing the worst. Add that to the training set and continue. That's sort of like the, I don't want to say elementary school, but like the like, you know, sometime in college version of like, if you think that training set size is your problem, that like your performance is worse on a population because you're Performance is worse on a population because your training set is too small. Well, this is something that may address that, right? Especially if, like, you know, this highest value of G is actually based on like validation as opposed to your training error or something. It also has some other nice properties that are maybe a little less obvious, right? So if you're sort of augmenting your training set in this way and continuing to just look at like finding a loss minimizing a loss minimizing A loss minimizing model on this set, that's going to tacitly move you towards prioritizing loss on the populations where you're currently performing worst. Yeah. If the base risk is very different between populations, because this results in sampling from the population with a lot of shadows, yeah, yeah, that's right. So the theorem that we could say about this, right, is like if the hypothesis says, If the hypothesis set and the loss function are convex, the above strategy will find a function, will find a model which basically minimizes the maximum risk for any population, but like that is going to be limited by the Bayes risk of that or self-population. What does we see from your TRAN? The fact that it's minimizing the maximum loss, like wouldn't totally. Wouldn't our minimize the maximum difference between the loss and the base? It's not the difference. Okay, so one comment here. All of this is like sort of based on thinking about all of this as like, pretend I have access to true loss, and there's no difference between train and test for a moment. Okay? Okay, then this is just saying the optimization is being nudged towards like. Towards like minimizing loss on the population where my loss is highest, right? And that will correspond, like ultimately, that's going to be limited by base risk. Yeah. Are you saying that when G is convex, you have that G of F of Y is a potential game? Because you're now playing an ERM versus the best response. So somehow you're claiming something about a potential game if that's equal. That's probably one way. That's probably one way to think about it. I'd have to page back in. I haven't thought about this in that way, but yeah, that may be a way to think about this. Right, I mean, like, we are using regret minimizing strategies in some of the case. No, I will take it back. If you're picking G through regret minimization, you mean F, F through. So this is like, this is a. So, this is like this is a loss-minimized paper. Sorry. If you're picking the group where F has the highest value, no regret, I have no issue. But if you're picking it with the one that has the highest value, then this should work only if it's a potential gain. Okay. But if you're doing no regrets, then you're already addressing my concern. Yeah, yeah, yeah. We should talk about this more often. Now, this is, it's not the case that this is like, this isn't just. It's not the case that this is like, this isn't a statement about anything, this isn't a statement about generalization. This is like sort of hallucinating I have access to like true losses. In practice, or sort of like, you know, our intuition might suggest that this also might address some statistical issues based by the population with lower performance, if that lower performance issue is in fact coming from a statistical issue. It's a lot harder to formalize that than you might think, and I can talk more about that. That. But I do want to say a little bit about modern ML. Yeah, exactly. So I'm going to happily talk more about that during Coffee Break. Okay, so I do want to say a little bit about some modern ML sources of some of these differences, right? And what I mean by that. Like this, apologies, this one slide is not meant for you guys. This slide was meant for economists. So yeah, just like think about what we're thinking of. Think about what we're thinking of as all of the things people are calling AI in the New York Times today. Okay, so a lot of these settings are much higher-dimensional. The number of samples we have is like no longer really growing polynomially with the number of dimensions we have. We're generally training high-complexity models to predict Y from X or to sample from some distribution. The set of labels, if we're even thinking about supervised learning, is generally. Thinking about supervised learning is generally like super hard, super large, right? Like Chat GPT is like completing sentences in various languages, right? Great. So the distributions, some of the things that are different here, the distributions over X's might differ fairly significantly by demographic in a way that's like somewhat different than in this like low-dimensional. Different than in this low-dimensional regular setting. Y conditioned on X may or may not, especially when some of, you know, if I look at images of people from different populations, right, it might be that like the X's are completely separable, right? Like, I mean, I'm not saying that's always the case, but like, that really makes a different problem than the fact that in tabular settings, I might have two records of people from two different populations that have different. From two different populations that have different lives, right, but are otherwise identical. So that makes the problem or the problems here that we face fairly different. It basically means that if our model is sufficiently rich, it can learn different relationships between X and Y for the different populations in a way that isn't forced to be coupled. And this comes from the high-dimensionality myth of a lot of the problems we're thinking about here. And the models often Right, and like the models often can sort of treat demographics reasonably separately this way if doing so is useful. This can mean we can learn to predict in a way that's tailored to different populations, but it can overfit to smaller populations and can lead to like, you know, any number of things that look like stereotyping or other problems like this. Okay. Generally, Generally, like in the previous case, one good way to improve performance for a population is gathering additional data. And I know I have like one or two more minutes, so I'm going to say one thing about that. So in practice, you know, it might be worth thinking about how to sample to mitigate performance disparities in some of these higher knowledge settings. And in awesome work with my student Rachel, along with Yoshi, who's faculty with me at UW. Who's faculty with me at UW? We thought about this in the context of face matching, right? Which is a particular kind of facial recognition I'll talk to you about later, but I'll talk to anyone who wants to know more details. But we were curious about what it looks like to think about face matching and sort of if I have differences in performance and face matching for different populations, how should I gather additional data? And basically, the high-level takeaways we found were that adding data from a particular group. Adding data from a particular group usually was pretty effective in improving performance for that group. It very rarely decreased performance for any other group. In fact, like almost everyone's performance improved by increasing data from any particular population. Okay, so apologies. I just want to skip to my last slide. Okay, so all right. So, all right, so thank you very much. I appreciate your attention. I'm happy to talk more about that.