Quantum Uncomplexity. Well, thanks to the organizers for inviting me. So, as I'm Anthony Munson, I'm a PhD candidate in physics at University of Maryland, College Park. And today I'll be talking about this quantity called uncomplexity, quantum uncomplexity, and talk about how it's a resource for quantum computation. Okay, so before we talk about quantum uncomplexity, what about quantum complexity? Now, I think you've Quantum complexity. Now, I think you've all been listening for the last hour or so about quantum complexity, so I won't rehash too much. I'll try and get through it quickly. So, the gist is just that there are different ways to define quantum complexity, but ultimately it quantifies the difficulty of preparing a state, in this case a state, from some simple reference state. So, throughout the talk, I'll be, in your mind, I want you to just be thinking about an in-qubit system and the qubit system and using the all zero state where zeros in the computational basis as that reference state. So as Nick mentioned, there are different, there's exact unitary complexity as well as approximate unitary complexity. Exact one is a little bit easier for intuition, even if it's not quite as useful in many cases. So the idea is that if I start off with So the idea is that if I start off with some here, pure state rho, or not rho, psi, I want to know what's the minimum number of elementary gates that I can use to construct the circuit taking me from psi to the all-zero state. And for notation, we'll just be referring to the complexity of the state psi and C of psi. Oops, real fast. Else. Okay. So, why do we care about quantum complexity? So, quantum complexity has featured in a number of subfields of physics in the last several years. Most obviously, perhaps in quantum information, where complexity has quantified the difficulty of discriminating states and preparing superposition. It's also made an entrance in condensed matter physics, where complexities scale linearly and system-sized have. And system-sized have distinguished topological phases, and perhaps most interesting to those in this audience, in high-energy physics, it's made an entry as well. So in ADSCFT, we can look at, it's been conjectured that if we look at the wormhole connecting two black holes, that the complexity of the field theoretic state dual to that wormhole is proportional to the wormhole width. So, what is quantum? So, what is quantum uncomplexity? So, intuitively, it's just the lack of complexity. We define this so that we can understand it as a resource. But before we can really formalize the lack of complexity, we need to be sure that there is a maximal complexity. And so, by a simple counting argument, it can be shown that for any qubit state rho, that it has some maximal complexity, which is of the order e to the. To the end. The reference for which is given there, and at the end of the talk, I have references if anyone's interested. So, once we establish that a maximal complexity exists, it's very straightforward to define the uncomplexity in the state row, which is the difference between the maximal complexity and the state's complexity. So, as I said, uncomplexity can be viewed as a resource. As Nick was discussing earlier, it's been shown that in Earlier, it's been shown that in some systems undergoing random dynamics, that complexity grows linearly for times exponential in the system size. And in other cases, it's just conjectured to be the case. In any case, this gives us some intuition that complexity is something that in systems undergoing random dynamics generally diminishes with time. Diminishes with time. And so we can think of uncomplexity, lack of complexity, as some resource that we can have in some initial state that is scarce in a sense. Another intuition comes from quantum computation, where we can think of a state which is uncomplex as one like a blank qubit. So if I have something like the all-zero state, then I have clear information about or knowledge of that state. Or knowledge of that state, and as such, I can perform a computation on it. So it's useful just as blank paper is useful for pencil writing. And then, lastly, recalling the connection with high energy theory, it was conjectured by Brown and Susskin that uncomplexity can be formally understood as a resource in quantum computation. And that's what I want to talk about today. So, this conjecture has been confirmed. This is work done in collaboration with my co-advisor, Nicole Junger-Halpern, as well as some collaborators in Berlin, in which we show that a resource theory of uncomplexity indeed can be defined. You can find it at this archive link. It's also recently been published in Physics Review A, so you can find it there as well. Okay, so now let's have, let's give an overview of where we're going. First, we're going Where we're going. First, we're going to begin with the resource theory of uncomplexity. Then we're going to look at operational tasks that can be defined within the theory. After that, we'll make a detour to define a quantity called the complexity entropy, which will allow us to quantify the optimal efficiencies with which we can perform the aforementioned task. So let's begin with the resource theory of uncomplexity. The resource theory of uncomplexity. So let's unpack that. What's a resource theory? It's very straightforward. It's a general framework in which we have an agent who can perform some set of simple tasks or simple operations that satisfy some simple rules. So the rules can be chosen freely, but they're usually well motivated, so that states that are difficult to prepare with the allowed operations become scarce. Allowed operations become scarce resources. And this allows us to, and those scarce resources can facilitate operational tasks. Importantly, and this will come up later, a theory is defined solely by its allowed operations on some specified set of states. So once we know the allowed operations, then the resource theory is entirely known. So to give some intuition, let's look at a couple of examples of resource theories. A couple of examples of resource theories. First, the perhaps canonical example is the resource theory of entanglement. So, in this case, our free states, those which are not scarce, are separable states. And the free operations are what are known as LOCC states. So, LOCC is an acronym for local quantum operations and classical communications. So, the idea is that if I have two agents, Alice and Bob, they're able to perform their They're able to perform their local computations with whatever quantum computers they have, and they're also able to communicate classically. But importantly, they're not able to use the power of full-fledged quantum communication. And that means that they can use entanglement in order to perform tasks that would otherwise be impossible, but they can't generate entanglement. And that's the reason why entanglement is a scarce resource. Course. So, another example coming from thermodynamics is the resource theory of athermality. So, in this case, the states are pairs of density matrices and time-independent Hamiltonian. So, basically, I have a density matrix rho, I have some time-independent Hamiltonian, basically, some initial state and some Hamiltonian. Some Hamiltonian to governance evolution. And the free states in this theory are the thermal states, so just Gibbs states, and the free operations are any processes that conserve total energy on your system bath exchanges. So in this resource theory, the valuable states are those which are far from equilibrium, so that I can perform some tasks, say work extraction, that I can't facilitate with thermal states. Thermal states. Okay, so now you have a sense of resource theories. Let's turn to the resource theory of uncomplexity. So, as I said before, the resource theory is entirely defined by its allowed operations. And so before I give the allowed operations for the resource theory, I need to introduce a primitive, which is a fuzzy gate. So, physically, this is a gate that is implemented with some noise, which is modeled in a particular In a particular way. So the idea is that you, as some experimenter, want to implement some unitary GTU, but you can only do it whenever you attempt to do so. You can only be sure that you will implement a unitary within some epsilon ball of the desired gate. And that epsilon ball. And that epsilon ball is defined with respect to the infinity norm, and the probability distribution will vanish outside of that epsilon ball. Moreover, for those who are interested in the technicality, we also require that the probability distribution is non-vanishing on some open set around you. So it goes in all directions initially before it take resolve. So this is for an individual. So, this is for an individual gate, and so the allowed operations are so-called fuzzy operations, which are just compositions of those fuzzy gates. Now, in contrast to the resource theories that I discussed on the previous slide, in this resource theory, there are no free states. So, you have the allowed operations, and then afterwards, you can find what states are free with those allowed operations. In this case, there are no such states. Case there are no such states. And this is clear by a straightforward argument. So if I consider the maximally complex n plus m qubit state, we see that by that counting argument I mentioned earlier, the complexity is of the order e to the n plus n. But if we consider tensoring the maximally complex n qubit state and the maximally complex n qubit state, they'll have a complexity which is They'll have a complexity which is the sum of e to the n and e to the n, which in general is less than the previous quantity. And so, what that means is that if I allow tensoring on, we run into an issue, because now I can create uncomplexity for free. So, in this case, we can have no free states, otherwise, uncomplexity is not a free resource. Or is, my bad, a free resource. Okay, so now let's. So now let's turn to the operational tasks defined within the theory. So there are a couple. First, I'll discuss uncomplexity extraction. So the idea is that you start off with some state that may be rather complex and you manipulate it in such a way so that you ideally end up with something simple or uncomplex. Going the other way, The other way, we start off with some amount of uncomplexity and we want to expend it in order to create something complex. So let's begin with the first task, uncomplexity extraction. It's fairly straightforward. What we want to do is start off with some state row. As I said before, we're trying to extract uncomplexity. So we apply a circuit composed of at most R fuzzy gates. At most, R fuzzy gates, and then we select some number W of the qubits at the end with the hope that at the end the selected qubits will be delta close to the all-zero state for those selected qubits in trace distance. So, importantly, that delta means that we don't have to get exactly something that's pure or that is, you know, I guess, maximally uncomplex. Uncomplex, but rather we just need it within some approximation. The definition for the uncomplexity expenditure task is a little bit more complicated, so I'll need to introduce a primitive first, which is the complexity of measurement operators. So in this case, we will start off by defining zero complexity of measurement operators. Zero complexity measurement operators, and thereafter we'll have a straightforward definition for R complexity measurement operators for any given R. So the idea with the zero complexity measurement operators, so let's simpler unravel this formula. So basically we're tensoring one of two single qubit operators, which is either a projector onto the Onto the zero state for a single qubit, or we're just applying the identity operator. And so those are all of the simple. So the idea is that these are ones that would be relatively simple to implement and would verify that a state is in that a subset of the qubits is in the alt zero state or not. In the alt-zero state or not. And then for the R-complexity measurement operators, what we're essentially doing is just pre-processing some zero-complexity measurement operator with some R-complex unitary. So we apply some R number of gates to the system and then we apply the projective measurements. So N0 is a state with the power of n elements? Elements? So M0 is a set of Q of M elements. Yeah, it looks fine. And then you choose Q0 as a set of elements. Exactly, exactly. Okay, so the setup for the uncomplexity expenditure is as follows. So we're going to have a computationally limited referee who wants to distinguish between row and the maximally mixed state. Rho and the maximally mixed state using some R-complex measurement operator. And moreover, the referee wants to guess rho successfully with some probability at least eta. And you, the agent, knowing your goal is to fool the referee with some simulacrum state, rho tilde, hopefully with as little complexity as possible. So now let's define this procedure. So you go to some uncomplexity bank. So you use some number of qubits that will be initialized in the all-zero state. And the rest you have no control over. This will just be junk. After that, you apply at most our fuzzy gates. And that will give you your symbol acron state, Rhode Tildo. You hand this off to the referee, and hopefully, you'll. To the referee, and hopefully you'll succeed in having the referee guess that the state rho tilde was actually rho with probability at least eta. And that probability means that there's some error in what you can, or at least some, you only need to reproduce rho approximately up to the resolution with which the referee can distinguish rho and rho. Rho and the maximum mixed state. Okay, so now that we've defined these operational tasks, we want to, as an interlude, define the complexity entropy, which will allow us to finish with giving the optimal task efficiencies. So, some motivation for the complexity entropy. One is just general purpose motivation. The entropies are typically The entropies are typically used to balance operational tasks. Sounds like foreshadowed. An example of that is just how a Shannon entropy does so for data compression. Moreover, we want to quantify how uncertain a state looks to a computationally limited observer. So this is a little bit tricky if we don't use an entropy which is engineered to Which is engineered to distinguish between the complexity of states. For instance, if you think about the von Neumann entropy, I can give you any pure state, and we're going to get a von Neumann entropy of zero. Now, clearly, you can have states of very high and very low complexity, pure states, and very high and very low complexity. And so the von Neumann entropy doesn't distinguish between any of those states. And so we're seeking entropy, the complexity entropy that we're seeing. The complexity entries that will differentiate states with respect to complexity. So here's the definition of the complexity entropy. So for an n-qubit state rho, we have some intolerance parameter that goes between 0 and 1, where 0 is not included. And let me break down this formula. So in essence, what we're doing is we're maximizing a surprisal. A surprisal with respect to a couple of constraints. So, this expression in the middle can be understood if you squint and take the trace of Q as something like the reciprocal of a probability, you'll see that the log of that quantity is like a surprisal. And the and we're minimizing with respect to two concerns of the Q is a an R complex measurement operator. our complex measurement operator. And we require that this Q, so the Q is, it has to be at most R complex and moreover, it has to identify rho successfully with a probability at least eta. So to recap what I just said, the second condition can be understood as bounding the type one error for identifying row. For identifying rho. And basically, this complexity entropy gets the minimal. So, once we constrain for the type 1 error, we're giving the minimal possible uncertainty due to the type 2 error. So, let's get some intuition. What is type 1 and type 2 error? So, the type 1 error here is the probability that you probability that you don't receive row and that you don't receive rho and say that it is row and the type 2 error would be the other way around. This actually maybe isn't the best description until we get to the end of the next slide because this can be understood as a hypothesis testing entropy in which kind of implicitly this formula. Kind of implicitly to this formula. You should think of it in the context of a hypothesis test comparing row in a maximally mixed state. So that other counterpart, the maximally mixed state, then type 1 and type 2 makes sense. So perhaps if it's not clear in a moment, I can clarify again. And you're allowing install a free image? Am I also optimizing a vertical? Not in this case, no. No. My question is from the second law of thermodynamics, the system becomes more complex than uncomplex? Because you're assigning entropy to the chip. He was saying, sorry, could you hear? So by the motivation that I gave before, that under random dynamics we expect the complexity enter to be, yeah, exactly. We would expect this to be the case. Why don't I allow an assistant? I like give myself as many resources as I can think that's a valid consideration. In this case, that's just not a part of the model, but I'm very agnostic to including that and perhaps it actually will work. Perhaps that actually would be an interesting extension of what I'm presenting here. Okay, so now let me give some intuition for the complexity entropy by considering a couple of limiting cases. So the first, we're going to consider a low complexity state. The easiest example would just be the all-zero state, and we'll see that if we don't have very much, we don't need very much complexity at all, in this case actually zero. In this case, actually, zero, in order to satisfy the constraints, because we can just do the all-zero projector. So, with the zero, we can use a zero complexity measurement operator. And with that measurement operator Q, we find that the trace of Q will just be 1, in which case the log of 1 is just 0, so the complexity entropy is just 0. So that's the minimal value that the complexity entropy can be. That complexity entropy can take on. In the opposite extreme, we start off with a high-complexity state, and we don't have many gates that we can apply. So even after we apply the optimal unitary, we still have a very high-complexity state. And the best that we can do is just use the identity measurement operator, which is always allowed. So basically, we're just kind of punting. So basically, we're just kind of punting. In that case, the trace of q is 2 to the n, the logarithm of which is just going to be n, so that's the maximum value. So this ranges in integer steps from 0 to n. And then going to what I just mentioned a moment ago, there's a relation to the hypothesis testing entropy, which quantifies the uncertainty in a hypothesis test between some state rho and the maximum mixed state. And you see the formula on the right-hand side, it's very similar. The right-hand side, it's very similar to what I presented on the other slide. The only difference is just that we restrict the measurement operator Q to be at most R complex in the previous case, whereas in the hypothesis testing entropy, there's no such constraint. Okay, so now to finish up, let's talk about the optimal tax. Let's talk about the optimal task efficiencies. We're going to present a couple of theorems related to the complexity entropy for respectively for the uncomplexity extraction and for uncomplexity expenditure. So each of these theorems will establish the existence of a protocol achieving the task as well as the near optimality of the protocol. So the theorem for uncompletion. For uncomplexity extraction is as follows. So we start off with some in qubit state rho. We have some complexity allowance R and an error parameter delta. We make some technical assumptions about how delta R and epsilon, the fuzziness, are related, and we show that some protocol extracts n minus the extracts n minus the complexity entropy of n minus the complexity entropy would we that many qubits we can get it delta close to the L0 state and then conversely we show that every uncomplexity extraction protocol will be bounded by a similar policy so to give some intuition we'll return to the low complex Will return to the low complexity and high complexity limits. So in the low complexity limit, we see that some protocol will extract n qubits, and every other will extract less than n, which is just trivial. So this makes sense. If you start off with something that's very uncomplex, then you should be able to basically have to do very little, if not nothing, in order just to extract out the maximal number of qubits. Qubits. And then in the high complexity limit, we just see that all protocols just extract zero qubits. So no matter what we do, we're just in a situation where we can't extract any uncomplexity. Now let's turn to uncomplexity expenditure. So we have a similar setup. We begin with an arbitrary n-qubit state rho, having again the complexity allowance r and this error parameter delta. Once again, we have an assumption relating delta, r, and epsilon, and we show that for every eta there and for every junk state, sigma, that row can be imitated successfully with n minus the With n minus the complexity entropy number of uncomplexity rows. So again, we can look at the low complexity and high complexity limits. So in the low complexity limit, rho can be imitated only with n qubits. This is because the referee can actually do a pretty good job of distinguishing rho and the maximum mixed state, so we really have to go to that. Mixed state, so we really have to go to that uncomplexity bank and get a lot of uncomplexity here in the limit in qubits in order to fold the referee. And then the high complexity limit, unsurprisingly, we can imitate, row can be imitated with zero qubits. So the referee can't do a very good job distinguishing. We can actually just get junk and still do just fine. Okay, so in summary, we defined the resource theory of uncomplexity, confirming Brown and Suskin's conjecture. Thereafter, we defined operational task in the theory, uncomplexity extraction and expenditure. We introduced the complexity entropy on the route and used that to define the optimal task efficiencies. So as some So, as some directions for future research, we can actually in the last year I've been working on a collaboration with previous collaborators for this resource theory work on using, detailing the properties of the complexity interview given kind of thing. Biography, so to speak, of this quantity and applying it to some problems of randomness extraction as well as data compression iteration and quantum development. Data compression iteration along with evident events. It's also interesting to consider the phases of uncomplexity extraction. So we have those. Those R, delta, and epsilon parameters. So one can ask at which point can we extract no uncomplexity? At which point can we extract maximal complexity? For what range of those parameters do we see that shift? And then because the resource theories can be. The resource theory was conjectured in the context of high-energy theory, black hole physics. It's also just interesting to explore how this resource theory can be used in that context. And this is the work. You can find it at this archive link. Thanks to Close Home. We still have time for question actually. For this black hole physics, what is this resource theory for uncomplexity? What is the uncomplexity for a black hole? The Hawking deviation will be uncomplex situation. What is uncomplex situation? So uncomplexity is defined in the same way there. I think that was the original, that was part of the motivation, thinking about the complexity of black hole radiation. So this is So, this is the work by Bravin Suskin, Second Law of Quantum Complexity. So, the idea is that you add another qubit to your system, the complexity has another basically exponential machine. So, the bulk works is cute if I add an unentangled degree of freedom. But if you uh so you're t when you're testing, you're getting a the maximum complexity, right? Yes. Or you said, uh are you referring to the slide where I talked about so when you're tensoring versus uh direct product I guess, right? You said tensoring versus two system of two qubits. Two system and two qubits. You're taking back order versus the tensoring, right? And then your argument is that you get maximum complexity when you're tensoring, right? Is that? You get the maximum. So the argument I had was just if you started off with two maximally complex states. Basically, once you tensor them on, then you'll have a state which has less complexity, less than maximal complexity for that complicated system. This is when you take tensor problems, right? When you take answer problems, right? Yes, it's an odd quantity in that respect. I mean, intuitively, with respect to certain complexity. Now you can just say, okay, well, if I have a maximum complex, if I have these two systems on which respectively I have maximally complex states, and I put them together, now you can imagine that there are additional unitaries that actually would entangle them further that weren't there before, which is the reason why. Like, for example, if you take a Jacobi group, like, you know, assume Group like you know, SU1,1 semi-direct with Heisenberg, it will have less complexity than the separate Heisenberg plus the complexity for SQL. So we can also measure them, and then complexity will go down this projected measurement yes. I mean if if you're projecting onto the zero, it's a very simple. Onto the zero state. So there's a subtlety which, so I said the within this talk, I didn't intend to imply that any measurements were actually going on, although they can be understood this complex entry in terms of measuremental operators. So the idea is that there exists a measuremental operator. There exists a measurement operator that, if you were to perform it and you had a state which was already, say, some qubits are already in the all-zero state, this would serve as some verification that the state was already uncomplex. Everything for next states as well? Everything. So when we were like uh like a few years ago Uh, I think a few years ago. Yes. So we didn't do everything in terms of pure speed. I mean, the complexity entropy is defined for mixed states. But like optimal distinguishing measurements for immediately say that everybody's going to be able to do that. To immediately say that everything is just for mixed states. I mean, wherever I add a state row within the presentation, then the mixed state should suffice. I mean, importantly, the complexity of introvy is defined for mixed states, but you're noticing an important subtlety, which is that I don't define the complexity of a mixed state. Yes. Which I mean. I mean, is it necessary for the definition of the complexity entropy, but I think otherwise you do need to do something like maybe purify the system, something like that. Complexity entropy is probably well-defined for questions, but it makes it more so complex. So I want to maybe think about that a little bit more. That's a little bit more. I mean, in the case of a pure state, it recovers the intuition properly. But yes, you're asking a thoughtful question, so I want to think about it for a moment and give a thoughtful answer. 