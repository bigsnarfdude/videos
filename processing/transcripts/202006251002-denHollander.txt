Full screen mode. Okay. There we go. Right, so welcome back, everybody. On Tuesday, Aiden and I presented an example of a we presented an example of Kawasaki particle hopping down. Of Kavasaki particle hopping dynamics on a finite piece of lattice, and we showed that in a certain metastable regime of low temperature and low density, there is nucleation of a gas into a liquid that and that we could actually really quantify in a detailed description this metastable crossover. And part of our, you know, a lot of the effort was controlled around understanding what exactly are the critical droplets. What exactly are the critical droplets that form the barrier for the nucleation? And for carbohydrate dynamics, it has a certain richness. And this is always very important because most of the action also in the variational principles that you use to do computations are all around what is happening near the critical droplet. They are the most delicate thing that we need to understand. Now, in today's lecture, we're going to move to a completely different example. A completely different example. We're going to look at Glauber spin-flip dynamics on finite but large graphs. And we're going to be interested in a different metastable regime, not of low temperature. We're going to consider any subcritical temperature, but we're going to look at the limit of large volume. So it's a specific other type of metastable regime that we're going to look at. And our task. Going to look at. And our task will again be to capture and to quantify metastable crossover times. And as you will see, there's a new ingredient coming around, namely we will need to do a bit of homogeneization because now the environment in which we're doing is a random graph. It's no longer a regular lettuce. So other things are going to come around. Are going to come around. Okay, so let me try and define the problem for you. And this is going to come again in four ingredients. The first thing that we're going to start out with is by saying, well, we have a graph, a graph G, and a graph is nothing other than a set of vertices V and a set of edges E that connects pairs of vertices. And we're going to assume that. And we're going to assume that this graph is connected. We don't want it to fall apart into two or more parts. And we're going to imagine that on every vertex there is an easing spin that can take the value plus one or minus one, and that these easing spins are going to interact with each other as easing spins usually do along the edges. So if there's not an edge between two vertices, these spins. Between two vertices, these spins at these vertices are not interacting, and if there is, they are right. And we always need to begin by defining a configuration space. Well, the configuration space is going to be now omega is the set of, is the set minus one plus one to the power of the vertex set. So this is all possible arrangements of easing spins on the vertices. And we're going to denote. Vertices. And we're going to denote the easing spins by sigma. So I'm in for Kawasaki dynamics, I was using the symbol eta because there the values were 0 and 1, occupied or empty. And here, to flag that, we're going to use a different symbol. So our configuration space is as it is given here. Then we need to define a Hamiltonian. So we need to assign to every space. Assigned to every spin configuration and energy, and we're going to use the standard easing Hamiltonian, which consists of an interaction part where you say take two easing spins and they interact with a ferromagnetic interaction strength J, which is strictly positive, a parameter we can play with. And the interaction is only present along the edges in the graph. And then we have an extreme. And then we have an external magnetic field, H, which we're also going to take positive. And this is an interaction that every spin has with the magnetic field. So spins prefer to align with the magnetic field, because if the spins are positive, then this term is negative, and low and lower energies are more favorable in the Gebs measure than higher energies. Than higher energies. And also, if spins are parallel, then this will make this sum big, and with the minus j, it will make it small. So this j really has a tendency of wanting to align spins where it can. Okay, so we have our configuration space and our Hamiltonian. And now we need to define spin-flip dynamics. And for this, we're going to again use the standard. Going to again use the standard metropolis dynamics associated with the Hamiltonian that we have just defined. And the allowed moves in this case are moves where a single spin flips either from up to down or from down to up. And you will accept a possible flip at a certain rate that is e to the minus beta times the energy difference that this flip would cause. Wood cause. And there is a parameter beta again in front. This is the strength of the, you know, that's a strength parameter in your dynamics that tells you how difficult it is to climb up in energy and how easy it is to go down in energy. And this plays the role again of inverse temperature, so that's very similar to what we have seen before also for Kawasaki dynamics. Also, for Kawasaki dynamics. And then there is a Gibbs measure, which is e to the minus beta H sigma on that configuration space. And you need to normalize it by a partition function. And the reference measure that we're going to take, so the measure that we take if there would be no interaction at all, is in this case going to be simply the counting measure. So that's why there's no measure here. We use the symbol. measure here we use the symbol q before we're not that's going to be the counting measure and it is a well-known fact that this equilibrium measure is the reversible equilibrium of this metropolis dynamics because that's automatic from the way the uh the this the metropolis dynamics is defined okay so with that puts us into uh in business in in introducing the key ingredients for our model The key ingredients for our model again, as we should always do. And there again are three sets that we are interested in, a metastable state, a stable state, and a crossover state. And we will have to figure out what these states are in this particular model. And that's not so obvious at this stage, but we will have to be looking for them. And we will see that they will have something to do with. Will have something to do with magnetization that takes place in the system. So we will slowly get there. I would like you to be fully aware of the fact that because the graph is non-homogeneous, we're dealing here with a non-homogeneous setting, and that is challenging. So there's nothing periodic here. We really have to. Here, we really have to come to grips with the fact that the interaction is only working along the edges. And if your graph is very inhomogeneous, that will have an effect. And in fact, later, we're even going to take this graph random. So then it means that you're essentially talking about an easing model in a random environment. And we will come to that as we. Come to that as we go along. Again, I'd like to show this paradigm picture that you have seen several times before already. So our state space, well, again, I'm drawing it in a one-dimensional fashion, which is of course ridiculous, but it's nice to keep this picture in mind all the time. And there will be a metastable across over stable. A metastable, a cross-over state, and a stable state, and we want to somehow imagine what is going on. And what will happen is that this stable state will correspond to a certain positive magnetization that the system would like to have as it is in equilibrium. And we're going to start the system off with too small a magnetization, in fact, a negative magnetization. And we need to wait until it goes over the hill. And because the magnetic field, external magnetic field, The magnetic field, external magnetic field, is positive, it likes to have a positive magnetization, but it needs to go over a barrier to do that. And that's what we're going to try and quantify. Now, on top here, I have written not energy, but free energy. And that is because this picture is no longer only looking at the Hamiltonian, not only at the energy. Not only at the energy. Because we are going to be interested in very large graphs, we will have to deal with entropy. So entropy is going to come into the game. How many configurations would correspond to a certain magnetization? And there's entropy entering into the game. And that's why it is now a game of free energy. So, competition between energy and entropy. And again, this is. Again, this is still a rough paradigm, and you will see this picture coming back into much more detail when we really sit down and say what is happening. Okay, now let me very quickly remind you, just to be self-consistent, of again the key formulas as they come out of this very important paper by Bovier Ekovgar. Important paper by Bovier, Ekov, Gerar, and Klein that I have already mentioned a few times. We're going to think of P sub M to denote the probability distribution on path space of our Glauber dynamics on this configuration space that carries this graph as the basis. And suppose you start in a certain state M, which we will have to specify. M, which we will have to specify. Again, the question is: how long does it take before you go over the hill? And there is this nice formula, which should be true in metastable regimes, that this is all essentially a story about capacities that we need to estimate. And we will have to quantify what this M and S really mean, and we will get to that. And in this case, we're going to be interested in large volumes and In large volumes and, in principle, arbitrary temperatures, as long as they correspond to a situation where there is a phase transition, because that's when metastable behavior can and will occur. Right, so a very quick reminder is that this capacity is given by this Dirichlet principle and for the specific easing. Specific easing spin flip Glauber dynamics that we are dealing with. This Dirichlet form takes a particular form. It's always a square of differences of a test function. And here there is a nice function in front of here that comes from this metropolis dynamics. And you're looking at all possible transitions where you flip a single spin. That's what this twiddle means. What this twiddle means. So, Twiddle means that the allowed is an unallowed move, and we're only thinking of moves that are corresponding to single spin flips. And here you see that if you want to minimize this Dirichlet form, you want to really make the action where this Hamiltonian is large, because then this term is small, and then you can permit yourself to do something interesting here. Interesting here. Whenever you go away from the top, this factor gets much bigger, and you want to essentially kill this term by making it almost like one minus one or zero minus zero. So these are things that we talked about in the first lecture, that there is a everything that is really happening in the meta stable regime revolves around where this Hamiltonian is maximal. Hamiltonian is maximal. And there's a dual Thomson principle, also very simple, which is a supremum. And it takes this particular form here, where now the supremum has taken over the unit flows. So all flows along edges in the graph, such that the flow in out of M is one, into S is also one, and every other vertex. Also, one, and every other vertex has a zero sum for the in and the out flow. And so, those are the tools again that are crucial for getting your hands on the problem. And again, here also for Glauber dynamics on random on graphs, this continues to be the key tools. You know, the key tools with which you're going to play. So now I'm going to do two things. I'm going to first start with a very simple graph, which is the complete graph. And this leads us immediately to a very famous and an old model, the Kurieweis, a model for spin-flip with spin-flip dynamics. And I'm going to first tell you what we know there. What we know there. This is a very classical situation, but we must first understand this classical situation before we are ready to go to more complicated graphs. And in fact, after the break, I will continue with the Erders-Rainy random graph. And then at the end of the lecture, I will even quickly mention a few other random graphs that people have looked at that are also quite interesting. Interesting. So we're having the complete graph on n vertices, capital N vertices. And this means that in our Easing Hamiltonian, everybody is interacting with everybody else. All edges in the graph are there, so it's a really very simple situation. And let's see what we know about that. This is an old problem that has, however, only really been pinned down in Pinned down in full detail in the works of Bovier. So since every spin is interacting with every other spin, it is natural to make a choice for this interaction parameter. We're not going to keep it fixed because we want to let n go to infinity. We're going to make it 1 over n, and where n is the volume of N is the volume of the vertex sets, counting the number of vertices. And we do that to ensure that the total interaction of a spin with all the other spins is still of order one. Because we want to pass to a limit n to infinity, and we do not want that to cause that a spin is having an infinite interaction with everybody else. So this choice is very natural. And we take 1 over n rather than maybe 10 over n because that's just a trivial. 10 over m because that's just a trivial scaling. And since we are in a mean field situation where everybody, every spin is interacting equally strongly with every other spin, we are in a very nice setting that you can use a lumping technique in which rather than us following the dynamics of the full configuration, we are only going to follow the dynamics of the magnetization. So we're going to project. So we're going to project down from a very high-dimensional problem to a very low-dimensional problem because there is so much symmetry in the problem that that works. And this is, of course, very standard. This is the usual thing that happens in mean field theory. But let's see how that works out in this particular setting. Right, so the quantity to look So, the quantity to look at is what we call the empirical magnetization. You sum all the spins and you divide by n, and this is a number somewhere between minus 1 and 1. And it is expressing what the net magnetization is of my configuration. And since the spins, when they flip, they change from minus one to plus one, this actually lives on. This actually lives on a fine grid of size 2 over n in this interval between minus 1 and plus 1. And if you look at what your dynamics is really doing, your spin-flip dynamics with the metropolis rates for the easing spin Hamiltonian on the complete graph, then it turns Graph, then it turns out that this quantity over time is actually evolving as a continuous time random walk that moves on a very fine grid in this interval, and that is behaving as if it's in a potential. So, if the potential goes up, it has trouble to go up, and if the potential goes down, it likes to go down. And if you do a computation, and this is part of And this is part of the exercises that come with this course, with this lecture. Then you will see that it is a random walk on that fine grid in a potential that is given by a very explicit function. And this function is actually nothing other than what would be called the free energy per vertex. And this has an energy part where you see that there's. Where you see that there is something quadratic coming from the interaction between spins, there is something linear coming from the interaction with the magnetic field, and there is an entropy term that keeps track of how many spin configurations actually correspond to a given value of the magnetization. And this is crucial. If you change the magnetization, then you change the number of configurations. The number of configurations that correspond to this magnetization, and every magnetization has its own entropy, and you need to compute that. It's a very simple computation when you go down from this big spin space to this one-dimensional magnetization space, that this is the factor that comes about. And this quantity being an energy and an entropy term. And an entropy term deserves the name to be called finite volume free energy per vertex because we're really normalizing everything by the number of vertices. So this magnetization is doing something simple. A random walk in an explicit potential, and that is an object that we can compute with. So, if our system I can imagine System, I can imagine that I put my system in a starting situation where the magnetization is negative and it wants to go and evolve towards a situation where the magnetization is positive because the external magnetic field is positive and it likes to align with that field. You say, well, that is going to perhaps not happen so easily because the magnetization has to move from a minus value to a plus value. A minus value to a plus value, and there, and in order to follow how long it takes, I might as well look at how long it takes the magnetization to move from a negative to a positive value. And so, by looking at this evolution of this empirical magnetization, which is very simple, we can still track what the metastability is doing really in the space also of all the configurations by using the symmetry. By using the symmetry here. Now, fine, we're going to want to let n go to infinity, keep beta fixed. We are interested in a very large configuration. And so what might happen? Well, what might happen is that in the limit as n goes to infinity, this empirical magnetization starts to perform a Brownian motion. A Brownian motion on that unit interval. So, what is a random walk on a very, very fine grid? As the grid becomes, converges to the full interval, the random walk becomes a Brownian motion. And it will be a Brownian motion in a potential that is quite simply the limit of the free energy per vertex that we had before. So, if I take this quantity and I let n go to infinity, And I let n go to infinity, then this object here, here's the n-dependence, this will simply converge to something. And where it converges to is a very simple function. Except I have taken out a factor minus log2 because this turns out to be redundant and it doesn't play any role. So I'm just kicking out a constant in order to make the computations a little more palatable. And this computation. And this computation also is part of the exercises. How do you do that and what is exactly happening? And so the empirical magnetization performs a Brownian motion on this interval in a certain potential, which we will have to look at. And I'll show you a picture in a second. And this is exactly the setting that Kamars in the 40s analyzed when he started to. When he started to become interested in metastable behavior. And so he looked at Brownian motion in a double well potential and did computations. And so for the Curie Weiss model, which comes from interacting, the world of interacting particle systems, we are in the limit as n goes to infinity, enter the world that Cameras looked at, and that has become a paradigm for the analysis of metastable behavior. Analysis of metastable behavior. So we're really having a very simple setting of what the magnetization is doing. Right, so it's time that I show you a picture of what this free energy per vertex at magnetization M looks like. And if you M looks like, and if you look at this formula and you look at it in more detail, then it turns out that this thing has a double well potential, and it looks something like this. There is a global minimum at a positive magnetization coming from the fact that if the system is positively magnetized, it is aligned with external magnetic field. It really likes that, and it has a Field, it really likes that and it has a lowest possible energy. If it has another minimum, which we call the meta-stable state, which is a negative value. So, if spins are negatively pointing out, they are not aligned with the magnetic field, but they're still very happy because they're all pointing negative and they're getting a lot of interaction from each other. From each other. And there is somewhere in between a threshold that you can compute. It's at some value m that is the solution of a certain consistency equation and that is somewhere in the middle and that also happens to be still negative. And it is because h is possible. So h minus h is the slope of this line at the origin. If h would be zero, then Would be zero, then you would have perfect symmetry in the system, and you would get a double well where this bump here would be just sitting at the origin. And you would get a completely symmetric well. And it is because H is not equal to zero that this is asymmetric, and that there is a metastable state and a stable state. I'm still recalling the question that Peter Winkler asked in the first lecture. The first lecture. Yes, we could also start the dynamics here and see how much time it would take to go over the hill to the other end. That we can do too. But this would be, it's not the classical situation of hysteresis and metastability that one usually looks at. And you must realize that there is a quadrature. There is a quadratic term, but this entropy plays a very important role. And the precise form of this double well potential is very much also dictated by this entropy. So if you give me a value of the magnetization, with every magnetization corresponds a certain number of spin configurations. Spin configurations, and that number needs to be kept track of because that will matter for how the spin configuration is going to evolve. So, we're having this setting here, and I can now state the theorem as we know it. And this will be after this theorem, we're going to take a brief break. And the theorem says if you are. If you are, if I'm going to take sets of configurations for which the magnetization is close to these minimal magnetizations, so this is not a bold phase, these are numbers in the unit interval. If I'm going to start by starting from a configuration, set of configurations for which the empirical magnetization is really close to these two numbers. Numbers, then in the Curie Weiss model, the time that you cross over is given by a formula that looks of the type as we've seen it before. It's a prefactor, and it's e to the n times a gamma. And this gamma is exactly the free energy height that the system has to go over times beta. So if you have your curve. If you have your curve and this FbH is completely explicit, you just look at what this barrier is, you multiply it by beta, and that is what your gamma is. Your beta can be any number larger than one, because one is the critical value for which the phase transition would disappear. And so this is a very natural thing in the Meta-Stoge behavior. And there's a pre-factor that turns out to be, well, Well, not so easy, and this was already found by Carmers and Eyring: that it depends on the numbers, on these numbers here and that number there, the M star and the M minus. And what it really, what plays a very important role is the curvature of this curve at these two points. So it's the curvature here and the curvature here. And the curvature here that also determine whether you know what this prefactor is. And the flatter the curves, the longer it's going to take to go over. And then there's a little observation, but very important observation: the H should not be larger than a certain threshold, and this threshold depends. And this threshold depends on beta, and it's very simple. And I made a picture here of what this curve looks like. And it means that whenever you are in the regime where there is a phase transition, so beta is larger than beta critical, which happens to be one here, then you may not make your H exceed a certain curve, which has this shape, because otherwise the magnetic. Otherwise, the magnetic field is so strong that you would not have mater stable behavior. The system would be very rapidly making the crossing, and you would not be in the meter-stable regime. So, this is a very nice and happy ending for the Q-revised model. There's so much symmetry that we have a complete description of the crossover time with computable quantities, and we already see here again that. And we already see here again that this prefector is a somewhat non-trivial object that you have to take into account. And it has something to do with what is happening here and here in this curve. So there is something, again, playing a role that we need to keep track of. Okay, I propose that we have a very short break here, and then I will continue with. And then I will continue with really tackling now the most important thing for this lecture where we really turn the graph into something. Okay, are you willing to take questions now? Sure. Yep. Okay, so if there's questions before the break, now's a good time to pose them. If not, we can take a wait a bit, but the other I'll wait a bit, but the other possibility is we take a break and people can stare at those prefactors and come up with questions. I will say that there's a nice exercise that Frank and Elena have prepared that deal with some of the capacity calculations, which help you see where this pre-factor comes from. So, Frank, again, you've been so clear that people are That you know, people are understanding everything, so okay. Well, this uh, this lecture is also conceptually a bit easier than the previous one, but um, something nice is coming up with the Eders-Rainey graph. You'll see, of course. I have a question, if I may ask. Yeah, please, regarding the parameters that you mentioned right now, in particular beta. beta or h to me what in the the caricature the portion of that curve is there a way to predict or assume complexity in some intervals i mean in other words uh thinking about energy and entropy exclusively is it possible to assume that within certain constraints uh there is so i mean these parameters i mean this condition I mean, these parameters, I mean, this condition here that you cannot make your H too big, which is captured, that you have to stay under this curve. This curve is not convex, it has a particular shape. That is needed in order to get your double well potential. If you are not having that condition, then this curve will not look like a double well. And so you would not have made the stable behavior. And so you would not have made the stable behavior. Okay, so but below the M, right? Below the line. No, no, in the previous, the one that you have, the caricature feature. Yeah. Right there, right. In that feature, below the line marks M for my magnetization and soon, you can see that the two minimum points. Two minimum points, right? Yeah, they tend to have an appearance of convex, right? Yeah, they are. Absolutely. And this also, the convexity also means, that's a good point, because it means that these numbers here are both non-negative. Because this has a positive curvature, this has a negative curvature. Since there's a minus in front of here, I wrote it explicitly like. Of here. I wrote it explicitly like that, in order to say this is a product of two things, and really there is curvature at these points, and this curvature plays a role in the metastable behavior. Right? So, indeed, there is convexity and there's concavity here, and in fact, there's a twist over point, and so there's much more you can say about this picture. About this picture. In fact, it's a very nice function. Right. Okay. Thank you. Okay, maybe now we will take a real three to four minute break. I think it's always a good idea. Yeah. Okay. Okay.       Hey Frank, I think maybe people are back and you can tell us what happens with random graphs. Yep. Okay, so now we're going to really bite the bullet and say what happens when we're going to move away from We're going to move away from this complete graph where which led to all high degree of symmetry, which where we could do this limping, lumping, sorry. So we're going to look at the Erdős-Rainy random graph. So this is the, you start with a complete graph, and for every edge, you decide to keep it with the probability p or to remove it with the probability 1 minus p, and p will be some fixed number. And p will be some fixed number between 0 and 1. So, what you then get is a, you're basically doing edge percolation on the complete graph. And now you have diluted your graph because you only keep a fraction P of the vertices. And what we're going to ask is: what happens if we do the exact same model on this graph? And because now some edges are missing, it means that some Edges are missing, it means that some of the spins are not interacting with some of the other spins, and in fact, we only keep a fraction p of the edges, so the interaction has been randomly diluted. So, what can we do here? Now, the very first moment, the very first thing that you realize when you get yourself in this setting is that you say, this idea of looking at the magnetization, it's gone. Magnetization, it's gone. Because if I would follow the magnetization, I would have to conclude that this is no longer a Markov process because it does not capture the, you know, it doesn't capture the information. So if I just know how many spins are up and how many spins there are down, I do not know the energy because I do not know where the, you know, where the which spins. Which spins are feeling an interaction, which not. So, lumping is gone. It's no use to look at the magnetization anymore, and it's becoming much harder. Still, you would think, well, if n gets very large, then probably this graph should be, you know, sort of homogeneous. So, and you would say. So, and you would say, well, the missing edges are probably distributed in a very homogeneous way. So, maybe magnetization, even though it's not Markovian, perhaps is still close to Markovian. Question marks, and we will have to play with that idea. But we have to come up with something really new. Now, what I'm going to do is to immediately give you two main results that we know. Results that we know for this in this setting, and then only later tell you a bit about what it takes to get these results. And it will have to do again with questions about what would this critical droplet be, how could we understand it, what techniques would we go about, and I'll say a few more things about that. That now, I first want to mention that there were two parallel projects going on in the fall. So Anton and Saida and Elena were working on this problem from one perspective. In fact, they were very much playing with the Dirichlet principle and the Thomson principle. And again, we're playing with test functions and we're thinking, well, if we just take the old Thinking, well, if we just take the old test functions of the Kurievise model suitably diluted, does that work? And indeed, it does work. And at the same time, I was working with Oliver Jovanovsky on trying to use capacities to sort of control the dynamics of this magnetization, empirical magnetization, even though it is not a Markov process. Is not a Markov process, so we could not immediately use the standard tools. And along two different routes, we came to results that are nice and intuitive and are also complementing each other nicely. So it was a very nice thing for us to be talking as a group of five also to hear what the other ones are doing. Nice time. So here's the theorem. The theorem I The theorem I derived with Oliver is rough in some sense and rougher in some sense and finer in some other sense, and I will explain that to you. So we are going to take J not equal to 1 over n, but 1 over P n now. And that's because a spin is only interacting roughly with P n spins. So the right normalization should now. The right normalization should now be Pn, 1 over Pn, and not 1 over N. We're going to stick to β being supercritical, the same as you have for the complete graph. And again, H should not be too big because this represents a metastable regime. And what we were able to do is to compare the metastable crossover time. Metastable crossover time on the Erdős-Rainy random graph with a slightly changed interaction parameter, one over Pn rather than one over n, with the formula for the Curie Weiss model. And so this statement here says that these two things are very similar, except for a certain factor which Which is not so small because it grows like some power of n, but it's really small compared to what we saw in the Curie-Weiss model. The leading order term was e to the n times gamma. So the leading order term is exponential in n, and there's a polynomial correction. And we were able to prove that this exponent cannot be too big. It's actually. Be too big. It's actually with probability one under the law of your Erdős-Rainy random graph bounded by something that is given like this in terms of the parameters. So there's a control over what the error is, but there is a polynomial correction term that we do not control. And I think you have to realize first that because the graph is now The graph is now drawn randomly, the answer should really be random. And so the Curie Weiss model has a deterministic scaling, and there is now a correction which we are able to say something about with probability tending to one under the law of the Erdős-Rainy random graph on n vertices. So there is a randomness now. So there is a randomness now, and we need to. This here is a statement about how small or how big this error is under the law of the Erdős-Rainy random graph. So on the one hand, we're happy that we got a good result. And the result was also nice because it was completely uniform in how you start. But we pay a price that there is a rather, well, A rather well, biggish error, much smaller than the leading order term that represents an error here. And we were asking ourselves the question, well, what really is going on? On the complete graph, the pre-factor is constant. We find that the pre-factor is apparently random. We could not really compute it. It's probably a very complicated. Probably a very complicated thing, but at least we have some control over how bad it could be. And the theorem is again true in the same range of parameters as we have it for the Curie-Weis model. And it's also uniform in the sense that it doesn't really matter what configuration close to this metastable state you start out with. The answer is uniform in how you start. That was a result. Start that was a result of certain coupling arguments that we did. Now, so this left us with the question, well, this pre-factor is complicated. Probably the precise form of the critical droplet is complicated and it is probably random. And because the Eddie's Rainy Grande graph, it's random. So, what is really going on? Really, going on here? Is this the best that one can do, or can one actually do something better? And in fact, one can do something better, and this was the result of Anton and Saida and Elena. They proved that if you look at the quotient of the Erdős-Rainy random graph crossing metastable crossover time, and that the Stable crossover time and that of the Curie Weiss graph, it is actually tight. So the pre-factor is not tending to infinity as n tends to infinity, but it is stochastically tight under the law of the Erdős-Rainy random graph. And they were able to quantify this by saying, well, if I have there are certain constants, and if I want this thing to be sandwiched between e to the plus s and e to the plus plus plus plus plus plus Between e to the plus s and e to the minus s with appropriate constants, then the probability that it's within that window goes to one sort of like a Gaussian. So it was a clear estimate of what this prefector could do. And it comes with absolute constants, and there are certain constants that are there that are not so easy to compute, but they're nevertheless. Nevertheless, computable. And so, this theorem is a considerable sharpening of what Oliver and I did, because it shows that this pre-factor is tight. It's not going to move to infinity as n tends to infinity. And apparently, it's random. And there are some, with this, there's some control. There's with this, there's some control on the tails of that random variable. So it may very well be the case that this quotient is simply going to converge to a random variable whose law we do not know, but at least we have some bounds on the tightness. So this is a big step forward in really understanding this result. Also, this Also, this stronger result comes with a little price. The magnetic field has to be taken small enough, and you must let your dynamics start in a particular initial distribution, which has a deep reason in potential theory. It's called the lost exit by distribution. And then formulas become a little bit nicer to work with. with and and you can you can apply your your computations a bit more sharply and so this was so this is now a very interesting situation where we are beginning to understand something more about this prefector and an interesting challenge would be try and prove that under the law of the Erdős Rainy random graph this is Any random graph, this is going to converge to a limiting random variable. And can you say something about the distribution of this limiting random variable? So that is music for the future. So a really hard thing to do, but also interesting to see that this pre-factor in the Erdős-Rainy random graph, which is still a very Random graph, which is still a very simple random graph model, appears to be again a very delicate object of which we have some control by looking at the geometry. But now it's not a matter of energy, it's energy and entropy. And understanding the full glory of this critical set is something that we, at this moment of time, are not yet quite able to do. To do so, those are the two results, and I'd like to say a few words about what it takes to derive these theorems. And this is interesting because it has been a challenge for both groups to really do that. So, Antona, Saida, and Elena have really. And Elena have really followed the classical route of potential theory. They started from the idea that a large Erdus-Raini random graph is probably very homogeneous. So they said, why are we not going to use the Dirichlet and the Thomson principle by inserting the same test functions as we have for Curie Weiss? As we have for Curie Weiss, corrected for this change in the interaction J going from 1 over n to 1 over Pn. And then let's see if we do that computation, do we get matching upper and lower bounds? And does that work? And along the way, you have to still fight with the non-homogeneity of your Erdős-Rainy random graph, but your test functions and test flows are simpler. I'm making a long Are simpler. I'm making a long story much easier than it actually is, but that is sort of the core of their argument. So they have to work with concentration estimates, capacity estimates, etc. So for all of us, there's a couple of questions. Good moment, yeah. So there's questions about can you actually get convergence as opposed to tightness? I guess you can get. To tightness, I guess you can get weak convergence of a sub-sequence. Yeah, yeah, yeah. At the moment, it doesn't go beyond tightness. We do not have convergence, but you would have convergence along sub-sequences, but no control of what these sub-sequences would be. Yeah, Nyogesh is asking about exponential laws. Do you expect them to exist? Exponential. Yeah, that's again a good question. Like for Kawasaki dynamics, I said I didn't mention. Psychiatrynamics, I said I didn't mention that, but indeed there's always the exponential distribution in the background, and we can prove that. This is a very universal law. It applies in most metastable situations also here, even though I didn't mention it. You must do a little work here because the set that you start out with is not a singleton. And so some work is being done, and also Anton and Saida and And Saida and Elena are working on certain features of that currently. Okay. Thank you. Okay, now Oliver and I had a different route. We wanted to try something without playing with the Dirichlet and the Thompson principle. What we did is that we said, well, there is this non-Markovian magnetization, but we're Magnetization, but we're going to try and sandwich it between two Curieweis dynamics in which I perturb the magnetic field a little bit. So we were able to show that if you make the magnetic field a little bigger or a little smaller, and how much you do will go to zero as n goes to infinity. Then we were able through coupling techniques and capacity estimates and concentration estimates and everything. Estimates and everything able to show that the empirical magnetization on the Erdős-Rainy graph is sandwiched very narrowly between what the magnetization is doing on two perturbed versions of the Q-device model. And that led to a sharp description of certain things. But with the perturbation, we also lost something, and therefore our And therefore, our pre-factor is much cruder. But for instance, this easily allows you to prove this exponential law for the metastable crossover. And there were some lovely things going on, coarse graining techniques, isoparametric inequalities all over the place. It is a technically rather demanding story, but what comes out are two lovely, nice theorems. Out are two lovely, nice theorems that are you know cheeky to look at. And I have no chance of really going into the story. But the essence behind all of this is that because the Erdős-Weini random graph is in the dense regime, there are many, many edges still, it is very homogeneous. And this high level of homogeneity plays a key role in really. A key role in really being able to understand what is going on. That you say it is really very much like a queervice model with some fluctuations that you need to control, and that is hard work. And that's why you can really compare the metastable crossover for these two models so well. And that lies at the heart of everything here. So there is homogeneization really playing an important. Really playing an important role. Okay, I want to wrap up with a few more slides in which I want to very quickly show you some other models of similar type that people have looked at related to random graphs. And And then we will be done. The very first is where I do everything on the complete graph, but I make my random field, I make my external field random. So I'm putting the randomness, let's say, on the vertices rather than on the edges. So the interaction is everywhere, but I'm assigning ID random variables to the vertices. To the vertices, and then I run my dynamics. And this model, which is quite challenging to do, has been looked at in two series of papers. Here in the original paper, they took a case where nu is discrete. And then what was lovely is that they could still use a form of lumping because they were just keeping track of where this magnetic field takes. Field takes, let's say, a finite number of discrete values, maybe h can be two and three. And then you are working not with an empirical magnetization, which is a scalar, but with an empirical magnetization, which is a vector. And so you're doing still some form of lumping, and that allows you to do various computations. And so you can still use the So, you can still use the lumping, and it turns out that you find something like you find in the Curie-Weiss model, but with a pre-factor that is really rather a complicated function of this probability distribution. And there was a paper by Bianchi, Bovier, and Jofe, two papers, where they then said, okay, if this has a continuous distribution, this magnetic random variables. Random variables, then well, the story becomes more complicated and you need to do a lot of work to get there. But this is again a success story in that case that people, you know, here you can deal with it because you can still use the lumping techniques. And that is very different from the model that we've been talking about before, because there there is no lumping. I mean, the fact that you have these random edges and a random interaction introduces. Random interaction introduces an intricate spatial structure that you have to fight with. And this model here is, a lot of progress has been done, and it was a big fight to do that, especially the continuous distribution case. But you can fortunately still use lumping techniques in some form or other. So, what else? So, what else has been done? Well, we have looked at metastability in the sparse regime. So graphs where the degrees are not proportional to the number of vertices, but are stochastically bounded. And there's a very famous model called the configuration model, which generates a random graph with the prescribed degree sequences. Many of you will have seen this. Many of you will have seen this. You start with n vertices, you prescribe degrees, and you put little half-edges out of them connecting up with exactly the number of degrees that you picked. And then you start to randomly pair these edges and you generate a graph. And we have looked at what happens when you do Glauber spin-flip dynamics on this graph. On this graph. And here we were looking at n going to infinity and sorry, and beta going to infinity. And there was a first paper by Sandor Domas who started to look at this problem and doing some rough estimates. And then over time, we were able to say something about this. And it's very challenging in the sparse regime. There are many things that we do not know. We will again see. We will again see something of the form that we have a three-factor and an exponential term, but it turns out to be quite difficult to get your hands on these terms. And these terms are, again, also very random because they depend on the realization of the graph. And in the sparse regime, there's nothing homogeneous. The homogeneization is not going to work for you. So it seems. And you can even do Kawasaki dynamics. even do Kawasaki dynamics on random graphs of this type. And there are a couple of papers where we have looked at this on random bipartite graphs, so that you have two classes of vertices, the blue ones and the red ones, and the blue ones only connect up with the red ones, but the red ones do not connect up and the blue ones do not connect up. So a kind of two community graph. And in that case, And in that case, it's possible to also understand metastability phenomena. And this involves Siamak Tati, Francesca Nardi, and Sam Borst and Matteos Fragara. This is also an interesting thing where we are beginning to, you know, eat our way into this topic, but it is in general. Topic, but it is in general quite challenging to really compute the fine details of what is going on. So, here is my final slide with a take-home message. It should be clear from what I've been explaining that prefectors of average meter stable crossover times are delicate objects for random graphs. For one reason, they are random. For one reason is that they are random themselves. And the other reason is that they depend in an intricate manner on the underlying geometry of the graph. And if the graph is very homogeneous, we still have good control of it. If the graph is not very homogeneous, like in a sparse graph, we know much less about it. And so at that level, we are still struggling very much with what is going on. What is going on? If you take the crossover time divided by it mean, that will have an exponential distribution. That is a very robust law that we are able to prove in many, many cases. The delicate part sits in really controlling this prefector and understanding what is really the geometry that matters, and can we understand that? We understand that. And I think this area is still very much at the beginning. It's been around only for a few years, and there's really a lot that remains to be done. So with that, I close and I thank you for your attention. Thanks, Frank, for another great lecture. So I think Sarah is going to let us unmute ourselves. And I hope you join me in thanking Frank. Me and thanking Frank.