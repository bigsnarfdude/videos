So when I was invited to talk in this, I saw the topic of complex data with missing miss measurement errors and high dimensionality. And so what I'm going to talk about is missing data. And it's an emerging challenge only in that the political landscape in the United States and in other countries has changed and sort of created emerging challenges for pollsters. So this is a very different kind of talk, I think, than what we just saw from Ed, very applied. So I'm going to start. So, I'm going to start with the problem statement. What's the issue here? So, the issue is that in the United States and in other countries, but especially in the United States, pre-election polling has had some negative press. So, I just have a couple of cartoons that illustrate this. These are not actually all that recent of cartoons. Here's one where the poll says four in five don't trust Washington, and but the survey says nine in 10 don't trust polls. We've also got the science of polling, where the media polls are some sort of magic brew. Media polls are some sort of magic brew made up by Gallup and ABC News, Washington Post, right? People are very, beginning to be very distrustful of polling and where these numbers come from. And all this results in this huge headache for pollsters, right? So I just came from the American Association of Public Opinion Research Conference, where there are a lot of people who work in polling, and it is true they all have headaches now because they can't figure out why they're having so much trouble correctly sort of predicting. Much trouble correctly sort of predicting the results of political polls in the United States. So, in 2020, the U.S. presidential polls had the highest error in 40 years, and it's widely considered to be a failure of polling. And interestingly, there were failures of polling in 2016. So that was the election where Donald Trump became president. But the problems they identified from 2016 do not appear to be the problems for 2020. So, two key things here. Number one, in 2016, Here. Number one, in 2016, there was a lot of late deciders, right? People who didn't decide if they were going to vote for Trump or Clinton until they read the ballot box, or they maybe responded to the poll one way and changed their mind. So this wasn't really a problem in 2020 because of the worldwide pandemic. We had a lot more early voting in the United States. Normally, that's a very small amount of people. So people were responding to this poll and they'd actually already voted. So there wasn't any changing of the minds for a lot of people. And then, secondly, there in 2016, when Wayne In 2016, when weighting these polls, the majority of the polls did not take education into account. And you can imagine that education is associated with who you voted for, when it was Trump versus Clinton or Trump versus Biden. And so in 2020, the pollsters all said, well, we're going to adjust for education. So that was done in 2020, but we still have these huge polling misses. So for those of you who don't know very much about political polls, and I didn't before I started this. And I did it before I started this project. Typical polls are probability samples, but they have extraordinarily low response rates. We're talking single digits, right? Four and a half to six and a half percent response. And of course, they do weighting adjustments. They don't just take that as a non-probability sample and run with it. But the weighting adjustments assume that selection or response into the poll is at random conditional on those variables used to compute the weights. So, for example, including conditional on education in the 2020 polls. In the 2020 polls. But in 2020, maybe Trump supporters were less likely to answer a poll, even conditional on demographic characteristics. So if you took two people who otherwise looked the same and one of them was a Biden supporter, one of them was a Trump supporter, we could hypothesize that the Trump supporter is less likely to answer the poll. The proliferation of via Trump of fake news and the left-wing media, et cetera. So, this last cartoon I'm going to show you is essentially. Cartoon I'm going to show you is essentially exactly the problem. Here, the pollster says, Should I put you genuinely undecided or damned if I'm telling you undecided? So we have this inkling that this may be what's going on as people with a certain political preference are less likely to answer the poll or tell the truth to the pollster. So there was a task force put together by the American Association for Public Opinion Research to look at the 2020 polling and to see what went wrong. And one of their conclusions was that the Democrats and Republicans who That the Democrats and Republicans who responded to the poll had different opinions than those who did not within party non-response. So, this is exactly an issue of non-ignorable missing data or sample selection. So it didn't, it maybe the demographic characteristics predicted whether you would answer, but also who you were going to vote for predicted whether you were going to answer this election poll. So, in a nutshell, the problem is we want to estimate a population proportion from a non-probability sample or a probability. probability sample or a probability sample with such a low response weight that we essentially feel like it's a non-probability sample. And the proportion we're interested in is the proportion voting for Trump in 2020. The problem is that we have this potential for selection bias due to non-ignorable selection or non-response mechanisms. And I'm sure majority of everybody on this call knows this, but ignorable means the probability of selection depends on observed characteristics, such as the demographics, and non-ignorable is where selection depends on unobserved characteristics. So the response to the poll. Characteristics. So, the response to the poll might depend on your answer to the poll, which we don't see if you don't answer the poll. So, the approach that we took is to use a model-based index of selection bias called the MUBP, which I'll tell you about in detail, that allows you to assess the potential for selection bias in proportion estimates. So it's a sensitivity analysis. It's basically saying, compared to our estimate when we assume ignorability, how bad might our estimate be if, in fact, response to the poll depended on your underlying. Poll depended on your underlying candidate preference. So the notation is going to be Y for the survey data. So that's the binary outcome variable. And we'll use subscript and X for included and excluded from the sample. Z is a set of fully observed auxiliary or design variables, the types of things that you're going to use for weighting, for example. And S is the selection indicator. So of course, we're interested in the distribution of Y conditional on Z, but we have this selection mechanism. But we have this selection mechanism muddying up the waters here, right? The joint distribution of Y and S is a product of these two distributions. So, when we think about probability sampling, probability sampling is an extremely ignorable selection, obviously putting aside informative designs, uninformative designs. So, selection into a probability sample can depend on those Z's, right, those auxiliary variables, but not Y. Inclusion in the sample is independent of Y and any unobserved variable. Of y and any unobserved variables. And so you can pretend S isn't a thing, or you can ignore the distribution of S when you're trying to do inference. But this is only the case if there is no non-response. And we all know, I'm pretty sure, no matter where we work in what country, what state, what area, there's non-response, especially in polling. So non-probability sampling is where there might be non-ignorable selection. So selection into your sample might depend on something you don't observe. On something you don't observe. And in particular, we're concerned about the case when it might observe on that binary outcome variable. It is very challenging to model S, and so, and although there's some literature on that, including combined with other samples. So what we're going to do is try to quantify that potential selection bias that is arising if we assume ignorability by making assumptions about S, assuming that it does depend on Y in some way. So there's been previous work in this area. Previous work in this area. So, many people on this Zoom are probably familiar with the R indicator, which is a function of response propensities. It's basically how representative is your sample, but that ignores Y, right? So that's just about the Zs, basically. There's a H1 indicator that does take Y into account, but now they are making an assumption of ignorable selection mechanism. Again, here, we really want to be in the case where we believe truly that the underlying mechanism is probably. Mechanism is probably non-ignorable. And then more recently, there was the SNUB, they call it the SMUB, which is exactly what we want. It's an index allowing for non-ignorable selection. We make assumptions about S. It gives you a range of selection bias, but it's for continuous Y. So we could just apply that to the binary indicator, but we thought we can do better. And we have simulations I'm not going to show you that we do much better, get much tighter bounds. So we want the SMUB, but for proportions. But for proportions. So, in order to show you how this works, I'm actually not going to use polling. I'm going to use a dummy example where a toy example where I have the full data so that I can show you the truth and how this works. So pretend we have this population. It's about 19, 20,000 people. It's from the National Survey of Family Growth. So we're just taking this full sample and pretending it's a population. And now we're going to pretend that the selected sample is all the smartphone users. So you can imagine that we might be. So, you can imagine that we might be conducting some sort of a survey where you have to have a smartphone to be participating. So, we're interested in how might that bias our estimates. So, in this particular instance, this is a relatively recently recent survey. So, a lot of people have smartphones, about 80%. That is an atypical selection fraction for a non-probability sample, usually much smaller. And the outcome of interest we've pulled from this is just whether or not you've ever been married. And this is broken down by gender for males and females. And I will note that the NSFG only captures gender as a NSFG only captures gender as a binary variable, even though we understand it to be a much more complex construct. So we know the true selection bias. These are real data that I have taken and sort of impose my selected sample upon. And so I actually observe everybody's marital status, but I'm going to pretend that I just have this selected sample. So in the full population, for example, for females, 46.8% of them have ever been married. And in the selected sample, it's only 46.6%. But that's a really, really tiny, tiny. Really, really tiny, tiny difference, right? There's not a lot of bias in estimating the proportion never married. And for males, it's a slightly higher bias, about 1%, but it's very, very close. With binary outcomes, a popular method of looking at bounding of bias is to assume everybody you don't observe is a zero or everybody you don't observe is a one. And these are called the Mansky bounds. This is often done in studies of smoking cessation where you assume everybody you don't see is a smoker and everybody you don't see is a non-smoker, right? You don't see as a non-smoker, right? Because those are the only possible values for everybody who's not in your sample. And the Manske bounds in this case are actually relatively tight for the bounds of the bias because we've only got about 20% of the sample not or of the population not in our sample. But in election polling, you take a tiny, tiny sample, right? A thousand people to try to generalize to millions of people. So the Manske balance are useless. So we're trying to do better than the Manske balance. So we have micro data for the selected. So we have micro data for the selected cases. In the NSFG example, we have that outcome why, yes, no, where they never married, and the auxiliary variables, typical demographic information. In the polling example, why is your indicator of voting for Trump? Yes, no, and it's from the poll. For the non-selected cases, so the cases who are not in the poll or who are not in my smartphone sample, we're going to assume we have summary statistics only on Z. So we have a mean vector and a variance matrix. So we do have. Matrix. So we do have to know something about who is not in our sample in order to make any kind of an educated assessment of bias. In practice, we could get this from a census or a large probability sample. In the cases of small sampling fractions, we could basically just use an estimate for the full population and say that's our non-selected sample if we have, you know, if we have a really small selection fraction. So now the development of the actual indicator or index MUDP. So this is. Index MUBP. So this is an extension of the SMUB for means to binary Y. It's based on pattern mixture models, and it makes an explicit assumption about the distribution of S. So even though it's based on pattern mixture models, we kind of take a selection model approach in thinking about what the true distribution of S might be in order to identify the models. So it's going to provide a sensitivity analysis to assess the range of bias under different assumptions about S. So here's the basic idea. We can measure Basic idea: We can measure the degree of selection bias in Z, right? Because we have Z observed for everybody in our sample, so we could say, let's say the mean of Z in our selected sample, and we can compare that to the mean of Z in the non-selected sample. And that tells us something about selection bias with respect to Z, right? So we can observe that. If our outcome Y is correlated with Z, then that tells me something about the selection bias in Y. Imagine if you have a predictor Z, some demonstration. If you have a predictor z, some demographic variable that is just super highly correlated with y. Well, if you see a lot of bias in z, then there's probably bias in y, right? Or very little bias in z, probably very little bias in y. Unfortunately, the reverse is also true. If y is not correlated with z, then seeing bias in z doesn't really tell you much about y. It's very uninformative. We need those correlated predictors in order to kind of tighten our estimates of selection bias. So we use selection. So we use selection, we use pattern mixture models to explicitly model the non-ignorable selection, right? Selection dependent on Y. So now into the theory. Y is our binary variable of interest, and it is only available for the selected sample. In the NSFG toy example, it's this yes, no, ever been married. Z, the auxiliary variables, it's almost always demographic data. And then in order to apply our method, we assume that there's an underlying normally distributed unobserved latent variable. Normally distributed unobserved latent variable u, right? So we're basically taking a probit model approach instead of the standard logistic regression approach because of the convenience of the multivariate normal that we're going to use here. So the outcome y takes the value one when u crosses the threshold greater than zero. This method is based on what in previous work we've called the proxy pattern mixture model, and it's because we create a proxy for y by collapsing our multi-dimensional z down into a single x. Into a single X. And the way we do this is with a probit regression. So we take the selected cases, we regress Y on all the Z's, and then we basically get like a Y hat, right? It's a probit regression, so it's not actually a Y hat, it's more like a U hat, right? You can imagine that you're just plugging in the values of Z for everybody with those beta hats from the probit regression, and you can get one X. So instead of a whole row of Z or whole set of columns of Z, you have a single X. And this is available for the selected cases, right? Every selected case I can. Selected cases, right? Every selected case, I can calculate their x value. And for the non-selected sample, I can actually calculate the mean of x, because I could take that predicted equation from the probate regression and plug in z bar, right? So I would be able to get the mean of x. I don't have individual x observations on the non-selected, but I do have the average. And then in addition, we have s, the selection indicator, and a set of other covariates, v, that are independent of y and x, but can be related to selection. But can be related to selection. So that would be something that is associated with whether or not you are in this sample, but is not related to y or x. So we assume a proxy pattern mixture model for the joint distribution of u and x. Remember, u is that latent variable and x is the proxy given s. So here we have a bivariate normal distribution for u and x, separate for the selected sample and the non-selected sample. So the superscript J here takes the values 0 or 1 for the non-selected and One for the non-selected and selected sample. This model, however, is clearly under-identified because for the people not in or for the units not in the sample, I don't observe anything about U for them, right? There's I have no information about U. So we assume we make some assumptions on the distribution of S that actually allows us to kind of back into identifiability. So the way that we identify this model is the standard method for pattern mixture models. We assume that selection into the sample is a function of V. Sample is a function of v that was the associated with s, but nothing else, variables, and a linear combination of x and u. So it looks a little funny written out here, but here we have the probability of selection. So being in the sample, being a smartphone user, for example, is some a function of v, that everything else variables, and also a linear combination of that proxy variable, that's the collapsed z's, and u, that's the latent variable underlying y. It's x star here just because we. It's x star here just because we've rescaled the proxy. So mathematical convenience. And you'll notice we have this phi floating around in here. So phi is a sensitivity parameter, no information in the data about it. So we're just going to have to pick a value for phi. And that's going, and with this assumption on the selection mechanism, it's going to identify the unidentified parameters in the pattern mixture model. So without loss of generality, this is a latent variable. We can set its scale to be one. And what are we interested in ultimately? We don't really care about you. interested in ultimately, we don't really care about you, we care about y. We're interested in the marginal mean of y. So the marginal mean of y is just a weighted average of the selected samples proportion and the non-selected samples proportion. So the key parameter in this joint distribution here is actually this row. It's the correlation between u and x in the selected sample and in the non-selected sample. And in particular, that is actually called the biserial correlation between the binary y and the continuous x. And the continuous X. And so this quantifies basically how strong is your model in predicting Y, right? Because it comes from creating the proxy. So you can estimate that correlation in the selected sample, and that's an important component of this model. If that is small, we're going to have a hard time bounding our bias. So the non-identifiable parameters of the pattern mixture model are these three, right? The mean of u for the non-selected, its variance, and the correlation between u and x. These are just Between u and x. These are just identified by an assumption on the selection mechanism that I already stated. So the selected value of the sensitivity parameter determines that selection mechanism. So if phi is zero, then this component that involves u is going to go away, and it's only going to be a function of x and v. So if selection is only a function of x, which remember came from the z's, which are fully observed, well, this is ignorable selection. Well, this is ignorable selection. There's no U floating around in here or any Y. So, this is the estimate we get from a weighting adjustment is going to be roughly the same as the estimate we're going to get out of this model when we assume phi is zero. On the other hand, if phi is one, phi is the range between zero and one, now we see that it's only dependent on u because the x star part drops out. So now we end up in this really, really bad scenario where selection into the sample. Where selection into the sample only depends on that unobserved characteristic that you're trying to measure. It is actually U, not Y, but since Y is dependent on thresholding U, it's dependent on Y. So this is an extremely non-ignormal selection, and we don't really believe this to be the truth. We know there are some demographic characteristics that predict, for example, selection into or answering a poll, right? Or even just responding to any survey. We know who tends to respond. But so this is a very unlikely to be the truth. Very unlikely to be the truth, but it's a bound, right? Sort of a worst case bound. Somewhere in the middle is probably where the truth lies, especially for that polling example. We're going to have some of those Z's and therefore the X that is determining your selection into the sample, whether you respond or not, but it's also going to be U, right? And therefore, Y. So for a specified value of the sensitivity parameter, we can estimate that overall mean of Y and compare it to the selected sample to Compare it to the selected sample to obtain what we call the NUDP for measure of unadjusted selection bias for a proportion. And we call the unadjusted selection bias because we're just comparing it to the raw mean in the selected sample, not any kind of weighting adjustment that would kind of try to get away or deal with the ignorable component. So, in a nutshell, we choose a selection mechanism by specifying that value of that sensitivity parameter between zero and one. We estimate the overall proportion. We estimate the overall proportion conditional on that value based on the pattern mixture model, and then we estimate the selection bias as the difference between what we see in the selected sample and our estimate from this model. So the formula for the MUVP is messy, but it helps you to see kind of how this index is working. So basically, as I said, we see Z, and therefore we see at least the mean of X in both the selected sample and the non-selected sample because we have the aggregate Z in the non-selected sample. We have the aggregate z in the non-selected sample. So, in order to come up with a mean for u for the non-selected sample, whoops, I just made my own screen very small, we're going to take the selected sample mean of u and shift it. And we're going to shift it based on, if you look over here, what's in red, how far away is the selected sample mean of x from the non-selected mean of x? If there's a lot of bias in your z's, those ones that are related to y, Those ones that are related to y, then we're going to do more shifting. And it's scaled by a standard deviation. How much it's shifted is where phi comes in. So this multiplicative factor out front here depends on the value of the sensitivity parameter and also on that correlation. So that correlation piece is very important. And a similar thing is happening with the variances, right? We're shifting this unknown non-selected sample variance by start with the selected sample variance. By start with the selected sample variance, which remember is one because it's a latent variable, and we're just shifting it by how far we're off in x. The biserial correlation comes in here, and if you were to work through the math and plug in the sensitivity parameter equals zero, you would actually just be left with this correlation here. And now it's a regression estimator, your standard, standard, ignorable estimator. If, on the other hand, you plug in one, you actually get one over that correlation. So that's where you can see that if the So that's where you can see that if the correlation is really small, there's going to be this big multiplicative factor, right? One over a small number is going to balloon out this estimate, and we're not going to be able to put very tight bounds on this. So how do we estimate this? Well, we do what we call modified maximum likelihood estimation because it's not true maximum likelihood because of this biserial correlation. So if you do true maximum likelihood estimation, your estimate of the mean of U will not actually return back to you the observed. not actually return back to you the observed mean of y in the selected sample. So instead we do this two-step method where you first estimate the selected sample mean of u based directly from the mean of y, which you observe in the selected sample, and then conditional on that, estimate the biserial correlation. Everything else is standard maximum likelihood. And we suggest a sensitivity analysis from 0, 0.5, and 1 to look at the range of potential problems. Alternatively, and this is what we Alternatively, and this is what we did for the polling data, you could be Bayesian. So we put non-informative priors on the identified parameters. The advantage of this approach is that the probit regression that creates the proxy in the MML estimation, we're treating that as the truth. But of course, there's error on those beta hats. And so being Bayesian allows us to incorporate that uncertainty. There's no information in the data about the sensitivity parameter. So we take it just to be uniform over its entire space. Other priors are possible. Entire space. Other priors are possible. We've looked at kind of point priors on 0, 0.5, and 1, and other creative options are possible. So let's look at what this, what the results are if we apply this to this NSFG example, where we're trying to estimate the proportion never married. So here we have on the left, it's for females, and on the right, it's for males. And this is actually plotting the bias, right? And remember, the bias was very, very small here, the true bias. So this is actually the index times a thousand. So the true bias is the black. So, the true bias is a black dot, and the two different colors here are the maximum, the modified maximum likelihood estimation versus Bayes. So, first of all, right away we see Bayes has wider intervals, and that's sort of truer because we're incorporating the uncertainty in what creates the proxy. The little diamond here is the MEBP of a half, right? So it's going to be the midpoint between zero and one. And we notice that we are able to successfully cover the true bias. Cover the true bias in this case. If we plot the Manske bounds here, remember the Manske bounds are not actually that wide here because we only have 20% of the sample not selected, but compared to the Manske bounds, our bounds are very, very tight. So we're doing better than plugging in all zeros and all ones, right? We've got much, much shorter intervals. In this case, when we're trying to predict whether or not an individual has ever been married, we have really good predictors of y in the selected sample. That biserial correlation is 0.7, 0.8. Real correlation is 0.7, 0.8. So we actually can get real tight intervals. If instead we look at a y that doesn't have great predictors, and in the NSFG, one of those is income. So this is an indicator for being low income. Using the same set of auxiliary Zs, we actually have a very weak model for low income. And that biserial correlation is 0.17. So very, very small. That's for females. So if I apply the MUBP model and then also look at the Manske bounds. And then also look at the Manske bounds at the limit, we get those Manske bounds from our method. So we're being bounded by plugging in all zeros or all ones, right? And in this case, it's all zeros. So this is illustrating an example of why this is a preferred method over just going ahead and using the normal model, the model that assumes y is normal, because if we use that, we don't get that natural bound. In this case, the bounds would just be completely enormous and ridiculous because you'd basically be returning a proportion outside of zero and one. On one. So now let's go back to the interesting application, which is the presidential polls. So that's a reminder: we've got this failure of political polling. We have a probability sample, but is it really? Because it's got a very, very single digit low response rate. Weighting adjustments are done, of course, and they assume selection is at random, conditional on those variables used to compute the weights, but we don't really believe that that is the case, right? We believe that Trump supporters are less. That Trump supporters are less likely to answer a poll, even when we condition on the demographic characteristics that we have. So we can use the MUBP to adjust the poll estimates, right? So we can use it to estimate the potential bias, and we can also use it to shift those poll estimates to say, well, that's the estimate, assume ignorability. What if we assume these varying degrees of non-ignorability? So, the data sources. We're interested in estimating the percentage voting for Trump. Voting for Trump. As a sample, we used seven separate publicly available data sets from pre-election polls in seven swing states conducted by the ABC Washington Post in 2020. So there's been a move towards a little bit more transparency in polling. And so some of these polls are starting to become publicly available. Prior to now, this was basically never done. It was all very, very secretive. So these are RDD surveys, very low response rates, as I've already said many times. And we're using these weighting adjustments. Times and we're using these weighting adjustments to census margins. I will note that they do incorporate a party ID weighting adjustment, and this is a complex thing that involves itself involves modeling, but we're just going to treat that as, yay, they have this information on party ID for the individuals who respond to the survey, I mean, to the poll. And that's good because as you can imagine, party ID is associated with who you're going to vote for. And remember, we need strong predictors in order to be able to get tight bounds. In this case, we do. In this case, we do, when we're looking at polling data after the fact, we do have the true official election outcomes in each state. So we know the percentage who actually did vote for Trump. So we can compare what we get using our model to what the truth is. The tricky part here is the population. So the population is likely voters. I'm sure you've heard that phrase if you've ever sort of read or listened to anything about polls, even if you're not a pollster. The tricky challenge is, how do you figure out who is a likely voter? Figure out who is a likely voter? How do you get that population-level summary of likely voter characteristics for the non-selected cases, but really it's for the population because we select such a tiny, tiny, tiny fraction? So if you think about a likely voter, that is a malleable characteristic. And so someone who votes in one election might not necessarily vote for the other election. Who is running in the election could change how likely someone is to be a voter, the political climate, other things going on in the world, availability. Things going on in the world, availability of early voters, all manner of things, and it changes. So, this is an unstable population, and it's really hard to find population summary of who, right, demographically, for example, who are the likely voters. So, we had to look at a lot of different data sources to try to figure out how can we estimate that mean of Z, right, the means of the auxiliary variables for the likely voters, for all likely voters, really the non-selected sample. So, we looked at the 2020 CPS voters. At the 2020 CPS voter supplements, we looked at the 2020 American National Election Studies pre-election survey, and we looked at a survey called the AP Nork Vote Cast. So ultimately, none of these were really great sources. So the first two, the CPS and the ANES, did not have some of the highly relevant party preference variables that are helpful for our model, right? To give us that high biserial correlation. The AP Nork vodcast did, but it's not actually. Did, but it's not actually data that's available before the election. So we really wanted something that you could use as a tool before the election so that we could say, hey, do this in 2024, right? Or do this for the midterms. But none of these data sources were good. And it kind of makes sense, right? Nobody's making a living going around just on a yearly basis, trying to estimate who might be a voter, right? So we decided to use the AP Nork vote cast data. So what we're doing is effectively a post-mortem on the poll results. We're basically saying, this is what the poll is. Basically, saying this is what the polls showed, this is the truth. Does our model explain that discrepancy? Right, might non-ignorable selection or non-ignorable non-response partially explain the poor performance of the polls, right? Because APOR is the organization is trying to figure out why is polling doing so poorly, and we hypothesize this is one of the reasons. So we have Y, the indicator for voting for Trump. Z is the auxiliary data available in the ABC Washington Post poll. Binary gender, again, acknowledging gender is a more complex. Again, acknowledging gender is a more complex construct, but pollsters have not caught up to that yet, and we just have a binary gender, age, education, race, political ideation, and party identification. So, as I hinted at, these are strong predictors of why. Our biserial correlations across the seven states range from 0.8 to 0.86. Very, very strong models in the selected sample predicting whether or not someone voted for Trump. We take the AP Nort vote cast data and get population level estimates of the mean OZ. Estimates of the mean of Z. We do note that the AP Nork vote cast data is not some miracle source, it is itself a survey. And in fact, it actually combines probability and non-probability sampling. There's definitely error, but we're going to treat Z, the mean of Z that comes out of this, as if it were the truth. So that's a whole other paper is how do we handle the fact that our population level estimates are actually adding additional uncertainty in here. So we're going to use the unweighted ABC sample as the selected sample. We're basically treating it as if it were an. Sample. So we're basically treating it as if it were a non-probability sample and then estimating the bias using the MUBP model with being Bayesian about it and putting a uniform prior on the sensitivity parameter. And then we can also actually just produce estimates of the proportion voting for Trump to compare to the poll's estimate by using the MEPP to shift the sample proportion. The polls selection fractions are really small. I already mentioned this. Manskey bounds are useless, right? I can't plug in all no's on Trump and all yeses on Trump to the million. On Trump and all yeses on Trump to the millions of people in the population. That's not going to give me anything informative. So on this slide, we see the true bias and the MUBP Bayes intervals for the seven states. So the true bias is the red circle. And I have them sorted in terms of sort of descending bias, going from where in Arizona, there was actually a little bit of overestimation of the support for Trump, all the way down to Wisconsin, where something went horribly wrong. Where something went horribly wrong, and they very vastly underestimated the support for candidate Trump. So the intervals shown are the intervals from our method, right? The Bayesian intervals. So sort of assuming some kind of moderate, non-ignorable selection mechanism because we put that uniform prior on the sensitivity parameter. And in almost all of the cases, we are able to cover the true bias, except in Pennsylvania and Wisconsin. So, two different things are happening. It. So, two different things are happening here. Both of them, we've got a big negative bias. But in Pennsylvania, our method is suggesting that that's not because of non-ignorable selection. Right? We are definitely, this is pretty much straddling the zero bias. And we're sort of saying, yeah, there's not much of a problem with non-ignorable selection, but clearly there's something going on because we've got a big negative bias. And in Wisconsin, huge, unheard of negative bias, but at least our method is suggesting that there is some negative bias, right? Our interval, our credible interval does not. Our interval, our credible interval, does not include zero bias. So we would look at this. If we could do this before the election without the red dots, we would look at Wisconsin and say, hold up, something is going on in Wisconsin. We have some evidence that if we had a non-ignorable response to this poll, that we would be underestimating the support for Trump. And the same is true in Minnesota, right? That does not cross the zero. So if we were able to see this before, we would say, oh, we have some evidence that if there's not a normal selection bias. There's not a normal selection bias, then in fact, we're underestimating the support for this candidate. So, we can also, of course, take these estimates, use them to adjust the estimate of the support for Trump. And that's what's going on here for each of the seven states in the same sort order. We have the unweighted estimate from the poll, which, of course, nobody would ever actually use, right? Of course, they're not going to use the unweighted estimate. You're going to use the weighted estimate. In the middle is the weighted estimate, and then the MUBP. Weighted estimate, and then the MUBP adjusted, right? So, our estimate adjusted for non-ignorable selection bias is on the far right. In each panel, the red triangle is the true proportion who voted for Trump in that state, and the black dot is either the point estimate or the median of the posterior estimates. So, most of the time, we're all the intervals are kind of covering the truth. It's not really all that interesting. These are relatively wide intervals. Remember, these polls actually have pretty small samples. These polls actually have pretty small samples on the order of a thousand or fewer people, but interesting to look at what's happening in Minnesota, Pennsylvania, and Wisconsin. So, in Minnesota, we note that our adjusted estimator is much, much closer to the truth. And the same is true in Wisconsin. It's just we don't quite get there with our credible interval. Of course, we didn't cover the bias and obviously not covering the true proportion, but at least we are very suggestive of there being a serious problem with non-ignorability in Wisconsin. Problem with non-ignorability in Wisconsin, or non-ignorability contributing to the problem, I should say, in Wisconsin. In Pennsylvania, Pennsylvania is our failure, right? Pennsylvania is the state that we're cursing because in Pennsylvania, there's definitely some kind of a negative bias, and the polls were underestimating the support for Trump, but we are not seeing that reflected in our method. Not ignorable answering of this poll does not seem to be causing that underestimate. And in fact, we're sort of doing worse. And in fact, we're sort of doing worse, we're further from the truth than the weighted estimate from the poll, right? And most of the other cases were closer, right? Most of the other cases were closer, but not in Pennsylvania. So in summary, the MEBP did correctly detect evidence of a negative selection bias in those two states, Minnesota and Wisconsin. It suggested negative bias in some other states. So if we look here, we could see that, you know, there's suggested that there might be some negative bias, but I'm not definitely not an Arizona. Bias, but I'm not definitely not in Arizona or Florida. But zero was also in those credible intervals for the bias, so we can't really say we have strong evidence that there's bias. There was a huge polling miss in Wisconsin. Everybody's still trying to figure out what went wrong there. And I would say we're showing that there's some evidence that some of the problem may have been this non-ignorable selection bias, right? The Trump supporters not wanting to answer the poll and hanging up on the pollsters. And the MUBP moved that estimate towards the truth, it just didn't get. Towards the truth, it just didn't get all the way there. So, there's something else going on as well. The MUBP adjustment often brings us closer to the truth than the standard weighting estimate, right? So, further lending credence to the fact that there is some non-ignorable selection bias. And interestingly, the credible intervals for our adjusted method are actually narrower than the weighted. So we can see we get narrower intervals. And there's some argument here as to whether that's actually a good thing or not, right? I don't know that we really want to get super, super tight intervals because we're kind of trying to bound the potential. Because we're kind of trying to bound the potential for bias. And how tight those intervals are is related to the correlation. So the fact that we have these really strong predictors makes those credible intervals be narrower. One miss, the failure of this method is that it did not suggest any bias in Pennsylvania, but there clearly was negative bias about five percentage points for President Trump or former President Trump. So that's clearly a state where I would say something else is going on, maybe in addition to or instead of this non-ignorable selection. Instead of this non-ignorable selection, so a key takeaway message that I want everybody to remember is that you need quality information on the population. If you don't have Z for the population in any kind of credible manner, then you can't really talk about selection bias because you have nothing to compare to. The only thing you could do is plug in all zeros and all ones, which is useless in polling. So, as a real quick summary, the MEB. Two minutes. Sorry, we're good. I'm just about done. I know everybody in on the other. Everybody on the other coast is trying to get to lunch, so I don't want to eat into lunch for everybody. So, the MUBP provides a sensitivity analysis to assess this potential for non-normal selection bias, where we go from ignorable, phi equals zero, where you can adjust it away through weighting or other methods, or all the way on the other extreme, a one, something we don't believe is really ever the case, but it's that worst case scenario where selection entirely depends on the thing you're trying to measure. And then the MUBP of a half, which you could use as kind of a compromised estimate of the bias. And that's basically. Compromise estimate of the bias. And that's basically what we do when we put the uniform prior, right? The 50th percentile is the 0.5. It is an estimator that is tailored to binary outcomes. So it's an improvement over the normal-based method. And again, I mentioned this earlier. We have some simulations that show that you just get a drastic improvement in those intervals. You shrink the intervals and you respect the bounds of zero and one on the proportion. And it only requires summary statistics for your covariates Z for the non-selected. So you don't actually have to have micro data on the full population. Have micro data on the full population. You just have to have some kind of an estimate of the average of the covariates in the population. With weak predictive information, this will return the natural Manske balance, right? So if you have a really weak set of Z's that just don't predict your Y in the selected sample, you're basically going to get back the estimates of plug in a zero for everybody and plug in a one for everybody. So that's actually a desirable property. So we have some related work that extends this to selection bias for linear regression coefficients and For linear regression coefficients and probate regression coefficients. That's out in the Annals of Applied Statistics. And we have some future work that we're working on extending this to generalizability of randomized trials in the presence of unmeasured effect modifiers. That is it. Thank you. Thank you very much for your talk. There is a question in chat. Do you want to read? Yes, I'm looking at it. The past election outcomes versus poll prediction. So the question is: can we use so? Can we use so so? The likely vote, the information in the poll and in the in the external data sources to get Z incorporate prior behavior. So this likely voter indicator that is in the poll, which is then who they use to estimate the polling, the poll estimate of who's going to vote for Trump, actually incorporates past behavior. So they ask people to report: did you vote and who did you vote for? And they also ask about. And they also ask about how likely are you to vote. So there's a lot of information that goes into that, and it's in both data sources. So we're using it in that regard, but we're not somehow, so we're not somehow taking information on the people in the sample and actually explicitly like, who did you vote for and incorporating that additional data source? That would be, I mean, you could use it as a predictor in the probit model, but sort of a more