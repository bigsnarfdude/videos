Here. Yeah, welcome back, everyone. This is the last Zoom talk for today. There's going to be another presenter talk right after. And I'm happy to announce Craig Mueller's talk on reciprocity for evaluations of eta function. Thank you, Laura. And thank you to the organizers for having me. I'm very excited to be here, both in Oaxaca and with all of you fine folks. With all of you fine folks. And particularly talking about this story. This is a project I've been excited about for an embarrassing amount of time, maybe eight years. So I should say this is, I'm going to be reporting on work with Mandy Chung, Tim McGee, and Travis Mandel. And I should include a disclaimer. These are all preliminary results. None of this has appeared yet. We're working on it. And everything's been carefully written down except for a few things, which I'll try to signpost when I get there. When I get there, let me give you a quick outline of the plan. So, the title kind of gives away the plan. I'm going to tell you about theta functions, a class of objects you've already heard about, a little bit about valuations, a particular kind of valuation. And then I'm going to tell you about this sort of remarkable symmetry in the pairing between the two that I like to call theta reciprocity. I'll give you, I don't know, maybe five or so statements of theta reciprocity to try to hook you in. And then I'll close with a quick application that I think is pretty neat. Quick application that I think is pretty neat. So, why don't we get into it? So, you've already heard about theta functions. So, I'll give you a quick recap. I'll kind of blitz through this, but I'll kind of hit the bullet points literally and figuratively. So, I'll work in a little bit bigger generality than Ben. First, I will work in the skew symmetrizable case. I'll work with frozen variables. Ben's theory, I think, is fine with frozen variables. He just didn't mention them. And then I'll even work with something a little bit more general, even than that, called. Something a little bit more general, even than that, called seeds. I won't explain those, but mostly I'll just be working with an extended exchange matrix. Okay, once you have such a thing, you have a family of Laurent series called theta functions. And if you only focus on those with finitely many terms, you get an algebra with positive structure constants, the so-called middle cluster algebra. It contains the cluster monomials and therefore the cluster algebra itself. And if you have this ordinary equals upper phenomenon. This ordinary equals upper phenomenon that we mentioned at the end of the last talk, it traps them between, and you actually get a basis for the cluster algebra itself. Hey, it's the name of the conference. So I sort of splinted through these bullet points because for 90% of this talk, all that matters is data functions are a family of Laurent polynomials that form a nice basis for any cluster algebra. Asterisk, it might need to be a little bit bigger. Okay, so this is just going to be about a particularly nice family of theta, of Laurent polynomials and a basis. Polynomials and a basis. Okay, but I still will give you a bit of a definition. We already saw it, so I'll hit it pretty quick. As Ben told us, the theta functions, I mean, almost every counting in cluster algebras is really trying to get at either the coefficients of a Laurent expansion or the structure constants of a product. And so here, to define a basis, you just need to come up with the right counting problem. And in the case of theta functions, the right counting problem is these broken lines in a scattering diagram that Ben was very careful to define. That Ben was very careful to define. I won't be careful to define, but I will still include some pictures. I will say, so this definition works for a general seed in a lattice M. By lattice, I mean something isomorphic to Z to VR, not something combinatorial or something weirder. I just want to sort of have that bigger generality because that's the generality in which theta reciprocity lives. But the examples will all be in terms of what are called A-type seeds, which are the ones coming from cluster algebras, the ones that give you cluster monomers. U-cluster monomies. Okay. Also, a little bit of a footnote: Ben showed us how we can move, excuse me, the base point around. But if I don't say otherwise, it'll be in the positive chamber, which will just be the positive quadrant in all of our 2D pictures. Okay, so here is a picture that should be very familiar to anyone who was here an hour ago. So, if I take sort of the simplest exchange matrix, the A2 cluster algebra, I get the following scattering diagram. And I'm going to explain considerably. Diagram. And I'm going to explain considerably less than band. I'm just going to tell you where the walls are. They are these places where broken lines are allowed to bend. And then you count these broken lines, which are these piecewise linear rays that start at infinity, usually with some prescribed derivative, and end at a fixed end point. And the interesting question about them, well, the main one is its final derivative, because that gives you the exponent on a term in your theta function. Okay. So here's almost the example Ben did. Almost the example Ben did. I guess it's a rotation of his example, but we get three broken lines with initial derivative 0, 1 at the end at this base point. I look at all of their final derivatives. I negate it for reasons. And then I cook up the corresponding Laurent polynomial. If I write it in this form, it might look a little weird. But if I write it in terms of the initial variables, it becomes something hopefully familiar to everybody, you know, everybody's favorite cluster variable in the A2 cluster algebra. A2 cluster algebra. If we do a more interesting example, so if I let my initial element in the lattice m be negative 1, negative 2, now my initial derivatives are 1, 2, so they're coming from the bottom left. And if I have this base point here, there are nine broken lines. I've drawn them there. You don't have to worry too much about it. Just they're quite a few. There's also a weight. Broken lines have a multiplicity to them. And so you actually do a weighted count. And so if I cook up this weighted count of the broken line, Weighted count of the broken lines, I get the following nine-term Laurent polynomial, which looks quite a bit more daunting. But if you stop for a second, you see that it factorizes quite nicely and can be written as the following product, which hopefully you recognize as a cluster monomial in the A2 cluster algebra. Okay, so this is just a specific incarnation of the phenomenon that Ben talked about and I mentioned, that these theta functions construct the cluster monomials, but also can give you more functions, enough to More functions enough to complete a basis. All right, so that's theta functions. Hopefully, everyone is familiar with these from an hour ago. So, let me talk about monomial valuations. This is a name for something that goes by many names, but it's sort of the simplest take on it. So recall that my theta functions are indexed by a lattice I'm calling M. I believe Ben called this L, but a lattice is a lattice by any name. But I'm going to think about the dual lattice, which is just all the group homomorphisms to Z. To Z. Sort of a funny thing. If in your mind you're thinking of M as an integer lattice, just D vectors of length D, then you can think of the dual lattice as also integers of the vectors of length D with pairing given by the dot product. So an element in N acts by the dot product. So a lot of what I'm going to say, we'll be thinking about the two lattices, but you can really just think of B's both as integer vectors paired with the dot product. Okay. But the point is. Okay, but the point is: if I take such a vector in the dual lattice, it naturally wants to act on all the elements in the original lattice m. So I can go to each exponent in one of these Laurent polynomials and apply n to it. This is going to give me an integer for each exponent. And so I'm going to define what I call a what called a monomial valuation, where I take a Laurent polynomial and just take the minimum of the value on the exponents that appear. Okay, so a very simple, you know, arithmetic. You know, arithmetic operation. It spits out an integer at the end. I should say this is a valuation in the sense of algebra. It's not all of the valuations of the cluster algebra. It's a special family of them. And this gives you a class of objects that's equivalent to things called boundary valuations or integer tropical points, but these have more of a geometric flavor. This is just a very simple algebraic definition. So that's the one I'll be working with. All right. So this is defined for any Laurent. So this is defined for any Laurent polynomial, but I obviously have some specific Laurent polynomials in which to apply them, the theta functions. So let's see what these do to the two theta functions I showed you. So the cluster variable and then the more complicated cluster monomial. So let's say, so here m is just z2. So I can think of n as also vectors of length two. And so I can write an element of that lattice as just a pair of integers. And so if I have the monomial valuation indexed by n1 and n2. Indexed by n1 and n2, and I apply it to this first theta function. Well, it just takes this vector, dots it with each of these vectors here, and takes the minimum. So no matter what n is, I'm going to get a value given by this formula. Pretty simple to compute. And if I picked a specific n, like 1, 0, then I just compute all of these, and I think I get these two are both negative 1, so the value is negative 1. I should say, if you haven't seen monomial values, I should say, if you haven't seen monomial valuations before, it may seem like a weird thing to do, but a lot of the questions you might ask about theta functions can be phrased in terms of valuations. For example, if I wanted to know the power of x1 that occurs in the denominator of the theta function, that would just be going to each of the exponents and asking for the smallest value of the first number that appears. So I'd be dotting with one, zero. So a lot of interesting questions can be formulated in this way. Things like g vectors, opposite g vectors can all be rephrased. G vectors can all be rephrased in terms of the values of certain monomial valuations. Okay. So we've seen the monomial valuation of the simple theta function. Let's do a more complicated one, this nine-term guy. So I haven't written it all out, mostly for space reasons, but a priori, if I take the valuation of this nine-term guy, I have to go to each exponent and take the minimum dot product that appears. But a funny thing happens. Only four of the nine monomials can actually realize. Four of the nine monomials can actually realize the minimum. I can throw the other five out and not change what the minimum actually is, and I end up getting the following four-term minimum. Okay, so this minimum of four linear expressions. So there's a pretty straightforward reason for this that's neat if you haven't seen it before, and it's that the monomial evaluation only cares about the Newton polytope of the theta function or whatever you're applying it to. The Newton polytope is just the convex hull of the exponents. The exponents. So let's take this complicated theta function and just draw a dot for each of the exponent vectors. The nine terms become the following nine dots. And I just take the convex hull as a subset of the plane and I get a wonky quadrilateral. But the point is, the value of the evaluation, I just take a vector n, I take its dot product with all of these points, and that's for the minimum one, which is the same as taking the dot product with all the points in this polygon, because the minimum is going to happen at a corner. Because the minimum is going to happen at a corner. So these four corners are the four monomials that actually mattered for the valuation. Okay, so the takeaway from this should be anything you're saying about the valuation of a theta function is really only a statement about the Newton polytopes of theta functions. This is kind of important because while I like the theta basis, there are other bases for cluster algebras you might like, and many of them at least seem to have the same Newton polytopes as the theta basis. So, um basis. So everything I say about the theta basis is equally true about any basis. Well, anything I may say about valuations of theta functions is equally true about any basis with the same Newton polytopes. Okay. So we're kind of losing some information. We're losing the information about first what the coefficients are and then also what's happening in the inside of this polytope. So we're losing information, which is both good and bad. And bad. And then there's one other perspective on these valuations I wanted to mention. And that says a tropicalization. Okay, so this is kind of a fun thing you can do. So theta functions are positive. We've seen this a couple of times, which means that they can be written without subtraction, right? You can write them as just a bunch of sums and products of various expressions and various monomials. And so I can just take an expression, some subtraction-free expression for a theta function, and I could tropicalize it by taking. And I could tropicalize it by taking each plus and putting a circle around it, which just means taking the minimum. Take each times and make it a circle, an O times, which becomes a plus. And then each exponent, each monomial, I should say, becomes p dot n. The value of n on p. And the nifty thing is this gives you, if I cook up this expression and then plug in a specific choice of n, I get exactly the value of the nomial valuation. Okay, so for example, if I take this expression here. So for example, if I take this expression here, the simple theta function we cooked up earlier, if I tropicalize it, these pluses become O pluses, which I'm thinking of as mins, and then these monomials just become dot products with n. But this is just really a fancy way of saying what I already did. Take the minimum of the dot products with each of the exponents, which is just how I define this. So I get exactly the same expression I had before. So this is like three different perspectives on what these monomial valuations are actually measuring. But it means that if we want to think of this expression as a function of n, you can equivalently think of it as the tropicalization of the theta function. Okay, so sometimes these are called tropical theta functions, at least if we fix the theta function and vary the valuation. Okay, so now we've met our two casts of characters. We've got the theta functions on one side, and we've got these valuations, which take in Laurent polynomials and spit out numbers. take in Laurent polynomials and spit out numbers on the other side. So the natural question, which we've already been playing with, is what do these numbers, how do these numbers behave? So I'm going to call this the theta pairing of N and M, because it cooks up this theta function and then plugs it into this monomial evaluation. And then I'm going to kind of continue the conversation we just started, which is how does this behave if I fix one of these two arguments? So for example, if I fix the theta function. So, for example, if I fix the theta function and vary the valuation, how does this look? And if I fix the valuation and vary the theta function, how does this look? Okay, so this is our reasonable question to play around with. And we already basically know the answer to the first one. If I fix the theta function, I just told you it's the tropicalization. It's the minimum of a bunch of linear terms. So here's functions we've already seen in a couple of different ways, but now I've drawn it more explicitly as a function of the plane. Explicitly as a function of the plane, right? So, this valuation as a function of n is this expression. And if I think of it as a function on the plane, well, it's a minimum of three linear expressions. So it's what's called a piecewise linear function. It's not linear everywhere, but it's linear almost everywhere. Particularly, it's linear on these three regions, and then it's got creases on the blue lines. And then on each of the linear domains, it's given by the following functions. Okay, so the simple theta function. So, the simple theta function has a tropicalization that looks like this, and the more complicated theta function has a tropicalization that looks like this. Okay, four terms and three terms correspond to four and three linear regions. So, pretty simple. We'd already played around with these guys and also very simple to compute, right? If you can compute the theta function, you can just tropicalize by formally making all the pluses mins, and you get exactly these expressions. And in many ways, it's easier because you're allowed to throw out everything that is in a vertex of the Newton polytope. Everything that is in a vertex of the Newton polytope. Okay? So it's a pretty easy thing to cook up if you can compute a single theta function. So, what if we fix the valuation and vary the theta function? Well, a priori, this is a much harder problem. Something that I sort of alluded to, and I think Ben also did, which is that theta functions have a lot of magically nice properties, but in practice, they're terribly difficult to compute. These broken lines are often very challenging to enumerate because there's these horrible Challenging to enumerate because there's these horrible regions. And if you fire a broken line through in one side, you have no idea what kinds of things can come out the other side. And you need to know if you're going to even like restrict the support of where a theta function can live, questions like that. So to ask what kinds of things I could get if I vary the theta function over all possible theta functions for this fixed seed without a trick will be quite hard. You need to know every theta function, some uniform. You need to know every theta function, some uniform expression, which we only know for very nice C. I don't know about you, but I can probably only name all the theta functions for finite type cluster algebras and rank two cluster algebras. Everything else, I will start to run into problems. There's some conjectures about what happens in the marked surface case, but generally speaking, this is a hard problem, even for like very simple rank three cluster algorithms. Nevertheless, our running example is both finite type and rank two, so I actually do know what all the In rank two. So I actually do know what all the theta functions look like. And with a little bit of patience, I can actually compute what this looks like as a function of n. I fixed some value of n here and let the theta functions vary over all theta functions. And lo and behold, I don't get something crazy. I get a piecewise linear function. It's going to be the minimum of three linear expressions. The domains of linearity are here and here, and they're given by the following terms. And then if I take a slightly more complicated monomial valuation and vary all the Monomial valuation and vary all the theta functions, I get the following expression, okay? Which is a minimum over four linear terms. All right, so hopefully at this point, your interest is being peaked because we had an easy question and a hard question whose answers looked very similar. And we'll get there. But the first observation to make is because these are piecewise linear, they can be realized as the minimums over some, as valuations over some Laurent polynomial. Valuations over some Laurent polynomials, right? And really, all I can say is what the Newton polytopes of those polynomials would have to be, because that's all valuations can see. So we could just make the very easy observation that if I fix a valuation and vary the theta function, it looks a lot like fixing a function and varying the valuation for Laurent polynomials with this and this Newton polytope, respectively. Okay, so now I will ask the question. So now I will ask the question that hopefully is already sort of in all of your minds. Can I do this with theta functions as f? Can I put a theta function here such that its valuations as I vary the valuations are in fact the valuations as I vary the theta functions. And the answer, of course, is yes. Otherwise, I wouldn't be leading you in this direction. And this is what I call theta reciprocity. So the statement is, if I have an M in the lab, The statement is: if I have an M in the lattice and an N in the dual lattice, then see how this goes. I should be pointing with something that the home audience can see. There we go. Right, if I look at the valuations of theta functions in the original seed with respect to n, I can swap the role of the theta function and the valuation as long as I change the seed to something called the mirror dual seed. This is a different seed in a different cluster output. This is a different seed in a different cluster. Well, not even necessarily in a cluster algebra, but it's some canonically determined piece of information by the original seed. And I'll talk more about exactly what this means in your favorite examples. But for right now, you should just think of it as a different enumerative problem that allows you to swap the roles of the valuation and the theta function. And this is really nice because as we saw, this is a very asymmetric problem, right? The theta functions are devilishly hard. The monomial valuations are as simple as minimizing a bunch of dot products. Okay, I should say this is not at all my own idea. This was conjectured when theta functions were introduced in Grossheck and Keel and Konsevich is a remark. And they also proved it when one of the two theta functions is a cluster variable. Okay, so this they made quite a bit of progress there. I should say, as we mentioned, sometimes theta functions are full series. And in fact, reciprocity holds there too. You just have to let infinity be or negative infinity be a valuation, right? Infinity be a valuation, right? So instead of taking the minimum, you just have to take the infimum, but then you will still have this symmetry. As I mentioned, this is only a statement about valuations of theta functions, which means it's really a statement about Newton polytopes of theta functions, which may be the same Newton polytopes as some other basis. So maybe your favorite basis has exactly this reciprocity law for exactly this reason. Finally, I'm asserting this for all cases. I should say the skew symmetric case. I should say the skew symmetric case we've written down carefully. The proof is pretty simple. The skew symmetrizable case is quite complicated. It involves some machinery that we've all written down, but if something breaks, it would be there. And that's really what the holdup is, is trying to see if we can do it all on one paper. So if we get impatient, it might wait till a second paper. So there's my signpost about the only things that we're a little bit worried about. Okay, so that's theta reciprocity, this cool way of swapping the role played by the theta function. The role played by the theta function and the role played by the valuation. So, what I'd like to do now is I'd like to kind of restate this a little bit. This should feel kind of abstract, but I want to show you how you can actually do some computations with this and some examples as well. So, actually, what I'd like to do is I'd like to show how we can use data reciprocity to compute specifically this expression without actually having to compute all these data functions, right? That's the kind of cost saving I'm advertising. That's the kind of cost saving I'm advertising to you. All right, so let's see, let's get a form of theta reciprocity that's elementary enough that we can actually apply it in the case of the A2 quiver. Of course. So this theorem holds in any skew symmetric case. Yeah, so Alfreda's asking if it holds in any skew symmetric case. Yes, and in fact, it's not even necessarily for A or X. There's some sort of interpolating definition. There's some sort of interpolating definition of seed that for which it works there. So, like quotients of A or X and stuff like that. Though, of course, we prove it for a print and then just deduce it by a specialization result. So, it's sort of an easy extension of case. Here? Yeah. Yes, yeah. So, so, yeah, I'll say a little bit about the, so this is a very initial seed dependent definition. I'll say maybe a little bit, one slide. I'll say maybe a little bit, one slide worth of definition about the invariant perspective on this, the intrinsic way to state this. Yes. All right. But instead of going towards the more abstract, let's go towards the more concrete. If you only like cluster algebras, you like having an exchange matrix and cooking up cluster variables and cluster monomials there, how do you apply theta reciprocity here? Well, if I start with an A-type seed, by which I mean the seed determined by an exchange matrix, you can ask what is. Exchange matrix. You can ask what is the mirror dual seed. And I can tell you, I mean, either you already know these words or you don't, and that's fine, but the mirror dual seed is the X-type seed of B transpose, not of B. This is a weird little difference that creates all the problems in this geosymmetrizable case. Fun footnote there. But the theta functions over there are friends of yours in another name. They're just the f polynomials with a monomial on front. Okay. So Okay, so specifically, if I want the theta function in the mirror dual seed of some A-type seed with a given n, I take y to the n. I like to use y for the variable on the dual side for reasons. And then I cook the f polynomial associated to the theta function. So you can think of this in the cluster algebra of B transpose, not of B, and then with G vector B transpose N. Okay. So this is. Okay, so this is some theta function. Well, it's some theta function, and then it gives you an f polynomial. But this gives us a Laurent polynomial in y. And in fact, you even kind of know where it's supported. You know, it's yn times a genuine polynomial. And then theta reciprocity for exchange, you know, in this case, says that valuations of theta functions in some A-type seed are going to be valuations of just yn times the f polynomial. And of course, it's multiple. Polynomial. And of course, it's multiplicative. So I can just spit out the m dot n part. Okay, so I get this expression here. So let's see an example of this, just so you believe me. As I said, we want to recover this formula for the valuations of the valuations with respect to one comma zero, sorry, zero, one as m varies, right? We had this formula that I claim I got from just brute force computation. How do we get it from theta reciprocity? How do we get it from theta reciprocity? Well, the formula I just gave you said that we need the f polynomial of B transpose n, which is just this, that's quick multiplication. And then, so if I look at the theta function in B transpose, so B transpose is still the A2 cluster algebra, but it's where I've reversed the orientation of the arrow. So the indexing is a little bit different. This is this weird thing that'll come up a couple of times that if you negate an exchange matrix, you get the same cluster algebra, but The same cluster algebra, but the way that you associate a theta function to a vector changes. You're changing what the g vectors mean effectively. Okay, so once you remember this and you sort of recompute, I think my cursor has crashed. All right, I do still have. There we go, the ability to move forward. Right. You can just cook up, oh, hey, it's this cluster variable again. Okay, so this theta function is this cluster variable in another name. And because it's a cluster variable, And because it's a cluster variable, it's quite easy to cook up to reverse engineer its f polynomial. You just write this as the g to the g vector times a polynomial in x to the b transpose times a bunch of non-negative vectors. And then you just use those as the exponents of your f polynomial. I'm reverse engineering the f polynomial here, and I get the following. Because this is a cluster variable, you could have also cooked up the f polynomial in any of a number of enumerative problems, quivergrass, Mannians, snake graphs. Onions, snake graphs, you name it. They'll cover this. And so this gives me the following dual theta function, which I can multiply out. And if I tropicalize this expression, wouldn't you know it, I get the formula that I started with. Okay. So, you know, it's about a slide, but it's a slide of pretty straightforward computation. Hopefully, you believe that this computation is quite a bit easier than listing every theta function and minimizing there. Well, maybe not for this cluster algebra. There's only five clusters, but if I had some weak. Algebra, there's only five clusters, but if I had some weird infinite type one, then this is, of course, considerably easier. Okay, so that's the kind of computation you can do with this. Now, to please Alfredo, let's go into the other direction, in the more abstract direction. Okay, so as I mentioned, this perspective, the definition I gave is very sort of initial seed dependent, right? Where I've parameterized the theta functions by elements in a lattice. But everything can be phrased more invariantly, more geometrically. Geometrically. So let me just give you an invariant statement of this. And it won't even be geometric, but it'll at least be invariant. And you can kind of reverse engineer the geometry from it. So let me let capital theta and capital theta dual denote the theta bases corresponding to the two seeds, respectively. So now I can take a theta function of each of the corresponding theta bases, and I can associate two numbers to that pair, right? Because each of them are indexed by evaluation. So I've taught you. So, I've taught you how to index it by a binomial valuation, but there's an invariant valuation you can index it by, these boundary valuations, which are valuations on some geometric object. But whatever version of valuation you get, it's still something you can apply to theta functions and get an integer out. So I can take theta, sort of look up its serial number in the form of evaluation on the other variety, and apply it to that other theta function and get an integer. And then I can just reverse the role played by these. Reverse the role played by these. I can take the dual theta function, cook up its g vector or valuation, and apply it to this theta function. So I get two integers associated with this pair, and theta reciprocity is merely the statement that these are always the same integer. And so you can think of this as a well-defined pairing between the two theta bases with integer values. I should mention, so Fock and Gonsherev have this paper, Cluster Ensembles, Quantization and the Dilogarithm, I think. Quantization and the dilogarithm, I think, where they papers like 50% conjectures by volume. And sort of most people sort of know the existence of a basis part, but a big part of it is a conjecture of such a pairing between a basis on the two dual objects. I should say it's sort of conjectured by Falk and Goncharov. Back in this time, their conjecture was there exists such a basis or a pair of bases such that this is true, which at the time I'm sure was very optimistic. Now we have. Now we have an embarrassment of riches. Our problem these days is we have too many such bases and trying to sort out what to do with these bases. But we at least have the assertion that Falk and Gonshrov's pairing conjecture holds for the theta bases. And this intrinsic description has sort of a nice, what do I want to say, sort of opens up a new avenues of inquiry because we there Um, we there's lots of things you can index the bases of a cluster algebra by, so right. So, I was indexing them by points in a lattice, but depending on your cluster algebra, you can index them by lots of different things, okay? And so if you've indexed your bases and your basis and your dual basis by some natural interesting class of objects, you know, multi-curves, laminations, plane partitions, young tableau, whatever, you can try to ask what is the pairing in that language, right? So the case where this is clearest, which is This is clearest, which is in Falk and Gantorov, it's certainly not unique to me. Which is that for marked surfaces, we expect that the basis is parameterized by simple multi-curves, the dual basis parameterized by certain laminations, and the theta pairing is just a multiple of the number of intersections. And there's some ambiguity here that I'm sweeping under the rug, hiding in the words certain and a multiple of, but that's basically the idea, right? If I can describe these two sets by a nice class of objects, what is this map in terms of? Class of objects, what is this map in terms of that language? Okay, and that's of course something I have very little insight in other than curiosity. All right, so that's a couple of ways to think about theta reciprocity. What I'd like to do next is I'd like to talk about what happens when we have a Lambda matrix. Again, I'm sort of stealing from Ben. So I'll give a definition almost what Ben gave. So a compatible pair, sort of a weird name. A compatible pair, sort of a weird name. You just want an exchange matrix and one of these Lambda matrices. So usually in practice, you start with B and you want to find a Lambda. So really, I don't know, you call this a compatible matrix or a compatible form or something, but whatever, we call it a compatible pair. And the condition is an extended exchange matrix, a skew symmetric matrix. And in fact, I don't even require that these have integer entries for everything I'm going to say. And you just require that the product of the two matrix. And you just require that the product of the two matrices is a diagonal. And for space reasons, I made it horizontal, but it's a diagonal matrix over a zero matrix. So in the mutable part, it's a diagonal matrix. And in the frozen part, it's a zero matrix. Then depending on who you look, where you look, compatible pairs also require the entries of D are positive, some but not everywhere. I'm just going to split it out as an extra word. So I'll say a compatible pair is positive if the entries of D are strictly positive. Of D are strictly positive. So I'll point out that at least if you like working in rank two, which you should, it's great. There's always one Lambda matrix that always works. So if your exchange matrix is just this guy over here, you can always just use this rotation matrix, right? 90 degree rotation, clockwise or counterclockwise. Exercise to the reader. And so we have sort of a standard Lambda matrix in rank two. And then a higher rank, you can hope that it's there. Rank, you can hope that it's there. Though I should say this might be sort of deceptive. Here I'm saying, wow, Lambda matrices always exist and it's easy to pick one. In general, there are simple obstructions like B has to be full rank for a Lambda matrix to even exist. So they're a little special, but they're still almost all of your favorite examples should have a Lambda matrix and often a canonical one. Well, depending on who you ask. All right. So what I'd like to do is I'd like to say, if you have a Lambda matrix, how can you? You have a Lambda matrix. How can you sort of improve or reformulate theta reciprocity? Okay, um, because sort of morally, a lot of what you end up doing with Lambda matrices is Lambda matrices allow you to translate a lot of properties that the X cluster variety has over to the A cluster variety. For example, X cluster varieties have a Poisson structure. They have a canonical quantization. There's no choices over there. And so a choice of Lambda gives you a way to transport that Poisson structure, that quantization over to A, which is why Lambda controls compatible quantization. Is why Lambda controls compatible quantizations and Poisson structures. Now we have another thing that involves a dual cluster variety, an x-type cluster variety, the other side of theta reciprocity. So we could use lambda to pull it back to the A side. So we get the following formulation of theta reciprocity, which I call lambda theta reciprocity. If I have a positive compatible pair, that matters this time. Then I could take two elements in the same lattice. And I can now, instead of taking valuations with respect to some n. Taking valuations with respect to some n, I take lambda times n, and then I throw in a sign because life is never quite easy. Actually, if I put negative here, the this sign, well, the sign would move. It's not a big deal, but the signs are a little inelegant. But the point is, instead of pairing elements in opposite lattices, I'm pairing two elements in the same lattice, okay? And the symmetry becomes the following, okay? So it's sort of like a skew symmetry in the pairing of elements in the same cluster algorithm. Of elements in the same cluster algebra. And here's this weird thing I mentioned before. These theta functions have the negative cluster exchange matrix of each other. So they're actually in the same cluster algebra. They're just indexed in a different way, right? Like this is also theta sub b of something, but finding that something is a little bit tricky, though it can be described by a monomial evaluation or a bunch of monomial evaluations. So this is really a statement about valuations of two different theta functions in the same cluster algebra, where you can think of it as a Cluster algebra, or you can think of it as a theta pairing between elements in the same cluster algebra. Okay, so here I have this standard example. So, right. So, ooh, this should be val, not a theta function. But right, so this is this valuation where I fix the valuation varying the theta functions that we've cooked up a couple of different times. But now I don't even want to leave my own house to cook up that f polynomial. To cook up that f polynomial, I just want to stay in the cluster algebra of A2. So, lambda theta rest process says just find some m prime such that negative lambda times it gives me this n, which I can, it's up here. And then I get the following expression. Theta reciprocity allows me to swap these. Now I just need valuations of a fixed theta function in the A2 cluster algebra. Turns out to be this one yet again. And then I apply lambda to these m's, they swap, and I get. To these m's, they swap, and I get this expression we've now seen three or four times. Okay, so we're over and over again finding ways to cook up these kinds of valuation expressions in terms of trapicalizations of theta functions that we hopefully know how to compute. All right. But while I've got a lambda, there's a very weird thing you can do with that. Okay, so here's a thing that's a little less related. Well, it's not so closely related to theta reciprocity. That's not even true. It's the heart of theta reciprocity. Vesprosity. That's not even true. It's the heart of theta reciprocity, but not obviously. All right, let me stop. I get so excited about Lambda momentum. There's a cool thing you can do if you have a Lambda major, okay, and you're just playing around with broken lines. So let's say I have a compatible pair and I have a broken line in the associated scattering diagram. Then at any point along that broken line where the derivative exists, so not at one of these bends, I can just take the position of the broken line just as a point in. The position of the broken line, just as a point in space, and I can take the derivative, multiply the derivative by lambda, and then pair the two. Okay, because you should be thinking of lambda as a map from m to n, so this sort of makes equivariant sense, but also just take it as the dot product of vectors. That's fine too. And so you get a nice integer. Well, if lambda is an integer, this will be an integer. But in general, a nice number associated to a time in the broken line. And it's a fun but simple exercise that this is independent of time. This is independent of time. This is constant everywhere on the broken line. So, this gives you an invariant of these fancy trajectories. So, you'll recall, I said one of the hardest things about broken lines is just sort of if you know where they are at one time, where can they be later? And this gives you at least one question that you know. Okay, it disappears into this horrible storm of infinitely dense walls, comes out the other side. I know it has the same lambda momentum. So, that's at least one quantity that is conserved, no matter how hairy things get. Okay. If we go back. If we go back to the rank two example where lambda can be taken as fixed, this rotation vector, then this is actually just the angular momentum counterclockwise around the origin, which is why we call it lambda momentum. It's generalizing angular momentum. And the fact that angular momentum is conserved in the rank two case is the heart of the proof that greedy basis equals the theta basis. And so now we have this higher rank version of this, at least in the presence of a lambda. Okay. Okay. Now, here is just a weird fact. So if I have two elements in the lattice M, I now can, I've already taught you guys how to cook up a theta function, cook up a nomial evaluation with lambda, and pair the two. This is some integer or at least some number if lambda is weird. This happens to be equal to the minimum lambda momentum of any broken line with initial derivative m and endpoint n prime. So you should think of these as broken lines. So, you should think of these as broken lines contributing to the theta function of m with endpoint m prime. So, this is a purely kinematic question. If you have fixed m and m prime, you could just draw a dot in space and try to hit it with broken lines and ask for the minimum momentum that you can do it with. Okay, so I wanted to sort of illustrate this with an example. Let's see how this goes. Okay, so here I've got Desmos, a fun little browser-based graphing. Esmos, a fun little browser-based graphing calculator. And there's a link in the slides if you want to play with these two. The thing I like about this is I can take this base point and drag it around. Here, the base point is not required to be integral, but if you're doing these valuations, you probably want these to be integral. And so you can see this A2 cluster scattering diagram and some choice of base point. So now, pursuant to the theorem, let me try to hit this base point with broken lines corresponding to m equal to zero. Corresponding to m equal to 0, negative 1. Okay, so that means their initial derivative will be 0, 1 coming up from the bottom. So I get the following three ways to hit this particular endpoint. And I've decorated them with their lambda momentum. So recall in rank two, lambda momentum is just angular momentum clockwise around the origin, counterclockwise around the origin. So those that go clockwise have negative lambda momentum, and those that go counterclockwise have positive. And you can see this one's positive, these two are negative. These two are negative. Okay. All right. But now I can move this around. And if I stay in this chamber, the broken lines move around and their lambda momentum changes, but is sort of qualitatively the same. But then when I pass through this wall, oh, oh no, the red one's no longer possible. And now the green one is the minimum lambda momentum. If I go through this wall, there's only one broken line, so it's definitely the minimum one. If I go over here, One. If I go over here, still minimum. And if I go over here, now there's a red one that's the minimum guy as well. And so I can go back here. And so we get the following description of all the minimum lambda momenta broken lines. And so I can just describe each region, write a function on each region of the minimum lambda momentum of a broken line that ends there. And I end up with the following expressions. Here, the minimum lambda momentum is always negative. Here, the minimum lambda momentum is always negative m prime two, negative the second coordinate of your endpoint, and so on. And wouldn't you know it? This function is exactly the minimum of m1, m1 minus m2, and negative m2. This function we've now derived a fifth time, I believe. We're seeing this as a valuation of this particular theta function, yet one more way. Okay, so I also want to do this for m equals negative one, negative two, but we were. Of two. But we recall that's got a lot of broken lines. We saw that that had nine broken lines in the positive quadrant. So I'm not going to try to draw them all. I'm just going to draw the one that minimizes the lambda momenta. So up here, I have this guy that bends quite a bit, this really bendy guy, and it's got lambda momenta, in this case, negative 11.45. And then if I go over through this wall, it qualitatively changes. It no longer has that bend at the end. If I go through this wall, now it's required to be straight. If I go through this wall, it's got one bend. If I go through this wall, it's got one bend. Then over here, I get this guy. Okay. And something I should say is that because the lambda momenta is constant everywhere, you can always evaluate it at the end point, which is going to be where you're at, dotted with lambda times your final derivative. So if the final derivative doesn't change as I move around, in that region, it will be a linear function. So that's why the colors are coded based on what the final derivative is. So all these red guys. Final derivative is so. All these red guys have the same final derivative, even though this guy has a different sort of structure than this broken line. Okay, it has the same final derivative. And so these two chambers are one linear region for this valuation. Okay, and so we can turn on little formulas for each of these expressions here. Let me get a fun guy. And we get the following expressions for the minimum lambda momenta of a broken line ending in Egypt. Momenta of a broken line ending in each of these chambers. Okay. All right. So that's a fun little digression. But the point is that this question about valuations of theta functions, which you can think of as tropicalizations of theta functions, is really a question about, well, is equivalence to a question about what the possible lambda momenta of broken lines with a given initial derivative and final endpoint are. Okay. And I should say this is what we actually end up proving, or at least the version of theta rest. End up proving, or at least the version of theta reciprocity in this formulation is what we end up proving. There, theta reciprocity is just some sort of symmetry. If a certain broken line exists, then a totally different broken line exists with the same, with the negative lambda momenta. Okay, and there's a trick we learned from a paper of Alfredo, Mandy, and Tim where you can sort of reflect these guys and get exactly the symmetry you need to shoot this. So we show theta reciprocity for lambda momenta, and then we derive the valuation version of it. Okay. Of it. Okay. Great. We're doing well on time. So that I think is the five versions of theta reciprocity I intended to tell you. What I'd like to do with the last little bit of time is to show you an application, something we can prove with theta reciprocity that doesn't, on the face of it, have any valuations in it, if you're suitably sick of them. So this application is going to deal with how theta functions behave under freezing and unfreezing. Behave under freezing and unfreezing. Okay. So I'll start with a definition. So I can ask if a theta function is polynomial in some cluster variable by just choosing a cluster containing that cluster variable and writing it in terms of that cluster. And if no negative powers appear, I say it is polynomial in that cluster variable. So there's that cluster variable just doesn't occur in the denominator. Now, you can naturally ask, does this depend on the choice of cluster? And it turns out it doesn't. There's this result from a few years ago. There's this result from a few years ago that if it holds in one cluster, it holds in them all. And I should say, this is not actually what they show. What they show is that all the clusters containing X are connected by mutation, and then it's easy to check that this is preserved under mutation. So that's what I mean when I attribute it to them. Okay. Okay, so there's a very simple question you can ask about a theta function, especially if x is an initial cluster variable, then you construct the theta function by cooking up its Laurent expansion. So this is just a question about whether you can have a certain Whether you can have a certain coordinate of any of the exponents be negative. Okay. So here is a theorem. It's sort of a special case of a bigger theorem we have. But if I have a theta function, which is polynomial in some set of frozen variables, then if I unfreeze those frozen variables, I get a much more complicated cluster algebra, but it's still a theta function there, right? I have this potentially much more complicated enumerative problem, a much more interesting scattering diagram, but nevertheless, that theta function downstairs lifts to a theta function. Downstairs lifts to a theta function upstairs because it had no x's in the denominator or no, none of these frozen variables in the denominator. What was that? Anyway, I should say there's a bunch of other equivalent things here. First, this actually goes both ways. If upstairs it's polynomial on a bunch of mutable variables, then freezing them, it's still a theta function. And in fact, all of these conditions are equivalent to this theta function being compatible with the frozen, well, the mutable variables upstairs. So there's a bunch of equivalent things. Variables upstairs. So there's a bunch of equivalent things that all give this lifting condition. But this is the version of it that's the most useful, at least to me, because if I freeze a bunch of cluster variables, I usually have a much simpler cluster algorithm. So, and maybe that's a case where I know lots of theta functions. Upstairs, absolute mystery. Downstairs, totally solved. So I can try to find all the theta functions downstairs, ask which are polynomial in the variables I froze, and those at least lift to polynomials upstairs. Polynomials upstairs. Okay. So you can ask, well, where do I know the theta functions? Well, I told you finite type, but those are all cluster monomials. So they'll lift to cluster monomials, nothing interesting there. And then the other one is rank two. And in rank two, we do have interesting theta functions that are not cluster monomials. So those we can now lift to bigger theta, to bigger cluster algebras. Okay, so I want to show an example of this. So let's say you pick a seed completely at random, but you see. Know. But you see, in this wildly complicated rank 47 seed, there's a double arrow. Now, I might not know what the cluster variable, the theta functions here are. Fun exercise, which of these are finite type. This one is definitely not. But the presence of this double arrow means that I can freeze every cluster out variable except these two. And now I have a rank two cluster algebra where I completely know that. Two cluster algebra, where I completely know the theta functions, right? So here I've actually given variables to the vertices before I didn't, because that would have been tedious. But here, let's call these five, let's say these five vertices have the following five functions. Now I can try to cook up theta functions in these five variables that are polynomial in B, C, and A. And if they are, I know they will be theta functions upstairs. And because we know very well the theta functions of a rank two cluster, specifically the two Kronecker, we know that there's. We know that there's in this particular cluster algebra, there's a bunch of cluster monomials. And then in the coefficient-free case, there's one ray that has there's sort of one linear family of theta functions that are not cluster monomials that are generated by sort of this nice little loop element and then various Chebyshev polynomials of it. And then you can get all the, if you have coefficients, you can just multiply them by various coefficients. There's sort of a most interesting non-cluster data function in this cluster algebra. function in this cluster algebra. So I can just write it down and then find this the, yeah, I can just write it down. It's this guy here. So I cook up this theta function, which I like calling a loop element in analog with the topological case. And because it has no A, B's or C's in the denominator, I know it's a theta function upstairs in the rank 47 cluster algorithm. Okay. All right. If you're curious, you can do this with any double arrow in any seed you encounter. I just sort of give you the recipe here. I just sort of give you the recipe here, and you can get the following loop element. This is sort of the minimal, the element of the theta basis that's sort of minimal with respect to sort of getting rid of things without becoming non-polynomial in the frozen variables. So you can cook this up. And then also, this is a theta function upstairs, and also all Chebyshev polynomials of it, of the second kind. So the reason I call these loop elements is because you could get all closed simple loops in cluster algebras of genus zero. Okay, if I have a cluster algebra gene is zero, if I have any closed loop, that gives me an element in the skein algebra and an element in the cluster algebra, and it will be given by this formula. Okay. All right. So that's neat. I figured I'd end with a few further directions. There's lots of things to play with. Here are three. So first is one I've already been alluding to. This is all about theta functions, but what about your favorite bases? What about the generic bases? What about the Bengals bases? What about the Bengals bases? What about the, you know, the MV basis, which may or may not be the theta basis, right? Do they have the same Newton polytopes? If so, then they at least have reciprocity. All right. Another question is, if you really like that technique I just showed you, when does it give you every theta function? When can I get every theta function as a lift from rank two theta functions? So it's true for marked surfaces of rank of genus zero. You can get them all that way. But if marked surfaces of genus greater, Way. But if mark services of genus greater than zero, there are loops that can't be realized in this rank two localization. They're very annoying. All right. Oh, and this other one I already mentioned, but might as well hit again. If you like to parameterize your basis or the theta basis by some class of objects, and you have a nice parameterization on both sides, what does this theta pairing look like for those guys? All right, and I think I'll stop there. So we will start with questions in Zoom. If you just unmute yourself. So check if there's any chat or phone charging here soon. I don't know if I want to hear anything. I don't hear it. Oh, there was a bunch of chat. Hopefully, that's not okay. Great. Okay, okay. Okay, great. Okay, okay. Okay, so then maybe we come to the questions of the room. All right, so just to follow up on your commentary on the second question here, those loops which can't be, or the, I guess, loop elements which can't be obtained from a rank two freezing, so they can, can they be obtained from So, they can they be obtained from some freezing? Is there like a bound on the rank? Or you just don't know? So, wait, hang on. To reiterate, that's a good question. So, the specific elements that can't be realized by, so you can be lifted from a rank two localization if there's a triangulation in which the loop is crossing just a regular 1, 1 annuals. And so, you simple topological question: what closed loops on a server? Topological question: What closed loops on a surface can't be bounded by this? And the answer is: if a closed loop divides your surface into two pieces, one of which has no marked points at all, then you can't be in one of these localizations. And the only way that that's possible is if you have some genus on one side. And as soon as you have any genus, you can always do this. You can move your handle over to one side and everything else over to the other side and cut through in a waste. So those guys presumably can be in a rank four localization. Can be in a rank four localization, you can get down to a dreaded torus. Um, and then I'd have to play around with some topology to see if you can get even worse loops. Yeah, I guess if you had like two genera over on one side and you had a loop bounding that, then you'd need some sort of higher dreaded torus probably. But I think if you knew the theta basis for all the dreaded tori, then maybe you could get all the loops there. Yeah, I think. So, the geometric motivation I was asking in JHKHK is actually that they expect that this reciprocity of the tropical pairing for affine local Labijaus with maximal boundary, right? And well, this is showing that this goes beyond that geometric picture. So maybe this can hold also, let's say, for generalized cluster algebra. For generalized cluster algebras, or you know, like so, yeah, I guess I should speak to the audience as well. Um, yeah, I guess I should do a little bit of due diligence in saying what I actually mean by a seed. I don't include generalized cluster algebra, so I'm really just considering any scattering diagram where there's finitely many incoming walls. All the incoming walls have a binomial on them, and then if I pair all the normals with the if I cook up a matrix where all the normals are paired with the highest power of the binomial, then Highest power of the binomial, then I get a skew symmetrizable matrix. So, this is really more or less just what you can get by taking various a prins and slicing them along graded pieces. But you can get a's and x's that way. So, just to be sure, I understand right what you just said with the lifting of the loop element, that proves that the Bengals. Element that proves that the Bengal spaces is equal to the theta basis in genus zero? Um, so it should be bracelet basis, uh, yes, yeah, because in in yeah, in the in the annulus, it's the bracelet basis. So, yes, this proves that in genus zero, the bracelet basis. Though I think Travis and Fan have a proof in full generality that they've been working on for a while, but yeah, this should give it at least in genus zero. Thank you. So, I'm not sure I'll formulate that precisely, but so this valuations assume can help you compute multi-gradients, right? When you have, so and sometimes you're interested in describing values for multi-gradients for all the variables, for example, and this will include some. And this will include them. So, would it help in some situation, considering reciprocity, to say something about the image of all possible multi-gradings in your algebra. That could help establish more examples. So, when upper cluster algebra is not equal to cluster algebra, I think funny. Yeah, so that is an interesting. Yeah, so that is an interesting question. Right. So I know you can try to give a multi-grading to a cluster algebra just graded by the abelian group, the co-kernel of the exchange matrix. And then there's a yeah, so the theta functions will always be homogeneous in that so that the right, because the f polynomials are always, you know, the exponents are in the image of the exchange matrix. Yeah, what? Hmm. Yeah, what is that on the other side of theta reciprocity? Yeah, I'd have to sit and think about it to see what it actually means there. So your question was, can you find like more multi-gradings? I would rather prefer to find multi-gradings, which will never occur for this particular cluster. Cluster algebra. So, yeah, I don't know. Maybe exclude some. Yeah. Okay. Yes. That is definitely an interesting question, but yeah, I have to see an example. Okay, maybe I have a quick question. The Lambda matrix, I think you mentioned at some point you're thinking of it also as a map from one lattice to another. Could you remind me which these? Could you remind me which these lattices were? Yeah. So, right. So, a skew symmetric form is sort of takes two things in M and spits out an integer, which means you can take one thing in M and just wait to spit out another thing in M, which means it's giving you something in N. So you can think of a skew symmetric form like Lambda as a map from M to its dual lattice N. And then the skew symmetrizability, skew symmetric condition is just, you know, there's some natural adjoint map that's going to. There's some natural adjoint map that's going to swap m and n, but also take the transpose so it's still m and n. So you just say it's the negative of that. So there's some invariant characterization of these. And then these lambda matrices should give you one of these maps on the cluster variety as well from the x variety to the a variety. Well, at least if it's integer valued. I was allowing them to be real just because I didn't even need any integers there. Thank you. Yeah. Yeah. Maybe it's a good time to stop also and postpone any further questions for discussion. We have in the room another talk starting in two minutes where we say goodbye to all the Zoom people. I don't know exactly if you can see me, but we'll be back online tomorrow, 9 a.m.