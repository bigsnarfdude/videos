So I'm gonna, as everybody else, kind of try to build a trajectory from the last presentation yesterday, but I want to make it a little bit more specific. So instead of just talking about general issues, I'll talk about three specific case studies that helped, I think, me at least to surface some of these issues, and then I'll summarize in a more general way later on. So, the first case study is going to be quite familiar to a few people in this room. This is a project. This is a project that we did with Dave, with Esteban, with Matt, and Elliot Mitchell was a student. I mean, he was a PhD student when he worked on it. And Arlene Smaldon is a diabetes educator who has been working with us a lot on this project. And throughout actually my time at Columbia. So this was a project where we wanted to help individuals with type 2 diabetes to make more informed decisions about their self-management activities. Self-management activities. We all know this, it's very hard for them to identify specifically for each individual how to optimize their diet and physical activity to minimize disruptions in their blood report levels. So this is everybody's aware of it. It's hard to identify those personal trends. So we thought, okay, let's use computational analysis. We have people who do models, and then we can use inference. And then we can use inference, we can learn associations between physical activity and nutrition and changes in blood vibrose levels, and then we can use those to help individuals identify personal self-management goals. How specifically should they change their meals and physical activity to maintain stable glucose levels? Dave and Esteban and Matt worked together to develop the solution. To develop the solution, I'm not going to discuss the details of it, but you can ask them if you have any questions about the model. Attributable component analysis, the beauty of it was, is, is that you can take a number of different variables and model them together. So we can see how the nutrition and physical and different aspects of nutrition, different macronutrients that are included in meals together. Meals together influence blood glucose levels. However, we run into a challenge where we needed to somehow take this complex model that models a lot of things together and translate it into one sentence that is formulating a goal for an individual in a way that people from low-income, low-education-level communities. Low education level communities can look at the sentence and say, Ah, I know exactly what to do next. So, what we did is we spent a lot of time with diabetes educators, including Arlene and others, and individuals with diabetes to understand, well, how do we formulate those goals that somebody can look at it and understand them? And the problem is that everything impacts blood local levels. So, you can't really disconnect meals, different aspects of meals. Different aspects of meals and physical activity and stress and sleep. You have to look at all of this together. You cannot possibly tell somebody, this is how you need to change everything. So after, you said an hour, an hour took to understand what it means to marginalize, the whole process took us three months to arrive at one sentence. And basically, what we ended up doing is Basically, what we ended up doing is instead of this beautiful modeling, this beautiful complexity of taking one variable at a time, one covariate at a time, and marginalizing across everything else. So we lost a lot of richness, and we ended up with a sentence that looks like this. Reduce your breakfast carbs to be no more than one carb choice. Examples of one carb choice are a slice of wholemeal toast, one cup of oatmeal, or one cup of What new war in one packet? So, and even that sentence, we ran some study with this, even that sentence is too complex and too abstract for somebody to understand. So now we're throwing a lot more machine learning onto this problem to translate this sentence into specific meal ideas, like images of meals that meet that goal. But again, what happened in the What happened in this process is that the final human-facing solution that we arrived at ended up being a compromise between the capabilities of computational modeling and the capabilities of humans who needed to understand or gain something from the model to be able to make decisions. And arriving at this solution required three to four months of Dave Four months of Dave, Matt, myself, Arlene, Marisa, Burger Master, who was a postdoc in behavioral nutrition, arguing practically daily to figure out what does that mean and how do we put all those things together to get at the point where again we are equally uncomfortable with our solution. Because that's the compromise, right? But nobody is happy. Is happy. The modeling aspect of it wasn't happy because it wasn't taking advantage of the computation. The dieticians were not happy because it wasn't representing exactly what it needed, so nobody was happy, but about the same in the same way. The nutritionists wanted to take advantage of things in the models that were very difficult, that were there, but were almost impossible to convey. Right. So this is a problem we continue to experience throughout many. To experience throughout many projects, where we have complexity, but we need to find a way to make it simple and easy to act on when it comes to decision support. So, this is a second case study, slightly different. And this is a work with Sujin Park, who is a faculty in our department, and Kayla Schiffer, who is a PhD student in our department. So, this is a completely different problem. Sujine is an attending physician in the neural ICU. She deals a lot with, I'm not even going to try to pronounce it, SAH, aneurysmal subar hemorrhage. There you go. Thank you. So it's a condition that patients have, and one of the most common causes of complication and mortality in this condition is a delay. Condition is a delayed cerebral ischemia. So it's a very deadly sort of complication, but very hard to identify because many patients are sedated. It's a very subtle change in presentation and there are no real, the tests to identify, to diagnose this, are very invasive and have a lot of risks and complications in themselves. So Sejine and her colleague worked on Her colleague worked on developing, they noticed that there are some non-invasively captured vital signs, physiological information, actually can be modeled to predict DCI quite accurately, more so than just observation of patients. So they developed this model. It's an ensemble classifier. As I think we were listening about machine learning today, they basically threw everything they could at it. Through everything they could at it, they came up with an ensemble method, it achieved relatively high accuracy. But the question became: how do you introduce this risk score into the clinical practice in the ICU? So Kayla did a short qualitative study where she tried to show clinicians in the ICU, they were not simulated cases, they were real patient cases, but retrospective, where clinicians were present. Clinicians were presented with some clinical information about patients and visualization of the trajectory of this risk score. And for those of you who are not familiar with this problem, as I am for example, it's supposed to start high, but it's supposed to reduce. So the normal trajectory is a reduction in this score over time. If it doesn't reduce, and this is the sort of the critical level, so this looks like how it's supposed to look like. Supposed to look like. If it continues above, that's a very dangerous place to be, so that requires some more invasive testing. If it lowers faster, it means patients recovering quicker, and there is a possibility of earlier discharge, which is a very desirable outcome in the ICU, but very risky. So, what she did is she showed clinicians clinical information, risk scores, and then asked them to sort of reason with. And then ask them to sort of reason with it and think aloud about what does it mean, provide assessment, and what their next steps will be. So, long story short, this is what we found. First is that knowing what goes in the model helps to understand an interpreter. For example, knowing that it's the vital signs that were taken as an input into the model helped Into the model, help to understand why this risk score trajectory looks different for heavily sedated patients than it is for patients with lower level of sedation. So it's not necessarily that their risk score was lower, it's that their vital signs were less were different. So just understanding what goes in the model. There was a lot of interest in this explainable UI. This explainable AI idea. And clinicians wanted explanations, but they didn't care about how the model came up with the inference. They wanted the model to explain the patient. What they wanted is a better explanation of what's going on with the patient, rather than this is the reasoning process in the model and how the model arrived with its conclusion. And we actually see that in many other studies outside of this conclusion. Studies outside of this context. And I've talked about this, but what was most interesting is that practically not a single clinician was ready to make any decisions based on the risk score. It was one input in their thought process. And if they were presented with just the risk score, basically what they needed to do is to go back to the EHR, spend a few hours digging out all the information. Digging out all the information on this patient just to see whether it was worth doing something else, whether it was trustworthy or not. So, if we're using risk scores to help clinicians make better decisions in a shorter amount of time, we failed miserably here because we just increased the amount of time needed to investigate different patient cases. So, Kayla ended up putting together an interface for the decision support to. Interface for the decision support tool, which would go beyond something like an alert for a high risk score, which wouldn't tell the clinicians what they needed to know. It was just the kind of indicator to get started on additional research. But she identified based on the interviews different pieces of information about the patient that the clinicians most likely need to see together with the risk score to contextualize. With the risk score to contextualize it and to understand whether it needs new action or not. So, again, the model itself was just a start of a conversation, basically. And the output of the model would trigger a whole chain of investigations that were necessary to even decide whether to do something about the score or not. And that was at least on the self-service. At least on the self-reports of clinicians, unrelated to the accuracy of the model. So, however accurate the model was, they feel pretty certain that it would be the same process of obtaining additional evidence to decide whether or not to act on it or not. So, here are some of the implications again: is that the predictive model itself is a The predictive model itself is definitely a useful additional input into clinical clinicians' reasoning, but it's not enough to lead to any specific actions in terms of patient care. And that the accuracy of the model is definitely important, but higher accuracy, particularly in this case, was not likely to actually change anything in the inclination's willingness to trust the model output. To trust the model output in itself, the importance, and again, we've seen this in many other studies, is where the explainability of the model is important, but it's not the explainability of the model. It's using the model as a framework to explain the specific patient case. What they needed is an explanation of what's going on with the patient rather than an explanation of how the model works. How the model works. So that's the second case. The third case is a little bit outside of this area. Didn't ask your question. Yes. So when you said a thing like this is not needed and this is needed, which the explainability of a model or the explanation process from a model? Like what would be an example of the type of explanation that they would want or the kind that is insufficient? or the kind that is insufficient. So why in this specific for this particular, what is it about this particular patient that led to this high score or this particular trajectory? So there's a model with inputs. If the model said that, you know, this score is high because these inputs, because of like these inputs are doing this thing. Yes. So it's a So it's a again, because this is a a sort of a machine learning model, it doesn't have necessarily any meaningful representation of physiology inside. What I think needed a little more is understanding which of the inputs actually led to the particular output that they were looking at. So, if it were like a logistic regression, would you kind of know what to do? Because you kind of have like the coefficients, right? This thing is important, this thing is important. The coefficients, right? This thing is important, this thing is important. I think the relative contribution of different covariates would be the question there, right? So for this particular patient, what triggered the specific trajectory? Because not all of them will play the same role. How much of the hesitancy to use the risk score do you think is just not being useful, used to the risk score? Like some of those clinical markets. Of those clinical markers, they've used them 10,000 times. So they have some idea of what they work, how they work, and they're very comfortable with them. And so some of it is a matter of just having used it a lot and it's very common versus we do and aren't certain. Well, perhaps, but also I think at the same time the severity of what they're supposed to do. Right? Because if you trust the lowering risk score in discharge. In risk score and discharge the patient sooner from the ICU, and you're wrong. That's really a bad thing to do. If you trust the risk score and shows you that there is a high risk of DCI, you drill a hole in patient skull. So that's not something you do lightly. Even to test for it, you still, it's a very invasive procedure. So this is where it's like, it really better be right. But if they got used to it being right, like it's Being right. Like, in other words, accuracy might matter if they it's it's like in reinforcement learning. The question is: do you have the luxury of this initial time to demonstrate that the model actually is right to get to over the threshold? So, which may or may not be the case. So, let me get to this other, the last part because it is also a fun project. So, this is outside of clinical settings. So this is outside of clinical settings and healthcare. Oh, sorry, Alina, just a quick follow-up on the question about logistic regression. At least in my experience servicing CDS models in electronic health records, it's not so much the weights, but the values of those parameters at that time. That, even though clinicians understand conceptually that it is sort of the multiplication. That is sort of the multiplication of those two things that are relevant. What they really want to see is: oh, you know, the last CT scan was X at a given time, and not that the beta for a CT scan was 3.3. Because that's the framework that they think in most of the time. Yes. Yeah. Very good point. Thank you. Yeah, also the clinical art, like to add to that. To add to that, the clinical artifact. Whatever that thing, whatever that feature is. Test score. Unseeded test score. Right. Model parameters are sometimes interpretable when you know what they mean. And test love results are interpretable by clinicians because that's what they work. Because that's what they work with. You could do that, right? I mean, like with a logistic regression. It's like a sum of a couple things, and you pick the biggest one, and that came from an input, and then you just show them that input. Is that something that's not even close to being enough? No, so we've done, like, I'll probably be presenting the concern study, and we did that in our study. But I think I would also add that Would also add that to connect it to what you're asking, but also what David said, is that with prediction, sometimes, and I don't know if this is the case in this study, it's a new paradigm shift, and clinicians might not know what to do with that prediction at that time because they've never been trained, they don't have mental model of what to do with this thing that they predicted. This thing that they predicted they haven't necessarily seen the physiological manifestations of it. That's another issue. Yeah. I agree. It's a bigger question and maybe outside of this, but I think that the whole ecosystem of reasoning with new types of information will have to evolve over time. Because as humans, we know what to do with data and we. Do with data, and we've learned like when I'm gonna weigh over it, but when new test result lab procedures and results become available, they're not in themselves interpretable until you establish benchmarks of what is reasonable, what's not reasonable, and what are the implications of different things. So, I think the same thing goes for any kind of computational output that's not going to be necessarily informative until there are benchmarks of Until there are benchmarks of how to interpret it, what to do about it, what's normal, what's not normal, things like that. So, let me quickly do this and then we're going to have more of a discussion. So, here the question is less about the specific modeling techniques and even the specific decision-making sort of context and more general human reasoning, which we also need to take into account when we introduce these kinds of models into Use these kinds of models into decision support tools. So, here we are, again, a lot of my work is in supporting human decisions, in self-management, so nutritional choices. If we tell people reduce carbohydrates in your meal, can they even tell between choosing between two meals which of those meals have high carbohydrate and lower carbohydrate? That's not a very straightforward or easy task. I sometimes cannot tell that. So, what if we use So, what if we used an AI system to make suggestions to people? Let's say it's a complicated model or some kind of model that can take a lot of annotated meals with different nutritional breakdowns and learn, just based on images and descriptions, just write an NSF proposal for that, can learn how to assess whether meals meet goals or not. And then we can make a suggestion for a person. This meal. Suggestion for a person. This meal has more carbohydrate, this meal has less carbohydrate. How would they take these suggestions and how would they understand them? And more importantly for us here, the question, and us is Christian Kais, who's a professor in computer science at Harvard. More importantly, the question for us was, can people learn from it? If we're starting to tell them, look at these two meals, this one is higher in carbohydrate, are we creating a dependency on their company? A dependency on their computational assistance, or are they? Can we use that to help people learn something and become better at nutritional judgment themselves? So that was the question in the study. We did it on a, Krzysztof and his colleagues are running this platform called Lab in the Wild, where millions of people come to this website to participate in very short experiments. Two meals are presented, and the question is, which meal has more carbohydrates? Which meal has more carbohydrate or whatever other macronutrient? The meals are very carefully chosen to have one important difference. They're very similar otherwise, but there's a key ingredient. There are beans in this meal, it's this salad, and there are no beans in that salad, which sort of tilts the scale for carbohydrate in this one specific meal. So, what we and then we I'll show you the conditions as The conditions, sometimes we give those suggestions from the intelligent AI system. We didn't even build it, we pretended that it's there, so it's completely simulated. We constructed the test so that there were three times individuals exposed to the same ingredient. So we give them a chance to learn. For example, that beans are high in the carbohydrate. Soybeans have more proteins than other kinds of beans. Proteins than other kinds of beans. So, those kinds of things, where again, it's the same ingredient they're exposed to three times. Pre-test, we learn whether they know it already or not. The actual intervention, they receive assistance, and then the pre-test, we try to see have they learned something from it. And the simulated meal assistant would give them something like this: the meal assistant thinks that this meal and high in fiber. Higher in fiber. And we also constructed explanations for this. For example, reason: corn contains more fiber than other vegetables, particularly green vegetables like cucumbers. So that was the idea, and we were interested in whether people make more accurate decisions when they're receiving this kind of assistance and whether they learn over time something about the ingredients that they're exposed to. So I'll just quickly run through the results. So, I'll just quickly run through the results. The first condition was: we just repeated it with no assistance. As you can probably imagine, just repeating the same thing again and again when you don't have any feedback doesn't really help very much. So, no improved accuracy, no learning. The next condition, we gave them correctness to feedback. So, they made their choice and then they receive an answer. Hmm, not quite, and this is an explanation. So, you made a choice. And this is an explanation. So you made the wrong choice, and this is why. Or, yes, that was the right decision, and this is why. And the explanation, I can talk about how we constructed those, but I'll move forward. So, this was a great condition. We actually knew that this works because this improves accuracy and this leads to learning. The next condition is where basically explainable AI is at the moment. We give them At the moment, we give them a suggestion from the AI system and provide an explanation. The AI meal assistant thinks that this meal is higher in carbohydrate in something protein, and this is an explanation for why. Funny enough, improvement in accuracy, no learning. It's an interesting thing, and I'll talk about why we think it's the case. It's the case. The last condition is a tricky one where the meal assistant provided an explanation, but no suggestion. So it gave them an explanation for one, why one of these meals may be higher in what? In protein. Cheese has more protein, cream cheese is typically just fat. And it asks people to make their own decision. Yes. Was that an option? Was there repetition? So you did the method? All of this, if it's cheese, cheese shows up three times for this particular macronutrient. This was the only condition where they both improved accuracy and learned something. So what we concluded from this is that there's no learning with recommendations when they're accompanied with explanation. When they're accompanying with explanations, we attribute it to something called cognitive engagement. So, this is a construct in cognitive psychology that shows how actively individuals engage with information. Basically, in our case, it would mean that nobody read those explanations, or if they read them, they completely ignored them. Which means that if we are hoping that explanations will change how individuals engage with AI, With AI, we better think again because to get something from explanations, people need to read them and engage with them and think about them. And that's not necessarily the case. We can force this engagement by showing explanations only, but that has a consequence of a cognitive burden. Now, imagine a decision support system which every time would just hint at the right answer, but not give you exactly. So, yes. Exactly. So, yes. Not necessarily the universally best solution, but all right. So here's my conclusion for all of this. How can I finish this? This is it. Yes. So we talked earlier today about how important it is to align models with data and with the real processes that they represent. I would like to add to it a decision context, which means that it's not Which means that it's not either or. The models need to be aligned with the data, the processes they represent, and with the decision context where they're meant to be used. Because if not, the improved accuracy of the models is not necessarily going to have an improved impact on decisions. And the best thing to do is to incorporate all of that while developing the models, rather than using the approach where Using the approach where we develop the model and then we somehow build tools on top of these models because that really limits the ability to make the models appropriate for decision-making contexts as well.