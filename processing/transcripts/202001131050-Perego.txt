Here I'm listing all the Sandia national labs and many people working on this at the national lab. These are my main collaborators at Los Alamos National Lab. There are other people at Los Alamos involved in this project. And then George Stadler from NYU. From NYU and another cooperative from Emory University. So I think this is the first time where I don't have to put a motivation slide and I'm glad that I don't have to do that. So straight to the topic. So I sheet initialization. So what I mean with that is that you want to find the initial or present day state of your ice sheet, and possibly that's And possibly that's like the thermomechanical state, not just the mechanical state, and estimate the unknown or poorly known parameters. This is the goal of the initialization. I think that a good initialization, and you can argue with that, should be consistent with these three things. And I couldn't find a better word than consistence because it also depends if you can say constraint. If you can say constraint, but then some mathematician would argue with that. So, anyway, so consistent with the observation. And observation, I mean like the geometry, surface, like observed velocity and other things that you can get from observation. Then, as Steve was pointing out, you also want to be consistent with trends. Mainly, I'm thinking about the thickness tenderivative, the thickness tendency. But in principle, if you had also like In principle, if you had also like the time derivative of the temperature, you could be consistent with that as well. And then, the other important thing is that you want to be consistent with the ice sheet model that you are using. Because depending on the model you are using, you would come up with different initialization. So, the typical fields that we are going to estimate as a result of this initialization would be the basal friction, or better, if you have a hydrology model, you should estimate the Should estimate the bus electrology parameters. Bed topography, geology parameters, if you wish, and maybe geothermal heat flux, and you could add to this list. So these are the few that are probably the most important. Disclaimer, I'm not talking about very important issues here. I I'm sure that will be addressed later in this workshop. So the first thing I'm not talking about is the availability or reliability of the observation. The reliability of the observation. So, for this talk, I'm assuming that you have really nice data, unbiased, and they come with some measurement of the uncertainty, like some root mean square error or some standard deviation. I'm not addressing probabilistic interpretation like Bayesian inference and certainty quantification, and I know that people will do that tomorrow and maybe later today. I'm not looking at pondering. I'm not looking at model reduction. So, here I'm looking at the large-scale problem as it comes out of the basically your model. Of course, it might be useful to make model reduction when you do initialization, but I'm not addressing that. I'm not talking about machine learning, except for PD constraint optimization. Depending on how you define machine learning, you could include that or not. So, just briefly, the equation mainly for notation purposes. So, we're solving these tox So, we're solving this tox problem, use the velocity and use the viscosity of our shape-thinning fluid. Sometimes when you do initialization, it might be useful to come up with a parameter, a tuning parameter, which is a stiffening factor, which is something that multiplies the viscosity. And the idea is that if your rheology model is not really perfect, then you can try to multiply this by this to To multiply this by this to this scalar, that somehow helps you like helps you with mitigating the error that we have in the model. Quick question on that? Yeah. Is there a reason why you don't put the vertical dependence in the stiffening place? Simpler. And also, I guess time dependence, same answer. Yes. Okay. Yeah. I mean, wait, no, time dependency, you would need a note. So this is for finding the initial state. Finding the initial state. And so, well, okay, we'll address that later, but you cannot predict the future, right? So, either you have a model to evolve that stiffening parameter. If you think of that like damage, then maybe you do, but in principle, you don't have notion of time there. So, and these are the other equations. So, this is already Jan presented this as well. Jan presented this as well. And so this is the evolution of the thickness. Unfortunately, I'm using H for the enthalpy. So this probably is not a great notation. But anyway, so the small h is the enthalpy and capital H is the thickness in this stock. And so this is the typical equation. So I think here I think here I have some early bibliography of people working on initialization. I should have had many papers, so I really don't want that to be exhaustive. But I think that nowadays most of the people agrees that it's good to, a good way to do initialization is by using a PD constraint optimization. And often this is introduced in a BayAsian framework. And there are alternatives to this. Some adoc methods that were mainly used like maybe a decade ago. Then another alternative would be using uncented Kalman filter to do data simulation. I think this is nice because it gives you a probability framework and so your initialization comes with some statistic associated with it. comes with some statistics associated with it. The issues that it's typically, I'm not an expert at this, but man standard is expensive. And also the issues that the number of forward solves that you need to do the initialization scales with the number of parameters. So if you have million parameters, you will need a million solves. And that's not scalable for our problems. If you have a reduced order, if you use a reduced model and you come up with hundreds of thousands of parameters, then maybe that's a viable option. Bayesian calibration. Bayesian calibration, I know many people here work on that, but if you take it in the brute force way, so Marconche, Monte Carlo, then it's also unfeasible for our problem because of the cost of dimensionality. Of course, you can be smart about that, and I'm sure Noemi and Omra will address that, but this is also, yeah, plus I said I was not addressing Bayesian problem. Bayesian calibration Bayesian problem. So I'm not doing that. So in terms of the optimization, there are two main philosophy. One is like performing a transient optimization. There are very good work by Goldberg and Heinberg in this framework. And the other one is steady optimization. And it's the one I'm focusing on. So in the steady, in the transit optimization, you assimilate the data as they are available in time. Available in time. In the state optimization, you kind of assume that all the data are taken now and your present day, and you try to find an initial steady state solution of your problem. When I say steady state, it might be doesn't really necessarily mean steady state. You might have measurements of the attendancies and you match them, so it's not exactly steady state. But the idea is that. Exactly steady state. But the idea is that there is no time involved in the initialization. So this is like a typical PD constraint optimization problem. This is something that almost everybody has done, I mean all of the big groups working in modeling. So the idea is that you find the friction at the bed. And in this case, I'm using like a linear row for the friction. linear row for the for the friction for the basal friction. And the idea is that you find you try to find this parameter that minimizes this functional. And this functional in this case is just a mismatch with the observed velocity. And this is subject to your ice sheet model. Typically Stokes, Ristord, Shallow Ice, whatever is your flow model. So basically the idea is that you find this parameter such that the Parameter such that the velocity computed with that parameter is minimized the mismatch with the actual observed velocity. And in this case, you assume temperature is given, and also you assume that all other parameters like better topography are known. Another approach that typically people use is, as I said before, is to try also to tune this stiffening factor. So that gives you a way. So that gives you a way to mitigate the issues with the rheology model. And so you have this extra term, fees, this stiffening factor that in principle should be close to one if your model is exact. And you allow this to change. Also, I should point out before Georg reminds me again. Here I'm using all like L2 minimization, so like the square, you're minimizing the square of this mismatch. You could do like Of this mismatch, you could do like something different. You could do, for example, L1 minimization when instead of the square, you minimize just the absolute value of this difference. In that case, you end up having phi almost everywhere close, well, close to one in many places. And then you have some sort of, I didn't mention that before, and then you also some sort of regularization term. I'll go back to this at the end of the talk, but the idea of the This at the end of the talk. But the idea of the regularization term is that you might have an infinite solution, if you need to problem that minimize this function. The regularization makes it unique by asking the solution typically to be smooth. You're asking your friction coefficient not to be too smooth, for example. Again, in this case, again, temperature is given. Very quickly, I'm not going to spend time on this. I'm not going to spend time on this. All the simulations you see are done with the Mali model, and in particular, which is basically made combined into models, MPAS, which is developed at Los Alamos and Albany, which is a finite element library developed at Sandia. It comes with a lot of functionality, non-linear solvers, and automatic differentiation. And I'm sure that Irena will present. I'm sure that Irena will present this way better than this, so I'm not going to spend much time on this. But anyway, the part that I'm presenting today is mainly related to the Albany part, which is the finite element solver. This is some of the results. In this case, we are just, this is the optimization one. So we are only inverting for the basal friction matching the velocity. And so this is the target velocity. And so this is the target velocity, the observed velocity, and this is the one we computed by minimizing the problem. And this is the estimated basal friction coefficient. Yeah, I don't want to spend too much time on this. This is another problem done on Antarctica. In this case, we are also minimizing this stiffness, or here I call it softness, but they will sorry. The stiffening coefficient. Stiffening coefficient. And again, this is the estimated basal friction. This is the estimated stiffening coefficient, which is almost everywhere close to one, but then there are some places where it's not. In particular, it's nice because you see in some places really sharp places where you probably associate it with sharp margin. And then this computer velocity and on the Computer velocity, and on the right, you have the data, the observed velocity. Marva, what do you need the temperature? Yeah, temperature is given at this point. So we have some other, I don't even remember. Yeah, Frank Patton. In this case, yes, Frank Patton. Have you considered? That's the one issue that probably is here. That we kind of using a temperature, not only is not being computed with a model that's consistent. Being computed with a model that's consistent with our. And then, of course, we keep it fixed, so we change the Bessel friction, but we don't then update the temperature. Of course, the way you would use this data is that once you have this, you would do a spin-up. And then so that you actually come up with the reasonable temperature field. And the problem is that at the end of the spin-up, the the joint you started with will won't be the same. So that's Won't be the same. So that's one of the biggest issues with this sort of initialization I presented. And so, in fact, some of the, here I'm actually reporting that. So, one is the second one, as you said, it's not consistent with the temperature. It's not consistent with the temperature. And the other thing is that it doesn't match trends, doesn't match in this case the thickness tendencies. The issue with that is that if you use The issue with that is that if you use that initialization as it is and you start marching on with your forward model, you will notice very fast transients that are not physical. And if you are interested in modeling the ice sheet in, I don't know, 10,000 years, probably you don't care about that. But if you're interested, and we are because of the OE funding, and not only because of that, interesting. At the, like what happens in FIFA. at the like uh what happens in fifty years then then those are really critical so you cannot you those are and and then if you do a spin-up then then you don't really know what would happen. Yeah. There's also the fact that in fifty years you can be proven wrong. You can't be proven wrong. You're 10,000 year. That's a main factor I think. Yeah. Thanks for reminding me that. Yeah. I hope 50. Yeah, probably. I won't care that much. But let's move it to 100 years to be safe. Anyway, and then the other thing with these models is that the basal friction is of course not steady at time. So I already I mentioned these problems. In particular, you you know that the the the response of the the the response of the the uh of the ice to temperature changes takes takes really long time uh to to get to us to a steady state so it's really important to get the temperature right and uh and yeah I'm not kind of addressing this unless I have time but of course it would be nice to you in order to address the effective piece of friction is is it's not steady you actually need a like a hydrology model to actually model it. So uh I'll I'll talk about that maybe later. I'll talk about that maybe later. Okay, so this is try to mitigate those issues. First of all, we are adding this term here that represents the mismatch with the vergence flux and your apparent surface mass balance. So the idea is that if you have the observation of the fitness tendency, you try to come up with initializations such that if you You, you, if you, your uh, your sp the derivative or your thickness tendency matches the observation. And the other one, and of course, you would you add an extra term because now you have some observation of the thickness that comes from Matt. Usually it's from Matthew and in our case. And you, because we are sloping, we don't work directly with the raw data. And then you want your thickness to be close to the observation. Close to the observation that we have. And then the other very important thing is that now not only we are trying to subject our equation to the flow model, but also to an entropy solver. So, which means that these are the constraints. So, the coupled velocity and enthalpy solver problem. So, which means that when you come up with a solution that will be self-consistent with the pressure. For your enthalpy solver, you assume steady state? Yeah. I don't see where the enthalpy is. Sorry? I don't see where the enthalpy is at the equation. In the previous question. So you solve a PD constraint of transition problem. You have your constraint, which is what you solve actually, which is a coupled problem of velocity and enthalpy. Okay? So for every parameter beta and h, you solve the power. Parameter beta and h, you solve this coupled problem, which is coupled, which is velocity and temperature, and then that gives you the observed velocity, that gives you the thickness, sorry, the observed velocity. And then you sorry, this gives you the velocity, the computed velocity, and then with that you compute your functional. And you tune bit and h so that you minimize that. Does it make sense? So are you not using steps anymore at all? Not using stiffness anymore in that model, or sorry? You've dropped the stiffness parameter? Yeah, in this case, yes. You can have that as well. It's just that you try to not well adapt to index, but yeah. Isn't the stiffness parameter effectively making up for efficiency in the temperature? It's pretty fast. For sure. So in this case, I think it's less important for sure. The other thing, of course, is that if you, anyway, your rheology model might not be perfect, so you might still need some tuning. But for sure, yeah, it's less important. But for sure, yeah, it's less important in this case than previously. So, another way of asking exactly is in your previous model, can you take your information for the stiffness and compare it with your temperature field and ask how comparable they are? Can you take the question? So, if in your previous version you'd computed stiffness and you had a starting temperature field, could you just compare those two in terms of your effect of the temperature? Effective of the temperature beyond the reality. And is the temperature field consistent compared to the version of the research? You could, but if you don't include that in an initialization framework, then most likely your answer would be no. I think that's my take. So, anyway, I don't have time, but the thing is that if you really want to include the temperature in this initialization framework, you have to have a Framework: You have to have an enthalpy model that's steady-state. And it's not easy, I'm actually pretty proud of this, but you have to have to come up with some regularization of this is a diagram of what can happen at the bed of your... And I have to thank both Ian and Georg to help me understand this thing. So it's basically the diagram what you have at the, what happens when you're at the bed and you can have like At the bed, and you can have like melting, freezing, cold ice, and all these things. And you want to regularize that, otherwise, if you have discontinuity, you're not going to solve it. Here I'm basically showing the planar benchmark. I don't know if you're aware of it, but showing that as that parameter goes to infinity, well, to 20, you basically get the right answer. And this is the And this is the the other nice thing is that for all these cases we tried, even with the just ten layers in the vertical and alpha pretty small, the temperature is almost what on, even if the entropy and the porosity or water content is not sorry. Anyway, so if you do that, and these are already, I still presented this, so you can come up with a model. In this case, I'm not tuning the thickness, but I'm including the temperature. Including the temperature. And this is like fully consistent with temperature. Same thing here, you have computed velocity, the absorbed velocity, and there you can see the root mean squared error. Here you have the basal temperature. In this case, I'm assuming that the bed is dry, so I'm not allowing refreezing. I would need some model for hydrology to actually solve the real thing. And just another the last cover. Another last caveat is that in this case I'm not tuning, I'm not trying to match the observed surface mass balance, not trying to match the tendencies. And you see that this is basically this is what is the apparent mass balance and this is what you get from the numerical from the initialized state. And here I'm constrained between minus 7 and 7, otherwise it would be completely. Otherwise, this would be completely grey. But in reality, this goes from like minus a couple of hundred to a couple of hundred. So you have really bad transients if you don't do anything to avoid that. And so we are trying to work with that. We did something with this simpler case, and now we are trying to address the actual large-scale problem. Okay, so there are other things, but these are I'll leave you with this if you if you want, we can discuss it. We can discuss that. Thanks.