Large complexity, a complexity greater than a constant in the system size. What led me into this subfield or this connection, this network of ideas related to quantum complexity was actually an application in holography in the context of the ADS-CFT conjecture. So, ADS is a space-time geometry, and CFT is a conformal field theory, a quantum theory that we imagine living on. Quantum theory that we imagine living on the boundary of this space-time. In this space-time, one can have two black holes connected by a wormhole, and the wormhole's volume can grow linearly for an exponentially long time. And one can ask what is the dual to this wormhole in the conformal field theory, because it is known that there is a duality between these two contexts. But one can look at the conformal field theory's quantum states, and one can measure local observables and even. And measure local observables and even kind of strange correlation functions that track quantum chaos. But it turns out that these quantities reach their equilibrium values over much shorter time scales. The wormhole growth paradox is, how is it that supposedly there is this duality between these two pictures, but there's this important quantity, the wormhole's volume or length, that grows linearly in time for such a long time. And there seems not to be any analogous quantity. Seems not to be any analogous quantity in the context of the conformal field theory. A resolution to this paradox was proposed in the complexity equals volume conjecture. This states that the dual to the wormholes volume is the complexity of this conformal field theory. And after this conjecture was put forth, the quantum state's complexity was even conjectured to have other significances in the context of this duality. Significances in the context of this duality. People have conjectured that the field theory's complexity is dual to a particular action in the context of this space-time, or I even recently saw a paper entitled Complexity Equals Anything. So however you slice it, however you expect complexity to show up in the context of high energy theory and black holes or Ethereum black holes, or condensed matter, or quantum advantage or supremacy, quantum complexity has proven a useful notion for thinking about recently. Here's the quantification of quantum complexity that I'm going to focus on. I'll call it the exact circuit complexity. It's the least number of two-cube gates that we need to implement a desired unitary. These gates don't need to be geometrically local. The importance of complexity in these different subfields, especially in high energy physics, led Adam Brown and Lenny Suskind at Stanford to pose two conjectures about complexity that since this paper first came out, the archive version first came out in 2017, have gained quite a bit of prominence. First, the first conjecture answers the question, Mary Mary, quite contrary, how does your complexity grow? The conjecture is that Conjecture is that under some typical random chaotic dynamics, quantum complexity tends to grow linearly in time for a time that's exponentially large in the system size. And this is, if this conjecture is true, then it would make sense for the complexity of a quantum state to be dual to the volume of a wormhole in that spacetime, because that wormhole's volume, what has been shown. Holes volume has been shown to scale in this way. The second conjecture was that we can define a resource theory for uncomplexity. This statement involves a couple of pieces of jargon, so let me explain those. First, uncomplexity. Let's take this notion of the exact circuit complexity of a unitary and deform it a bit to define the exact complexity of a quantum state. This is the difficulty of preparing a desired state. Of preparing a desired state from a simple tensor product. So the uncomplexity is the maximum possible complexity minus the actual complexity. It says gap. Uncomplexity is useful. For instance, we use simple tensor products as inputs to quantum computations as clean scrap paper. So we need states that are close to this simple tensor product. And so a state's uncomplexity or closeness to the all-zero. Uncomplexity or closeness to the all-zero state is what we might call a resource. There is a construct in quantum information theory for quantifying resources, of course, called the resource theory framework. A resource theory is a simple information theoretic model for any situation in which the operations we can perform and the systems we can access are constrained. For instance, we can't, during a quantum computation, easily access all of the zero. Easily access all of the zero qubits we could possibly wish for, we have to spend work in order to cool qubits down to their zero states. The resource theory framework is actually one of my home bases. So this conjecture by Adam and Lenning is actually why I made it into the field of quantum complexity. We can use a resource theory to formalize operational tasks like the distillation of high quality entanglement from low quality entanglement and calculate the optimal efficiencies with which we can perform. Optimal efficiencies with which we can perform these tasks. I've had the privilege and pleasure of working with a wonderful team of people to prove these two conjectures. Jonas Hofferkamp, Philippe Feist, and Teja Kotakonda have been working in Jens Isert's group in Berlin, and Anthony is in my group in Maryland. This paper at the top contains the proof of the linear growth for an exponentially long time of quantum complexity. Long time of quantum complexity conjecture. And this second paper contains the construction of the resource theory for uncomplexity. I'm going to focus on the first paper so that we have time to go into some detail. But here's where I'd like to go in the rest of this talk. First, I'll discuss why this problem is hard and what has been done so far. Then I'll describe the setup and introduce a set of terminology and a particular mindset that will be useful for solving this problem. Be useful for solving this problem. Then I'll state the main theorem and sketch the proof, talk about a few extensions and opportunities for future work. Lower bounding of quantum complexity for long times, exponentially long times, is difficult. It has been attempted for many years by many people. And people have found some workarounds or proved results in special cases, for instance, by focusing on short times or by Short times or by focusing on high-dimensional subsystems rather than qubits. People have also used the toolkits of unitary t-designs, which approximate uniform distributions. And sometimes people just assume that circuits lack collisions. Here's what I mean by that. And here is why it's difficult to lower bound quantum complexity. Suppose that we're performing a circuit. We're performing a circuit. We perform one gate and the complexity increases. We perform a C naught gate entangling these two qubits, then complexity increases. Then we could perform another C naught gate right here. These two gates cancel each other. We say that they collide. And from here to here, the complexity actually decreases. So later gates can cancel earlier gates, and the complexity can decrease. We expect that collisions are rare. Collisions are rare. That is difficult to prove, but of course, it's worth trying to prove. Here is the approach that we took to proving it over certain time scales. We're going to focus on a system of n qubits. I'll assume for convenience that n is even. We'll imagine these qubits undergoing a unitary circuit in which each gate is selected har randomly from SE4, the set of From SU4, the set of all two-qubit gates. Again, these gates don't need to be geometrically local. And in fact, the results generalize from the HAR random measure to just any measure that is absolutely continuous with respect to the HAR measure. So wherever the HAR measure is zero, any other measure that you use must be zero. This model of a random unitary circuit. Random unitary circuit captures features of quantum many body chaos, so it's been used quite a lot over the past few years in order to model chaos. I'll denote the plus one eigenstate of sigma z by zero, and I'll sometimes notate tensor products like this. Let's introduce terminology, a set of terminology, and a mindset for thinking about these random circuits. About these random circuits. Let's say that an architecture, which I'll denote by A, is an arrangement, a layout of a fixed number of gates. I'll call that number R. A very common example of an architecture is called a brickwork architecture. You're probably familiar with it. In the first layer, a gate acts on qubits one and two, a gate acts on qubits three and four, and so on. In the next layer, a gate acts on qubits two and three, and so on. And then this coupling pattern. On, and then this coupling pattern repeats itself. If we take an architecture and we choose particular gates to slot in, then we get a circuit. If we contract the circuit along these lines, then we get an n qubit unitary in SU2 to the n. This contraction is implemented by some contraction map that depends on the architecture. Suppose that you insert two vertical cuts into a circuit. The gates in between the cuts, I'll call a block. Now, block can contain a backward light cone. It does if there is one qubit, which I'll call T, that connects to every other qubit via some path of gates. And the path of gates that connects T to some other qubit, call it T prime, might be unique to that other T prime. Be unique to that other T prime. For instance, this qubit enters this gate and so connects to this qubit, which enters this gate and so connects to this qubit and so on and so forth until qubit N has connected via a path of gates to qubit T. All of the gates in these paths form the backward light cone. So if a block contains a black backward light cone cone cone cone cone A block contains a black backward light cone, then that block is, in a sense, well connected. Does anybody have questions at this point? Okay, then I'll move on. We can now state the main theorem. Let A denote any architecture that we can form by taking T blocks. Can form by taking T blocks, each with at most L gates and each with a backward light cone, and concatenating those blocks together. In one very simple example, we can imagine taking a block that has a brickwork pattern and that has at most L gates for an appropriate L and taking N copies of this block. Again, that is only an example. Again, R denotes the total number of gates in the circuit. Of gates in the circuit. This architecture can implement each of many different unitaries that you get from slotting in different choices of gates. So let you denote any one of these unitaries implementable with this architecture. The unitary's exact complexity obeys this lower bound. The lower bound is the total number of gates over nine times the maximum number of gates per block minus the number of qubits over Minus the number of qubits over three. This lower bound, importantly, depends linearly on r, the total number of gates. The total number of gates grows linearly in time. And so this lower bound does grow linearly in time as we wanted, in accordance with our conjecture. Furthermore, this bound holds for times exponentially large in the system size. And that's what we really wanted to prove in accordance with our conjecture. To prove in accordance with our conjecture, there is this term subtracted off here, but it's not very important. Again, it's exponentially long at exponentially long times that this statement is particularly difficult to prove. At exponentially long times, this total number of gates is exponentially larger than the system size. And so this term will far outweigh this term. Could I ask a question? Yep. So does this bound hold like with high probability or? Uh, like with high probability, or since probability one, okay. I see. Uh, in the limitation, I'll show sorry with probability one, and I'll show in a proof sketch why it holds with probability one. I'll also show how you can eliminate this assumption and turn the bound into a probabilistic one. Cool, thanks. Sure. Any more questions? Okay. The key idea behind this proof is an object that we call the accessible dimension of an architecture. Again, we said that an architecture A is a layout for a set of gates. If we slot gates into the architecture and we apply the contraction map F of A, then we get a unitary. Consider the set of all such unitaries that you can implement using this fixed architecture. That set of all those unitaries forms the image of the contraction map, F of A. I'll call that image telegraphic U of A. The accessible dimension of the architecture is a measure of the number of degrees of freedom that we need to describe the set of unitaries locally. That's the general idea. Locally. That's the general idea. To be more precise, I need to provide some background, basic background about algebraic geometry. We're going to use the toolkit of algebraic geometry and differential topology in contrast with the mathematical toolkits that are very commonly used in analyzing quantum complexity, namely Nielsen's geometry, which is a differential geometry of the space of unitaries that was introduced by. Of unitaries that was introduced by Michael Nielsen and collaborators, and unitary t-designs. So, this work introduces quite a different toolkit into this field. Suppose that we have a set of equations. It could have a set of solutions, and that set is called an algebraic set. An example of an algebraic set is SU4 times R. This consists of the sets of R2 qubits. Sets of R2 qubit gates. Why is this an algebraic set? Because it consists of the solutions to these equations, which specify that we're talking about a set of unitaries and a set of special unitaries. Every algebraic set is a semi-algebraic set. Suppose that we have a set of equations and possibly inequalities. The set of solutions to that forms a semi-algebra. Solutions to that form a semi-algebraic set. The Tarski-Seidenberg principle governs semi-algebraic sets. Let W denote any semi-algebraic set, and suppose that we have some polynomial map between the real numbers, F. Then if we act with this map on this semi-algebraic set, then we get another semi-algebraic set. In our case, W, the original semi-algebraic set, is going to be SE4. set is going to be SU4 times R, which consists of the sets of R two qubit gates. This polynomial map F is our contraction map F A. It's a polynomial function because it contracts two qubit gates, which are basically just matrices. So it performs matrix multiplication. And this resulting semi-algebraic set is calligraphic U of A, the set of all unitaries implemented. Set of all unitaries implementable with our fixed architecture. Anytime you have a semi-algebraic set, W prime, you can decompose it into a union of smooth manifolds. One of those manifolds has the greatest dimension. And the greatest dimension of any manifold in this composition is called the dimension of the whole semi-algebraic set, W prime. We said that U of A. We said that U of A is a semi-algebraic set. And so the dimension of U of A is what we're calling the accessible dimension of the architecture A. Again, its significance is it's a measure of the number of degrees of freedom needed to describe this set of unitaries locally. We're going to use this accessible dimension. We're going to learn about it. We're going to learn about it using algebraic geometry and differential topology, and from the accessible dimension, infer about the complexity, which is what we really care about. Here's a sketch of the proof. First, we want to lower bound the accessible dimension. This is the hardest step. It turns out that the accessible dimension is lower bounded by t, the number of blocks in our circuit. The proof involves, as I said before, algebraic geometry and differential topology. It also involves the construction of a Clifford circuits. The Cliffords are the operators that transform the Pauli operators to the Pauli operators to within phases. Let's say that P is an n qubit Pauli string, by which I mean a tensor product of n factors, each of which is a single qubit Pauli operator or an identity. qubit Pauli operator or an identity operator on one qubit. So if we take any n qubit Pelli string that is not just the identity, then we can transform it via some Clifford circuits into a Pali operator that acts non-trivially on just one qubit, which we can choose to be the nth qubit. And we can choose that for that action to be the action of the z operator. So we use this fact. And also that the n qubit Pauli strings The n-qubit Pauli strings form a basis for the space of n-qubit Hermitian operators. The number of non-trivial n-qubit Pauli strings is 4 to the n minus 1. And ultimately, this is why our bound holds for quote-unquote times or numbers of blocks in our circuits up to 4 to the n minus 1. Once we lower bound the accessible dimension, we upper bound the accessible dimension. We upper bound the accessible dimension. The accessible dimension turns out to be upper bounded by nine times the total number of gates plus three times the number of qubits. This claim is easier to prove, although it still requires some thought. We can prove it by a parameter counting argument. Now let's take these Lamata and put them together in order to prove the theorem. Let's review the theorem's assumptions since we saw. Review the theorem's assumptions since we saw them several slides ago. A is some architecture, any architecture, formed from T blocks such that each of the blocks contains a backward light cone and contains up to L gates. So the total number of gates is R, which is at most T the number of blocks times L, the maximum number of gates per block. If we have this architecture, we can implement a whole bunch of unitaries. They form the sets categorized. They form the sets calligraphic U of A. Now, suppose that we randomly pluck from this set any unitary U. What is the probability that we can implement this U also with a smaller circuit of only R prime gates, wherein R prime violates our lower bound? In other words, what is the probability that the unitary U has an exact circuit complexity? An exact circuit complexity that violates our lower bound. We're going to show that that probability is zero by using the two lamata that we just saw. Our prime, we said, is this lesser number of gates that violates our lower bound? We can express the total number of gates over the maximum number of blocks of gates per block in terms of t, the number of blocks. Then we can solve this inequality for t. This inequality for t, so that t is greater than nine times this lesser number of gates plus three times the number of qubits. We've already proved a lower bound on the accessible dimension, so we can chain these two inequalities together to find that the accessible dimension is greater than nine times the lesser number of gates plus three times the number of qubits. Now, consider any one of these smaller architectures. these smaller architectures, a prime that has only our prime gates. We can apply to that architecture our second lemma about the accessible dimension. Its accessible dimension is at most nine times the lesser number of gates plus three times the number of qubits. Now we can chain these two inequalities together. The accessible dimension of the smaller architecture is strictly less than the accessible dimension of the larger architecture. The accessible dimension of the larger architecture. That inequality is so important, I'm going to rewrite it. It implies that the set of all unitaries we can implement with that smaller architecture forms a subset of the set of all unitaries we can implement with the larger architecture. Furthermore, this subset is of measure zero in the larger subset or in the larger set. And we can prove this lemma using the And we can prove this lemma using the dimension theory of real algebraic sets. Therefore, according to this measure zero lemma, if you randomly pick a unitary that you can implement with one of these larger architectures, then your probability of being able to implement it with a smaller architecture is actually zero. And that is exactly what we said. Is exactly what we said we wanted to prove. Does anybody have any questions about that? Yeah, I have a quick question, Nicole. Okay. It's very interesting. So all these implementations are supposed to be exact implementation of the full unitary, right? Right. And I'm going to talk about approximation. I'm going to talk about approximations later. That was my question. Okay. But yes, everything discussed until now. Okay, but yes, everything discussed until now is exact. Sounded like there might be another question. I had exactly the same question. Okay. Any more? All right. We can extend this result in a few ways. One is we can lower bound not just the exact unitary complexity, but also the Exact unitary complexity, but also the exact state complexity. Let sides note our favorite pure n qubit state. Let's denote by C sub S the exact quantum state complexity, the least number of two qubit gates we need to prepare this state from a simple tensor product. We can prepare psi by preparing the simple tensor product and then implementing some unitary. If that unitary satisfies the assumptions of psychiatry, Unitary satisfies the assumptions of our theorem, then the quantum state complexity obeys the same lower bound, also for a time that's exponentially large in the system size. But the form of the exponential is a bit different because the space of states is parameterized differently than the space of unitaries. This extension of the main theorem proved useful in our resource theory when we wanted to prove some typical resource theory type results. Some typical resource theory type results about monotone functions. Now, suppose that we let the architecture be random. So we want to remove from the set of assumptions in our theorem the assumption that every block contains a backward light cone. And we can still prove a lower bound, but it becomes probabilistic. In an example construction of such a random architecture, we could at Random architecture, we could at each time step pick randomly a pair of nearest neighbor qubits, in addition to randomly picking the gate that we implement on those qubits. So at the first time step, we might pick these two qubits, then we could pick these two, and then these, and so on. With decent probability, these gates on their own form backward light cones, in which case there is a linear lower bound. There is a linear lower bound on the exact quantum circuit complexity. Here's the form of the lower bound. Here's the probability that the complexity of this unitary obeys this lower bound. What's really important about the lower bound is that, again, it depends linearly on r, the total number of gates, which is a proxy for the time. And what is the probability that this linear lower bound on the complexity is a. Lower bound on the complexity is obeyed, it's a high probability, one minus some correction. As a matter of fact, this is a family of inequalities because it's parameterized by this number alpha that runs between zero and one. In an example, to get a feel for what this parameter does to this family, let's suppose that alpha is large. Then this lower bound is not suppressed, it is quite large. Not suppressed, it is quite large, and so the probability that the complexity obeys this lower bound turns out to be suppressed accordingly. Because if alpha is large, then this denominator is small. So this correction is large. So we have one minus something largish. And so if alpha is large, then the lower bound is not suppressed. And as a trade-off, the probability is relatively low. Probability is relatively low. As you might guess from the form of this inequality, a key tool in the proof was Chebyshev's inequality. Now we get to approximate circuit complexity. Suppose that there is some unitary that satisfies the assumptions in our theorem, and we would like to approximate that unitary with another unitary, a U prime, that we can implement with a shorter circuit. That we can implement with a shorter circuit that violates our bound. Then, according to our result, this approximation u' probably has a large distance measured, for instance, in the Frobenius norm from the target unitary. For all values of some parameter delta, there exists an epsilon such that with a high probability, one minus delta, the Frobenius distance between The continuous distance between the target unitary and the implemented unitary is large, at least epsilon. Well, I say large just to give intuition for what this result is like. But as a matter of fact, unfortunately, this epsilon can be uncontrollably small, in which case this statement isn't extremely interesting because it could basically just say that the distance between these unitaries is at least approximately zero. Approximately zero, which again isn't too interesting. Why is this result undesirable in this way? We're saying consider the set of all approximating unitaries that are implemented with a lesser architecture. And we're saying if you implement one of the unitaries in this set, it's as though you've implemented a unitary in a larger set. A larger set, a set of all unitaries that are close enough to the set of approximations. So it's like we're taking this set of unitaries implemented with the smaller architecture and expanding it in all directions in the space of unitaries. So if we implement one of these unitaries with a smaller architecture, it's as though we're implementing something nearby. It's as though we're doing something as good as. We're doing something as good as implementing a unitary that's nearby. And that effectively expands the set of unitaries in all directions in the unitary space, which unfortunately means that the accessible dimension, that very useful conceptual tool, the accessible dimension of this set, leaps to its maximum value. And so that very important inequality that I wrote twice in the course of that proof sketch is generally not satisfied. So it seems that the accessible dimension, while it was extremely To mention while it was extremely useful for nailing down the desired statement about exact circuit complexities growing linearly in time for an exponentially long time. The accessible dimension is just too crude a tool for getting a handle on the bound for approximate circuit complexity. But that leads us to opportunities for future work. It would be wonderful to extend our bound from exact circuit complexity. From exact circuit complexity to approximate circuit complexity. One might be able to do this by not only invoking the accessible dimension of these two sets, the set of unitaries that form the target unitaries implemented with the actual architecture we have in mind, and the set of unitaries implementable with a smaller architecture. But also reason about how these unitaries are spread out in the space of all unitaries. Are spread out in the space volume enteries and how they overlap. If we can strongly lower bound the approximate circuit complexity, then we can lower bound Nielsen's complexity. I mentioned briefly that Michael Nielsen and collaborators came up with a measure of quantum complexity that is based on differential geometry. This is a measure of complexity that is very much beloved to high energy theorists because they love working with differential geometry. Love working with differential geometry because they're so used to working with space-times. So, this such a result would be useful for bringing the two communities from quantum information theory and high energy theory closer together. Nielsen's complexity has been shown to upper bound the approximate circuit complexity. So, if we can lower bound the latter in task number one, then we can lower bound Nielsen's complexity as well. Third, this Third, this notion of the accessible dimension turned out to be very useful in proving much-desired statements about a lower bound on the exact quantum circuit complexity. So the accessible dimension might be a useful concept generally in quantum many body physics. Again, it's a very different sort of mathematical tool from the tools that are usually used in this sphere. And so possible applications. And so, possible applications could include any of the hot topics in quantum many body physics, such as that involve random dynamics, such as Brownian circuits, and hybrid circuits that involve both random unitaries and measurements. Also, I didn't talk much about the resource theory for uncomplexity, but that comes with its own set of opportunities. In summary, we saw that quantum complexity has been a useful concept finding relevant Been a useful concept finding relevance across many body quantum physics. Adam Brown and Lenny Suskins posed two prominent conjectures in their paper in 2017. First, about the growth of quantum complexity under random chaotic type dynamics. They conjectured that it grows exponentially in time for, excuse me, linearly in time for an exponentially long time. And second, that we can define a resource. We can define a resource theory for uncomplexity. I claimed that there are two proofs and showed a sketch of one of those proofs because we focused on the linear growth conjecture. We used the toolkits of algebraic geometry and more implicitly differential topology, especially introducing the notion of unaccessible dimension, and saw that there are more opportunities for the future. For the future. So, thanks for your time. All right, thank you, Nicole. Are there any questions? I see a question from Simona. Hi, yes. So, thanks for the nice talk. Actually, I have a bit of a technical question concerning the broad. Concerning the probabilistic input, as far as I understand, somehow more or less your first proof or first statement which you show to us is almost deterministic, almost in the sense that the only thing which you really need there is that the measure, your a priori measure doesn't concentrate on a lower dimensional manifold. Concentrate on a lower dimensional manifold. Right? I mean, that's kind of what you use there. And intuitively, somehow you counter this problem of sort of annihilating gates, as you showed to us in the beginning with these C0 gates. I can be that you have a C0 gate, I can be in another C0 gate. gate and we add another CNOT gate and that just you know doesn't increase the complexity obviously by having this crucial backward light cone assumption right I mean that was me somehow the I don't really understand that completely intuitively but that seems to be somehow the the hammer which make makes it all Takes it all. Now, in the second result, and that's where my question goes. In the second result, you had a probabilistic estimate on whether this backward light cone assumption is true probabilistically before somehow your randomly selected gate. So, can you? I mean, you said somehow it's essentially a Chebyshev inequality. Can you say a bit more? Is it kind of a Chebyshev, you know, Chebyshev inequalities? I can use Chebyshev in me kind of the stupid ones, I mean, with a variance estimate, or more sophisticated ones with exponential Chebyshev. What kind of a Chebyshev? What kind of a chapy shift are you using there? Let me share the relevant slide again. Okay, so to confirm what you said earlier, yes, the main result holds with a probability one. It is important that we assume. It is important that we assume that each block contains a backward light cone. And you mentioned collisions. So our result basically shows that on the time scale of blocks, so between blocks, collisions don't happen, even though we can't say anything about collisions within a block. But it's sort of, if you look at the evolution on a coarse grain scale, then you won't see. Then you won't see collisions, and that's why the complexity will grow with linearly with R, which is the total number of gates, which depends on the number of blocks. So for you, collisions are just immediate in annihilations. So a simple example of a collision is what I think you might be calling an immediate one, as in you perform a gate, and then the very next gates. The very next gate reduces the complexity and undoes the previous gate. But you could imagine more elaborate collisions, like a C naught way over on one edge, and then a C naught on the opposite edge, and then a C naught that undoes the very last or the C naught on the second edge, and then a C naught that undoes the first C naught. But it turns But it turns out that those collisions don't happen on this more coarse grain scale of looking across blocks. So again, this is the form of the inequality. I guess to assess which particular sort of or sort of quality. I mean, I have no feeling how complicated in me sort of this estimate really is. You said it's basically a Chebyshev. It's basically a Chebyshev-Markov estimate. And we saw, I assume, you know, one of the questions is: what kind of Chebyshev-Markov estimate is it? Yeah, to discuss that, to tell the truth, I haven't been looking at this paper a whole lot in months. So I'd have to go, and this is, I think, sort of a detailed matter of one of the appendices. So I'd have to go back into the appendix and re-familiarize myself with the details of the proof. Maybe we could discuss that kind of on the side. Sure. Sure. Are there further questions? Yeah, can I ask a quick question? But I mean, it's kind of a little bit naive. So you mentioned that they are using semi-algebraic sets, using Tarski-Zeldenberg principle, and the fact that a seam algebraic set is a union of manifolds. So both Tarski-Zeldenberg and this other statement, they're kind of existential statement. Time statement.