Talk about some work, and this is joint work with a graduate student, Riccardo Rossetti at Duke in the 6th department, and also Boba Keiser, who's here, and we'll be talking about some related work later today. So, I'm going to start with just kind of a basic overview of AMP. It's going to be different from the versions of AMP we've seen previously in this workshop, and that it's a bit more of an abstract formulation. So, I will try to provide the background and also some of the Background and also some of the technical details we need to talk about what we can say about it. So, I'll start with what's sometimes now been called a generalized first-order method. Maybe this is a first-order method for a high-dimensional inference problem. And it's in principle described on what's going to call an n-by-n data matrix. I put data and matrix here in quotes because in reality, sometimes people just think of this as an IID Gaussian matrix, or something that's very far from what might actually. Or something that's very far from what might actually be data, but in practice, it might be something that is data or is derived from data via some pre-processing. So that's kind of the idea, but kind of the steps that take data and turn it into this matrix Z, I'm not going to go through so much in this column. And what you do is after initialization with some deterministic initialization points, we're iterating. And every iteration involves a matrix vector multiplication with this f function, and then you can add in something with the g function. And so the notation here is that f and g. So, the notation here is that f and g are applied to the full past to the previous iteration. So, x less than t is just everything that came before. So, each one of these x and t's itself is an n-dimensional vector, and this generates what I'll think of as a process, right, with n-dimensional iterates indexed by the time. And when we think about this, we'll often think about z being a random matrix, and so there'll be an induced, a joint distribution over the iterates from this recursion. So, this is again a generalized first-order method or you want. Again, generalized first-order method, or what you want to call it, some large class. Now, approximate message passing is a framework that's distinguished by adding a very particular type of correction term within this framework. And, you know, kind of the idea of this correction term at a high level is to decorrelate the iterates and accelerate convergence. So if you think about optimization, accelerating convergence makes sense. There's a lot of, from the optimization point of view, acceleration methods. This is one. This is one. And maybe I'll just note that in principle, this correction term depends on the previous value. So I could have absorbed it into the G term if I wanted to. So it falls within that generalized first order framework, but kind of pulling out in this specific way makes it clear the form of the correction term that we'll be using. So I'm going to start with the simplest example I can think of of this, and that is where you have a matrix. This one is our, and really is our data matrix, and it's some noise matrix plus a low ranks. And it's some noise matrix plus a low-rank spike. This is a symmetric spike matrix model. And when I say Z is GOE, what I mean is Z is a symmetric matrix, independent Gaussians above the diagonal of variance 1, independent diagonals on the diagonal of variance 2. And so the power method just does this. It says, take current, sorry this is an initialization, just keep hitting it with your matrix. And what we're trying to do here is find the principal eigenvector. So the eigenvector corresponding to the largest eigenvalue. To the largest eigenvalue, and we hope that maybe this will correlate with the spike. Now, the power method, we know what the power method is going to do. It's going to converge, as long as you initialize with something with some positive correlation with the maximum eigenvalue, it will go up there. We know how fast it'll go up there. You have balance on it based on the spectral gap. And just to note, the power method can be seen as a special case of what I showed you before. So let's just say you take M, and we, so we kind of, the F part here corresponds to the previous iterate hitting the Z. Hitting the Z, the noise part, and what you see is the signal in this matrix gets absorbed into the G function. So because this is a rank one, when I hit it with a vector, it turns into this kind of rank one, this correlation between my iterate and my current and my ground truth, and then it's proportional to my theta. So this is the only time I'm going to show you the mapping kind of from principle, the data inference problem, to the general recursion to see that it can be done. Okay, so we knew power method would converge. We knew power method would converge. New power method would converge. We knew power method would get to the leading eigenvector. That's well established, very well known. What is the value of the limiting correlation? Well, this can be deduced from random matrix theory. So people do low-rank perturbations, random matrices, and you can actually then calculate that this correlation should be exactly 1 over 1 minus the square root of this lambda, and this is where I'm assuming that theta is properly normalized to be a sphere of radius square root of n. So again, random matrix theory tells us, so for a generic matrix, I don't know where this. For a generic matrix, I don't know where this goes, but for this random matrix ensemble, it's predictable where the power metric will approach to. Okay. So now what happens if I do A and P? Well, A and P has a correction term. I just add it in here. I'm going to subtract off some constant of a previous area. And what we see is we converge up to the same magical point, but we do so more rapidly. So this fits in with the story of it accelerates convergence. All right. So why is there so much excitement about AM? So, why is there so much excitement about AMP? Is it just because it can, you know, get us to the power method a little bit faster for some special class of matrices? Well, there's something more. And the something more is, well, here's my AMP recursion, but there's another process by which we can track its behavior. And so I'm going to call this a state evolution process. And really what we're doing is we're going to define recursively mean parameters and covariance parameters. They're going to be defined in terms of the same functions that we have defining our A and P recursion. That we have defining our AMP recursion as follows. So we initialize with some given points that are deterministic, and then we're going to use the expectations of our functions under a Gaussian process. And this Gaussian process, again, is the one defined recursively such that its mean and variance are given by our parameters. So note that the covariance in particular, these are going to be, have the prop, they're independent across rows. So the covariance will always be proportional to the identity matrix, but there can be dependence across iterations, which is what's modeled. Cross iterations, which is what's modeled by the sigma st parameters. So, this evolution here is called state evolution. There might be some flexibility as some how people might want to divide the means. Sometimes this is left off and people just think of a centered version, but I'll get back to that later about what the means. So, in general, the high-level takeaway from AMP algorithms is that if you have a suitable random matrix for Z, your functions F and G are suitable. Your functions f and g are suitably regular, so maybe separable or lipschitz or pseudo-lip shits, something nice. Then, and you design these BTS terms correctly, and by that I mean you define them maybe as a function of the distribution of Z and some knowledge of F and G. Maybe you have some idea what they should be, but then you use some empirical estimates that converge, whatever. As long as you do the right thing in the biasing terms, there's a correspondence between the AMP recursion and the state of illusion recursion. And that's. And that's the real, I think, kind of power behind AMP is that this state evolution process can be tracked. You just have some low-dimensional numerical integrals typically, and you can understand how it behaves. So if we go back to our example of the power method, this is what the state evolution implies for the correlation between your iterates, as you've got to the mean of the state evolution, and the ground truth. It says that this correlation will follow some recurrence. Recursion, and it's going to, and this will converge up to a fixed point. So, that fixed point, incidentally, is exactly the one we're hoping to get. And so, you can ask, how well does A and P track with this state evolution? If I just plot the state evolution, you see it lines up exactly. So, it's not just that we knew where this was going to go, we knew exactly how it was going to get there, what it was going to do along the way. All right, so this is the real power of AMP. This was the simplest method I could think of, power method. Market. You can go much beyond that with AMP. For example, if you knew that that spike was not just generic, but plus one, minus values, you can take that into account to your function f. Instead of just projecting on the sphere, you shrink towards plus or minus ones, and you will get to a better fixed point in some cases. So you can go beyond power method. It also allows you to incorporate a lot more prior information, other types of information, and so there's a lot you can do, but I'm not going to go into those details at this time. Those details in this talk. Alright, so what's the impact of AMP? And this is a question I get asked a lot. I don't know why I get asked this question so much. Sometimes by statisticians who keep hearing about AMP and saying, but what do you do? What do you do with it? What do you deal with this? So first thing I want to know is, you know, it really is instrumental in helping think about designing methods for applications. So for a lot of applications, in the classes of ones, you start out with regression and the compressed sensing problem, right? To design things that can do very well. Design things that can do very well. And there's been a lot of talks, particularly yesterday, talking about how it can be used for applications in massive random access. Again, it's a very powerful decoder that can get, because if you track it, it can have nice properties. And also, we used it in the design of distributed methods for kind of stochastic updates. It's something Bobas will talk about. And I want to note that in these cases, you can use AMP if you want, or you can. AMP, if you want. Or you can use AMP to design another that might be more robust or more explainable or something that maybe other people like better, but it's designed through the AMP framework. And that's kind of one way it can impact algorithms in practice. Another, though, impact has really been on the theory side, where you can use AMP as a proof technique. And so, you know, some specific examples where, you know, in trying to pin down some information and theoretic limits, sometimes you have something like an area theorem where you want to integrate over some MSE. Where you want to integrate over some MSC, well, AMP gives you a bound on MSC, because you know exactly how it does. Then you integrate that over different levels, and it actually gets you then able to pin down some of these fundamental transitions. It's also been used for giving very precise theory for a lot of methods people care about. So in practice, people, especially in statistics, really like to run LASA. PCA is a major workhorse, or it's constrained versions. It's still a good idea. Stochastic gradient descent, other types of learning methods, these can now be again at. Types of learning methods, these can now be again attacked where you have an AMP algorithm that mimics the behavior. So it's not AMP itself, but you can simulate it with AMP and then show how it works. So I think Marco, who's going to be doing a talk soon, has done a lot of work on that. It's also, so another case was there's kind of sampling problems, say, when is sampling easy or difficult in certain distributions? And so there's certain sampling approaches, say like stochastic localization, and people conjecture: oh, I should be able to sample up to this threshold. Oh, I should be able to sample up to this threshold, but don't know how to prove it. AMP was one of the first proof techniques that was able to show: well, you can actually go up to the conjectured threshold exactly. And again, it's using AMP as a subroutine in the sampling procedure. Does anyone actually want to use this as a sampling technique? I don't know. I don't want to say no. But from what I've heard, the people who are working it, they like the methods they have, but they couldn't prove. This proof then established for the methods they have that they would also mix appropriately. It's appropriate. And then also, there's been a lot of interest in about classes of problems and computational hardness. And there's a large framework, and maybe Simeon will talk about this about polynomial hardness. How well can you do with low-degree polynomials? Well, AMP can be viewed or approximated as a low-degree polynomial. So it is an exemplar limited in the class of low-degree polynomials. And if you show AMP achieves some information-theoretic limit, you have now established what low-degree polynomials can do. Low-degree polynomials can do. So it's also being used as a tool on the dividing line between what is and possibly is not possible with efficient methods. All right. Okay. Evolution of algorithms. I'm going to tell you a little bit about how the various AMPs have evolved over time. This is an image that was generated by, well, okay, I don't know how to attribute it. So it was generated by ChatGPT, but it was prompt-engineered by Henry Pfister. By Henry Pfister. So if you have images you'd like to make for your talk, go to Henry Pfister. He does multiple iterations. He writes pleas and stuff into the prompt. And it generated this and it even had a description about how simple algorithms turn into these mind-blowing ones that you need. Okay, so in the beginning, well, this is a beginning point, I should say. There was obviously many algorithms and ideas that led prior to this, but really kind of this AMP, as I formulated it today, started with this work of Donahoe, Maleki Mantinare, and Baati Mantinare. And Bayatia Metanare, where they were looking at kind of a case where the Z is just a GOE matrix. And when I say GOE, I should really be clear. We're looking at a regression problem where you have an IID matrix. But in the algorithm you run, you alternate between hitting with your matrix and your matrix transpose. Matrix and just transpose. So to map this to the simple recursion, you just embed this into a symmetrized matrix and it turns into GOE. So I'm hiding that detail, is that you know these regression problems, it's not like the matrix you had itself was GOE, but that's sufficient to look at. That's sufficient to look at. And the functions they looked at were separable. So you kind of took the same denoiser and applied it to every component. And the memory was finite. So in the thing, I said you could depend on the entire past, and your correction term could depend on the past. And the first ones depended on the last iteration. And then there's matrix value versions where you depend on like previous K. So I think very shortly thereafter, I think a really kind of breakthrough on the idea was that you could go beyond. Breakthrough on the idea was that you go beyond this GOE to a class of general orthogonal, orthogonally invariant random matrix ensembles. So basically, if you're GOE, you have a specific spectrum, but you can now swap that out with any other spectrum. And this is actually really huge when you think about the applicability of this to real-world problems, is the ability to handle general orthogonal ensembles is massively important and really makes the difference between the algorithms working when you apply into your matrix or not. So very big, but there's kind of a catch here, which is that there's some pre-processing. Of a catch here, which is that there's some pre-processing that's required. So either you do pre-processing, compute an SVD of your matrix to start with, or you have a matrix inversion during your iterations. Neither of these is kind of compatible with this idea of a first-order method. So for large problems, they kind of deal breakers. For moderate problems, works great. And the theory is great, and it also has really nice connections back to this expectation-consistent approximate inference framework of over and one thing. All right, so now there's been a line of work where people were figuring out how you can. Where people were figuring out how you can extend to this class of the orthogonal matrices, well, keeping within the framework of the efficient design of this generalized first order method. And the key idea here was that you need to go to full memory on your correction term. So that was the difference. Somehow there's some magic for GOE, you only need finite correction. But for the general ensembles, if you do full correction, I think this kind of first understood in this paper by Albert, CACMAC, and Winther, but kind of that they derive. But kind of, you know, they derived it kind of using this dynamical, non-rigorous dynamical function theory. And then Jufan kind of put together the pieces and kind of gave a result. And then there's been a lot of variations on this, kind of extending it in various directions. So one thing I'll note is there's also been some really nice work, again, with Deudeja, Liu, and Sen, showing that you can even kind of talk about relaxing this full orthogonal invariance to classes of semi-random orthogonal. And the very nice And the very nice condition on what works and what doesn't. So, this is kind of very pleasing in terms of pushing the directions of what you can do with orthogonal matrices. Now, it turns out that even these formulations might be limited, so maybe you do need to do some pre-processings. This is Barbier, sorry, Mondelli, and sorry, I forgot S, sans. So, they have some recent work too. We're showing. Oh, thank you. Yeah, and so my understanding, and Marco, just chime in from above, if I'm wrong, is that you kind of show that if you do some pre-processing, instead of a matrix, you have a polynomial here, which is still within the realms of positive and intrinsical algorithm, then you can actually do better and get to some more of these fundamental limits. So it's kind of a broader framework for doing this. And again, you need some type of FOMO. Right? So I think that this is the Full moment. So I think this is the evolution of pushing forward and what we can do with the matrix. But as far as guarantees are concerned, there's another kind of separate direction that's very, I think, important. And this goes to the work of Berthier, Montenari, and Nguyen, and also follow-up work by Gerbelo and Berthier. They go back to the GOE. But what they really generalized was this function assumption to non-separable. And, you know, they only have a They only have a Lipschitz condition on the function class. And in terms of, again, algorithms you might care about, this is huge. Because the separability really kind of makes sense for some applications, but for others, you know, once you can go non-separable, it expands the idea of what you can use as a denoise. And it also shows that separability was not really a key component at all for why these AMP algorithms worked. It's something richer. So much of what I'm going to talk today is going to follow along the lines of this. Today is going to follow along the lines of this. We're going to focus on GOE, but we're going to really lean into the non-separability assumptions. All right, so now to be a bit more technical and talk about some things precisely, I want to introduce some notation. So it's much easier for me to think about matrices or just fixed-friendly big letters than things with lots of time indices. So I'm going to often think about just a fixed number of iterations and represent the AMP process and the Y process as N by T matrices. I will also kind of strict together. I will also kind of strict together these functions as some matrix-valued functions. And it's important to remember they have a very special property. So I don't know, some exact thing is called strictly causal, which is, you know, the tth column depends only on the inputs before t, right? So not just any function. And writing it this way, the generic AMP recursion can now be written as saying, you know, AMP generates a matrix, and I say it satisfies a fixed point equation. And this is a fixed point equation in X because X appears on the left and over the right. X appears on the left and on the right. Now B is just a lower, a strictly lower triangular, should be strictly lower triangular matrix of the coefficients. So this has some structure to it. It's a little weird, right? Because it's a fixed point solution. You say, okay, there are multiple solutions. No, there's only one. It's unique. It's the one given by the constructive A and P process. But nonetheless, thinking about it as a solution to this matrix fixed point question equation can be nice sometimes. All right, so just kind of fix this into your mind when I talk about big X and big Y from now on. When I talk about big X and big Y from now on, it's always AMP process or Gaussian process. All right, so now we ask a question: which is: how do we compare the X and the Y? This was another ChatGPT image, courtesy of Henry the prompt engineer. And I really even like how this arrow, you can tell it's chat GPT because the arrow doesn't really make sense. But I think this is actually more representative of the research process as well. Because I think I went down this arrow this way. This arrow, this way, somewhere in this arrow page. Okay, so what's traditionally been done is we're going to think about an asymptotic setting where the number, we'll fix the number of iterations, and then we're going to increase the problem size. So now n is growing with t fixed. And it's a little bit of a pain because now you have to specify these functions and denoisers at every level, but that's typically what's done. And then you ask a question. You say, how close are they? You say, well, test functions. Thinking about the behavior. Test functions. Thinking about the behavior of test functions applied to these is a very powerful way of understanding convergence. And it's so kind of what's worked out is just a sequence of test functions denoted by psi. Note that these psi's are scaling with n, right? So these matrices are getting larger and larger and it's growing, right? I have to have a different function for each one. And then I think about, okay, I construct a random variable, which I'm applying psi to my AMP process, but then I'm centering about the expectation. But then I'm centering about the expectation under my Gaussian. I'm looking at the difference, and I'm saying, does this sequence of random variables converge to zero in probabilities then it gets large? Now, if this holds for a single psi, maybe a very boring one, well, you're happy if this holds for a large, rich class of test functions. So the larger the class of test functions, the richer the notion of convergence. There's also a question of, do you want it to just be hold for a fixed sequence and then spawn some probability, or would you like to somehow take a soup over Or would you like to somehow take a soup over some class inside and play those games? You have a large enough set, you can then kind of argue that it doesn't matter if you can approximate everything in a given class. So this type of framework is very conducive also to proof techniques, because when you're doing recursive proof arguments, you can kind of just leverage that this property holds at one iteration, apply it at the next. So the most common form that goes back to kind of the earliest work was you would describe convergence at the level of the empirical measures. And with this test function, Measures. And with this test function framework, that simply means that you restrict your test function to be an empirical average of some other function, phi, but this is operating in the finite dimensional way. So this is operating on the road. So this is just from T to R. So this is across every point in the iteration. Of course, if you do this for a suitably large class of phi, you've now established convergence, oh, again, at the level of the empirical measure. But questions that are not answered by the empirical measure, you don't have. So working within this regime, it's actually, I think, the It's actually, I think, possible to show, and again, with separability assumptions on the functions, exponentially fast concentration. So, this is due in the case of the GOE to Rush and then Federal Amani, and then Cindy has some recent work with Katamari, setting it to a class of orthogonal invariant matrices for regression problem. As far as I know, these are the only kind of concentration results with this order of exponent, and they actually imply something very, very strong. So that's kind of what you can do at the level of the empirical measure. I think the proofs also hinge crucially on the separability of the function class. And then I mentioned there's this work by Berthier et al. And what they're essentially doing is when they go into these non-separable functions, only under a Lipschitz constraint, they extend the class of test functions to these things they call pseudo-Lipschitz test functions. So if you look at it, what it says is these things are Is these things are, if there was, this term wasn't here, it'd simply say that they're ellipses. But then you add in this extra term, it's the pseudo-Lipschitz part, but then the pseudo-Lipschitz condition scaled by square root n. So this is, again, it's, they proved their theory for it. It's fine, but I've always had a bit of a trouble understanding how to interpret what does the square root n mean. And I actually put a simplified version here. You also put powers on these terms as well. All right, so this is kind of what's out there and how these have been assessed. How far these have been assessed. All right, so there's also non-asymptotic decompositions that look just at the process, right, for a fixed end. And I think the one that kind of really inspired me or got me thinking differently was an interesting work by Eli Liu, where, you know, so he proposes a question of, say, you want to simulate an A and P trajectory. I said you have a random matrix C, it induces a distribution on this process. Say you want to simulate it, but you don't want to generate a giant matrix and do all these matrix vector multiplications, right? If I'm only doing T matrix voltage, T. Matrix T multiplications of a giant matrix, most of the dimensions in that matrix didn't matter. All I cared about were the directions that I actually multiplied, and you can make those on the fly. And that's the sampling idea. So it's actually, at its heart, it's really a decomposition of the AMP process. So it's a recursive decomposition that gives you an exact distribution for the iterates induced by matrix. Then there's also work, and so this is by Jen Li and Yu-Ting-wei. So Yu-Ting-wei is also on a lot of these papers looking at general. Way is also in a lot of these papers looking at generalized first-order methods, where here they're again giving a decomposition. So, this one's more tailored towards a decomposition you might like to have for a given proof. If you want to say, oh, I have a direct comparison, where they break out the thing you want and then residual terms. And then in some settings, they're able to show these residual terms are controllable. And the real upside of this is comes with the number of iterations. So, maybe I didn't think it before, but some of the theory is limited that you can only do a fixed number of iterations or allow the iterations to. Do a fixed number of iterations or allow the iterations to grow extremely slowly. This type of decomposition, with these additional efforts, is able to have the number of iterations grow substantially more, and that's significant for some problems. All right, so this, I think, more or less concludes my background overview of AMP. So now I'll be talking about kind of new insights that I've learned more or less over the last year. So I say new insights, I think someone pointed out that these might not be new, they might not be insightful, but they are to me. Well, at least they're new. Insightful, but they are to me. Well, at least the new part. And maybe just some background. So last summer, I was at ISIT in Taiwan, and I ran into Bovak, who I hadn't seen for ages. And you know, that fungo went up the top of Taiwan, Taipei 101 and all this. But at some point, Bovak says to me, I have a cool optimization problem with partial updates, but it's hard to prove stuff. And I've seen a lot of people give a lot of talks on AMP, and in every talk, AMP always wins, it does the best, and it is. Always wins, it does the best, and it is the solution to everything. So I said, you should try AMP. It'll definitely work. And what I meant was: huh, maybe AMP will work, right? I don't know. You know, I've seen lots of cool, you know, every time you see an AMP talk, it beats everything else. All right, so, okay, then the Moax says, okay, how do I implement my setting? And I think I looked at it. I tried for a bit. I thought, huh, I'm not really sure how these settings work. I'm not really sure how these things work. So I said to Bovak, I'm sure it's easy, just read the tutorial. I'm buying myself some more time. And yeah, so I think Bovak did. And he came back. He goes, I don't see how it follows. It seems different. And he's correct, right? And to which I responded, um. So this is great because it's a problem where we know AMP should work. We know there should be a simple solution at the end of the day. You can always kind of guess it, but how do you understand it? Guess it, but how do you understand it? How do you analyze it? And this was the challenge, and this is kind of what then you know, we started working on a lot with Grice Ricardo over the last, well, or since ISIT, and that brought us to more or less, you know, some of the things we've learned here. So, some of the insights we can talk about now, what we learned along the way, thinking about this problem, but also some related things. Okay. So, the first one is actually not about proof technique or even Bobek's problem or whatever. This is just pedagogical. I've been trying to teach AMP. Trying to teach AMP in my compressed sensing course for 10 years, and I would say I have, by and large, kind of failed. I think a student's the gist that AMP is cool, but every time I put up like some long convergence thing, they kind of rightly get a little, I don't know, they say this is a lot for class. So I've always been searching. There are nice heuristics on how to explain it, but I've always been trying to say, okay, whatever. So here's a simple question. What happens if GOE is replaced by an IID Gaussian matrix? Now let me be clear. Now, let me be clear. As I said before, AMP is often applied to IID matrices, but that's after you take the IID matrix and symmetrize them. Here I'm talking about applying to an IID matrix without that symmetrization step. So for regression, this is a terrible idea. You cannot solve a regression problem if you only multiply a matrix by one side. It's not going to work. So no one would ever think about doing this. For a symmetric matrix, it kind of makes sense, kind of, I guess, in the sense that you can't. Of, I guess, in the sense that if you have a symmetric signal, but asymmetric noise, you can kind of do it, but I don't go, and again, I don't think I've seen anyone do this, but that's the question I'm asking: what happens? And the surprise was that you don't need any correction term. So actually, just making this change, you have all the AMP theory, everything just goes through for the uncorrected version. I don't know that I've seen this anywhere, and so I think that somehow this may be another way of describing state evolution. Way of describing state evolution says, imagine that you replace GOE with IAD Gaussian, run it, and that's what your AMP corrected version should do. And if you don't believe me, well, here's this, you know, I just did on the power method. Play swapped out the noise, ran, you know, AMP, but AMP without corrections equivalent to power method. So if you change up the noise, you know, you get exactly the same. But of course, this isn't really fair in the sense that, you know, so okay, so there's a statement, which you kind of simulate IID just by taking a GOAT. IID just by taking a GOE and adding skew symmetric noise, and then you get IID. But effectively, you've decreased your SNR by one half. So if somebody gives you, you know, symmetric signal, asymmetric noise, you should really first step one, project on symmetric matrix, increase your SNR by a factor of two, then run GOEU. But if you don't want to do that, you can do this too. All right, so this was my first finding that surprised me. Curious if this has been, yeah. Curious if this has been used anywhere. Okay, so two. This one's more, I think, directly in line with how we want to think about the convergence results. So recall that this was this class of pseudo ellipsis functions that's introduced by Berthier et al for describing convergence. And so one of the results we have is looking at this, you say, so in the typical sequence, what you think about y here is special, right? It's a Gaussian matrix, so it has some mean. Matrix. So it has some mean and it has covariance. And it also has a covariance of this form where you have a kind of state evolution covariance and then you're independent across rows. So the first condition I is the condition that's given established by their concentration in the birthday et al. paper. And it says that again for every fixed sequence of functions, you have this convergence. And what's nice is that we can actually have kind of a dual interpretation, which is that it actually says. Which is that it actually says you can find a sequence of couplings, that is, joint distributions, where x follows the distribution induced by A and P, Y is your Gaussian, with the property that x and y are closest together with high probability. What does closest together mean? Close, close-ish, I guess that's weird. And that means that, well, if you normalize by square root n, then this term will converge to zero over probability. So this is the notion of convergence, or an equivalent notion of convergence, that's what's given. If you're saying our x is Given. If you're saying, our x is the difference between x and y going to zero, no. The difference between x and y can be growing. It just can't be growing, you know, on average, more than root n. So I find that this was a, and so maybe, I don't know, I'm doing on time, but if I don't get to the end of the talk, that's fine. I really wanted to focus on this insight. So the spoiler alert is that the coupling result that I may or may not get to present says that you can replace square root n here with any sequence that's divergent. With any sequence that's divergent. And so another way of saying it is that, in fact, there exists a coupling under which if we take out n, you know, in fact, the expectation of this is order one with respect to the number, dimension n. And in fact, the expectation of the pth moment is order one with respect to dimension n for any p. So that'll be the main result that we get, is we're able to improve kind of on the tightness of this with respect to n. And again, maybe another spoiler alert, the dependence on the number of iterations t. The dependence on the number of iterations t is not great, but it's probably optimal given the minimality of our assumptions. So, all right. Okay, so now this is insight three, and I think this is the most important insight. And so I want to really make a nice figure for it, but then I realized I could just draw my figure on the board. So, okay. And I would say that this, oh, okay. Oh, yeah, and I have multiple colors to sharp. Well, I don't, I can just. Chalk. Well, I don't. I can just do it on the side. Oh, okay. Fine. Okay, there's green chalk on a green board. I'll get this one. Okay, so the point is, is this board is now distribution space. And we're thinking about the distribution of the AMP algorithm. And so somewhere on this board, let's call it right here. And we'll call this X if, I'll just put X. So this is our AMP process. I should put X at T, but I'm on the board, so I'm lazy. AMP. I'm lazy. A and P, if you want. And it's defined by those functions, f and g. You could also say, well, we'll think z is GOE or whatever, but it's our A and P process. And what's going to happen when I say state evolution that we're going to approximate it, we're going to approximate it by a Gaussian. Any Gaussian? No. There's some class here. I'm going to call this product Gaussian. And that is to say, it's Gaussians, but But they have to be independent across the rows, and that's the form that state evolution gives you. So, when I say that, I mean, I think about what does state evolution do? Is it going to map me from here to some point in here? We'll call this state evolution. And of course, we're happy if, you know, this is distances mean something on this board. We'd be happy if these points are close, you know, maybe in a relative sense. Why should my AMP? Why should my A and P integration be close to the product Gaussian structure? It doesn't need to be. It could be very, very far from it. This particularly happens in the stochastic updates problem that Bobek was posing. It's one where this just doesn't make sense. Different rows are seen at different times. They don't have the same variance. They go to the same statistics. So this introduces the idea of you can do something, you can think about applying a transformation to your A. Applying a transformation to your AMP. So I'm going to denote this by T, and this is going to get me to maybe a new AMP. So I'll call this AMP tilde, denoted by X tilde. Now, when I say transformation here, I think about it as a transformation I can apply to the iterates. But this transformation also has the property that the new process generated by it is itself an A and P process. So I could have quotedly defined this transformation at the level. Equivalently define this transformation at the level of the functions f and g. So here maybe I have f tilde and g tilde as the parameters of my distribution. Moreover, this transformations I have in mind are invertible. T inverse, so I can go back. So what's the impact of this is, well, I could also now do the state evolution for my transform process, right? It's going to map me. Process, right? It's going to map me to some point in here. Okay? We can call this y tilde and this one y. Now, as I've drone it here, maybe the distance from here to here is closer than the distance from here to here. So maybe my transformation takes this to something that's more isotropic across the rows. And so when I do this approximation, it's nice. And now the question is: say I want an approximation back to x. Approximation back to x. What should I do? Well, I told you I can define my transformation at the level of a process, so I can actually apply the inverse to my Gaussian process. And what's that going to do? Well, it's going to take me to a new point. Where's that new point going to be? Well, in principle, it's going to be in some larger class of distributions. Larger class. Why is that? Well, if I That? Well, if my t is a linear transformation or an affine transformation, then t inverse is the. So let me just apply t inverse here. Maybe I need to green. All right, this is t inverse, and that's going to give me, I don't know, I'll call it t inverse, t to the minus 1 of y tilde. It's this point here. I can apply it to this, and this is the push-forward measure of my Gaussian under this mapping. Under this mapping. If my mapping is affine, then this is a Gaussian distribution, and it's an approximation that's possibly much closer to my original starting point. If my transformation is non-affine, then this will no longer be a Gaussian distribution. It'll be a non-Gaussian approximation, but potentially much closer to my starting point. So I think that this idea of using transformations to think about mapping the A and P around, and then using About mapping the A and P around, and then using that single definition I gave you for constructing state evolution, and then mapping back, for me, it was conceptually what helped me understand the full scope of what you can do with these types of NRP algorithms. There is a class of transformations under which you are, this is, these kind of commute and you're preserved. So then y totally maps back to the 2y. That's called, I'll call that scalar affine. It's ones under which you stay within the product Gaussian class. And there's a notable one in there. Same class, and there's a notable one in there which I call the Whitening transformation. And that is the transformation that takes any A and P algorithm and turns its state into evolution into one that has mean zero and identity covariance. So I use this Whitening transformation. I really like it because then whenever I do theory, I just assume that my comparison is a standard Gaussian matrix. I define a coupling from the standard Gaussian matrix, then map it back via the inverse transform, and there's no loss in generality in doing anything like this. Do any of this. Okay, so that's okay, so I just put some notes here about two specific types of these invertible transformations. The first one, centering. After doing this, I was going back and I realized that this is actually what's used in some of the Realize that this is actually what's used in some of the recent work. This is Selentano, Montanari, Montari, and Wei, where they're looking at generalized first-order methods and they use some of these techniques. One of the things that they note is that, okay, so one of the things they note is this is the correspondence, right? Any generalized first-order method can be turned into an AMP algorithm for a bat. But that's a non-linear transform, so your Gaussian approximations need not be Gaussian anymore. You can always get rid of the G part completely if you want, and just study conversions of this form. But again, the complexity in your state of evolution, But again, the complexity in your state evolution that's hidden via those transforms can be nasty. The whitening one, and maybe the thing I'll just point here is that I kind of said it in words. I'm going to map to a nice version that has standardized state evolution, and I'm going to define the norm in terms of my state evolution covariance matrix such that distances are preserved. So the usual distance under my whitened iterates translates to the distance between my original ones in this induced model. And I actually think this is a reasonable way. And I actually think this is a reasonable way to do it. If you don't like this norm, you can then do more careful things to get back. But I think if you, there's kind of, I think, no real loss in power in doing this. All right. And maybe insight number four is Gaussian concentration is extra special. And I had to add extra in there, and you'd think that I didn't know Gaussian concentration was special before. I do. I know Gaussian petrogen is cool. I just didn't realize how cool. And so. And so the general form of AMP proofs often works in the abductive framework on more sound distance. So you have a good approximation of to a given time, then use that to show you a good approximation of the next time. And in particular, this was in the analysis that Cindy did, and they did this T factorial dependence on the number of iterations, which is kind of big. But I think an insight here is that once you're only, you know, being close to Gaussian in some sense does not necessarily imply that you're close to having Gaussian. That you're close to having Gaussian concentration. Or another way of saying it is being close to having Gaussian concentration is a very strong way of saying you're very close to Gaussian, right? It's kind of like a basis of things like Stein's method. And so, you know, even if you're just close in Vostrostein distance or some probability or some coupling, you could have very worse concentration behavior. And so, kind of, one consequence of this is that, you know, because of this kind of, oh, we're close, but not exactly Gaussian at each stage, you're losing out some of that power. Some of that power. And that's, I think, one of the things that the method I present is able to circumvent this particular issue. Okay. So now I'll state a result. And I think this one I tried to be precise and put all the assumptions on the board. So there's three assumptions. The first is that our state evolution covariance is positive definite. And at least at the level I state the results, there's no bound on the condition number of it. The condition number of it. You just want to be inverted. The second is that those matrix-valued functions f and g are Lipschitz with some Lipschitz constant L with respect to that norm that's defined by the state evolution. Okay, so if they're Lipschitz continuous, then they're Lipschitz continuous with some constant L with respect to this. And third, all simz is GOE, and we're using the standard construction of the biasing coefficients for the GOE. So this is just the standard. So, this is just the standard one. And the statement is that there exists now a joint distribution on x and y such that this difference is close. And I tried to make this look somewhat clean, somewhat not, I don't know, it's still kind of ugly, but the main takeaway is that you can even evaluate this with n equals 1, and you see there's nothing changing, right? It's kind of n independent. So, the only thing that n really does is says there's a phase transition in the tail behavior. If you're thinking about deviation, If you're thinking about deviations less than square root n, you're suck gaussy. If you're thinking about deviations bigger than square root n, your tail's decaying not so right, but right? So that's all it says, but we have full control for all of you. And then there's also this discrepancy between g and minus g. But I don't want to go any other than say, you know, if you get these transforms, you could replace this, you know, as a discrepancy between g and its best linear approximation if you want. Right, so for a lot of applications, this term is just zero or it's Applications, this term is just zero, or it's bounded, say, by the rank of some low-ranked matrix, or something like that. And maybe just for a corollary, the comparison is that if you apply this to the same setting as kind of the standard stuff, it says, okay, if you have any diverging sequence now, this difference is going to go to zero. So that's kind of the order of scale that you get. And maybe I should know that if I, yeah, I'll just leave it at that. So there's a lot more I could say. I'm not being very clear about what's in. Say, I'm not being very clear about what's in the dependence of C and T and L, but I think kind of the main takeaways were, again, conceptually, why is this the right scaling in N. And so I guess with the time left, I'll just say at a very high level some key ideas in the proof. Then the first is, is rather than working recursively, is to think about what I'll call a stability value. So remember, A and P you can think about as a solution to this fixed point equation. All this lemma here says is that if you have some other approximate solution to this fixed point equation, Approximate solution to this fixed point equation, then you can bound the difference between x and y in terms of your approximation error in solving that fixed point equation. So any y, you just kind of plug in and say, how badly does it match up here? And that gives you an upper bound. This constant c, well, this will generate the constant we see before. It's exponential in t, so that's part of the problem, but that can't really be at this level of assumptions. You can't really improve this in general, and then I can construct examples where this is tight-itch. Let's see. Maybe I don't go through the proof other than to highlight that it's kind of elementary. Otherwise, and that you actually get an explicit form for the derivative of this map. And so there's no slack here. If you really wanted to be very detailed and dive into the details, you could imagine attacking this term here. But we just use a triangle inequality and get this upper bound, and that's where the exponential equality comes from. Now, the second point was coupling. And as I said, what I'll do is I'll define my, sorry, first I map to the nice world, but I'm going to define my coupling and then evaluate it as an approximate solution to the fixed points condition. So this coupling doesn't look beautiful, but there is a beautiful coupling if you look at the IID matrix. I just figured for time I'd present the full detailed one. And this works. So you can you have to introduce a little bit of extra noise and such. But the idea is that this is how you can kind of design a joint. How you can kind of design jointly a distribution on z and y, and then for the final steps in the proof, we're going to take that z and y that we got from our coupling, plug it into the stability bound, and now we have an upper bound on our error where everything in here is just Gaussian. We're just looking at these functions of IID Gaussians. So you play some games, break it down, and now we can apply the full force of Gaussian concentration to show that this is small. And in particular, And in particular, I found that this is an old result about Gaussian concentration for possibly non-Lipschitz functions. So actually, hey, I remember learning this in grad school, but you have this pi of two factors. So in the case that we have Lipschitz functions, there's a constant that's not quite right, but this holds much more generally. So actually, I've seen a couple papers recently revisiting the power of this for certain concentration frameworks, and it turned out to be the useful tool for the terms of theory here. Useful tool for the terms of theory. So I'm going to conclude there. Yeah, and here's some insights, but that's what I said. So thanks for that.