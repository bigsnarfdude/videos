Thank you, thank you for organizing. And so what I'm going to present is basically my research, but I try to kind of explain why I think this can be an interesting point of view on numerical linear algebra. On numerical linear algebra. So the title is not really clear for the moment. So I will start from the application we want to solve. And the application is basically related to the numerical, to the nuclear magnetic resonance. And basically what we need to solve is a Schrodinger equation, which is just a linear ODE, where the Hamiltonian H is time-dependent. Okay? So the problem here is not like. So the problem here is not like having a new method to solve all these in general, but the problem is how do we solve these kind of problems where this H can become exponentially larger and larger the more number of spins we want to solve, so the larger number of particles you want to model, then you'll end up with something that is to the number of particles in size. So that's the main problem we are dealing with in this project. We are dealing with this project. And what I are going to see today is basically a classical description of a new approach. So we are starting from this problem that I just explained to you, which is basically solving a linear non-autonomous for DE. We will see that there is a new expression for the solution of this kind of objects. And that's where we started from. Then we have discretization, and finally, we will end up with some new matrix problem. Some new matrix problem. And of course, this is where we are mostly interested. And this matrix problem, thanks to the structure that is inherited by this new kind of procedure, has some nice structure like chronicle, low rank, and other features. And finally, I will show some numerical experiments because the main point here is kind of making a point that it makes sense to look at this because it is working. It is working. So I don't yet have the proof that is more efficient than other algorithms, but I'm quite more and more confident about that. So it's something that is definitely worth to have a look at. And then what I would like us to tell you is how we can use this new framework where I'm solving this ODE to have some interpretation about matrix analysis in general. Okay. Okay, so the problem as I said, for the moment I'm just taking a scalar or D, but just to make things easier to explain. Everything I'm going to present can be generalized to the case of metrics depending on the time t. Okay? And that's actually the interesting part of this work because this is easier to solve, let's say, and we already have an expression that is quite obvious to the solution of this OD. But you cannot generalize. Or D, but you cannot generalize. If I replace F with a matrix depending on D, you cannot generalize this easily. So that's why we need a new expression. And the new expression can be given by moving the problem into a new algebraic structure that is defined thanks to this that we call star product, which is nothing but a generalization of Volterra composition. Or if you want, you take the function that You take the function depending on two variables, t and s. And in the first one, you replace the second variable. In the second one, here you've placed the first one. You integrate in the tau, and this is the definition of the star product. Okay, so if you want, this is also a generalization of a convolution. If you replace the dependence of this function accordingly, you end up with a convolution. A convolution. So now we need to do something else here to make everything work. We need to use Heaviside functions. So basically, this distribution is 1 whenever t is larger than s, so equal, otherwise 0. Why this makes sense? Because here we have this condition, of course. s is the starting time of your evolution. You cannot backward backward, and so we are including this condition through This condition to using this highly side function here. Okay? So now I'm going to replace my f tilde here, the coefficients in the ODE, with f tilde times theta. So I'm cutting whatever is happening before the starting time. And once you do this, you get some. And once you do this, you get some nice structure. Because now the product of two of such functions, f tilde times theta, g tilde times theta, is closed. So you're moving inside is always well defined, assuming, of course, f and g, let's say, are analytic, just to keep everything simple for the moment. Then the sum is again the usual sum. But then we can do something more here. We can introduce the identity. We can introduce the identity of this product, that is just the Dirac delta. We can even define inverses under some assumptions that are given in terms of Dirac delta derivatives. And especially we can define the star resolvent of a function defined in this way, which is the inverse of a quantity like that. So I'm not going to explain anything here about this because it's too complicated to explain in this talk, but we have some publications. But we have some publication explaining how this works. And if it is not clear, don't worry. I'm not going to talk about the rax delta derivative and everything. Just remember that we have this new product that is working with functions depending on two variables, t and s, and then we can use it with inversion, identity, everything is fine. So once we have this, of course, here it means we are cutting, as I was saying, our function. So there is. Same our function, so they are zero until a certain point, and then they are starting and going on as they were defined at the beginning. But what is important here is that we can we actually have a closed formula for the solution of the ODEs with also and with not just scalar function but also matrices in this algebraic structure that I just described. So this is theta. So, this is theta, the Heaviside function, times the star resolvent of the function. And what you get is a new function, uts, that is the solution of this ODE for every T and S. So for every initial time. So when S is equal to minus 1, I'm going to solve that particular equation. Questions? So again, if this is a bit mysterious, uh yes, it's quite difficult. Yes, it's quite difficult. And the point is, we have a new expression for the solution of the ODE. And now, since we want to do numerics, we are going to discretize it and then solve a linear system. That's what we are going to do. And to discretize this expression, we use Legion polynomials. We expand this function that now depends on two variables, t and s, in a series of Legion polynomials. And then what is the nice part? And then what is the nice part here is that if you collect all the coefficients of the expansion in a matrix, then of course you can rewrite if f of t and s in this way, at least when t is different from s. And then in particular, what is nice is that now you can compute the star product, so a convolution-like stuff, with just matrix multiplication. So if I take a function f, I get my discrete I get my disc, I have the matrix with the coefficients of the expansion F here. The identity is just the identity matrix. The inverse is just the inverse of the matrix when it exists, of course. And then the product, the star product of two of these functions or distribution is just the product of the two correspondent matrices. Sum is the sum, and especially the st star resolvent here is just the usual resolvent of the matrix. Usual solvent of the matrix. So now we moved from something that was a bit mysterious but a new expression to something that we know very well. It's just linear algebra. And actually these matrices F, the one that you get from the discretization of your function, have really nice structure. Well, they are non-symmetric and quite difficult, but they are banded. So even if you consider the infinite expansion, So, even if you consider the infinite expansion, the matrix matrix product is well defined. You have the coefficients of the argon, so they well behave. And then, especially, okay, we skip this part because it's just a detail. And then basically, we can use this algebra of matrices that I just described, obtained by this discrete. Described obtained by this discretization for tests by replacing the formula we saw before with the star product with our usual matrix linear algebra formula here. So you see we have h, that is the discretization of the heavy side function, and then this inverse the inverse disresolvent of the function f. And then here we just have phi of t that are the polynomials computed at the time t, and they are the At the time t, and they are the polynomials, the Legend polynomials computed at the time minus one, and you get the solution of this OD. So you see, what you have to do is to solve a linear system. This vector times this quantity, and then you get the solution. What you actually get are the coefficients of the Legendre expansion. So you don't have the solution, what you get from solving this problem in this way is the expansion of the solution. Way is the expansion of the solution in Legend polynomials. So you just get the coefficients. Okay. So to summarize this, we have a discretization step where we compute the matrix F with the coefficients of the expansion of F. Then we solve a linear system. Then we also have to further multiply by a matrix, which corresponds to integrate. Which corresponds to integrate, and then we have to select the good coefficients as a technical part, and in the end, you get your solution. Okay, so does it work? Yes, it's working. So, we have a method to compute the coefficients. It's not that easy because that function is discontinuous because we have the theta that is cutting even the analytic function. So, you have a discontinuity, but you can do it quite easily. Discontinuities, but you can do it quite efficiently. And then you solve the linear system. In this case, we use gm RES, but you can use also backslashes. It's working well for the scalar case. And here you see we solve this scalar case. Here you have the real part and imaginary part of the solution. Here you actually have the error of our approximation in red. And using Rungekutta. And using Runge Kuta method just to compare it with something a blue one. Okay? So you can reach fairly almost the same accuracy as OD4Y with MATLAB by running this. This is not always true, sometimes it's worse. Depends a bit also on the condition number of the matrix because of course we are solving a linear system. But generally speaking, this is what you observe usually. You observe usually. So, this is working, and then this can be extended as a set to the matrix scale. So, now we have instead of a function, we have a matrix depending on t and we assume that this matrix, this is a kind of basically the application we are looking at has this kind of structure. So, you have some part of the matrix that is multiplied by the same function. Multiplied by the same function, okay? So you can decompose this matrix as some aj that is really sparse times your scalar function f of j. And if you have this kind of structure, then what you have to do to find your linear system, the one you want to solve, to solve the ODE, you just have to to solve this linear system here. You see you have the same AJ and then Kronecker product the discretization of the Fj here. Of the Fj here. And then you solve the DNA system. This, of course, can also be rewritten in a matrix form. And here you see, when you do this, you have a rank one right-hand side in this problem. And this means that we are expecting, and actually, this is what we have observed, that the solution acts as low rank. Usually, in the experiment I made until now, it's like far. Now it's like five to ten rank for very kind of very large matrices. Okay, so another example, now it's a scalar, there's a matrix case, we have matrices T and B, and then this kind of this function cosine of 2 pi and of 4 pi, this data, and again it's working kind of same. This is a relative and absolute error and we An absolute error, and we reach it 10 to the minus well 13. So again, much more than the actual application needs. Another example here, the solution is larger, so you see the relative error output are a bit different, but again, same observation. So this was to explain that how much time do I have left? Well, a lot. Oh a lot. Okay. So maybe I'll have more time to talk about the linear algebra. Six minutes, okay. Then this goes well. Okay, so what I want to point out here is that basically we have a new class of matrices to study and solution that have some really nice property because you have the chronicle structure, you have that the solution has low rank, you have that also. You have that also these matrices that are coming from the discretization are structured by topless and Hankel matrices, so there is structure also there. And overall, there is plenty of way I can see where we can accelerate the solution of this problem by exploiting all these nice properties coming from that function. At the moment, we were able to exploit the Kronecker and the Lorenz structure, and we actually applied. Structure, and we actually obtained a quite fast algorithm for the solution of this problem. Hopefully, we are going to finish the work soon. So, this is more or less what I want to present about the research we have been doing until now. And the main point here is that this is working, and I think it means that there is something interesting that is going on in this star algebra, actually. The star algebra, actually, star algebraic structure that I presented at the beginning. Because everything is coming from there. The discretization project processed by the gem polynomial, the functions, the matrices that we are getting from that. And all these nice low-rank structures, I believe, are coming from this closed form, having found the right space where you can express the solution as a closed form. As a close four. So this means that maybe we can have a further look at that. So now some ideas about matrix analysis and the relation with this star product. And what I'd like to talk is actually launches stability that we have already talked a bit about on the previous discussion part. So let's assume, but in general, Let's assume, but in general, how we can, how I think it could be interesting to see some problem in the analysis of algorithm through this new kind of object that we introduced. The point is that we can use some parameterized matrices. For example, let's say that we have a diagonal matrix for simplicity, lambda 1, lambda n. Lambda 1, lambda n, and then we perturb it a bit with some parameter t. So this is a small perturbation of that yabela matrix. Now if I apply Lunches on A tilde of T here, for every T I will get some Jacobi matrix depending on T. But I don't really know, at least for what I know, how this J T is behaving. This JT is behaving while T is changing. But I can run Lunchus inside the star product algebraic structure. And this is an algorithm that we have already described and given condition for its convergence. Of course, this is just symbolic computation in the sense. This is just mathematics. It's not numerical mathematics. But you can still define. So, but you can still define launches using the star product instead of a matrix product, matrix vector product. And everything is working. We gave condition also for invertibility of the coefficient. So, also breakdown is there. But you run it on this matrix A of D S, and so as a result, what you get is a matrix that is the Jacobi matrix. Cobi metrics depending on T and also on S, but S is V of course, so let's forget about that for a while. So you see, then we prove that there is a relation between this J of T S and this matrix here. So this, if you compute, it's a bit, it depends how you scale it, but if you compute J on T, what you get is the matrix Jt. So the this, if you are able to compute this and Are you able to compute this, and it's not easy, then you get a description of what is happening here. Okay, almost done. So the point here is that there is a connection and we proved it. And that we know that actually Launchens in finite decision, as we already said, behave as Lacians is an exact arithmetic applied to a larger perturbed matrix that may be also described as I did before. Described as I did before. So maybe we can understand something more about Lancas by looking at the start Lunctus model. And it's not easy. So in general, this can be applied, I think, to many other methods in linear algebra. And so that's a kind of thing I wanted to present today. Another small example is moments. You can define moments, and so you can do network analysis, but also asking questions like what is a code ratio in this stuff. Question: Like, what is the code ratio in this dark product algebra? What are the toggle-polynomics? And blah blah blah. So, that's all. Thank you for your attention. Thank you. Questions? I just had a few quick questions. Maybe I missed this, but when you're solving your nature self-comp sounds like what method do you use to get the low price? To get the low packs, I didn't say because it's still not like a, but basically, you don't need three losses-based methods to solve this, you can just run out of basic stationary derivative methods, and it's still converting quite quickly and preserves the Lohran structure at each iteration. And then I guess the only other question maybe related because you're using the maybe that lets you take advantage of this analog part of the kilo to plus tangle. And are those banded typically or do they have decay? Or do they have decay? Because they have decay. So that's absolutely something that we are interested in exploring more. We have just found out there is this structure and now we are thinking about any suggestion on the right of the thing. Thank you. Can we go in a second network applications that being just Can you just comment quickly on that? So basically, it's really well known that if you take the k-small element of an adjacency matrix, this is just the number of closed rogues of length k, right? But you can also, I don't know, think about this, like you take the matrix exponential of that, we know that we have centrality indexes, for example. But the exponential is the solution of ODE, where A is time-independent. So if you do the same, but now you So, if you do the same, but now you think about solving that ODE, you end up with the time-order exponential, that is what we are actually computing. And now, the question is: what are these star k moments? These are functions of D and S, and they count the same kind of stuff because the structure of the graph is the same, the non-zero part and it's the same. But now what is the meaning of this? So this is an open question we are trying to understand better. understand better. Also with Paula Boydo we had a discussion a few months ago about this. Speaker is Javier on the reasonable effectiveness of block lunches or unreasonable, right? Single effect lunches, just unsingle effectiveness. Single vector Kree love. Kree love or Krylov? Oh, I've been scared of saying the wrong thing my entire life. I just picked Kree Love because I guess it sounded nicer. I think it's Krelov. Where is it from originally? Do you know? As a name? Russian. If you think bold and underlined is a bit too much, you could think of pictures. You don't want to do that. This is Drunk Work with Cameron and Chris, so you can ask any of us questions. And Chris, so you can ask any of us questions about it. So, I'm going to be talking about low-rank approximation. So, just to make sure we're on the same page, we are given a square matrix, it's PSD. I just need to say that for convenience, just to simplify our lives, don't actually need it. A target rank K, error tolerance epsilon, and our goal is to find some orthogonal matrix Q that has K columns. And our goals were to be nearly optimal than, let's say, the Frobenius or spectral norm. The Frobenius or spectral norm or shadow norm, keep it right. And so nearly optimal means as good as possible with an epsilon factor with respect to the best low-rank approximation, AK. And you can say, ideally, hey, the best Q matrix here, if it were really optimal, would just be the top K eigenvectors of A. So maybe we should just use Kree-lab iteration. And this is an idea that has been explored in different flavors by different people, and I'll have Flavors by different people, and I'll have a couple citations up, and they will never be comprehensive in this literature. And so, we'll look at this particular flavor of a block Krilov method. So, basically, it's got three stops. Pick a starting point, some start block. Capital B with little b columns. Little b is the block size. That's important. Thing we're going to talk a lot about. Usually, Gaussian starting matrix. From there, you build a Matrix. From there, you build a Krylov subspace, so let Z be the orthonormal basis of B, A, B all the way to A to the T B, number of iterations. I'm going to be talking about things at infinite precision, so I'm not really going to care which way you were talking about this. And lastly, you return a solution, and so here you do some post-processing that's the right thing to do for the root approximation. What stands out here is the question of how should you pick the block size, little b, like the one free. Little B. It's like the one free parameter that really sticks out. And you can ask different communities or different groups of people for their advice. And there's sort of roughly two big camps, I guess I would say, two flavors of ideas. One is going to be what I'm going to call a large block size. So the block size is greater than or equal to k. This is coming off of a rich line of work off of Trop, Halco, Martinson, Hugh, Journey, Ipson, Woodruff, and I'm sure many. Maya, as Ipson, Woodruff, and I'm sure many other people in this room. The upside, like the motivation in some sense, what feels like the really core motivation for looking at these large log methods is that they give you really strong theoretical results that are tied to specifically the low-rank approximation task. Not just finding the eigenspace, or well, finding the optimal subspace suffices, but it can actually be easier in a computational complexity perspective. Computational complexity perspective to find the low-rank approximation. A good example of this is Music Musico 2015 gave this gap-independent convergence guarantee. So if your block size is exactly k and your starting block is a IID Gaussian matrix, and this is a little squished here, you need to run for a number of iterations, which is 1 over epsilon log d over epsilon. That's quite nice, because it's well gap independent. Because it's will gap independent, it does not depend on the properties of any. And actually, what I'll write down here to the side is I'm going to talk about in this to compare different methods, I'm going to need some measure of the complexity of an algorithm. For that, I'm going to use matrix vector products. It is a method, it is debatable, but it's what I'm going to use. I'm going to quickly jot down, because we haven't written out, the number of matrix vector products this algorithm makes. Which is just k times this block size of k, this many iterations, you just multiply them. Alright. They also give another guarantee. If you want to converge faster than this sublinear rate here, you can let g k to be, so the gap from eigenvalue k to eigenvalue b, be the gap from eigenvalue k to eigenvalue b plus 1, because that's the way the math works. And if you do this, if you This, if you define this variable, you can get a nice faster spectral decay convergence. So if you use a block size that's at least k. A Gaussian starting matrix, and you can converge in 1 over the root gap log d over epsilon. And that's quite nice because now your only dependence on epsilon is in this log vector. So now you're converging at a linear rate. Again, matrix vector complexity. Matrix vector complexity is going to be what I focus on. So I'm just going to write it down here to the side. Okay. Again, just blocks ice times number five actions. Yes? If you assume something on the decay, can you get rid of the dependence on the dimension? Uh there are depends on what exact problem you're looking at. Exact problem you're looking at. For some forms of the guarantee, it's known that you can't. So I listed low-rank approximation as a test, as a task. There's a different one where you want to find, like, in some sense, near-optimal individual eigenvectors. This is what Music and Musico called the per-vector guarantee.