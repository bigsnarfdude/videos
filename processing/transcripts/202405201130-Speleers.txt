I always think about approximation with spline, so it's something new. These were the titles that I got from Deepesh, so don't blame me for that. The announcement was not meant to blame you. No, no, it isn't much. So, okay, so thank you for the introduction. Also, thank you for inviting me to be here. Me to be here and to give the talk. And basically, the purpose of the talk is to give an introduction to the Benstein BZ form of Polonio, so the part that I added to the title. And also, I want to give you some flavor of why this Bernstein disease form can be useful for representing, analyzing plant spaces. Okay. Okay, and especially the Bernstein-Besie form is very useful, say, when you look at splines on triangulations. So, splines in the same sense as what Kaibu mentioned, as piece-wise polynomials with certain smoothness in between. And say if So, if you look at the outline of my talk, I have basically two main parts. I cannot get rid of these messages, so you have to survive with them. So, basically, in the first part, I focus on univariate spines. In the second part, I focus on splines on spine relations. And I will basically give more or less the same story in both cases. So, here we see, say, the So here we see, say, the outline. And the reason why, okay, somehow the target are splines of triangulations. But the reason why I start with, say, saying something about univariate splines is because, first of all, the similarity to the splines on triangulation is so large that somehow it might be easier to pick up things. And of course, in the unified case, things are more simple. And okay, so basically, I Okay, so basically, I want to define, say, what is this Bernstein disease form of polynomials, so show nice properties. The nice thing is, especially, say, hold to say dry smoothness conditions between polynomials, because in the end, we want to get to spline. So, stitch polynomials together and say something about the minimal determining set. So, we heard already something about that in the previous talk. In the previous talk. And then, okay, so also something about approximation, also depending a bit on what the time permits me to do. And few words on, say, a better representation for splines, which are the so-called B-splines, but again, just very, very few, very few words on that. Okay. And, okay, so before. And okay, so before I start, I want to say that okay, if you want to know more about that, you have already seen this picture, but it shows that basically this is the reference for splines and triangulations. It's a book by Lai and Shoemaker. And okay, so of course, during this week, you can ask questions to me or other experts in the audience, but otherwise, you can always look up in Look up in this book. Okay, so let's start. So, as I mentioned, univite splines are more as the warming up for the real work. Let's call it like that. So, let me start with defining what are the Bernstein polynomials. Sure, most of you know what they are, but just let me start from, say, from scratch. So, most of the time, they are defined on, say, the unit interval 0, 1. And if I look at the best. And if I look at the Bernstein polynomials of the grid t, then you can define d plus one of these guys. So I index them from zero up to d. And then their definition is quite simple. So we take a variable t, raise it to the power i, which is our index, times one minus d, raise it to the power d minus i, and then you multiply with some with some constant, which is a normalization. So that's maybe not the most crucial part. And this is my Path, and this is my set of Bernstein polynomials of a degree t. So, this is the most classical definition. It's explicit, it's simple to understand. You can actually also say define it differently, which might be also nice to know. There exists a very simple recurrence formula to build up these lines starting from lower degree Bernstein polynomials. So, actually, you can write each of them of the them of degree t as a convex or say as a linear combination which is a convex combination of two lower degree ones where you just take as the weights t and one minus t. So here in this case as I mentioned that is mainly focused on the interval so you want so it's really it's a nice way and then somehow you can recurse that till you arrive at the constant degree Bennstein polynomials which is just one and it's constant which takes value. Which takes value. So, that says is an alternative definition. And okay, here in the first part, I mainly focus on the intervals if you want, but you can say easily extend it to any interval, just to a simple change of variable. And then you basically stretch these guys to any interval from, say, x0 to x1. So again, this would be useful if you work with splines, because then we can see the different intervals, and of course, I cannot just stick with 0, 1. Just stick with 01, but then you just move them around on all the different intervals. Okay, so then let us have a look at some pictures. So, okay, these are the constant is nothing to see, there's just one which is one. You have two linears, three quadratics, cubics, and quadratics, and so on. And you see that somehow they have nice shapes. And it's not only that they look nice. And it's not only that they look nice, they also have nice properties. So, let me just list some of them without going too much into the details. But one, say, okay, I think these are, so that one nice thing is that they are more negative in the interval 0, 1. As I mentioned, they have say one nice bell-shaped function. They have one maximum, and actually, you know where the maximum is. So, the index i divided by the degree, very simple to remember. Or very simple to remember. They also have a very nice formula for integration and derivatives, so that's a nice tool also for analysis. For example, all the Bernstein polynomials of the same degree have the same integral, which is somehow a bit surprising, and it's just one over dp plus one. Computing derivatives is you can derive, so basically you can write the derivative of one Benstein polynomial of dp as just As just subtracting two Bernstein polynomials of degree d minus one, multiply it by the degree, and that's that's it. So, um, and of course, if you want to know higher derivatives, you just repeat these guys because then, well, you can plug in derivative also here and there and so on. Another nice property, especially when you apply them in, say, the context of geometric design, is that they form the partition of unity. So, that's why we had this. Of unity, so that's why we have this kind of scaling factor in front of them. So they sum up to one, and also they have some nice interpolation at the endpoints. So only the first one is one at zero, and only the last one is one at one. And they have some more properties like tangents and so on. But okay, just to so basically, if you go back to this picture, you can somehow like. To this picture, you can somehow quite easily check that these properties are satisfied or maybe not the derivative formula, but anyways, this is you have to believe that this is true. Okay, and then the Bellistein polynomials of the same degree, they are all linear independent, their number is d plus one, so they form actually basis for the polynomial space of the 3D. So you can write any So, you can write any polynomial of degree t or lower as a linear combination of these Benstein polynomials. And the coefficient in this representation is usually called the BZ coefficient. So, that's where the name Bernstein-BZ form comes from. So, yes, it's not the best word to. Is not the best word to call it, but let us stick with this terminology for now. And that's basically it. So let us maybe have a look at one particular polynomial, which is interesting for our purposes later. So if we go for degrees greater than or equal to one, of course, then t, our first monomial, is part of our space. Is part of our space, so we can represent it exactly in terms of the Benstein polynomials. And actually, we know the coefficients as they are nothing else than i divided by d. So this is somehow a very simple, nice thing. And if you remember of, say, the slide, these are actually the points where our Bernstein polynomials get their maximum value. Okay, so. So, and these values are important and they basically call them for knowing the main points, also to have the association with the second part of the talk. So they are important. And then we can say, give say some kind of more geometrical interpretation, say two over. Say to our BZ coefficients where we say we define new points where we take as x as ordinates these domain points, this ti, so we i and as the y coordinates we take the coefficients and then say we can say okay these are points in R2 and they are called the counter points and we can connect them in a piecewise linear fashion. Can connect them in a piecewise linear fashion, and we call, say, this piecewise linear line the control polygon. And somehow it's called control polygon because somehow gives you directly, if you look at this control polygon without knowing, say, what is the shape of our polynomial, well, it gives us directly, say, a flavor of what should be the shape of the polynomial. Of the polynomial. So it gives us some control on the shape. And that's not saying that's not a surprise because that's basically a direct consequence of the properties that I mentioned in the beginning about the Benstein polynomials. Because we know they are positive, they send up to one. So basically, each point of the polynomial can be seen as a convex combination of this. Combination of this control point. So the curve must be in the convex hole of this control point. And in addition, we have this interpolation at the end points, we have also some tangent conditions. So somehow you can get some prediction of, say, how the curve might look like. And that's basically why I say this kind of Bernstein disease. Why I say this kind of Bernstein-BZ representation is a very useful tool in the context of geometric design. Because in the end, then you can just play around with, say, a few points, place them around and say, okay, this is the shape that they have in mind. You mimic them, this shape with your control points, and then say you get a polynomial curve that might roughly get the shape that you have. Okay, so that's basically what I mentioned. So, okay, let us keep say also this in mind that we have this kind of say geometric interpretation of the Bernstein dz representation of polynomials. And I will use this kind of say geometric interpretation also to explain a few things later on and make things say very graphic. things say very graphical so we don't we don't do much we don't too much photograph okay and then say if you mention say the the bernstein this year representation then one um say algorithm that is very very important is the so-called the castillot algorithm and is an algorithm for evaluating um polynomials in um in bernstein form and it's a it's very famous because somehow it And it's very famous because somehow it is very simple. And moreover, it's also a very stable algorithm. So, okay, so let me say explain what is exactly the algorithm and how to interpret it. And some of the foundation is the following recurrence formula. So here I wrote, say, my polynomial in a slightly different way, just to indicate, say, also the dependence on, say, the D plus one coefficient. Say the d plus one coefficients in the Benstein G form. And if you somehow remember, say, from the very beginning, that the Benstein polynomials also have a recurrence formula, you can somehow rewrite these and apply it also, say, to the polynomial representation. And then you can write, say, my polynomial of degree D as a convex combination of two polynomials of degree D minus one, where in the first case, Where in the first case I pick say the first d coefficients, in the second case, I pick the last d coefficients from my representation, and then I somehow just multiply it by t and 1 minus t, which is again between, say, 0, 1. So it's a nice convex combination. And then, of course, as I mentioned before, with the Bernstein polynomials, say then to evaluate this guy, again, you can do the same, write these in terms of two polynomials of the green dimension. Of two, polynomials of degree b minus two, and so on, till you arrive at, say, a polynomial of degree zero constant, which is nothing else than the coefficient that it contains. And usually, say, you write this kind of construction in, say, with a triangular figure, and that's how you really implement it, and that's how the algorithm looks like. So, we start with, say, your polynomial, you decompose it in zip to guide. You decompose it into two guys, the ones that are on here, then this guy again, you can decompose it till you arrive at some coefficients. And if you do that, you get a nice triangular figure of how to compute your, how to evaluate your polynomial in the value, a specific value of t. So you start with your coefficients, then basically after one step of the Decastral algorithm, you recombine two consecutive BZ coefficients. BZ coefficients with multiplying them with T and one minus T, you get, say, new coefficients of a degree, one higher, degree one polynomial. Then again, you recombine two of them till you arrive at the tip of the tip of the triangle. Okay, so let's say it's a very nice algorithm because it basically says and all the computations are done by just taking simple convex. taking simple convex combinations easy to remember just t and one minus t so somehow you can never forget this this simple um this simple this simple but there's much more behind this this algorithm and okay let me just tell you the very few very few things that are important for for us especially if you want to move to to the splines um so okay so let us say look slightly Let us say, look slightly different at, say, the algorithm. Again, as I mentioned, I try to be as graphical as possible. So let me move back, say, to this control polygon. And then I will apply my algorithm not only, say, to my VZ coefficients, but also to my domain points. Or basically, I apply the algorithm to the control points. And then, okay, you basically get another point, which is one of. Another point, which is one of one point somewhere in on the blue on the blue curve, and say to visualize, say, all the steps in the algorithm, let me also put some colors. So, say, okay, the first step are the control points, so they are black. And what you get after one iteration of the algorithm I recolor in yellow, the second iteration, I recall in red, and then so in this case, I just focus on the group three. I just focus on the group three. It's already enough, but the principle, of course, generalizes to everything. Okay. So then, okay, so let me apply the algorithm and then interpret again these values as points on my figure. And by doing that, I get somehow figures like that. So, okay, let me maybe just start with a simple case. t is one half. It basically means my convex. It basically means my convex combination are nothing else than computing midpoints. So after one step, I basically compute the midpoint, the midpoint, the midpoint. These are my yellow points. And somehow I can connect them again. Then I compute another sweep of midpoints. And then the last midpoint is exactly the value on the curve. And of course, this works for any value of t. So, for example, just to give another example, otherwise this looks maybe too symmetric, and you might say, okay. Maybe too symmetric, and you might say, Okay, it's by chance, it's a coincidence. No, no, no, it really works for any t. And okay, so here is what it is. And of course, you see that really we get the value on the polynomial. Okay, so far, so good. So, now what I mentioned before, somehow I know a representation, the Bernstein polynomials were somehow defined on the interval. Polynomials were somehow defined on the interval 0, 1. That's why, in my two examples, I take, say, a p-value between 0 and 1. But of course, the polynomial is defined on the whole line, not just on 01. And then you can say, ask the question, what happens if I take a t-value outside 0, 1? Okay, so let us see what. Let us see what our first okay. Let me first say something. Okay, forget about p outside zero one. We stayed in zero one and okay, have and say look at some other say nice nice feature of this Bernstein VCA representation and especially the Castillo intermediate parts. So as we see. So, as we see, so we have computed several points, and say the blue point is exactly, say, the point on the curve. Then, one property is basically called, say, subdivision. And say, this, say, the algorithm allows us to basically say, okay, what if we just want to cut the curve, say our polynomial curve, in two pieces, of course, there are still. Of course, we have still the same polynomial, but say on another, say, on just a sub-interval. And then, what could be, say, the Bengstein-Bizi form of these polynomial pieces on the sub-interval? And the answer is, well, look again at, say, all these intermediate points and just pick some points from what you have computed by applying the de Castillo algorithm. And basically, as And basically, as I try to somehow show over here, say the first piece from here to there is basically the counterpoints of the Benstein VZ form of that piece and nothing else than, okay, I picked this black one, one yellow, one red, and a blue red. And then the second piece is again the same, but then we start from the end. You pick, say, the black one, then one yellow, one red, and one blue. And somehow you can say, okay, maybe how can we remember which one to take? So if we go back to the algorithm, then somehow you can, and especially this triangular structure, you can quite easily remember which ones you have to take. So for the first polynomial piece from zero up to t, you just take all the points on the lower line. So the black, yellow, red, and blue. Yellow, red, and blue. For the one that corresponds to the interval t up to one, we pick the one, say, on the other side of the triangle, the black, the yellow, the red, and the blue. And that's it. And then, of course, if we go for higher degrees, the principle is the same. Same, you look at, say, the lower line, say, the diagonal line, and that's and that's it. So somehow, um, this de Castillo algorithm allows you to. Algorithm allows you to say also compute all the representations of the same polynomial on say specific sub-intervals. Of course, there is say one particularity that say the two polynomial pieces, one has to start in zero and the other one has to end in one. But that's say the rest. Okay. So, okay, that's what I showed here again for t equal to one. Here again for t equal to one-half, and then also for my second case, t equal to one-third, you see exactly the same thing, and it looks still okay. Of course, you have to trust me a little bit that this is fine, that it not only looks nice, but it's also that. Okay, and then we finally come to my question. So, what if we Question: So, what if we say go outside our interval of interest? Because, of course, somehow in geometric design, the interest is 0, 1 because then we stay in the convex hull and so on. If you say, would go outside, then the algorithm still applies, it's still correct. But, of course, you cannot expect that everything is complex combination anymore. So, here, say, for example, Um so here say for example if I take t equal to t equal to two then I get the value of my polynomial at position two and then say I again say colored my yellow points and my red points and my blue point is okay this is the total value in the same way as before. So of course it's somehow disconnected from the initial control polygon. Um, contra polygon, but that's expected because somehow we don't stay in the convex hull, and it's not convex combination, they are just affine combinations. So, so this is not a surprise. But the reason why I go to say this kind of strange case is exactly, say, to illustrate also my property of, say, the subdivision. Because this subdivision, by looking Because this subdivision, by looking at some coefficients, I could say extract another representation of the same polynomial, but on other interval. Now, here in this case, since my t is not inside 0, 1, but outside, I basically get, say, two polynomial pieces from 0 up to t and from 1 up to t. And, but no, I'm just interested in the case one up to t, because say, if I again connect If I again connect the right control polygons and say I look at the Bernstein-BZ form of these control polygons, then what I get is not just a polynomial, I get again another representation of the same polynomial, but now on an interval which is outside my initial interval and is not, say, I would learn, it's really, say, a consecutive interval. And here, okay, maybe. And here, okay, maybe again it was too symmetric. I went for t equal to two, but I can take t whatever I want. So that's say that's nice because so in this way, say my first polynomial piece basically is that, and somehow by applying this to Castillo, I get another representation of the same polynomial, but on other interface. Okay. And okay, that's basically that. That's basically a direct consequence of say the castillo agreement. And also, not somehow surprised that it also works outside our interval 0, 1 because it's just a pull-in off. So, nothing special. But then we can start playing around with these control points of this new piece. So, what happens if I start moving one of these points? So, now I treat this part as a new polynomial that I New polynomial that I handle independently of the origin. And so let me start at the end and say, move it around. So I just move the random point. And then what do I see? Of course, no, in this case, this polynomial piece is another polynomial than that one. But still, they are connected in a nice way because you can somehow prove that Blue that these two polynomials, the blue one and the purple one, are basically as a C2 connection point. They're not the same, but still they have, say, we construct a piecewise polynomial, which is highest. And then somehow you can guess what is happening if we start moving some more points. So let me also move the second last point. Point, then of course, um, the polynomial changes more, and maybe also not so surprised that somehow the connection with my original polynomial becomes weaker. And in this case, no, I just have a piecewise polynomial which is still smoothly connected, but only in a CZ fashion. And then, okay, so now we basically know the rules. You basically know the rules, and if I connect, say, also the second, move the second one around, then I just have C0 fashion. So now we can, say, look at it from the other way around. So if we want to build, say, piecewise polynomial function with a certain smoothness, then say we start with a given polynomial piece on an interval that we know, and then say, in order to check that, say, And then, say, in order to check that, say, my next polynomial piece, for example, joins in a C0 fashion is well, the only thing that I have to check is at this point they are the same because the zero step in the algorithm is just you take the same point. If I want, let's say, these polynomial joints in a C1 fashion, I have to check that this point and this point are basically can be generated by applying one step of the time. One step of the task to do, and then basically, we have also say a nice geometric interpretation that these three points must be correlated. Because you know that somehow you just take this point as an affine combination, so it must be on the same. So, that's that's a nice, this is always a geometric check that you can do to be sure that this is long. That says this is long, but of course, if you go for Hannibal's motors, it becomes a bit more tricky to check at least at least physical. But somehow, the idea is exactly that. So you say, okay, I want a certain CR smoothness. Then, so in general, then basically what you have to do is say the first r plus one control points of my next polynomial are fixed by say by this. By say by the smoothness conditions, and you know what they should be. Namely, you just apply R steps in the Castillo algorithm, and the rest you can choose whatever you want, you don't care, they will not affect, say, this collection quantum. Okay, um, so that was say the graphical interpretation. Um, okay, I will not try to prove it, it's actually not so not so difficult. Not so difficult, and just univariate polynomials. But maybe just to give you a few points, is well, or at least one way of proving it is just looking at, say, our derivative formula. It's quite simple in the case of a polynomial in the Bermstein-BZ representation. Again, we can exploit the fact that the derivative of a BermStein polynomial can be written as a subtraction of two lower-degree Lower degree Bernstein polynomials. And then, so we have the derivative of my polynomial can again be written directly in a Bernstein BZ form of degree D minus one. My coefficients are nothing else than just subtraction of two consecutive BZ coefficients before, and then I multiply it by D, and that's it. If you want to compute higher derivatives again, you can interpret these as, say, one polynomial. These as say one pronoun here, and you apply as often as you want to get, say, formula that you need. And then if you want to check, say, smoothness conditions, things are actually even more simple because we know at the endpoints, we have this kind of special interpolation property of the Benstein polynomials. So only one of them is one, and all the others are zero. So somehow you can forget about the sum. Now you can forget about the sum, it's just one guy. So you basically, the arc derivative of the polynomial at the endpoints is nothing more than some kind of difference of the coefficients multiplied with the right factor. And then say, putting these two constraints together, you can rewrite a bit these things, and then you get exactly that the coefficients really. The coefficients really are the same as if they would come from applying the Dicastula. So, again, say this is an exercise that you can easily check yourself. Okay, so then, say, a few remarks is that, say, somehow as you see, it's not only a nice visible tool of, say, imposing or checking smoothness conditions, but what is also a nice is also say a nice property is that say the coefficients that are involved and say the smoothness are exactly the same as the number of conditions that we impose. So if you impose CR, there are only r plus one coefficients involved in this whole construction. So that's nice. And basically that's also one reason why somehow you can use this. One reason why somehow you can use this kind of tool also to checking or computing say dimension formulas because it's a matter of just thinking in terms of coefficients and see which guys are somehow determined by others. And then another nice consequence, which of course is quite obvious, but it's somehow a very important observation, is that since all these coefficients that All these coefficients that you say that are fixed by the smoothness condition, that since you can compute them using some steps in the Castillo log, it just requires multiplication by t and one minus t, we basically know that these coefficients will never blow up if your point stays at a certain distance. So you have some control of what happens with these points. These points. And that's important also for analysis, especially if you want to prove some approximation properties, that you never have some kind of instability, at least with this kind of thing. Okay. So that was, say, the part on, say, the polynomial. So now we are in a spline workshop. So now let us apply this kind of tool in Um, in the context of um slide, so we need to be careful. Um, so now we basically look at say piecewise polynomials with certain smoothness. Um, so then how can we represent elements in this case? So again, on each interval, I can say, okay, I look at the Benstein-BZ form of the corresponding polynomial, and somehow these polynomials are not independent, but they are constrained by. But they are constrained by smoothness. But somehow we know what is the effect, so we can determine, say, which coefficients might be already known by others fixed, by others, or whatnot. And then this brings us to the concept of minimal determining set, which basically says this, so a determining set is nothing else than, okay, if we look at Ospine, say we look at, say, all the VZ8 coefficients of all the polynomial pieces of my fun. pieces of my fun and then say a set is a determining set so this is just a set of domain points if you say put the corresponding coefficient equal to zero then say the other coefficients will be determined by the smoothness conditions and if you arrive at the spine which is zero then we say this is a determining set okay and then say if this set has small calories And then, say, if this set has minimal cardinality, we have a minimal determining set. And say, by construction, that basically means that the cardinality of this current must be equal to the dimension of our spline space. Okay, in the unified case, we know the dimensions, so somehow things are easy. I will not say too much about that. And then, so let me say, maybe look at some examples. What could be, say, potential minimum determinant. Say potential minimum determining sets for say spline spaces with in this case four points: this one, this one, this one. So, here we see, say, the domain points, and black means the point is in the determining, the minimal determining set. Say a point, say, which is which is hollow means it's not in the determining set. And then, say, if you just want to look at, say, continuity, then it's not a surprise. Continuity, then, it's not a surprise. All the guys have to be in a determining set in the minimal determinant and the MDS. So it would like that. And check the dimension is fine. So if you go for, say, C1 splines, then basically, so we start, say, with the polynomial, then C1 means you apply one step of the pastillo. This guy is fixed by the algorithm, but the other two you can choose as you want. And then say you go, you move to the next. And then say you go, you move to the next, this guy is fixed, and the other ones you can take whatever you want. So, say these black guys in S. And if you go for C2, then of course more of these guys will be fixed by the smoothness, then you can choose this as a guy. So, again, somehow in the univalid case, it's quite easy to determine, say, what is a minimum of the value. So it's not okay. Then say this MPS leads automatically to two basis of the spine space. And formally, you can somehow define it like that. But say intuitively, it means that one basis function is determined by, okay, look, let's say, the domain points which are in the minimal determining set. You take, say, then if you specify the coefficients of these domain points, The coefficients of these domain points, then your spine is completely determined. And then you take a sample of choice, you just take one of them equal to one, and all drops to zero. And of course, if you do that for all the domain points, you have say right number of functions which must be linear independent, and they will form a basis for the space. And that is called NDS basis or basis, whatever you like to do it. So, okay, let me again say, look at some. Okay, let me again say, look at some examples to illustrate, say, also some properties of these bases. And maybe it's a simple construction, but you have to be careful. So let us take the same example as before. So with four points, smoothness C2. So you see here, say, the MBS on the bottom. And then the corresponding basis function. As you see, I just have taken the one. See, I just have taken one for one, okay. The first one, one for the second one, one for the third, and till I arrive at the last one. So, this is my full set of basis functions. And so I have computed, say, also, say, the white ones, which are determined by the smoothness conditions. And then, okay, you get this kind of thing. So, somehow, what you already see is that, okay, I did it with purpose. I've got to say my Cut say the y interval between say zero and a minus three of three. But somehow you see that this can be oscillating. And what is even worse is that, say, if I would go for more intervals and do somehow the same construction of computing there, yes, then say some of these basis functions might have support on the whole interval. So this is not the local basis. So when I say local base, it means not all the basis. So when I say local base, it means not all the basis functions have local support. And that's somehow easy complication because in addition, say these oscillations will just propagate. So it's not only this particular chunk, it's not only not locally supported, but also you have some instability in it. And this happens, say, if the smoothness is too high with respect to the degree. So here I have some homaximal smoothness. Have some homaximal smoothness. So things become better if I lower the smoothness. So, just in this case, let me go for C1 and draw the corresponding, okay, not all of them, but this two, you get somehow the thing. And now you see that somehow the supports are maximum two intervals, and they are also bound because there is some kind of repetition. Say this guy is exactly the same as this. Okay, this is because I have uniform notes, but just two. Because I have uniform notes, but just to illustrate that it will never become worse, even if I go for more n, because you just repeat the same thing. And say for i to the one, I have local basis which is stable. So in general, you can basically put, say, some constraint if the degree is high enough with respect to the smoothness, and I want this constraint is 2i plus 1, then say you have a very nice basis. The other one is. I mean, the other one is still a basis, but maybe not as nice as you want, especially for practical purposes. Okay. And then, since there was also approximation in the title, I have to say something about approximation, but I will be say quite quickly and give you some flavors. So, basically, when you say about or when you want to check approximation of the space that you have, Of the approximate space that you are dealing with, then often you want to check some kind of say inequalities like the one that you see over here. So you have some function f which is supposed to be say smooth enough. So this is a sober space of order f plus one. If you don't know what it is, forget about it, just some smooth function. And then say you look for some spline in your approximation space, and then you want to bound it in terms of something like that, say some. Something like that, say some norm which might depend on some derivative, then some constant, and then this is somehow the important factor is that say you have the mesh size, so the length, the maximum length of the interval, yes. What about the approximation with respect to p um so this is um basically okay you this is a good point because I I don't know this is I no no, this is so m basically, if you want to, okay, this depends on the smoothness of the function class. If you say, okay, we take them as smooth as we want, we can take m by d, and then you plug in d over d so then you just have d plus one minus k. And then the constant k is something abstract. The k is something abstract, which might depend on d. The only thing is that it's independent. The only thing is that it's independent of your function f, and it's independent of this function. So that's just, I was a bit sloppy and somehow criticizing these kind of things. But okay, usually the reason why we are interested in this kind of an equality is that, say, if you have this fine space, and then if you want to, if you take smaller intervals, then somehow you want that your approximation becomes better. And this really. And this really tells you how much better it will be because f is fixed, this case should be independent of the mesh size, and then you get, say, your mesh size to some power. And the higher the S somehow, the quicker the error goes down. Okay, and then, so here I did not mention anything about, say, design. Often you also want to have, say, some kind of tool to compute the function like that. Tool to compute the function like that. And say, if you find some kind of scheme that gives you this kind of method, you say, okay, you get the right approximation. Okay, so I don't want to go too much into the details. Just give you some points as what can be done. We can actually use our MVS machinery to prove this estimate. The only thing that I require is I can. The only thing that I require is I cannot use any MDS basis, but I need the local one. And that has, say, some pros and cons, because then, of course, I have this kind of restriction in this kind of tool, which is just the restriction of the proven tool. And this, in the univariate case, the approximation holds for any D even for maximum goodness. For maximum latest. But then, what I really exploit in the standard proof is the low test support install. And then, okay, so once I have a basis, I basically want to represent my approximant in terms of this basis. And then the question is, how do I compute my GUI, my class, the entire point, or my approximant? And basically, you can do the following simple trick, which gives. Simple trick which gives you basically everything is um so for each shape coefficient you look up the corresponding domain point it belongs to say a certain interval and you say okay you compute the tailored polynomial on this interval or I say the tailored polynomial there are different versions usually in practice you go for some kind of average tailored defined by some sort of equal but okay that's not important but the whatever tailored polynomial take whatever tailored polynomial that you like on that interval and then you look at the benchmark of this of this polynomial you pick the corresponding coefficient and that's the guy in in all the in all and then basically um the proof follows by exploiting okay you know some approximation is made for this tailor you exploit then this ability and the local support you put pieces a bit together it's not too complicated but Um, it's not too complicated, but um, okay, if you are interested, I can show you. Okay, so and then say a few more words about, say, another basis. So, we have seen that this MDS basis is nice as long as the degree is high enough. There exists actually in the univariate case something much better, which is called the Biesbline basis. We define it in a recursive way like this. We will not go into the details. Not go into the details, they have again super nice properties, especially support, at least when it comes to approximation. And then the Bensteins are actually a special case of these things. And so also you can compute them by representing in terms of the Bernstein-BZ form. And actually, you can just compute this Bernstein-BZ form by just looking at their supports and posing, say, the smoothness of T. Supports imposing say the smoothness at the end points, and then the rest follows up to some normalization. Okay, so and that's also C2 and C1. Okay, so now let me move to the second part. I will go quickly because in the end, the second part is more or less the same story as in the unified case, just there are some more tricky aspects which for sure I can. Which for sure I cannot say too much, but just point them out what say what is going on. So, of course, we have to define no Bernstein polynomials on a triangle. Usually, instead of unit entropy, we go for say unit triangle. We define them in the same way, take say your coefficients, raise and so forward, and say we take as many of them as say and this. Many of them as say indices that we construct such that the sum of them is equal to the degree. And here I assume zero is equal to one minus the sum of the x and y coefficients. And again, we have a nice recurrence relation. Then you can say, okay, the unit triangle might be, again, constraining. You can generalize this to any triangle, again, using, say, some kind of Using, say, some kind of change of variables where, say, the general parameter x is written, say, in this form in terms of, say, the vertices of the triangle, which are known as the barycentric coordinates of the point x with respect to the triangle. Okay, so whatever you can do on the unit triangle, it works in general. Okay, here you see, say, some examples of quadratic polynomials. Again, you have. Covet on all again, they have the nice, nicely shaped, they have exactly the same properties: non-negative, they have one maximum, you know where it is. They have some symmetry, the integral and derivative forms, again, are super simple. They form a partition of unity and have also some interpolation properties at the vertices of the triangle. And then they are interdependent, so you can represent any polynomial in terms of this. Represent any polynomial in terms of this Wernstein polynomials. And then again, we can look, say, basically now I show exactly the same slides as before, just like upgrade them and bomb higher dimension. So we represent, say, okay, this is my point t. The coefficient, okay, which I know this is a vector of x and y, then my coefficient is also a vector, but you can quite easily find them by again taking. We find them by then taking a nice combination of the vertices where the end dishes play a role. They are called the main points. You can organize them in a nice triangulation in your triangle. So somehow you see which things you have to connect. No time to say too much about that. But at least you get, say, the flavor. Then this rise to counterpoints, as before, you associate this. As before, you associate these two constructions with these two main points, you can put them in, say 3D, you can connect them in this kind of fashion, and again, your polynomial surface will be nicely encapsulated in this control net. And you have again control on how this polynomial looks like. And again, this is a nice tool for geometric design, because if you the polynomial remains in the context of this step. Of this step. Okay, so then a few words about say evaluation. Again, there exist some extension of the Castello. The formulas look a bit ugly, but actually they're easy to remember. So again, say you can write your polynomial of degree t in terms of three polynomials of degree d minus one that you multiply with t0, t1, and t2. Two and basically, the coefficients that you select is just by somehow drawing away one layer of coefficients at here. So, let me maybe try to visualize. So, somehow, okay, here I just go for the V2 because it becomes already ugly enough. So, I have my full polynomial. I decompose it in two polynomials of the V. Okay, this is a quadratic polynomial of the V1. So, we see. Of the v1, so we've seen three coefficients. No, and this guy again decomposes polynomials of the v0, which are nothing else than my starting coefficients. You can do it for the pieces, and then you can say, okay, it's a big mess. But there's some structure if you look at it from, say, the right perspective. And let me help you in visualizing what we have to do. So we have to think of them as organize all these points, all these values in some kind of simplex. In some kind of simplex. Let's say the first layer is exactly the thing that you see over here. So somehow I connect them with the same name. Then say after one step, you basically compute another set of coefficients which you can interpret as coefficient of a polynomial of degree d minus one, which is linear, and then degree minus two, and so on. So we always go for the same kind of thing. And then you basically connect always. You will basically connect always the three coefficients that correspond to the triangle with the tip on top of it. And the weights are t0, t1, t2. They are, say, in-unit triangle, so convex combination, so very nice and smooth. Algorithm. Okay, I don't have a 3D visualization of the surface. And again, you can also say do the same for the set in the curve case. Of the set in the curve case, you could cut the curve into pieces. No, we can somehow cut the surface in three pieces. So, if I evaluate my point, say, for example, here, then I can, say, connect this point with the edges of my of my vertices of my triangle, and then I get, say, three sub-triangles. And if I represent the same polynomial on these sub-triangles, I can pick. Angles, I can pick the coefficients directly from the castilloid. And I did some attempts. Okay, the colors are not so great, but basically what you have to do, so let me maybe go back to this picture. So say the three polynomials that you have to do is, okay, remember, say the univariate case, where we go for the two extremes, the horizontal line and the diagonal. Here, we basically take the coefficients on, say, a boundary of the simplex. So, say. Of the syntax. So, say the back boundary, then we have the bottom boundary, and then the boundary in front. And that's what I try to do with coloring, but I'm not sure the color is so clear. Let's say these guys are exactly the coefficients corresponding to that part, these guys are the ones corresponding to this part, and then the others are see the last edge is with this and this and that. This was this, and this. Okay, so somehow again, this thing gives you everything. And again, we can ask us the same question: what happens if we go outside the triangle? Of course, the story, the answer is exactly the same. You can do it, it works, and we get the Pernstein within a representation of the same polynomial of the triangle outside, which is exactly the neighbor of the triangle we are interested in. So, okay, here I say colour sounds. Okay, here I say color some points. So we can compute the extension. And then what if we say change some of the coefficients? Say if I change the red one, then I basically lower the smoothness, I get something. Okay, this is quadratic. So I get just C1. If I change also the yellow ones, I get C0. And the rule in general is just say, say, if you have polynomials. So, if you have a polynomial, then you compute some steps in the Castillo to check a general smoothness across this edge. So, C0 is just to pick these guys. If you want C1, you compute one more layer. If you want C2, you compute an extra layer. And then, of course, in quadratics, you get the full frequency. Okay. And then you also have some for C1, some post-polarity things that you. Some popularity things that you can check. You can prove it in the same way. And then you basically can also build, say, basis for the space and exactly the same way. So somehow you build up an ideas by, say, figuring out, say, what could be the independent domain points such that it completely specifies the spine. And here, say, And here, say, if you're able to build, say, an MDS, then actually you also know that their number is exactly the dimension of the space. So, no, the space dimensions times is not easy to compute. So, somehow, these could be a tool to figuring out what could be the space temperature. And then, say, some examples. I go for a very simple simplifying relation to illustrate the idea. Say, for example, for Say, for example, for triangles, I start with this polynomial, then I want to have say C1, then say the first range is determined. If I look at this triangle, if I know this polynomial, then this is specified. So I can only fix this guy. So the meaning is the same. Black isn't yes. Open is determined. And then say you can repeat, say, once you have this thing, this is fixed by the smoothness. Is fixed by the smoothness, and then you can hit these, and you see zero is just everything together. But it's not as simple as I show here. Actually, finding MDSs is a hard and tough problem, especially because there's also something that is known as dimensional instability, which means that if you look at say a triangulation and somehow you have figured out the dimension. Somehow you have figured out the dimension, then it can happen that if you perturb one vertex in the triangulation, that the dimension changes. That's bad. So that's the unstable period. And usually you like to avoid this kind of thing. So say one famous example is say the one here with quadratics for triangles, where if these and even edges are somehow aligned, then you gain. Then you gain one extra value, or it's one more dimension than if they are not in the other. So that's bad news. And then you have some other issues. So for example, in this setting, the dimension is somehow stable, but the MDS is in a way unstable because, say, for example, if I take the MDS that I've chosen over here, then I cannot use this MDS on that region. And yes, on that, because that will not be an NDS anymore. Somehow, there's some instability in the MDS. So you have to be careful in building this. And then, of course, metrics. To avoid that, say, if you are particularly interested in C1 or graphics, then say one nice trick, which was already mentioned by Taibu, is using some kind of extra splits, what are called maker structures. Are called macro structures. And say one famous one for the quadratic case, the Polk Sequence split, which, okay, if you start from a triangulation, you say subdivide each triangle in six subtriangles by connecting, say, you pick a point here, you connect it to, say, the vertices, and then you also connect, say, two split points of neighboring values, and then further on you do whatever. And why the And why does this help? Well, say in this particular case, you get a very nice simple dimension form independent of the triangulation. So just two times the number of vertices in a triangulation. And you also can quite easily determine an MBS just three domain points, say, in the one neighborhood of the vertex. And that's you are done. It's stable, it's nice, whatever, whatever, whatever you want. Okay. Okay, so then you can build the basis. Again, for practical purposes, we want to have a basis which is local and stable. This is not guaranteed automatically, even if you know the basis, but you can ensure it if the degree is again high enough. And in this case, the bound is 32. So that's basically some kind of sacrifice you have to do to get, say, an. say um say an assurance of of of locality and stability and okay if you look at this kind of splits you can you can know what this thing and you can say do prove exactly the same things again the ingredients are exactly the same a local and yes and say they're the classiente plant exactly in the same fashion as before just one remark here is that um okay to have a local and ds you have okay again some restriction on the tree and so Okay, again, some restriction on degree and smoothness. In the univariate case, this was somehow constraining. In the bivariate case, it's less problematic because if the degree is too low with respect to the smoothness, you don't have optimal approximation anyways. This you can do. So somehow you have already inherent bounds on the degree and the smoothness to guarantee, say, this kind of optimal approximation. Okay, and then. Okay, and then say one last thing is that you can also build nice B-span basis for on triangulations beyond MDS. Okay, I will not, modify all the time, so I will just quickly squeeze over that. We don't have, say, as in the univariate case, a general construction, but say what we know, especially when we have triangulations that are subdivided by the PS6 bit, as I explained before, you can actually, you have Before, you can actually build a nice B-spine basis on top of it. And the way you can do it is just by, say, determining the basis functions, looking at some admit conditions. So the dimension of the mass was D times the number of vertices. Then this admit problem is specified as okay, you fix the function and the gradient values at all the vertices, and that determines your spine. And then your basis function is. So, your basis function is determined by fixing, say, one of these values at one vertex, and you put zero everywhere else. So, this guarantees you locality. And then the question is, how do you figure out this alpha, beta, gamma? So, it's just a nice geometric construction where you have a vertex, say you look at the first layer of the main points. If you then choose a triangle, you can say call it a red triangle. You can say, call it a red triangle in whatever way you want, so that it contains these points. This called this triangle, you can connect it to the values alpha, delta, gamma. And say geometrically, that basically means if you look at the surface, that the surface will, that this will be in the tangent plane of the surface. Somehow have some flavor of what is going on, and then you get a nice well-shaped function on a general term. Okay, that's all I want to say. So the bare stuff. Um, I want to say so the Bernstein-BZ form is actually a very nice representation of polynomials. They were originally developed for design, but since you have nice smoothness conditions, somehow they are a nice tool to analyze smoothness of spines through the concept of minimal sets. And then you can use it also to figure out what the dimension approximation and so on. And actually, you can also use this kind of tool for building better basis for. Of tools for building better basis functions, just trying to represent in terms of in terms of and okay, that's all I have to say. Sorry, we went over time. Yeah, thank you very much, Perfecto. So, you show how the construct is based on these predicted measures, right? So, more of those. Right. A more on a philosophical level, is there any chance that sometime someone will do something on general meshes? Or on the contrary, could this be used as kind of a proof? See, that's that's the holy grail. And say on general meshes for general diseases, I would say no hope. Because first of all, you have this dimensional stability. So maybe you have to lower the expectations, say, at least for spaces where you have stable dimensions. You have stable dimensions because if this dimension is not stable, I would say we hope to build the basis for it. And even then, the difficult property to achieve is exactly the non-negativity of the basis functions. The rest, say, partition of unity locality, that's more easy to do. Non-negativity is usually the tricky point. And for example, on say the simple split like the Pluff Tucker in three pieces. Like the Pluff Tucker in three pieces, that we probably know, then I don't know any solution for that. So at least not where you have, say, very locally supported basis, that's the, say, they have mid-bases that you would have. So with this support, no hope. You can actually prove it. But could this be used to prove it? You say no, but there is a change. Say for the closed tracker, I can prove it. Maybe five-line proof. I mean, with the same support. Say if you go for the one. Say if you go for the wandering, I mean the support of say the head mid bases that the classical basis with that support, no. Maybe if you go further, I mean that they cannot prove, but with this support, then you can show that it's not. So yeah, it's working on triangulation is tricky. It's not for a reason that this is, I mean, this problem is there since say the seventies and it's still open. And it's still open for 50 years. But at least for some spaces, you can do it. And these are, anyways, the more maybe the Kloftak is a practical space. But you can do it. And basically, for any smoothness, you can build a B spine basis, at least in the bivariate case. And 3DS is another story. Well, there are some solutions, but I would say in the bivariate case, maybe. I would say in the biofuel case, maybe we are at the point that we are somehow stabilized and maybe satisfied with what we know in 3D far from, and there is a lot to do in 3D. But at least there are some solutions. Could you go back to the slide where you had a picture of disability? Yes. So I'm going to ask a question completely from the internet. Completely from the ignorance. Somehow it feels to me like, let's look at the bottom, whereas you have an extra dimension that feels like that there's something topological going on, which is that the extra dimension is due to something zero because of the circuits. That's exactly, I mean, I'm not the right person to answer this question, but yes, that's exactly. You see exactly the same issues popping up also when you approach the dimension from a homological point of view, and that's. A homological point of view, and that's exactly yes, that's the reason because this slopes the same? I would say this is understood. But there is no homology there, but notice it's just you have different equations and then you have a relation when you have like three different slopes. Yes, but I would think that there should be an interpretation where you have something completely uniform going on. Something completely uniform going on, and in that case, some complex fails to be exact. It's just if you think, for instance, if you have those diagonals, if you have, if you want to make a relation which is equal to zero, then you will need to multiply those two lines together. While on the left, you can have a linear combination of them, and you don't need to go to degree two to have such a relation equal to zero with the left. Yes, in terms of the Bernstein BC representation, exactly that these points are collinear, and the constraint is exactly here, they involve four points, here they involve only two points. And that's the equivalent of what you are saying. But it's maybe a bit easier to see at least with looking at this picture. More questions or comments? This is not the case, so thanks again.