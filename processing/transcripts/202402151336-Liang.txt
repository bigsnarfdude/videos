For the very invitation and all the organizers for the wonderful workshop. Right, so today I will talk about randomization inference when n equals 1. As the title suggests, there is something regarding individualization or personalization. One approach is to fit deep neural networks, which is not the approach I'm going to do. I'm going to probably take you 100 years one. Look at really simple statistics and see how. Simple statistics and see how we can use that together with experimentation with one individual, how much signal can we teach that. This joint work with Ben Rect. Here's the plan. I will first motivate the talk, tell you why randomized control trials doesn't exactly answer the question I want to answer. And then we're going to look at the literature. Try to borrow insights and toolbox from the literature because this problem has been studied. Problem has been studied in a certain way in both causal inference literature and the system identification. And then from those two literature, we are happy with one simple model we sort of propose because they reach the similar model. And then I will give you the problem setup, give you an idea what is the type of question we want to answer. Then we'll move on to this. Again, no big deal or nothing. No big neural networks, but there is some notion of robustness here. Right, motivation. Right, so we all know randomized control trials are GREAT. Why is GREAT? It's a device to measure effects over a population. Not only is it a good device, it also gives us an idea of how accurate that estimated effect is. That estimated effects are. I give you an accuracy assessment. Great. But just knowing a certain policy or certain effect is, certain policy or certain treatment is good over a population. Say it works under 20% of the population. It doesn't tell you anything about whether you are 20% or you're not. A lot of questions deeply, they're individual questions. For example, I want to manage my I want to manage my own personal health. I want to figure out how do I design a physical therapy over time just individually to myself. I want to learn an instrument and leverage how to design a good sort of strategy for me to learn, how to treat a chronic condition. In the literature, there are this kind of end-of-one trial, meaning just clinical. Trial, meaning just clinical trial restricted to one individual, where you are both the treatment and the control interleaving over time, was probably has just as long a history as randomized clinical trials. For example, if you look at N01 trials that have been used to cure asphritas, find cost of ricins, many other diseases. Diseases. It also played a key role in the discovery of vitamins. People were treating a sequence of diet to chickens or humans or rabbits and try to figure out what are the crucial elements there that determines a certain disease. Very, very recently, it receives renewed interest, partially because of all these technical names. In operation research, people In operation research, people revisit this sequential A-B testing. Essentially, it's the same as the one trials. What is different, instead of treatment and control or two algorithms, instead of patient, you are looking at a group of users or maybe one users. Randomized clinical trials just divide the whole population into two groups. It's a cross-sectional study, and then you. Study and then you put one treatment for one group, control for the other group. Here, you have two algorithms. For one individual, you are going to design a sequence of treatment over time, interleaving, algorithm A, algorithm B, and then you try to observe how this user benefits or suffers from this treatment and try to tease out something individual from this user. It's called switchback experiments. Switchback experiments or interleaving designs, it is what is used in LinkedIn NetSpace. So, in particular, think about LinkedIn could simultaneously expose all job listings to the seekers in a given market and then design a sequence of treatment, whether you treat with a new algorithm or old algorithm, in a period of thirty minutes. In a period of 30 minutes. At the end of the 30 minutes, you randomly switch whether it is RNA or RNA. The interleaving strategy is a special case of this general methodology. For this very simple, very robust experimental design, how much information can we know? We're going to analyze that. But before that, I'm going to go back to these two literatures, see how much people understood. First, causal inference, the other is control. This is my data predictation. Is control. This is my data prediction. Cause of inference, probably is the most outdated reference 100 years ago. Niemann has this really nice paper studying agricultural experiments. Problem is very simple. Let's suppose you have two crops, different varieties. You have a sequence of plots, fields that you want to plant on. Something is unknown. It's unknown what. Something is unknown. It's unknown what are the potential yields in indexed by different crops and different plots, different tube. Very simple insight, but back then probably not that trivial. If we randomized the mean outcome in the treatment and control is estimable. Randomized control trial is a device for you to estimate the effect. Simple, we all know. Also, Neiman DeWide evaluated a formula for the variance. What does it mean? It gives you a Variance. What does it mean? It gives you a measure of precision for the estimated effect. Randomized control trials, not only you can estimate, but also you know the error wrong. And one big insight back then, but maybe now is taken for granted, is that probability theory can be used even when yellows from different class do not follow Gautenga. Whatever we study speaking to the randomness of the experimentation. Opens up a whole potential optimum framework to randomize the inference. We all understand randomized control trials. It's a good Then, randomized control trials is a good measurement device for integrated certain qualifications. But it is limited to answering individual questions. Answers question how a policy works in population. It's elegant because it removes all the confounding factors. You don't need to do causal inference using regression plus assumptions because it removes all the confounding factors. A reasonable estimation of the error bar, but randomized control trust does not answer. Regular mass control trust does not answer how a policy works for an individual. Also, what happens over time? Individual treatment effect? Probably not too much because to analyze this, you require so-called ignorability assumption. This effectively randomized spectral trials, each gene, the covariance. So, let's think about how do we extend Niemann's framework. Let's look at the assumptions. These are very simple. If we work on these, simple if we work on this potential outcome framework I'm just going to quickly go through it potential outcome work potential outcome framework essentially introduce two sequences of outcomes indexed by individuals yi1 or yi0 your observed outcome is one of these two it's a mazy method version each individual just observed one of the outcome and the binary treatment variable xi is either 0 or 1 Either 0 or 1 tells you what is the observed value, either it's treatment or penalties. If you equally write the observed relation model, it's very simple for. Here, the only thing I did is I make sure this random variable is more like mean zero. Then, for this cross-sectional data, what people care about is average hit manifest written this way. Like you care about, on average, one versus the other, S1 is also trivial. Smond is also trivial. Just write this symmetrized version multiplied by 2yi and analyzing how good this estimator performs. This is called Horizon estimator, one line. Very simple. But what it matters is that there is so-called stable unit treatment assumption, supa, meaning there's no interference. What do we mean? We mean the outcome yi, either yi1 or yi0, only depends on the treatment. Only depends on the treatment of your own. It doesn't depend on the treatment of other people. So it introduces, people don't like that assumption because a lot of real-world settings, there is interference, meaning other people's treatment will influence you also. Let's take it to time series data. What we have is one individual, right? We are going to run experiment over time. But certainly in this time series setting, why do I get treated? Whether I get treated in the past period have a big effect in my future outcome. So, interference here is real. You cannot uniquely ignore it. SUTWA is violated in either end of one trials or microeconomic studies, system control. They're all violated. So, how do we think about the interference? How do you model the outcome that does not only depend on the treatment at the right time, but also depending on the whole thing? Back time, but also depending on the whole treatment part. One naive idea is: what if we change the index? Instead of the index is one, it's any binary sequence. You can use discrete Fourier analysis, analysis of Boolean functions to analyze this. But curse of dimensionality makes anything here really hard statistical. So this seems mostly the broader version of considering all interference. Considering all interference, not so hopeful. How about in time series literature? People were studying these kinds of models, but restricting to a much smaller model. What if there is time invariant coefficient and those disproportionate analysis only concentrated on the first few eigenvalues? Right? In this special case, it's looking at there exist highly invariant coefficient, and you Time invariant coefficient, and your response depending on the positive treatment, just in this very simple linear from this sort of render causality, where they model this error by IID zero mean. It seems like a reasonable model to do something meaningful from it. It seems not too limited because, I mean, the time varying part may be time invariance part may be a little bit questionable, error part means zero or not, but still seems a reason. But still, seems a reasonable model to start. It leverages time invariance and linear reality to provide some useful answers. Whether time series X, for instance. How about system identification literally? What I just said is one potential way of modeling this interference. Let's look at other literature, which is the control theory. So what control does? Control wants to model the input output behavior. Model the input/output behavior of a system, bigger dynamic system, and further you want to design policies to maximize the certainty. So, in short, there is some underlying state that evolves according to some dynamic systems, where s is the state, x is the decision variable or the actions, epsilon is the noise part, yt is the output. So, written in this So, written in this language, there exists some unknown functions ft and h t. So, your goal is to try to find a proper estimate of the function, first is so-called system identification from data, so that the inputs XT can be planned to steer YT to better outcomes. I think about this: how to design a certain therapy so that we improve it. Well, again, the insight is FTN. Well, again, the insight is Ft and Ht are complex. Not a lot you can do. Let's restrict to linear class, which people do a lot. In the linear class, if you raw out what I just mentioned, meaning restrict the f and h to be linear functions, you can write out cleanly how the response depending on the past actions. Okay, the scalars of time variant. Let's further restrict. What if the linear dynamic system is timely mode? Dynamic system is timely modern. Then you have this really simple convolution model occurring. This is similar to what Granger's causality model looks like. Here the G has a very simple intuition. It's called impulse response function. Basically, we poke the system today, what is the effect tomorrow? The day after tomorrow, the day after tomorrow is tomorrow. So it is a good model-automodel interference. Right, so we are going to study these. We are going to model interference by the impulse response function and see how much we can feed out things. And because of the dependency and carryover effect, a lot of the statistical estimation question, inference question, suddenly become not so easy. But still, the high-level message is a lot of mathematics will work out. So what you can derive under Neiman's framework, you can equivalently derive. Can equip them with drive. That's one of the take-hole messages. So let me say, in the high level, what is the problem setup? There are two types of actions, A or B. Each time you pick one action, XT, one of these two, think about this algorithms or two peels, two drugs. Try it out, set on an outcome, YT, you want to measure. At the end, you want to infer the effect of A versus B. Which one sort of is better? one sort of is better by correlating, when I say correlating, I use a code here of the time series y2x. So in other words, your design sequence is Xt, a long time series. You observe the sequence is Yt. You have to do some notion of correlation study to t dial meaningful. It's challenging because of the interference. Y depending on the whole puzzle, I guess. Let me just say in words, sorry, in picture what I mentioned in the last picture. What I mentioned in the last picture. What I want to do is very simple: treatment or control variable sequence XT. That's the one sequence you'll have. The other sequence is the observed response YT. You want to estimate or infer some function nodes of the linear repulse function, an impulse response function, the green one, just based on observing these two noisy sequence, allowing a lot of robustness, which really. Robustness, which we all mentioned in a second. So, how to get back to this infinite-dimensional green curve out of just two-time series, allowing a lot of robustness. And why this is typical instance? Because sometimes the drugs had first when you take it, there are dizziness period, and then goes up, goes up, is a question. Your treatment rating is also a time series. Treatment time series. There's also no time series. Yes. Yes. That's very similar to my PEG or the function of my. Yes. There's certainly one interesting applications that. Your control X and then see how Y response. Yes, exactly. That's the system identification. I'm known. You're trying function. I only know the shape or you don't know the shape. You don't even know the shape. I want to be fully non-parametric in the shape. Great question. Great question. But certainly, that's one type of data we're looking at. I'm a theorist, I don't look at that data, but personalized house record is another sort of when you send prompts or interventions to people, and then people measure by wearing that or watching and see how it goes. And other the radical thoughts of both our applications. Okay, counterfactual reasoning. So, what do we want to answer? We want to have counterfactual. So, what do we want to answer? We want to have counterfactual reasonings. Hopefully, ideally, later, the control of the system. Once you identify the system, you can optimize the pulse. Of course, all this depending on the impulse response function. Without knowing the impulse response function, just by observing these two sequences, how to estimate g, how to do a certainty computation of the impulse response function, functions of solvents. So, in picture, we are going to. So, in picture, we are going to tell you how to identify it, how to estimate, how to provide a certain quantity for any kind of median functions very interesting. Right, what are the contributions? First, that's probably corresponding to the same. I want to robust arbitrary error sequences. I have no idea how this error is collected. It's not mean zero. If you're comparing two drops, in fact, this error could be the average effect of two drops. It's definitely not mean zero. I want it to be roll. I want it to be robust to arbitrary error sequences. It generalized the potential outcome framework for time series analysis, generalized Niemann's framework allowing for interference. A very simple notion of interference, but very important in time series setting. It proposed new estimators using convolution. They generalized horizontal estimator to type series. Again, I want to do simple, robust statistics because if I have strong experiments, I don't need to do very fast. I don't need to do very fancy machine learning. If I have not strong data, probably some more fancy machine learning may work better. Asymptotic inference, I want to add something to the system identification literature. But system identification is usually how to identify the system. Can you eventually identify the system? It tells you nothing about with the finite samples how much errors you have. Okay, I think I have 15 minutes. Now let's get to the theory and some of the different. Now, let's get to the theory and some of the difficulties. Again, let me rewrite what I just mentioned. I'm observing, let's look at the time-invariant impulse-response functions for a second. Some of the results can be generalized to time-invariant impulse-response function, not all of them. What I just mentioned essentially is saying a convolution model, but error, you have no idea whether it's mean zero, you have no idea what is the distribution of it. All you know. Distribution of it, all you know is X is some randomization sequence, Y is some response. You want to tease out function, where this is a convolution written in this linear convolution form. Meaning that if you look at this evaluated time t, it's just ig reversed linear combined with this statement x, where it's error or electoral, it's any error oblivious to the randomization of this. Oblivious to the randomization of things. You can even further relax this, but let's look at that for a second. Convolution models interference effect. We are going to consider time invariant, impulse response. But remember, impulse response function literally can be an infinite dimensional curve. We don't want to put any structural assumptions on that. That's one of the questions I'll ask. If you have, probably you should use that information and give you some wider guidance. Estimant, we are looking at linear functions of indexed by time invariant vector q. So these are estimants you're interested in indexed by q. You said a wide class of estimant could be look at the total memory k effect, meaning look at the short-term memory k, what is the total effect. It could also be a one vector, meaning memorizing, understanding the total impact. It could also be looking at the contrast of days 3 to 5 minus the Days three to five minus the factor of day one to see anything you specify. Estimator, we turns out that we are going to use further analysis, discrete further analysis, analyze. It's called inference questions. We define convolution estimator. Don't worry about this because we're using further analysis, so this is a circular extension, but that's not important. Just forget about this little circle there. We're going to take 2x minus 1. Going to take 2x minus 1 just as horizontal in this potential outcome framework and convolve with q. Maybe some structured version of q. Talk about that in a second. Multiply by 2y, take the average here. What are special cases? A lot of times people care about the cumulative lackey effect, meaning that I want to understand what is the lackey period, then you plug in the q to be a specific function. If you care about the if If you care about the effect, in the tomorrow, you plug in this Q to be just a delta function for some components. You specify that the estimator for the cumulative effect, you plug in the chunk in vector. Very simple statistics. You can generalize this to a time-variant case where the impulse-response function could differ across time. You can generalize what I just mentioned if the curve. If the curve G have only one term effect, if you choose this Q to be also one term effect, this exactly corresponds to estimate. It generalizes Niemann's framework. You can derive the thing that Neiman derived in their paper 100 years ago. But the interesting case we'll be looking at when there is interference, can you still do a symbol? Still, do a syntax. Can you still estimate what is uncertain configuration? Is there any asymptotic normality you can say? And in the next 10 minutes or so, I'm going to get into that. And depending on the time, maybe I'm not going to get into all the technical details, but hopefully it tells you what is the new component. First thing first, in expectation correct. This estimator is unbiased. Not too hard. It's a little harder than the Niemann's framework because here, Because here, whatever you analyze for NUMA, you just need this x variable. It's a mean 0. So 2x minus 1 is mean 0. And you just use the second moment. It's always 1. So only two moments. This you need to raise it to the first moment, but not too hard. This is saying that this randomization gives you a device to evaluate any effect involving this impulse response problem. Second is uncertainty quantification of that measured effect. How much it varies? Here's the variance formula. It looks a little complex, but I want to mention to you that this sort of telling you that the certain equal equation only mainly depends on some notion of G convolved with Q, some function nodes of G. In particular, if you look at In particular, if you look at special cases, it reduces exactly to what the demand derived for order something to estimate. I hadn't talked to you how to estimate this, because to make this useful, you have to estimate this based on the data. Turns out this is constant. We're going to estimate this function normal based on data later as a plug-in estimate for the variance. How about asymptotic normality? There's a big way to go from knowing the variance to knowing the distribution of it. I'm trivial to interfere. Here's the reason. Question: What is your interference here? I'm not quite sure. Interference time interference, right? Your treatment action in yesterday and the day before yesterday affects my future outcome. Okay, your interference is referring to like the previous time point could influence the future time point. The index is time here, right? If you think about over If you think about our cross-sectional data, index is person, right? And the interference is person are forming different groups of other people whose treatment may affect my outcome. Your treatment sequence is binary sequence? Binary sequence, right? If it's mean-zero constant variance. Do you have any specific kinds of mechanism for generating that? I'm looking at the most simplest one, which is the interleaving strategy, Netflix, and LinkedIn use. And that is very robust. And that is very robust, meaning it can tease out any kind of structure. But the bandwidth or the time is that's the question regarding experimental design. How to choose the bandwidth or the switch time is an experimental design parameter. We can further thinking about how to optimize, but that's a question we want to work on. This is a very popular stuff. Because that we have to study to give practitioner a good guide how to. A good guide how to what is the time frequency of one of the post-op systems. Happy to shout out some. Yeah, we can cheat later. Sure. So a syntax in the MALI, the sad truth is it does not always work. I can cook up counter examples with it. It doesn't work. That was first my thought. Because of the interference, things will be hard. But it turns out that under very mild conditions, that instance is not a lot. Meaning that as long as this impulse response decays, This impulse response decay is reasonable. And the support vector of Q is not a full vector. For example, the gross polylog in KT. You can show this as a double. The variance captures everything. Proving this is much harder. Because it's non-trivial for temporal independent problem, just having the variance, if you don't have a lot of independence, how do we deal with it? Have a lot of independence how to deal with things. But just maybe five minutes? So, five minutes, maybe I would just quickly mention what are the technical things. So, you can write out this. It's not too hard. It turns out it depends on some notion of a rather marker-Kels term and a linear term. This is not strong. Right? And then, if you write this out, you can sort of equivalently write it as a martingale sequences. As some marking out sequences, but marking out sequences involve some notion of a random marker Pelos term, where this middle weight matrix Hij depends on a really complex version of the function of your impulse response. And in general, that's not the symbolic model. So we have to be a bit brave enough to calculate higher moments. And once you calculate higher moments, it turns out. Once you calculate the higher moments, it turns out that some crucial properties of these function norms matter, that determines whether it's a normal or not. It involves some eight moment calculations. In these, you have to do careful bookkeepings. But this is saying that under some weight conditions on those functionals, even though this age depending on this most response function, you'll yield wave, you can divide distribution reductions. Distribution results. Again, it's now generally true. You can construct cases when this doesn't work, but it's true under these assumptions we have. High-level message as some is detecting G, convo fist Q, as some is not supporting really well. How to construct confidence intervals? I give you a very simple one, I tell you the shape of the distribution. Shape of the distribution, but end of the day, you want to compute an estimate for the error on the data. Turns out, because of the random poking sequences, you can show some form of uniformity, and based on that, you can have uniform consistency, and uh which gives us a way of have plugging estimates for variance using writing qualities. I don't think I don't think. Once you have that, you can control convenience interval. Here, compared to Neheman feature Rubin, where it relies on some notion of a correlation of the sequence, here we impose some time homogeneity. That's why we can sort of leverage to get convenience intervals, empirics, and skip. This filter direction, maybe one minute. It might work out, but it remains a difficult problem in practice. One of the questions is. One of the questions is: what is the adjacent measurement time window? That's an experimental design parameter you can try to optimize once you put down a certain continuous side model. But at least it's a measurement device for practitioners to tease out the interference effect without assuming anything about an interference effect. The interference is hard because essentially it's just one real time. So, for hard interference problems, this problem is just very, very hard. How about the interference of our networks? What we just did, you can generalize it to networks. Then the convolutional analysis, rather, the index by time could be indexed by some graph distance. And then it can solve some notion of interference question on networks. Experimental design. Other experimental designs you want to optimize. Or synchronous decision making. Once you identify the system, how do you design optimal policy to guide the system? We sort of do this in a Do this in a explore, then exploit fashion, but can you do better exploration and exploitation at the same time? All open questions. Thank you, everyone. Here's the paper information. Yeah, we like we only have time for one clear question. So uh the complicated term you wrote down is the uh I didn't uh chat very carefully, but it looked like Chat very carefully, but it looks like when we use the test case, it is rather even more simple.