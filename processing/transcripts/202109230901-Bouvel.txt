And you will see that the distribution I use introduces a bias, a bias depending on the number of records of the permutations. And this is a joint work which we have done a while ago with Nicola Aug√©, Cyril Nico and Karin Pivoto, and which we started a couple of more, a few months ago, with just Cyril and Karin to obtain a bit more information. I first want to talk about the context of this work, even though that's not quite what I will present today. So, the context of our work was originally the analysis of algorithms working on array of numbers. You can think of sorting algorithms, for instance. And when you want to analyze such an algorithm, you maybe start by modeling your arrays of numbers by permutations. And then you can do some average case analysis. Some average case analysis of your algorithm, how it behaves, assuming a uniform distribution on your data set. That's typically what you first do. But the results you obtain, also they give you some information already. Sometimes they are not quite realistic in the sense that they don't represent what you observe in practice on the typical use of your algorithms. And the problem. And the problem is that in some contexts, actually, that's quite common, that the typical data on which use an algorithm is not at all uniform. Typically, sorting algorithms are used on arrays that are already almost sorted to some extent. And that's something that has been studied on the example of the team sort algorithms by my colleagues with Vincent Jugi as well. So when we see this picture, it's not. We see this picture, it's natural to try and see if we can, in theory, say something that is more in line with what we observe in practice. So we have to find a way to introduce a non-uniform distribution on our data set, which is simple enough that we can study it mathematically, but which is still, I don't know, a bit more. Um, I don't know, a bit more accurate in terms of modeling the data that we have, so that's why we are interested in non-uniform permutations and studying these objects. So, I'm sure you're familiar with some classical models of non-uniform permutations, like the Ewens distribution or the Madows distributions. But those are not the distributions that I will be studying. Both have in common that in these models, the probability In these models, the probability of a permutation is proportional to a parameter theta to the number of some parameters of the permutation, number of cycles for events or number of inversions for the Malus distribution. And the non-uniform distribution that I use is of this shape also. So maybe it's better to look here first. In my model, the probability of a permutation will be proportional to a parameter theta. To a parameter theta to the power number of records of sigma. So a record is sometimes also called a left-to-right maximum, but I will use record because it's shorter. And the definition of a record is here. I'm sorry, I'm moving throughout the slide, but so a record is an element that is larger than everyone preceding it when you read from left to right. And here in this example, you have five records which are shown in bold. Bold. Okay, so the idea was that if a permutation has many records, then it's kind of sorted already. It's almost sorted in a sense that you can actually make a bit, you can make formal. There is a way to measure how sorted a permutation is. Well, to measure how a given statistic How a given statistic represents how much a permutation is sorted. And this was formalized by Manila, and we have shown that the number of non-records actually, you really see that this is what we want, is what is called a measure of pre-sortedness. So that's really a way to measure counting the records, is really a way to measure how a permutation is close to being sorted. And by And by giving weights to permutation, which are of this form, what we do is we give, choosing theta appropriately, we give higher probabilities to permutations that are almost sorted. So that's maybe a good model for representing the typical data on which sorting algorithms are used. So, although this non-uniform distribution on permutations is On permutations is new to our knowledge. It's closely related to the E-Wens distribution. Indeed, it's just the image of this E-Wens distribution through the Fouata bijection, which I'm sure many of you have seen, but just in case you haven't, here is a permutation. So imagine this one would be Evans distributed. You just decompose this permutation in a product of cycles. Of cycles. In every cycle, you start by the maximum and you write the cycle in increasing order of this maxima. That's what I'm doing here. And then you just erase the parenthesis. And that's the image of my original pernutation through the footer bijection. And the number of cycles is mapped to the number of records. So if this one was events distributed, the one that I have on the far right is a distributed record. On the far right is distributed in for the distribution that I that I'm studying here. So I will call this permutation record-biased. Please tell me if something is unclear, interrupt me, don't hesitate, or if you have questions so that I'm not just speaking to my computer. All right, so what I would like to be doing during this talk. To be doing during this talk is not speak so much of the analysis of algorithm, which was our original motivation, but describe things that we have obtained more recently, which describe rather some properties of this model of record bias permutations. And I will have three type of properties, let's say, that I want to talk about. First one is random sampling. I want to describe several ways that you can efficiently sample a random permutation. Efficiently sample a random permutation under this bias distribution. Efficiently means in linear time here. And second, I would like to describe you the behavior of some classical permutation statistics for our model. Essentially, the expected values of this statistic and their asymptotic degree, the asymptotic distribution. And this is where we had application to the analysis of algorithms, which I will discuss briefly. Will discuss briefly. I have just one slide about it. And finally, I would like to answer the following question: if you take a typical record-biased permutation of large size, what does it look like? And answering this question actually means describing the permutant limit of our permutation. So, you all have some familiarities with the permutons now after Peter Winkler's talk, but I recall. But I recall the basics, which is everything I need. Okay, so before we look at those three aspects, I just want to discuss the several ways you can think of a permutation because I'll be playing a bit with the different ways in this talk. So I have taken an example of a permutation here by By listing the integers from one to eight and their images, so very explicitly. And what I often do is I just take the second line and that describes my permutation, and I call that the word representation of the permutation or the one-line representation sometimes. But what is what I guess some of you do most of the time is you think of a permutation as a product of cycles. A product of cycles, so this one is maybe a bit not so nice for this decomposition. But anyway, you see what I mean? That's what I was using earlier when I had this water bijection. So this is a different way of thinking of those permutations, kind of octagonal to the first one, but it's kind of nice to have both in mind because of the relation with the Ewens distribution for our model. And finally, especially when I will be thinking of permutons, I'll be happy to represent a permutation by what I call its diagram, but it's essentially a permutation matrix where the one, one cell is here. So I have, it's an n for a permutation of size n, it's an n by n grid where I have one dot per row and per column, and the dot in column i is at high signal value. So this is the diagram of my permutation grid. Diagram of my permutation rate. So, that being said, we can start discussing our model and how to sample permutation according to this bias distribution. So, one way is to go through the fuata bijection using the Evans distribution, but maybe the most naive way, if I may say, based on the existing literature. Literature. So here I'm just reviewing how you do to sample a permutation which is A1's distributing. You will insert the elements from 1 to n and at every step you have two possibilities. Either you create a new cycle that consists only of i and you do this with a probability proportional to theta. So that's what you do here, or here for two, here for three. Here for two, here for three, here also for three. Or you can insert i in an existing cycle at every possible place with a probability this time proportional to one. So that's all the other edges in my tree. And I hope this picture is self-explanatory of what is happening for this sampling procedure. So doing doing this, you So doing this you can sample efficiently the errance permutation and you just push that through the Fourier bijection which you can implement in linear time to obtain your record bias permutations. But you can also do it without going through the Ewens distribution and the Fouetta bijection in two different ways, depending on whether you look at your permutation as words or as diagrams. Or as diagrams. So let's do it first for the permutation seen as words. And here is how you can do it. You will again, so you start with an empty array of n cells and you will insert the integers from 1 to n in this array and you have two possibilities. Either you insert in the leftmost empty cell, so doing so you create a record and this you do with a probability proportional to theta or you insert in Or you insert in any other empty cell with a probability proportional to one, this time you don't create a bracket. So the same kind of tree shows you the distribution that you obtain on a permutation of three elements. And again, if you are careful enough concerning the data structure that you use, you can implement this simple procedure. Implement this simple procedure in a linear time and sample the permutations efficiently. So that's when you see permutations as words and you have a similar procedure, which is different. If you look at the probabilities, they're not the same, but the normalization factor is different here and there. And here is how you insert your how you sample your permutation. Sample your permutations as diagrams for our record balance distribution. You will insert at every step a new column with obviously a new row, but you can choose the height of the row you insert and you put the new dot at the intersection of the new row and the new column. And you have the choice of either inserting a topmost row with probability proportional to theta or a row which is not topmost. A row which is not topmost, so it's below an existing point, any existing point, and you do that with a probability proportional to one. And here is the tree that you obtain to represent this sampling procedure, adding on permutation of side three. Similarly, with appropriate data structure, this procedure can be implemented in minute time, and so you have several ways of efficiently sampling. Several ways of efficiently sampling your commutations. So now that we have our efficient random samples, which we will use again later, we can stop playing with it in sample permutation and see what they look like, let's say. So here you have some histograms for some statistics on our record bias permutation. Statistics being the number of records, the number of designs, the number of inversions, and The number of inversions and the first value. Those have been obtained for size 100 and a million permutations for varying values of theta, and the color code tells you how to match the values of theta and the codes. So to tell you the story, we have produced this picture for a similar talk I gave at the A of A meeting in June, and we didn't know how to explain the pictures. We had a guest, of course. We had a guess, of course, but we had no proof. And now I can tell you that this is true. These are indeed nice bell curves. And I can even explain this picture here, because as I will show you, the value of the first element normalized asymptotically behaves like as a beta distribution. So I'll come to that once we examine the statistics more precisely. More precisely. The second thing we can do playing with our sampler is sampling many permutations, plotting the diagrams. So I've recalled what a diagram is. But anyway, the pictures that we have obtained with the samplers are these ones here in the bottom row. And they've just been obtained by overlapping diagrams of here. Of here it's 10,000 permutations of size 100 for varying values, varying values of the parameter theta. And you really see on the pictures that a typical diagram appears. And describing this typical diagram and proving that it's really the typical shape of a random permutation record by S1. Record-biased one is formalized by describing the pernuton limit of these record-biased permutations, and that's what we will do in the final part of the talk. So I'm reaching now the second part, where I want to discuss the behaviors of these flow statistics. Now is a good time for questions, if there are any. Otherwise, I move on. I have a question. When you show the statistics of the number of records, it seems to be a symmetric graph, but shouldn't it be just an exponential? So you mean this picture here? Yes. Because it's a power of theta. Sorry? Because the number of records should be proportional to a power of theta, right? Power of theta, right? So, why is it symmetric and not just an exponential graph? Um, so then theta is I'm not sure I know so excuse me maybe maybe I can help if I understand. I think there's a confusion because in the model, the probability of a permutation. Of a permutation is proportional to the number or the power to the theta to the number of records. But then you have different numbers of permutation with different number of records. So now when you look at this plot here, it is a combination of this weight and the number of permutations of different types. And that's why you get a different distribution. Okay. Yes, yes. Okay, thanks. Okay, thanks. Well, thank you. Svante, was it you answering the question? Anyway, thank you for the question and for the answer. My apologies. And yeah, here is the picture again. I could have gone directly here. Anyway, so just to state what we know about this picture, for our record bias distribution, record bias distribution we know the expected number of records which is this uh summation here closed form we so this is valid when theta is fixed or when theta depends on n we are not enforcing anything on theta at that stage but if we fix theta to be any positive real um we can uh yeah examine the asymptotic behavior of this Examine the asymptotic behavior of this expectation. And here it's a theta log n. And we can prove that we have nice bed graphs. So this statistic is asymptotic regunction. And actually, this is not new at all. It's just reading known results on the E1's distribution through the FOATA bijection. Because what I'm doing, if I look at the number of records for our distribution, it's just that I'm looking at the number of sites. Just that I'm looking at the number of cycles in everyone's distributed permutation and pushing everything through the Fourier bijection. So, all this is known and well known. I just want to point out that reading out this formula is actually very simple from the sampling procedure through the diagram. Because, on this procedure, you really read the probability that there is a record at a given position by. A record at a given position by looking at the probability that when you insert the ith column, you insert on top. So, this is just the probability that is given by the sampling procedure. And the number of records will just be the sum of the probability that there is a record at a given position. That's what you have here. So, you have a nice and simple way to recover this formula through the random sampler, if you wish. So, the story is a bit similar for the next statistic, which Bit similar for the next statistic, which is the number of descent. So I'm just recalling that a descent is a position in your permutation where the value at position i minus one is larger than the one at the next position i. We have similarly a closed form for the expected number of descents. It's asymptotic behavior once you fixed theta and the fact that the statistic number of descents is a symptotic reduction. And the point And the point is, this number of descent corresponds to a statistic. So, number of descents on our record-biased permutation corresponds through the flatter digestion to some statistic on A1's distributed permutations, which is less famous than the number of cycles. It's the number of anti-exceedencies, which means the number of elements, so the number of position i's such that sigma of i is smaller than i. Smaller than I. So actually, this exact one wasn't studied when the number of weak exceedences was studied, which is the number of I's, such as T movi, is at least as large as I. So it's the exact complement. Every index is either an anti-exceedance or a weak exceedance. And anyway, yeah, so these results were obtained by Valentine-Fehlier a while ago. And again, if you push Again, if you push here's the result, pull a filter vitation, you recover all those results exactly. And similarly, the formula that you have for the expected number of descent can also be recovered by looking at the sum of the probabilities that there is a descent at I, estimating those probabilities by using our random samples of permutations as diagrams. And you need to be a bit more careful because you examine two consecutive. Because you examine two consecutive elements to decide whether there is a decent or not, but that's not complicated in it. Okay, so it gets maybe a bit more interesting for the next statistic, the number of inversions, because this time I'm not aware of anything that corresponds to the number of inversions for our permutation on the E-Wen's permutation through the Fueta bijections. Nevertheless, we have the same kind of results. The same kind of results: a summation formula for the expected number of inversions, the asymptotic behavior of this expected number once you fixed theta. So here it's exactly the same as in the uniform distribution. And the fact that the statistic number of inversion is asynchronously Gaussian. All right, so I want to tell you a little bit how we proved that. I want to tell you a little bit how we prove that now that we don't have the known results on A1's distribution to help. So, we will count inversions finishing at a given position j. So, in sub j is the number of position, oh, maybe I should, sorry, I should recall what an inversion is in case you just forgot about that. So, I will say that an inversion is a pair i j with i less than j such that t. With i less than j such that sigma of i is larger than sigma of j. So it's really two elements that are inverted. But I did, which is what I call the inversion, is the position where the inversion is seen. So now in in j, I'm counting the inversion that have their second value at position j and clearly the number total number And clearly, the number of total number of inversion is obtained by summing over j these quantities. So our sampling procedure through the diagram is very appropriate for obtaining nice properties of the numbers of these random variables in sub j. Indeed, the number of inversions that finish at position j really depends on Really depends on at which height you insert the element in column J. Not of the insertions that you have done before, not of whatever you will do after, just of that single insertion and where in height you do it. So it tells you first that the value is completely determined by the height, but also that if you look at the number of inversion finishing at The number of inversion finishing at position j or at position j prime, those two are independent from each other. And this independence will be very useful, as you will see. So, but let me first discuss the expectation of my random variable in. Okay, first that of inf j made with this remark that I just did with. This remark that I just did with this sampling procedure through the diagrams, the probability that there are k inversions finishing in position j is the probability that I'm inserting at the corresponding height that gives me k inversion. So if I have no inversion, so k equals zero, which means I'm inserting in the top row, this is the probability of doing this. And every other number of inversion finishing at position j corresponds to every insertion not in the top row. So it happens with this probability. I have the impression my speech is not very clear, but hopefully you get the idea. So from these very simple probabilities, you just get the expectation of in j as a summation formula, you sum again over j and The sum again over j, and uh, let me go back one minute, and that's uh how you obtain this summation formula maybe by gathering a few of the traits, but really nothing fancy here, just using this sampling procedure of a standard concept. Now, for the asymptotic normality of our statistic int, the fact that we have expressed it as a sum of independent random variables makes it a very classical concept. Classical context for obtaining asymptotic normality. And here we just need to compare the order. Well, I just want to comment on those power here, which I don't know. You could have something different there, but we compare the order, which we compute similarly as there of those two sums, the expectation of in j to the power 3 and the third power of the square root of the nth. All right, so nothing very complicated here, but it provides you with something new about our model. So I'm discussing the last statistic, which I have, the first value, which of course behaves very differently. The types of results that we have are still the same thing. Still the same thing. We have a closed form, which is this time very nice for the expected value of the first element. Which, I mean, yeah, there isn't much to say about the asymptotic behavior once you fix theta here, but you can observe it if you wish. So, of course, as you expect, this depends on theta, unlike the previous statistics where you had the similar. Where you had the similar behavior as in the uniform case. Here, you really see a different behavior. And we can explain, we can prove that once we scaled, this first value follows a beta distribution of parameter one and theta. So again, there is an analog of this first value for record various. Value for record values permutation in a once-distributed permutation through the FORTA bijection. If you work backwards the bijection, what you're doing when you look at the first value is just that you look among the maxima in the cycles at the minimum value that appears. And I'm not sure this is surprising, but this statistic was not studied on A1's distributed permutation. So if you wish, you can also take our results on. You can also take our results on the first value in a record bias permutation, and they give you through the four-tabijection work backwards, let's say the distribution of this maximum, sorry, yeah, maximal value among the minima in the circles. Oh, the control, minimal value among the maximum in the circles. So, again, a small slide about how we prove this, because this time we are not using the random. Because this time we are not using the random sampling procedure as a diagram, but as words. So that's a that also justifies that these different ways of sampling our permutations have different interests. All right, so what does it mean if you look, if you think of generating the words or the arrays that represent the permutation as I shown with my little tree earlier, what does it mean that the first value is equal to The first value is equal to k. It means that the elements one until to k minus one have not been inserted in the as records in the first available position, but that the kth element was. So the probability that sigma one is equal to k is just the probability that for i equal one up to k minus one, you insert somewhere which is not the first available spot, and for value k, you insert where in the first available spot. Available spectrum. So you get a product formula for this probability which involves rising factorials. And if you sum that over k, sorry, if you compute the expectations, I mean the k times these quantities, simplifications arise and you get this very simple formula. I wrote magical simplifications because the way I can prove that is very much computational. Very much computational, recognizing coefficients of a product of generating functions, and I don't have a simple explanation for this simple formula. So I just want to make the point that if you have a simple explanation for this simple formula, I will be very happy to hear it. Anyway, that's how we compute for the moment this expectation. But these, let's say, magical or computational. Let's say magical or computational simplifications are not completely not interesting. I mean, they are useful because they allow us to obtain the expectation, but because we can also reuse them to compute all moments of our random variables. And the computation actually involves the Eulerian polynomial because this satisfies this nice identity, which I've seen or present in the slides of Sergio Lizardia. In the slides of Sergi Elizardia, as well, in a completely different context. So, Sergi, if you're there, just so you know, we are using similar formulas. And I find this coincidence kind of nice. But anyway, at the end of the day, we are able to compute all moments of our random variables and recognize after normalization the moments of a beta distribution. Normalization: the moments of a beta distribution, and we are just happy that we have explained the picture that was okay. So I have a couple more remarks about this distribution of statistics. First one is about the various regimes that you can choose for theta. For all four statistics, I had three results. A formula for the expectation, it's a simple. For the expectation, its asymptotic behavior once you fixed theta and the limiting distribution when you fixed theta. But the first result, the formula for the expectation, is true in general, not just when theta is fixed. Theta may depend on n here. And we have played a little bit with this formula we have obtained and looked at their asymptotic behavior, not just when theta is fixed, but in other regimes for theta. And here are the different behavior that you obtain. Different behaviors that you obtain. And you see that the number of decent and the number of inversions really behave like in the uniform distribution for a long while. The regime where really you observe something that is not trivial and is qualitatively different from the uniform case is this regime when. This regime when theta is equal to lambda n. And this is the regime that we will study in the final part of the talk when we will be looking at the per Newton limit. But before we go there, I have, well, I'm not sure. I had one more slide about the use of our results for the analysis of algorithms, but maybe I skip that. Maybe I skip that, but I because I think I'm running a bit short in time and I want to talk about this permutant limit. But if you want to come back to this analysis of algorithms, I'm happy to answer questions afterwards. So describing the permutant limit of record bias permutation in this regime where theta is equal to lambda n is what I want to do to explain these nice pictures that I've shown you earlier and which we obtained by overlapping. And which we obtain by overlapping diagrams of many permutations random sampled among the record binance distributions. So just to recall what you have heard yesterday about permutons. So what is a permutant? So I call them mu in general. And a permutron mu is a probability measure on the unit square which has uniform projections or uniform square square square squared. Has uniform projections or uniform modula, which means that whenever you take either a horizontal or a vertical strip of width B minus A, the measure of that strip is B minus A. So permutations are permutals, provided that you play this game, you take the diagram of a permutation, you normalize it to the unit square. It to the unit square, and you replace every dotted cell by a little gray square, and you distribute the mass one uniformly on those gray squares. So you have, I don't know, embedded your permutations in the bigger context of permutables, which are special measures. This is very handy to define what it means for a sequence of permutations to converge, because To converge, because you just inherit the definitions from the weak convergence of measures. In practice, it means that a sequence of permutations will converge to a permuton mu when the weight given to every rectangle by the permutone associated to my permutation sigma n and the weight associated to this rectangle by the permuton mu are asymptotically the same. So, I just want to point out that this family of rectangles on which I need to take the soup is very big, but if I'm looking at, which is what I always want to do, if I'm looking at the sequence of permutations where each sigma n has size n, I can take a smaller family of rectangles where I align the lines of my rectangle on the grid of my permutation. And this is very handy when. Very handy when you want to prove a convergence like all right. So, this is just the framework of the permutations where I want to state my convergence results for record bias permutations. So now let me take a sigma n to be a random record bias permutation of size n for the parameter theta, which is lambda n. The sequence of The sequence of permutations of the SSCT permutons, if you wish, they will converge in probability to a given permuton, which is defined as the sum of two things, mu C, which is something supported on a curve, and mu U, which will be uniform under this curve, and which are defined as follow. So I consider the curve of equation y equals f lambda of x for this. lambda of x for this function f lambda and I've plotted for several values of lambda the corresponding curves. Then mu u is the uniform measure on the area below the curve with a total mass one over lambda plus one times the area. While mu C is a measure supported on the curve which is the Which is the measure with density lambda over lambda plus x with respect to the Lebesgue measure on the unit interval on the x-axis that I've pushed on my curve. This is what I'm trying to say properly. So this describes a permuton. I haven't proved it's a permuton, but I'm telling you this is a permuton, the mu, which you make the sum of mu c and mu u. Of you knew C and me will, and it will be the limit of my record bias permutations. And in this statement, there are actually two steps: guessing the correct limit, mu and proving convergence. And I don't think showing you how you prove the convergence would be very informative, but showing you how you guess the limit, I think it's much more interesting. So let me show you, and that will be the final substantial slide. Substantial slide, how you guess this limiting permutum mu. So, first, the idea of expressing mu as a sum of something on a curve and some uniform below the curve was just suggested by the picture that I've shown you. The things we had to determine were the equation of the curve and how to distribute the mass between the curve and what is below the curve. What is below the curve? So, to find the equation of the curve, we have to keep in mind that the curve was the top part, like the upper limit of where there were points in my permutation. So, this really means that I'm looking at the records of my permutation and I interpolate between those. And this describes my discrete curve, if you want, and I want the limit of that curve. So, to determine the curve. So to determine the equation of this limit curve what you do is you look at where the last record what record is for a given position i you look at the maximum value that you have seen before it. So that's the last record that you have seen. And you can compute by combinatorial means these probabilities that the maximum value you have seen before position i is equal to j. And you do that when i and j are x and yn approximately. X and Yn approximatively, respectively. And this probability will just be a big formula that depends on X, Y, N, and Lambda. It has actually the form some factor times a big quantity to the n. And because you want this to be a non-trivial probability, you want the something to the n to be equal to one. Otherwise, you would get either You would get either probability tending to infinity or the sum of the probability once you sum over j being zero, which whereas it has to be one. So this quantity equal to one determines a relation between the x and the y. And this is exactly what gives you the equation of your curve. So now that you have the equation of the curves, you want to know how to distribute the mass between How to distribute the mass between the curve and what is below. So, again, my curve represents the records of my permutation. So, the total measure that I have on my curve should be the total mass of the records. And that you can just compute, just give you a Riemann sum, which converges to this integral, and that gives you, that tells you which measure you want to put on the curve. And now you have to distribute. And now you have to distribute the maps between below and on the curve in such a way that what you have is a permutant. And that's how you determine the C lambda and the fact that you need to take the measure with this density with respect to the Lebesgue measure on the curve, not to distribute it in a different way. So that's essentially the idea for guessing this limit that I've talked about. Limit that I've talked about. So, to just wrap up before you ask your questions, if you have any or comment on something, the model that I've studied is a model of non-uniform random permutations, where there is a bias that depends on the number of records of the permutations. This model was inspired by analysis of algorithms, and we had application there, even though I There, even though I did not mention them in the end, I've shown you that although this model is new, it's the image under the four-ata bijection of the known E1's distribution. And the results that we have obtained are some sampling procedure of our permutations and the analysis of some permutation statistics, some classical ones which were inspired, but where we needed for our analysis of algorithm motivations. Motivations. So for the statistic, we have an estimation of their expectations and limiting behavior in the theta fixed regime and in the interesting regime theta equal lambda and we were able to describe this permutron limit. So I'm sorry if I was a bit long. I thank you for your attention and I'm happy to take questions or suggestions. Any questions? Yeah, I have a question. Hi. So in this last slide, in which you say how to find the function f lambda, you said that there is this probability that has to be equal to one. So is this what you need? That theta is equal to. Where you need that theta is equal to lambda n? Or what is missing for having the same limit of the permutant for other regimes of theta? Yeah, yeah, you. Yeah, that's why you use that theta is equal to lambda n because what you have is some factor which I don't care about, but then something that depends on x, y, and lambda to the n. But I don't have any n. Don't have any n under, no, no, any theta. There are no more theta, they were just all replaced by 1 to n. And I haven't tried to see what would happen if we don't substitute theta for lambda n, but I'm not sure we would obtain something. I could have a look, but I'm not sure. Have a look, but I'm not sure if you're interested in terms of computing the permutation. So, the permutant limit, I guess, would just be the uniform or smaller lambda? Possibly, based on I'm going back, based on this table here. It wouldn't be very different from the uniform in any case. From in any case. So you have this method of figuring out the distribution of the first element, but this seems very specific. So can you extend this also to the kth element for other k? That's not at all something we have looked at. Maybe, maybe there is something we can do by picking the probability that sigma, I don't know, two, three, whatever you like fixed is equal to k. No, not quite. Maybe there is something, I don't know. Maybe there is something. I don't know. Maybe we can work out something with, I don't know, formulas which would be a bit more involved here by considering cases, depending on whether this second element is a record or not. But we haven't looked at it, but maybe it's doable. But then I don't know whether we could. We could, I mean, I see how maybe we could obtain a formula for this first step, but I'm not sure we would be able to compute the expectations and the moments point because I'm not sure the magical simplifications would arise again. Sergio, would you put your hand up? Yeah. So I have a question. So I was wondering if for all these statistics where you computed the expectations and the asymptotics, whether you Synthetics, whether you could also get exact formulas like generating functions for permutations according to the number of records and descent or records and inversions and so on. That's not something we have looked at at all. Maybe we could, because we have every time very explicit formulas for the probability that there is, let's say, a decent at a given position. So maybe we could put all that in a generating function, but I don't know what would come out of it. Like if you just count the records, then you get the sterling numbers of the first kind. So I was thinking if you have two parameters at the same time, you could get a generating function. A generating function with several variables, and maybe that gives a nice expression. I don't know. That's a very interesting question, whether you can combine. I'm a bit confused about how the distribution we are using comes into play with the generating, the continuum generating function. Yeah, so I'm just thinking if you if you have a. So let's say you want a generating function where you have a variable that marks the number of records. So that would be the generating function for the storing numbers of the first kind. Those would be the coefficients because permutations with a given number of records is given by those. But now say you add another variable where the exponent counts the number of inversions or the number of descent. You still get some nice exponents. Still get some nice expression for that generating function, and then, I mean, if you had that, then probably these results would also follow by you know asymptotic analysis of the singularity analysis of the generating functions, maybe. In several variables, which is not something so easy. Well, I think in this case, you could still apply like the flagella set weak tools. It wouldn't be like the really complicated version, I think, but I don't know. Maybe, yeah. I guess it depends on the generating function. I guess it depends on the generating function. But we haven't tried that food at all. But thanks for the suggestion. Yes. So there is a question in the chat from Ben Yang, whether you use the variational principles that Peter talked about yesterday for the permital limit? So, not at all. I think it's kind of orthogonal because the because the yeah the the way I approach permuton is really typically as limits of families of constraint permutations so here all pattern and all patterns can occur they I don't know what the densities of these patterns are but I'm I'm I'm really looking at those objects from a different angle. They are planetons as well, but I'm seeing them as limits of objects, discrete objects that I'm interested in. I'm not trying to realize some properties of the permutations, like containing a certain density of patterns. I'm not able to combine both approaches, but because I'm not so familiar, I'm not enough familiar with these variational principles, but I yeah. If it's possible, I don't know how it can be combined. If I may interject something here, the variational principle that would apply here if one applied is the original one where the basis. The original one where the base is the uniform permuton. And you wouldn't be able to use that to prove this because your permuton is partly singular and would have minus infinity entropy. So you had to use different methods. Jacopo, have you got your lap as well? Yeah, so it's more related to the choice of the model. To the choice of the model. So at the beginning, you mentioned that the motivation is that many data in the reality are partially ordered. And yeah, I totally agree with that. But now the question is, why you are biasing just the left to right maxima? In my mind, it would be more natural to bias the left to right maxima plus the right to left minima in such a way that you get something that is really You get something that is really close to the identity, not close to the identity on one side, but very unsorted on the other side. And I guess it should be not that difficult to get it from your result because if I have to guess, the record should be quite independent. The honest answer to that is that when we started this project, none of us were familiar with random permutations. Permutations, I mean, non-uniform distribution on random permutations. So we stay as close as possible to known things. So we took the image of the event through the freighter bijection. Okay. But that's the honest answer, but I agree with you. I totally guess that you can really generalize to this result because if I have to make a guess, is that if you take the left to right maxima and the right to left minima, And the right to left minima, the two lines behave sort of independently, and you will have with IID probability that if it's one of the two records, it will be with probability one half in one of the two lines.