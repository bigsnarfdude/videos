Okay, there we go. Okay. So first I want to start by just a quick spreading of the word of my book. So I wrote this for graduate students because I wanted to have some kind of help out there for not just teaching, but research is mostly on research, like the steps on how to do research. So the first part of the book. So, the first part of the book, I then focused on the transition and then the latex, all the things that you need, and then like collaboration and conference. So I just wanted to make it like a handbook. And the second part is about Nevi-Stokes, Schrodinger equation, Google, all the applications that I knew. I explained modern papers on these so to get people exposed to what's out there. Exposed to what's out there in mathematics. So, like in Google, there's a page rank which tells people, like, it uses linear algebra to figure out which side to list first. So that's page rank. And then I just picked some modern papers to explain and get graduate students to get exposed to them. So I just wanted to say that because I wanted to spread the word and try to advertise it. And try to advertise it because I wrote it for graduate students and I wanted to get it out there. Okay, anyway, so that's one thing. And then now let's just focus on the research. So I've done research on stochastic Never Stokes and Schulting equation. So Never Stokes is the fundamental model in fluids, and I think most of you have heard of it. Shorting equation is also important. It's usually used for optics and laser technology. And laser technology. So, the setting of Nevi-Stokes, we usually then start, I don't know if you can see the cursor, but we start with this equation, the first one, and then we write it as abstract form in the second line. And then these are the spaces that we use. H is just L2, and then V is H1. And then the solution is in this space, C continuous, and then H and L2 V. And L2V. And then for shorting equation, it's a little similar. It just has the complex version. Well, I don't know if it's similar, but the spaces at least kind of are similar. But you have the nonlinear term, and we usually just denote it as F of U for the nonlinearity. All right, so I want to focus on large deviations, moderate deviation, central limit theorem, and law vital lagath. So these are the limits that I study, and large deviation. I study and large emissions. There are two methods to approach large events. There is this asingon method, which is the classical version, the classical method. And then reconvergence approach is kind of the modern version or technique. And that one is by Buryaja Dubi and was extended in this paper by Arto Marlis in Analyze Parapolitique. And after this, And after this paper came out, a lot of people use the convergence approach and it has a lot of citations. So basically, moderate deviations is just large divisions on another equation. So like not equation, but it's the same equation, but you center your process and then you apply a different rate. So I'll explain all that in a second, but it's really similar. So large and model divisions are similar. So, large and modern divisions are similar. Okay, so this slide kind of puts them all together. So, for large divisions, you focus on the first equation, the original equation, and you multiply it by square root of epsilon, just a small noise. And then you consider the first equation, the original. So you don't do anything to the equation, but for moderate divisions, you subtract u0 and you multiply by rate that goes. By rate that goes to zero slower than large retention. So this rate A of epsilon, if you can see, A of epsilon over square of epsilon goes to infinity as epsilon goes to zero. So that means square of epsilon goes to zero faster. And that's the rate for large equations. So that's why it's called moderate because it's moderately going to zero. And the central limit theorem describes the normal Describes the normal events. And that one you don't multiply by anything. So, I mean, you just kind of have A of epsilon being one. So it'd be one over square root of epsilon multiplied by the central process. So square root of epsilon is cancel and you just have nothing in front of the noise. And then log e to log atrum, it's just multiply again the central process by square root of two log log square root of one over epsilon. Um, the square root of one over epsilon. All right, so uh, for the large efficient setting, uh, we need the original equation, which um, I just um considered Nervous Stokes for this example, and then the control PD, which we replace the noise by something L2, so an edge making here. And then the stochastic control is just the noise term plus the controlled, which is this is the controlled term. So that's the stochastic control. Term. So that's the stochastic controlled. That's the term that we usually use. Right, so that's the setting that we have. We have the original, and then these are the two other equations that we consider. And then Asencard method, so just comparing the two methods that we have in large equations, the two major methods. So Asencard method, you want to find large equations for X1, let's say. And for that, you need, for this method, you need. That you need for this method, you need another process that you know satisfies large divisions. So, you start with something that you know has large derivations with some rate function. And then you have to prove this fermion of Lenson quality. That's the key factor. And also, you need to prove that this map is continuous, which is in the setting of SPDs, is going to be the controlled PD, which is the second. Ed, which is the second equation. So that's the map from H to XH has to be continuous. And then you get long distributions by Asenko method. And then Asenka method gives you the rate function based on the rate function for the one that you already know. So the process that you already know. So Francoinson equality is the main ingredient of this method. And then for reconvergence, they also have two conditions. They also have two conditions. So, first condition is to prove that this set is compact, and this is G0 is basically the unique solution to control PD. And G epsilon of this, so that's just the stochastic control. So, stochastic control is just the control term plus the noise term. And you want to make sure that this goes to the G0, which is the control DPD in distribution. Control D P D in distribution as H epsilon goes to H. All right, and then satisfy large deviations by rate function, which is given very similar to what we just saw for. Well, actually, not in this setting, but you're going to see that the rate functions are the same. All right, in both cases. All right. So let's just talk about what has been done on Nervous Tokes. So regarding large revisions and So, regarding large equations and central numerical limits that I'm considering. So, large equations has been done by reconvergence approach. It hasn't been done by Asencom method. So, I try to bring in Asencom method in this paper, which was recently published. And this one is what it was moderate division. So, I chose moderate divisions to do because it Because it then gives me law of HR Lagath. So I can use the Asenka method, the Freedom-Winson quality, to get Law of Hm Lagatron. So I'll show you how. But for large durations by Circer and Sundar, they proved large durations for Nevi-Stokes, for 2D incompressible Nevi-Stokes. And they first proved a strong extensive. Proved their strong existence and uniqueness and proved it by weak convergence. And then Hagima Beshaya and Millet, they proved in SIM the large equations by weak convergence when the noise term is multiplied by the viscosity. So they said viscosity goes to zero. And this paper, Shovenshev and Millet, they proved like a class of Like a class of equations that describe all the fluid equations. So they get just grouped into one equation, and that equation could be having cases on Lerry-Stokes, Bosinesk, MHD, and Shell model. So they just grouped all the fluid dynamics equations into one and proved Lao divisions by reconvergence approach. And then model divisions has been done for reconvergence. For reconvergence, and this one is by Veng, Zheng, and Sheng. All right, and then central limit theorem was also done in that paper, in the same paper. All right, for stochastic shortening equation, so for reconvergence, I've done proved it with Zhao Yang Q, and we'd published it in 2020. And this was why. 20, and this was by V-convergence for stochastic Schroding equation, non-dinear. And Gautier was also in this business of proving large divisions, but he did it for ASEANCOM method. And modern divisions also worked with Lizai, Hanelor Lizai, she's in Romania, and we are finishing our paper. Finishing our paper for model divisions and the Lavajo Lagathrum and Central Lometric. All right, so stochastic neuristokes, just to go deeper into the result, is given by this equation. And this is the abstract form. And I proved the law of Lagathram, like the setting of law of visual logathrum, to be able to prove law of visual gathering afterwards. So the setting, this is something that we could use for. Something that we could use for moderate divisions because the rate function is also the same as what we need for the rate function for moderate divisions. So this is the process I used to prove moderate deviations to then be able to prove larger log at them. So I again chose this rate function and multiplied and this is the Z, the actual process and then the control PV I just And then the control PD, I just moved the W and replaced it with edge. And then the hasen-gone method, just a reminder, we need to prove different elements of equality and to prove that the map is continuous, the map which is H2XH, which is the control. All right, so fernomality in this setting will be given by this. So you just have, hopefully you can see the cursor, but so you have to. You can see in the cursor, but so you have the space of your solution. So z epsilon minus xh. And then this, you know that W is, so we considered a Q winner process. And based on this paper, and which is stated at the end of this slide, they again proved that large efficiency is satisfied, but the extended shoulders. But the extended Schiller's theorem that you have Law derivations for this WQ linear process. So basically, then this is a process that we know satisfies Law derivations. That's what we needed. So this we know, and then we want to find Law derivations for Z epsilon. And to make things easier, we can apply Grizzano's transformation theorem to write the probability in the more compact form, which is. In the more compact form, which is given here, and is basically the making z to be z the equation plus the control term. And this is going to be a lot easier because you'll have minus h and it's just something that we can work on. All right. And then from the mid-sided equality to prove it, the basic method is to use time discretization. And that's really what. And that's really what as and common method is about. You know, it's all about exponential estimates and time dispersation. So we use that and we break it up into two equations and two inequalities. So this greater than better and then less than better. So these two, and then we have to prove that they are less than or equal to exponential term given this way. And I wanted to point out that. Way. And I wanted to point out that the paper by David Nollard and Robera. So this paper proved large efficiency by Asencom method for some stochastic Walter equations. And this paper really helped me understand Asencom method. So I want to acknowledge that. All right. And for getting rid of the terms to get the exponential bound, we just have, I don't know if this looks a lot, but it's just doing time to discretion. Doing time discretization and then bounding each term and making t minus ti. And then this goes to zero. So you get rid of all the terms except the exponential term. And that exponential term is the one that will give you the e to the negative two r neural log. That's the term that we want on the other side. And to be able to get that, I looked at a lot of papers and all of them. And all of them wanted the integrand to be bounded. So they required the integrand to be bounded for this exponential bound. But I have the expectation of the integrant. So I have the expectation of this bounded, but not itself. So Sarai and Fragner, their paper just wrote the e to the whatever as the sum, which is a nice idea. So I Nice idea. So, I use their idea in this paper to find the exponential pound that I needed. So, instead of chow, I mean LD is the one that a lot of people use in this setting. But it worked and I got the exponential bound that was needed. Anyways, all right, so for modern deviations, this is what was done. First, I proved the Fernando-Winson equality and then proved the continuity of the Continuity of the map from H to XH restricted on the compact surface I, which is the rate function, and then have the model divisions and the model divisions is H epsilon. So this is like in Cameron Martin space. All right. Okay, so for a stress and compact lobby like atom, that's the one. So there are the other. That's the one. So, there are other law-visual lagathram kinds of topics. So, there are, I don't know, classical, there's like Tromga type, there are different types of law of lagathram, but strengthened compact law visual is the one that we will use for stochastic PDs. And for that, you need the process itself to be relatively compact. So, that's one condition. And we need a compact set such that the compact set is exactly, so you have to prove that compact. Is exactly so you have to prove that compact set is exactly the limit set of XJ. So this is the connection. So the connection comes out to be the compact set, which is given by the rate function of your large or moderate division. So these papers are the ones that connected large divisions or moderate. So some of them have proved moderate divisions to Lao Vigor Lagato. And the key Logato. And the key step is the key link is this set that you need, and you would, you know, format by rate function. And it's really started with Ducho and Strokebook on large erections. All right, so to prove relatively compact, I use this theorem in Gaga's Toles and Syne paper. They prove, I think. Prove, I think, exists and uniqueness for Nebraska-Stokes in this paper. But I have this theorem that I could use because I need relatively compact. And this theorem actually gives me the convergence of sub-sequence almost surely, which is stronger than what I need because I need convergence in distribution. So I just proved those two equations or I don't know, limits and got my relatively. And got relatively compact, so that's relatively compact part. And to show that L is the limit set, so yeah, we'd formulate L as the one that we just said, so rate function of the model divisions. And then we also have to, we also use for non-rent syllability, so that's another link to as and commeter. And that's and the so we have this. We have this Freedom's inequality, and we apply Borel-Cantelli theorem to show that the probability of Linstrope of this is equal to one. So that kind of tells you that the opposite of it is, well, yeah, that one is equal to one. Yeah. So it goes to zero and the opposite of it will be one. So yeah. All right. And then to prove that L is the only limit set, we prove. We use it by model division or use a model division principle to use the rate function. And then we have the L as the only limit set, and that's what we need. All right. So for shortening equation, so what has been done is there has been mouse solution. So the board and the bush are like the main. And the busha are like the main people, and like they haven't done a lot of work. And this paper by board in 2003 was cited a lot in what I saw. So they proved the mouse society in H1 for additive and multiplicative noise. And they also had earlier papers that they proved it in another space, but the one in H1 was the one that people cited the most. And then they used the semiconductor. And they use the semi-curve. And all of these papers, like most of them, except the last one, use the semiconduct of Schrodinger equation. And they also went by stretchards estimates. And stretchers estimates, whenever we have Schrodinger, like a lot of people, especially in deterministic setting, they use these estimates to bound the solution with semi-group. So when you have semi-group, you can use these. Semi-group, you can use these, and it has the technology, like especially with the space, like you have to make sure that the spaces are right for these bounds. So I went by this paper, the last paper, Lizai and Keller, which they proved variational solution and they have this space that is exactly the same as Nervous-Stokes equation. Nevi-Stokes equation that we saw. So that didn't deal with Searcher's estimates, and it was a lot more accessible to me. But the other people on Delta B board did like strong solution in H1 and then they focused on these estimates. But for Lisa Yang Keller's paper, they proved that variational solution, variational solution of this equation, which is just V in V. Just v in v, so capital V. So that's the text function. And they proved that this is, and the Vaison solution is stronger than mod solution. So it's kind of similar, but that's what they proved in the paper. And the nice thing about this paper is that they also proved that the real of if you, so this term, it's a fresh one, is positive. So when you ask. Is positive, so when you apply the Ito formula, you get this term, the second term in the right, and then you can just drop it. And if you remember, F stands for the non-linear term. So at the beginning, when you do it formula, whenever you do estimates, you can just get rid of the non-linear term and you don't have to worry about it. So that's the nice thing about this. And it got to be closer to. Be closer to Never Stokes and not dealing with so much about the complex nature of Schrodinger equation. And something about when you do A2 formula in complex version, complex setting. So if it has an I, you would have a negative imaginary. And if it is real, then it just be real of that. And it's exactly the same as the usual eater formula. All right, so is this any unique thing? So, existing uniqueness of the original has already been done by Liza and Keller paper. So, then we proved, me and Xiao Yang Q, we proved the existence and uniqueness of control PD and stochastic control PD. And we went by Galerican approximation method, which is the typical method for this existence and exists. And we went by variational as the variational solution still. And that's okay. Solution still. And that's okay for large equations. You can go by that instead of mild solution. Okay, so for this one, we proved it by reconvergence. So we need these two equations, these two conditions, as we said. And the first condition, let me just say that it's compact. There are two main methods to prove it. Some people then prove that it's sequentially compact. So then it will be compact. And some people use the fact that Some people use the fact that this space where edges is compact space. So, to prove the continuity of this map will give you compactness because the continuous version, a continuous map of a compact set is compact. So that's one other way. And then for distributed, is there a question? Oh, okay. So it's just all right. Okay. All right. And then All right. And then for the second part, is that it's a conversion in distribution. And that one is the tightness. Tightness is the key for that one. So that's what we kind of did. So in here, we first proved that U epsilon, so the controlled stochastic control PD, which is the noise plus the control, goes to the UH. Goes to the UH in L2 of H. And we use this Leon's urban Leon lemma. And it has this compact embedding. And we prove the boundedness in this set. So to get tightness. And we prove this estimate so that, like we chose sequential compact in this paper, and we have. And we have this one goes to zero because of our lemma. And then we also had the convergence this estimate, which we got by the lemma that we proved. So basically, we did timeness. I mean, we did estimates directly and also, you know, the paper in timeless. Okay, so then we got the theorem here and also. And also, we proved the exit problem. So, this second term is exit. So, what is the probability of the time exiting to be less than or equal to some given T0? And the upper bound, we proved it by Edel formula, just directly using the Edel formula on U. And then the lower bound was proven by large emissions that we proved. So, and also we did the expectation of. The expectation of T epsilon. Okay. All right. So moderate deviations. Oh, yeah. For moderate deviations. So that was for large divisions on non-linear shorting equation, the one that we just did. So this one we achieved for non-linear shorting equation. And then I tried to do moderate divisions on non-linear shorting, but I got stuck because for moderate divisions, you centered the process. Divisions you centered process, right? But then your non-linear term gets to be subtracted by f of u zero, and then this is no longer going to be cancelled because the things that they proved in the Lisa Kendall paper was just F and then U and then not like a subtraction, like not two terms or anything that was close to what I had. So I couldn't Had. So I couldn't apply any of the results that they had to get rid of the non-linear terms, and I couldn't bound the process. I needed to bound the process to be able to do my estimates. So I reached out to Lizai, and we got to just get rid of the non-linear term and replace it by linear. So in here, for moderate divisions, we just focus on linear shortening equation. So then you this. So, then you, this potential is going to satisfy like this bound on itself, it has boundedness, and the partials are bounded also. So then we went through and we did asencom method and reconvergence approach, and we also approved the central limit theorem. So, we got everything together in one paper. So, and the nice thing about that one is that we could compare. About that, one is that we could compare the two methods. So, for Isenka method, we have different winds and equality. And the space for H is a little different that people use is camera mounting space. And then here, it's a little similar, so it's still absolutely continuous. For as an method, they use mostly H dot, you know, the derivative of H with respect to time. With respect to time, and then the rate function is the similar to it's just h dot and an edge. But if you think about it, it's really the same. So I know they're different in proving, but it turns out that they're actually very quite the same. Because the first condition is that h to xh has to be continuous for eigencomp method. And for reconvergence, and as I said, a lot of people use continuity of xh. Continuity of XH to prove that this is compact. So you have exactly the same first condition. And then for the second condition, for Esenga method, you have the Friendly Mension equality. And if you think about that, is that this Z tilde is the stochastic control of the MPD, which is the noise plus the control. And the difference between that and XH has to go to zero because you're saying if it is greater than zero. Because you're saying if it is greater than rho, then it's less than or equal to exponential of negative r. So this is really smaller than a very small number. So and you have for reconvergence that these have to be the converging in distribution. And that's exactly the u h epsilon is exactly zeta or tilde. So you have the same two conditions here as well. It's just that Here as well. It's just that the rate functions and the rate functions are the same, but it's just the edge. So, like here is h dot and an edge. So, it's just minor, but the main ideas are actually quite the same. So, it was nice to compare the two. All right, so that was it. Thank you. All right. Any questions? So, maybe I have a very quick one. So, you had this comparison. It puzzles me. So, you have two different methods, you get the large deviations, but the results are different? No. So, I mean, large deviation is just proving. Like, you just have to give rate function and then you have large revision. So, if you're saying large revisions is just dead, you know, so your Both of them give you large emissions, you know. But the rate function is different, right? Yeah, it's just like I'm thinking maybe it's because of the space. Like here is in camera motoring space and this one is like in a different space. Oh, I see. I see. Yeah, I think that's why. But they should be, I mean, you would think that it should be the same. So I need to get deeper into that and be able to explain it a little bit better in the paper. Better on the paper because it's good to really compare the two and say that they're exactly the same. Okay, okay, I see. Thank you. Thank you. All right. So if there's no other question, let's thank Paris again. Thank you. All right. So finally, we can have some coffee. And should we start five minutes?