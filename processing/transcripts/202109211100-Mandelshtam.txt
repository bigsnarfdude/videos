Oliver Manista from Waterloo. So thank you to the organizers for inviting me to give a talk and also thank you for the opportunity to be here in person, which is great. So I'll be talking about joint work with Arvin IR. Joint work with Arvind Eyer and James Martin on McDonald polynomials and the multi-species zero-hinge process. I don't know if I can get this though. So, a quick outline of my talk. So, I'll start by a very short definition of McDonald polynomials and their connection to statistical mechanics. And then I will describe a new combinatorial formula for modified McDonald polynomials. And then I will define this particle model called the multi-species TASERP, a totally TASERP, a totally asymmetric zero-range process. And finally, I will describe a Markov chain on Tableau that projects to that taser. Okay, so a very quick review of symmetric functions. So we have our family of commuting variables, X1 through Xn, and Lambda is the ring. And lambda is the ring of symmetric functions in these variables with rational coefficients. And we say a function is symmetric if it's invariant under permutation of its variables. And there are several natural bases for lambda. So all these bases are indexed by partitions. So we have the monomial, symmetric functions, elementary, homogeneous, power sums. And so we have. So, one of the most important bases is the sure functions, so s lambda, and one way to define them is as the unique family of polynomials, which is, on one hand, upper triangular with respect to the monomial basis, and they're orthogonal with respect to the standard inner product on symmetric functions. So I will define McDonald polynomials in an analogous way. So let's now add the parameters q and t to our ring of symmetric functions lambda. And so McDonald introduced this family of polynomials p lambda which are upper triangular with respect to the monomials, monomial basis. Monomial basis, and they are the unique orthogonal basis with respect to the McDonald inner product, which is the standard inner product on symmetric functions with parameters Q and T. So this is the Qt generalization of shared functions in a sense. Okay, and here's an example of a McDonald polynomial. So, as you can see, it's not an actual polynomial, so the coefficients are rational functions in Q and T. Okay, so since their introduction, McDonald polynomials have been very widely studied. So, not just in algebraic combinatorics, but they also are quite prominent in other fields such as representation theory. Representation theory and algebraic geometry. And in particular, they simultaneously generalize a number of other important polynomials that frequently appear. So at q equals t, we get back the shear functions. At q equals zero, we get Hall-Littlewood polynomials. And we also get Jack polynomials by taking a certain limit. So now the So now the modified McDonald polynomials are another form of P lambda. So we denote them by this H tilde. And they are a combinatorial form of the P lambdas. And I'll explain what that means shortly. So they were defined by Garcia and Heyman through plethistic substitution from a normalized form of P lambda. So plethism is Plethism is essentially a formal substitution of variables. So that's what that notation means. And what is J? Oh, J is a normalized form of P lambda. So it's almost equivalent to clearing the denominators of P lambda. Okay. Okay, and so um yeah, so so I mean oh, I guess anyway so so the modified McDonald polynomials are our central objects in algebraic combinatoric. In algebraic combinatorics. So they are, in particular, actual polynomials with positive integer coefficients in Q and T. And that's why I describe them as the combinatorial form of P lambda because they have positive integer coefficients. So many people have studied the combinatorius of Studied the combinatorics of these polynomials. So Hagelin, Heyman, Lur in 2005 came up with this beautiful Tableau formula for both P lambda and H as sums over Tableau with statistics match and int. So recently with Sylvie Cortell and Lauren Williams, we found Williams, we found another formula for P lambda in terms of multi-line cues or in terms of multi-line cues with some additional parameters. So multi-line cues were mentioned by Svante in the last talk. So they are objects that describe the stationary probabilities of the multi-species A sub on a circle. And this was first discovered by Ferrari and Martin for the case. For the case t equals zero. And about 10 years later, James Martin generalized the formula for general t. So recently with Sylvia Courtel, Jim Haglin, Sarah Mason, and Lauren Williams, we conjectured a new formula for the modified McDonald, H soda, using MADE and H. Using match and a new statistic, which we called Quinn. And so this statistic is actually inspired by the statistics on the multi-line cues from the ASAP. And I should also mention that another formula for H tilde was recently found in terms of color paths and integrability by Garbawni and Wheeler. Like our Bollywood wheel. So there's been a long history of studying McDonald polynomials in connection with statistical mechanics. So here are just two examples of this. So by Dakonis and Rom and also Borden and Corwin. So they found some probabilistic interpretations for coefficients in McDonald polynomials. But most importantly, for our purposes, so So there's this theorem by Cantini, Tahirin-Wheeler from 2015 that says that the McDonnell polynomial specializes to the partition function of the multi-species ASEP on a circle. So when we set all variables to one except for t, then we get the sum over the unknown. The unnormalized probabilities of the A stuff. So that's what I mean when I say a partition function. Okay, so let me just recap what I just described. So we have this particle process called the ASAP. So Svante described the ASAP in his talk. So I'm not going to talk about it. Talk about that very much. And we have this connection to McDonald polynomials, so the P lambda is specialized to the partition function of the A sub. And then we have multi-line Qs and Tableau, which on one hand compute probabilities for states of the ASEP, and on the other hand, they give us a formula for P lambda. And so now we have the modified McDonald polynomial, H tilde. We have this conjectured formula with this statistic quin, and the quin statistic comes from the statistic on multiline Q's. And we are wondering, so is there an analogous interacting particle system whose probabilities can be computed by the objects in this formula? By the objects in this formula, and whose partition function is a specialization of the modified McDonald. And so the title of my talk already gave away the answer. So yes, there is such a process. So that's the multi-species taser, which I will describe in my talk later. So So let me now move on to the combinatorics. So I will describe the Haglin-Haman-Lura formula first. So let's start with a partitioned lambda and let's fix some positive integer n. So a diagram of lambda is so the diagram is a partition diagram, but we have partition diagram but we have uh our so we have k bottom justified columns so so k is the number of parts in lambda and uh column j has lambda j boxes so so here lambda is four three three two so this is not how partitions are are typically uh drawn in french notation um So a tableau of type lambda n is a filling of the cells of this diagram with integers 1 through n. And so in our case, we can take any filling of those cells. And each tableau corresponds to some monomial weight. So the weight is just counting how many How many instances of each integer there are? So there are three ones, three twos, three threes, and three fours. So that's why this is our weight corresponding to that filling. And the theorem, the formula is very simple. So the modified McDonald polynomial is McDonald polynomial is given as a sum over all possible fillings of this diagram where we take Q to the match, T to the Inv. So match is the major index. It has to do with descents on the Tableau, and INV is some statistic that looks at inversion triples in the filling. Okay, so now let me describe the new statistics. So, now let me describe the new statistic, QUINV. So, QUINV stands for Q-inversion, and I'll explain where that name comes from later. So, an L triple is a triple of cells in this configuration. So we have two cells that are adjacent in the same column, and one cell that's in the same row as the one below. So, to the right. To the right. And we can also have degenerate L-triples when both y and z are at the tops of their columns, and so the cell x doesn't exist. And so now a q inversion occurs when the entries of a triple increase cyclically when we read them counterclockwise. So here's an example of Of a Q inversion. So when we read this counterclockwise, we get one, three, four. So the entries are increasing. So that is one of our triples. Or that's one of our Q inversions. And then, so quin of sigma is the total number of Q inversions in sigma. So let's count our other one. So here's another one. One and then this one, and that's a degenerate human version. And so the total number of them is four in this filling. And so our formula is very similar to the HHL formula that I showed you previously, except that we replace the IN statistics. The inv statistic with the quinv statistic. And here's an example. So let's take lambda to be 2, 1 and n is 2. And here are all the possible fillings of this shape with ones and twos. And here are their weights. And then we take the sum over all of these weights. And that gives us the McDonald polynomial corresponding to 2, 1. Polynomial corresponding to 21. So I didn't tell you earlier what the in statistic is, but it is very similar to how we describe the quin statistic. So the inp statistic also counts the number of triples whose entries are increasing when read counterclockwise. However, the configuration of the triple The configuration of the triple is this gamma shape, whereas our triple is this L shape. And so these statistics appear very, very similar. However, there does not seem to be a way to go from one to the other. And we don't know if there exists a bijective proof of this result. And so So, moreover, the Quinn statistic gives us a very beautiful link to queuing systems and to multi-line diagrams. And that's where that name comes from. So, the tableau that I showed you are actually representing a queuing system. And we can think of a queuing system as an arrangement of lattice paths. And the map to go from a filling to this diagram is actually quite simple. So each column, so I color coded these columns to make it easier to see what's going on. So each column of the filling is going to correspond to a single lattice path in this grid. And so actually, this grid is on a cylinder. Grid is on a cylinder, so location one is adjacent to location four. And each entry is going to record the column in the grid that contains the corresponding particle. So if we follow this green column, we start in column two, so that's here. We go to column two, so that's here, and then we go to column one. To column one, and then we go to column three, and we end at column two. And then the blue one similarly is two, one, four, two, and so on. So that's that's our map from Tableau to Lattice Pass. So going back to the multi-line cues, so a multi-line cue looks very similar to this, except that each To this, except that each site in the grid can contain at most one particle, whereas in our case, we can have any number of particles at each location. So in a sense, we can think of this type of multi-line diagram as a pathistic version of multi-line cues, where we remove the restriction of having at most one particle in each site. At most one particle in each site. And so if we use the same map from fillings to lattice paths, then if we look at multi-line cues, then we get certain non-attacking fillings. So the non-attacking fillings are fillings that look like this, except they have some extra restrictions on the entries that we can have. Which on the entries that we can have in particular, each row has at most one of each integer. And the quim statistic corresponds to something that we can call a refusal in this queuing system. So we can think of the queuing system as a way to assign customer As a way to assign customers to services. So if we look at just two rows that are adjacent, so the row above is a set of customers and the row below is a set of services. And the customers are going to travel down to the row below and then look for an available service. And they're going to look to the left until they find somebody who's available and then they will match to it. To it. And if they find someone who's available, but they pass by it, then we call that a refusal when a customer refuses an available service. And these refusals, in fact, correspond to the Q inversions. So here, this 3, 4 corresponds to this orange particle traveling past the 1. Past the one and refusing it, and then matching with the particle in site four. Then we have another example of this, so where this green particle at site one refuses this blue particle, so that's the four here, in order to match with the green one. So that's where the Q inversion statistic arises from. Arises from. Okay, so now, so if we look back at this diagram, and if we isolate the bottom row, we actually get a certain particle process in this bottom row. And this particle process is called the taser. I'll define it here. So the taser is a So the taser is a continuous time stochastic process defined by Spitzer in 1970. So Spitzer also defined the ASAP, which was described earlier. And we can define it on arbitrary graphs, but in our case, we just have a circular lattice with n sites. And so here n is five, so we have five sites. And in the simplest case, we have k. In the simplest case, we have k indistinguishable particles, and the states are any arrangements of those particles on the lattice. And so in this case, and we record a state by just chopping the circle at some arbitrary location and then reading off the number of particles clockwise. So here we have 2, 0, 3, 1, 1. So that's Three, one, one. So that's so that's how we represent that state. Okay, and this is a Markov process on these configurations, and the dynamics of that process are given by some function f where the function f only depends on the content of the site from which a job. Of the site from which a jump occurs. So the transitions are: any particle can jump counterclockwise into an adjacent location, and the rates of the jumps are some function of the number of particles at that location. And what's interesting about this process is that the stationary distribution is That the stationary distribution is always a product measure. So that means that we can write the stationary distribution as a product of some functions of the numbers of particles on each slate. And so, yeah, so this is called a zero range process because the rate of each transition has no range, so it only depends on the So it only depends on the site at which the transition occurs. Okay, and so in our case, we have a multi-species process. So that means that we don't, instead of having k indistinguishable particles, we have particles of various species that have priorities over each other. So our species are going to be given by this. Given by this partition, where each part is a particle type. And we're going to say that particles of larger labels have priorities over the smaller ones. And so many, there exist many variants of multi-species taser. So a lot of them were studied by this Japanese group, Kuniba Mariamo Okado. Others have studied these processes as well. As well. And it turns out that all of these processes that they've studied are inequalable, which is very nice. And so the version that I'm going to describe, my next slide, was first studied by Takeyama in 2015. Okay, so in our case, we fixed circular lattice on n sites and they partitioned lambda for the types of particles. So here I've chosen B. I've chosen these to be my particle types. And each site can have any number of particles where particles of the same type are indistinguishable. And our state space is all possible configurations of these particles on this lattice. And again, I'm going to represent my state by By this multi-step composition, where I just chop my lattice at some location and then read it clockwise. Okay, so here I'll describe the dynamics of the process. So at any point, any particle can jump. So, any particle can jump counterclockwise into the adjacent location. So, each particle is equipped with an exponential clock. And so the rate at which a particle jumps is some function, is some polynomial in a parameter t, where t just keeps track of how many particles there are that are larger than it. Are that are larger than it in that site, and how many particles there are of the same type in that site. So, so here are just some examples. So, let's take this state. So let's say the 2 wants to jump. So, there are no particles larger than it. So, it jumps with rate x1 inverse. And x1 is because it's jumping from site 1. Now, let's say one of the ones wants to jump instead. Jump instead. So then there's one particle larger than it, and there's another one. So we have the rate t times one plus t also x1 inverse. And then here are transitions from this state as well. So that's how we define this process. Okay, so here's a quick example. So let's take a taser of tests. So let's take a taser of type 211. So here are all the possible elements of the state space. Here's our transition matrix. So these are the probabilities of the transitions from one state to another. And we are interested in computing the stationary distribution of this process. So that just means the left eigenvector. just means the last eigenvector corresponding to eigenvalue one of this matrix so two one one means there is two particles of confused what the lambda is oh lambda is two one one so there's one particle of type two and two particles of type one so so yeah so so lambda is just Yeah, so so lambda is just an ordering of all the particle types. Okay, so how do we connect the taser to tableau? Well, it turns out that we can, that each tableau, each filling that I described before corresponds to a state of the taser by reading off the entries in the bottom row. And so the correspondence is the following. So if we read the bottom row, the entry tells us which site of the TASER that corresponding particle belongs to. And the particles are given by the columns. So the first column corresponds to a particle of type 2 because Particle of type 2 because the column has size 2. And because there's a 1 here, that means that the particle of type 2 is in site 1. So it's in site 1 here. And then we have, so each of these columns corresponds to a particle of type 1. And so we have one particle of type 1 and site 1, so that's this one, and one particle of type 1 and site 3, which is this one. Three, which is this one. So these are all the possible fillings that correspond, whose bottom row corresponds to the state of the task. Okay. And so if you recall, if we recall the queuing system, so we had this map from a filling. So, we had this map from a filling to a queuing system with some lattice paths. So, the bottom row of that lattice path configuration is precisely the state, TEMO, is precisely the state to which this filling corresponds to. Okay, and so our theorem is that the stationary probability of a given Of a given state of this process is proportional to the sum over all fillings corresponding to that state. And so from here we get that the partition function, or which so by partition function, I just mean the sum over the unnormalized probabilities is a specialization of the McDowell polynomial. The McDowell polynomial when we set q equals 1. Okay, so here's an example where I compute the stationary probability for this state of a taser of type 211. So here are the four possible tableau that we can draw, and these are their weights. weights and so uh and so yeah if we if we sum if we sum over all these fillings then we get this probability here that we calculate okay so our proof uh for this result is it's actually very combinatorial so we so we're going to define a markov process on the fillings themselves and that The fillings themselves, and that Markov process is going to project onto the Taser Markov chain. So, first, some notation. So, let's take a cell in the filling. So, the arm of that cell is all of the cells to the left in the same row and all of the cells to the right in the row below. And so, arm. So, arm of sigma c is the number of cells contained in the arm with the same content. So, let's say the content of that cell is K. So, we can, if we identify all of the Ks in that filling, and then if we count which ones or how many of them lie in the arm, that's what the statistic represents. And so in this Markov process, each cell that has content different from the cell below is equipped with an exponential clock. And the rate on that clock is T to the arm of that cell times X inverse of its content. So here's, and a transition can be triggered by Can be triggered by any cell. So these transitions are often called ringing paths on multi-line cues, for example. So if we trigger a transition by a cell, then we're going to look at the string of cells that are contiguous and cyclically increasing above that cell. And we're going to increment each of them by one. Going to increment each of them by one. So the three becomes a four, the four becomes a one, the one becomes a two, and the four becomes a one because my maximal value is four. So n equals four in this case. Okay, so that's my transition, and the transition occurs with rate t to the arm. So the arm of the cell is one. Is one because there's one three in this in this set of cells times x three inverse. Okay, and the theorem is that the stationary distribution of this Markov process on the tableau is x to the content times t to the quinth. And okay, and so note that. So, if a transition is triggered by a cell in the bottom row, then the rate of that transition is actually going to match the rate of the transition corresponding to that particle in the taser. And so, I just described the chain on partitions where our Partitions where all parts are distinct, so that means that we have one particle of each type. If we have repeated particle types, then we need to do a little bit more work to define the Markov process. But it is essentially defined in this way. Okay, and so how we prove that this is the Margo process that Margo process that with the probability that I mentioned. So we're going to show that it satisfies a certain balance condition. So let's take a state and let's look at all of the possible outgoing transitions. And let's take the sum over the weights of those transitions. And then let's take all the possible incoming transitions into that state and take the weights of those. That state and take the weights of those. So the balance equation tells us that the sum over the weighted sum over the incoming transitions has to be equal to the weighted sum of the outgoing transitions. So if this balance equation is satisfied for this definition of the weight of sigma, then we have given the correct statement. Then we have given the correct stationary distribution on the smart belt sheet. And the proof for this is very combinatorial, so we just show that there is a bijection from this set to this set that is weight preserving. Okay, so and so now we have to project this Markov process onto the taser. So, first of all, we notice that First of all, we notice that the distribution of the bottom row is the sum over all possible fillings of the rows above when we have a two-row filling. And so now we can use this fact inductively to show that when we have a filling for an arbitrary lambda, then A filling for an arbitrary lambda, then the distribution of the bottom row, which is the taser, is equal to the sum over all fillings of the rows above. And finally, we show that the dynamics on the bottom row of the tableau is equal to the dynamics of the TASERF. And so that proves that this. That this Markov chain projects onto the taser with the weight that we gave. So, here are some other probabilities that we're able to compute using this enriched Markov process. So, first of all, so if we look at the top K rows of a diagram, A diagram, then the probability of observing that configuration in the diagram is equal to the probability of this filling. So that's one interesting fact. Another interesting fact is that if we look at If we look at fixing the configuration of the first L sites of the TASO, then the probability of that is going to be symmetric in the remaining variables. So the probability of having my first L sites fixed by some word W is symmetric in the variables XL plus one up to Xn. So that's So that's also an interesting fact that we found. And finally, we were able to get an explicit formula for certain densities. So if we look at the expectation for the number of particles of type J at site I, so this is equal to this formula where, so we have h H tilde corresponding to the partition that consists of a bunch of ones. So this corresponds to lambda being just a column, a single column. So and the proof of this is quite nice. So we can do this by just So we can do this by just extending the simple calculation for lambda being a single column and then using the fact that particles of higher species see all lower species or they don't see any of the lower species particles. And so we can just project a multi-species taser onto a single species taser, essentially. A single species tissue essentially what we're counting densities. Okay, so just some final remarks. So it would be very interesting to find explicit formulas for these fixed configurations on the first L-sites of the taser. So far, we just know that these probabilities are symmetric in certain variables. Certain variables, but it would be great to have some more explicit results there. Also, on the combinatorial side, it would be very interesting to find a bijection between these two statistics that appear to be almost the same. But yes, and then we have some partial conjectures. Conjectures in that direction. And I will stop there. So, thank you very much.