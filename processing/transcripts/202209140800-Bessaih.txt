Thank you very much. Thank you very much for all the committee for the invitation and I am happy to be here with you guys even virtually. Okay, so today I am going to talk about some results on some numerical schemes, mainly time numerical schemes. Time numerical schemes because in these models we have also the spatial numerical scheme, but I will not talk about them, I don't have enough time. But numerical schemes for some hydrodynamic models. I will start with Navier's talks, but later I will give some results or some maybe some projects that we have on other models related to hydrodynamics in general. And these results. General. And these results are in collaboration with Animie. Actually, she is the expert on numerics. And we are also doing some theoretical results on these hydrodynamic models in general. Okay. Okay. So, okay, sorry. So, okay, so the outline of my talk is, as I said, I will start with the 2D Navis talks, the stochastic 2D Navy Stocks, I will introduce for the non-experts. introduce for the non-experts i will give it's pretty well posed and i will give you just in general the word the the theoretical results in general and then i will focus especially on the fully implicit Euler method because in this week I think some other people introduced other kind of numerical methods but my focus will be only on Euler only on our implicit auler method and we i will show you the the algorithm i will show you the speed of convergence some results that we have and then i will introduce you to these other models and then some work in progress okay so uh 2d stochastic navy stooks so what is it i will just introduce you to the model and um specifically this is Specifically, this is actually a restriction. We are working on a two-dimensional torus. So we have periodic boundary conditions. So we don't have any results for bounded domain with, for example, Gilgret or other conditions. Actually, we haven't seen any results in this regard and we have, you know, we are thinking about it. But all these results are in a periodic only condition. A periodic only condition in a domain with the torus. The equation here is given by the equation: the velocity, u is the velocity, p is the pressure, divergence equal to zero. This is the incompressibility condition. And then these are the periodic boundary conditions. So the period we take it to be L in general. So we take the torus to be zero L square with the boundary conditions and the initial condition. The boundary conditions and the initial condition is u0. Later, I will write it really in general, very, uh, in a very general setting. I will write it, rewrite it as a three, where A here is the stocks operator with the domain, its domain, and the nonlinearity here projected on divergence equal to zero is this d of u. This is our non-linearity specific of linguistics. The domain, the functional setting where we are working. Of setting where we are working is the space H indoid with the L2 norm and the space V indoid with the H1 norm. And our Navier-Stokes equation is given by the following equation. Our noise is multiplicative. I will give you the assumptions on this. So before I go ahead, the fact that these conditions here, this lemma, these are nice properties somehow. These are nice properties somehow for the Navier-Stokes equation for any boundary condition. This condition, you know, this lemma is true for any boundary condition, while the restriction where we really need it, condition number two here in this lemma, is specific for periodic boundary conditions. And without this condition, we cannot implement and we cannot have our results. This is where our Results. This is where our restriction is embedded somehow. And let me just say that the non-linearity of the Navier strokes is not lips, it's not globally leapsheet, but it is locally leapsheet in the H1 norm, let's say. So, for example, it shows in the following. So, for locally, it is lipstick, but it is not globally. So, the non-linearity is. So, the non-linearity is pretty important for us. So, I will give you some references about the people that have results on the Navier's talks. I'm sure there are many and I have to apologize for people that are working on numerical schemes in general. All my references are really related only to the Navier Stooks with the rate of convergence, by the way, because there are other people that are working on. That are working on numerical schemes for some approximated model of the Navier-Stokes, but they don't provide rate of convergence, so I am not mentioning them here. So an interesting paper, actually, even the first one doesn't have rate of convergence because it's a 3D dimensional case. The first one here, Baloch Regina and Caroline Hall, where they implement a time and space discretization. Discretization. The second one, also Carrie Paul, is the 2D. So they have some rate of convergence, but they are localized. So this means that they are only in probability. Our paper with my paper with Annie and Jezniak, this is about splitting. So it's another kind of somehow algorithm in time. And this paper. And this paper, Bright Doxon, with this, this is also about 2D Navier Stokes, multiplicative noise. And it makes, so it improves the results of Caredip Hall, because the results of Karedi-Hall had a rate of convergence of order one-four, while Brett Dachshund tried to, you know, make it almost one half. Okay? But still, Okay, but still. Can I just ask a quick question? The last paper, is it converged? Is it for the Merkel scheme in time or in space? And which kind of Merkel scheme is it? It is both of them are time and space. And do you remember which scheme they have in time? Ah, they have implicit aler. Yeah, implicit, and we do also the semi-implicit, by the way. I am not mentioning it here, but also. Is not mentioning it here, but also the semi-implicit owner. Okay, thank you. Yes, all of these: the one, three, one, two, and three, they are Euler schemes, fully implicit, and there is also results about the semi-implicit, but it's really Euler, and the convergence is in probability only. They're localized, and there is rate of convergence. Our paper, actually, where we do. Our paper actually, where we do with Ani, we started our result that was somehow nice. We were able to have, in this particular case, there was only time rate of convergence. We did the Euler, a fully implicit Euler, but we have also semi-implicit Euler. And in this particular case, we were able to have a strong convergence, so L2 convergence. This is what I'm going to describe today. Is what I'm going to describe today, and then we were able to implement also space and time using finite elements. Again, this is new space and time, and we have also rate of convergence, strong rate of convergence of order almost one half. And the last two, actually the third one, I'm going to talk about the third, the third result. The third result here, which is with an additive noise, actually, we can do it directly, we don't need a localization. I hope I will have time to go through everything. Stop me anytime if you have a question. For the last result here, it's an approximation of this. This is 3D. This is 3D, but we don't do 3D in Abyss took because we don't have uniqueness. And so we do some regularization. So, we do some regularization, and this is the Brinkman for Scheimer Navier Stokes that I will show you at the end. Okay, so my assumption. So the W here is we take it, it's a K-valued Brownian motion on a probability space. The initial condition, we can take it to be deterministic, but we're in H1, or to be random and to be LP in omega. In omega with values in H1. Okay, with I will give you some other properties, probably I will need a little bit more if I take it to be random. Okay, for the G, the diffusion coefficient, we will take it to be, it's an operator which is Hilbert Schmidt from K into H with the following linear growth and lift sheets, globally leap sheets. So very nice. We take it really to Very nice. We take it really to be nice. We can take also an extra condition: is that it is the H1 norm of this diffusion coefficient is also has a linear growth in the H1 norm, Lieberchief. Okay, so G and G1 are necessary if we want to describe solutions that are strong in the sense that strong in the PD sense. And it's pretty classical. And it's pretty classical, it's known. So, if you assume G, actually, you can have the existence and you take the initial condition to be just in L1. The solution exists and is unique, and almost surely it's in this very nice space, continuously with values in H in L2, and L4, you know, even any LP with values in H1. And the solution is a generalized solution, almost surely. Almost surely, so almost surely against the test function, you can define the solution. And we have very nice estimates, or the moments are nicely defined. As I said before, if you assume a little bit more for the diffusion and for the initial condition, in case you take it to be random, if it's deterministic, it's clear, then the solution becomes a strong solution in the P D sense. So it becomes continuous. P D sense. So it becomes continuous with varies in H1. Okay, so we are dealing not with the weak solutions but with strong solution in the PB sense. And we have even better estimates. For example, moments of the solution, they are in the H1 norm. The super, the expected value of the super inside of the U of T in the H1 norm of any mode, any order to P. Any mode of any order 2p are bounded and more. In particular, this is also, I mean, we proved it in our paper, but actually it's pretty classical. If the noise is bounded, if you have a bounded noise, so if k1 is equal to l equal to zero, so it's really a bounded noise, so we have also exponential. First of all, exponential moments of the norm of U in H1, so the gradient. With the condition, we have to pay for it. The condition is that the coefficient in front of the norm has to be small enough of order the viscosity divided by the intensity in front of the noise. So this is nice. This is a nice property. The other properties that are pretty known for these strong solutions, proven in both papers of Brezhniak, Caroli-Pol and Caroli-Paul, is the regularity, the time regularity of the solution. So in particular, and this is very crucial to prove this rate of convergence, because it would be given by this regularity. So Akima. so uh akem i have a question um yes so your noise your spatial noise is quite nice right no my spatial noise is not white it is uh it is called uh it's nice it's very nice very nice and is it an assumption which is uh standard i mean is this assuming all the references yeah yeah yeah okay for the existence i may i also ask May I also ask a question for excuse me? Let me before I because I want to be precise because maybe Arnaud is Arnaud is here. I mean, there are people have looked at Navier Stokes for a noise that is not very regular in space. There are some results there, but for now, as far as I understand, it's very difficult to get. It's very difficult to get solutions that are with space-time-white noise. And even if you, you know, when you look at them, you are not looking at them in the class of strong solutions. They have to be really very, very, in a very wide class of solutions. Yeah, but you could get a mild solution with the rest-type noise or. Re type noise, or I don't know. Maybe that one may be better. Space-white noise, space-time-white noise. No, no, no, no, no. I would not ask. Okay, yes. I think there was somebody else wanted to ask a question. Sorry. Oh, yeah. Can you hear me? Yes. Oh, right. Okay. I saw that you couldn't hear me. So you have some assumptions for G. Are there any interesting examples? For G, are there any interesting examples for such a G which satisfies those under linear growth conditions? Yeah, I mean, we didn't write them, but with other papers, we can describe them explicitly, actually. Yes. You know, like we can write them using just how is it called? Yeah, we can write explicit. Explicit, you know, shape diffusion and with this follow the following noise. Yes, all right, okay, thank you. Yes, okay, with Annie, because we need a very we need more like norms, so we had to prove because it was lacking in their paper. So we recently proved that we have some kind of regularity on Some kind of regularity on each, you know, small time interval in the H1 norm. So, if you take the tj, which is you take a partition of the interval 0t, for example, I will describe it later for the algorithm, then we can prove the following, you know, this lemma, which is very nice. For any q bigger equal than one, you have the following. A nice property related to the regularity, but it's this is specifically we constructed it in such a way we use it for our convergence, rate of convergence. Okay, so let me describe now the algorithm, the fully implicit algorithm. I think there is no surprise there. We take a partition of the interval zero capital T in n points. Tk is k capital T over N. We start with end we start with the initial so we construct the sequence u n but the initial one at point t zero is the initial condition u0 h is the step size and we construct you know the goal here is to find this sequence uk in h1 in h1 such that we almost surely with this is our algorithm as i said this is we have a multiplicative knowledge and this A multiplicative noise. I'm describing it for the multiplicator. Okay, and this delta k is just the increment of the Wiener process W. Okay, so actually Carolie Hall and Brett Docson and many other people they use this the same algorithm. This is the fully implicit. We can also describe the semi-implicit, but I am focusing on the fully implicit. Implicit. It is quite standard to prove that this discrete problem, because this is a discrete problem, you have to make sure that it has a solution and that this solution satisfies very nice estimates that we have. And there are certain assumptions about the English condition, some moments. If you take Q0 to be deterministic, then U0 to be deterministic, then you don't need this condition. Sorry, I had another question. Yes, yes. So, do you think, well, I guess an explicit scheme would be divergent in that case? No, an explicit scheme is that in this particular case, you will have uk minus one, like in order for you to make it linear. So, but then you have to prove. But then you have to prove if the explicit scheme works, fine. What is the point of using an implicit? It's not because, to be honest with you, for us, in order for us to make the explicit scheme work, we have to go through the implicit scheme. No, but if you have an explicit scheme, you will have a step size restriction for the step size H. It's not about the step size. The step size, else it would blow up. As if you took the classical Hermariama scheme, you would have a step-size restriction, no? No, the problem is not there. The problem is that you can write it down, but the point is that you have, first of all, you have to find you have to prove that I think you can prove stability, can prove stability of the. Can prove stability of the of the solution you can but not for all not for all age uh i think i think for all age i don't see what do you mean for not for all age i did we didn't have this problem yeah if you if you for example look at uh only um as the part that contains the laplacian the first term h eta gradient uk gradient Gradient UK, gradient. So here it is implicit. So meaning that if you want to solve this system, you can do it without a restriction on page. But if you would have an UK minus one here. No, here we use UK. Then you need a... Yeah, I know, but this was to answer Sammy's question. Why would you do an explicit scheme? scheme so what what you could do is have a k minus what you could do is have a k minus one in the first term and then a k in the second term so this is what uh um what you called uh some implicit semi-implicit exactly it's not there completely yes we are right and then probably you're right i understand what you are saying about the fact that then when you wanted to make to converge which we we don't i mean if we didn't have this term you have to have a restriction on age to put it on the other Restriction on H to put it on the other side. But yeah, so actually, we don't do it directly. In fact, it's a problem. The semi-implicity is not that easy to answer your question. Okay, thank you. Okay, but good point. In fact, because of this, we are working on something right now that I'm not going to talk about is how to. About is how to never mind. I mean, to solve this problem, we are working on something else because fully implicit, you are right. Do you implement the fully implicit? Not really, because we have to linearize at a certain step. Okay, so the error. So, let me denote the error Ek to be UTK minus UK, K between 0 and capital N, and then let us write down. And then they let us write down now the error, the equation for the error. We write it down, taking the difference between the two equations. And because the noise is multiplicative, we will end up with the following somehow martingale term. And the goal, as everybody knows, is that we have to make sure that this We have to make sure that this error converges to zero when n goes to infinity in some functional space, and we have to find some rate of convergence. So we have to work, you know, do some calculations and, you know, you have to do all the stuff. So, I mean, when you do, when you take the test function to be ej, the non-linear term, which is the crucial point here, when you rewrite it, you will end up with three terms. So the first, the second, So, the first, the second, and the third. The first one is the bad guy somehow, you know, like because it has EJ and EJ. The second and the third are the ones that where we are going to use somehow the regularity, the US minus UTJ, and some bounds about UTJ, such that we can get, we can use like a Grand Ball lemma and get some rate of conversion. And get some rate of convergence that will be given by this difference, in fact. This one we have to deal with it. Okay? So, when we, you know, in fact, as I said, the integral between tj minus one and tj of the t1, you work on it a little bit, you will end up with a nice term because this is a term with the delta one small enough, you can put it on the left side. And on the right side, we will be left with the following term: so, h, the step size. So h, the step size, which is nice, the norm of ej, but then it is multiplied by the gradient of utj, which we have the estimates, but still, both of them are random. And the t2 and the t3, you know, they are nice because you will end up with h multiplied by this norm, which is on what's on the left side, and the extra term let me. term that we can deal with. Okay? And the T3 the same, similar. They are similar. Okay. At the end, when you put everything together, I just put the term that is a little bit problematic. So you do the sum, taking the sum over J and then taking the maximum, we end up with the following estimate. The maximum of the norm of EK in L2 plus the following. Two plus the following less or equal than some nice term is constant h, the sum of the norm of ek, but multiplied by the gradient of utj in L2 square. And the other, I apologize, and the other terms here, the nice terms, this is okay, this is okay, but we have also the martingale stochastic integral. Here we are still left with the stochastic integral. And the only way to kill this. And the only way to kill this stochastic integral is to take the expected value. This is why we have to take the expected value at this level. And so we use the expected value, use the Gronnobal lemma in order for us to estimate this term. And as you see, we will end up estimating the expected value of this product, which is different from the product of the two. Now we have to deal with it. This is why. It. This is why it is clear that because of this, the moments of this term are all bounded, people what do they do? They localize, you know. So we take a probability space. So you define a probability space omega km for which you have, you know, you're localized by this constant m. These are all nice, you know, they are all nice, they are measurable spaces. Are measurable spaces, etc. And we know that the probability of the complementary actually converges to zero when m, when this constant goes to infinity. So everything works fine. And in fact, this is what everybody does, localizing, you know, technicalities, of course, but if you take U0 with certain moments, the expected value of the local The expected value of the localized error on this probability space is bounded, and you have a rate of convergence. As you see here, this is two, this is one half, but this means that it's really a one-fourth, one-fourth minus rate of convergence for calamit hole. Okay, as I said before, dogs and break today. Today did a little bit better, but still it is localized. As a consequence, you know, we have a convergence in probability that we, you know, actually used for, we did a little bit better, and we use this to get the convergence in probability with any in our first paper. So the probability of the error has a rate of one-half minus in our case, and I use. And this is just the proof I will pass on this. No, now, as this is for the convergence in probability, so with an E, what we with Anenier, so what we did is that because we used, this is true for any, we discovered that every time you have moments, you can have a convergence in a strong rate of convergence. Strong rate of convergence. Now, maybe the rate is not great, which is in fact it's logarithmic in this case. So, in 2019, we take the initial condition to be random with moments of order 2 to the power q with q bigger equal than 3. Then we have the following rate of convergence, which is strong. And I will show you the main point. The error, the expected value of the error squared is less or equal. Squared is less or equal than the log of n to the power following rate. So the rate is not polynomial, but at least it is strong. Okay, this is our first step. And the main point was that the expected value of the error, we can split it into pieces. We have the expected value of The expected value of the localized one plus the expected value of the complementary of the states on the localized and the complementary, localized on the complementary. This one is what we and Caroli Paul and Dogson, White and etc. have, which is of order, in our case, it was actually of order one half minus. This is our new feature. Our new feature is that this expected value, in fact, you use all the you know inequality, etc. But at the end, the main point is that this because we know that the moments, we have this estimate, all the moments are bounded, then it is enough to have a rate of convergence of the complementary of the local space, the localized. The states, the localized probability space of the omega complementer. Okay, so the main point is that the rate then will be once we have to find the rate of this space, which is going to be the following. And then, you know, we have just to balance now the first rate with the second rate. And this is how we get the logarithm. Okay? But it's logarithmic. And this is for meticativity. Logarithmic, and this is hormylication. Okay, additive noise. Let me see. I think I have 10 minutes, right? 10 minutes. Okay, 10 minutes. Okay, additive noise. In the case we have really pure additive noise, we can have a strong rate of convergence without localizing. And it is pretty sharp. It is one half minus. Is one half minus. Let me rewrite my equation and let me assume, like, just for simplicity, that this w is a q inner process with trace of q which is finite. And actually, I need a little bit more. I need the a one half, the trace of q, the l2 norm of this to be finite. Sorry, a one half q trace of a one half q in the l2, the trace to be finite. Sorry. But this is. But this is the minimum. And we rewrite: this is our fully implicit scheme. This is additive noise. Now, let me say that for this particular case, we are also able to prove exponential moments for the discretized solution, which is not simple. So if the initial condition is in H1, If the initial condition is in H1, deterministic, or you can take also the case when it is random. I give you for simplicity this case. And if we take this coefficient alpha, to be small enough of order the viscosity divided by trace of Q, up to a constant, you know, other constants are involved. But the main point is that it has to be smaller than this, this great constant. Then you have exponential moments of the norm of. Moments of the norm of the discretized solution in the H1 norm, this boundary. So now let me rewrite the error. The error now will be solution of the following equation, but now unlike the other case where you have a multiplicative noise here, we have really an it's equal to zero. Okay, so again, when we play with it, you know. Again, when we play with it, you know, I do some calculations, we do some calculations, we end up with the following estimate, almost surely, a pointwise estimate. The maximum of the error in L2 is less or equal than a function, you know, quantity z plus t over n, the mesh size g multiplied by the sum of ej square. Okay, where this g is not. Where this g is nothing else than one plus a constant that depends on a small coefficient delta, the stoop of the norm of h1 of the u in l2 or the a1 half of us. This is the h1 norm of the solution. And the z is a bunch of norms. In particular, it is t over n, the norm of the uk in the l2 to the power 4, but you have also But you have also terms of the following shape, the sum of the tj, tj minus 1, a1 half u s minus 2 tj in a2 square, and other terms similar. The main point is that these are the terms that will give me the rate of convergence. These are the same terms that will be given the rate of convergence because this one is bounded. This is a t over n of order t over n. So when we use the discrete, yes? Can you remind me what is that? Yes. Can you remind me what is delta? I am a little bit lost. Sorry. The delta that you have in the G? The delta is a coefficient that I didn't introduce. It's just a constant. You know, like the, yeah, sorry about that. No, no, it's okay. Thank you. Yeah. Thank you. So this is a bunch of constants that involve estimates. Thank you. Okay, so at the end, we will end up using the Granville lemma. We can use the Granville lemma. We can use the Gromba lemma before using the expected value because we don't have the problem of having the stochastic integral. So the maximum of the norm of the error will be less or equal than this Z quantity exponential of Tg. And now, only now we take the expected value. And now when we take the expected value of this 15, we will end up with the really the rate, the strong rate of convergence that will. Strong rate of convergence that would depend, of course, of the expected value of Z exponential T G. But it is clear that here, the Z exponential T G, these are Z and G are both random variables I gave you before. You know, the G and the Z are random variables. But in particular, so when we do the Holder inequality with the right coefficients, etc., what is important is that What is important is that, in particular, the expected value of the z to the power q, and I use the Herder continuity to the power number Q, is going to be bounded by T over N to the power eta. And this eta is between 0 and 1. So this is what will give me the rate of convergence. While the G, the expected value of the exponential of Tg, let me remind you again that G is this, because I have some. This is because I have somehow the exponential moment, it will give me, you know, I know how to bound it, so the rate of convergence will be given by the following. As a result, we have the following theorem. Of course, we have some restrictions. If we take the initial condition to be just deterministic or random with some restrictions, and the trace of Q to be smaller than mu squared over a certain constant multiplied by T. So the main point is this balance between. point is this this balance between the trace of Q and the viscosity then we have a rate of convergence and it is strong and not localized is direct and it is sharp I mean the constants are not sharp but the rate is pretty sharp eta is here it's between zero and one strictly so we have a rate of order one half minus okay Okay, maybe I have five minutes. So these are actually our new results in the additive noise. Now, what we are trying, we have other results with the rate of convergence with multiplicative noise and also additive, if you want, for the Brinkman for Scheimman Naviestox, which are let me just these are 3D in 3D. In 3D, again, periodic boundary conditions, I am sorry. This is just the usual Navistox in 3D, but we have to add this term. This is the Brickman-Persheimer term for alpha bigger than 1,60, in order for us to prove that the solution exists, all the estimates are okay. And we have a very nice rate of convergence, also. Okay, projects in progress and open questions. Right now, we are implementing this method. Right now, we are implementing this method for the 2D bussiness. We are almost done, you know. But it's more complicated because you have a coupled two-coupled equation. Navis talks with like a temperature equation or concentration equation, and it's more complicated. We have in mind to do the convergence pathways of the Navier stocks. We have in mind to use boundary conditions, although. Boundary conditions, although it's more complicated, so I have to say that it's pretty challenging. The Divichlet boundary condition, the most interesting one for Navier Stokes, and maybe implement this for the Euler equation, which is also harder, and maybe the weak convergence. And I will stop here. Thank you, Akima, for this very nice and clear presentation. For this very nice and clear presentation. Are there any comments or questions? So feel free to unmute yourself or raise your hand on Zoom. So I don't see any hand on Zoom. So maybe I can start. In your last point in your future project, you had something about weak convergence. Do you have any idea, any hope? Or did you implement did you implement Did you implement the scheme just to see what you get numerically? No, it's true. So, here, to be honest with you, yeah, the hope, one hope would be to do it on the finite dimensional setting. Because, I mean, not to do it on the Navy stocks, but to do it on the Galerkin approach. stokes that to do it on the Galerkin approximation otherwise it's it's it's very difficult i mean it's extremely difficult good point yeah i have yes pay june pay june sorry right okay uh so you have the convergence analysis uh the rate of convergence analysis for the semi-dispartization the disparization The disqualization was taken with respect to time. So, is there any we have, I didn't say it, but all of these for Navier's talks at least, we have also space. Okay, so you have you do have like analysis. What is quantization, including the spatial? Yes, we have space and time. So, then what would be the results for the full force? For the space, because it's really involving for. Because it's really involving for the space, it is of order one, for the time, it's one half minus. Which came is it in space? In space, it's finite elements, produced finite, finite elements. Okay. So then for the full for the spatial quantization, how do you enforce the divergence free condition? Good point. This is why I didn't, you know, go through here. We use this. We use this we have to approximate the somehow we we have to approximate the the model by a model we have a penalization term let's say to enforce this you know so we use this finite element of Vulgarius I forgot what's the name but we have to have like a penalization term to enforce in fact that divergence equal to zero good point Divergence equals to zero. Good point. Okay. You mean the lead-like element or you mentioned that the elements, right? What you mean is that the lead-like element or not? The edge element or just regular final element? Ah, regular, regular. Oh, regular elements. Okay. I mean, we didn't implement. I have to tell you that we didn't implement, but we have a project. We have a project right now, but it's not complete. I have a collaborator who is an expert in finite elements and we are doing some kind of okay, it's not really semi-implicit, but he is going to implement it with the finite. The implementation will be done. But finite element. Now I don't know how he's going to implement it. You know, he knows how to do that. Thank you. Thank you. Any first and then Sami. Yeah, if I can add something to answer the question. Hakima mentioned some divergent finite elements, but they are very seldom. So in fact, what we do is we take some finite element that satisfies the usual LBB condition, infsuk condition, and then we change the bilinear term in order to take care of. In order to take care of the fact that finite elements are not divergence-free, I don't know whether it answers your question. I hope. Right, okay. And this is what I said. We add like a penalization term. Yeah, that's another way to do it, right? Penalization, yeah. Yeah, it's an extra term. I think Sammy had a question. Yes. Or at least your hand is raised on the Yeah, yeah. I just wanted to see if it worked. Okay, thank you. Yeah, I will lower it now. So, yeah, I was curious about the pathwise convergence. Is it something you would do just for additive noise? And what kind of technique would you use for this pathwise thing? As far as our discussion, Ani, can I say? Annie, can I say? Maybe we can, right? You said you do. But you mean that it might be a secret? No. No, no, no, no, because we didn't write. No, I mean, what I am saying is that we didn't write things correctly. We didn't write things yet, but it will go through a kind of Borel-Canteni lemma. All right, so it's still in this probabilistic setting, right? You're not thinking of a more Thinking of a more rough path approach. Ah, no, no. Oh, I understand what you mean. No, not rough path. I wouldn't even know how to do the rough path. How would you do the rough path? I have no idea. Yeah, I mean, it's complicated. It's complicated, exactly. I'm not even sure there's something done, right? The for the Navier for Naviest Toks and the Rafpa community, Rafpa for Naviest Toks. No, yeah, I'm not sure. I think there is some work. Gubinelli, maybe? No, Martina of Manova. Martina Hofmanova, Mike Leahy, David Holmes from Imperial. I think they have some. Oh, yeah. But is it Navier Stokes? Yes. It's Navier Stokes, but it's not. I don't think it's existence. I think it's about non-uniqueness or something. Think it's about non-uniqueness or something like that. No, no, no, no, no, no. We have a result on rough path existence uniqueness for Navier Stokes, either additive noise, multiplicative noise, or transport noise. Ah, you mean with rough path theory? Yes. Okay, yeah, yeah. I think I heard one of her collaborators made, but maybe, yeah, maybe you're correct. I wouldn't correct, but I don't know the details. But I don't know the details. Okay. Okay. Are there any further comments or questions for Akima? May I ask a question? Yes, you sure. Yeah, for the error estimate in equation 15, it looks like you only need the exponential. The exponential integrability of the exact solution, right? Yes, in this particular case, yes. Why there's no integral, exponential integrability of numerical solution? We don't need because we you can rewrite, we choose to rewrite. We choose to rewrite the non-linear term in such that we involve only the solution u. But you can also put instead of u. It depends how you rewrite the non-linear term only. Right? If I may add something to what Hakima said, in fact, Hakima presented the result about exponential integrability for the About exponential integrability for the discrete for the discretization, the time discretization, and that's needed in order to look at space-time discretization. Yeah, exactly. But in this particular case, we need only the integrability of the solution. Okay, so what about the multiplicative case? The error equation looks The error equation looks similar. Not really, because, yes, I know, because I mean, it is true, but let me go back to here. So, here, you see, here it's almost the same, but you have the MN. Okay. Okay. So, in this particular case, I mean, we don't do it that way. I mean, first, we don't do ground values. We first localize somehow, and then you take the expected value to kill. Actually, you take the expected value, you localize, and you take the expected value. But once you localize in the exponential, you have the constant that the You have the constant that depends on the localized solution, on the constant of the localized solution, and then you have to deal with that. I mean, there is a way to do, you know, like getting the rate of convergence, the strong rate of convergence also here, but it goes through a localization. And I prefer to give you directly the point using the direct approach. approach did i did i explain it correct simply i'm not sure for that for that error equation can you use the this uh the stochastic version of green uh groundwall equality to get some some inequality involving exponential integrability We didn't use that. Okay. Okay, thanks. It depends. In a purely multiplicative situation, we don't change moments because we don't know how to prove them. But we have a bounded. Akima gave details about the proof for purely additive noise, right? But there is something in between situation where you have bounded diffusion coefficient. Bounded diffusion coefficient. And then you do have exponential influence, and you can get some polynomial rate of convergence, but it's not as good as what is for purely additive noise. Okay. Okay, thanks. But let me, let me, you are right. So let me, let me maybe focus a little bit more on to answer your question. In this setting nine, you can still use, as you said, Still use, as you said, you can say after you localize, or you can even use the Grand Valle lemma, and you will end up with an exponential. You are correct, you will end up with an exponential of this node. But this is, as Anis said, all of this we are doing it in the case of a multiplicative noise. But in the case of a multiplicative noise, the exponential moments we are not able to do. Moments, we are not able to compute them. We don't know. The only case where we have exponential moments that we can use is when you have either additive noise or actually bounded noise. I didn't do it here because I didn't have time to explain everything. Even in the bounded noise, you can have exponential moments. You can still use here the Grombo lemma, as you said, but it's not, it's useless at this point. At this point for multiplicative noise. Is it clearer now? You mentioned the knowing results about the exponential integrability, integrability of the exact solution. The setting is not the same as yours. Say it again. Yes, this one. Yeah, yeah, yes, this one. The setting, yes, the setting of the equation is different. What do you mean? So, what is your question exactly? So, this exponential moment in general, when you have an equation, in order for you to prove exponential moment, it is necessary to have a bounded noise. Like you can have like a multiple. You can have like a multiplicative but bounded the diffusion has to be bounded. Here I write k1 equal to l equal to zero. In the additive noise, it's clear. It's you know, you have to take trace of k q bounded. But you can also do it in the case where you have a bounded noise. But you cannot prove it for multiplicative noise, like purely detective. It's not true. We don't know how to do it. Okay, I see. This is a constant. I see this constant K1 and L are some constants about the diffusion. Yes, here. You see, here you have G of U, you can take G, the diffusion in front of the noise, which is a function of U, it's an operator. It has a linear growth. So when K1 is not zero, you allow somehow the dependence. You know, the dependence in you, you see. But if k1 is zero, this is bounded, and only in this case you can prove that it's necessary. You can prove this exponential moment. I see. You see? And with this condition, thanks. This is absolutely necessary. Plus, of course, you have this coefficient, you know. Okay. Okay, I think it's perhaps time to thank first of all, thank you, Akima, for the presentation and also for this very nice discussion.