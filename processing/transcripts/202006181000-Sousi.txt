Okay, okay, great. So, thanks a lot. So, this is my final lecture, and today I'm going to talk about the third point of the overview, which was refined mixed-heat relations mixing. So first I'm going to set up some notation and recall the definition of the relaxation time. And once I do that, I'll follow the paper by Basu, Hermon, and Paris. So this lecture will be based on This lecture will be based on this paper by Sue Mond and Paris, which is called a characterization of cutoff for reversible markov chains. So probably Jonathan would be the most appropriate person to give this talk, but since it's in my mini course, I'll try to explain, but he will be there to answer questions in the chat. Okay, so let me first set up some. So, let me first set up some notation. So, on the website, I posted, I just wrote a one-page note where I recalled the spectral theorem, the spectral representation theorem for reversible matrices. So, let me first define the inner product that one uses. So, if we have a finite state space and we have S and we have two functions f and g from s into r, then their inner product then their inner product with respect to the invariant distribution pi is defined to be the sum overall x in s of f of x g of x pi of x and now the spectral representation theorem tells us that if p is reversible with respect to pi which is the invariant Which is the invariant distribution, then there exist lambda j, j from 1 up to the size of s, eigenvalues with corresponding. Corresponding eigenvectors fj j from 1 up to the size of s and lambda 1 can be taken to be 1 and f1 can be taken to be all ones. Okay, and then one can express Okay, and then one can express p dxy divided by pi of y as 1. This corresponds to lambda 1 equal to 1 plus the sum from j from 2 up to the size of s of lambda j to the t times fj of x times fj of y. Okay, so the proof of this theorem, I'm not going to prove it. Theorem. I'm not going to prove it, it's in the notes, but I'll just write that the idea is to use the matrix Axy, which is defined to be square root of pi x divided by square root of piy times pxy. And check that this is a symmetric matrix. And so one can analyze the matrix A, and then one can pass back to the matrix. then one can pass back to the matrix P to obtain the eigenvectors of the matrix P by multiplying with a correct matrix the eigenvectors of the matrix A. Okay, so if we have a reversible Markov chain, then so if P is reversible, we write the eigenvalues in decreasing correlation. In decreasing order 1 equal to lambda 1, strictly greater to lambda 2 because the invariance between this unique greater than lambda n, which is greater than or equal to minus 1. And now I'm going to define, define. Define lambda star to be the maximum of the absolute value of lambda, where lambda is an eigenvalue of p with lambda not equal to 1. So, this is what lambda star is. We take the maximum absolute value. Are the maximum absolute value of all eigenvalues, which are not one, and we also define lambda to be one minus lambda star, and this is called the absolute spectral gap. It's called absolute as opposed to gamma equal to one minus lambda two, which is called the spectral gap. The spectral gap. Now, one exercise that I said is that to check that if a chain is lazy, then actually all eigenvalues are non-negative, and so gamma star is equal to gamma. Okay, so for lazy change is equal to one. Is equal to one. And final definition is that the relaxation time T rel is defined to be one over gamma star. This is the relaxation time. So today I'll be talking about reversible, sorry, I'll be talking about, yeah, of course, reversible, but also lazy Markov chains. So T-REL is one of T rel is one over gamma star or one over gamma, whatever you like. So the two are equal. And finally piece of notation: if f is a function from s to r, we write e pi of f for the expectation of f under pi, so just the sum overall x, f of x, pi of x. And variance with respect to pi of f is defined to be the expectation of the recentered f. So f minus its expectation. F. So F minus its expectation squared. Okay, and then I said one exercise, which is to show the Poincaré inequality, which is that the variance with respect to pi of Pt of F is upper bounded by e to the minus 2t over the relaxation time times the variance. Time times the variance of F, where P is reversible, lazy on a finite state space. And this is true for all functions f and all t greater than or equal to zero. So what this inequality says, this inequality here, is that when if you wait time t, which is a multiple of the relaxation time, then this is just. Time, then this is just going to give you a constant. And so this means that after a constant number of T-rel steps, the variance of PT drops to any accuracy you like. And when I write P of f, I think of P acting as an operator, acting on functions. So Pf of X is the sum over all y of Pxy f of y. F of why. Okay, so this is all the notation that I want to set up. So all I want you to remember is just the definition of the relaxation time, which is one of the spectral gaps here. And we are also going to meet the Poincaré inequality. Poincare inequality. And I said this as an exercise, and all it requires is just the spectral representation of P to the T. Okay, so now I'll come back to the paper of Jonathan with Ridy Bassou and Devalperis. Before I state the theorem, let me set up some notation. So in the previous lectures, in the first lecture, I showed that the mixing time of The mixing time of a reversible marker chain is up to end lazy, is up to constants, the maximum heating time of large sets. But what we showed is, as I said already, is that this equivalence is up to constants. And in our theorem, actually, we didn't have good control on the constants. Now, in Jonathan's result, what they obtained a refined relationship between these two The relationship between these two quantities and the constants actually are equal to one. So before I state the theorem, let me define the quantity which is the analogous quantity to th of alpha that I was talking about before, which was the maximum heating time of sets of size at least alpha. So define for epsilon and alpha, which are in zero, one One, heat, let me write it alpha of epsilon to be the first T so that when we take the maximum over all starting states and over all sets A of measure at least alpha, the probability of hitting A after time T is less than or equal to. After time t is less than or equal to epsilon. So we want to the probability, so what we are saying here is that if we have a set A which is large, the probability that we hit A after time t is going to be smaller than epsilon. And again, oops, I forgot to write px here. Again, here we take the word starting state x. Starting state X. So theorem, so let me just write B H P is that for let P the irreversible Lazy latex Markov chain on S, which is finite with P and pi then for all epsilon in zero one the mix of two epsilon equals equals equals mix of two epsilon is upper bounded by heat of one minus epsilon epsilon plus two times the relaxation time times log of two over epsilon cubed and t mix of one minus epsilon is upper bounded by heat upper bounded by heat of 1 minus epsilon of 1 minus 2 epsilon plus 2 T rel times the logarithm of 8 over epsilon cubed. I don't think the beeping noise is coming from me. I can also hear it. So I mean analogous quantity. Here, instead of looking at the which is the maximum expected heating time of a large set, here we are looking at the tail of the heating time. So it's a modified result also in terms of that we are not looking at expectations, but we are. That we are not looking at expectations, but we are looking at tales of heating times. But I called it analogous because we relate the mixing time to this heat quantity. So that's one theorem. Actually, that's not the most general result from the paper, but the reason I chose to, they talk about this result in the introduction, and the more read, the more general result, instead of taking two actions here. Result instead of taking two epsilon here, one can take epsilon plus delta and get some more precise, um, some more precise bounds here with a different constant. But the idea is exactly the same, so it will be easier for presentation purposes to just stick with this. So before, so actually, I'm hopefully going to present the whole proof of this result today. So let me call this star and this double star. I will leave double star. This double star, I will leave double star as an exercise, but I will prove star. And this is the hard direction of their paper because they also obtain a lower bound of similar order. So I'm not going to write the lower bound. One can find it in their paper, and I will not have time to prove it. But the lower bound in these kind of results, it's always easier because it usually uses the idea that if you haven't hit a pixette then idea that if you haven't hit a pixet then you can't have mixed. So the lower bound again involves this heat quantity minus some multiple of the relaxation time. So before I jump into the proof, I would like to write one remark. So what we see from this statement is that the mixing time is upper bounded by these two quantities. So actually the mixing Actually, the mixing happens in two stages. So, the first stage is governed by this term, and the second one is governed by the relaxation time. Laziness is not used in the proof. That's right. I guess laziness is used because one can just put here the relaxation time. Time, but um, okay, let's just stick with a lazy change for now, and but I agree the relaxation time one can justify in terms of the um so yeah, it doesn't matter, it's just that I only stated one car inequality for a lazy chain just because all of the eigenvalues are upper bound by the second one, but yeah, laziness is not essential. So, mixing happens in two stages. In the first one, governed by the heat quantity, we have to wait to escape. Some small set with high probability. So when I say high probability, I mean probability one minus epsilon. And in the second stage, we wait for the relaxation time. For the relaxation time steps. And the second stage, we are going to prove for the second stage, we are going to use the Poincaré inequality for the second stage. Okay, so I guess I'll just start the proof now. What is it? Yeah. So, as I said, I'm just going to prove proof of star, and this one is left as an exercise. It's exactly analogous to what I will present. Okay, so what do we want to prove here? We want to show that if we wait for this amount. If we wait for this amount of time, then we are mixed and the total variation distance is at most two epsilon. So let me set T to be heat of one minus epsilon epsilon and S to be the second time, two relaxation time times log of two over epsilon cubed. So we want to show So, we want to show that for all X and all sets A, we have that Pt plus S X A minus pi of A is less than or equal to 2 epsilon. So, if we saw that for any starting state X and for Any starting state X, and for any set A, the distance between P plus S X A and Pi is I'm just looking at the chat. Yeah, I guess we can just take action to be at most a half. You're right. Yeah, of course, that's right. So, yeah, thanks so much. Okay, so what, so if we show what I wrote over here, then we will be done. For any starting state X and any set A, if we have this inequality, then that's exactly what we are looking for. So what is the idea? Like before, the idea is that we want to define an intermediate set that I'm going to call G. So I'm going to define an intermediate set G and what we want is to first wait to hit G and then once we hit G we want to have some good control on the probability of being in A after T plus S steps. So idea want want to define an intermediate set G such that we hit it with high probability before time t. When I say high probability, again here high probability always means one minus epsilon or something like that before time t. Before time t and conditional on hitting it by time t we want to be close to pi of a at time t plus s pi at most epsilon. By at most epsilon. What do I mean? I mean that the probability starting from x that x t plus s is in a given that the first hitting time of g happened before time t minus pi of a I want this to be less than or equal to epsilon. Now, if the probability that we hit G That we hit g by time t is large, so the probability that we don't hit it is less than epsilon, then we will be done because the probability starting from so let me now write it in bloom because I'm going back to the proof. So let's just write so that So let's just write so that we see what we need to define g to be. The probability that starting from x, that x t plus s is in a minus pi of a. I'm going to write the first probability as x t plus s is in a given tau g less than or equal to t times the probability that tau g is less than or equal to t plus the probability xt plus s. probability xt plus s is in a given tau g greater than t times the probability tau g greater than t minus pi of a so this whole thing just make this through here so this whole thing is upper bounded by the probability starting from x that x t plus s is in a given tau g less than or equal to given tau g less than or equal to t minus pi of a plus the probability time from x that tau g is greater than t so I upper bounded this difference here by the sum of this absolute value plus the tail probability for the hidden time of G. So if we So, if we have a set G for which we can show that this absolute value is upper bounded by epsilon, and for which we can also control the tail of G, then we will be done. So, let me, we can control it by epsilon, so that this tail probably is upper bound by epsilon, then we'll be done because we'll show that this difference is at most 2 epsilon, which is what we want. Epsilon, which is what we wanted. So let me upper bound this now by the maximum over all y in g. Actually, let me write supreme to be more correct, y in g and r greater than or equal to s of the probability starting from y that xr is in a minus pi of a plus the probability starting from x. The probability starting from x that g is greater than t. So, what I did so far is I started, I said that we want to show this inequality. So, I said we want to find a set G. We don't know what G is yet, but we want to arrive at the definition of G. So, I said, let's suppose we have some set G, then we can always upper bound this absolute value by the sum that I wrote. By the sum that I wrote down here. And now we want to find the right g so that this is less than epsilon and this is less than epsilon, and then we'll be done. So let's define g to be what we want it to be and let's hope that so by what we hope to be, I mean that we can control the first term automatically by the definition of g and then let's hope that the second term. And then let's hope that the second term will also be controlled. So define G to be all points Y for which, okay, let me write it, the soup over all R greater than or equal to S of the probability starting from Y that X R is in A minus pi of A is less than or equal to epsilon. Then we get that this is going to be upper bounded by epsilon plus the probability tau g greater than t so remains to show that That px tau g greater than t is less than epsilon. Let me write, suffice this to show. So once we have that, then we are obviously done. Now, so I'm going to take a break in one or two minutes. So now let's go. Let's go. So, what I've done so far is I just define the set G here, what we want it to be, so that we can control this term. And now, so we forget about this term, and now we just focus on the second term. So now we want to show that the probability that we haven't hit g by time t is upper bounded by epsilon. So far, I haven't used anywhere what t and s are. So I'm going to use s a little later, but let's now look at t. But let's now look at t. So t is hit 1 minus epsilon epsilon. And if we go back to the definition, it means that if A is a set of size at least 1 minus epsilon, then the probability we haven't hit it by time t is at most epsilon. So suffice to show that this tells an epsilon. And again, now I will write it suffices to prove prove that pi of g is greater than one minus epsilon since t is equal to heat of one minus epsilon epsilon so now we prove so need to show that pi of g is greater than one Is greater than one minus epsilon. So, this is going to be the rest of the proof now. So, I think it's a good place to take a two-minute break now. Thank you. So, feel free to ask any questions in the chat.  Yeah. So let me know where to stop. Okay. I guess there is not enough time now, so maybe at There is not enough time now, so maybe at the end of the lecture, what will be better? Okay. Yes, you can significant questions about earlier lectures. We can discuss them afterwards. So the forum, the Zoe. So the forum, the Zulip forum is also available and remains active also after the course ends. So you can also have any discussions about the lectures or the exercises there. So Georgios is asking about the name of uh relaxation time? I think Jonathan is the right person to answer that. Yeah, I'm not sure about the history of that, but it's related to the fact that when the chain is already at equilibrium or close to equilibrium, it kind of measures. It kind of measures the time frame in which observable becomes decorrelated, the worst-case time window for this to happen. Yeah, I'm not sure who came up with this terminology, but it's used in David Aldos's work and in his book with James Phil. Yes, so Pitwisk is asking whether today's result implies the upper bound from the first lecture. And Fadad is asking about explicit comparison between Explicit comparison between the heating time and its refined version? Yeah, so right. So I want to mention what Jonathan is mentioning here: that the relaxation time is always a lower bound for the mixing time. Because this could, yeah, I should have said that. So the relaxation time is always a lower bound for the mix. So the second term here oops, sorry. So this second term can be ignored. Can be ignored when we only care about the constants. Can you explain and think about one of the inequalities from the proof, the first inequality? Yeah, okay, sorry. So there is one step I omit it. So pi of a here, you can write it as pi of a times the probability that tau is greater than the t plus. That alga is greater than t plus the probability that algae is less than or equal to t. And so then you apply the triangle inequality and you get exactly what I wrote. You upper bound here, I forgot about the probability that alg is less than or equal to t. I'm not multiplying because it's upper bounded by one. And here I upper bound by one the absolute value of the difference between the two. So you just need to write by way as by Phi of A as times one, you need to replace the one by the sum of these two probabilities. Linden is asking about the second inequality with the soup. Right, so here I condition that we have hit G by time t, so then we want to be in A after T plus steps. After t plus s steps, so at least after s steps, we want to be in A. So I just take the worst, the worst starting state in G. So I apply the strong mark of property here at the first healing time of G. Perhaps we can continue? Okay, yeah. Okay, very good. So okay, so Okay, so we're okay, so we are all on the same page now. So we want to show that this is upper bound by 2 epsilon. We've reduced it to showing that the tail of the heating time of G is the probability that it's greater than T is less than epsilon. And now we reduce this to showing that the stationary measure of G is at least one minus epsilon. Minus epsilon. So let's go. So the remainder of the proof will be focused on showing that pi of g is greater than 1 minus epsilon. So now let's go back to the definition of g here. So g is defined as all the starting points for which the probability of being in A after any r greater than or equal to s minus pi of a is smaller than epsilon. So suppose that we were to take r Suppose that we were to take R to be equal to S here, so suppose that we didn't have the soup, we just had R equal to S, then if we look at the definition of S, S is the right multiple of the relaxation time. This log is going to, so let's forget about the log, let's just think of it as a, I mean, just a multiple which only depends on epsilon. So So, if here we forget about the supremum and we only look at arc equal to s, then it would be easy to show that pi of g is large just by using Poincaré inequality. And I'm going to explain that in the more general setting that we'll see now, but I just want to say that if r was equal to s, then just the Poincaré inequality would be enough and we would be able to show that pi of g is greater than one minus epsilon. g is greater than one minus epsilon. Here we want to take the soup over all times. So when you have control at so for r equal to s we would have good control of this but now we want to take a we want to take a soup. So usually when you want to control a maximum of things and you only have control of one thing, this rings a bell and it looks like one would want to use something like dupes. Would want to use something like dupes inequality. So, in this setup, there is a theorem that is extremely useful and it actually solves exactly this issue of taking the soup overall R and this is called star's maximal inequality. And I'm going to mention it and prove it today. But first, let me introduce some notation and so we arrive at the statement of stars. So, first of all, So, first of all, if we have a function f from s to r, define f star of x to be the supremum over all k greater than or equal to zero of p to k f of x. So, again, here I'm thinking of the matrix P as an operator acting on functions. And here I define And here I define f star of x to be the soup over all k greater than or equal to zero of p to the two k f of x. Now I want to define, so here we want to look at, oops, sorry, we want to look at this difference. So I'm going to define the function f t of x to be p t x a minus pi of a which can also which can also be written as the matrix P to the T applied to the indicator of the set A minus pi of A and all of this applied to the point X. Okay, so this is my function Fp. So what is F0? F0 is simply this function, the indicator of A recent with respect to pi recentered. And now f t star of x is going to be the soup over all k greater than or equal to zero of p to the 2k ft of x, which is by the definition the sup over all k greater than or equal to 0 of p to the 2k plus t x comma a minus pi of a because it just multiplies. Because I just multiply this matrix by p to the 2k. And I'm also going to take p of f t star of x, so the function p of f t, and now I'll take the maximum function of this, which is the soup over all k greater than or equal to zero of p to the 2k of p of f t of x, which is the soup. which is the soup overall k greater than or equal to zero of p to the two k plus d plus one of xa minus pi of a so it's not clear now why i'm defining all of these quantities but if you look at the set g the set of all y's so that this soup is less than epsilon then g is exactly the set of all y's the set of all y's for which f star of y and p f star of y are less than or equal to epsilon so i defined a these functions here f star so for any function we define f star by taking the soup over all k greater than or equal to zero p to Overall k greater than or equal to 0p to the 2k. And now I define ft to be the function that I would like to use here. So just the matrix p to the t applied to this indicator centered. And now ft star and pft star become exactly this absolute value. So the supreme overall r because we have to take care of both even and odd times. So now I just rephrase. I just rephrased the definition of g in this way instead, in terms of the star functions. And we want to show that pi of g is greater than 1 minus epsilon, which is equivalent to showing that pi of g complement, oops, pi of g complement is less than epsilon, but pi of g complement is upper bounded by pi of y so that f s star of y is star of y is greater than epsilon plus pi of y so that p f star of y is greater than epsilon and now we can use um so these functions are all positive so we can just take the star functions are all positive so we can take them The square of both sides of these inequalities. And so this is going to be upper bounded by e pi of f star squared divided by epsilon squared plus e pi of p f star squared divided by epsilon squared or here I used Or here I used Markov's inequality. So now we reduce the problem to controlling the second moment of this maximal function f star. So now I said before that if g were defined without the supremum, so just for r equal to s, we would be able to use Poincaré and Be able to use Poincaré and finish the proof. So let's do this now. So I'm going to write, so for a function f, let me just, because we'll use it in a second, we write for p in one infinity, fp to the power p is going to be the expectation under pi of f absolute value of f to the power Absolute value of f to the p and the expectation is with respect to the measure pi. So now f star, the second norm of this is sorry, I'm sorry, I don't want to take the star. I just want to take, actually, let me take also P of F star so that I control both of them, P of F, sorry, without the star. So PFS, the second norm, is upper bounded by the norm of Fs because P is a contraction. And now F star is, so sorry, Fs, the norm of Fs, Fs is defined to be P to the power S of applied to the indicator of A minus pi of A. So Of A. So this is exactly the variance under pi of p to the s applied to the indicator of A. And now if we apply Poincaré M, this is going to be upper bound by e to the minus 2n. To be upper bound by e to the minus 2s over the relaxation time times the variance under pi of the indicator of A. And now we can just substitute what S was. So this is going to become epsilon cubed divided by 2 times pi of A times 1 minus pi of A for the variance of, so here one can. Variance of so here Poincaré and here substituting the value of s and pi of a times one minus pi of a is the variance of the indicator so this whole thing is upper bounded by epsilon cubed divided by eight okay Okay. So, what we showed is that the two norms of both PFS and of FS without the stars, they're both upper bounded by epsilon cubed of rate. And this is just if we were to define, this would be useful if we were to define G just at time s. So then, by Poincaré, we would be done because we showed that. We showed that this is upper bounded by epsilon cubed over 8. We would substitute here and we'll get that pi of g complement is quite small, is epsilon over 4. Now we want to have some control on the norm, but with the star. And this is where now we are going to use star's maximal inequality. So theorem. So the setting is as before. So P is reversible with respect to pi, P is in one infinity and for any f for any f For any f from s to r we have that f star, the p norm of f star is upper bounded by p over p minus 1 times the p norm of f. And f star is like I defined earlier. So this is reminiscent of tube's maximal inequality, L P maximal inequality. This P over P minus 1 is exactly the same. P over p minus one is exactly the same constant that appears there. And I'm going to present the proof, and you will see that indeed the p over p minus one comes from dupes maximum inequality. So the idea of the proof is to define a suitable martingale to which one can apply dupes inequality. Okay, now armed with stars, which we'll prove in a second, let's go back to finish the proof. So PFS star, the two norm, that's what we need. That's what we need over here is going to be upper bounded by actually we want the square of this. So it's going to be upper bounded. So here we take p equal to 2. So it's going to become 4 times PFS, the second norm. And we already said that this is upper bounded. Said that this is upper bounded by epsilon cubed over 8, so this becomes epsilon cubed over 2. And applying it only also to F star, second norm, is going to upper bound it by 4 times Fs, second norm. And again, using the bound from above, this is upper bound by epsilon cubed over 2. So if we go back to So if we go back to, let me write that as three stars. So plug into three stars to get that pi of G complement is upper bounded by epsilon over 2 plus epsilon over 2, which is equal to epsilon. And this finishes the proof. And this finishes the proof for the upper bound of pi of g complement, which is exactly what we want to show. So, modulo star's maximal equality that I haven't proved yet, we showed that pi of g has a large measure, and this completes the proof of the theorem. So, what remains is to prove star's inequality. So, proof of star. Okay, so let me just recall the definition of n. f star of x is defined as the swoop over all k greater than or equal to zero of p to the two k f of x, actual value of this. So we want to show, as I said before, We want to show, as I said before, this looks like Dupes maximal inequality, so we want to find the right marking area. So let xn now, let x be a mark of chain with x0 distributed according to pi. Now, if we look at p to the 2n f applied to x0, so what we want is to take the soup over all. The soup over all n of so we want to control the p-norm of f star, which means we want to control the p-norm of piece, which is the same under the p-norm is always defined with respect to the invariant distribution pi. So it's the same as saying that we want to control the soup over all n greater than or equal to zero of the absolute value of this and want to control the p-norm of that if x0 is distributed. norm of that if x0 is distributed according to pi. So what is p2n f of x0? It's the expectation of f of x2n given x0, just by definition. And now I'm going to apply the tower property. So I'm going to condition on some more things. So it's the expectation of fiven xn and x0 given x0. So I condition given x zero so a condition both on x n and x zero so to our property now because of the markov property the expectation inside is going to become we can forget about x zero so it's the expectation of f of x to n given x n and all of this given x zero Given x zero by the mark of property. Okay, so now let's set Rn to be this conditional expectation, f of x to n given xn. Now, what the goal is, we want to show, so p to n f of x0 is the expectation of Rn given x0. So, what the goal is to expect. So, what the code is show that R is a backwards marking area. In other words, we are going to show that if any of the other things are going to be the same thing, we can see that we are going to If n is fixed, then if we look at R capital N minus little n for n between 0 and capital N, then this is a true martingale. So once we have that this a martingale, then we can apply Duke's maximum equality to this martingale. Martingale and obtain an upper bound for the L P norm of the supremum of this R in terms of the P norm of F. And once we do that, then we can pass back to this quantity using conditional Jensen. And there is one final step where we have to send this capital N to infinity, but this will just follow from one-ton convergence. Will just follow from quantum convergence. Okay, so the first step is to show that this is a martingale. So it's important that we started x0 according to pi and the trend is reversible. Since x0 is according to pi and x is reversible, it follows that xn xn plus 1. xn xn plus 1 up to x2n has the same distribution as xn xn minus 1 x0. So Rn, which is the expectation of f of x to n given xn is equal to the expectation of To the expectation of f of x0 given xn, because these two sequences here have the same distribution by reversibility and by the fact that x0 is according to pi. And now, using the Markov property, we can write this expectation as the expectation of f of x0, and I'm going to condition on the whole field. Condition on the whole future after time n, and this doesn't change the expectation just by the Markov property. So if we now set fn to be sigma of xn, xn plus 1, and so on, then and fix n then this last nicole here. And this last equality here, this last equality gives us that R n minus n is a martingale with respect to the filtration Fn okay so this is good news because we want to control the maximum of R so if we take the maximum if we take the maximum of Rn and between little n and n between zero and capital n the p norm of this of course is the same as if we take r at time n minus n and now we apply Dupes LP inequality to obtain that this is upper bounded That this is upper bounded by p over p minus 1 times the p norm of r 0. But what is r 0? r 0 is simply f of x 0, which is p over p minus 1 times the p norm of f. So what we showed is that the p norm of that the p-norm of Rn is of the maximum of Rn is upper bounded by what is written in the statement of the theorem by this quantity. So now but that's not what we want to bound. What we want to bound was the supremum overall n greater than or equal to zero of p to the 2n f of no actually let No, actually, let me just write it in a different way. So let me make it as a matter. I'll just write the maximum and less than or equal to n f of x 0. This is equal to the maximum over all n of the expectation. Let me get rid of the actual value. The expectation. The expectation of Rn given x0, and this is upper bounded by the expectation of the maximum of Rn given X0. And now conditional tension implies that That the max of p to the 2n f of x0, the p-norm of this is upper bounded by the p-norm of the maximum of Rn and we already showed that this is upper bounded by p over p minus one times the f norm the p norm of f. the f norm the p norm of f and now letting capital n go to infinity and using monotone convergence complete separate so just to quickly go So, just to quickly go over the proof again, so the goal was to find a martingale to which we could apply Dupes Maximal inequality. The martingale that we found, so what we did is we expressed p to the 2n, f of x0, as the expectation of Rn, and Rn is a backwards martingale. So then we were able to apply Dupes maximum inequality to this martingale R, and get the Martingale R and get the bound that we wanted. And then the last step was: okay, one needs to use conditional jensen because this is defined in terms of the conditional expectation of Rn. And then we just let n go to infinity and we use Montan convergence. And this proves a result, which is a very powerful result because it relates the supremum over... So where was F star defining? Where was F star defined? So for a function f, we define f star to be the soup over all k greater than or equal to zero of the expectation of time to k when you start from little x. And one can control the p-norm for any p of this in terms of the p-norm of the function f in the way I wrote here. So I think I'm going to stop here. To stop here, okay. Uh, thank you, Pella. We will first unmute everyone to thank Pella. Yeah, that's right.