Thank you for having me with you. Despite the distance, I'm happy to share that time with you. So I'll be talking about joint work with Viet Dong and Antoine Housard about the Anderson operator. So my main message here will be the following thing. That's a random operator. And the message is that if you want to know something about an operator, it's good to know something about its hip kernel. And that's what I really want to illustrate with what I'm going to tell you. With what I'm going to tell you. So let's go back to the starting point. So the main actor here will be space-white noise, let's say in a two-dimensional setting. Have the torus in mind if you're not familiar with closed remaining manifolds. So space-white noise, that's a random Gaussian distribution centered and whose covariance when tested on test functions is given by the L2 scalar product. Well, that's a Roman guy. That's a random guy, and that's actually a random distribution, not a function, with regularity. I think you're still on the first slide here. Are you? Yeah, I think. Okay, good. So this guy here has regularity minus one. Almost surely. Let me go to the next one. No. Yeah. Okay. Regularity minus one. So the operator I want to look at is the operator I want to look at is the Anderson operator. So that's a perturbation of the Laplace operator with a space potential, xi. So if you want to have a picture of that, an infinitesimal picture of that would be you are working on a discrete torus and you have a random walk on that. And that random walk, each time it goes to one place, there's a random potential in it, properly scaled in terms of n. Well, the formal limit of that limit of that scaled random walk, scaled in time, is the operator, and that's an operator, Laplace plus multiplication by psi. I mean emphasize that the psi times u is really psi multiplied by u. That's not xi as a distribution acting on u, which is a different guy. So what is the problem here? The problem is that I like to define that formal guy as an unbounded operation. Guy, as an unbounded operator on the natural L2 space, meaning that I need to define that operator with a domain, that is a set of functions, such that when I apply the operator, I end up with an L2 functions. And here is the core problem. For the reason that, well, we have two sides on that operator. We have the problematic one, psi times u. It's problematic in the sense that ψ being a distribution, if I want to make sense. If I want to make sense of ψ times a function, I need my function to have some regularity, otherwise the product will not be well defined. So if ψ has regularity alpha minus 2, which is anything smaller than minus 1, to make sense of the product, I need the regularity, the a priori regularity of u, beta, to satisfy my relation, my red relation. Essentially, beta needs to be strictly bigger than one. Now, on the other hand, the Laplace term. The Laplace term, Laplace U, what it's doing is that, well, it eats a guy of regularity beta and it gives you back a guy of regularity beta minus two. And the point is that beta minus two is strictly bigger than alpha minus two, the regularity of the noise and the regularity of the product. So we are in a situation where there is no hope that you can compensate the irregularity of the product ψ U by the regularity of Laplace U. Larity of Laplace U. So there's no compensation and no hope to have an element, Laplace U plus ψ U, which is in L2. So, despite that point, well, the first guy that were able to define the Anderson operator as an unbounded operator, they managed to do that. That was Alez and Chuk in 2015. They constructed that operator as a nice operator with a nice spectral theory using Spectral theory using the new, then newly developed tools of power control calculus. And they were even able to prove some nice tail estimates for the lowest eigenvalue. Slightly after, Labe, Serie Labet, used the tools of regularity structures to do something similar on the three-dimensional torus. And that's not just for fun. Three is really harder than two-dimensional. That same year, Gubinelli, Gokan, and Zach Huber. Gubinelli, Gokan, and Zach Huber, they gave a simplified construction of what Alais and Schuk did and in a two- and three-dimensional setting. And well, Mousar in 20 even further simplified the construction of Gubini, Ubocan, and Zach Huber, and this allowed him to prove an almost shun by Lau. So that's an interesting kind of estimate. That's something about the distribution of the random eigenvalues of the random operator. Eigenvalues of the random operator. Okay, so I said it has a nice spectral theory. So you can look at where the eigenvalues are and count how many of them are smaller than a large lambda. And Vi-Lau is actually a first-order asymptotic for that quantity. Okay, to give you a hint about how things work, let me say that this way. What I just said about the regularity of the problems of multiplication points to the Multiplication points in the direction of the fact that only looking at the a priori regularity of the objects in the domain is not sufficient. We need a final description of the potential elements of the domain to give meaning to H, the operator applied to you. So that final description actually involves, that's where we involve, the power control structure. So if you film it in that, So, if you've imagined that, that's something fairly easy. So, the primary structure on the elements of the domain is that they are made up of a principal term, that's the p-term, up to remainder term, the u-sharp. The remainder is called remainder for the reason that it has better regularity, almost two, than the expected regularity of u itself. U is expected to have regularity one minus, and the remainder is two times that. That's why it's called a remainder. Why it's called a remainder. Now, for the main part of the structure of U, you assume that's only priority statements. You assume that it has that power-controlled form, which involves an operator called power product. That's a bilinear continuous operator that eats to guys in a very non-symmetric way. So, in that Pu prime X, the main player is capital X. is capital X, by which I mean that P U prime X is a new function that looks like X at small scales. And in particular, that power product of U prime and X has the same regularity as X. And U prime now is something that we assume to be unknown. So in that business, X here is built only from the noise. Okay, that's the only thing you need to keep in mind. thing you need to keep in mind. So my part control structure amounts to say my unknown is going to be given by a main part plus a remainder. So the one unknown is replaced by two unknowns, U prime and U sharp. So what do we gain by doing that? Well, we gain a wonderful thing. The fact that we can actually define the product of a power control U by capital by ψ if and only if we are able to Only if we are able to do the same for x times xi. So this looks like a silly statement, for we still have a problem for defining x times psi. However, there's a big step that has been done here for the reason that defining x times psi is only something that involves the noise and not any operator or whatever. That's a problem only about the noise. So at the So at that point we are still in trouble because x has regularity alpha one minus, xi has regularity alpha minus two, and still the sum of the two regularities are up to something negative and that's not sufficient for making sense of the product. Well, that's bad, but the good news is that what we are really interested in is to build something that behaves as x times psi. And the trick is to say, okay, we can't make sense. trick is to say, okay, we can't make sense of x of omega times psi of omega. However, we can build a random variable, and that's where probability helps, that behaves as the x times psi should behave. Okay, so that's the trick. And that quantum variable, it is defined in the most naive way. We regularize the noise, we do the multiplication x, r, xi, r, and we set the regularization parameter to zero. So if we do that, it doesn't work. So, if we do that, it doesn't work because it's actually diverging. But the good news is that the only stuff that is diverging in that quantity is the expectation of that quantity. So, if you remove from x r, psi, r, its expectation, it's converging to something. And that something will be what we define to be x times psi, the random variable. It's converging not almost surely, it's converging as a random variable in some L2 or in some probabilistic space. So, wrapping things up in the converse direction, we realize that if instead of working with x times xi regularized, we introduce all along the way the diverging counter term, we actually end up with a family of renormalized and regularized operators whose resolvent, that is the inverse operator, has a limit. And that's what we define as the resolvent. What we define as the resolvent of the inverse H. The good point of that construction is that we end up with an operator, capital H, or its inverse, if you want, that turns out to be a continuous function of the enhanced noise. So for anyone that has seen some stuff about rough paths, that's really exactly the same situation as what we have when we work with rough differential equations or stochastic differential equations seen as. Differential equations seen as rough differential equations. The good point about that H is that, well, it's well defined, it has a nice resolvent, compact, and so it comes with a nice spectral theory. Okay, so that's my starting point. What can we say about that random operator? And this is where the heat kernel stuff comes in. So, to talk to you about that thing, let me uh That thing, let me. Uh, um, let's here is a statement: that that map that I'm that star map that I'm looking at, here's what it does. I told you that the Anderson operator is a perturbation of the Laplace operator. Okay, that's Laplace plus a white noise potential. So its heat kernel should be a perturbation of the Laplace heat kernel. Okay, that's what I'm looking at here, the difference between the Laplace heat kernel and my underson heat kernel, acting on the initial starting, the initial condition V0. Condition V0. So the theorem here says that if you take the initial condition in the dynamics to be of some regularity, then you know where Pt minus Pt Laplace ends up. It ends up in a space of functions of x of some regularity with some explosion when t goes to zero. Well, depending on the regularity of the initial condition. So fine. What can we do? So, fine. What can we do with that kind of information? Well, actually, despite the fact that it looks like a very weak information, it gives some really nice information about the operator. And the first of it is that we can get back by law in a really efficient way, for instance, because there is a sharp relation between the distribution of the eigenvalues and the Laplace transform of these eigenvalues. And this Laplace transform. Values and this Laplace transform that's via the Turien theorems. And these Laplace transforms actually turn out to be something that you can compute in terms of the heat kernel. That's the trace of the heat operator. So by a one-line computation, building on the regularity that I gave you a second ago, you can get back that value. So here's some more information that you can get from the type of information on the heat kernel that I gave you. You can get back some estimates. You can get back some estimates on the size of the eigenfunctions with different notions of size, either as an LP element or as an element in some hold space. And the result is that you get some estimates on this size in terms of the corresponding eigenvalue, random eigenvalue. Okay, you can do better. You can also have some similar estimates. Similar estimates on the spectral projectors. That is, that's projectors on the first eigenvalues, all the eigenvalues smaller than a given number. And once again, you have some explicit estimates that come as a consequence of the nice control that you have on the heat kernel. Okay, so that's really my message here. Knowing having a precise control on the heat kernel gives you... on the heat kernel gives you a nice control on the operator itself here via the eigenvalues and the eigenfunctions. Fair enough. Okay, I'd like now to take a little bit of time to talk to you about a nice and interesting object that you can associate to that random operator. And that's called the Anderson-Gaussian free field. So for the Field. So for those who have never seen anything about the Gaussian free field, let me tell you what it is. That's a random field that is Gaussian. So that's a random distribution which is Gaussian, centered, and whose covariance is given by the following explicit integral representation. It's given by a kernel, which is the green function of the Laplace operator. So to compare that with So, to compare that with the space-white noise that I showed you in my first slide, the space-white noise has a similar covariance structure. However, the kernel is given by the direct mass xy. So x has to be equal to y. So you end up with the scalar, the L2 scalar product. Here, that's the structure. So, what we know a lot, well, guys know a lot about this random Gaussian free field. And in particular, one knows that it has almost surely some regularity. That it has almost surely some regularity, anything smaller than zero. Now, the Anderson-Gaussian free field is something similar that you build on top of the Anderson operator rather than building it on top of the Laplace operator. So as such, that's a doubly random field. You have two levels of randomness. First, you have the level of randomness that comes from the fact that capital H That comes from the fact that capital H on the standard operator is random. That's the first table. And once you fix that randomness, that's my small omega, then you have an additional randomness that you use to define the random Gaussian distribution, center, whose covariance is given by the exact same formula, except that you use the green function of the Anderson operator, G rather than G Laplace. Okay. Okay, so, well, as you expect, not surprisingly, that W random field, the Anderson GFF, has the same regularity as the GFF. That is, it has regularity anything smaller than zero. And you can even describe in some detail its Karma-Martin space. Good. Well, I'd like to give you here a nice to illustrate what happens here with the nice stuff. So I told you that the Anderson GFF, that's a random distribution. So as such, it doesn't make sense to take its square. However, one can define its width square. Okay, and this is how we do it. We do that the same way as we define the square of GFF. You do the naive thing, you regularize your run. You regularize your random distribution, you take the square and you send the regularization format to zero. It's not converging, but once again, the only stuff that is not converging here is the expectation of the object. And you end up with a statement saying that, well, okay, that weak renormalized square of the Anderson GFF is converging in low to something as the regularization parameter goes to zero. That's what we call the weak square. That's what we call the weak square. Fine. Moreover, and that's an interesting point: that random variable has a totally explicit generating function or partition function, as they call it sometimes. That partition function, the Laplace transform of my phi squared, Wick's phi squared, turns out to be that random function for the reason that when I function for the reason that when i'm checking the expectation i'm i'm only i'm only working here with the ex with one of the degrees of of randomness i fix my underson operator and take the expectation with respect to the other randomness so my partition function turns out to be a random function and that random function of lambda is actually analytic on all of c and that's something that turns out to be crucial to say the following thing the fact that The fact that imagine that you have two settings, you have two torus or two manifolds, each of them with its metric. So you have two Anderson operators and you have two GFF and two Anderson GFF. Now here is the point. Working with the previous statement, we can show that, well, the spectrum of the Anderson Of the Anderson operator is actually encoded inside the distribution of the random analytic function capital Z. So more precisely, if you take two settings, then the spectra of the operators have the same distribution if and only if the two random holomorphic functions have the same distribution. So you see that you have in this way a way of encoding one object. Encoding one object here, the spectrum, inside an object of a different nature, here, a random holomorphic function. And that turns out to be an interesting and useful encoding. Well, I'd like to finish that talk by giving you some questions, homework. So that first question is really silly. I told you, okay, we can build. Okay, we can build a random operator by some renormalization process or whatever. We have a random operator and it has a nice spectral theory. It comes for free from functional analysis that the lowest eigenvalue is simple. Always, that's always the case. However, I've tried hard and wasn't able to prove that almost surely actually the operator has a simple spectrum. All the eigenvalues are simple. All the eigenvalues are simple, which looks like something obvious because there is so much irregularity inside your oscillations for the potential that there is no reason that you should have two eigenvalues equal. So, because usually, well, multiplicity for the eigenvalues is related to having symmetries. So, if your potential has no symmetry, which is the case here, when you take one sample of space-white noise, there's no reason that you should. White noise, there's no reason that you should have some multiple eigenvalues. But well, I wasn't able to prove that. Here is a last element before I finish. Well, I told you that we have a nice description of the heat kernel of the Anderson operator. Nice means not really great. I just told you that the heat kernel of the Anderson is the heat kernel of the Laplace operator plus something smaller, which I can... Something smaller, which I somehow quantified in my first statement. Just knowing that, well, we can at least say something about an interesting function which you can always associate with any kind of operator that has a discrete spectrum, the zeta function of an operator. That's a Meromorphic function, which is a priori defined by the sum of the powers of the eigenvalues. So the minus s powers of your eigenvalues. That's a random function. Function. So knowing that Pt is close to Pt Laplace tells you that, well, that function can be extended as a Meromorphic function on the half plane real parts bigger than a half. Fair enough. And there's really something that goes on. Well, there's a pole in that zeta function at one, s equal one. So you really have something neuromorphic. Now, here's the miracle. I suspect strongly. I suspect strongly that actually, when you take the expectation of that zeta function, which is random, you end up with a function that is actually meromorphic on the entire complex plane. Now, what would be the kind of benefit that you would have by such a statement is that the fact that, well, if you can import from the holomorphic functions and memomorphic functions a number of And memoric functions, a number of things, you could get some finer information on the spectrum of lambda. That's what usually happens in the deterministic case when you look at the Laplace operator or any kind of elliptic operator. And as a last word, the reason why that's an interesting question is the fact that I'm half, well, almost sure that actually the almost sure memoric extension that I say here happens is the bed. Say here happens is the best that you can hope for. I'm sure that there is no way of extending your zeta function on that half-plane, whereas when you take the expectation, you can extend it on the entire complex plane. So that would be a really nice situation. That said, I'll thank you for your attention. Thanks a lot, Israel, for your support. Thanks a lot, Israel, for your support.