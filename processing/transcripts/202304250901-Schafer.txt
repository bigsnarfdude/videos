Invitation to be here, and thanks for coming out this morning for the talk. As Nick said, I'm just going to give some thoughts that I have related to model selection, in particular to this problem of background selection. And I just want to quickly thank Lucas and Mikael and Nick and also Larry Wasserman at CMU for suggestions and contributions. Suggestions or contributions they made to this discussion. Of course, any stupid things that are said are completely my responsibility. So I was asked to give a statistician's view on model selection, which of course is a very large topic. But over the next 20 minutes, plus or minus, I'm going to get on a few topics like parametric versus non-parametric models. AIC is going to come up. I'm going to talk about the discrete profiling method. The discrete profiling method, some ideas I have related to using semi-parametric approaches, how the method of SIVs might play a role there, and also model averaging, which came up yesterday. So I attacked on some slides at the end. There's a chance I won't get to that if you've seen the slides. I know I have a lot, but I tend to go very quickly through the slides. We'll see what happens. And again, focus specifically on this background model selection problem. Here's some data. Problem. Here's some data that Nick has used in previous talks and papers when I asked him. He kindly provided to me this data set that I can just use to demonstrate a few things here. And this is the form of the model that I'm going to use. And I'm going to say at the outset, I don't work in particle physics because this is not exactly consistent with the model that has been discussed to this point, but I'm just. But I'm just modeling this as a background, which I'm denoting eta, plus a Gaussian bump whose size is controlled by this parameter theta. So the count in each bin is being modeled as an independent Poisson with mean given up by this parameter length. I'm going to be using maximum likelihood to estimate. To estimate these models, I'm going to consider different polynomial forms for the background, but that's just a choice to make things simple. It could be a different set of models, of course. All right, so here's just showing some fits, fitting a linear background, quadratic, cubic, quartic, and you can start to see as you make the background sufficiently flexible, you start to do, you start. Flexible, you start to do, you start to actually pick up that signal there at 1.5. And we talked about this quite a bit yesterday, but this is just an example of how the confidence interval for theta could be constructed. This is just demonstrating the quartet case. And here are the results for the first four models showing the MLE confidence interval for theta. Confidence interval for theta, again based on the previous slide, number of parameters, and also the value of AIC, which is calculated as minus two times the log likelihood plus two times the number of parameters, a classic approach to model selection, if there ever was one. But there's no reason why you have to stop here. You could, of course, continue and consider ever more complex models. And this sort of leads me to the first. And this sort of leads me to the first point that I wanted to make here: is that this approach that I'm describing here is it parametric or non-parametric? And I would argue that this is de facto a, well, what I'll ultimately call semi-parametric approach. But this process of considering ever more complex models is really non-parametric. So when we say non-parametric, that doesn't mean that necessarily there aren't. Metric, that doesn't mean that necessarily there aren't parameters involved. There are real-valued parameters that characterize the polynomial models, of course. It's really more about how those models are, the full class of models under consideration. And even if you aren't doing what you might classically call a non-parametric approach, if you are considering ever more complex models as you get more data, right? As you get more data, right? As you get more data, you'll be able to fit increasingly more complex models. And if you are, in fact, doing that, you are, in a sense, really considering a non-parametric approach. So that's just the first point I want to make here because it transitions into the next topic. And AIC is a widely used approach for choosing the value of the smoothing parameter in this case. And the smoothing parameter In this case. And the smoothing parameter, although it's kind of backwards in a set, would be the order of the polynomial, right? As you choose the order larger, you're getting a more complex function. As you get more data, you don't have to perform as much smoothing. You're letting the data speak for themselves. So, again, this is de facto what I would call a semi-parametric approach. You have the parameter You have the parameter of interest, this theta, right? You're really interested in estimating the influence, the size of this bump. And then on top of that, you have what we call a nuisance parameter, eta, this background column. And the complexity of the model would be limited by the estimation of theta. So if I was just doing this, maybe. Doing this, maybe outside of a formal approach, I wouldn't choose an overly complex background model because I know that it's going to lead me to a situation where I'm not going to get a good estimate of theta. I'm going to choose as complex of a model for the background in order to get a decent estimate on theta. So, again, what I would argue is that what's happening there is you're effectively ensuring the consistency of theta. As you get more and more data, you could fit the more consistency. More and more data, you could fit a more complex background, but you're always doing it in such a way to provide some confidence that you're getting a decent estimate of theta. I would refer to this as consistency, right? Yeah. How do you count the number of parameters if you do something like this? Because imagine yourself that peak is in the middle, then any odd degree polynomials will not affect the intensity of the signal. So you basically So you basically don't have to count those as parameters. Those parameters though are still affecting the shape of the background, right? So they're still interested in the shape of the background because now it's not a nuisance. You're interested in the amount of signal you have. I agree with that, but you're still fitting a it still is a It still is a component of the initial model, right? You're fitting the background and the signal simultaneously, and the number of parameters in the background. Yeah, but now using the archaic information criterion probably is not appropriate because it's not the purpose to feed the background. But you want some confidence that you have a decent model of the background so that then So that then you're not, then you can confidently say something about that estimate data. I agree, it's a very complicated trade-off that you have to manage there, but the AIC is making a statement about the entire model, background and the parameters here. So it's trying to trade off those things against each other. All right, let me just Alright, let me just do. I'm going to assume that we put some restrictions on background shape. I think we all would recognize that if there's not some constraint placed on the shape of the background, it's really a non-identifiability problem here. So I have this space capital Ada that's assumed to line. That's assumed to lie. All right, so I was thinking about the problem this way, and I was thinking back to this paper. That I want to be clear, I'm certainly not an expert in this area, but I was thinking back to this paper that I'd seen by Susan Murphy and Vandervaart on profile likelihood. So they here show that semi-parametric profile likelihoods, where the nuisance parameters are profiled out, behave like. Parameters we profiled out behave like ordinary likelihoods. You have a quadratic expansion, score function, and the Fisher information in the classic case get mapped into this efficient score function and efficient Fisher information. I'll mention them in a minute. I'm trying to keep the math details to a minimum here, but I'll mention those in a minute. Then this expansion may be used, among others, to prove the asymptotic normality of the MLE of theta, which, of course, is the parameter risk interest. Which, of course, is the parameter risk interest. And then that can be used for performing tests based off of theta, which again seems to be the key thing of interest in this situation. So classically, the score function is just the derivative of the log likelihood with respect to theta, here being evaluated at a particular point, theta naught, eta naught. When they talk about the efficient score function, About the efficient score function, what's happening is you're taking that score function and subtracting off a projection of the score onto a space of score functions for eta. And again, I want to try to avoid getting too technical here, but the way to think about it is that you're at a position in the parameter space, and then it's looking for the direction in the parameter space, the set of The parameter space, the set of so-called parametric approximating models, finding the one which is least favorable. So, in the space eta, of all the possible models, it wants to find the direction such that it can find a parametric approximation to that model, which is least favorable in the sense of makes it most difficult to estimate theta. So, you're being conservative in that respect. So, just a quick illustration. So, just a quick illustration here. You can imagine the red might be the log likelihood just in terms of theta, ignoring the other parameters. And then what's happening is as you subtract that off, it's taking the log likelihood and sort of broadening it, making the curvature at the peak less to give you this more accurate. To give you this more accurate or conservative, but a more realistic assessment of the uncertainty in this parameter statement. That's the effect of this. And then you, the variance of that efficient score function will give you this efficient Fisher information, which then, as in the classic case, the inverse of that gives you covariance, in this case, just variance. In this case, just variance on the MLE for figure, which again is what you are primarily interested in in this case. All right, so just kind of stepping back for a second, what this is going to do effectively is, if you think about in this situation, what's going to be the most, what's going to present the most difficulty in terms of the most important? To present the most difficulty in terms of estimating theta. But really, what it's going to get at is the amount of flexibility in the space of background models at this position 125. So this really is just a lot of a mathematical foundation for quantifying something which I think we would all agree with is a natural way of getting the uncertainty on theta hat, which is. On theta hat, which is just exploring how much flexibility is there in the class of background models at that particular location. So it's looking at that location as you move in the space of all possible background models, which direction gives you the most variability in theta. And it uses that to get the shape of the likelihood function at that point. function at that point and then the the how broad the peak is at that point that gives you your assessment of it okay so that's kind of a a very heuristic way of thinking about it but again it from my you know from a statistician's perspective this seems like the natural way of thinking about these sorts of problems and i will disappoint you at this point and uh tell you i did not At this point, and tell you, I did not follow through with the full analysis for this particular situation, so I can't tell you exactly how it all works out. But I do think, again, it provides a natural way of thinking about it. If you didn't know the location, would this all become much more complicated? I'm not thinking about that situation. Let's take this one for now. It's much more difficult to define the local model complexity than the global one. Than the global ones. I'm sorry, I I just said. I just said that I would answer that question to the opposite, that it's much more difficult to define the local model complexity than the global one. Because the global one you could define with the number of expansion terms, for example. And for that particular piece that you have, you know, under your signal, it's much more difficult to define the relevant point. Yeah. That's probably true. Okay, so that's sort of the one idea that popped in my head when I was thinking about this. But of course, an important question here is what controls the complexity of the background? What limits the scope of the possible backgrounds under consideration, which is sort of the key question in making this sort of thing work. This sort of thing works. Maybe there's some sort of physical constraints that allow you to place some constraints on capital Ada. But another idea that I was thinking about was to go back to some of my ideas of the so-called method of sieves. So here, the basic idea is that you construct a sequence of spaces, 8S sub n. They grow in complexity. They grow in complexity with the sample size. You do this in such a way that, from a theoretical standpoint, you ensure that you get a consistent estimate of theta. So you could imagine a sequence of background models, maybe just even like step function type things that get increasingly complex as the sample size grows, but that increase isn't so fast that So fast that you don't get a decent estimate or a consistent estimate of theta. So you imagine doing this. Once you fix eta sub n, then you can perform maximum likelihood over that space. You'd be getting MLE. I think you'd be confusing the camera by moving around so much. No, it doesn't even show my slides. You could get the MLE over the background and theta simultaneously. Simultaneously, you could use the sort of ideas that I was just describing. So then I was thinking about how this compares with discrete profiling. We had a great talk about this. Nick gave a great talk on this topic as part of the SAM series that Anna and McHale organized last year, which is very informative. And so here, in discrete profiling, you look at all the different, I hope I characterized it correctly. Different, I hope I characterize it correctly. You look at the different log likelihoods for the different models. Here you can see the order on the side here. And then the basic idea is that you're going to consider the choice of the model as another nuisance parameter. And you're going to profile over that. So, in this case, I haven't made any correction for the complexity of the model. So, it naturally is maximized. Is maximized by the most complex model. So they, co-authors, talk about how you can adjust this by basically using AIC, which effectively comes down to subtracting off the number of parameters when you're in a lot of likelihood scale. So then you get a surface that looks like this, and you use that for constructing. Constructing a confidence interval on theta. So I think there's an interesting parallel here in the sense of the adjustment that you make here by making, subtracting off the number of parameters, is an attempt to get the models on a comparable scale so they're of comparable complexity. So with the method, the approach that I was just characterizing a minute ago, you sort of start with models that are of a Start with models that are of a similar complexity, you restrict to models that are of similar complexity, right? This eta sub n. And then in that space, you think of, you search for the most difficult, this least favorable direction in a sense, and use the breath at that point to get you the uncertainty. So I don't have any theoretical results or anything like that, but I do think there's a connection between these two ideas. And I think. I think it sort of suggests that there's a reason why this seems to work and why that method works as well. That they're both effectively trying to do similar things. All right. So I have a few minutes. I'm at 20 minutes. This will be. I did want to talk a little bit about model averaging as well. This came up yesterday and Yesterday, and there was some discussion about the fact that it doesn't really make sense because when you average the different models, you might not necessarily get something which is within your class of models. But there is work out there that thinks about it, I think, from a slightly different perspective. Again, in this context, we have the background, it's a nuisance parameter, we're really just We're really just interested in estimating theta. So you can actually average over the different models and average their different estimates of theta in order to improve the estimation. So there's a couple of references here. I actually have both of these books with me in my bag if anybody wants to look at them. But instead of fixing on just one particular model, you average over multiple models in the different estimates that they have. Sorry, can I just. Sorry, can I just clarify? I'm thinking back to Nick, the diagram that someone showed yesterday about the two cars and the two links. So, this is not breaking that rule, right? Because we're not saying I'm going to put a model in between different. I think, I guess I'm just thinking about it in a different way, and these discussions here do as well. You know, there is this background model. The fact that if I average the backgrounds between the different ones, Average the backgrounds between the different ones, I don't necessarily get a decent background model in some sense or a realistic background. I don't think they would see that as a concern, right? You just have a bunch of different estimates of theta. Sorry, maybe I have a. So let me go jump ahead to here. Each of these models gives a different estimate of theta. And then using this notion of IKEA weight, you can You can average those different estimates together, and on top of that, they have equations for the variance of this weighted estimator. And using that, you can construct a confidence interval for the unknown parameter. So I guess I'm sort of circumventing that issue. Maybe I just don't even really see it as being an issue. You're just using the fact that you have a bunch of different estimates from different models. Bunch of different estimates from different models. You have different degrees of confidence in each of those models, and you're trying to average them together in an intelligent way. Let me just go back to here for a second. Just to say, Chad, get putting it in. Yeah, no, sorry, sorry. So, sorry, I'd leave off. So, again, the discussion here, if you take a general information criterion, both AIC and PIC fit into this form, right? This form, right? It's minus two times the likelihood plus some penalty term. They define model weights in this way. And one way of looking at that is if I set the penalty to log in P of K, where P of K is the number of parameters in that model, then we're using BIC. And then the relative weight is just approximately equal to the Bayes factor between those two models. Two models. So, this really becomes just an abayent model averaging approach. What I show in the table is using AIC instead to get those weights. So, yeah, you can put different weights on the different estimates. There is theory supporting this, and you can get an averaged estimate of theta from this. And again, there's much additional work. Much additional work in this direction. You can get improved versions of confidence intervals and different attempts at this. Again, my main goal here is to maybe provide some pointers for some directions if somebody's interested in trying these things out. Okay. And yeah, these are all the references available online. Thank you. All right, thanks very much, Chad. So we had a couple of questions already, but before we had time for a little bit more. So you say you're not involved in the background, but say I'm a viewerist and I am interested in the background, I just want to get rid of your signal. Okay. Could I at the end do the same averaging for all your different backgrounds and then I get a very smooth background which I can compare to my whatever usability I like. I I I would say you might be able to do that, but I I would argue that if you're primarily con interested in the background, I would take a non-farametric approach to estimating it. You know, there's a lot of strong theory and ideas there for doing that. You know, you could fit a non-parametric function and it'll give you all the flexibility that you need, as long as you're choosing the smoothing parameters. As long as you're choosing the smoothing parameters appropriately and such, you should be able to get a good estimate with error bars on it. So I would argue that this sort of thing is not needed in that sort of situation. Maybe, Glenn, did you want to say something? Since you almost put your hand up, do you want to say so Can you say something about how th these criteria using the IK? Using like IK, which is ASC. How does that stand in relation to some sort of notion of bias versus variance trade-off? Is it not the case that as I increase the number of parameters here, the bias is going down and the variance is going up? So how does this all stand in relation to some notion of bias? Well, the weights are adjusting for that, right? But quantitatively, could there be a quasi-column in this table that somehow... Table that somehow sure, yeah, exactly. Trends that that bias trade-off is probably for the background, right? If you keep adding terms, you're probably going to end up biasing your estimate of the line because it'll be absorbed eventually. You should clearly also have an estimate of theta, a bias in the estimate of theta. Yeah. If I have that pump that I try to fit a, say, a flat background, then that's gonna that's gonna press the yeah that's always the Yeah, that's always a lot of fun. But when you start overfitting the background, I would expect that would mean that you underfit the line. That is to say, it gets absorbed. The background just goes up and absorbs that line at some point. It's probably not the case. Maybe it's what it's doing. We have another question over here. Nowadays, the first slide of a call song. The first slide of a course on fitting regression models is kind of well, we used to do polynomials, but we don't anymore because we don't behave bad. So we pretty quickly go on to learn smooth resistance. So would we get totally different answers here if we used a spine fitting? With a kind of canonical approach? I guess I would have to try it. I don't think you'd get really a different approach. It could allow local pumps. That's the... Right, you would have to be very careful about keeping the complexity sufficiently low so that you get a deep estimate theta. Which again is this non-identifiability problem that I don't know, I'm not going to solve it here, that's for sure. But you have to put some constraints on that space in order to do something. The issue with splines is that they are kind of much more local physics. Much more local fitness. Like, you know, if it comes like with these plants, for example, like those base functions are like definitely like bumps, right? So now the problem is that you don't want them to fit the actual bump. You still want the signal to be there. So now if your background model is actually able to fit bumps like that, then you have a problem like that. But isn't the issue that you would. The question is: if I have a smooth background model and I have a separate bump, it's an interesting issue. Is that smooth background model? Imagine that has degrees of freedom of three and a half. And if I don't have that, and then I throw in the, and if I leave out the bug, the I now have degrees of freedom of nine. So then I've got to trade off a model that's got one, sort of three and a half plus one degrees of freedom. It's one of the It's one with nine. Then I imagine that the AIC for the simpler model is null. Then I refer that. And otherwise, I would say, well, okay. I cannot account for that bump using a smoother. Okay. Maybe just one super quick question, if there is one. So the method that you were describing by Murphy is sort of widening the line liquid. So presumably that gives you That gives you less power and better coverage. Is that so? Well, I would argue it gives you a realistic assessment of that uncertainty. Yeah, you're looking at the full class of possibilities, usually using profile likelihood, like it was discussed often yesterday, and you're looking in the direction where the peak is the broadest. Searching over all the possible background models. Over all the possible background models. Again, I think it would be an interesting exercise to try to work it out, but I expect it's just going to come down to quantifying the amount of flexibility in the background at that spot. Okay, let's thank the chat again. We have another talking out by Lucas. Thank you. I don't know if they downloaded your They download it or they're actually downloading. He's just trying to avoid the source.