And I'm really looking forward to Alice's talk on networks and dynamics and mean field approaches so far. Alice, TVA. Thank you. So I think technically I'm a professional research affiliate B and I haven't found out what the B stands for in my. My apologies, I left out the B. Maybe one day I will upgrade to A. I don't even know if it's an upgrade. Anyhow, so I went with the ambitious title Connecting Dynamics. The ambitious title Connecting Dynamics On and Off Networks to Data: Motif-Based and Mean-filled Approaches. And I chose that title because I basically thought, This is a cool crowd. I want to talk to you about everything that I've ever done, and I need a title where all of that fits in. So here we go. And so specifically, there are two ideas that I want to talk about, and one is related to And one is related to both motifs on a more network theoretical end and network inference on the data science applications of that. And that is mostly work that I've done during my PhD with Mason Porter and then later during my first postdoc at the University of Washington with Bing Brunton. And then the second project is going to be the one that I'm currently working on at Dadworth College, which is modeling alcohol use. Call you by co-evolving social networks. And so, just one thing as a disclaimer: that first part is pretty much wrapped up, and I am very confident talking about it. That second part is very much work in progress. I am really excited to share that here, though, because I feel like this is a very nice environment where people have lots of ideas. And that is a thing where we're still in the progress of figuring out what our story is. So, I love. What is our story? So, I'd love to get your feedback. And as I indicated on Monday, I think there's a ton of split hops that I'd be interested in doing. And if anybody here in the room is interested in being involved in that, I would be super excited to get you involved. But I'm going to start out with the motifs and network inference bit. And so for that, the whole work was motivated because I was very interested in two questions. Very interested in two questions. So, the first one is: how does network structure influence what happens on a network in terms of dynamics? And the second one is: can we learn a network structure by looking at the dynamics that happen on it? And so there's two pieces of work. So the first one was my PhD work with Mason Porter. The second one is the work for my first postdoc. And starting off with the first bit, I think one thing that we learned looking at Looking at how motifs and networks can influence what happens on a network, is that we need to make a distinction between what we mean by motifs in the sense that there's a type of motif that people usually think about when they think about motifs and networks, and that is basically just a subgraph of a graph. We would call that a motif. To make that a little bit more rigorous, I'm going to say a structure motif is any type of small graph. Is any type of small graph, and it occurs within a larger network if that structure motif S is isomorphic to some subgraph of the larger network. And so since we have a crowd that is not all network scientists, I'm going to take it slow and give you a bit of an example. So, say we start out with this network here with six nodes and is it six or seven edges or something? And so, this is our system. And so, this is our system, and we're interested in what motifs live in that system. For example, we might be interested: does this motif live in the system? And that motif is, in this case, just a small graph, it is a directed two path, and now we're just going to go into our main system and we're going to look for occurrences of that. So, for example, one of them would be the edges that are indicated in blue. So, we have a two path over here, we have another two path over here, which are indicated by the orange. Here, which are indicated by the orange navel edges. And so it turns out that overall we have five occurrences. Maybe I miscounted, maybe there's one more that I forgot. But so overall, what I want you to keep in mind is that on a structural basis, you can think of motifs as just small graphs that appear as sub-graphs of larger networks. And I wanted to kind of keep separate in your head the idea of a motif. The idea of a motif itself and the number of occurrences that live in a larger system. And so we're going to go back and I'll think about how do we connect that to dynamics. And the way that Mason and I went about this was to think of motifs in terms of the processes that happen on the network and define motifs on the basis of walks rather than subgraphs. So our definition was that a process motif is a Our definition was that a process motif is a connected directed weighted multigraph in which each edge represents a walk and the edge weights represent the length of walks in a network. So that's all a mouthful, so I think what helps a lot more is looking at examples. So here we're going to start with just a very small graph. So this is now our system. Our system has just two nodes and two edges. And now we're interested in what walks. In what walks can happen on the system, what combinations of walks can happen on the system, and so just one very simple example is: we can certainly do a walk. We start at one node, we end at another note, and we did it in one step. In this case, there are two ways to do that. Either you start at I, you go to J, or you start at J and you go to I. So this is a process motif that occurs on this network, and these are the occurrences of that process motif. Of that process multiple on that network. And we can go wild with that because this is a cyclic graph. So we know that there's no end to how long we can keep walking on it. So we can think of walks that take two steps and then at the same node again. We can think of walks that take two steps. Note that here, having a process of motif like that is kind of a more general case of this. So by denoting something in this form, I don't assume. This fourth. I don't assume that these nodes necessarily need to be distinct. So whenever something like that occurs in a system, what is that you're trying to say? So whenever I say that this process motif occurs in my system, I automatically know that this process motif also occurs in that system because one is a special. In that system, because one is a special case of the effort. A very special system that I want to look at, which is going to be important for the network inference bit later, is an even simpler system where we have one edge and two nodes. Really just the directed edge between two nodes. And so, if we look at that system, it's a directed graph, which limits the number of walks that can happen on that system. And in particular, for the On our system, and in particular for this very special case, there's actually just one process motif that can happen on it, and there's just one occurrence of that process motif. And more importantly, when this process motif is happening on a network, we know that this graph must be a subgraph of our system. This sounds very trivial, but it's going to be important for the network inference bit later on. But I promise that I talk a bit about dynamics, and I haven't talked about dynamics at all. Dynamics, and I haven't talked about dynamics at all so far. So let's do that for a bit. We need a dynamical system that we're going to throw on our network, and here I'm using the onste-Nullenbeck process, which is a linear stochastic process. I'm thinking of it in a multivariate setting. I think it's mostly known for a non-multivariate setting where people use it to model Brownian motion. What changes from this univariate to the multivariate setting is that all of these x's are now vectors. These x's are non-vectors, where every node in my network has a variable x assigned to it, so x is a vector of those variables, and we have the adjacency matrix over here that couples the states of the different rooms. Some things to keep in mind for later is that, first of all, this is a continuous time process, and we have a non-zero autocorrelation function in the system, even in the case where the nodes are uncoupled. So, if we have a couple of The nodes are uncoupled. So, if we have a completely uncoupled system and the adjacency matrix is zero everywhere, we would still be able to look at the variable than this and say this blue node over here, and we would see that we have an exponentially decaying autocorrelation function, so the autocorrelation function is non-zero. This is a weird thing to note now, to point out now, but we will see how that is going to be relevant later. For now, I just want to point out. I just want to point out all the cool things that you can do with this in terms of thinking of process motifs. And so, one cool thing about this process is that because it's linear, there's a lot of stuff that you can do analytically. And one of the things is that you can analytically derive a steady-state covariance matrix that ends up having this form over here. So we have a scalar term that depends. Term that depends, among other things, on the parameters of a dynamical system. And then we have a term over here that is dependent on the powers of the adjacency matrix and the adjacency matrix transpose. Now, with terms like that, you can interpret them as watts in a system. So, usually, a to the power l would mean that all of the elements of that matrix tell something about the many ways that you can get from one node to in the net. Can get from one node in the network to another node in L steps. If we have a product such as we have it here with A and A transpose, then that indicates motifs like this, where we start out with some unspecified node, and in lowercase L steps we go to node I and in uppercase L minus lowercase L steps we go to node J. Steps we go to node J. And so these are the process motifs that I interpret this equation over here as telling me that these are the process motifs that contribute to covariance in our system. And the stuff that I circled in in blue, that scalar weighting factor, depends on parameters of our system, also depends on the length of the walks in this process motif over here, and we can interpret that as the contribution of each. Interpret that as the contribution of each occurrence of a process motif to covariance in our system. And the nice thing about that is that it's incredibly intuitive what you get in the end. So this is plotting out the contributions as a function of the lengths of the walks in the process route. And so one thing that we see here is that if we want to see a strong contribution to covariance in our system, all we really want to do is live down here in that section where both the walk from The walk from that source node to I and the walk from that source node to J are very short. And that is because the Ornstein-Ullenbeck process has the signal in that process signals the K as they traverse edges. So if you have to go very long distances from that source node to both I and J, you just lose your signal quality and therefore don't have any contribution or very small contribution to covariance. The other thing is that if we're looking for Is that if we're looking for a fixed number of steps to take, the strongest contribution comes from the process motives that live in this yellow corridor over here, which indicates that we're taking roughly the same number of steps to both I and J. And that is because the onsteen process also has built-in that signals decay over time. So if I'm alone in the middle and I'm going to tell like a really unbelievable rumor to Nancy today. Rumor to Nancy today, and I'm going to tell it to Mari in a week. And I want to see the correlation in how upset you are about rumors that you hear, that is going to have a very small contribution because the signal arrived at the two of you at very different times. So there's a bunch of other things that you can do with that. You can also, instead of just looking at covariance, you can look at correlation and entropy as well. But I'm not going to go into detail with that because I want to talk about everything that I'm interested in. And so I'm going to. Interested in. And so I'm going to move on to network inference. That story is that first paper, we submitted it, we got it disrejected with the comment, this looks nice, but it's not useful to anything. I was a very passionate PhD student and very upset about that, so I went on to do a postdoc just to prove the officer. And so here, we're looking at a network inference setup. People mean a lot of different things when they talk about. People mean a lot of different things when they talk about network inference, so I do want to go about what I mean by it. So I'm thinking of a setup where you know the nodes of your network, and you can see them, you can measure what they do, but you don't know the actual structure of that network. But you measure what those nodes do, and in doing so, you get a multivariate time series data set. Then you do some data science magic, and at the end of it, you hopefully get a network structure that vaguely resembles the one that you started. Symbols and whatever you started that you didn't know from the beginning. When people do that, there's a ton of very fancy machine learning stuff that people do, especially at the University of Washington. All of that went a bit over my head. So I looked at what people end up using in practice. And so, because the nonlinear stuff assumes a lot of assumptions that are not always given by the data, it ends up being It ends up being that in practice, when people want to estimate networks from multi-brand time series data, they go with like easy toolboxes. And a lot of those easy ones measure edges based on pairwise correlations or some similar measure of similarity between nodes over time, and they assume something about the underlying process. Most of those approaches assume some linear dynamics underlying what you see, and they either assume going. C and they either assume non-steen all over processes we had before, or a lot more assume a vector autoregressive model, which is the screen time model, in which if you don't have any coupling in the system, you have direct delta autocorrelation functions. So if you don't have coupling in your system, a node signal is not correlated to its past and future states. And so this is a big difference between those two systems. Two systems and leads to qualitative differences in terms of the quality of network inference that we get. So I was interested in studying that a bit in detail and seeing where those qualitative differences come from. So we came up with this stochastic DNA difference equation that essentially interpolates between the onste-Ullen process and the vector-of-regressive model. So for this time-step parameter delta. Times that parameter delta t, if you plug in one, you get the vector autoregression out of it. If you let that go to zero, it converges to the Ornstein rule vector process. And so we wanted to see if having that is going to help us understand when do these other methods succeed and fail, or whether it could help us improve network inference as we have it. Bringing us back to the idea of motifs, now that we have a time-discrete process, we cannot Process. We can not only look at the contributions of each of these motifs to covariance in a system, we can also look at the contributions to lag covariance matrices where we have discrete time steps. So here we've shifted one of our time series in computing the covariance matrix by one time step, two time steps, three time steps, and so on. This is for two different values of the time step parameter. And so we see that we get slightly different results. We get slightly different results, although we always get a strong contribution from the motif that we're interested in, which is the one-step walk that would indicate that there's an edge between I and J. And one thing that we can see from this is why correlational covariance is not great at inferring networks. And that is that when we measure a positive lag1 covariance between two nodes, we can think of where. We can think of where could that possibly come from. So it could come from the exact motif that we're trying to measure, but all of these others also contribute. So it might be that this isn't there, and we just have a ton of these motifs instead. Ideally, we would want to have a function where the contribution vector looks like this. So whenever that is positive, we know 100% for certain that we have a one-step walk. That we have a one-step walk from J to I, because none of these other motifs contribute. So, this is what we ideally want to have, and if we had that, we could just perfectly infer networks from pairwise interactions all the time, at least in the setting of linear stochastic dynamics. This is the lag1 covariance matrix, where we see it is far from perfect. However, what you can do is you can say, like, well, I have a lag one covariance matrix, I have a lag zero covariance matrix, I have a bunch of them. I can A bunch of them, I can combine them together in some form of linear combination to get something that is closer to the ideal case. And so we just did that. The salmon-colored one and the red one are two of the proposals that we had in terms of neutralizing some of the motifs that we don't want to see. So here we've neutralized the contribution of these confounding factor motifs, and in this case, we neutralized the contribution of these reverse causation motifs. These reverse causation motives. And we tested it out in various different settings to infer networks from synthetic data. Here, what you see is the difference in accuracy between the method that we propose and some of the ones that are already out there. So things that are fairly well known is, for example, linear Granger causality, transfer entropy, convergent cross-mapping, lag correlation with. Lag correlation without any type of correction factor. And so, overall, we see that at least for the parameters that I plotted here, there's a huge window of parameter space where we're doing quite well. And I'm phrasing it that way because the business of network inference is you always find that parameter space where your method is good, and you plot it, and then you tell everyone that it's better than everything else. And obviously, we could have zoomed in on this pixel over here, and then we would have gotten different results. So, it really depends on. Results. So it really depends on where you think that your dynamics, what kind of ground is that you have. So this is coming to the end of the first part of my talk and I've not touched on the second half at all, but that's fine. So I think motifs are cool. I think thinking of motifs in connection to dynamical systems is cool. We can think of the actual contributions that each individual motif has. Each individual motif has to dynamical properties of a system, at least in linear dynamics, and that does cool stuff to improve how people infer networks from data. I want to see if there's a three-minute version of the second project over here. And so that is a project of alcohol use in college students, basically done as a number. Basically, that is an important subject because it turns out that a lot of people, young adults and not so young adults, have problems with alcohol and it affects their mental health, their physical health, and their productivity. And the thing that I was particularly interested in was modeling that through co-evolving systems, where you might have people who have a higher likelihood to drink a lot, people Drink a lot, people who might not drink at all. And these people have social networks together, as is common on college campuses. It's a very social time. And the data indicates that the people that you hang out with does have an influence on your attitude towards drinking alcohol. If we model that as a co-evolving system, we need to keep track of all of those node states over here. Of all of those node states over here, and we need to keep track of all the elements of our adjacency matrices, of our adjacency matrix that tell us how that network evolves. So, we have a bunch of variables here that we need to keep track of. I came up with a dynamical systems model that I wanted to use for that, which was heavily inspired by seeing Heather talk about violent continent models a month ago. There's a time scale parameter, most importantly, a tolerance parameter. Most importantly, a tolerance parameter that says that if you're like similar-ish to me, I'm going to want to spend more time with you in the future. If we're not similar enough, so given by that parameter, then we're going to reduce the amount of time that we're going to spend together in the future. And there's two functions over here that you would specify to describe how those types. Describe how those tie strengths grow or decrease in the model over time. So, for example, you can have just linear growth, exponential growth, or logistic growth of the tie strength of people with whom you agree. And that's a bunch of variables to track, so I didn't really want to do that, so I thought of thinking of that in like a mean field framework or like a block structure framework where you have basically a bunch of nodes that have exactly the same. A bunch of nodes that have exactly the same coupling structure. Started out with three groups that ended up being too many, so we boiled it down to two. Now we just have two variables, opinion variables to track, and really just two interaction variables to track, so we're down to a system of four. And there's some simplifications that we could do so that we end up with a system of actually just two variables changing over time. And in that reduced system, we get some really cool dynamics. We get some really cool dynamics that I don't fully understand yet. And I would love to talk to anyone in the room about what the hell is going on in these plots. So, this is what it looks like for linear growth. For exponential growth, instead of having these funny saddle points, we get something like phase transition that happens here at the axis of zero communication between the communities. And we can also do logistic growth so that we don't have any tie strings. We don't have any tie strings going off to infinity. We have a manifold of stable states over here on the axis of consensus. So basically, if everybody has the same opinion in the system, nothing's happening anymore. That is something that we have in all of these plots. We can scale that down again. And then this is what it looks like if instead of like some weird transformed variable, we're showing the results for the actual tie strength in the system. So in the logistic case, The system. So, in the logistic case, that would be limited to limit between 0 and 1. Apologies for running through that. As I said, this is very much work in progress. So, in terms of the conclusions, it's like all over the place. A few things to point out is that this is a weighted co-evolving network, which is kind of a funky thing that I think not too many people have done before. I love it because it allows me to do proper like dynamical system stuff and do stability analysis and the things that I get. Analysis and the things that I get. I think a lot of the cost that I'm seeing comes from introducing that tolerance parameter and seeing it's not just Al and I don't have to have exactly the same attitude towards alcohol to be like, yeah, she's cool, I want to hang out with her more. There's a tolerance value. And so that does a lot of interesting stuff to the dynamics in terms of things that I'm interested in in the future. Like what happens if that tolerance parameter is different for different people? Is there a way to go back to the three Is there a way to go back to the three-block case or even four-block case without losing track of all of the variables that you have? Is there, so I'm looking at Irina here, like, I would end up with a high-dimensional system with like some... So definitely the line of consensus is always a set of fixed points. So I have that criterion of having not just isolated fixed points. So I have no idea what to do with that. So I'd love to get people. With that, so I'd love to get people's feedback. So, thank you very much. All right, what questions do we have for Alice? Comparing graphs in the first half of the talk and doing dynamical systems on the second?