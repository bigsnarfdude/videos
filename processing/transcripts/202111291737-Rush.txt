So to wrap up today, I'm going to tell you a little bit about variational inference and robustness. And this is joint work with two colleagues from Colombia, Marco Avela, who's a statistician, and Pepe Montial, who's an econometrician, and then also a student, Emma Carr Velez, who's an econometrician from Northwestern. So what I'm going to do today is the following. So we're going to look at something called the alpha posterior, which is just Posterior, which is just our sort of regular Bayesian posterior, posterior, but we have the likelihood scaled by some alpha parameter. So sort of down weighted by the alpha parameter. And we're also going to look at variational approximations to this alpha posterior instead of the regular posterior there. And the idea is that, you know, there's been a lot of recent work analyzing statistical properties of both the alpha posterior itself and variational. Posterior itself and variational approximations to it, and we're hoping to contribute to this literature by showing that these procedures, if they're properly tuned, are robust to a sort of model misspecification. So this is kind of the plan I have for today. So to start, I just want to give us a quick, you know, reminder of what variational inference is. So the basic idea is the following. We want to learn from data. I, you know, we want to learn from data. How are we going to do this? Well, we're going to put a model on the data that's a joint probability distribution over my observables, my data x, and then some hidden or latent variables that I'm calling theta. And so the actual learning is going to be through we want to use the data to say something about our hidden variables so that this can tell us something about the data generating mechanism. And so the usual way we do this sort of The usual way we do this sort of posterior inference is looking, of course, at the posterior distribution of our hidden variables given the data. But unfortunately, in many cases, we can't actually compute this posterior distribution exactly because computing this marginal and the denominator is either impossible or computationally difficult or what have you. So the usual thing that folks The usual thing that folks will do in these cases is to, instead of trying to access this posterior directly, we approximate it in some way. So variational inference is one way to approximate this posterior. The idea is it turns this sort of inference into optimization, right? So the idea is the following. What do we do for variational inference? Well, we're going to posit some sort of variational family. So this is a family of distributions. This is a family of distributions over these hidden variables that I'm going to think of as being parameterized by some vector new. And then I'm going to sort of find the distribution in this family that best represents the true posterior by trying to minimize the Kolbach-Leibler divergence between, you know, you find the member of my variational family that minimizes the Kulblerler. Of my variational family that minimizes the cobalt glabular divergence to the posterior itself. So, here's a little illustration that I stole from Dave Bly and some other folks, NERVS tutorial from 2016. So, the idea is, you know, the entire space is all the possible distributions over my latent variables. The circle is my variational family, my sort of set I'm looking in. We have the truth. I'm looking in. We have the truth that's up here outside of the circle, and so variational inference is trying to find this new star that minimizes this KL divergence. Okay, so if you've never seen this before, you should be thinking, like, how do we actually minimize this KL divergence if I told you that there's no way we can calculate this in the first place, right? But this is kind of like the lucky thing about variational inference. Sorry, I think I got a little. Sorry, I think I got a little bit ahead of myself. But okay, so the idea, right, is that we're looking for this new star that minimizes the KL divergence. I'm just reminding you what the KL divergence is, but we know it. So, you know, let's skip to what I wanted to say, right? So how do we actually do this, right? And so the key point here is that we can represent this KL divergence as, you know, three terms here. As you know, three terms here, right? Two depend on our set Q that we're optimizing over, the first two, and then we have the third term that's just the log marginal, this thing that we can't actually compute. But the lucky thing is this term doesn't depend on the cues, the news that we're optimizing over. So we can just drop it when we think about doing the optimization. So all of this is to convince you that this is just kind of a That this is just kind of a tractable thing that we can do, right? We can actually maximize this elbow equation, which is equivalent to minimizing the KL divergence, and this elbow equation doesn't have my marginal, which is the thing that's hard to compute, right? And so, okay, so now that we're convinced that this optimization is a reasonable thing to attempt, the last thing to do is kind of decide what variational family. Of decide what variational family, what should this family be in order to do this optimization? And so the idea is that the following, right? If this family is too complex, right, this optimization is going to be difficult, right? If it's just all possible functions, we'll have a hard time doing the optimization. But if it's too restrictive, then we won't actually get a good approximation to the truth. So a common choice in this situation is just the mean field family that assumes that the That assumes that the latent variables are mutually independent and each governed by their own sort of variational factors. And so here, you know, I haven't sort of said anything about the form of the densities that go into this mean field family, and generally they're chosen appropriately for the type of the corresponding latent variable. So if it's contiguous, perhaps they're Gaussian. If it's discrete, that they wouldn't be, right? They wouldn't be, right? So it's kind of an active area of research to sort of study, you know, more complex variational families. But today, at least, I'm only going to think about the Gaussian mean field family in this talk. So in particular, I'm going to assume my variational family is this mean field family where I'm assuming mutual independence of my hidden variables. And in fact, I'm just going to assume that the component distributions Component distributions are Gaussian. Should I think that it's one-dimensional, theta j and nuj? Yeah, so theta j now we're thinking of as just one dimensional. The nu can, you know, be more than that. You know, if it's Gaussian, it could be like the mean and the variance. Okay, so that's my quick review of variational inference. And now I'm going to tell you a little bit about robustness. Little bit about robustness. Okay, so what is statistical robustness? Well, in our context, what do we want to do? Well, we want to give some sort of procedure to do this posterior inference that performs well even if our assumed model is wrong. So that joint distribution we put on the latent variables and the data in the first place, what if that's not right? We'd like to still have a procedure that does something reasonable. Procedure that does something reasonable. So, this is what I mean when I say we want to be robust of model with specification. What if the original model is wrong? So, one thing that people have done to try to get some robustness in these sorts of inference procedures is to look at alpha posteriors, which have also been called tempered or fractional or power posteriors in the literature. Literature. And so this is the posterior we looked at earlier. It's our usual posterior, but we throw that alpha parameter on the likelihood, and it's going to downweight the likelihood in some sense. So we'll study, again, alpha posteriors and their variational approximations, which just means, you know, the member of the Gaussian mean field family that's closest to the alpha posterior in KL divergence. And so I've And so I've written, you know, this guy as having being parameterized by Î¼ and sigma, right? But it's actually a member of the Gaussian mean field family, so this sigma is going to be a diagonal matrix, but just for notation purposes, I might think it is such. So what we want to do is try to characterize the robustness properties of alpha posteriors and their variational approximations rigorously. So, a lot of folks, like I said, have been studying alpha posteriors. This goes back at least to the 1990s with some work by Bach and Baron and Cover. But more recently, there's been a lot of work kind of studying the statistical properties of both the posteriors and their variational approximations, often in sort of high-dimensional or non-parametric settings. So, our contribution and what we're trying to do is look at robust. Look at the robustness of these objects to misspecification, particularly in kind of a classic statistical regime, which is just, you know, parametric low-dimensional models, right? So I'm thinking about a setting where the dimension of my latent variables is fixed at p, but my sample size will grow, right? So this is a quick question. Even for the Gaussian mean field uh family, um is it convex in mu? In mu? Uh okay. The optimization program is it okay you mean like when we look at this elbow here the elbow of uh um uh Uh well so I mean I guess it it depends on like what my like my guy is here, right? I think there was a type of I think that there was an algorithm before the cable. Perhaps maybe that was an alpha max. Sorry. Where? Right at the beginning and then on the second the slide before the one you're up to. Okay, sorry, sorry. I see for the Gaussian case, you can do it efficiently, you prove a bit that. I mean, so it's so it, you know, in general, yes, this is, there are reasonable algorithms to do these sorts of things. Like in a lot of cases, just like coordinators and type things will work. Okay. Um, right. On that side you had a nicer. With a kale and not a okay. Oh yes, sorry, yes. So that should be evident, right? My bad. Okay. So we're looking at this parametric framework, right? So I'm going to just save my parametric family, right? So the idea is, you know, I have a statistical model. I have a statistical model for my observed data. And so this is going to be just a parametric family of densities that I'm going to call cursive p, right? And we're parameterized by my hidden variables a vector theta that I'm thinking about as being in, living in Rm. So, you know, okay, so what does it mean to be misspecified, right? If P0 is the true sort of The true sort of density for x, then the statistical model, my model, is going to be misspecified if p naught doesn't belong to p, right? So there's no theta in my parameter space that determines p naught. Okay, so to get to the results, I'm going to define my maximum likelihood hest evator just as you always do, right? So it's going to be the value of my... The value of my parameter within my parameter space that maximizes the likelihood. And I'm also going to assume sort of some, I'm going to assume that I'm in the usual world of this sort of maximum likelihood estimation. So, in particular, I'm going to assume the uniqueness of the estimator and the asymptotic normality of sort of the Normalized estimator where my theta star here is either the true parameter, right? So the maximum likelihood estimator will approach the true parameter if we're correctly specified, or if we're misspecified, this theta star is going to be my pseudo-true parameter, so the best value. And so we kind of make these really usual assumptions, just Usual assumptions, just, you know, so we don't want our kind of robustness results to like depend on like wacky things happening, right? We're just kind of everything is behaving nicely. And this, you know, holds generally under fairly mild conditions. Okay, so the first thing that we aimed to do was actually look at the asymptotic properties of the alpha posteriors and these variational approximations. Posteriors and these variational approximations. So, this is when we study, when one studies, you know, asymptotic properties of posterior distributions, this is usually referred to as Bernstein-Von Mises theorems or BVM theorems. And so these sorts of BVM theorems tell us things like, you know, posteriors are asymptotically normal. And so, you know, the first thing that we wanted to do was study the asymptotic process. Do we study the asymptotic properties of the posteriors we're interested in, and also the variational approximations? And so, what we were able to do is, you know, in the beginning we weren't really sure how the down weighting of the likelihood would change the asymptotic distributions, but it turns out that, you know, what we find is that our posteriors and the variational approximations are also asymptotically normal. And the way that we prove this is by extending to Two known sort of results. So, the first result is asymptotic normality for the usual posteriors under model misspecification. So, we're going to extend that to study alpha posteriors under model misspecification. And then also the result of Yishen and Dave that tells asymptotic or gives asymptotic normality of variational approximations to the usual posteriors under misspecified models. Under misspecified models. And so, you know, when I say asymptotic normality here, what I actually mean is that these results show that the T V distance between the studied distribution and their sort of Gaussian, the limiting Gaussian distribution, approaches zero as the size of the data grows. So the first two results I'm going to present are just the kind of asymptotic Gaussian. The kind of asymptotic Gaussian distributions associated with the alpha posteriors and the variational approximations. So before I, you know, so I'll start with looking at the asymptotic distribution of the alpha posterior, but I need some assumptions. And it turns out that, you know, when we extend Klein and Vanderbaard's result for asymptotic Gaussianity of usual posterior zendomadomous specification, we get the exact same assumption. Assumptions. So, you know, I state the assumptions, but I kind of hid them here because the details are nasty and I think perhaps not so important. But the first assumption is something about the likelihood ratio of my presumed model, of my, you know, the likelihood that we use to calculate the posterior. And so we need to assume that this is stochastically, locally asymptotically normal. And this is a standard. Normal. And this is a standard sort of assumption in this literature. And the second assumption just is an assumption about the rate at which the posterior concentrates around the pseudo-true parameter. And we need this rate to basically match the rate of the previous assumption. But these two assumptions, especially if the data is IID, they can be verified. Verified and they're not too restrictive. So under those assumptions, and supposing that our prior gives sufficient sort of, is nice and gives sufficient mass around our pseudo-true parameter, then what we can show is that the alpha posterior has a limiting Gaussian distribution. Gaussian distribution where the limiting Gaussian has the same mean as the regular posterior, right? But we've downweighted the variance. So here, this matrix is the matrix for my assumption one. It's like the curvature of the likelihood. So it is an inverse Fisher matrix in some cases. But the idea is the way that the alpha parameter has changed this is that, you know, we sort of, sorry, I think I said downwood, but Sorry, I think I said downweighted, but we've actually upweighted the variance here, right? So it's as if we kind of have like fewer data points somehow, because you know, if you have fewer data points, your mean doesn't change, but your variance gets bigger. So we upweight the variance there. And so, okay, so now we can do the same game with the variational approximation to the alpha posterior. And, you know, what perhaps you would think is that, you know, if I'm going to If I'm going to project my alpha posterior onto the Gaussian mean field, then its limiting distribution would be the projection of the limiting distribution of the alpha posterior itself. And so this is kind of what we find, right, is that the asymptotic Gaussian unity here just projects everything onto my Gaussian mean field. So, okay, so I have, you know. I have some notation that I need to write, which is in particular, we want to find the density in the Gaussian mean field that's closest to the limit of my alpha posterior, which we can compute sort of in closed form. So the difference is now that my variance has been, my covariance matrix has been diagonalized. Matrix has been diagonalized. And so under some regularity conditions, we're able to show that my variational approximation to the alpha posterior has a limiting Gaussian that again has the same mean. The mean doesn't change by the alpha weighting, but the variance gets larger here. And so in some sense, if you think And so, in some sense, if you think about mean field variational inference, we kind of know generally that it tends to underestimate the covariance in the response. So in some sense, it's perhaps a good thing right here that we're actually upweighting the variance because it's correcting some of the downweighting that the variational approximation does. Okay, so I have like five minutes left or something, I think. Is that okay? I think is that? Okay. Okay. So now with these asymptotic distributions, we actually do the robustness analysis. So the question we wanted to answer is, okay, if the alpha posteriors and their approximations downweight the likelihood, can we say something about whether or not they're more robust than the standard Bayesian inference? And so the first question. And so the first question we had to address is: you know, how do we actually measure such robustness to model mispecification? And so this is what we decided to do. So our big idea was motivated by this work by Gustafsson. And his idea is the following. So if we have two different procedures that both lead to incorrect posterior inference, then we still might prefer one over the other if one, you know, is closer in terms. You know, is closer in terms of KL divergence to the true posterior. So even though they're both wrong, we might want the one that gives us the closer answer in KL divergence. And so this idea motivated us to look at KL divergences of the following form. So we wanted to measure the closeness of the alpha posterior or its variational approximation to the correct posterior. So this is the kind of object we were aiming. So, this is the kind of object we were aiming to study. So, okay, so in terms of the misspecification model, we had to make some assumptions to enable tractability of studying this object here. And so the assumption we make is the following. So we don't assume that our model is always misspecified. So we don't assume that P is always wrong, but we instead assume But we instead assume that there's some probability epsilon of misspecification. And so the idea is that, you know, with probability 1 minus epsilon, your data comes from a distribution in my parametric family P, but with probability epsilon, it doesn't. And so this is like kind of a weird like epsilon contamination model. It's not like epsilon percentage of the points are Like epsilon percentage of the points are messed up. It's like you're totally messed up like epsilon percent of the time. And so with this notion, we then study the expected KL divergences under this epsilon or 1 minus epsilon probability of misspecification. So therefore, when we think about looking at that KL between the correct and the reported, the correct And the reported, the correct posterior is going to change based on whether we're misspecified or not. So in particular, if we're well specified with probability 1 minus epsilon, then p correct is simply like my p alpha by alpha equals 1, right? So no tempering, right? My usual posterior, that's the correct posterior. But if we're misspecified, my p correct is going to be. My p-correct is going to be something else, and so I'm going to call this something else p-star. And in fact, I'm going to think that it's based on a different parametric model that doesn't overlap with our own. So maybe some parametric model F that has a theta generating the data that doesn't belong to P. Okay, and so then what we want to look at is this expected KL divergence where Expected KL divergence, where you know, now I have two terms. So I have a probability of misspecification epsilon, and so I'm looking at the KL divergence between the thing I want to study, my alpha posterior, and the correct posterior here, which is this P star. And then I have a term for my correctly specified case, where I study the KL divergence between my alpha posterior and alpha equals 1. And alpha equals 1, my real posterior. And so, looking at this guy, we can think that what we want to find is that we'll find robustness of the alpha posterior if this is minimized by some alpha that's not 1, some alpha that's less than 1. Then that's saying that the alpha tempering is working. Okay, and then you can play the same game, right, with the variational approximation. So, I have this. Approximation, so I have the same sort of expected KL divergence, but now I've replaced my alpha posterior with you know my variational approximation, but the correct still stays the same. And so it turns out that you know analyzing these sort of expected KLs directly is hard. So what we did is you know we instead analyzed the KLs between their limiting Gaussian distributions, which is then easy because we know Kal divergence between Gaussian distributions. KL divergence between Gaussians, right? So, what I want to do, right, is this is what I'd really like to study, but what I'm studying instead is just replacing all these guys with their limiting Gaussians. And so, you know, most of this is from the BVM I gave you earlier, but you know, some of it is from classic BVM theory. And so I'm going to call this R as a function of alpha, right? And eventually I'd like to minimize this as a function of alpha. And I'm similarly. function of alpha and I'm similarly going to define an R tilde of alpha that's just the same thing but with the Gaussian approximations, the variational approximations. And so, okay, again, right, what we'd like to do is try to find the alphas that minimize these expected K L's, because these are going to be the alphas that give us, you know, the sort of most robust Inference. So, in particular, I'm going to define alpha stars as the alphas that are minimizing these two expected KLs for, you know, KL with the alpha posterior, KL with the variational approximation to the alpha posterior. And the result we find is the following. So we can calculate sort of the asymptotic values of these alpha stars. So this is the alpha star. stars. So this is the alpha star for the alpha posteriors, this is the alpha star for the variational approximations. And kind of the message is that both of these guys are strictly less than one, right? And we get their values exactly. And so I'm maybe getting close, but so I only have, so okay, so now let's try to interpret this result and see what it actually tells us about robustness and Robustness and MVI. So, I mean, the first takeaway, right, is that these are less than one, so some tempering is indeed what we want. But what we can also do is compute the exact limit of the optimal, like the optimized KLs. So if we plug in the alpha stars. And this is kind of interesting because what it shows is that, you know, both of these terms, so optimized with our optimal alphas. With our optimal alphas, they're going to increase logarithmically in the size of the misspecification. Whereas if we use alpha equals one, the terms in fact increase linearly in the size of misspecification. So, you know, these KL divergences grow more quickly if you don't use the alpha tempering. Moreover, what we can characterize is that under some conditions, we actually Conditions, we actually can show that optimized variational approximation is more robust than optimized alpha posteriors. So, doing the variational approximation even adds robustness in some settings. So, this is perhaps like not too surprising because something like approximating with a Gaussian median field, like regularizing in that way, should might be expected to give some. Might be expected to give some robustness. And so, kind of the takeaway is we believe that this provides a rationale for the use of variational inference, like generally due to their robustness to model misspecification, as opposed to solely because it allows us to computate or compute the, man, I nailed it on time, to compute the posterior. Posterior efficiently. And so there's some future work. We don't have much about how to actually do it, to select the alpha. And in practice, this gives us sort of a theoretical prescription for choosing alpha that can't easily be translated into something to tell a practitioner. We'd like to look at other loss functions, and generally, we kind of Functions and generally, we kind of wonder if, you know, what if we can characterize like robustness of just some function of the likelihood, not, you know, not necessarily it being the alpha power. So, you know, then we might be able to say something about which function is optimal in terms of the robustness properties. So, thanks. Okay, so. So so so what was exactly the role of epsilon in this setup? And does that show up in the guarantees? Yes. Do you want to in the guarantees or in the setup is a... Yeah, so this is a little this is a little bit wacky, right? So the reason kind of this is here, right, is that if you look at something like, you know, Look at something like, you know, this, right? And it kind of says, well, if you know you're misspecified, then what we find is that asymptotically, alpha should be zero, right? If you know you're misspecified, then don't use the model, right? And then this term, asymptotically, you know, if you know you're correctly specified, then alpha should be one, right? You should use the model. So kind of like this balancing with the epsilon is needed to for Is needed for something interesting to happen. And so, in particular, you know, I didn't sort of explicitly say this, but in the theory, we actually need this epsilon to be decreasing, so going to zero, in a sense, because n epsilon has to approach some sort of constant. So, this is kind of like, you know, what we're imagining is, you know, if someone If someone is a careful modeler, right, and they believe, you know, they worked really hard to make their model correct, right? And so they believe that it's probably correct, but they acknowledge that there could be wrong in some way. And perhaps when you get more and more data, you're sort of more and more likely to believe that your model is correct. And so this is at least the story we're telling to the reviewers right now. Hoping that they buy it. Yeah. What for the theta not and the theta star here? Yeah, okay. So these are the limiting values coming from the two parametric families. So you have the parametric family that we assume, and so it's going to limit to the pseudo-true parameter. And then we have the parametric model that is, you know, when P is incorrect, we're believing that our data are drawn from. We're believing that our data are drawn from this other family. So this is the same thing. Yeah. Okay. So yeah, so this is, again, I acknowledge that it's a little bit wacky. So when we think about, you know, being misspecified, we actually have a separate parametric model that we're thinking of the data being generated from. And so that's the limit there. The limit there. Is it important that the other parametric algorithm have the same dimension? Oh, so we assumed that it did. And I'm not sure, I haven't thought about what it would look like if it did that. I mean, and I mean, certainly this result assumes that it does in order that that difference makes sense there. you know, it makes makes sense there. But um yeah, I'm I'm yeah, I haven't thought about that. Uh yeah, I think it might be hard to lose that assumption. Okay. Thanks again. 