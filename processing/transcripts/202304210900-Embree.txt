The talk of Mark Humphrey from CG Colec. With the title, you can see two approaches to pursue the spectra for generalized high-defined problems. Well, thank you very much. Thank you for the wonderful talks this week. I'm glad to see you all here on the last day. We've heard some talk of pseudospectra throughout the week, and I wanted to just give some thoughts about different approaches to pseudospectra for matrix pencil problems. Matrix pencil problems and really illustrate the ideas with two case studies, if you will. And so just to set our perspectives a little bit, let's start with the pseudospectra of a standard matrix. And so we can define the epsilon pseudospectrum as the areas in the complex plane where the norm of the resolvent is larger than one over epsilon. So we're thinking of epsilon as a small number. We're looking at areas where the norm of the resolvent is The norm of the resultant is quite large. And I'm stating everything here for a matrix, but of course, many of these ideas extend naturally to operators in various settings. Equivalently, one can define the epsilon pseudo-spectrum as the set of all eigenvalues of A plus E, where E is a perturbation whose norm is bounded by epsilon. And there are reasons why we use strict inequalities here, though some definitions take weak inequalities. And so we have these two nice definitions. We have these two nice definitions, and we can prove their equivalence really just by looking at the norm of the resolvent and saying the resolvent has a large norm when the singular values of Z minus A are small. And we can use the singular values and singular vectors then to construct actually a rank one matrix E, such that our point Z is in the spectrum of A plus E. And similarly, if we have a point in the spectrum of A plus E, we can manipulate the eigenvalue. We can manipulate the eigenvalue eigenvector equation in a way that exposes the resolvent and some norm inequalities that show that the norm of the resolvent must be large. And so we have these two happily equivalent definitions. And we might prefer one perspective or the other, depending on the setting we're interested in analyzing. And so maybe the quintessential application of pseudo-spectra are really to Tocqueville's matrices and operators. I think this is perhaps the greatest triumph in some. Perhaps the greatest triumph in some ways, because the theory is so pretty. So, if we have a Toplitz matrix, an n by n Toplitz matrix, that is a matrix that's constant on the diagonals, we can construct a function called the symbol through this Laurent polynomial, particularly if it's a banded operator, where we have a of z is the coefficients a k times powers of z. And if we apply that function to the unit circle, we get a Unit circle, we get a symbol curve. So that image of the unit circle under the symbol. And that's intimately connected with spectral theory for the Toeplitz operator. And so Albrecht Butcher would credit Henry Landau with one of the first pseudo-spectral applications back in the 1970s. Reichel and Trefethen looked at this problem in the early 1990s. But Albrecht Butcher and his colleagues have really sort of completed the theory here in a beautiful way and really made the key crucial connections. And really made the key crucial connections. So, just as an example, on the left, the symbol curve, and then on the right, the set of eigenvalues that we would get, the limiting set, as n goes to infinity. So, we see the eigenvalues of a 50 by 50 version of this matrix fall on these kinds of curves. We can actually characterize that limiting set using theory from Schmidt and Spitzer from the 1960s. And we can compute what that limiting set is. And it's a nice agreement with the eigenvalues of the 50 by 50 version. Of the 50 by 50 version. But if we maybe increase the matrix to 100 by 100, the agreement still looks very nice. We can even maybe question how they distribute on that curve. But when we get a little bit larger, we see 200. That's a little funny business going on there. 400 now, whoa, everything changed. So we're now well off of this limiting set, which we can improve the eigenvalues of A 400. Eventually, 400 gets larger and larger. We'll accumulate on it. But we don't see that computation. But we don't see that computationally. 800 is even worse. What's going on here? Well, the spectrum of the Toplitz operator will be all the points on this symbol curve and the points they contain with non-zero winding number. And so the norm of the resultant actually grows exponentially within those regions for these banded Topelitz operators. The pseudo-spectrum of just the 100 by 100 Topolitz matrix already starts to hint at that infinite dimensional spectrum. Infinite dimensional spectrum. And if we go a little bit larger, just to the 200 version, we see how rapidly the pseudo-spectra have grown. And so the pseudo-spectra really are revealing the spectrum of the infinite dimensional operator. And they're giving us insight into why the eigenvalues are so sensitive to perturbations. So I'll show a lot of plots like this, and I appreciate that this minus five down here must not be visible to any of you. But it is 10 to the minus 5 here refers to the log base 10 of epsilon. The log base 10 of epsilon is 10 to the minus 5. So the norm of the resolvent is 10 to the fifth on this green curve, far away from the eigenvalues for sure. And perturbations of size 10 to the minus 5 could move eigenvalues sort of anywhere in that green region. So we'll look at a lot of plots like this. So we can think of eigenvalues of perturbed tokens matrices for pseudospector. But we also are interested in other applications. Applications. So often we mention the behavior of a dynamical system where the matrix A, even a two by two matrix A, maybe is non-self adjoint, and that non-self adjointness can induce some transient growth in the solution. And so we see that even though this is a stable system, eigenvalues minus one and minus two, we grow from an initial condition of size order one to something like 20 before we eventually get that asymptotic decay. And in that turn Of topic decay. And that transient growth might be a curiosity or just some annoyance, but it has real implications if we view the linear system as a linearization of a non-linear dynamical system. So take that same A matrix, but now pop in a little y squared term in this dynamical system. Y dot equals Ay plus 1 20th Y squared. And that little hint of nonlinearity gets spun up by this transient growth, and eventually we see much rapid growth. We see much rapid growth in the nonlinear system. So, linear transient growth can feed nonlinearity. This is a phenomenon that a number of people have realized in different settings, and Mick Trefethen championed this idea in the early 1990s in the context of transition to turbulence in fluid flows. So, how do we detect the potential for transient growth? Well, of course, there's a long connection between E to the TA and the resolvent going back. TA and the resolvent, going back to the sort of Laplace transform formula or the expression for e to the dA as a Cauchy integral. So as we start to think about functions of matrices more generally, we often think of the norm of the resolvent. And we can even get bounds on the size of the transient growth in terms of the size of the pseudospectrum. So how far does the epsilon pseudospectrum extend into the half right half plane as a function of epsilon? Plane as a function of epsilon. If it's extending more than epsilon into the right half plane, we're guaranteed some kind of transient growth. And so for this simple case that I've been looking at, we can look at the epsilon pseudo-spectrum for epsilon is 10 to the minus one and see that it extends beyond 10 to the minus 1 into the right half plane. That guarantees us that the norm of e to the ta must grow a certain amount. The blue curve must cross the dashed frontier at some point in time. Frontier at some point in time. And there are ways of trying to gauge what time that might be, and so forth. But just as an illustration, there's a connection between transient dynamics and the result, and that comes through these formulas for the solution operator. Everything so far has just been for standard matrices, and now we're prepared to start thinking about matrix pencil type problems. So let me frame these two differences in perspective. We have this perturbed. Perspective. We have this perturbed eigenvalue definition. The pseudo-spectrum is a set of all eigenvalues of A plus E. Great way to understand asymptotic stability of uncertain systems. If the entries in A are measurements and we're not sure whether they're accurate or not, or we're interested in doing a parameter study, or our system is sensitive to noise perhaps. We can ask questions like: is some matrix near A unstable by looking at the eigenvalues of A plus E? Now, this invites one to think about. Now, this invites one to think about structured pseudospectra. A common question is: you know, why are we allowing complex perturbations? If we have a system A that comes from a physical system with real parameters, maybe we should look at only real perturbations, or maybe perturbations that preserve that toplet structure, or give us a stochastic matrix. And so, there's a literature in these structured pseudospectra, they can be more difficult to compute and they can be almost impossible. And they can be almost impossible to compute if the structure gets too elaborate, but it's an interesting perspective. And then we have the norm of resolvent definition. So equivalent, but really more suited to the perspective of bounding, you know, when is norm of E to the TA or norm of A to the K or just norm of F of A interesting. And of course, it's rooted in semi-group theory, this idea of the connection to the solution operator for a dynamical system. And this perspective was really kind of, I think. And this perspective was really kind of, I think, expressed very nicely in a paper by Green and WagenConnect in 2006 that's really on delay equations. And most of the paper actually deals with the perturbed eigenvalue approach, but they have a beautiful section that looks at the semi-group approach in the context of delay equations. So the idea is to discretize the infinitesimal generator of the semi-group and analyze that instead of analyzing coefficient matrices in the problem. And that's very much aligned with the approach that we take for matrix. With the approach that we take for matrix pencil problems. Okay, so when we just have a simple eigenvalue problem, these two approaches agree, but we want to understand more complicated systems. And so this is now the first of the two vignettes I want to tell you about. Pseudospectra that are suitable for understanding transient analysis of differential algebraic equations. This is a paper with my former undergraduate, Lake Keeler, who got his PhD in spectral geometry at UNC and is now at LU. At UNC, and it's now at Bellingham School. I think these two pictures kind of illustrate what we'd be interested in analyzing here. So simple, linear, time invariant, differential algebraic equations, Ex dot equals AX. Here E has changed from being a perturbation to being a coefficient matrix. I hope that's not too jarring. Coefficient matrix A, matrix E. This is a matrix that is. This is a matrix that gives us a stable differential algebraic equation. So we could start out at some initial point on our trajectory. It's eventually going to go to the origin because it's a stable system, but it has transient growth. And so it grows before it eventually decays. And of course, all of that growth is in this two-dimensional plane that characterizes the algebraic constraint imposed by this singular matrix E. So the algebraic constraint gives us a two-dimensional field on which Dimensional field on which to play, and the trajectory grows on that plane before it eventually decays. This is a case with two real eigenvalues and one infinite. Here's a case with complex eigenvalues, also stable. Again, the solution restricted to a plane. Stable, it's going to go into the origin, but it's going to grow transiently and spiral into the origin, all again within that constraint plane. What we'd like to be able to understand. What we'd like to be able to understand is, you know, how far can that transient go before it eventually comes back into the origin. So we'd like to develop tools for understanding that. Now, we're not interested in three by three toy problems on a two-dimensional plane. We'd like to motivated to understand stability of fluid problems. So, doing linear stability analysis for incompressible fluid flow problems, where we'll get a differential algebraic equation of this. Algebraic equation of this form with an E matrix that has a clear singularity and then a sort of saddle point structure to the A matrix. This is a matrix that's a problem that's well known to induce an index 2 DAE. And so it's got some particularly interesting structure, 2x2 Jordan blocks associated with the eigenvalue at infinity, if you will. And so we would like to be able to say something. Like to be able to say something about the potential for systems like this to exhibit transient growth because we're going to arrive at this system as a linearization of the Navier-Stokes equations. We'd like to be able to understand is fluid flow in a pipe or over a backward-facing step going to be stable or not? Or will it go turbulent? And so we could do linear stability analysis. So we could linearize the problem, see if all the eigenvalues are in the left half plane, declare victory if they are, and call it a stable flow. If they are and call it a stable flow. If their eigenvalues in the right half plane, we'd say it's unstable. But if it's stable, does it have the potential to exhibit transient growth? Would be our question. And so we'd like to be able to understand problems of this sort. And so we need to develop the tools to do that. So it might be natural to just say, let's use the generalization of that eigenvalue of perturbations definition. So instead of just allowing perturbations to A, Instead of just allowing perturbations to A, we'll now allow perturbations to both coefficients. Now, how large can those coefficients be? Well, the standard way that's been developed for these generalized eigenvalue problems has been to allow the perturbation sizes to vary with the coefficients. So we have two free parameters now, a gamma and a delta, to tell us how much we want to change one or another. Maybe you know your E matrix is a mass matrix and it's certain and you don't want to allow any perturbations there, so you would set delta. There, so you would set delta to zero, and you're only going to allow perturbations to A because maybe there's some uncertainty in that coefficient. For example, it's well known that this definition has a resolvent-like equivalent definition that, of course, involves the gamma and delta parameters, but it's more or less the same, looking at the norm of CE minus A in this. So, we might say, let's look at our matrix pencil. Oh, sure, of course. Yeah. Go back to the definition of the very epsilon missing in the first. Oh, of course. Yes. Epsilon, gamma, and epsilon, delta. Yes, thank you. Oh, caught. I'll lie many times in this talk now that that definition is wrong because I've repeated it several times. It's a better definition than the one I've stated, but I'll still critique it for context of understanding transient analysis. Understanding transient analysis of these equations. Okay. Now it's interesting, this set can actually contain the entire, be the entire complex plane if we allow a perturbation gamma and delta that makes A plus gamma, E plus delta a singular pencil. So you aren't kind of guaranteed to get an epsilon contour when you draw these because it could be that you've reached an epsilon large enough that the entire complex plane is your pseudo-spectrum. Okay, but will this definition help us understand transient dynamics? Well, when Nick Trefethen and I worked on our book on pseudospectra, we took a stand that as we define the pseudospectra of a matrix pencil, if we're interested in understanding transient dynamics of systems like this, then, well, if we pre-multiply that system by just an invertible matrix T on both sides, we don't change the dynamics. T on both sides, we don't change the dynamics at all, right? Yet we change the pseudo-spectra by this definition. And so our approach that we took, sort of mimicking some work by Axel Rua in the mid-90s, is to say at least if E is invertible, then the pseudo-spectra of E inverse A is the correct object to study. But you might want to do that in a different norm than the two norm, depending on what you're actually trying to measure. On what you're actually trying to measure. So sometimes you might have a norm that's defined in terms of the Cholesky factorization, and that would give you slightly different formulation. And some of the work of Fred Riedel and Axel Rua and so forth looked at things like that. Okay, so we took a stand and said this is the right approach, at least if you're interested in understanding transient dynamics. We didn't address the not invertible case, or at least we only made a tentative suggestion that I think proves not to be the right thing based on regularizing heat. Based on regularizing E. I want to tell you what I think the right approach should be. But first, let me just show you why we care about this pre-multiplication by T. Let's look at that system with that spiraled into the origin. If we use this definition and we pre-multiply by just three different t's, the first is the identity on the left, then just a couple of different invertible t's, you see the pseudospectra change significantly here. They don't look so bad. They don't look so bad. Here they're a little larger, maybe. It's hard to compare sometimes because they're all on the same epsilon levels, but some of the epsilons, maybe they're too large, you get the whole complex plane, for example. So if I looked at some of these, I might say one looks worse than the other just looking at the pseudo-spectra, but they all are the same, induce the same dynamics. So somehow these definitions are missing. So quickly, I can tell you just what to do. So, quickly, I can tell you just what to do, and it's no surprise to many of you in the room who are experts in these problems. We can just go back to the way of generating the solution operator for this equation. We prefer to use the sure form as numerical analysts rather than an alternative form here, like a Germany composition. But we can just, assuming A is invertible, we can look at the Sherdy composition of A inverse E, group up the nilpotent, block. Up the nilpotent block here, which is associated with infinite eigenvalues, and the non-zero block and g. And then we can write down the solution of the differential equation in terms of this G matrix, G inverse in particular, because we're looking at A inverse E there. We've got to flip it back. And that gives us a way of analyzing the solution. Transient growth in X is going to correspond to transient growth in G inverse. U1 is a matrix with orthonormal colonies. Matrix with orthonormal columns. I guess you should just call it here. And so that's the trick. So the epsilon pseudo-spectrum of this AE pencil should really just be the standard pseudospectrum of G inverse. And then you can apply all your standard machinery to just analyze the standard pseudospectrum of that G inverse matrix. I briefly note that if you have a problem where A isn't invertible, you can also. Problem where A isn't invertible, you can always find a mu that's not in the spectrum of the pencil if you have a regular pencil. And so you can do the same thing with A minus mu E inverse E, and it all works. And so we have this sort of transient dynamics definition of the pseudo-spectrum, which just goes back to our old definition applied to the solution operator for the problem. And so we can generalize this with different norms. You can do this in the large-scale setting. So you can just compute, let's say, right-most eigenvalues, and you can apply it. Eigenvalues, and you can apply it in many settings. Here's one pseudo-spectra for flow over a backward-facing step. And so, this we actually want to apply this to a fluids problem, get above three by three matrices here. And so, we look at different viscosities in our problem, and we can just do a parameter study, plot the pseudospectra. The amount that the pseudospectrum extends into the right half plane gives us bounds on the minimum amount of transient growth in the solution operator. The solution operator, and so forth. And so we can see as the viscosity gets smaller, how did the pseudo-spectra crunch in on the right? It looks like the spectrum is getting smaller because I'm doing projections here onto the rightmost part of the eigenvariators. So we can see that non-normality grow with the viscosity shrinks. Okay, so a tool for understanding pseudospectra for matrix pencil dynamics. What I'd like to tell you about in the last five minutes. To tell you about in the last five minutes or so is the second vignette, pseudospectra for Lovener matrix pencils. So at that point in my life, I thought that I didn't have much application for the perturbation of eigenvalue approach, the pseudo-spectra of matrix pencils, until I started thinking about Leubner matrix problems. So we again have a dynamical system, but the setting is going to be different. We're not looking now at transient growth of solutions. Not looking now at transient growth of solutions to this system. We're in the business of data-driven modeling. So, beautiful book I'll advertise by Thanos and Toulis at Rice and Chris Beattie and SirCon Congression at Virginia Tech that kind of goes into this theory in beautiful detail. But the idea is that if we can measure the transfer function, H of S, for this dynamical system, can we then recover the poles of the system or indeed get a real Poles of the system, or indeed get a realization of the entire system. It's a classic systems realization problem. Andrew Mayo and Thanison Toulis developed a beautiful way of doing this through rational approximation instead of just usual Markov parameters. So the idea is that we'll have tangential measurements of the transfer function at interpolation points in the complex plane. So the keys are going to be these interpolation points and the resulting measurements. It took an hour or two to go. It took an hour or two to go through the theory carefully. So I'll just give you the kind of high-level view here. Tangential measurements of our transfer function. We can imagine collecting these kinds of measurements experimentally by driving our system at a certain frequency and measuring the response. We collect these measurements and interpolation points, and this is the kind of the big picture, all that data gets crunched into these Loebner matrices. A Loebner matrix. Matrices, a Leubner matrix and a shifted Leubner matrix, highly structured matrices made up of measurements. And so if there's uncertainty in our measurements, there'll be uncertainty in our Leubner matrices. Once we've got these Leubner matrices, depending on some technical assumptions involving rank of various things, we can actually construct an interpolant to our transfer function. And if we have the right rank and the right amount of data, we can actually realize our transfer function using the Leibniz-May. Using the Leubner matrix and the shifted Leubner matrix as our system matrices. And so now we can, so you have measurements, you can now construct this pencil, you can compute eigenvalues of your system just purely from data by looking at eigenvalues of your Loebner pencil. So we might want to ask some pseudo-spectral questions, like what are the eigenvalues of that pencil? How do the interpolation points affect the stability of the problem? What if we have noisy measurements? Of the problem? What if we have noisy measurements? Those are all pseudo-spectral questions that really lend themselves to this eigenvalues of perturbation approach. As we're thinking now of making perturbations to our Leubner matrix, our shifted Leubner matrix, our Leubner matrix, bounding the size of those perturbations. Now, we'd really like to probably do a structured pseudo-spectral approach where you preserved the Leubner structure and the shifted Loebner structure with those perturbations. That would be very tricky to do. And so we'll just handle standard. And so we'll just handle standard pseudo-spectra. And I'm just going to end with a few plots. So we take a matrix A, whose eigenvalue is just minus 1 to minus 10, and we'll set our E to be the identity. This is just to get a transfer function. We're going to forget that we knew this and just build a transfer function for a single input, single output system. The poles, of course, are minus 1 to minus 10. I'm going to interpolate at the blue squares for my left interpolation points and the red. Points and the red diamonds for my right interpolation points. And I'm just curious: how did the location of these interpolation points affect the stability of the resulting matrix pencil eigenvalues? And what you can see is that if you, in this case, interleave your eigenvalues, left, right, left, right, left, right, or sort all the left and all the right, you get tremendously different results. In this case, the eigenvalue. In this case, the eigenvalues are stable. The pseudospectra look beautiful. In this case, this is 10 to the minus 6. You see, the poles are much more sensitive to perturbations as a result merely of putting your interpolation points in the wrong place. They're all in the same place, just classifying left and right differently. Here's even worse. We're taking the interpolation points over in the right half plane, even though our poles are in the left. And you can see something funny is going on because we don't even recover the poles. Something funny is going on because we don't even recover the poles exactly that they're so sensitive. Or left and right points sorted like that. In these cases, terrible pseudospectra, incredibly sensitive. So if you're putting your poles in the wrong place or arranging them, even kind of over the spectrum, but just sorting them in the wrong way, terrible sensitivity. This is 10 to the minus 17 in green, 10 to the minus 15 and blue there. And you say that doesn't look like a normal pseudoscript. That's all rounding errors, looting those patients. Those patients. What about sensitivity to noise? We just take that nice configuration with the interleaved points that give us the really good pseudospectrum. We took that last case, but we'll interleave the points. We won't do something terrible. We can see the difference that those configurations make. If I have noise of size 10 to the minus 1 in my measurements, I still recover the system very accurately if I put the points in the right place. Here's noise of size 5. Here's noise of size 5 times 10 to the minus 9. So vastly different noise levels, right? This is hardly any noise. And yet you see how sensitive the recovery is if we put our interpolation points in the wrong place. And pseudospectra inform that. And so one last example here: 10,000 trials, perturbation of size 10 to the minus 10, and we can see we put our polls in a slightly more favorable way, we get quite robust results. Favorable way, we get quite robust recovery. If we put our polls in a different way, some of the polls are accurate, and the pseudospectra can hint at that. Others are inaccurate, even to this very small level of militarism. Okay, let me wrap up. I hope shown through these examples that pseudospectra provide a powerful tool for understanding transient dynamics, but also eigenvalues on certain systems. And the moral of the story is that if you're interested in analyzing transient dynamics, work with the solution operator for your system. Work with the solution operator for your system, the generator of the setting group. To analyze eigenvalue sensitivity, use this eigenvalue perturbations approach, and that can give you gradients. But regardless of your approach, you want to work in a norm that makes sense for your problem as informed by the underlying business. I'll wrap up, and if there's time, I'll take a question over. Thank you.