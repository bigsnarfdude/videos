Thank you for asking me to give this survey to start things off. I'll apologize in advance for things that I don't say and things that I say that are wrong. So please bear with me. I have a lot of slides, so some of the things I may go through more quickly than I might otherwise. Otherwise, so, and I would, I hope over the course of the next few days, we'll you know flesh this out in a lot more ways. So, I've got this title, and so it was, yeah, 20 years ago, Eric Allender asked Tony Patassi and me to write a survey article on proof of complexity for the ATCS Bulletin. And so, we picked a grandiose title, so I kind of kept that. So, I kind of kept that with the title of this talk and stressed the importance and interesting things in proof complexity, which at the time was probably less well known than it is now, and stated a number of open problems and directions. So, now we're 20 years later. So, I'm going to talk about progress on our open problems, but I'm going to begin with directions. Begin with directions and importance that we didn't anticipate. So, just to get started, so here was our list, actually. So, I'm not going to read them all now, but we'll actually get through a surprisingly large number of these problems. These were subheadings, paragraph headings in our survey. So, I'm going to first So, I'm going to first talk about kind of what I think of as the big major trends and developments of the last couple of decades, which, frankly, we mostly didn't anticipate. And these are really good news for how proof complexity has gone on. So, the first is really a convergence of research on algorithmic hardness and hardness of approximation. And hardness of approximation, and all those aspects, and even algorithms for approximation algorithms for hard problems together with proof complexity. And 20 years ago, something that we kind of take for granted now really wasn't there to much of an extent. So the big actors here in the proof complexity round the big subjects are the semi-algebraic proof complexity. Semi-algebraic proof complexity hierarchies. Starting with the Loba-Scriver lift and project system, degree one and two. By now, a number of you probably have only seen more recent things in these hierarchies. But the basic idea there: you started with linear inequalities. You can multiply, if you know something, a linear term is a non-negative, then you can multiply by. Negative, then you can multiply by the literal polynomials. You would now have a degree 2 thing, degree 2 expression. You sum them up, use the facts that xi squared minus xi is 0, and get a new linear inequality. And the rank of these proofs is the maximum depth of these lift and project systems to derive whatever target you had. They're also extended. There are also extensions where, in this step, you could add the square of any linear form, not necessarily one that you've even started with. And then there's ls plus times, which where you can, in addition to these things, you can also multiply two non-negative terms to get a new one. So these, the latter has barely been touched, but these have been the previous two. Touch, but these have been the previous two are the major actors. Then the ones that you probably are all more familiar with, the Shirali-Adams proof hierarchy, which you can kind of, which you can think of as a polynomial lifting system where you add monomials, generalized monomials in variables and their negations, essentially somewhat like PCR, and rank or degree now is the maximum degree. Degree now is the maximum degree term. And it was originally stated actually as an LP lifting method. We actually added an extra variable for each one of these terms and put consistency constraints between them. And it can simulate in the given rank the same thing as LS. And of course, the more familiar sum of squares system, which comes in different names, Perillot used that term. Lasser didn't use the term. Lassaire didn't use the term, everybody else called it after him, and Grigoryev and Vrobjov actually defined positive Stellensats, which includes the sum of square systems. I'm not going to go through these systems more. Noah will talk about them a lot more after my talks. But they're key milestones in the decades, and it just gives a sense of the development. Gives a sense of the development. So, Grigoriev had a couple of really key papers in terms of lower bounds, extending lower bounds from polynomial calculus to positive stellar size, and lower bounds for these Napsack inequalities, which basically says that you can divide by two and round very simple so that it's hard to divide by two and round certain types of equations. But in terms of the connections to But in terms of the connections to optimization, a key paper is this paper by Aurora Bolobos and Lovas, who gave a first integrality gap lower bound for LS and some even more restricted systems. And then in the mid to late 2000s, the development of pseudo-distribution methods for proving lower bounds, which are, in some sense, the natural analogs and extensions of the combinatorial design. Of the combinatorial designs that have been used in Neustalensat's lower bounds. And then Sch√ºdebeck in 2008 independently rediscovered Grigoriev's proof and made connections to the SAT problems and optimization, well, and then subsequent connections to other optimization problems. So this really kind of set things up for Kind of set things up for L'Aser's sum of squares, positive Stalinsach, whatever name you give, as a key player algorithmically. And in fact, the next decade has been a lot about the tremendous power of sum of squares, capturing the best, well, first of all, if the unique games gave in the unique games conjecture, you would actually capture the best algorithms for any approximation algorithms for any. Any approximation algorithms for any NP-complete problem just by a very simple degree two lift. And more directly, the sum of squares hierarchy actually captures the best current algorithms known for the unique games problem. And then surprising results that essentially any gap result we could approve in optimization could actually be captured by sum of squares and the gap goes away for some. Of squares, and the gap goes away for sum of squares. And there's been a connection sort of back to proof complexity with new methods from optimization, pseudo-calibration method of Hopkins et al. So basically, now we know sum of squares is a major algorithmic tool and a method for understanding things like planted problems, average case, principal component analysis, lots more. Another big, and maybe in terms of Terms of direct connections to proof complexity, an even bigger general theme over the course of the last couple of decades has been lifting. And so the basic notion of lifting is you have a Boolean function in n variables, f and a Boolean function g in m variables, or sometimes pairs. Variables, or sometimes pairs of variables, if you're going to talk about communication complexity problems, you have a gadget. And so the lift, which I'll use with this notation, f lifted by g, is f composed with g to the n on disjoint variables. So we can also lift CNF formulas in the same way. Given a KCNF formula f in n variables and a Boolean function, we can lift that formula to a new Boolean formula. To a new Boolean formula by replacing each xi in the formula by g of some new variables, independent variables, and then expanding each original clause into clausal form. And the general notion in lifting, which I haven't expressed here, is the idea is that if F is hard in one model, then F lifted by G is hard in a stronger model. And similarly, with lifting CNF formulas. And kind of the really And kind of the really seminal result in the area of lifting goes back to proof complexity. So if you take an unsat CNF formula, we know the search problem of given an input assignment, find the index of the clause falsified by that assignment, exactly captures the tree resolution depth of the function. So if we take the decision tree complexity of this search problem, Tree complexity of this search problem that exactly captures tree resolution depth. And the original origins involved the index gadget. Given an x and a y, where the y is of length equal to the number of elements of x, the index function just tells you, given y, what bit you want. And Ros and McKenzie showed that for any CNF formula f, the search problem for f lift. Problem for F lifted by the index gadget is precisely the communication complexity of that problem is log m times the decision tree complexity of the search problem. So again, lifting produces a problem hard for a tougher model, and this search problem is associated, can be chosen to be associated to a monitor. To a monitor, there's a natural monotone cartwheer-Wiggerson game associated with that search problem. So you actually get monotone circuit lower bounds, depth lower bounds from this lifting. So this did not get the recognition at the time. I mean, the result was recognized, but I don't think it took a while before the real power of this lifting actually. Actually, became clear. So there was lifting, a simple lifting, though, did make a significant impact in proof complexity. Just lifting by something as simple as the k-bit and function. So for example, pebbling formulas on graphs are very easy, even for tree resolution, but if you put But if you put the AND gadget on just two bits for each of the original variables, you get tree resolution size that's exponential in the pebbling number of the graphs. And these formulas and variants that, for example, use parity gadgets have been used in lots of other places to get space and time-space trade-off lower bounds. Graph ordering principles. So this says if you have an order. Says if you have an ordering given by a graph, where edges of a graph describe some transitive order, then there's a minimal element. If you take that formula, you do an AND substitution with k things, you get formulas that are easy for resk, so the k DNF extension of resolution, but exponentially hard for k minus 1. So, and then the k-fold-sighten formula, which is just the and substitution of the siton formula on suitable expander graphs, they're hard for tree-like semantic semi-algebraic degree k minus 1 or hard for tree-like res Lin. So anyway, these are basic kinds of results of the form that we had been somewhat used to doing before. Of before, they just applied to these simple extensions. But the really big things were taking this Ross-McKenzie work and extending it in various ways directly in communication complexity for these applications, but also to give really strong applications of proof complexity, some of which feed back into the The hardness of approximation, as we'll see. So, in extension complexity, and I'll have more slides about this. The extension complexity results we have, many of them actually follow from results in proof complexity. So, these aren't the first results, but first, fully exponential as opposed to Fully exponential as opposed to sub-exponential lower bounds for LP extension, communication complexity from search problem, and another one is that proof complexity has fed back into circuit complexity with Nostellensat's degree lower bounds giving us lower bounds for monotone span programs and monotone comparator network size lower bounds. So not just monotone. Monotone, simple monotone depth floor balance. So I'm going to talk a little bit about those things. So extension complexity due to Yanukakis. Many problems have natural expression as 0, 1 polytopes, and the input instance is actually given by the objective function. But these polytopes have exponential numbers of facets. And the extension approach is to add extra variables. Approach is to add extra variables and take this more complicated thing here. It has eight faces, up here it has only six, and in general you can get exponential savings, for example, for minimum spanning trees, and then use the polynomial time linear programming algorithm up in the extended space in order to optimize over the ground space. And so LP extension complexity, basically, its number of variables, minimum number of facets. Number of variables, minimum number of facets of any extension. And one really nice result in LP extension complexity is that if D is the Shirali atom's degree required to obtain some approximation, so if you want to approximate any constraint satisfaction problem and you need degree D in Shirali atoms, then you need extension complexity n to the omega of D, no matter how you do it. So that's a So that's based on lifting from proof complexity. So, for example, the LP extension complexity for even getting a half plus epsilon approximation for max cut or 7 eighths plus epsilon approximation for max 3 set is 2 to the, is subexponential at least. So that's ordinary extension complexity for LPs. And there's SDP extension complexity where you start with the formulation. Where you start with a formulation, but you lift to an SDP. I'm not going to draw a picture of how that works, and use the polytime SDP algorithms. And again, how many constraints do you need? And Lee Ragovender and Storer said for any CSP, the SDP extension complexity is polynomial if and only if it has polynomial sum of squares degree. And in fact, if you take, and moreover, if you take a look at TSP or CLEEEC, you get. At TSP or clique, you get sub-exponential lower bounds, but where does this fog follow from? It directly relies on Grigoryev's Knapsack lower bound for sum of squares. So this is a huge, you know, these are major results in the entire field of complexity solving. Problems have been open for decades, solved by combination of proof complexity. Combination of proof complexity and lift tech. The theorem will be a garment. Is it approximate or is it exact? So here these are exact. So I don't think there are approximate results known yet. But I'm not certain. I can't be certain. Certain. I can't can't be certain, but uh there may be yeah. The universality of generality. That dealt with approximation. I don't, so for example, I don't think they can prove things, they can prove the optimality of the max cut SDP, for example, as far as I know. I know. But again, I may be wrong. So it's been a while since. All right. So the proof complexity, the circuit complexity, I'll do one of the connections. So Rouse McKenzie showed that monotone circuit depth, in other words, tree-like monotone size lower bounds, followed from lower bounds for tree-like resolution plus lifting. What happens if we look at non-tree-like monotone circuits? Like monotone circuits. So, this is this restricted class called comparator circuits. The notion comes from sorting networks. So, you take two bits, make one of them the OR, the other the AND. So, this sorts the two bits. But how is this as a model of computation? Because it's got a limited form of fan-out two. And Patassi and Robert, in a couple of papers, show that given an invariable CNF formula C with no chance. Formula C with no chance as refutation degree at least D, you can plug in a gadget and come up with an explicit monotone function. It's associated with the monotone search problem for the lifted version of C. Then you go from this D to the N to the omega of D for that problem. And it's a nice explicit problem. And for example, if the degree is linear, you get truly exponential sides. So these, you know, this is a super strong result from our proof complexity result. From our proof complexity results. So that's, you know, these are just examples. There's lots more to look at in the area. And one of the most amazing things in proof complexity lifting, at least, because it's proof complexity to proof complexity in a way that I certainly didn't expect. So this is Garget all. So if you've got a resolution. So, if you've got a resolution, a formula with resolution with d, then you can plug in with the number of variables to be polynomial m and do the index gadget. You require semantic cutting plane size n to the omega d. And so this is a completely new lower bound method for cutting planes, lots of new problems. It applies monotone for real circuit lower bounds, and they also get other circuit complex so. Other circuit complexity, so this is more circuit complexity, lower bounds from proof complexity. So these are, you know, as far as I'm concerned, of the last decade, these are some of the really most exciting, this whole area is some of the most exciting things. One of the areas that I've been following a lot is in places like workshops like BIRS and Dogstool have been instrumental in bringing together. Instrumental in bringing together is the connections between proof complexity and SAT solving. And this is something in the decade. So, one of the questions we had was better resolution proof search. And we speculated that ideas would come from these degree-width-based methods from the late 90s. Instead, it actually came from the SAT-solving side, something called conflict-directed clause-learning, or CDCL SAT-solvers. And they added some techniques, watching And they added some techniques, watched literals, multiplicative weight update methods to select literals, and clause learning, something called one UIP or asserting clause learning. And these are a couple of the major milestones. Chaff, ZCHAF was the first, and then MiniSat, which added to those something called conflict clause minimization. And these things have completely Completely revolutionized the landscape of proof search. Orders of magnitude improvement and solvable problem size. SAT solvers and proof search are practical tools. You go to any major company doing hardware or software, they are using SAT solvers and relying on proof complexity ideas in part in what they do. We're on the front lines. Lines. So one of the things that we've tried to do is actually figure out what is the right proof complexity model for SAT solvers. And there are various questions about specific, you know, how general can you be? Non-deterministic literal selection? Sure, non-deterministic learning. And then there's something called restarts, which I forgot to mention. That turns out to be equivalent to resolution. There was this idea that you could add extra variables and That you could add extra variables and clauses so that you could effectively p-simulate all of resolution. That is, you take your original formula, do some simple translation on it to add some extra variables, and then you could run a non-deterministic CDCL solver and actually simulate any resolution proof. And in fact, and this is where it's fed back, work by Albert and Work by Albert and others have added some notions of that if you did what's called phase saving, next time you branch on the same variable, you start with the same sign, and the ordinary learning that SAT solvers actually do, which is one UOPI learning, plus non-deterministic restarts, give you resolution. And so this has been, you go to Been, you go to any one of these talks, people really focus on this connection and this notion of phase saving is something that has partially got added to SAT solvers as a result of these works. So the other thing that's come out of this connection is proof complexity for SAT solvers. So this is the SAT solving world coming back for us. SAT solving world coming back for us because the problem was there are annual competitions for SAT solvers, the SAT race. And every year there are SAT solvers that are, you know, we don't care how you find your satisfying assignment. You don't have to even be sure that you've got one. So these are the incomplete solvers. But if you were in this complete solver track, then you were supposed to, the claim is, well, if there is a satisfying assignment, you'll be sure to find it. But they found there were some. It. But they found there was sometimes disagreement between complete SAT solvers submitted on the claims of unsatisfiability. You know, these SAT-solving algorithms got very complicated, did a lot of tricks, and they're potential bugs. So you needed to make all claims checkable. And the general idea is solvers should record a trace of their actions. So, how to do that? What format? Because it turns out. What format? Because it turns out, salvers can do many things outside of strict CDCL on the original formula. They might do something called pre-processing, and then another thing called in-processing. They might do other tricks. And Heula, Hunt, and Vetschler came up with a proof system which has much more in it than resolution called the D-RAT proof system. Called the DRAT proof system. RAT stands for Resolution Asymmetric Tautology. It guarantees that you have formulas that are satisfiability equivalent, but not necessarily logically the same formula. And so if it's satisfiability equivalent and you're proving that it's not satisfiable, you're perfectly fine. And so it actually allows the introduction of variables like an extension rule. Of variables like an extension rule, but it's stronger than resolution even without adding extension rules. And the other thing is this DRAT proof system has produced the largest proofs that anybody has, you know, largest, it's a propositional proofs that anybody has actually ever done, and that actually have solved interesting problems. So there's the Pythagorean triples problem, which I won't get into. Which I won't get into, has a proof of, I think it's 200 gigabytes, is the compressed version of the D-RAT proof, but it has been checked, automatically checked. So this is pretty amazing. As proof complexity theorists, we should really want to understand this D-RAT proof system. And there's been partial work doing this by Heula and others, by Sambus, that I haven't mentioned here. Another thing that came out of thinking about Another thing that came out of thinking about SAT solvers and what they'd have to do, it's not just a matter of how many lines of the proof. Space is actually important in running these things. You can do propagations at an extremely high rate, but you can't write them to memory very quickly. So proof space becomes important. This was first noted probably as much for theoretical reasons as practical. Reasons as practical reasons in this first paper of Aleknovich et al. Sorry for the misspelling. And there are a lot of nice connections. There's been a lot of work over 20 years. And I'm really going to barely touch the surface. Jakob can tell you more since he and his co-authors more than anybody solve many of the problems in the area. But, you know, time-space trade-offs, and even in the super linear. Trade-offs, and even in the superlinear realm, and so on. So, see surveys on that. I think it's been a really nice development. One development that's sort of maybe theoretically related to this better SAT solving, but we are interested in a worst-case problem as well. So, how hard is it to find short-resolution proofs that do exist? So, if there's one out there, is the solver? So, if there's one out there, is the solver going to actually find it? And the best we knew is this sub-exponential bound, which was in my paper with Tony in the 90s with a recursive version, and then Bensasson and Wiedgerson produced a nice width-based, simpler algorithm that makes the analysis pretty obvious. So, in 2001, Aleknevich and In 2001, Aleknevich and Sasha showed that finding short resolution proofs is WP hard. And WP relates to a fixed parameter hierarchy of problems above P. And so it's hard to do this. They showed it's hard to do this unless that hierarchy collapses. And last year, there's this great breakthrough. year there's this great breakthrough of Atsarius and Mueller that in fact finding short resolution proofs is NP hard. This has new ideas in it and it's a very pretty paper that I think will have many other applications. The hard examples are based on consistency statements with a slight tweak from the traditional encoding but if this is You know, in general, sometimes you think, okay, NP statements are, okay, there are many of them. But this is, I think, of a very interesting, different kind of character that I think is really interesting and have lots of ideas. Another theme of the last couple decades is proof complexity of QBF, and there are a lot of different rules. Are a lot of different rules? So, this has been of significant interest in practice, actually. And the question of analyzing the theory of different versions of how you deal with quantifiers and how you deal with elimination, scolon functions, and so on, Q resolution, these are some of the names. I'm not going to say much about them, but they really are of significant interest. They really are of significant interest in practice from a proof complexity point of view. Well, sure, if you've got a formula and you put some quantifiers in front of it and replace some exists with some more complicated alternation, you can get formulas, lower bounds that carry over. But there are also even more reasons for hardness, which is studied in some of these papers. And I think the new algorithmic ideas here are of particular interest. So, okay, so those are some, not all the developments that we didn't think about. So, what about our list? So, here's the list. I'm not going to have you read them. I'll talk about, so what have we done so far? We talked about better resolution proof search. So, we've talked about one of the, and maybe a little bit of another one on the list. So, there's our, of course, favorite pigeonhole principle. Favorite pigeonhole principle. And the, of course, we were, at first we were thinking of, okay, n plus 1 to n, and maybe, well, n squared over log n, that was kind of the best was known. The weak pigeonhole principle is something like 2 to the n. That's a really weak pigeonhole principle. And none of our width techniques that were available. That were available when we wrote the survey, or that we knew about when we wrote the survey, seemed to give any clue on how to beat this n-squared law over log n. But it didn't seem that resolution could even prove this super weak version. And a series of beautiful papers, Ron Raws and a couple papers by Sachar Osborov. Papers by Sasha Rosborov show that the weak pigeon hole principle and the weak functional pigeonhole principle, I think I should have a tilde up there, it's not exactly right, but basically have sub-exponential, require sub-exponential size resolution proofs. And Sasha pointed out that this actually says something about the power of resolution to prove whether NP has polysized circuits, because in particular, Polysize circuits, because in particular, it shows that this means that resolution cannot prove that SAT does not have a polynomial size DNF formula. So because you, so there are two to the end lines in the truth table, if you had a polysize, you would say the number of, essentially the number of ones in that truth table, you can't separate for polynomial versus exponential. Or that's one way of that's my kind of intuitive Cliff Notes version. Cliff Notes version of the statement. So that was one of the open problems that we asked, and that's completely solved. So I've got a nice green color for things that I think of as pretty much completely solved. So odd charge graphs and AC0 fregends. So could you prove lower bounds for odd charge graphs? And we thought that this would require new techniques. But in fact, L.A. Bensa. But in fact, L.A. Ben-Sasson, to get AC0 Frege lower bounds, L.A. Ben-Sassan showed in a clever reduction from the previous lower bounds for the pigeonhole principle that you could actually get sub-exponential lower bounds just by reduction. You didn't need a new technique to get lower bounds for the exciting formulas. And it stood that way for a long time. And this gives very weak lower bounds as a function of the depth. Of the depth. You get lower bounds only up to log, log, n depth. And it was an open question: could you beat log log n depth? That was kind of an implicit thing that we may have actually even talked about that in the survey. I think we did. And Patassi, Rossman, Servettio, and Tan got a lower bound which at first sight doesn't look better, but actually works to much better depth, up to square root log n depth. And they did this by a new general. And they did this by a new general switching lemma approach. So it was really new ideas that we had hoped would come from thinking about this problem. And then in a very nice paper by Johan Hostad, Johan showed that site inform is on even very simple graphs, grid graphs, require depth D A C0 Frege refutations of 2 to the n to the omega 1 over d. Of 2 to the n to the omega 1 over d. This is qualitatively optimal. In a qualitative sense, it matches our best lower bounds for parity, qualitatively, up to that constant in the exponent. And it requires a very carefully tailored switching lemma as opposed to the more general switching lemma approach. It sort of fits a little bit in that, but it requires a fairly sophisticated handling of the specific formula. So, again, another problem on our list. On our list, I think completely knocked off. More general lower bounds for cutting planes proofs? Well, we asked three questions: something beyond interpolation of split formulas, random KC and F, or excitement formulas. So slightly out of, so Rubesh, Pudlock, and Fleming, Pankradov, Patasi, Robert showed that, well, we can't do constant, they couldn't do constant K, but fate. Couldn't do constant K, but theta of log n CNF, random CNF, are exponentially hard for cutting planes. And they both use the monotone, a real circuit version of the Kartrem-Wiggerson game as opposed to the formula version. And this version is expressed nicely in a paper by Khrubesh and Pudlock, which builds on Redzporov's generalization of the ordinary game. Of the ordinary game. And they give a more general version of the interpolation method. I'm going to skip over that in the interest of time. They also show that the bit pigeonhol principle is hard, even though the ordinary pigeonhol principle is easy. And of course, there's the paper I already talked about on lifting from resolution. So this is another area where I think we've actually got the situation under quite a bit of. Situation under quite a bit of control, though I'll mention an open problem later. General lower bounds for polynomial calculus and characteristic 2. Previously, there were strong lower bounds, for example, for random KCNF based on random XOR equations. So you can't refute random KXOR equations. You certainly can't refute the random KCNF. But that didn't work for Keristric 2. So Alektovich and Rezporo. Alektovich and Rezporov showed, in fact, that you could do this. And the ideas are an extension of reduction operator ideas from Razporov's pigeonhol principle, lower bound for polynomial calculus. And it uses a closure operation on sets and binary expanders, which has later found a number of applications. So here, one thing I'd say, although it's green and it solves the immediate I'd say, although it's green and it solves the immediate question we had, the technique is appropriately general, but it is a real bear to apply to other problems. And so even though this is green, I think we have a greater need for a general technique in the area. So we actually asked, this was a non-proof complexity question because people in 2000 were really interested in the proof complexity, and I've left out some things from Some things from while I'm saying this, of work of Uri Faiga on certifying above the threshold, which algorithms, but I'll have to skip. Anyway, the basic thing is the general form of the threshold for random ksat is really tightly known, though this is something that's going to zero with k, but for smallest k, this dominates. So this is really only effective for k sufficiently. For k sufficiently large. But we really know this very well. So this is pretty nice, just something to know. So where are we on our list? We've got all these things in green, which are great. Then we, what about the rest? Well, hard tautology for TC0 and Frege and Frege. We didn't expect lower bounds, but the best lower bounds that anybody had for Frege, for example, had n squared. For example, had n squared symbols, which was like n lines with a formula of length n, and it was absolutely trivial. So, we were really interested, first of all, hard candidates, like something that you would guess is hard and have reasonable belief, and also maybe could you prove something better than these most trivial lower bounds in both of these cases. So, the candidate we mentioned were candidates were random KCNF and these matrix identities. Matrix identities, matrix product identities, which say if AB is the identity, then BA is the identity. And this was suggested by Cook and Rakov. And very recently, so there are a number of papers on, I want to mention a recent one, a paper by Prubestian Samarat, that this is actually provable in NC2 Frege, which is nicely previous results. Nicely, previous results were just extended Frege. And so it's still, you know, that's an interesting question. Is it hard for TC0 Frega or ordinary Frege, of course, was NC1 Frega? Another family of candidates, which came out very, very soon after our survey, is this candidate of given a pseudo-random generator whose image that we know is not the whole of the space. Of the space, and you've got some y-naught which is not in the image. If that generator is computable by a small circuit, then we can write a formula using extension variables that expresses the statement that y is in fact in the image of the generator, say as an unsatisfiable statement. And they showed it was hard for resolution in PCR at the time, but no other But no other unconditional lower bounds have been known since. So progress here, but we're nowhere close. Lower bounds for AC0 Frege mod R, we seem no closer than we were 20 years ago. We don't even have general lower bounds for resLin mod Q, which is kind of the simplest next thing. We do have tree-like bounds for Bounds for these models, but we don't have any general lower bounds. Lovashriver size, which is the question we asked about. We asked in the survey, prove lower bounds, size lower bounds for LS proofs of the parity principle, which says that K2N plus 1 does not have a perfect matching. Simple proof in cutting planes to do this. So if we could do it, we'd separate LP and cutting planes. So far, nothing. Only we've got a rank lower bound. Uh only we've got a rank lower bounds for static proof system or tree-like lower bounds, so tree-like bounds really don't cut it as general size lower bounds. Polynomial calculus is dynamic, moreover. And actually, one of the really nice results: sum of squares can simulate it over the reals, even though sum of squares is static. But what happens if we do a semi-algebraic analog, which would be a positive? Algebraic analog, which would be a positive semi-positive Stellensatt's calculus, which would be a dynamic system. We have nothing. LS is kind of the simplest such system, and we're nowhere. So here are our proof systems, and I put red for proof systems where we have really good techniques. Actually, cutting planes, maybe we could invert to red, but these are areas where we're somewhat limited. And here, where we have none at all, these are over the real. These are over the reels, so I have a dash thing if it's doing it mod R. So, for pigeonhole principle, we got almost everything except this AC0 Frege. We've got a full separation. What about random formulas? Well, we're kind of stuck here. It's not known to be easy in any proof system. And, you know, there's slight issues at the edge. It's not perfect, but we really. Not perfect, but we really, this is still an open problem where we've got lots of work to do. So I'm putting these, we've done some partial progress on these things. Big red when we haven't done anything. What about probabilistically checkable algebraic proofs? There, there's actually been quite interesting development. Groscho and Patassi developed the ideal proof system, and the method is very simple. The method is very simple. You've got your, given your unsatisfiable system of polynomial equations f, a proof is just an algebraic circuit on n plus m inputs, where m is the number of formulas. So if you plug in 0 for all them, you get 0, but when you plug in these fi's, you get 1. And this somehow changed the, it was a different way of looking at algebraic proof systems than previous systems. It's unlike polynomial. It's unlike polynomial calculus or the other algebraic proof systems that had been thought about before. There was no requirement of being in the ideal generated, the things you computed being in the ideal generated, and there are really nice connections to polynomial identity testing. So here, IPS lower bounds that are super polynomial, or here's a shocker, even on the number of polynomial calculus lines implies. Lines imply that VP, the algebraic P, is not equal to VNP. And here's another very important implication. So if there are axioms that you can define for depth D polynomial identity testing, if they have polysize AC0 mod P Frege proofs, then we get actual separation for the mod P version of VNP from Of VNP from depth D algebraic circuits. So either we get lower bounds for Frege proofs or we have a separation. So these are some really nice consequences and there's lots more to do. There's a really nice whole range of subclasses that Forge et al. developed and there are conditional lower bounds and nice connections with the whole area of circuit complexity. And some lower bounds for these simpler systems are These simpler systems are only known based on single-line hard functions to compute, such as subset sum, and are not known for clausal formulas. So even proving some of these for clausal formulas would be interesting. By the way, these single-eyed hard things such as subset sum, Part and Samaret recently showed that for the Resly and R version, the general one, that The general one, that single-line formula, which is sometimes called the bit predicate, bit something formulas, are hard to refute. So this, again, not a causal form, doesn't work for mod 2 or any of that, but that's pretty interesting progress. Some more directions. Cutting plane solvers. So we've had two decades of We've had two decades of really good resolution-based solvers. We know that there are things about inequalities. We have LP algorithms that are, you know, algorithms that are solving LPs for us that are practically interesting and some of them that even solve integer programs for us in practice. Can we actually get the full value, full extra value of cutting planes over resolution in practice? Planes over resolution in practice. Or another way of thinking about it is: is there a nice CP analog of CDCL? So we do have CDCL added to systems, but it's resolution CDCL plus a little bit added to cutting plane systems. And there are many different practical attempts and practical systems for cutting plane style proofs. They generally go by the name pseudo-built. They generally go by the name pseudoboolean solvers or PV solvers in practice. They go back a couple decades or more. The recent ones, Sat4J and RoundingSat, are probably the current leaders. Here are some other ones from before. The key use case for these is concise linear representation of input constraints. So rather than, for example, trying to express the pigeonhole principle. Trying to express the Pigeonal principle in clauses, write the Pigeonal principle as a sum constraint. And that turns out to be really important. It's a big savings, but it's also really important because if you plug in ordinary just translation of clauses, all they do is very slowly simulate ordinary CDCL solvers, ordinary resolution-based solvers when given those translations. When given those translations. So you get no benefit at all on clausal inputs, essentially. So the other thing is almost all pseudoboolean solvers do not implement the full CP rules and are provably weaker as a result. This is a nice Itchkai paper from last year, and there's other work by this set of authors on the question. So these would actually be real. These would actually be really useful in practice to have pseudoboolean solvers and proof search methods that would do something for us, both us in the proof complexity world, because they're interesting problems, and people who are practically interested in verification. And I'm particularly interested in some specific problems using these solvers. Specific problems using these solvers. Another problem, so I'm going to quickly go through a few problems at the end. So, three-party DAG protocol lifting. So, this lower bound for cutting planes proofs by Gargetall used two-party DAG protocol lifting for CP size lower bounds. Doing that in general for three parties would be a breakthrough and would give us these degree two semi-algebraic proof size lower bounds. To semi-algebraic proof size lower bounds potentially. So I think this is a pretty exciting direction to try to work on. There are a number of hard issues there. Lower bounds for cutting planes extensions. So rather than reslin, which is resolution over equalities, what about resolution over inequalities? There are, I mean, again, this may be a harder problem, but May be a harder problem, but so for example, there's RCP, which is resolution over CP. There's a stabbing planes proof system, which is that we introduced that is related to RCP. And again, there might be sort of proof search. I put lower bounds, but there's also proof search for these systems. Again, proof complexity of SAT solvers, a little bit more. Of SAT solvers, a little bit more. How powerful is CDCL without restarts? We said the non-deterministic versions, if you're non-deterministic at everything, in literal selection and what you learn, you can clearly simulate regular resolution. What about the rest? So, Bonet Bus Johansson defined a system. This is with something lemmas. I forget what RT stands for. It's a regular resolution. It's a regular resolution with some lemmas, which models this very non-deterministic version of CDCL, but in the strictures of how one would do it in practice. Then they showed that all known candidates that we have to separate general resolution from regular resolution actually have short proofs in this system. So we are at a loss for candidates for a At a loss for candidates for a separation here. Maybe it's equal, unclear. That would be interesting. And of course, how powerful are D-RAC proofs without new variables? Once you put in new variables, extended Frege, they're clearly extended Freg, but it's really interesting without new variables. Some candidates for lower bounds, sighting formulas and cutting plans. This is one we asked, still open. Still open. A non-k-colorability of random graphs in polynomial calculus for appropriate choice of p. This is something that's easy in resolution. We don't have polynomial calculus lower bounds. Again, random KC and F formulas in bounded depth Frege, the parity principle. Oh, Grigoryev's knapsack, which is not a causal form, only applies. It applies when you take this and then you can divide by two and round. Can divide by two and round down, but only applies when m is near the middle. It's about half the number of variables. And there's no proof outside that realm. And that actually might be relevant to this parity principle collection, pigeonhole principle or anything else in ResLin, many other directions, conditional strong lower bounds, cone-proof systems, consistency state. Proof systems, consistency statements, et cetera. So much more to do. Question from Paul. Yeah, you you mentioned the logo in the cash for the logo. They require more than log n over log log n. So Hasta's result, no, the original one was log log n depth, which was the best we knew for anything. But now we have log n over log log n, and it's as good as we know anything in HC0. So the question is, this status for the most vision for status? That's a good question. I haven't thought about whether, because the graphs are different. Yeah, pigeonal principle, can we get log n over log log n depth? Yeah, that would be quite interesting as well. Any other question while I'm the one set up? Yeah. For the 3x roll, is there a ratio between the clauses and the number of variables where it's supposed to be an algorithm? So 3x. So 3xOR, I think you just have to get a little bit above linear where you become unsat. So I don't think the ratio is, I think that's XORsat. It's usually, because it's just a linear Gaussian elimination. You see if there's no upper bound. There is upper bond, I think, if the Tc0, if the ratio is. the the ratio is is is is not is super linear. Oh, for for three XOR, yeah, oh okay. I was talking about random KC and F at the higher, but the XOR case, you're right, you're right. So you slightly levelist, that it is linear, then you have uh upper bounds for some posts. That would be interesting. I I haven't Be interesting. I haven't yet thought enough. We should talk. Okay, so let's thank Paul Gen for a wonderful