The topic is first, I want to motivate why does one look at other models than Turing machines. And the idea is that Turing machines have a quite small, primitive step and difficult to read programs. And the alternative is one does direct operations on numbers, which can be arbitrary large integers. arbitrary large integers and registers store those and they can do primitive operations in O of one. So counter machines, the primitive operations are just plus one or so called increment, minus one, decrement and comparison with zero. Turing machine equivalent to tour there Turing equivalent to Turing machine but addition of n digit number But addition of n-digit numbers takes two to the order of two to the n steps. Therefore, a counter-machine is not a good model to model polynomial time in another setting. Although from the recursion theory viewpoint, it is the same. Do you hear me now better? Okay. So Hartumanis and Simon showed that machines which have addition, multiplication, subtraction, comparison and bitwise operation can all solve all NP problems in polynomially many time steps or polynomial time. However, if you remove the multiplication, then Multiplication, then it is the same as traditional polynomial time. And all the primitive operations in the model of Floyd and Knus are plus, minus, greater than, equal, and smaller than. So you can do these three comparisons. And examples are expressions like x is y plus three or that. y plus 3 or that is x v plus w or if x less than 55 then go to line 12. And strictly speaking, Freud and Knuss didn't allow constants. So what they wanted is that you make an additional register and this holds the constant one and then you express everything with this. And how And however, besides allowing constants, we did not change the model. The constants made the life a bit easier, but the price is that sometimes the number of registers needed differs by one. Floyd and Schnooze showed for their model without constant operands that greatest common divisor can be compared. common divisa can be computed in linear time that means time measured in the number n of the digits uh is Is measured again in the number of digits or bits of the input numbers. They asked six open problems. One and six are not relevant for this talk. Problem one was about the performance of one function there. They had determined the order, but didn't get out the exact. The exact constant factor of it. And problem six was doing with matrices. And there it is much sensitive to the question how the input is read. So do you read the metrics by lines or by columns, or can you repeatedly read an input and things like this? But for the others, But for the others, it is easier. So this problem two, can an integer addition machine with only five registers compute x square in O of log x operations? So that means the linear time. Can it compute the quotient in also in linear time? Where here the time is linear. The time is linear time of the output, not linear time of the input. And the number of five registers is important. With six registers, Freud and Knuth are able to do it. Then, three, can an integer addition machine compute x to the y mod z in less than, so in O of small? So in O of small of log y, log z operations, given that x and y such that x and y are less than that, then can an integer addition machine sort an arbitrary sequence of positive integers in small o of m plus log q one times and so on qm log m steps and the last And the last, can the powers of 2 in binary representation of a number x be computed and output by an integer addition machine in small O of log x square steps. So they had a quadratic algorithm and asked whether a faster one exists. For example, if x is equal 13, one should output the number. One should output the numbers 8, 4 and 1, because 8 plus 4 plus 1 is 13. But they didn't require any order, so the order you can do in the way it goes best. So linear time multiplication can be done with four registers, five if no constants are allowed. If no constants are allowed, the second result is the listings of the powers in the binary representation of the input can be computed and output in linear time. There are various versions on how to define big O and little O and so on in several variables for the Wikipedia version. So that is Wiki. Wikipedia version, so that is which on the Wikipedia show page. It is shown that open problem three and four have a negative answer, so it cannot be improved. However, for other versions, no result is obtained. And the following, the talk presents the methods of the proof, how to append the proof and programs. And I will I will try to highlight the algorithms, but I will, of course, have to take into account the time I have. So this is a sample program using the method of Floyd and Knuss for computing the remainder. And for this, recall that Fibonacci numbers are 0, 1, 1, and that always. One one, and that always the next one is a sum of the previous before. So one read x and y, and then one does the following. Y with one less y less than x is a constraint. Otherwise, you reject the input. I respared this question and testing thing. If that is why, so we'll let that to be y. Y, so we'll let that to be y, sorry, and now the convention is that z is always the larger and y the smaller one of two neighboring Fibonacci numbers. What is done in three here is that one moves from a pair of Fibonacci numbers to the next larger pair. So that is the sum of the old two numbers, and y is adjusted so that it is neighboring. If that is less than x, then go to 3. If x is greater equal that, then begin x is x minus z and if y is equal z, then go to 7. So what these do is here, this should help to This should help to get the remainder. So, to remove the number Z, which is a multiple of the original Y, which was Z, and subtract this. And then this is a stop when one has gone down with a Fibonacci numbers, what one does in this step, so much that they are again equal. Then one stops and writes everything X, write the result X. Note for this. Note for this step that it is an ingredient after once that has to be greater equal x, that that is always at most double of that, and so one does need to do it only once. Yeah, there is some grass on nuns in the In the I hope you can hear me well. So and so option now I explained some ideas for optimizing register numbers. Register numbers. First is one can separate the program in blocks, and each block has a different value of a constant size register. Constant range variable. So if a variable only takes a value zero and one or minus one and one, one makes two designated program blocks, and one is for the one value, the other value, and one. Value, the other value, and one changes the value of this variable by jumping between the two blocks. Furthermore, a variable holding a power of two can be used to be read out a number on the top. So one makes a power of two or of another consequence big that it sits above the other number, and then one doubles up the other number. one doubles up the other number and reads always the top widget. And a coding bit at the end can be used to signal when a number is completely read out. So this coding bit then allows to see when one is ready and so one doesn't need another register to keep track of it. So here, for example, you want to process the input. You want to process the input 11010. So you power up until you have a larger number, which is here 1 million or 64, as it is binary. Then one doubles up the number, compares it with one zero zero, and since it is larger than one zero zero zero zero zero zero, the bit is one and one subtracts sixty four. Subtract 64. Then one doubles up the other number, compare it again with 64, and the bit is 1 and 1000 gets subtracted. However, after this 1 is now a 0. So this time the number is smaller than 164, and so the bit is 0 and no subtraction. And this one continues until. Until all bits are read out except the coding bit, and then the 64 which we compare is identical with the current value of this number, and the loop reaches the end. And here an example of the same idea to compute the digit sum modulo 2. For time reasons I go not too much into it, but if you, for example, would read the program at the bottom, then you see that this program is shorter and due to the use of a content rate variable, easier to identify. Easier to understand than the program on the top. Now, better. And so the ideas are you use register y to read out a bit of a of x in the top, and then the program goes into a loop. And each time a one is read as a bit. as a bit. a is replaced by 1 minus a and the value of x is corrected by remo subtracting y from x and this one does until once the whole number is read here an example of multiplication this is a school book method so one adds shift adds shifted by the position the numbers one one one times one two three four and gets then the result one three six nine seven four and now the shifted version the computer cannot create on the spot so what the algorithm does is it keeps running sums and the results are always doubled up so first we it up. So first 0 is 111 is added to 0. Then this is this time as we have decimal numbers multiplied with 10 and then 222 is added. Then we get this result multiply with 10. 333 is added and then 444 is added. The same is done in binary, but there we have only two cases. Adding zero, that means doing nothing, or adding the operator which was... Adding the operator, which was year 111. So the coding digit is explained by this example. So you don't take one, two, three, four, zero. You take one, two, three, four, zero, one. So the one is a coding digit at the end. You start with the running sum zero, which is here double. Which is here W, the comparator, or whatever you call it, is in V. The two values are in X and Y, and then we do the same as well. Here we have an additional digit zero, which we didn't have before, and therefore zero comes in the result. And here, the algorithm is given on the bottom of the slide. So, this is the program for the multiplication. The first three lines are the preparation, so determining the size and making all numbers positive. Then the fourth one is optimization. One swaps X and Y so that one has to run through the loop less times if they are of different size. And next is to append the coding bit to X. And then here goes the V is computed, which is the first power of 2 larger than X. And then here. And then here is a part of the algorithm where one reads the bits at the top. And whenever a bit is read and one, one adds y to w and subtracts v from x. And so at the end here, the sign is adjusted. Here the sign is adjusted and the output is given. Okay. Any question on multiplication? If not, I will go on. Any questions? No, then I go on, otherwise, I get too much into time pressure. The division is a bit similar. So one will divide by one, two, three, four. By one, two, three, four in this example, we know more or less what comes out one one, and so the one multiplies with 10 or in binary, one reuses binary with two until the powers more or less until the one, two, three, four, zero, zero is still below this, but not above. And then if here it is later, one subtract is and get one. One subtract is and get one three five seven four for digit one. So one multiplies with ten and get another digit one, and then the two are equal. So one subtracts and gets zero. But note that in some cases there will be a remainder, so the division algorithm has to take note of this. This is a division algorithm with four numbers. It reads Y and Z. They were called like this in the question of Floyd and Knut. And at the end, the result will be in X here one makes One swaps the values and makes so the first one is if z is greater zero then you replace that by minus that and y by minus z and if that is zero you go directly to six for avoiding to divide by zero and if y is greater zero one makes a to be minus one and a to be minus one and y to be minus y so that y becomes positive h else h is one so these are the preparation to take care of sine and other constraints and now if u greater equal y then go to four and then one has furthermore u is u plus u and one u plus u and one goes back to three so this is to make the u plus the u greater equal y so you run through this until u is greater equal y if y is greater equal u then begin y is my y minus u and x is incremented by one to reflect it and the next would be if u is that then go to That then go to five, so you go out of the loop, and then here x doubled up, y doubled up, that double up, and repeat the loop of line four. So this is the main part of the division. And here is only to make if a is minus one, then begin x is minus x. So you must make the result negative. And this. And this is to make sure that minus one divided by minus five is minus one. So you down round and you don't give zero. Therefore, this command, and then one outputs x. So the next idea is to read out the powers of two. What we had done before. What we had done before was the open problem two of Freud and Knuth. This is the open problem five, and there the question was whether it can be done in subquadratic time at all, and this algorithm does it in linear time. So the idea is first to invert the order of bits in the binary number, because you process it only as a number, you don't see the bits in clear writing. In clear writing. And the idea is you read the top bit out and add a register which gets double up for the next of the loop. And therefore, you add the digits so that they go in the inverse order. So while they read from the top order to the below order, they are added up in the new number in the order of one, two, four, six, eight and so on. For six, eight, and so on. So from bottom to top. And then the formal algorithm is this. This is first a preparation. So you make negative numbers positive and then you go on. So y and z are auxiliary variables to. Set our auxiliary variables to be put at one. UN auxiliar variable, a register, sorry, which is initialized at zero. And now one net doubles up y until y is greater x. If y and x are the same, then you go to 4. This is breaking the loop. If x is greater y. If x is greater y, then you have to add that to u, where z is a power of 2, which starts with 1 and goes up after this. And furthermore, one subtract x from y from x in order to make the x smaller. And then x is doubled up so that the next bit can be read out. And that is double up so that the added. So that the added bit goes at the next position. So after this is done, the variable u holds the bits in inverted order. So that is one, one starts with double, let x to be u plus u. And so one does similar again reading out the bits. And again, once one reads them now for One reads them now from the bottom to the top because the order is inverted. So that was in step four reset to one. And here, if there is a bit one, you write that. Then x is x minus y. So you subtract the comparator from x. And then that is doubled up. x is doubled up. And if x is greater zero, you go to five. You go to five, otherwise, the process stops, and this is the algorithm. Regular sets, I think you all know what it is. So, sets recognized by finite automata. And now you ask, for example, if. Now you ask, for example, if the number is binary, can it be read out and processed as a finite automatic simulated? How many registers do you need? And here one needs, if the representation is binary or octal or hexadecimal or other power of two as a basis, one needs two registers. For ternary numbers and other numbers, for ternary numbers and other numbers so when one analyze the ternary reputation one needs three registers this asymmetry comes from one fact which freud and news have fixed they always said when updating a register you can add two registers together to give the new register so you can do x equal x plus x but you cannot do x equal x plus x x equals x plus x plus x. And therefore, you need a third register to hold the intermediate value if you do this. If you wouldn't have the constraint that on the right side there are only two operands, you could do with two registers. So, and this algorithm for witnessing this is a direct simulation of the automaton with respect. Meton with respect with the outreach the digits of the number from high order digits to low order digits. For finite automata, it doesn't matter in which direction you go. The number of states depend on it, but not whether the finite automaton exists. And a function is automatic if Automatic if one of the following equivalent conditions holds. The graph of F is recognized by a finite automaton. That is, it's the automaton read X and Y in parallel with special ultimate supplied at the end of X and Y, respectively. And if both are exhausted and the automaton is in accepting state, then it would mean then it would mean that f of x is defined and equal to y. If they are in a rejecting state, you only know that f of x is either undefined or different from y. There is a deterministic one-tape linear time Turing machine which replaces input x by f of x on the tape with input and output starting at the same position. Same position. So this is model two. And model three, the same as two, but the one-tape linear time tooling machine can be non-deterministic. So all three characterize the automatic functions, in this case, from one input to another. One can also define them for several inputs. There one needs a special convention for input presentation together. Good presentation to get this one tape linear time characterization. And regular sets are sets where the characteristic function is automatic. So the result is every addition machine with only one register, which uses only constantly many in Constantly many inputs. So it doesn't read one input after the other because the automatic functions are only defined if the number of inputs is constant. So if you do this, then every addition machine, independent of the time it needs, computes an automatic function. On the other hand, every automatic function with constantly many inputs can be computed by an addition machine. addition machine with three registers for binary representation or four registers in general in linear time. And if there are several inputs, one more input register is needed because one must first translate the first register in some work encoding, then read the second input and translate it into and adjust the work encoding and so on until one can and so on until one can process the automatic function. And the function computing the membership in a regular set has a register machine with two registers in the case of binary representation and an equivalent representation like octa or x or decimal. For other bases, three registers are used due to Floyd and Knus. prescribing that you cannot do x equal x plus x plus x. So here is a schema for giving the program to check whether the input X is in a regular set given with K are equ Given with K every representation of the number X. So you first create the power of Ky, which is bigger than X. And also you make a coding one at the end of X. These two things are done in line 1 and 2. Using that means that that holds an inter... means that that holds an intermediate value if k is not a power of 2. Otherwise, you can here do log k times y equal y plus y. Then in third, now you read the front digit out of x using y and this operation is abbreviated. is abbreviated but this is only rewriting it is in reality just multiplying with k and doing sub continued subtraction from y until you are smaller than y so this is abbreviated in this statement here yeah and then one make a to be the A to be the delta of A, B. And this is only a jump operation between blocks of the program. And then one goes back to 3 to continue the loop. And at the end, and this is again just a jump operation, depending on the whether for the value of A, either one writes the 1 or writes the 0. So for the open problem three and four, it depends how one defines the big and small O of several variables. So Wikipedia says f is a Failure says f is m from n is in O of G of M and N. If there is a constant C at numbers M0 and N 0, such as M greater N 0 or N greater N 0, K equal N 0, then F of M n less equal G of G M N. And this means if you want to disprove that something is disprove that something is in O, F of M n is in O of G of M n. If you want to disprove it, you can fix one variable suitably and then do the diagonalization. And the same is also for little O. And the alternative also popular version is that you don't have a OR here, but Or here, but you have an end. And since you have this end, the ideas which we used don't work to solve this problem three and four. That means we really need the Wikipedia definition and not the other one, which is at least as popular, B is as popular as A, and what is right is more or less undefined. As undefined, everyone does what he wants. So, and for version A, for example, m plus n is not in small o of m times n. For version B, you have that m plus m is in small o of m is in small o of m times n because you require that m and n go both to infinity and then you will see that m times n divided by m plus n will become bigger and bigger and bigger and therefore it is m plus m is in small o of m n so this is of mn. So this is to explain the difference between the little O of one and two variables, or the versions A and B for little O. So, and Floyd and Knuss asked whether one can compute X to the Y model of that in time small O of n times M, where n is the number of digits of Y and M is Of y and m is the number of digits of x. In the following, 3 is answered for variant A, the Wiki Period version of little O calculus, and this answer does not work for variant B. So let y equals 3 or some bigger constant, and n be the number of digits of y and m the number of digits of x. of x, then for sufficiently large z one cannot compute the function x to x to the y modulo z in time small o of m times n. And what is here the idea? The idea is you first represent it the numbers modulo z naught with zero up to the up to the range from 0 to z minus 1, but you reduce the range for minus z over 2 with plus z over 2. Of course, 0 up to that half over 2 are represented as before, but the other numbers you subtract that once more and represent them as negative numbers. And furthermore, one Furthermore, one assumes that the constant of the little o is below 0.1 divided by y times n. And then one looks how if x is 2 to the m divided by y plus 1, and x to the y is 2 to the m times y divided by y plus 1. Then this is smaller than that divided by two. So thus one would need modulo z. The first non-zero digits of the largest register go from m over y plus one to m times y over y plus one, which requires at least m times y minus one divided by y. y minus 1 divided by y plus 1 additions. And this amount of additions is larger than C times M times N for C less equal 0 dot 1 divided by n times y. And so note that n is constant because we fixed y and therefore and y minus 1 divided by y plus 1 is greater equal. by y plus one is greater equal 0.5. So the algorithm cannot make enough additions and subtractions for producing a result which modulo z equals x to the power y. So this was the idea and we could do it only for the Wikipedia version. In the other version one would require that y also goes to infinity and further To infinity, and furthermore, there is some dependence of the rate y goes to infinity, which depends on the constant c chosen, and therefore one cannot destroy it. So, since it is a recursion theory conference, at the last step, some decidability problem property. problem properties, not problems, so no open problems here. Or one tabling yard time Turing machines, one can decide whether two programs are equal by using that the first order theory of automatic structures is decidable. And for addition machines with one register, the same holds. So you can decide do two programs the same or two addition. them the same or two addition machines but for linear time addition machines functions so computed with uh in linear time with addition machines equality of programs is undecipherable as one can code Hilbert stats problem that is you can take several inputs and if they evaluate to zero in this given multivariate polynomial Multivariate polynomial. The output is zero, otherwise it is one. And then Matjatsevich showed that one cannot test whether this is a constant one function. And for multi-tape linear time Turing machines, the quality is also undecidable because you can code up post-correspondence problem and then well, so one would Well, so one would give one if there's an input index sequence which gives for both sides the same output and zero otherwise, and then one cannot check if it's the constant zero function. So addition machines, as said, have primitive operations, addition, subtraction, comparison, and read and write. comparison and read and write. And primitive operations are all of one time, or just one code them with one step. Thus, their model differs from Turing machines which have much more restricted primitive operations and therefore the timing are not the same, but polynomial time versions of both coincide. Freud and Knuth showed that multiplication and division can be done in linear time with six registers. Time with six-registered and ask whether this result is optimal. And furthermore, they ask whether outputting the powers of two of a number, so that means the positions where the number has a one, but now it has the power of two and not as a bit. So whether this can be done in less than quadratic time. Prime.