Steve? Okay, thank you. And first off, I want to say how sorry I am that I'm not able to be there in person. We were really, really looking forward to meeting you all there in Oaxaca. My wife, Bing, and I just moved back to Seattle and have a few kind of personal things come up that kept us here. But I'm very excited. Kept us here. But I'm very excited to give you a kind of high-level overview of some of the things that we're doing in my lab and with my collaborators at this intersection of machine learning and scientific discovery. And I'm going to talk in general about how to use machine learning for scientific discovery, but most of the examples I'm going to show are going to be in the field of fluid dynamics, which we know is multi-scale, multi-physics. And so a lot of what And so, a lot of what I'm going to talk about will port over to other multi-scale, multi-physics problems. So, before continuing, I want to acknowledge my many excellent co-authors, colleagues, and collaborators on this work. So, pretty much all of the data-driven discovery work is with Nathan Kutz in the upper left. And then, most of the fluid dynamics research is with John Christophe Loiseux from Paris Tech in the upper right. And throughout the talk, I'm going to highlight the work of many. Throughout the talk, I'm going to highlight the work of many excellent postdocs and PhD students who are shown in this middle row. Okay, so before jumping into the technical part, I want to give a little high level. When I say machine learning, what I mean is building models from data using optimization and regression. And I like to oversimplify a little bit because I think this highlights that machine learning is deeply connected to Connected to modeling that we have been doing for the last 70 years at least, since Kalman and the Kalman filter. But realistically, in scientific history, if you go back to Newton and Kepler, Copernicus, Galileo, all the way back to Aristotle, they were building models from their data, in many cases using optimization or regression. And so this is really a natural continuity of what we've been doing in scientific discovery for centuries. Been doing in scientific discovery for centuries. Now, my field of fluid mechanics has a lot of opportunities where we can really leverage these advanced technologies to solve really hard fundamental problems. So the engineering tasks that we care about for multi-scale complex systems like a turbulent fluid flow are things like dimensionality reduction. It might take millions or billions of degrees of freedom to simulate a complex fluid flow on a supercomputer. Fluid flow on a supercomputer. How do we find the dominant patterns in that data that are actionable, that we care about for things like lift and drag and fuel efficiency and things like that? Reduced order modeling. How do I model how those dominant coherent structures and patterns evolve in time? And then those models lead to better sensing, estimation, and control. We don't just want to model these systems. We eventually want to control these systems. And all of those tasks. And all of those tasks in blue end up being hard optimization problems. And the challenges that I'm showing in pink are essentially because the physics itself is non-linear and multi-scale in space and time. And that leads to high-dimensional and non-convex optimization problems. But if you think about it, those are exactly the kinds of problems that machine learning is getting really good at handling. If you think about training a deep neural network with millions of people, Deep neural network with millions of free parameters, it's a very high-dimensional non-convex optimization. But with enough data and enough high-performance computing resources, we are starting to get really good at solving these problems. So I think it's a very exciting era that we live in where we are able to revisit some of the hardest and most central problems that we have in engineering in blue because of these emerging data-driven optimization and modeling techniques for machine learning. And again, I'm going to put And again, I'm going to point out all of the properties that make a fluid hard are characteristic of many complex systems we care about, like neuroscience, also very nonlinear, very multiscale, and we need to sense it sparsely and model and predict and control dominant patterns. Epidemiology, how a disease spreads through a continent, multi-scale in space and time, non-linear process. We need to effectively be able to model, predict, estimate, and control with linear. Estimate and control with limited sensor measurements. Okay, good. And why do we think this is possible? Why do we think that these engineering tasks are possible in blue and why we think machine learning can help is specifically because even in very complex systems, patterns often emerge in the data. So patterns exist even in very complex data, like this is the atmospheric flow of clouds over Guadalupe. Over Guadalupe Island. And you can see that even though this is probably beyond the capability of our largest supercomputers to simulate at all scales, dominant patterns are emerging in this system. Good. And so I think this idea is why we believe machine learning works, and this is also why we believe it's possible to apply these tools in fluids. If you want to learn more about this. If you want to learn more about this big picture general idea, I'm going to make a shameless plug for the second edition of our book, Data-Driven Science and Engineering by myself and Tim Kutz. It just came out on Amazon, but essentially we're talking specifically about how do we pair machine learning tools with these dynamical systems and controls tools. Okay, so I'm going to launch in now to the technical part of the talk about how do we actually About how do we actually build models for scientific discovery with machine learning? And I want to point out that the models we want when we are dealing with scientific discovery is that we want machine learning models that are interpretable and generalizable. If you're going to use machine learning on an aircraft or on an energy system, on the power grid, you need those models to be explainable and generalizable. And instead of defining these, I'm going to give you my favorite example, which is F equals MA. This is the quintessential. A. This is the quintessential interpretable model. It's clearly interpretable because it has three terms: F, M, and A. They have units, and you can discuss these with a friend. You can write them down for a particular problem and communicate and analyze and discuss these equations. Highly interpretable. And they're generalizable in the sense that you can learn F equals MA from an apple falling on Earth. And this law is still true when we land humans on the moon. That is the On the moon. That is the ultimate generalizable model. Okay, now I guarantee you could use deep neural networks and you could train a deep neural network on videos of apples falling. You would probably need a thousand videos of a thousand apples falling and it will eventually learn to make new videos of apples falling that will completely fool all of us humans. But that model will not be useful when we decide to land humans on Mars. Okay, so I need On Mars. Okay, so I need my models to be generalizable beyond where I trained my training data was generated. And this is not just Newton and F equals MA. This has been the principle of parsimony. This has been the gold standard in physics from Aristotle to Einstein for 2000 years. The models that we think are physical, that have some hope of generalizing, are the models that are as simple as possible, but no simpler. As possible, but no simpler. Okay, that's Occam's razor, that's Pareto's 80-20 rule, that's Einstein, that's Newton, that's Aristotle. Okay, that has been the gold standard of what is physics and what is generalizable for 2,000 years. And I think that's very reasonable for us to carry this principle into the modern machine learning era. And mathematically, the tools we have at our disposal to find those simplest models, but no simpler, are low dimensionality and sparsity. So low dimensionality means we So low dimensionality means we are trying to describe our system in terms of as few variables as possible. In that million degree of freedom fluid flow, are there three or four dominant patterns that we care about that we can model and use to predict? So low dimensionality, not getting distracted by high dimensionality. And then sparsity means once I've identified those patterns, how do I find the simplest differential equation with the fewest, the sparsest? With the fewest, the sparsest number of terms that describes how those patterns co-evolve in time. And this is the picture that we use in my group a lot. This is something, you know, Nathan Cooks and I both draw on the board with our students all the time, is this deep autoencoder neural network where we put a dynamical system in the middle. So we might have measurements from a very, very complex multi-scale system, high-dimensional measurements, X. This could be a movie of a fluid flow. A movie of a fluid flow. This could be a high-fidelity simulation with millions or billions of degrees of freedom. And what we're doing, what we're trying to do is through this auto-encoder architecture, is identify the fewest variables Z that accurately reconstruct the data so that the Z variables can also be modeled with a sparse differential equation F of Z. And that gives us kind of a hope of having models that generalize because they are parsimonious. They're the simplest models. Parsimonious. They're the simplest models that describe the data, but no simpler. So I'll come back to this architecture in a minute. Okay, so if we fast forward from Isaac Newton and F equals MA, in the computational world, in the modern era for very multi-scale fluid processes like Rayleigh-Bernard convection, the kinds of models we want that are low-dimensional and sparse look a heck of a lot like the Lorentz-Fedder-Hamilton model from 1963. 1963. So, this model is designed to describe chaotic thermal convection, an extremely complex physical process that drives the chaos in Earth's weather systems. But if you look at this model, it has those two hallmark features we're looking for. It's low-dimensional. X, Y, and Z are three state variables that describe the amplitudes of three spatial modes for this physical system that were proposed by Saltzman. Physical system that were proposed by Saltzman a year earlier. So it's low-dimensional, it's low-dimensional. Our model is in terms of three modal amplitudes, x, y, and z. And it's sparse. The right-hand side of that differential equation, out of all of the possible differential equations that could describe this rich chaotic phenomena, this system is quite simple. It has seven terms and five of them are linear. It's about as simple of a model as you can get. And so, again, this is the kind of model we want machine learning to learn. And that's what I'm going to do. Machine learning to learn, and that's what I'm going to show you: is how we essentially have developed a procedure to automate this Lorentz 1963 modeling procedure. Okay, so I'm going to show you this algorithm we have called the Sparse Identification of Nonlinear Dynamics, or the CINDI algorithm. And I'm actually going to warm you up and show you how this works on data from this Lorentz system. So we're going to pretend that we have measurements x, y, and z, and we can compute their derivatives in time x dot y dot z dot. Time x dot y dot z dot, but I don't have access to the governing equations. I just have the data that came from this system. Now, by far, the simplest model that could describe the system is a linear model, an A matrix, a three by three system where X dot, Y dot, and Z dot are a linear combination of X, Y, and Z. That's what the dynamic mode decomposition would be giving us. But we know that DMD linear models are too simple to capture this rich. Are too simple to capture this rich chaotic phenomenon. So that A matrix has a single fixed point at the origin, whereas our true data has multiple fixed points. It has three fixed points, and it has unstable periodic orbits and a rich chaotic attractor. So we know that this A matrix is not going to be able to do a good job of capturing this dynamics. And so instead, what we do is we augment our right-hand side library to include more candidate terms that could possibly describe the dynamics. Now, instead of just The dynamics. Now, instead of just x, y, and z, we have x squared, x, y, y squared, all the way up to fifth-order polynomials in this case. Now, this library does not have to be polynomial. You could have anything you like. This could be sines and cosines, Bessel's functions, rational functions, you name it. For fluid systems, for continuum mechanic systems, for systems that are convectively nonlinear, generally speaking, polynomial terms actually do a very good job. This is what we expect. A very good job. This is what we expect to get when we do Galerkin projection. So it's reasonable to do polynomial models for lots of systems. And so now we want the fewest terms in this library that add up to x dot, that add up to y dot, and then add up to z dot. I could just do a least squares regression and find the best fit possible in theta, but then I would have a model with 81 terms in the differential equation, and we know that that's going to be overfit. So instead, we want the sparsest combination. So instead, we want the sparsest combination that equals X dot, the sparsest combination that equals Y dot, and the sparsest combination that equals Z dot. And there are commodity algorithms out there that solve this problem now. Cindy can use any of those sparse optimization procedures. And in doing so, we learn those terms that are highlighted in blue and red and green are exactly the terms in the differential equation that generated our observed data. So these sparse systems. So, these sparse systems actually do reflect the underlying structure of the true dynamical system. Now, this is rediscovery. We knew the answer ahead of time, and this is essentially a sanity check that, yes, we are able to relearn things we already know. If we know the answer all ahead of time, Cindy rediscovers the correct answer. That's a very important sanity check. I'll show you in the following slides many examples of systems where we did not know the answer. Systems where we did not know the answer, where this discovers entirely new, new physics and new equations. Okay, if you're interested in this, we have a pretty well-maintained open source library, PyCindi. This is maintained by all of our postdocs and grad students and also a pretty large community worldwide. So if you have data, time series data, and you want to build dynamical systems models that are interpretable and generalizable, you can download this and try it out yourself. Okay, good. Okay, good. So, I want to show you a couple of extensions and then we'll talk about some applications. So, Sam Rudy, when he was a PhD student working with Nathan Kutz and myself, did some super cool work on extending this discovery method to learning partial differential equations. So if you have spatial temporal data, like this flow past a cylinder, again, think about the flow past Guadalupe Island I showed you before. If you have data, what you can do is you can now build that. What you can do is you can now build that library regression problem, but instead of polynomials of modal amplitudes, here we have our field variables and their partial derivatives. And so now we're trying to learn a partial differential equation, possibly nonlinear, that best fits the data and is as simple as possible. And when you apply the sparse regression procedure, you learn, in fact, that the simplest model that describes this data is, in fact, the Navier-Stokes equations that we know govern incompressible. That we know governs incompressible fluid flows. So, this is again rediscovery, but it's very, very promising that even in complex physics like this, this fluid flow, you learn the governing partial differential equation purely from observational data. And since writing this paper in 2017, in the last five years, colleagues from across the world and also inside of our own group have used this procedure for now discovering. For now, discovering entirely new physics that have never been written down before. So, for example, in problems like plasma physics, the magnetohydrodynamics equations are often considered the Navier-Stokes of plasmas, but they're not nearly as accurate of a model. Navier-Stokes is almost a perfect description of many, many incompressible fluid flows. It's a very, very good description. Magnetohydrodynamics is much more of an approximation. It's not perfect. It's not perfect. And so, with this procedure, you can actually add or subtract new mechanisms. You can have terms that are not in the MHD equations, and you can learn kind of new MHD equations that go beyond that fit the data better. So, that's one example. Another example is in the realm of active materials and active matter. So, non-Newtonian fluids like the flow of your blood in your veins, that is kind of like a fluid. It kind of adheres to Navier-Stokes, but there Of adheres to Navier-Stokes, but there are additional physical mechanisms that again are not perfectly modeled yet that you can discover using this procedure. And I think the third area where this is becoming very widely used is in problems like closure modeling for things like the Reynolds average Navier-Stokes equations or Large Eddy simulations. You can learn better closure models with these library regression procedures. I'll point out because spatial temporal data Because spatial temporal data is so vast, we have so much of this data from PDEs, often you can get away with dramatically downsampling your data, only measure in patches. And as long as your patches are big enough to compute derivatives, typically this is good enough to learn these models. Okay, so I want to recap a little bit. So sparse identification of nonlinear dynamics is a machine learning procedure, a framework of trying to build. A framework of trying to build dynamical systems models from data, and our models might be ordinary differential equations or partial differential equations. So, for example, in the case of the Lorentz system, we might want to learn a system of ordinary differential equations. In the case of Navier-Stokes, we might want to learn the Navier-Stokes equations. And there's a few options now for how we use these models or how we could go beyond these models. So, I'm going to zoom into this case. So, I'm going to zoom into this case where we're going to be talking about partial differential equations now. So, let's say I've applied the Cindy procedure to active matter or to learn the Navier-Stokes equations or whatever. I have a PDE now that I've learned. What do I do with that information? Okay, so this is, I'm going to switch to another vignette of how we can use machine learning once we have a partial differential equation. So, this is work by Jared Callahan when he was a PhD student in my lab. He just received his PhD. He just received his PhD about two months ago. And what we're going to show you here is how, if you have a partial differential equation, let's say it's a pretty complicated PDE with lots of terms, maybe, you know, it's the full three-dimensional Navier-Stokes equations, which has quite a few terms. How do we find regions of space and time where only a subset of that physics, a subset of the terms of the PDE, are dominant and active and balancing each other. And balancing each other, and all of the other terms are approximately zero. This is the notion of dominant balance, and this is the underpinning of classical asymptotic analysis from the 1950s and 1960s. In fact, most of the applied math departments in the United States grew up out of aerospace departments to solve these dominant balance asymptotic problems for things like the boundary layer that I'm showing you here. So, this boundary layer Boundary layer is a three-dimensional flow. You can download this from the Johns Hopkins Turbulence Database, and you can reproduce these results I'm about to show you yourself. Okay, so the idea is we're going to take a complex PDE like this, and we're going to try to find regions where a subset of the physics are active. Now, to demonstrate this, I'm going to actually start on the Reynolds average Navier-Stokes equations, which is already a dramatic simplification from Already a dramatic simplification from Fulnavier-Stokes, but this is already complicated enough. So we take our turbulent channel boundary layer flow and we do a time average across the mid-plane. So we get a two-dimensional averaged field that is governed by this partial differential equation, this Reynolds average Navier-Stokes equation. So that has six terms. So the first thing we can do is we can recognize that you can actually compute every single term in this PDE everywhere in the space. Everywhere in space. So, this is the spatial field of that term in the PDE. And you can do this for all six terms in the partial differential equation. So, right off the bat, you can see that there are regions where, for example, blue is large positive and red is large negative. You can see that there are regions where the blue and the orange are large and balancing each other, and all of the other terms are approximately zero. They're approximately black. They're approximately black. So, this is already showing that this dominant balance idea holds. Now, what we're going to do is we're going to, like every single point in space here, we can define those six numbers, those six coordinate axes are these six terms in the equation. So I can take all of these data points, all of these spatial points, and I can scatter plot them in a six-dimensional equation space where each axis of the equation space is one term of the partial differential equation. Of the partial differential equation. So that's what I'm showing here. These are two-dimensional slices of a six-dimensional equation space, and every white point corresponds to a spatial location. Now, already you see that this data lives on these subspaces. There are these subspaces where the data lives. That's where our dominant balance physics lives. And so, what Jared did, the first thing is he applied a simple machine learning clustering algorithm. In this case, it's a Gaussian mixture model. Case, it's a Gaussian mixture model to break this up into groups. And then for each cluster, like the red cluster, he tries to find the sparsest subspace, the subspace with as many zero coordinates as possible that describes that data. And so these are the clusterings of all of those groups. So the red data points are mostly explained by these terms of the partial differential equation. The blue terms are mostly described by two other terms, and so on and so forth. And here's where Forth. And here's where it gets really interesting. Because every point in red, and blue, and green originally was a point in space, we can replot these back in the spatial boundary layer. We can color code our boundary layer based on which subset of the physics is active. So the red region is regions of space where those two terms of the physics are dominant and active and balancing. And if you look above it, you transition into the inertial sublayer where Transition into the inertial sublayer where another two terms are in a dominant balance with each other. So, this is pretty amazing. We can do essentially a data-driven clustering of our dynamics based on what dynamics is active and dominant in those regions of space. Okay, good. And at this point, you should be already thinking, for example, if I look at that viscous and inertial sublayer, this transition, they have. The red layer has two terms in the equation. The blue layer has two terms. They share one in common: this Reynolds stress U prime V prime average. So that's a Reynolds stress term that both of these sublayers have. And then they have one term that's different. And so what that means is that if I move in that Y direction from the viscous layer into the inertial layer, there should be a non-dimensional parameter that governs the transition. That governs the transition from that red term into that new blue term. And that's exactly how asymptotic analysis works. I would find that non-dimensional parameter and I would match asymptotics on the boundary of these two sublayers. So this is, again, rediscovery. We knew this. This was 100 years of boundary layer theory that I'm showing you here. But now it's able to, we are able to learn this purely from the measurement data. We don't need to do, you know, 100. To do 100 years of research to learn that there are these different regions of dominant balance physics. You can apply this to other systems. That was the boundary layer. You can apply this to things like super continuum high energy lasers. So, you know, we've applied this to study very, very, very high intensity optics. We've applied this to geophysical flows like the flow past the Gulf of Mexico. So here we find that there are different dominance balance regions in the Gulf of Mexico. Balanced regions in the Gulf of Mexico where different weather patterns would form. So the pink and the orange essentially have different weather phenomena that are governed there. And you can also apply this to neuroscience data or to disease data. So my wife, Bing, who you heard from earlier today, we are working on applying this procedure to model brain waves. So we want to model partial differential equations for the brain and also find regions where different subsets of the terms are. Where different subsets of the terms are active. Maybe there are some parts of your brain that are more dispersive and some parts that are more convective and some that are more diffusive. Maybe when you're sleeping or when you're excited, you have different balances of these fundamental terms of the partial differential equation. Good. So if we think about that boundary layer equation, it also suggests that we should be able to, in an automated way, learn those non-dimensional parameters I told you about that govern the transition. I told you about that govern the transitions between different regions. So, this is brand new work out of the lab by our postdoc, Joe Backerge. Joe is awesome and he's on the job market, so you guys should strongly consider hiring him because he's brilliant. The idea here is to take this classic discovery technique of the Buckingham pie theorem for non-dimensionalization and try to figure out how, with data-driven optimizations, we can automatically learn these. We can automatically learn these non-dimensional pie groups that parametrize our complex systems, that govern these bifurcations and transitions and dominant balance regimes. So this paper is actually quite intense and heavy. There are three methods he proposes to find these non-dimensional numbers. I'm just showing one of those three choices here based on a neural network architecture we call the bucky net. And the idea is on this Is on this toy system, this bead on a hoop, it's a toy dynamical system with five parameters: mass, radius, friction, B, gravity, G, and omega is the rotational frequency. If you take those five dimensional parameters, we know from physics that there are two non-dimensional numbers that determine all of the behavior of the system. And so you can learn these kinds of non-dimensional numbers if you notice in logger. Notice in logarithm, if I took the logarithm of gamma and epsilon, those integer exponents become integer coefficients in a sub. So it's one log r plus two log omega minus log g. That's log gamma. And so if I take my dimensional data, I logarithm it. Now what I'm trying to do again is find the sparsest subspace of that five-dimensional That five-dimensional logarithm space. And those are where my candidate pie groups live. And so essentially, you can combine this CINDI or PDE find algorithm with this bucky net to not only learn those governing equations, but also learn the simplest parameterization in terms of these non-dimensional pie groups. And if you apply this to a more complex system like this boundary layer flow that I was showing you just a couple of slides ago. You just a couple of slides ago, you can actually learn this purple blasius boundary layer scaling, which is a very, very important result for the self-similarity of the boundary layer. The true solution is on the left. Our discovered solution is on the right. And you can see that kind of we're capturing the essence of this Belasius boundary layer scaling using this BuckyNet procedure. So very exciting, brand new results. Again, what we're doing is we're taking landmark results. Landmark results in the history of scientific discovery, tools that we as humans have defined how we do computational discovery, Buckingham Pie theorem, asymptotic analysis and dominant balance, you know, reduced order modeling like the Lorentz system. And we are finding ways of automating these with data and with machine learning techniques so that our models retain that physicality, that interpretability, that generalizability. Good. Okay. Okay, so we've talked about the CINDI procedure, how we build models from data with optimization, ODEs and PDEs. And there is a third choice. So let's say I have that flow past a cylinder. There is a third choice. Instead of modeling a PDE, I can break it into a modal expansion. In fact, this is exactly what Lorentz did based on Saltzman's modal expansion as he took a fluid flow and wrote it down into a modal decomposition, where x, y, and z are the amplitudes of these three. Z are the amplitudes of these three orthogonal modes, and now I can learn an ordinary differential equation again using Cindy. So, that lower right equation, this is the famous Barnd-Noak model from 2003, his generalized mean field model. And that is the kind of model that we can now discover using the CIMBI procedure. So, I'm going to show you how we do that and then give some examples in fluids. Okay, so I'm going to warm up on these two example fluid flows, flow past a cylinder and Fluid flows, flow past a cylinder, and cavity flow. The plot on the right, A delta is the shift mode amplitude. This is a proxy for the drag. It's something we care about as engineers. And this is a quantity that evolves in time. The white curve is the ground truth from a high-fidelity simulation. And the blue and yellow are the kind of state-of-the-art Galerkin projection models where you take the Navier-Stokes and you do an orthogonal projection onto orthogonal modes. Orthogonal projection onto orthogonal nodes. And you'll notice that they are qualitatively okay, but quantitatively, they're not very accurate. They have the wrong rise time, you know, the wrong linear growth constant, they overshoot, and they have steady state error. So they're kind of not quantitatively correct. But if you take that data and instead of doing a Galurkin projection of the governing equations, you do a Cindy regression, you get almost perfect quantitative agreement with. Perfect quantitative agreement with the data. So these models are CINDI models. This is work by John Christoph Lazo. I should point this out. Sorry. This is the first collaboration I had with JC when he was a postdoc at KTH. He emailed me within a couple of weeks of the Cindy paper coming out and said, Hey, Steve, I think we can make this way better by adding constraints to make it energy preserving and by adding higher order nonlinearities for the fluids. And so this has been what And so, this has been one of my favorite collaborations in the last five years. JC is amazing. You guys should definitely meet him and work with him. And you can see by adding those constraints or by adding those qubit nonlinearities, these CINDI models have much, much better quantitative agreement. Now, I'm not just obsessed with quantitative agreement. Obviously, you want your model to be accurate. That's important. But the thing that I love about these models more than how quantitatively accurate they are. Than how quantitatively accurate they are, is that they are very simple. A Galerkin model in the middle panel are quite complicated, they're dense models. These CIMBI models are sparse by construction. And so what that means is that JC can take the model that we learn, write it down on a piece of paper and go to a coffee shop and play around with it and come up with this. This is the actual model that JC wrote down for the flow pasta cylinder. And to date, this is. And to date, this is the simplest and the most accurate model of that system that I'm aware of. So essentially, it shows that this entire behavior of this 100,000 degree of freedom computational system can be described as a spring mass damper with nonlinear damping. The oscillation, the amplitude of that vortex shedding, and the frequency are given by this Van der Pohl oscillator. So that's the power of sparse nonlinear modeling, is not just that it's more accurate. Linear modeling is not just that it's more accurate, but that it's also much more interpretable. Okay, good. Other things that I think are important here, this is follow-on work with JC Loiseau, Barrett Nowak, and myself, where we started extending this to more complicated systems. So this is the fluidic pinball, which is a more complicated flow that can exhibit some chaos and some pseudo-turbulent wake behavior. Turbulent wake behavior. And here, what I think is really interesting is in this case, we pretended that we actually don't have access to the flow field data. We don't have access to that rich spatial temporal data. We only have access to lift and drag measurements on those three cylinders. So this is much more realistic if you think about what Airbus or Boeing or COMAC have access to, our limited measurements of forces and accelerations on wings. And even in that extremely reduced And even in that extremely reduced coordinate system, when you build these CINSE models, you get almost perfect quantitative agreement that you see in the bottom row. So that solves a lot of interesting problems that I think are worth understanding more. Okay, at this point, there are options. I could tell you about different physical examples we've applied this to. I could tell you about more methodological extensions. This is just a, you know, kind of demonstration. Kind of demonstration of all a number of the systems we've applied this to in the last five years. So I've shown you a couple of these. In the lower left-hand corner, we have applied CINDI to model a three-dimensional numerical simulation of a fusion reactor. We get very, very good model agreements for this fusion reactor. In the upper right, we can model chaotic thermal convection, sorry, electroconvective systems. And we also have applied this to fully turbulent new. fully turbulent numerical sorry fully turbulent experiments from wind tunnels with George Riguez's lab at Imperial. Okay, good. So if you have questions about these, I can talk about these in more detail, but I think I'm going to move on and we're going to talk a little bit more about how we get, how we model these systems. Okay, so maybe I'll talk about the turbulent system for a minute. So this turbulent system in the bottom, we know is We know it is very, very multiscale, and it is not well described by a low-dimensional state X. And so oftentimes we try to describe the turbulent dynamics not as x dot equals f of x, a deterministic system, but as a stochastic differential equation where the small scale turbulent fluctuations are the noise forcing. So, this is work by George Callahan, where he George Callahan, where he started developing an extension to CIND for what we call Langevin dynamics or stochastic differential equations. So here on the left, we're essentially modeling the evolution of a distribution of how a coherent set, a set of coherent structures X evolve in time, given that there is microstructure forcing it, given by this correlated noise process sigma X W T. And so this is. And so, this is one plausible strategy for modeling turbulent systems like that turbulent wake. And the mathematical procedure is a little bit complicated, but essentially what we do is we parametrize both the drift and the diffusion dynamics. And we apply a similar CINDI optimization procedure to learn the parameterizations of both the coherent structures in F and the correlated noise forcing in sigma that captures the subscale turbulence. Turbulence. And in doing so, we have been able to get extremely good agreement, even in very, very high Reynolds number fully turbulent wake measurements from George Riguis's lab at Imperial. So if you had asked me three years ago if we can apply Cindy to turbulence, I would have said no, it probably can't be done. But this is a pretty major step forward that allows us to do that. Another key innovation is how do we get good coordinates in the first place? How do we get good coordinates in the first place for very, very high-dimensional fluid flows? So, I've already shown you that, you know, from this flow past a cylinder, we often use POD coordinates, which is an orthogonal basis we compute through the singular value decomposition. And in a sense, the POD basis is a data-driven generalization of the Fourier transform that's tailored to the particular data. And it can be modeled. You can think about You can think about what the POD is doing, it is the solution of a neural network optimization given by this shallow linear autoencoder. So, those POD modes are the columns of the matrix U in the encoder that chokes us down to this low-dimensional latent space Z I talked about earlier. Now, for everyone in the audience, I want to be very clear. You should never use an autoencoder to compute the SVD or to compute the POD. It's a terrible. Or to compute the POD. It's a terrible way to compute this. But by understanding that it is related to neural networks, we can generalize the POD. Instead of giving a linear orthogonal subspace, you can learn a nonlinear manifold coordinate system through a deep autoencoder. So here, now we have many, many layers in the encoder and decoder, and we can have nonlinear activation functions. And in doing so, we learn a better nonlinear coordinate system. Better nonlinear coordinate system where we can hope to express our dynamics. So, I'll point out in that cartoon I showed you earlier, we have this kind of nonlinear autoencoder structure where we're trying to learn coordinates Z, and we want to learn those coordinates specifically so that we can learn sparse dynamical systems Z dot equals F of Z. Again, there is a strong parallel in the history of science. Those models, Z dot equals F of Z, Those are things like F equals MA and the Lorentz equations and E equals MC squared, things like that. But before we could learn those models, we had to learn the right coordinate system first. Before Newton could learn F equals MA, we had to learn about the Copernican solar system. And so here's a picture I like to show that kind of highlights this point. This is a cartoon by Malin Christerson. Cartoon by Malin Christerson that shows what the night sky looks like for humans when we look up. So, this is what we have seen for all of human existence when we look up at the sky at night. You wear the blue dot in the middle, and we see everything revolving around us. These are the planets and the sun. And when we are in an Earth-centric coordinate system, the planets do some crazy things. Sometimes they turn around and they go in the wrong direction. And it's very, very hard to describe. And it's very, very hard to describe a simple physical law that governs the motion of these planets. In fact, that's why we named our planets after our gods and our gods after our planets, because they seem to have their own free agency. And that's the parsimonious explanation is they're gods and they do what they like. But what's a much simpler explanation is changing the coordinate system. Once you change the coordinate system, so the sun is the center of the solar system, now we can learn the physics, the simple. We can learn the physics, the simple law at the heart of everything because the data is in the right coordinate system. So that's the goal. I showed you how to learn the physics if you're in the right coordinate system. Now we need to learn the right coordinate system so we can learn the right physics. That's this picture here. And so that's exactly what Kathleen Champion did in her PhD thesis when she was working with Nathan Kutz and myself. Kathleen did some really, really brilliant work where she essentially combined this auto and Essentially, combined this auto-encoder coordinate system discovery technique, a deep neural network to learn coordinates Z. She combined that with our CINDI infrastructure to learn sparse dynamical systems in that latent space Z. And I'll give you kind of the summary is that you can't do this sequentially like you could before. Before we did POD and then Cindy and everything was fine. Here, you have to learn them together because it's such a Together, because it's such a complex joint optimization, you actually have to add loss functions to your autoencoder to jointly learn the coordinates so that they admit a sparse dynamical system. And so that's been some really interesting work, and it opens up these much more complicated systems. Okay, so I'm getting towards the end of my talk, and I think, yeah, maybe what I'll do is I'll just give a couple of more slides and then I'll stop for questions. So you can also add even more stringent. Add even more stringent constraints that instead of our dynamics being sparse in the middle, we require them to be linear. This is the thesis work of Bethany, I'm sorry, the work when Bethany Lush was a postdoc with us, with Nathan Kutz and myself. And this is deeply related to Kootman operator theory. So many of you have heard of Kootman theory, where you try to find a coordinate system that linearizes your dynamics. And essentially, these autoencoders give you a strategy for finding those coordinate transforms. Finding those coordinate transforms. If you want to know more about this, we just wrote like a 110-page review paper in the SIAM review all about this that tells you how kind of to reconcile this classical Koopman theory with modern neural networks. Okay, the last thing I want to show you, just because it's so cool, is, you know, all of these cases, we've had full measurements. We've had the full measurements of the system, and we've been able to find good coordinate systems where we can represent. Find good coordinate systems where we can represent the dynamics. But what if I only have incomplete or partial measurements of my system? So that is a really interesting thread, and that's actually much more realistic for things like brain science, is you only have limited measurements. So in our original Cindy paper, we got a wonderful referee comment, which is, okay, this works great, but what if you don't have full measurements X, Y, Z? What if you just have measurements X and X dot? What do you do then? What do you do then? So, we started playing around with this, and we realized that if we time-delay embed our data into a Henkel matrix, people have known about this for 40 years: that time-delay embeddings is how we augment our data. When we, if we take this delay embedding matrix and we take its singular value decomposition, these V coordinates, these right singular vectors, give us an embedding of the data that is topologically conjugate or diffeomorphic to the original attractor. To the original attractor, which means we might have some hope of modeling that system and the v coordinates, even though I only measured the x-coordinate. So the question is: can we learn dynamics in this new v-coordinate system that we obtain through time delay embedding? And for about the last four decades, this has been an open question since Tawkin's theorem to now. And we're just now starting to get real firm answers to this using deep neural networks. So, again, this is work by Joe Backergee. So, again, this is work by Joe Backergee, the postdoc I told you about earlier. Again, he is brilliant, and you should consider hiring him as a faculty in your department because he really is fantastic. The idea is we have this delay coordinate system. Can we learn that diffeomorphism with a neural network to a coordinate system that emits a sparse nonlinear model? Yeah, so we're trying to learn the diffeomorphism in blue and the sparse model in blue F. Model in blue F that gives us models. So we can pose this as a CINDI autoencoder, essentially building on the work of Kathleen Champion, but where now instead of full state measurements, we have these time delay measurements coming into the system. So we have our unknown Lorentz system, and this is actually the output of his system is through Joe's optimization, we learn this discovered system in yellow. What blew me away, I did not think this was possible. I did not think this was possible. The discovered system is actually sparser than the original Lorentz system. Lorentz has seven terms. The new model has six terms. It is actually sparser. There is a simpler model than Lorentz that describes this data, and it has the same symmetries and attractor structure. Now, you'll notice that it's more nonlinear. I think four of those six terms are nonlinear, whereas in Lorentz, only two of them were. So there's that trade-off. But for me, this was very, very interesting. Was very, very interesting. And I'll show you the optimization. So, this is the actual result of the optimization of the learner trying to find the coordinate system so that a sparse model pops out. And you can see it's kind of figuring out the symmetry, it's finding the orientation. And once everything clicks into place, it'll actually get and converge onto a very, very sparse nonlinear model. So, you can see this happening now. It's converging onto the super duper sparse model that I showed you before. That I showed you before. So it's kind of amazing, and we're still wrapping our heads around what the implications are, but this is starting to resolve about a four-decade open problem in dynamical systems. You can also apply this to data like video data of a Lorentz water wheel at MIT and learn similar dynamics like this. Okay, so I think it's time for me to summarize and stop. We're learning dynamics from data using essentially sparse optimization. Essentially, sparse optimization and regression. And we want these models to be interpretable and generalizable. So, if you have data from a system that's either an ordinary or partial differential equation and you believe that it does have some rules, then this is a set of procedures to get really simple models from that data. Okay, so I'll stop here and I'd be happy to ask questions. Thank you all so much. Thank you, Steve. You can hear it. So, all right, so the floor is now open for the questions. Folks on the Zoom, you can either unmute yourself or type your questions in the chat box. So, let's start from the folks that are present in the room. Questions? Comments? Oh, right here. Hold on. Hi, Steve. Real nice talk. This is Charles Menevo. So about your last topic, if I am. The last topic. If I understood correctly, when you only have one of the coordinates, then you do the extension with a time delay, and then you embed it. And without any information on the other coordinate, you find the simplest possible. And you say that's as close as possible to the original one. I was wondering, sometimes it might happen that the original, the true one, is in fact more complicated. Is in fact more complicated than what you find. In that case, I suppose you can't really claim, I guess, that you found the original one. You find the simplest one. That's a great question. Thank you. Yeah, so that's an excellent point. And I think that deserves a little clarification. That's really good. So we are not actually claiming that we are learning the original system because, to some extent, without knowing what Y and Z are. Knowing what y and z are, like it's kind of impossible. That just is not, that information is not there. So, so this actually started as a hypothesis. Is the simplest model in V coordinates? Does that end up being the original Lorentz system? Like, is that the simplest model that pops out of this? That was the hypothesis. The answer appears to be no, actually. So, at the end of this optimization, like the simplest model we get, it looks a lot like Lorentz, but it is fundamentally different. It's a different model with different structure. A different model with different structure. And so that was fascinating. So that was kind of like, you know, I don't do so much hypothesis-driven research. So it's always nice when you learn something that you weren't expecting. And in some sense, this actually is simpler than Loretta. So kind of to your point, there are cases where you might actually learn something better than the original model. I'm not saying this is better. I'm just saying it's different. But what's super fascinating to me, and if you go back. And if you go back to this historical Taukins era of kind of late 70s, early 80s, Ruel and Tawkins, all of their work was really driven by trying to understand, can we take low-dimensional measurements of a turbulent system, which we know evolved on a chaotic attractor, and can we use time delay embedding to get something diffeomorphic that kind of captures that essential structure of that turbulent attractor, but without measuring the full flow fields. Uh, the full flow fields, and so you know, this is very speculative. We have not done any research extending this to turbulent fluid dynamics, but I think it's extremely interesting and exciting to start now thinking about, let's say I have an airfoil that's at a near stall angle of attack. Can I use a pressure measurement or a series of pressure measurements on that airfoil to start predicting stall and getting? You know, to start predicting stall and getting models for that phenomena, like using this kind of procedure. So, yeah, great question. Thanks, Charles. One more follow-up. We have a bit of time, it seems. So, a follow-up question. So, as you mentioned, the original objective, I think some of the applications of Ruel Tachens at the time were to, for example, calculate the fractal dimension of attractors, which would be invariant to the sort of these smooth transformations from one set of coordinates to another. So, I was wondering whether. coordinates to another so i was wondering whether have you have you tested that on this system does this does this attractor have the same dimension as the original lawrence one that's such a good question um so i actually just before this uh talk had a meeting with uh with with joe the postdoc who worked on this and we we have like a list of the things we want to check that's on the list we have not checked that um yeah so so there's a number of things uh the fractal dimension making sure that we have the same uh let's say chaotic structure Let's say chaotic structure. You know, that definitely requires more investigation, but preliminary results are promising that at least, like, we want to find, does this have the same type of unstable periodic orbits and do they map, you know, back and forth through this transformation? If you look at these equations and the z coordinates, I mean, I don't know, you could pretend at least that the two, you know, two of them are eight and two of them are 16. Them are 16. That could easily be one parameter, and the 2.5 and 1.4 could be two other parameters. So you could at least plausibly think that maybe the system has three parameters, just like the original Lorentz system had rho, beta, sigma. And so that's another thing we're investigating is can you do numerical continuation of those parameters and see similar bifurcations as Lorentz? So, yeah, I mean, those are all things we're investigating, but it's early days. We haven't done that yet. It's early days. We haven't done that yet. Great. I suppose Lyapunov Exponus would be the obvious other one. Yes. And actually, I think I believe we definitely received that feedback before, and that's on our list. It's great, great feedback. Yeah. Right. Thank you. Thank you. Okay. Let me take the opportunity to ask a question too. So I like that slide where you are able to. Slide where you are able to use the Gaussian mixture model to kind of identify the dominant balance. And it reminds a very nice way when we used to do this two-quatient turbulence model and invoke these assumptions like production equal dissipation and then approaching the near-wall asymptotics and all. Actually, this explainability makes much more sense when. Makes much more sense when, like a lot of people are talking about these machine-driven, machine-learning-driven turbulence closures. Maybe this approach should be put over there on top to make that interpretable. Because in some cases, you know, you're essentially generalizing the two equation added viscosity models, and maybe you can identify which are the terms that are really controlling and how the coefficients should go. Have you extended that to another canonical problems, or is this the only one that you? Mechanical problems, or this is the only one that you have shown this with dominant balance? I'm really interested in this. That's such a good question. And I think you've hit the nail on the head. Like, that's exactly one of the directions we want to move in is incorporating this into kind of that closure modeling world. And there's a number of ways you could imagine doing this. You could imagine, you know, so we have applied it to some other flow fields, this geophysical flow in the Gulf of Mexico. We've applied it to rotating detonation engines and combustion systems. Engines and combustion systems. I haven't shown that here. But you could totally imagine that, you know, in those pink regions versus those orange regions, there might be different closure models as well, in addition to the different dominant balance. Other things we're doing, I've been working. So I was on sabbatical this last year at Caltech. I just got back like three weeks ago. And I've been talking a lot with Jane Bay, who is a turbulence researcher. And Jane has And Jane has some super cool ideas of how to extend this dominant balance, where we kind of already knew the answer, to problems where we don't know the answer. So, her problems of interest are multi-scale, I'm sorry, stratified turbulence, like multi-phase, multi-phase flows, and also compressible flows with an adverse pressure gradient. So, those I think are two really, really exciting areas where we don't have as good of an idea of what. We don't have as good of an idea of what the dominant balance or non-dimensional numbers are or how to do closures there. So, yeah, that's not my expertise, but I think Jane is doing some really exciting work on this. This is fantastic. Thank you. Thank you. All right. So, we still have time for maybe one more question, but in the interest of time, if you wanted. You know, in the interest of time, if you want to take a break, thank you, Steve. And hopefully, in the next workshop, we can have you in person. I would love to be in person. Thank you so much. I'm sorry I couldn't be there. Yeah, thank you all. Yeah, no problem. I really enjoyed your talk. Take care. Bye. So we are heading into a short coffee break and then I'll close up today's session with my talk after the coffee break. So please do take a short coffee break and we'll have our last talk. Our last talk of the day at 4:30. Take care. Bye. 