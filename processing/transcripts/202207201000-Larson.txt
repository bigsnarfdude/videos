Yeah, thanks. This is fun. This is an outstanding conference. I'm very thankful. I'm especially thankful for collaborators who've taught me a lot. I wouldn't be here without them. We won't be too sappy, I guess. I'm interested in non-smooth composite optimization problems. So here, this one slide will be the slide. If you have other things to do, you don't need to see past the slide. But ODM is a function that is a composition of two things, meaning this kind of composition. Sometimes you'll see composite optimization being. Sometimes you'll see composite optimization being the sum of things. But here I have some outer function h of f of x, cap f of x. f is assumed to be smooth, but expensive to evaluate. Some simulation, black box. I should have made a black box. And I'm going to say H here is non-smooth. You can do lots of fun things as smooth as well. I'll give a digression to sort of point sorts like Gabrielle was talking about, where maybe you have the product of things or quotients of things. H could be something extra fun and convoluted that is some known mapping. That is some known mapping of this simulation output of this expensive F. Great. So here's the basic idea: I want to build models, p models for each component of F, and I'm going to use those model gradients in the place of I don't know the gradients of F. If they were available, maybe I could use them instead of the model gradients. This is definitely a possibility, accessible interest, but we're going to focus on the case where I don't have those derivatives available. So here's walking you through my Here's sort of walking you through my 22-minute talk. I'm going to talk all about H first. I'm going to talk about what is manifold sampling just at a high level. I'm going to talk about two or three different forms of manifold sampling. And maybe if I get to numerical tests, I get there. Otherwise, I got some questions that I'm interested in people's feedback on, whether here or later. So I'm going to talk about just this H. What sort of H am I talking about on the outside, just to give examples of this. For our analysis, H has to be something called a continuous selection. Called a continuous selection of a finite set. So it's got to be a continuous function, and its definition must be in some defined by some finite set of functions. So this could be, Matt says, all practical use cases, I say many because someone's going to tell me why it's not all, but they can vary in complexity. So maybe you want the component-wise max or min of Z, Z being the argument into H, the maximum component out of a simulation, or the min component out of the simulation, or the max or min. Of the simulation, or the max or min of some functions that map simulation output. I'll show an example of that later. Maybe it's the one norm. Great, you want to minimize the one norm output of a simulation. Using this information can be massively beneficial as opposed to just modeling and working with the composed quantity, the one norm of the simulation output. Let's model a simulation output and use the fact that I know I want a one norm or a two norm. We can do lots of things. Or, hey, here's a funky one. Is it that censored L1 line? Is it the censored L1 loss? And so, great, I want the I want to say the max, I want to maximize the deviation from the data to the simulation output, but I don't want it if it's way off, I don't want it to go too far off. So this is a censored loss. It's a piecewise affine mapping. So great. So what do you need in practice? And I'm going to hop over to some code here, but great. I need sort of, if you give me a point, your script needs to calculate all the values, say the max or the min or something. It needs to say what. Of the man or something, it needs to say what's active at that point, which index or indices are active, and then the gradients of those components. So I can show what that looks like in practice. Oh, and then also, I guess, on the other side, if somebody said, here, give for a given index, calculate the value and also calculate the gradients of those things. So I'm going to hop over to some code. Here's some MATLAB code. I assume everyone is MATLAB literate. And let's just say we wanted to, like, I've got some piecewise minimum squared. minimum squared. This is the h function for a piecewise minimum squared. Great. So it puts in z, h is like the hashes. Sorry, it's the and and great. So here you come in. You're given a vector z. This would be some simulation output. And then I'm going to say the objective h is the min of the z squared. Okay, cool. Then I need to find out what indices are active at that point. And so you could just do some relative accuracy or close enough. It can work really well. Enough. It can work really well in practice to do this. And I can talk a lot about sort of for practical reasons why you do that, but you say, hey, what indices are defining this min squared component value? And then for each of those indices that I have, I'm going to compute the gradients. And the gradients are just, well, two times the component because it's a squared. Great. So this is what you need to return, which great, which sort of component is active and what's its gradient. And then if somebody else came in and said, I also not only do I want to. I also not only do I want to give you, say, a Z, a point in the input of H space, but also some components that I want to figure out. If that were the case, then great, you're going to have to go through each of those components, calculate what the value would be for each of them, just again the sum squared, and then what would the gradient be for them? So this is a very simple example. There are more convoluted examples. For example, the sensored L1 loss is more convoluted. You sort of need to be concerned about which side or which activity is defined. Side or which activity is defining your thing, but you don't have to compute them all beforehand. It just depends on what you give me back. Here, I compute the h value and I can go through sort of how we calculate the gradient in each case. If the component of z is less than the sensor and the absolute is this, great, then you need a gradient that's one. Okay, so it is, we have many examples of this. We're interested in growing these examples. If you have other H's, we can help make them. It's somewhat of a convoluted thing. It's atypical for optimization methods to have to worry about this, but we're happy to help write such things. But ultimately, Such things, but ultimately, what you need to do is to be able to compute gradients of components of H. Yeah, active means, yeah, for a given Z. For the given Z. So say for the max component, if I put in Z, you're going to have to say, oh, component four is what defines the max, or component three is what defines the min. I mean, depending on what your H function is. Now, if it's this, there's many different possibilities. It could be like. sort of many different possibilities it could be like is it the max defining it is it the absolute which side of the absolute are you on and so there's sort of many different manifolds here that define essentially here if i give a point on here some unique way you need to be able to define this part as being uh that manifold so that if i want to query this manifold at a different point even where h isn't defined isn't being defined by that i could do so and so here's a sensor.1 So here's a sensor.1LOS function that has sort of these piecewise affine parts. It's not convex, it doesn't have to be convex. Can we go back and tell us more about the continuous selection? I think that might help a lot. The continuous selection. So if I put in a Z, you have to have some rule that defines what the value of H is. And it must come from some finite set of things. So just max of each of the components would be the number of components different. The number of components, different functions that it could be. So the sort of set of that frac H would be all of the individual components. And then H is the max. So any and C, I can see which of those components are max. And then that would boil down to that set. So active then would be those J that index the H and the frac sets at that particular and also I want to be able And also, I want to be able to then query other components, right? Like, so what is the value of this component at a different index? Whatever functions define here. I need to be able to query that, even if they don't define. But that's easy in the max case, or even in the sensor Del1 Lax case. And the H are going to be differentiable. Ah, yes. So we can write a finite set of smooth functions. So that's why I can do this grad HI. Can do this grad HI and as a result, if you take that on there, ah, you see, I love it, he's like my hype man. No, no, he's like setting me up, perfect, just great. So, I'm going to give more examples of each just because people always like, what? It's always just max. Nobody cares about anything other than max or sum squared or something. Great. We have examples from ice sheet simulations where they do want some max deviation. I'll get you back. Quantum networks. They simulate this massive quantum network and then they want to take the trace of this major. And then they want to take the trace of this matrix that comes out, and they want to have the difference between the trace elements to be small or something. It's a known mapping. Getting that state matrix out of the quantum simulation is a massive computational expense. You could squash it all down to a single scalar, or you could try to use the entire thing because it took you like a day on a supercomputer to an hour on a supercomputer to get a number or to get the density matrix for the quantum scale. Particle accelerators is another example I can talk just a bit more about. Can talk just a bit more about. Consider a particle accelerator that's parameterized by x. X are magnet strengths or their position of magnets at CERN, and they're spinning around and simulating particles going around and around. They smash them together. They consider how big the beam is. And I want a really tight beam usually. Or for the example I'm considering, you get sort of three quantities that come out all along the beam line. There's like the position of the cloud, the momentum. The cloud, the momentum of the cloud, and the correlation of the cloud is what it is in practice, but I didn't give notation for that. And then one way to quantify the beam spread is something called normalized emittance. It's the square root, you can get rid of for optimization purposes, but it's the product of the two outputs minus the third output squared. That's their sort of way they quantify them. And this is how particle accelerator people do it. And then you could imagine, say, I want to search over the beam line to find the location. Location mean over J where this thing is minimized. So this is not piecewise linear, not convex, non-smooth outer edge. And say the point where the beam was the tightest could be where I would put my little sample that would get irradiated by this beam. Cool. Okay. So yeah, here's an example where they use plasma to accelerate wakes through and they have this dry bunch and the trailing bunch and they want the bunch to be tight so it all hits at one time. To be tight, so it all hits at one time. They don't want this big shotgun pattern, they want a tight shotgun pattern. Okay, great. So, that's sort of what is H? All about H, that outer H. Hopefully I've convinced you H is everywhere, which can be smooth. I'm mostly going to focus on non-smooth here. So, yeah, manifold sampling. What are manifold sampling methods? Like I mentioned, they're model-based methods. They're also trust region methods. We build models of the things we don't have, and then we try to combine them in an intelligent way. Don't have to be derivative-free, but definitely. Right, don't have to be derivative-free, but definitely inspired by DFO. All we really need is gradient accurate estimates of each FI. So, I think Aude and Hare textbook would call this order one gradient accuracy or something. But I just need that. I don't need functions to agree. You only need the models to agree with their gradients. Because I assume f is expensive, I want to store everything to use, for example, for model building. You can use it for other things, but f was. It for other things, but f was expensive. I want to store in this set y every point that I evaluated along the way and their corresponding function values. And then I can use these HJ values or the gradients for any possible selection function, even if it's not active. So it's sort of a critical point. And then manifold, how do these methods differ? I've been talking about manifold sampling for years, I guess, but we sort of have a new approach today. But the methods differ in how they consider what indices. Different in how they consider what indices j to consider and how these values of the gradients and the functions are used. So I've got some animations here, but pretty much all of them use sort of this composed quantity. You can imagine, say, the chain rule of h of f of x is the gradient of f times h, great, this, except for this being. So there I get the chain rule, but of course I replace after doing the chain rule, this would happen again with if h was just h in a smooth thing, it would be the same kind of idea that. It would be the same kind of idea that Gabrielle was talking about yesterday. And we're going to collect these in some matrix G, each of these sort of gradients. You could sometimes maybe you could think of them as something like a bundle methods sub-differentials. So how do we consider indices? Previous work, old manifold sampling, would do something like we would consider this set of indices to be what was active where I was at. All that information had to be there. Information had to be there. And I could have up to all of sort of the activities in a ball around me. That was sort of the conditions we needed before. Now we have sort of a new condition or a more general condition where you want to pick the indices that are nearby that are, if they're better, then they need to be within delta squared. And if they're worse, they're lower, then they need to be within a delta. This is something that came out of the analysis. But pretty much in a ball, you're going to look in the neighborhood and see. You're going to look in the neighborhood and see which manifolds have been evaluated or identified. And I'll show an animation here in just a second. Yeah. Oh, my goodness. Wow. Yes. Okay. Yes. Absolutely. Great. So old school manifold sampling, like way back in the days of 2016 or something, we would build a smooth model where you'd say project zero onto the convex hole of. You'd say project zero on the convex hole of those gradient approximations. Okay, and then we get a smooth. Well, you need a gradient to satisfy this, but usually we just built a linear model. So we have a non-smooth H, and we take a smooth, build a smooth function that satisfied this. And you minimize this model. I mean, we just only condition was the gradient had to agree with the projection of the zero, the convex hull. Sorry, Clark, Clark results. So here's an animation of this. Let's say I'm starting here on this sensor dollar mosque. I got my diamond as my starting point, and I've evaluated. Is my starting point, and I've evaluated these points sort of right enough to build models of F. I also am able to see: hey, they're all from the same manifold, so I whenever I project zero out of the convex hole, that point's downhill. Take the negative of that direction. And so I go here. Oh, it's a good point. And I know that's on a manifold that I know. I can go there, accept the step, and maybe I increase my trust region radius. Again, now I only sort of have four points, but they're all on the same manifold. Now, here's where it gets interesting. I go here and my model. I go here and my model again, this is a linear model pointing downhill. Oh, here's a new manifold. Some different thing is defining the sensor del MOS. Something else is the max and the most basic example. I need to have that information, especially its gradient information included in my sort of generators, my bundle when I'm deciding my direction. So then I include this information plus this information, projects there on the connect sole, you go this way. Okay, this is old, sort of inefficient. Great, now I know this. Sort of inefficient. Great. Now I know this manifold. I can keep it. I can go here. The unfortunate thing about this approach is that when you come here, now great, it points here. I have this manifold, which sort of points down, this manifold, which points up, which means project zero, convex hull, down and up makes zero. So I get a flat master model. I mean, there is a stationary point within my trust region radius. So maybe that's a reasonable thing to do. It says I love it. Great. So then I just have to shrink, like I just have to shrink delta. And so, because of this, you can only sort of approach stationary points from one side. As soon as they enter into your trust region radius and you pick up the fact that there's sort of stationary information nearby, you can only approach it from one side in the dual approach. And great, so we can proceed. Here's some more steps of old, old school approach. Again, here, I've got a large delta and I have sort of And I have sort of, you know, project zero on a convex hole, I get zero. Means I have a stationary point nearby, but I can't take a step. The great thing about our latest method, what we're going to call primal manifold sampling, the primal tress region subproblem is defined by the max of sort of the gradients that are nearby. I've got animations of this as well. We also have the option of a Hessian term, second order term, sort of planting that seed. We don't know what to do here. In all of our tests, we just set it to zero, but the analysis does work with the width. Analysis does work with a general symmetric matrix. And so then great. Again, I get sort of the max of each of these. Even if my function isn't the max, my primal trust region subproblem is such. So here's the animation in 1D, and then I'll do it again in 2D. Let's say I had the function that's defined by blue and pink and blue, and it's the max of them. Great, so it's the function itself is this black thing. And I'm sitting here at Y1, and I evaluate Y1 and say, hey. And I evaluate Y1 and say, hey, what do I know? Well, from blue, I know my model would be this. That would be the primal model. That would be the dual model. If you could say, then minimize this over this box, you'd come to Y2. You'd get this point. And you say, oh, I have a new function that is active here. I've never observed this manifold before. Let's evaluate it centered here. So that's why I get this gradient and not this gradient. I evaluate it here and then I take the maximum. Gradient evaluated here, and then I take the maximum. This is what the there's not what the dual, though. The dual is going to say, take this and this project zero onto the convex hull, and I get a zero model, this flat model. And again, so that means if I'm going from here, I will converge to the scale and find this large stationary point, but I can only sort of do it from the right. I never get to hop over in 1D, because as soon as I evaluate this point in the dual method, which was very inefficient, or so with a primal, you take the max of these two linearizations and Of these two linearizations, and you get this, which is a very nice model, and a very close, not perfect model of the function at the kink. You'd go there and then update your model and proceed. Here it is in 2D, where I even show the contours of the manifold trus region cell problem, the primal cell problem underneath there. So, great, I've evaluated these points and I get sort of just this model, which is just the linearization of this manifold. Same thing. Great, so I proceed here. Great, so I proceed here. I come over here. Here's a point. Now, with the primal method, I don't actually have to know this information, it just has to be sufficiently downhill. It's not in this case. So I need to include it in an update, my master model. And I get this thing. Oh, man, look at that. I got contours of the function and contours of the trust region subproblem in the same plot. I didn't give the time to gasp. I'm sorry. Great, cool. Yeah, so great. And then, here, again, I can proceed here. I've got this information. I keep my same master model from before, the primal model. And I can go over here, other information. And again, it used to be with the dual method, you had to have the manifold you were going on be in your bundle. With the latest method, we just put on the archive yesterday. That's no longer necessary. So, as long as you have sufficient decrease. So, as long as you have sufficient decrease, you're good, which I mean, makes sense intuitively. Great. So, then I'm over here. And again, now I've got sort of manifold. You can sort of see which, you can see from the contours of the trustees and so on, which manifolds I know about, right? I got lines going like this. It means I know about this manifold. I got lines going like this. It means I know about this manifold. Okay, well, I never really identified this one, but that's okay. I don't necessarily need to have done so in order to converge here. But now, immediately I've sort of picked up enough here, even from these contours, I don't have. Enough here, even from these contours. I don't have everything, but I got enough and I found my stationary point. Great. So I can tell you why the primal problem is called the primal and why the dual is called the dual, but I'm just going to sort of skip past that quick because that's if you take, I guess I'll say it in a sentence like math. You take this problem, reformulate the Lagrangian dual, strong regional dual of that, you get a stationary measure. Sorry, is that a hater? Is that a hit? With the stationary measure, if you look at your stationary measure, in the absence of bounds, you get the dual. You get the dual of, you get our original dual subproblem, which is the projection on the zero of the convex hull of those gradients, hence why primal is called primal and dual is called. All right, great. Oh, and we get to measure descent with like what you'd expect for descent. True decrease divided by predicted decrease. Man, if you remember back in the day, manifold sampling, you used to have to like find manifolds that over approximated where you're going or underapproximate where you were. Underapproximated where you were. If you don't remember these days, it was terrible. But I was trying to convince people you can find these, they're not that hard because you're working in H space and H is cheap. But it's like this totally ugly thing that you no longer have to do anymore. Great. So I can say, what is the manifold sampling loop? I can walk through the manifold sampling. Whoever was like, pause mid-algorithm, that was beautiful. Great. Pause works mid-algorithm in Beamer. Who knew? Great. Set your constants. Construct your indices that are active. Construct your indices that are active. Make sure you got a gradient-accurate model. Minimize your primal subproblem to get these points and get your approximate stationary measure. If your stationary measure is small, you need to shrink. Otherwise, hey, I can go and evaluate my function, update my set of points, compute my row. If row is good, I can accept and go. If not, then I can sort of see: did I get new information? If I haven't achieved new information, If I haven't achieved any new information by doing this evaluation work, then maybe I just shrink delta and go on. Otherwise, maybe I got new information, I update my models, and maybe I can find a better decrease. Great. Now I get to go to my favorite thing I've ever made mathematically. Maybe it's up there. Goomba. Man, so excited. Okay, even better. This might have been from a discussion with Warren over beers, five years ago or something. But let's see. What CSA was? This is good. Okay, instead of minimizing. Okay, instead of minimizing this master model, this non-smooting, hey, I know H. Let's minimize H of the models. The models are quadratic models of the S. Let's just minimize this. Okay, well, for so great, this gets you the best acronym that's ever existed. Glass box optimization of models of black box in a hypersphere. Goomba. That's beautiful. My grandpa always called me a Goomba so that I could do this. And I've like told my Italian uncles, they're so proud. So proud. They're so proud. So proud. Sensor 01 loss. Great. So now I've got, I want to minimize the absolute value of a constant, constant, quadratic inside of a max. Man, that's nasty. I don't have to find it exactly. But in Goomba, okay, so if my models underneath are great, then I mean, the contours, if the underlying Fs or quadratics, then I mean, you just, you find the answer in one step. You go here. Grant, I minimize that immediately. I know where H is going to be active. Where H is going to be active, H is known. It's that sensor double loss function. I go here immediately. In three steps, I'm there. Now, that's assuming I could solve that nasty subproblem. So, who are you going to call? I don't know, Barron? Baron, like the branch and reduce global optimization method? You're going to get it 10 seconds, 30 seconds, an hour? I don't know. It depends on how much the cost of F is, is what I really argue. And that's a question I don't entirely have the answer to, but I guess a tenth of the cost of F? I mean, it depends on how expensive. The cost of F? I mean, it depends on how expensive your F is. It's how much time do you want to spend approximately solving this? And I stress approximately. All right, so I got to see my Goomba animation again. Great. So here's what Goomba did. Oh, and then, well, okay, so great. You're going to approximately solve your Goomba sub-problem. Same thing, everything as before. You're going to evaluate it, see if it's good. Goomba needs a slightly different row test. But if it fails, then we have recourse to manifold sampling. Recourse to manifold sampling. And so we can have a convergent Goomba that is guaranteed to find a solution. Because, say, you end up, and it does occasionally happen in our numerical tests, that approximately solving the sub-problem doesn't produce any improvement over the starting point that we gave it. I think the starting point we gave it was just zeros or maybe the steepest descent direction. Stefan mentioned the Cauchy direction or something. Some other things could be used to warm start this to try to find a better solution, but these nasty convex things give it 30 seconds to barren. Give it 30 seconds to Baron, it might not improve on the starting point. So, but then we can recourse to an iteration of manifold sampling. And then we can show some numerical tests where everything sort of wins. We use the 53 Mori-Wild FX naught pairs of dimension 2 to 12. Their output dimension is 2 to 65. These f have no known gradients for benchmarking purposes. We're going to try, I don't know, four different outer H functions. We do this square just to make sure things are nice. Sure, things are nice. Or the sensor double and loss or some max or some quadratic mappings. And I can tell you how we generated the data for those if you want. And then we also tried constrained and unconstrained problems. Again, how did we generate the constrained? We looked at the unconstrained and tried to cut it in half. So hopefully we hit a boundary. But these are just box constraints currently. And then for Goomba, we say, hey, let's try a bunch of local and global solvers each iteration and give them 30 seconds. If you go to GAN, 30 seconds. If you go to GAMS personally and say, hey, I've got something, what should I use? They say, you know, sometimes Conopt works, sometimes Baron. They give a list of things. They say, some different things work at different times on different problems. We have yet to find one solver that works for every time along the way, but the bundle of solvers does. And so great. So here you get to see some data profiles. We're measuring stationarity by sampling. Stationarity by sampling around every point evaluated, 50 points in a ball, and seeing if it is actually sort of stationary in a ball near every point evaluated by each method. We didn't compare against F Mint Khan or Pictor Paiger thing, but because they have no knowledge of this problem, it's really cheating to show that they don't do well on these. But you can see how much sort of recourse to manifold sampling matters. So, this sort of red is without manifold sampling prime. We don't recourse to manifold sampling and for, I don't know. Recourse to manifold sampling, and for I don't know, five-ish, 10% of our problems. The recourse to manifold sampling is critical to at least for the local solvers we're using in Goomba. This is for the unconstrained problems. We never mapped the old manifold sampling dual from unconstrained problems to constrained problems. So there's no blue line here. But for the constrained problems as well, you can see, hey, manifold sampling primal. Again, this is with a massive caveat that there exists efficient methods. Go to chapter six of Khan Guiltuan to say. Chapter six of Con Guiltoin to say to solve the max of linear things over a trust region, that trust region subproblem can be solved in finite time. And we can point to a method that could do it in a textbook. And Goomba is solving possibly global problems over trust region, which is itself as hard as anything else. Now it's a known H mapping of quadratic functions, but max or sensor Del one loss, mappings of 64. You know, mappings of 64 quadra headaches or something is a hard problem to solve. Okay, so my conclusions and then my questions, and I did it. When optimizing, if you have H of F and H is easy, it's sum squared, it's the one norm, it's whatever. I mean, this, I can't believe how many times I go to some new field and they have something where they output some simulation quantity and then they map it using something they pick or they know. Their problem isn't a scalar function, it's a known. Scalar function, it's a known mapping that they pick of simulation output. Model that f and combine those using what you know about each. If it's non-smooth, then you need to do sort of Clark differentially sub-differential things. If it's smooth, then you can do just chain roll. Great. And then I had some questions. Yeah, we don't exactly know how to use Hessians in this space. I've looked at the difference between, say, the projection onto zero of the downhill directions and the projection onto zero of the Newton directions. It doesn't really matter much. You have sort of a Really matter much. You have sort of a king. I'm yet to sort of draw a picture where second-order information and second-order non-station, second-order non-smooth stationarity is helpful. So we've always just been using sort of first-order terms, but I think there may be something to be gained from that. Maybe? I don't know a problem with such a thing. How much time do you want to spend in your trust reason sub problem? We've always written that line in our papers, all of us. Like, we assume F is expensive enough to evaluate to get over all the cost of the evaluations. Over all the costs of the evaluations and the algebra of the algorithm. But I'm really breaking those rules because, like, you don't put the cap, you'll can spend an hour trying to minimize what you call Baron, which is a global method to find a global, trying to find a global optimum of a quadratic mapping of 100 and some quadratics plus maxes and absolute values. You probably don't want to spend an hour doing that if F is not an hour, but I'm not sure exactly, I don't know. But I'm not sure exactly, I don't know, back in the envelope, I don't know a tenth of the cost of the evaluation of the article. But most interesting, we're going to have some software out available. We're going through the last steps of that. So people can take what we have to interface currently MATLAB with GAMS. We don't have GAMS. We can look at other solvers. We're also developing Python implementations of these. So MATLAB and Python, if people are interested in contributing or using them and building on them, we're happy to help grow that space. we're happy to help grow that space. Thank you for your time. I'm happy to take questions.