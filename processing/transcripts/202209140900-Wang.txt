Thank you for the introduction and thanks all the organizers for inviting me to this online workshop. It's really good to see you here. Today's talk is based on our recent progress on inverse scattering problems for time harmonic stochastic wave equations. And this talk is based on some recent work, jointly worked with Worked with Jianliang Li from Hunan Normal University and Peijin Li from Purdue University. Okay, in this talk, I'm going to first give a brief introduction about the scattering problem and then introduce the random fields to be reconstructed in our inverse problems. And in the third part, I'm going to introduce our main results. That's the uniqueness for stochastic inverse problems. Inverse problems based on stochastic wave equations. And finally, concluding with some open questions. Okay, let me start with the scattering problem. The word scattering here is actually a term in physics and it actually describes the process that some incident waves or sources are forced to deviate as scattered waves when passing through some medium. Passing through some media. So, in many applications, actually, due to the randomness of the environment or the fine-scale fluctuation in numerical simulation, it would be more reasonable to describe the source or some other parameters related to the scattered, such as the media or the potential, as random fields. So, in this case, the physical model tends to be a stochastic partial differential equation. Differential equation. Okay, so here I use an abstract form of a linear stochastic partial differential equation. Here, L denotes the linear operator, which will be different for different kinds of waves. For example, for the time harmonic acoustic wave, the operator L is exactly the Laplacian operator. Okay, K here is a real number, which is the wave number. wave number and n is the random potential parameter rho is sorry n is the random media parameter rho is the random potential parameter and f is the random source so based on this kind of spde the direct problem is to get the solution us based on given random parameters n rho and f okay and for the inverse problem it is to determine the distributions or some The distributions or some other statistics for the random fields n, rho, and f based on some proper observations of the scattered wave. That is the solution. Okay, so for this kind of SPDE, the direct problem has been widely started and it's actually relatively easy because this is only a linear equation. But for the stochastic English problem, it's totally a different. It's totally a different story. The main difficulties for stochastic inverse problems are twofold. Okay, the first one, the first challenge is the non-uniqueness for inverse problems. This is because in reality, only limited data can be measured and used in the reconstruction. So for example, we can construct a non-relating source. Construct a non-relating source. So that is, we consider the time harmonic acoustic wave equation perturbed by a deterministic source f here. If f happens to be in the form Laplacian plus k squared performed on a small function way, the function way here is assumed to be compactly supported in a bounded domain D. And in this case, we can get easily that the solution u is actually equal to the function. Is actually equal to the function way. It implies that outside of the compact support d, the solution u will be zero. So we cannot use the data outside of the domain D to uniquely determine the deterministic source F here. Okay, so this is for the deterministic case. And also for some special random sources, the inverse problems may also be ill posed. Also, be ill-posed. For example, we consider this kind of one-dimensional acoustic wave equation preserved by a random source. Okay, so in front of the white noise W dot here, the function mu is n-known. So we want to reconstruct it from this equation. For this linear SPDE, it will be easy to get the solution explicitly, which is the stochastic integral with respect. integral with respect to the Brownian shift. And here the wave number k is a real number. So if we calculate the second moment of the solution u, we can get that it is equal to some constant depending on the wave number times the integral of the function mu of y. That is to say, we cannot uniquely determine the function mu itself based on this kind of second moment data. So we can only Moment data. So we can only get the average of in this case. Okay, so that is to say for stochastic for inverse problems, actually the uniqueness depends on what kind of quantities you want to reconstruct and also what kind of data you're going to use. So this is a common problem for both deterministic inverse problems and stochastic cases. And as for And as for stochastic inverse problems, another challenge is due to the randomness of the unknown parameters. So, as you can imagine, if we want to reconstruct the distribution or some statistics of some unknown random parameters, then of course we need some statistics of the solution. So it means that a lot of realizations will be required, which makes the problem rather tough. makes the problem rather time consuming. Okay, so in this talk, I'm going to introduce how can for what kind of stochastic random stochastic inverse problems we can get its uniqueness in some sense and how can we reduce the computational cost. Okay, before that, I would like to mention that there has already been many results related to the reconstruction for random source. To the reconstruction for random sources or random potentials. And if the random source F here is the Gaussian-white noise, then of course the mild solution of the equation together with the Ito isometry can be used to get the recovery formula for the random source F. But this framework is not available for general correlated noise because the Edo isometry is not available anymore. So for Gaussian So, for Gaussian correlated noise case, the micro-local analysis for the covariance operator of the random source F will be used. And we extend this result to an even rougher random source case. That is, F belongs to the sublack space W negative 1 minus mu. By defining an equivalent random source and investigating the properties for Investigate the properties for its kernels. Okay. And for the inverse random potential problems, it contains the term rho times u, which is actually a multiplicative noise term. Okay, so for this kind of equation, even though the equation itself is linear, but the inverse problem to reconstruct the potential row is nonlinear because the anon potential row is coupled with Potential row is coupled with the data u here. So the random potential problem will be much more complicated compared with the random source case. For inverse random potential problems, all the existing results, as I know, are about the correlated Gaussian noise case, which are smaller than the white noise. So it's still unknown if everything is going to work, if the random potential row here is a white noise. Potential role here is a white noise. Okay, so in the following, I'm going to introduce other results for both the inverse random source and inverse random potential problems based on these recent works. Okay, so more precisely, I'm going to use the sine harmonic acoustic waves to illustrate how we can deal with the inverse random source problem. And then I'm going to And then I'm going to use the vector-value elective wave equation to show how we can deal with this nonlinearity and solve the inverse random potential problem. And in both cases, both the random source F and the random potential rho here are generalized correlated Gaussian random fields, which we call the isotropic Gaussian random fields. And this kind of Gaussian fields is actually a general. Fields is actually a generalization of the white noise. So, to get this kind of generalization, let's first recall the white noise case. So, if the random field F here is in the form of the square root of some small function mu times the white noise w dot, in this case, its covariance operator can be defined through this deterministic integral with test functions phi and pass. The functions phi and psi, and the strength function μ is involved in the definition as well. Okay, but this kind of definition is not easy to generalize to the correlated noise case because the HO isometry is used here. So instead, we consider an equivalent definition for the covariance operator QF. Note that for the white noise W dot here, its covariance. Dot here, its covariance operator is actually a pseudo-differential operator. So for pseudo-differential operators, we can get its expression, which is an integral of the symbol A here. So if we compare these two equivalent definitions, we can easily get that for this kind of white noise random field F, its symbol A is actually equal to mu of X. That is to say, it's That is to say, it has only one term, which is also the principal symbol, and is independent of the second variable here. And this kind of form will be easy to generalize. So we can generalize the symbol here as an expansion like this. So for the generalizations, the symbol A contains not only the leading term, which is the principal symbol, but also a residual term. Symbol, but also a residual term here, and for the leading term, it's of order negative m. Okay, but for inverse problems, actually, as I mentioned before, in reality, only limited data can be used. For example, maybe only observations in a bounded domain outside of the support of the random field F can be observed. So in that case, the limited data cannot give Is not enough, cannot give us enough observations to reconstruct the whole distribution of the random field F. That is to say, it is almost impossible to reconstruct the whole symbol A based on the limited data. So instead, we are wondering if we can uniquely determine the strengths function mu here in the leading term. We want to reconstruct this kind of function because. Reconstruct this kind of function because the function μ here is involved in the leading term, and this is enough to approximately describe the strengths of the random perturbation. So, our purpose is to neglect the contribution of the residual term and to get the strengths mu only. Okay, so what we considered is the astrobiographic Gaussian random field. Astrophic Gaussian random field. And its definition is that the covariance operator should be a pseudo-differential operator. And the principal symbol is in the form mu of x, which is the strings function to be reconstructed, and times the norm of Kc to minus m. Okay, based on this kind of definition, we can also get at the. So here you're assuming that m is known, right? Yes, exactly. Yes, exactly. M is known. Okay. Yeah, this is a very good point because if M is not known, we cannot determine what kind of data we are going to use. And is it, I mean, realistic to assume that M is known? Actually, we know that the parameter M actually denotes the regularity of the random field. Regularity of the random field. So it's somehow reasonable to assume that the random source or random potential is in the regularity of it is in a domain or in some or as a fixed number which we can know in advance. Okay, so for the kernel function here, we can just plug the definition of the pseudo-differential operator into this Delta product here, and we will get this triple integral. And the integral inside the bracket here gives us the kernel function, actually. So this integral actually equals to the expectation of f of x times f of y. f of x times f of y in the distribution sense. Okay, so if we take a closer look at this integral, this is actually the inverse Fourier transform of the symbol A with respect to the second variable cosi here and takes value at the at the value x minus y. Okay, so we denote this kernel function as kf x comma x minus y. The first variable The first variable x here comes from the strings mu, and the second variable x minus y comes from the derivatives, which is related to the term of casi. Okay, so if regardless of the strings, that is the first variable vanishes, then the kernel will depend only on the distance between x and y. So that's why we call this kind of random fields as tropic. Astropic. And for example, for the white noise case, we have already known that the simple A is actually equal to K. It's independent, sorry, it's equal to nu. It's independent of C. So if we consider its kernel function, which is the inverse Fourier transform of A with respect to Cassi, we will get that it's equal to mu of X times the Dirt function. And we know that the Dirty function here is know that the dirt function here is exactly the kernel of the white noise. Okay, so this result coincides with the result we have already known. And for this kind of random fields, so the parameter m here is assumed to be known because we need to know its regularity. And we know that for this kind of Gaussian source, its regularity depends on its covariance operator. Depends on its covariance operator, so it depends on the symbol of it, so it depends on actually the parameter m here. Okay, to get the regularity for this kind of random field, we consider another kind of frictional noise, F tilde, which is defined based on the frictional lab function performed on the white noise W dot. Okay, we consider. Okay, we consider this kind of noise because for this kind of noise, its regularity will be easier to obtain since the regularity for the white noise is known. And we can also calculate that for F theta here, the leading term in its symbol, that is its principal symbol, is in the same form as the esotropic Gaussian random field. So we have the same order M here. We then get F2. We then get F t has the same regularity as the isotropic Gaussian random field. Okay, and in particular, if the order m belongs to the interval d to d plus 2. In this case, we can show that F terror defined this way is actually equivalent to the classical fractional brain motion with first parameter m minus d over 2. Okay, so that is to say for m belongs to the Say for m belongs to this interval, the isotropic Gaussian random field has the same singularity as the classical fractional burn motion, so its holder continues. And if the order m is smaller, then the random field F will be rougher. So in this case, if M is smaller than or equal to the dimension D, then F will belong to a solid space with a negative index. So in this case, the random field So, in this case, the random field F should be interpreted as distributions. Okay, so in the following, we will focus on this rough case because for the smaller case here, it can be treated in the same way or some easier approach can be applied to solve this small case. So, in the following, we only focus on the distribution case. So, based on such kind of random fields, we first consider the inverse random. First, consider the inverse random source problems for the time harmonic wave equations in lossy media. And here, the equation is also known as the Hampel's equation. The constant sigma here describes the background media. Since the media is a lossy media, so sigma here will be positive. In particular, if the media is lossless, then the constant sigma will be generated to zero. Generate to zero. And the random source on the left on the right-hand side is assumed to be an isotropic Gaussian random field. So we want to reconstruct the strength function mu involved in the random source here. And for simplicity, we can define a complex with number kappa here such that kappa squared is equal to the term in the bracket here. And it's easy to calculate that the root. It's easy to calculate that the real part of kappa is actually equivalent to the real wave number k. But for the imaginary part here, it will go to a constant when the wave number k goes to infinity. And this kind of asymptotic expansion will help to get the recovery formula. So, anyway, for this linear stochastic partial differential equation, we can get the solution explicitly, which is a convolution. Explicitly, which is a convolution between the fundamental solution phi and the random source f. And we know that for different dimensions, the fundamental solution phi have different forms, and they have also different singularities. And it will lead to the fact that for different dimensions, we will need different kinds of regularity assumptions on the random source F. Okay, next, I just take the one-dimensional case for example. Case for example, we define another kernel function a, which is actually the original kernel of the random source times an indicator function theta. And theta here is equal to one if the point x belongs to the measurement domain, and the theta will be zero if x belongs to the support of the random source f. It indicates that we can only observe the data outside of the support. Outside of the support of the random source. Okay, and based on this kernel function, we can get that the second moment of the solution u can be rewritten as this double integral, and the exponential function here comes from its fundamental solution. Okay, and to deal with this exponential function involved in this double integral, we use a coordinate transformation tau here. This transformation will map the This transformation will map the terms in the first bracket to the variable j, and it will map the terms in the second bracket to the variable h, such that we can rewrite this double integral to another double integral with respect to j and h and with a Jacobin here. Okay, but the problem is that for the kernel A here, we don't know what its behavior is under this transformation path. So to So, to deal with the kernel term here in red, we need to split the transformation tool into the composition of two other coordinate transformations, such that this kind of kernel involved a symbol here have some kind of asymptotic extension under the first coordinate transformation. Transformation. And after we get the expansion here, then the leading term in this expansion will keep invariant under the second transformation. So based on this kind of procedure, we're able to deal with this kernel term. And finally, we can rewrite the second moment into this kind of expansion. And the leading term here comes from the principal symbol of the random source. And the higher order. Random source and the higher order term comes from the residual okay, because this is also a higher order term. And in the leading term here, we have actually a single integral here instead of a double integral. This is because based on the coordinate transformation tau, we are able to get integrals with respect to j and h. And we know that for the integral with respect to j, it's actually an inverse for a transformation. Actually, an inverse Fourier transform. Okay, so we're able to eliminate the integral with respect to j, and we will finally get a single integral like this. And for this kind of expectation, we know that for the real part of kappa, it's equivalent to k. So the absolute value for kappa is also equivalent to k. And the imaginary part of kappa will converge to a constant. Will converge to a constant when k goes to infinity. So, of course, we can multiply both sides by k to the power of m plus 2 and then let k go to infinity to eliminate the high order terms so that we can finally get this kind of recovery formula. So, that is to say, when the wave number is large enough, we can use this kind of second moment data at a single wave number k to With number k to get this kind of deterministic integral with the strength mu involved in. Okay, and actually, based on this kind of integral, if sigma here, the constant, is larger than zero, then the kernel here is not trivial. So we can use deconvolution to uniquely determine the strength function mu. But if in particular a sigma here is zero, then the uniqueness can only be ensured for the Uniqueness can only be ensured for the two and three-dimensional case. This is because if sigma equals zero and d equals one, the kernel here, the friction, turns to be the constant one. And in that case, this kind of second moment data can only give us the integral of mu. This is exactly the non-uniqueness example I gave at the beginning. Okay, and also this kind of theoretical results can be verified through numerical experiments. So for the one Experiments. So, for the one-dimensional case, sigma should be larger than zero, and we take f as an additive as an Gaussian white noise. And the strengths mu is given like this small function. In these figures here, the blue line are the exact strings function mu. The red lines denote the reconstruction results. And we can find out that for sigma equals 0.3 or even larger based on the. Even larger based on the data at a single frequency, K, we can almost get the correct result. But these two results are not good enough. This is because even though theoretically we can get the uniqueness for the inverse problem, but the recovery is not stable. Okay, so numerically, we add some more observations from with numbers k to from with numbers k. From with numbers k equals 1 to 16. Okay, so based on this kind of multi-frequency data, we're able to get a better reconstruction. So, this is also a point we want to improve in the following slides. Okay, so we want to try to use multi-frequency data instead of this kind of single-frequency data to make the problem more stable. Okay, and for the two-dimensional case, sigma can be zero. Dimensional case sigma can be zero, and we still choose the white noise case. And the left figure is the exact strength function mu, and the right figure denotes the recovery result. So these are the results for inverse random source problems. Okay, and next for the inverse random potential problems, we will consider the two-dimensional vector-valued elastic wave equation. Valued elastic wave equation. Okay. And the potential parameter row here is also assumed to be an isotropic Gaussian random field, and we want to reconstruct its strengths. And for simplicity, we assume that the source function on the right-hand side is a point source, which is located at the position y here with polarization direction a. Okay, for this kind of elastic waves, the quintensor. Waves, the quintensor is much more complicated compared to the acoustic wave case. And it's the fundamental solution is actually a combination of the fundamental solutions for acoustic waves, but at different frequencies, at different wave numbers. And we know that for elastic waves, it contains both the shear component and the compressional component. So kappa S and a kappa P here denotes the shear component with Here denotes the shear wave number and the compressional wave number, respectively. And for the two-dimensional case, the fundamental solution phi for the acoustic waves is expressed as this Henkel function of the first kind. This kind of function is not given explicitly, but instead we have its symptotic expansion like this. Okay, so for this kind of fundamental solution to deal with that, we have To deal with that, we somehow a truncation technique should be used. And for the director problem, we show that the elastic wave equation has also a unique solution satisfying this integral equation, which is known as the Liebmann-Schringer equation. And for the right-hand side, the integral can be denoted by a potential operator, k here. And the last term comes from the point source. term comes from the point source. So we denote it by U0 and U0 here is actually the incident wave. And the world postness result here is actually obtained based on the compactness of the operator kappa. And kappa here is actually a random operator because the random potential is involved in. Okay. And furthermore, for this kind of potential operator K, it has some decay. K, it has some decay property with respect to the width number k. It will help us to separate the random potential rho and the data u here to solve the nonlinearity of this kind of inverse problem. Okay, so more precisely to separate rho and u, we need to construct a sequence based on this Ligman-Schringer equation. So we define the Bohr sequence by this. By this kind of equation. So, uj plus one is defined by negative k performed on uj. So, if we sum up all these equations with respect to j, we can get that the summation satisfy this kind of equation with a tail here. And if we can show that when the capital N goes to infinity, the tail term will converge to zero, then we can get the summation will converge to the solution of the Lee-Manschung equation. Solution of the leaf men's equation, actually, and this is actually true because the potential operator kappa here has the decay property with respect to the weak number k. So if k here is large enough, the norm of it will be less than one. So this term will converge to zero. Okay, so the solution of the elastic wave U is actually equal to this Bohn series, and U0 here is the incident wave. Here is the incident wave, so we can also get the scattered wave, which is the total wave u minus the incident wave u zero. So it's also a series here. Okay, to consider the inverse random potential problem, in particular, we choose the point source, the position of the point source y here, equal to x. That is to say, we put artificially some point sources in the measurement domain such that Experiment domain such that we can also get the observations at the same position. And in this case, the scattered wave US will depend only on X instead of Y. And we slip the scattered wave US into the terms U1, U2, and the residual terms B. Okay, so we need to calculate the contributions of each terms separately. For the first term, U1. For the first term u1, we know that it is generated by the with u0, and we know the expression for u0. So plug it in, we can get this kind of integral. This integral is also a convolution between a random field row and some fundamental solutions. So the formula here is really similar to the inverse random source problem. Okay, so it can be treated similarly. And for the second pro for the second term U2 here, it's generated by U. Here, it's generated by u1 and u0. So, we can get the double integral like this. So, for the estimate of u1, similar to the random source case, we consider it a second moment, and we can show that the second moment has also this kind of extension with the leading term involved the strings mu and a higher order term here. So, we still multiply both sides by k to some powers, and we can get a similar result. Get a similar result as the inverse random source problem. And in this case, we still use the observations at a single frequency to get this kind of deterministic integral, which can be used to uniquely determine the strength function of here. But for this kind of result, we still have two problems. The first one is that, as I mentioned before, we use only single with number data to With number data to recover the strain signal here. So, this kind of simulation will be unstable. Okay, so we wish to use multi-frequency data instead to make the problem more stable. So, this is the first problem. And the second one is that for the data on the left-hand side, we have an expectation here. So, a lot of realizations will be used to approximate this expectation. This expectation. Okay, so we wish to reduce the number of realizations which will be used. So, hopefully, if we can use a single realization to recover that, it will be better. Okay, so to solve these two kind of problems, that is to say we want to replace the expectation here, actually, by the average with respect to the wave number of k. So, this kind of propagation. So, this kind of property is somehow can be regarded as an analog of the ericity in the frequency domain because we want to use the average with respect to the frequency to replace the spatial average over the probability space. But according to the proof of this kind of property, to me, it's more like some asymptotic version of the law of large numbers. Version of the law of large numbers, actually, because in this case, for the particles at different wave numbers, we want to show that the average of all the particles will converge to the expectation of each particle. So in this case, we need the assumption that the particles should be independent. So that's what we proved in the formula lemma here. We showed that the different particles here are asymptotically independent. Independent. So that is to say, for different wave numbers k1 and k2, if they are far away from each other, then the correlation of different particles can be bonded by a very small quantity, which is approaches to zero when k1 minus k2 is large enough. Okay, so based on this kind of property, we are able to replace the expectation here by the average over the wave number. Over the wave number. Okay, so in this case, in the almost surely sense, we can use this kind of multi-frequency data at a single realization to get this kind of deterministic integral. This is the estimate for U1. Okay, and for U2 here, it can be written as a double integral contains the potential row of Z times the potential row of Z prime. So if we want to estimate the So if we want to estimate the contribution of U2 square, it implies that we need to estimate the product of the random potential row at four different positions. Okay, so for this kind of estimation, first of all, we need the truncation technique, as I mentioned before, to deal with the green tensor here, which is a series. And to deal with the product of the random potentials, we need to define a small modification. To define a small modification, rho epsilon, which is a convolution between rho and the modifier. Okay, and after get this small modification, we are able to calculate the product of rho epsilon at four different positions based on the weak formula. So based on this kind of procedure, we're able to get the contribution of U2, which is actually zero. And this kind of result is easy to. This kind of result is easy to imagine because for U2 here, compared with U1, it contains more Green tensors here. And for the Green tensor here, we know that it has the decay property with respect to the wave number. So U2 can be regarded as a higher order term. Okay, and similarly for the residual term B here, it contains even higher order terms. So based on the decay property of the potential. Based on the decay property of the potential operator K, we can show that the contribution of B is also zero. So, combining these estimations together, we can finally get that for the inverse potential problem, we're able to use in the almost a short instance that a single realization of this kind of multi-frequency data to get this kind of deterministic integral. And again, since x belongs to the measurement domain. X belongs to the measurement domain. Y here belongs to the compact support of the random potential. So this integral is not singular. So we can use the deconvolution to get the function mu. Okay, so this is the result for the inverse potential problem. And in conclusion, the inverse random source problem has been widely started, has been well started actually, with F being widely. With F being white noises or correlated noises. And from the point of view of inverse problem, this is also a linear problem. And the inverse random potential problem is a little bit more complicated because this is a non-linear inverse problem. So for this kind of problem, it has been investigated for acoustic waves and elastic waves based on the properties of the grim tensor. And for the electromagnetic waves, it is still unknown if we can get the uniqueness result for this kind of inverse random potential problem. This is because the linear operator for Maxwell equations is much more complicated. So, I haven't seen any results related to this kind of waves for this random problem. And another interesting and important problem. interesting and important problem is the random media case. So that is the media parameter n here is characterized by a random field. And in this case, from the point of view of inverse problems, it's also a nonlinear problem. And it has a similar form as the potential case. So we can also rewrite it as the Liebmann-Schunger integral equation. But I have to state that the framework for the random potential case For the random potential case cannot be used to the random media case. So, this is because if we construct the Bohn sequence similarly as the random potential case, the series does not converge. Because for the potential operator K here, we still have the wave number k squared in front of it. It makes the Boeing series does not converge. Okay, so for the random media case, the inverse. Uh, the random media case, the inverse scattering problems are still open. Okay, all these results are about time harmonic wave equations, and actually for time-dependent SPDEs, there's also several related works, but only for the random source cases. Okay, so if the random source depends on the white branch motion or depends on the frictional branch motion, we're able to reconstruct. We're able to reconstruct the coefficient, which depends on the spatial variable only. And also for the case that the random source depends on the branching sheet, and we are also able to get to uniquely recover the coefficient in front of the browning sheet, which depends only on the time variable key here. Variable key here. Okay, but for some other kind of random sources, it's still open. So we don't know if we can still get the uniqueness result for the other random sources. And also for random potentials or random media, there's no results related to this part to the best of my knowledge. Okay, so I think I'll stop here. Thank you for your attention. Thank you, Xo, for this very nice and clear presentation. Are there any comments or questions? So, as always, feel free to just unmute yourself or raise your hand, as Sami showed us on Zoom. Just have a quick look. Okay, maybe, maybe I guess. Okay, yes. Yes, is it? Do you think it's much harder to do it in the time-dependent case? I mean, at least for this first Gaussian case. Yes, exactly. Actually, the existing framework depends on that for the former case that the noise depends only on time. The noise depends only on time. In that case, we're able to reconstruct the coefficients which depends only on x. This is because we can use the eigen expansion to rewrite the spde to s ODEs. In that case, we're able to reconstruct the coefficients of the function j. And also for the time-dependent space-dependent noise case, if the noise depends only on Uh, if the noise depends only on x and we want to recover the coefficient depending only on t, in this case, we can use the Fourier transform to rewrite this time-dependent equation into the frequency domain. So it turns to be a time-harmonic equation. So in that case, we can also uniquely determine the Fourier mode of f of t and also we can use the phase retrieval. And also, we can use the phase retrieval, which is frequently used in engineering to get the function act itself. But for some other kind of random sources, actually, we don't have any ideas how can we reconstruct the coefficients in front of the random source if it depends on both the spatial variable and the temporal variable. That's because they are. Yes, because they are correlated. So we can neither rewrite it as ODEs or rewrite it as time harmonic equations. Any further comments or questions for Sue? Maybe I can ask. It's probably a stupid question, but I'm not familiar with these kinds of problems. So you had You had results for two different random terms, so random sources and random potential. What happens if you combine them? If you have one equation with can you do this? And also in the very beginning, the model that you had had the three source terms, as others are the three random term, random media, random potential, and random source. So once you get one of these, can you do a combination or not? Well, that's a very good point. Well, that's a very good point. And actually, there's also several works related to this part. Let me go back to the references. Actually, for the acoustic waves case, so if we have a random potential term here and we have a random source as well, we can also recover their strengths. But the procedure is that we need to recover the strengths of the random source first. Cover the strengths of the random source first, and then based on this kind of reconstruction, we can further reconstruct the strengths of the random potential. But in this case, we need some other reconstrictions on the regularities for both the random source and the random potential. So in that case, these random fields need to be regular enough. So the reconstructions on the random fields will be different. But if we only want to reconstruct the random field, We only want to reconstruct the random source itself, or we only need to reconstruct the potential, then the random fields can be much more rougher than the complex case. Okay, thank you. And for nonlinear problems, are there any results or is it out of reach? No, as I know, there's no results for non-linear SPDs. Non-linear SPDs. So, for a deterministic case, there's many results related to the inverse problem for both the numerical simulation or uniqueness or stability. But for the stochastic SPDE case, there's no such results for non-linear case. Okay, because even for the linear SPDE, the inverse problem is already non-linear. So, if for non-linear PDEs, I don't know. I don't know if we can still reconstruct these experiences. Okay. So last chance to ask a quick question or a comment or a longer question, if you want. There is a question on the chat. Can you read it or should I read it? So there is a question by, maybe I will read it. Maybe I will read it for everybody. By Andre, regarding the reconstruction as shown in the figure around slide 13 or 14, did you reconstruct using other sets of K? If yes, did the reconstruction improve quickly with additional K values? Okay, that's a very good question here. So for the reconstruction here. Yes, here. So if we use the data add. If we use the data at a single week number k here, we will get similar results here. So we can approximately get the reconstruction, but it's not so good. So actually, we add some additional observations for different wave number case. So as we can find out that in the second row here. So in the second row, we keep the we choose sigma as 0.3. And for the wave members K here, it can be chosen from 1. It can be chosen from 1 to 16. So we have 16 wave numbers. By adding these additional observations, we can get a better reconstruction for the strengths. So this is actually because that if we use this multi-frequency data, then the problem will be more stable. So compared with the single wave number case, the results will be better. So that's why in the second part for the random potential. In the second part for the random potential problem, theoretically, we also want to use multi-frequency data instead to get the recovery results. Okay. I'm not sure if I answered your question. Andri, maybe you can just confirm that you get an answer. Okay, it seems that the answer, the question is answered. Okay, so apparently there are no more questions or comments. I can still go ahead and Sam. Yes, I was wondering if you had something more not inverse, but in direct, I mean, yeah, in the straightforward direction, like, so I know you like to talk about those brown noises or pink noises or whatever. Pink noises or whatever. Do you know that if, in some cases, right, take one of your systems and you have as an input pink noise, then it will become a brown noise or something like that. Okay, I think you're asking the noise in the data, right? Yeah. Yes, so that's Yes, so that's also a very good point. So, actually, for this kind of random stochastic inverse problems, we characterize the parameters as random fields. And actually, for the observation U here, in reality, if we collect other observations, there will also be some kind of noises applied to this kind of observation. Observation. And for the reconstruction, here, if it is still good, if the data is polluted by some kind of noises, it's somehow related to the question of stability. Yes, so if with some additional noises on the data, we can still get a better result. It implies that the reconstruction is stable. So if the noises is stable. So, if the noises are small enough, we can get an accurate reconstruction. But if we add some small noise on the data, the reconstruction will be totally different. In that case, the problem will be unstable. So, in that case, actually, we may only add at most, I think, 1% or 0%. Percent or 0.1 percent noises to the observations. So that's actually different problems compared with stochastic immersive problems. And in that case, for that kind of noise is added to the observation, we usually use the white noise. Okay. Thank you. Thank you. Thank you. Okay, but I think that Suban is tired now. So thank you for this very nice discussion, for your presentation, and thanks also Hakima for your presentation.