Okay, what these directors? What these director is? The basic idea is that people have been looking at a different way of holding functional computation apart from the standard gauge parallels. We should see first of all the classical scenario now of The classical scenario now of the bubble, attribution neurotic boards. Why? Because they can perform a kind of task which cannot be described as algorithmic. A way in which people tend to consider neural networks, visual neural networks, as some sort of magic black box in which you pick it up the answer without knowing exactly what's going on. Without knowing exactly what's going on. Suppose we told you about how an artificial neural network works, basically it's the following. Neurons are units, which can be either on or off, and they're on or off depending on the signal that we see from nearby neurons. So the near-to-domination, when a neuron is off, it sends signals to a season for albino neurons, which are connected by meters. And according to the sum, the weight itself. According to the sum, the weighted sum of all the input signals, the neuron files are signal to the desktop. So, the typical architecture of the former, you've got several layers, the neurons, and you've got an input layer in which your inputs beyond your variable, you can use your input signals. This will send again a signal according to the given weight to the next layer, and then again to the next layer, and so on and so forth. So what amputees output layer when you make a measurement? By adjusting the weight of this network, you can decide when and how the neural spire. And basically you can train the network to evaluate basically near gate functions. And you can train your You train your system to perform a series of tasks, like for instance recognizing nodes or displays. Now, let's. When you have a neural network, typically in immersed neural networks, the signal propagates forward, so it's it's a it's a one-directional neural network. But you can have also networks in which you have loops, okay? Which have loops, okay? And in this case, the system level internal memory. How this memory plays a role in a somehow slightly simplified payload architecture. I mean, to try, before going to the next slide, why do a techno so interesting apart from the um the fact that we can perform uh tasks which are difficult to perform algorithmically? They are often faster. Often faster than the conventional computer, and the energy consumption is definitely lower, that can be more efficient. You cannot keep track of what's going on, it's of course box, but again they work and they can be more efficient. Okay. Now the uh kind of architecture I I'm going to discuss um now is a sort of simplified version of the derivative. Rather than training the whole network, the preservable computing is structured in the following way. You've got an input layer which will input your signal. It can be binary or as you would say in the case of a boundary system, can be also put in a variable. Anyway, you input your system in the input layer and after this signal is then passed. Signal is then filed within what is called a reservoir layer according to some weights. So it's a weight matrix which tells you how much each input influences the networks of the reservoir layer. And then you have the output layer which you measure. And the interesting point is that um you don't have the um reservoir layer which is large. Which is large and is completely random, is not trained. So you don't adjust the weight of your network in order to perform a given task. It's basically a random matrix and it's fixed in time. It does not change in time. And at the same way, you do not train your input mode. So the inputs and the connectivity methods of logs. The same one menu half-fixed. What you train is the way you would choose. Train is the way in which you link your output layer in order to evaluate to perform a given task. So what you train is the way in which at the output layer you extract some information. So this very much simplifies the your your um your training process and on the other hand the non-linearity of your um of your uh of your uh of the which you require which is required by your by your task uh is given by the randomness of the reservoir. And these systems have been used for various kinds of tasks. I will show you them in the next slide, in particular classification and time sequences. Particular time sequences are a particular task in which you consider Which you can see this, which reservoir proved to be classical values, they've proved to be very efficient. The idea is that the reservoir has a network. So these are the step forward to see. Suppose that your system is in your input is in step xi and then let x be the set of the step you are. Okay, so your input at a given time at find k, at step k in your calculation. In your calculation, in your input, your system xk, the state of your reservoir will be a function of the state of the reservoir at the previous step and the signal. And if you iterate this process, you see that basically the state of your reservoir at step A depends on all the paths that you can. This is a feature which is particularly useful when you are interested, for instance. When you are interested, for instance, in identifying or processing and classifying, we would imply fancy stuff. We will see in the next slide. On the other hand, something you can do after you inject a given signal, you reset the state of your reservoir. So in this case, the state of the reservoir step K does not depend on the past history, but only the state of the Only the state of the system of the input at the same step. Very loosely speaking, this is a very loose analogy with the model which is one of my pet models, is this is a collision model with memory and this is a memory-less collision model. So we're not pushed any further this analogy because at least they'll never investigate it in detail by itself. But there are analogies between using this. Okay, as I said before, the extreme learning machine is basically you can use as a reservoir computer which resets the state of your reservoir after each injection whenever you inject signal their SK. Now let's go to science that should fancy all this versus classification as a two-parallel. Classification as a two-periga example of tasks which you can perform efficiently on the one hand on reservoir computers and on the other hand on a single learning machine. Suppose you want to recognize speech and you want to translate speech into a written text. So the input is out indicating speech with a defined sequence of signals, your methods or your voice, and with this signal enters your results. With this signal, it enters your reservoir, and as an output, you want some text. Now, of course, since you must recognize a time-dependent signal, you must first listen to the whole world. So you must... The written text, the output text, depends not just on a single pastime, but a whole sequence of fasting. All the sounds, the vowels, and the consonants, and the sounds. The consonants and the sound you produce over a given time span. And therefore, the several computers are useful because we have this card, this kind of task, because they keep memory of the signal over a certain amount of number of steps. Of course, this input-output must be contrasting. In other words, the whole past history could be text, would you know? All past history will be kept within the memory, especially because the reservoir will be a finite memory, cannot keep a record on an infinite past. But what is enough is the fact that it keeps memory on the recent past. So the idea is that the structure of the reservoir must be such that the input sequence that coincides during a short time span will be derived to the same, will be derived. Would be rise to the same kind of output, same kind of written text, but on the other hand, the remote path does not input textual right. And on the other hand, it must be guaranteed separability. In other words, if your sequences are different within the time span, they must be mapped into this. Extreme learning machines are absolutely for those of being boxed. For that would be impossible to process a time sequence for a very simple reason that you don't have memory. If you reset the state of your reservoir, you basically reset the memory. On the other hand, particularly good at classifying objects whenever the time sequence is irrelevant. For instance, you are given a set of pictures of various animals and you show this picture to your reservoir and you ask, Is it a bot? Is it a cat? And you train your system to recognize that there is kind of levels of input, which are in a case in which all the way in which you feed the input would be your function scenario. The function scenario, you have I must actually correct what I wrote on the left. On the left-hand side. In a quantum scenario, your reservoir is a quantum system. Your input can be quantum or classical. What I mean by quantum or classical. It can be a classical signal driving the quantum reservoir, or they can be a quantum system which you feed in. And the quantum system can be used either to encode quantum information or classical information. Information or classical information. So the input can be either classical or pattern, and the Python system itself can be used to perform a classical or a Python task. The readout can be somehow a signal which you can use to perform, which implies an output, which is either classical or pattern. For instance, nothing prevents you from using the applicant as it wire. Using the applicant reservoir to recognize, to identify the classical pancles. Indeed, one of the first applications or the first simulations of our talent reservoirs in touch groups identified classical cancers, speech groups. The interesting point is that bump reservoir can be implemented on a broad variety of physical plants. a broad variety of digital platforms. So in this sense, you don't need to have the the complete control you need to have for tasks or like on a quantum computer, where you bother about coherences and the exact timing of your input sequences and so on and so forth. This is a somehow more flexible system and as you can easily guess the kind of platforms which have been used more in the recent More in the recent past to simulate quantum reservoirs, being aquatic platforms and circuit 3D platforms. Let's look at an example of a quantum reservoir community which is performing a quantum task. This is a paper by Coleman Franklin and Co-Workers, which appeared years ago. The idea is the following. They have a set of states which are Set of states which are squeezed thermal states, so input are squeezed thermal states, and you want to somehow measure a sort of weakness. So you input such states in the zip line, and depending on the amount of freezing and unfermal noise, this state can be either separable or can be a separable state or a Or entangle state. It's a two-mode squeeze state. The blue and the red are just to mean that it's a two-mode squeeze state. A purely two-mode squeeze state is of course entangled, but if you squeeze a two-mode thermal state, depending on the amount of squeezing or the thermal noise, the state can be either powerful state or can be an advanced. They are considering the following. They have the reservoir, it's a fermionic reservoir, and they use a fermionic in that case they want to use a fermionic reservoir to evaluate these methods. And the reservoir is a standard analogy, and it's a fairly hard animation. The input state, as I mentioned before, is a State, as I mentioned before, is a squeezed thermal state. The coupling is a one-directional coupling. Coupling means that the signal is fed into the reservoir and the reservoir, as I said, is a fermionic fermiab model, which, however, you have glosses and you have fun. For those of you who don't like me, I mean, master equations, that's the open system dynamic. The open system dynamics, which describes the dynamics of the public systems. And this is an effective coupling, which is due to the fact that coupling is a cascade coupling, which is in situ. And yes, and this is the local losses on the local losses on the input as well. Okay. And the system is extremely efficient. So the methods to try. So the manage to train, the first train the output weight at the end of the measure is the population or a set of nodes of the reservoir, and then the train, the feed these output statistics, according to some weight, to a final output which tells you the state is separable or is a proper state, is a variable state, or is a red valid state. So basically, you're measuring a weakness in a way and doing some magic in this training output training type. And the system is super efficient. They manage to achieve a very high efficiency. Of course, they have a training set, they have a test set, and then they cluster. Okay, and then they classify a set of numbers with a number beforehand what they are. Another pattern is the following. What can you actually do with the extreme machine learning? I mean, the setup that they are being considering patterns, strictly speaking, is a third problem. Is a very well, in a sense that they are injecting into the system multiple sequences of these pulses, not just a single pass, but a sequence of tasks. To make the process more efficient. But the problem is, if you have a quantum extreme learning machine, so basically to your set as a computer is a bar after each usage, what what can you do? Well, uh Now, let's again look at the quantum reservoir from a slightly different perspective. Why are quantum reservoirs why can data false and task? The way the water is basically the body, if you look perfectly, you have a control system, this system interacts with the reservoir, and you measure the reservoir. But this is nothing but, if you look peripherally, this is basically at the OBM. But the interesting point in the way in which they work is they you enlarge your hilt space and you perform a measurement of your larger hinder space. But an input space, this input space interacts with the reservoir, you are enlarging your reservoir, and if you're measuring the, you are making a measurement of the reservoir. This is basically a POBM in which the POBM depends on the states. Of the end depends on the stage of your input. So, if you think of the quantum reservoir as an ancillary system, you must change the perspective. You are making a measurement of the ancillary system depending on the set of the input. Slightly changed perspective from the usual picture of the ODR. Okay, and then you train the art of statistics to process your process your to to achieve your task. So the this output statistics is basically a legal function of your input state. So basically you you your function, the dysplopium is a linear process. But how the non-linearity which is intrinsic in the classical In the classical scenario, in the classical scenario, reservoir allows you to perform non-linear tasks. In the kind of scenario, it's not possible in the sense that the whole process is linear. So, I mean, how is it possible to do, for instance, classification of systems like the one we did before? The idea is that you are basically, nothing prevents you from Prevents you from having a set of input states which depend on density operators, which are the system you want to classify, for instance. And this set of density operators depends non-linearly on some sort of parameters. Think, for instance, thermal state. Thermal state depends on temperature. The dependence of the density operator of temperature is a non-linear function. So the non-linearity. So the non-linearity is not in the time evolution, because it is intrinsically easier, or in the measurement of physics. The non-regality is in the way in which you encode some information into your space. But on the other hand, something can be. You can think of a different scenario. You can think of an extreme scenario. You can think of a extreme machine learning in which you input multiple copies of the same system. This is different from the reservoir computing scenario. In the reservoir computing, you are injecting sequences of products. Look at the compare the top figure to the bottom figure. If there is no more computing, you are analyzing fine sequences of functional states. time sequences of online states. So your input sequence is a sequence of different density operators which are your time sequence. These are injected in your reservoir and each of these will modify the set of the reservoir but the set of the reservoir will depend on the state of the past sequence of your input states. On the other hand, you can have a powerful machine learning A part of machine learning, which you have a reservoir, and before recepting it, you inject multiple copies of the same input density operator. So you inject, let's say, two, three, four copies of the same input density operator. This allows you to evaluate something which is a function of functional of your density operator which states like things like the purity. Like things like impurity or something which is dependent on the first port power that presents the value. So, something you can do is you can evaluate functions which are no linear functions of your density value. So, using a given function ST machine learning machine, but you can inject multiple copies of your input state. So, if you want to evaluate the QP, then you must inject it. To repeat, then you must inject within your reservoir two copies of the same basis of the reservoir. In this case, it's true that you reset your reservoir at the end of its usage, but on the other end, you have two copies of the same density of processing at the same time. This allows you to extract task perform tasks in which you need to again, for instance, This is basically it. I hope I've been too fast, but please stand for questions, which is awesome. Thanks. Thank you very much. Questions, please? Thanks for the talk. Talk and Marmila, by the way, I guess you probably can you see if he can't see us, but Marcio, can you see us? You can, right? Yes, you're all the right. Yeah, okay, good. Okay, great. So, um, yeah, thanks for the talk. Um, I guess uh one of the questions I have is what's the usual optimization process involved? So uh so as you mentioned, you need to train the wets uh uh connecting the reservoir to the outputs. And is it just a standard sort of grading descent algorithm? Sort of breaking descent algorithm, or is it some sort of genetic algorithm? Like, is there a standard or is it kind of try whatever works? Okay, now if I said correctly, the question is what do you do to the outputs here? Yeah. Let's look at this particular example. The reservoir is a set of permeodic nodes. And what you do is the following. You take a subset of your reservoir, you basically look at even the whole reservoir. And you measure the population of each side. And you have a different statistics outside and you see, okay, if I get here, if you get this set of NJ outputs, and I know what the input is, for instance, entangled, you say this set is, this set This input state, this set of output measurement corresponds to an entangled state. And you adjust, so your white out is an entangled cost of and you train this weight in such a way that your your cost function says one. So basically the training is this is if you write a regression. If you write a regression, this is a matrix which links your int, your should not stop pointing with by integer because you cannot see by integer. Basically, this is a matrix which links your alpha statistics with the Y out, which is the answer you want. So you want to say yes or no. This is a matrix which says according to this. To these statistics, you get another positive moment. It's a fitting detector. So you adjust the weight in order to get the right fit bit. Once you do the fitting for a reasonable number of inputs, this weight has been adjusted in such a way that you will work all sorts of inputs to you don't know every order you will pay map or program. So basically, it's basically a training classifier. It's an extra Using standard classification. Right, right. Then the weights are adjusted through some sort of back propagation, radio descent. Right, right, okay. The measurements are non-deterministic, right? Because it's a quantum system, so your measurements fluctuate. Fluctuate when you mean when you're not. But is it non-deterministic? Because we have a quantum system, right? So the measurements, there's a probability distribution. In classical reservoir computing or classical neural networks, the output's the number, and you perform one experiment and you can see that output and it's uh the same number, you have the same weight in the neural network. But in this scenario, it's a quantum system. This is an interesting point. This is an interesting point. You should think of training as a single shot procedure. You inject a single state, you take a set of outputs, and you train your system. Sometimes they inject two copies, but they inject two copies to make the system more more somehow more stable than it does. But still, it's a single shot. A single shot. Right. So if the classification is non-deterministic, then you could foreseeably have different measurement statistics with the same input. And sometimes it would classify one way, and sometimes it would classify the other. Is that right? In the boundary cases, I would imagine, depending on measurement outcome, it could either be classified as a cat or a dog. Is that right? Because the measurements gives you. Right? Because the measurements give you the statistical fluctuation you don't get in classical reservoir computing. But okay, but the idea is basically just correct with the question. It's like it's precisely the same thing as showing a picture of a cat or a dog. You show it only once, you get you get your statistics on a single shot. Yes. Yes. I understand. I guess the question is: this is a non-deterministic process, right? Because it's a quantum measurement. So sometimes you're feeding a steak and it would say cat and sometimes it could say dog because we have done quantum measurements. So on a single shot scenario basis, the male and the outcome fluctuates. If you show twice the center, you would get different numbers again. You can train. You can tweak twice the same state and you get different statistics. But you pay the system to the training process. Look, the same power, okay? You get different statistics, but the power is always the same. But how it's always the same. And the system learns to recognize. Right, right. So I see. Okay. Thanks for explaining. Or questions? If somebody from the online audience has questions, they can ask them too. May I ask a question, Anton Torshiken from Moscow? Could you please give an example of a problem with different input states? As far Input states. As far as I understood, here you put, you solve the problem of either state is entangled or separable, and then you every time you input the same state or your which you can. You input the same coverage of states. Okay, it it it is interesting because I mean in this particular paper, which is a paper by by Francis Adside by the Patreon technique again. Matter is let me take a game at the full screen, okay? In this paper, the the very first the the aim was to have a sort of entanglement with this. And the training set was a set of squeezed, two-mode squeezed thermal state, which is uh of a of the Gaussian targeting. But then if you input uh sir, you input every time the same state in No, no, no, no, no, no, no, no, no, no, no, no, no. No, no, no, no, no. The way you train your system is the following. Your input state is a squeezed thermal state and you fix the temperature of your state, you fix the amount of squeezing as a promote and you inject the state into the system. Depending on the value of the temperature and on the squeezing parameter, of course, the system can be a prototype, is it superbable or it is entirely. Or it case entirely, okay? And you randomly sample your randomly sample your input. You choose a given set of possible values of alpha, a possible value of temperatures, and train the system. But in the first part of the experiment, you know the temperature, you know the screen, but you know beforehand whether the state is screening. The stage is squeezed, or if it is entangled, or not. So, what you say, the system is the following. You treat the system in the following way. And you say, look, I'm injecting an entanglement state. Look at the statistics and remember that it's an entanglement. Then you prepare a different state. And you inject the state into the system and say, your key style is a product of state. And you calculate this and you keep training the way. Training the way. Then you talk training, then you get a state. You don't know what our plate is, you know what the temperature is, and you ask, is the state a proper state or the entire state? And you will classify them. Of course, you test. You do a testing, but after the training, you do a testing. Now, the interesting point is that they've done the training and the tests for Gaussian states. Gaussian states. But then they pasted the state after training. They tested the system with the system which are either entitled or product, but which are not Gaussian. And still the system reasonably can distinguish them, even when not of the Gaussian functions. This is not tricky, but it's the nice thing. Yes, very interesting, but then I don't understand. Understand, so it seems that if you every time inject different states, then the memory in the reservoir from the previous state could disturb our classification process because states can be unrelated. Isn't it more logical to reset the several every time? There are losses. Look, there are losses in puns. So the losses in the pun act as a reset. So you must, before you get your So you must, before you inject your lip state, you must allow for the liquid water to relax and stay in the limited state. Ah, so infect the reset. Basically, the reset is the losses and the pump. There is no one. So you must allow for some child to be one induction on the next one. Ah, because I understood that that uh it will somet somehow we use a memory in the reservoir. And the question is there example where we can use a memory of the reserve bar? They somehow use a memory as a measure because what they have tried to do is rather than injecting a signal sign uh a signal s uh a single signal, they send two pulses, one after the other, on a face time which is a bit shorter than the intersection. That is shorter than the cut session. So, this way the process is somehow more efficient. So, they two passes one up to the other, then the weight, then two parts is part of the other and then the weight. But two people, two same parts of pulses. Exactly. Two copies of the same part. Okay, okay, thank you. Thank you for the interesting book. I have one more quick question. Just are you familiar with Latrois's work in terms of? With Latrox's work in terms of doing classification with a single qubit while repeatedly injecting the same classical data onto it. And they were able to give a formal proof that the single qubit is universal in the sense that it can do any classification problem in principle. In a way, that is kind of a reservoir computer, a very simple reservoir computer consisting of a single qubit. I'm just curious, Juan, whether you've explored any relations between these two paradigms. I don't know if the question does the V and the on the set side and say hi also. Fair enough. Then you can send me the question from the chat or by email and I'll try to answer it. Masimo, yeah, you can send please do send the slides and we can distribute them. Okay, fine. It is too bad you're not here because then we could talk in the coffee break. But now we can't and we have to stop here. Yeah, I'm approaching on being supple time, basically. Well, yes, exactly. Well, thank you very much, Masco. Very nice talk. Hello, Francois. We cannot hear you well, no? Can you say something? Better now. It is a little bit better now, yes. Can you share your screen?