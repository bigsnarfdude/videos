Yes, so sorry for taking the coffee break away. I don't know what happened. So I'm sorry that I got scheduled at this time. But I only can slide, so I hope there's a coffee break in between my talk and the next one. If I'm faster, I can create a coffee break. I can create a coffee break. Yes. So I only have 12 slides, but I have also never given this talk before, so you should never know. But uh So you'll never know. But this appeared on Archive last week. Okay, I'm very excited. This is with one new talented grandchild here of Peter Yasu, one of my students, Valid's first paper. And so my other two courses, let me say a few words. So Leonard is doing deep learning theory, and Matthew Traeger has like a natural electronic background, but then the But then did a PhD in computer vision, which we heard about today, or this week several times, and is now working in machine learning theory and Amazon. So I think of those two as our machine learning authors, and then Wahid and I, we try to wave the algebra flag. Okay, so slide one. What is a neural network? Okay, so the easiest setup in a neural network is Setup a neural network is, I mean, in general, it's in fact just a family of parametrized functions. So you see in these tiny pictures, this is what I call a two-layer network, because these three layers don't really matter. What matters is what happens between them. And there's two functions, and these functions are parametrized. So they depend on thetas, which are often, like, for instance, the weights that you put. Like, for instance, the weights that you put along these edges. I'm not giving a formal definition here because there are many ways on what generality you want to define it, but that's the general idea. So neural networks are parametrized families of functions. So mu will be the network parametrization map in this talk. So it eats this parameter vector theta. This parameter vector theta, and then for each layer there will be a function that depends on this theta, and you compose all of these functions, and then you get another beautiful, complicated function, and the set of all of those is what's sometimes called the neuromanifold, although it's not always a manifold, but it's just how it is in the literature. And of course the function space, I prefer function space since it's not always a Space, this is not always a question. Good, so then what you do typically with neural networks, you train them. So what does that mean? That means that there's some function that's typically called the loss function that you want to optimize. And so this is an optimization problem where we have this composition of functions. Where we have this compositional functions, which makes this a little bit special, and that's why it's hard to understand, essentially. So here we have the network parameterization map that needs parameters and outputs functions. And then you have some functional that you want to optimize over this complicated space that you typically don't understand. And this function often depends on training data. Okay, so when you actually do machine learning or with gradient descent optimization or something like that, then this optimization of course takes place in the terminal space. And so my point of view on the theory of machine learning with neural networks is that in fact we want to understand this optimization problem though, because this is where really the interesting geometry happens and this is where our critical points actually have meaning. So in theory we want to understand So in theory we want to understand this right hand side, but then connect it with the output that actual algorithms compute by considering this composition. Okay. So there is many ways on how you can bring geometry into this picture. So my two favorite questions, this is not a complete list, but my two favorite questions are those. How does playing around How does playing around with the architecture of a network affect the geometry of the space in the middle? And then, how does the geometry of that space impact this optimization problem? This is the type of questions I like to answer. Okay, question. No one's like this. Space M is a space of all possible functions, right? Yes, so you have to choose. Functions, right? Yes. So you have to choose the architecture, but also the family of functions you want. Yeah, I mean, with architecture, I mean, everything. It could be the number of layers, the size of each layer, it could be what activation function in case you know what that means happens in the middle. It could be what shape the layers have. There's different versions of what what architecture can mean. Whatever shape this could be a general graph, it doesn't have to be. Yeah. It's also the the type of function you consider, right? Yes, exactly. So that's still typically what is then given by this activation function, as they call it. So in this talk, I will not do this. In this talk, I will only focus on networks where there is no non-linearity. Okay. Okay, so a baby example. So the most So baby examples. So the most well studied neural network in the theoretical literature is those that are of course never used in practice. Linear fully connected networks and this is an actual example of such a network where now every layer is just a linear map. So this first layer here is a linear map from a four-dimensional to a two-dimensional space and then you have a linear map from a two-dimensional to a three-dimensional A linear map from a two-dimensional to a three-dimensional space, and this is what a linear neural network is. So, this network parametrization map just eats two matrices and multiplies them. Very easy. And so this function space is then, of course, the set of all three by four matrices that have rank at most two, because of this bottleneck or whatever. Bottleneck or whatever in the middle. So, this is of course easy to do in general. If I have many layers, then I'm just taking many matrices and multiplying them. And we understand the geometry of this function space very, very well. Know kind of everything about it, because it's just the matrices whose rank is bounded by the minimum dimension that appears. Beautiful determinant variety, and now machine learning. And now, machine learning. If you would do machine learning with these networks, you would solve optimization problems over these determinant varieties, and their singularities will be important. And so I'm not talking about this, but it's just like the warm-up. Okay. So what I want to talk about is convolution in networks. So what we wanted to take a step away from this list. From these linear fully connected networks that have been studied like 100,000 times, and look at sort of promising architectures that are used in most real-life implementations. So we still stick with networks that don't have any non-linearity, so they are not so practically relevant, but almost every network that works well in practice works with convolutional layers and not fully connected layers. And so we wanted to understand essentially. Understand essentially, I mean, what is the impact of convolution? What can you prove about it? Because I might as well, in theory, study these winger networks, but now change just from fully connected layers to these convolutional guys, see what happens. So, a convolutional network, this is a tiny example, now has the property that not all edges exist anymore. And there is some weight sharing. So these two edges are, for instance, both yellow because they have the same parameter that is trained during the training process. Okay, so formally this means that this network parametrization map, I mean this is still just linear maps, this is still matrices that I multiply, but now my matrices have a special shape. They are not allowed to be arbitrary matrices anymore. Matrices anymore. These are somehow generalized tribulus matrices, right? Because I am allowed to shift by something larger than just one in every row. And every row has sort of prescribed entries which are allowed to be non-zero. So these colors correspond to the weights on the edges. On real edges. So, from now on, in this talk, we want to study network parametrization that's mu, that multiply a bunch of matrices, but the matrices are generalized to this matrices. Okay, so here is that's just what I said in general, and in general, these matrices look like this. The entries that I didn't write, they all have to be. The entries that I didn't write, they all have to be zero. Fixing some terminology from the machine learning community. So these matrices are, that's why I call it convolutional matrices, since they represent convolutions. Then by how far you shift to the right is called the stride. So this matrix up here has stride 2, since I'm shifting twice to the right. And these norms. And these non-zero entries in a row, they are called the filter. And I mean, what the convolution does, now I didn't put this on the slides, but essentially, like, this takes a filter of length three, so you can imagine like a block around these first three nodes. Then you do a linear operation, and then you keep just repeating this in your operation by shifting the filter down through your vector. So, this would be a very long vector. I would take this filter of size 3 and first impress. Size 3, and first of all, those 3, those 3, and so on. So it's like a linear graphic. That's what composition is. Yes, yes. I just wanted to make the connection to normal matrix multiplication and normal construct matrix less helped. Thank you. Just reminds me this time there's a problem in the geometry between the 27 lines, so then you compute this just a matrix of that. And of course you can do this in higher dimensions, yeah? I mean in this uh talk I will only focus on one-dimensional contributions where I am a Dimensional convolutions where I am applying convolutions on vectors. But you can, of course, imagine that your input is not always a single vector, but in fact, a matrix or a tensor. And then each convolution will also be a big tensor. Yeah, so this makes sense. I have allergies. Okay, so now how does this connect to sparse polynomial factorization? So this is the one technical slide where one has to be a little bit reactive to not get lost for the To not get lost for the rest of the talk. Okay, so what is well known, of course, easy observation, is if you have a bunch of convolutions, their composition is also a convolution. Okay? And the stride of the convolution that you get as many compositions is easy to compute, adjust the product. So it's a little bit less easy to compute what the filter of this new convolution is. This new convolution is. And the way I like to think about it is: I don't like to think of matrix multiplication, I like to think of polynomial multiplication. Okay, so what we do, I'm sure this must be also have done a thousand times before, but we haven't really like maybe in signal processing literature. I don't know. In any case, how I like to think of convolutions is. How I like to think of convolutions is that for every filter, so these W's, where there are the entries of such a convolutional matrix, I identify it with a polynomial, a polynomial whose coefficient vector is simply W. Okay, but this polynomial, we have to be careful, I will do, it depends on some other number, capital S, what type of monomials I want. I want. So, this is like, I don't know if this is a good notation. If you have suggestions for better notation, please let me know. So, this is polynomials that are not just in the variables x and y, but in x to the power of s and y to the power of s, and in those variables, have degree k minus 1, where k is just the filter size. Does that make sense? So I'm just assigning for every vector. For every vector polynomial, but I'm not using like standard polynomials, I'm changing which monomials I like to have. So if you do, yeah. That's the way you write like we have like to be homogeneous. Yes, yes, yes. When it's K, I just want in those powers. Yeah. So if you do that, okay, here just the technical work. I mean, if I think of a convolutional matrix with filter W, then I also want to apply this map, not just that it assigns vectors to polynomials, but also matrices to those polynomials. And then you can write this beautiful equation. This is the important part. Apart that this map mu that just takes the product of convolutional matrices is in fact just polynomial multiplication, but you have to do it in this correct way. So look at this, let us look at this one again. Suffrain seems to notice very well, but maybe some other people don't. So you take your vectors, W, it's scientists to conclusion. Is signed these convolutional matrix. If you take the product of those and interpret that big new convolutional matrix as a polynomial, and here you see this is really just the usual polynomial. There is no exponents involved here. But then, if I want to interpret this into different factors, I have to identify each of these single factor filters with the correct exponents. With like correct exponents. Time of difficulty because of W's of index. Like up there, the WI are coefficients, and now you have different. Oh, that's true. But just saying, should be better next time. And the k is all the same? Yeah, so no, these can be different. I try to not have so much rotation on the slide, but that's also the interesting thing, right? So I need so this map, in fact, depends always on the capital S or on the P. Always on the capital S on the K, but you kind of know it implicitly by whatever vector W you plug in. So these W i's might be vectors of different sizes. Each layer might have a different size. And so you get different degrees. Silly question. So in the observation above, the order of the T's matters because they're matrices. Right. But when we do the pi's down here, But when we do the pies down here, you draw them the same order, but yes, exactly. So, that beauty of thinking about it, I mean, there's many beautiful things of thinking about polynomials, of course, but one thing is that you realize that the dimension of the matrices just don't matter at all. The only thing that matters, and so this means that essentially what size of these layers are, it doesn't really matter at all as long as they are compatible with each other. The only thing that matters is these. That matters is these drives, these are the small s's, because they are giving me these big exponents. And what matters is the size of these vectors w. Because this will now determine the degrees of these polynomials. Okay, so in other terms, this value formula here tells us that we can reinterpret the network parameters. The network parametrization menu as follows. It is just polynomial multiplication, but polynomials of like a special shape. Okay, and so the function space of a linear convolutional network, you can think of it as polynomials with very specific sparse multiplication. That's essentially the title of the talk. Of the talk. Now we want to do stuff, of course, with this observation. So here's again the map. Okay, so first, okay, this is an easy proposition, nothing fancy going on here. So this function space, again, it's just a set of polynomials with a certain sparse factorization. This is now a semi-algebraic set. It's not a variety anymore, like in the fully connected world. It's a semi-algebraic set. It's the semi-algebraic set, which is at least close to the New Leader topology, which is nice from the visual perspective. And of course, we know its dimension. Here is some example picture. So you see this here is degree 4 polynomials that factor into a quadratic term and into another quadratic term, but the mixed term in the middle doesn't appear. That looks like opening umbrella. Yes, it is. I mean, so of course, this is a slice. Yes, I mean, this is a five-dimensional vector space. So, we took like some nice slices that exactly argue with me. And brother, very good. Yes. I was waiting for that. That's a pinch point. Exactly. So, yeah, so these guys generalize Whitney Apple as to see, since we know that Whitney Apple is a really example of a complicated As a example of a complicated similar algebraic set, these guys are fairly complicated. Like, well, these are not so nice anymore, like determined bytes. And so, this function space on the right is very much related to the one on the left. It's also degree 4 polynomials. And the only thing I do here is I take this quadratic factor and I want it to split up further into two linear factors. That's why this is a smaller thing. And the data line is the boundary. Exactly. Is it easy to say why? Yeah, it is kind of easy to say. It is just because, so in fact, you could, I mean, this map is also well defined over projective space. Okay, so this is a, and we can just projectivize it. Okay, and then you have, of course, that if you look at the If you look at the, I mean, to study all of this, right? You go over first to the complex numbers, and then we will see you have that the image of projective morphism is completely enclosed. And then you just because it's a projective image, so there is nothing deep going on here. I mean, for machine learning people, it's very deep, yeah? If you get super classified by this and want to know how to prove this, I think yeah, for us it's also the image, it's also For us, it's also the image, it's also a complicated. I mean, and the machine learning people, they want to understand, of course, if you are familiar with, for instance, the Netflix problem, where you have low rank, you want to low rank matrix approximation, right? And so you kind of want to find the matrix of lower rank. But sometimes or often it happens that you find in the optimization solution an even lower rank matrix. And this can be a feature, right? Because it tells you something there's more special. Because it tells you something, there's more special structure going on. So, in general, if you do optimization into these sort of sets, it's interesting, but can be a feature or a bug, depending on what your application is, if you find critical points that are, for instance, either on the boundary or that are singular points, because they often correspond to more like special information, right? Possibly it is Okay. And so that's essentially what we wanted to do in this paper to understand. Things like singular points and special points about it would always be purple points. The question is whether they have their office or not. Actually, well you have to be a little bit careful. It actually was because these guys are very low dimensional things in like a large dimensional vector space. So it is actually not true a priori that boundary We will read that boundary points are created points. We in fact show that it's actually kind of, I mean, as usual, optimization if you have something that's not singular. You throw in singular locus, you put in your optimization of that. Right, but the thing is, this is automatically resolved because in the machine learning problem, right, there's always the thing we do the optimization in this big parameter space. And then here you have And then here you have this set. Exactly. And so the question is: I mean, yes, so in sort of most mathematical papers, we kind of start with this. But if you want to give answers that machine learning people care about, you have to understand this. And then it's like the question: what type of critical points, if you compute all critical points in parameter space, what type of points are they over here? And this heavily depends. And this heavily depends on this geometry of the space. And this is essentially what I want to see on slide 12. So, I mean, these tend to be vastly over-parameterized. Yes, they are. They are. That is critical manifolds in their capital. Okay, so our goal was a little bit to describe the singularities of these semi-algebraic sets to see as much as we can about the boundary, to try to understand these special points. Try to understand these special points and then give like a first answer on this optimization question exactly. Like critical points, what do they correspond to? And we have a very surprising result. I'm very excited about it. But let's just get there. So this example, let me go. Oh, I have it here. It's a little bit technical, but this example is a good illustration of what I'm doing on this slide. So as I said, that these architectures on the top are very much related. The top are very much related to each other. In fact, on the right-hand side, I only require that this quadratic factor splits up further into two linear factors. So now, if a very complicated, you know, many layer architecture walks through the door, I want to do the reverse operation. So I have many, many, many, many layers. But If sort of these exponents here are the same in neighboring layers, I want to merge them. And this is what we call reducing the architecture because we make less layers and we make the architecture easier. So this is what's happened in the AI model again. Here is like the architecture on the right-hand side. I merge these two together, these two layers, by just multiplying those polynomials first. Those polynomials first, and then I get another polynomial multiplication map down here. And what happens when you do this is, of course, that possibly this function space gets larger. Typically gets larger, but there's a risky closures are the same. That's the upshot of it. So if I first want to understand the risky closure of these guys, I can do this merging procedure to make architectures. To make architectures where I then can assume that all these exponents are getting strictly larger. And that's very helpful. Is this clear why it's the same culture? So we prove it. I mean, I want to, in this picture, you cannot see a... Surprise. It seems what you're doing. I mean, there's the fact that if you have a polynomial that factors absolutely being over C, it may not factor over R. And I think that looks like the difference. It is just the idea that J. But complexes. Yeah, zero closure. These are you took a smooth point of this as a smooth point of the complex variety, so they have the same zeros closure. Complex objects, the real subjects. But the idea is just that n not every not every uh real polynomial of degree two factors into two real linear forms, but it does factor into two cognitive forms. You have these open regions, like You have these open regions, between open regions, that was a risky dance. So if you take like but you have to prove something because we're doing like something with all these layers, right? So you still have to be a little bit careful because we have like many different layers, this merging procedure, but it works. So here, I don't give a technical definition on this slide, because horrible to write down the numbers, what happens when you do this merging, right? But the idea is just to merge neighboring layers. Just to merge neighboring layers with the same exponent vector, we get a new architecture. So this D and S is just the tuple of the degrees of the polynomials, the tuple of the exponents. Now I get a new one. I always write tilde for this new reduced thing. And now I can assume that these are strictly getting larger. So sorry. Okay, okay, so I guess originally you had them sorted, not quickly sorted, now you're thinking. It's not a restriction yet. I mean, you're saying given an architecture, I can do this procedure. And then these are related like this. So in general, the function space gets bigger, zariski closure stays the same, and with Zariski closure I mean in the real polynomial space. But if I want to understand the singularities of this function space, then I mean we do this by understanding the singularities. And we do this by understanding the singularities of the Ziriski closure. And then you can just now restrict to this assumption, because you don't lose anything. So that's what I say here. So now I want to describe the similar. If I have now a reduced architecture with L layers, I can tell you all the similarities. It's very nice. So of course, if my architecture has a single layer, Single layer. This map is just the identity, so nothing happens. So this function space, there's a risky closure of it, is just the whole point of it. And so that a reduced architecture has a single year happens exactly if in like the original non-reduced architectures all strides were equal to one. If all strides were equal to one. And this seems like very difficult, but we wrote another previous paper, a whole paper, like 45 pages in Siaga, only about those architectures and about that optimization problem. And then it looks like this. Then you really have some sort of ambient space and then you have the full dimensionality algebraic set. And when you do optimization, you very often land on the bottom. Then on the bone. That's how this case looks like. But back to this paper. So, in this paper, we were really interested in what happens with arbitrary strides. So, now, first observation, if I have in the reduced architecture more than one layer, this picture is always false. So, the degree of this variety is always large and bottom. So, it is never a vector space unless Vector space, unless in this sort of strike one case, otherwise, it's always going to be something of larger degree, and in particular, it has to be something smaller dimensional in the ambient space. It's like a low-dimensional variety of larger degree than one. And its single low cross is actually very explicitly described with it, but took us like a year to figure out. And I don't think we were very clever when we Uh I don't think we were very clever when we started trying this. But uh yes. So this uh these are in fact also always singular and zero simply because these are refined codes. So these are can never be completely smooth. But some of the interesting singularities is just a union of all function spaces with the same reduced stride sequence that are strictly smaller. That are strictly smaller. That are strictly contained in the one I'm looking at. So you basically stratify these spaces? Yes. But this is a very nice description. Yeah, yeah, exactly. So it's very nice that it is that analogous. You can really see the single analogos is just sort of all function spaces that are strictly contained in the one you're looking at with the same sequence of strides. And you can choose to. And you can choose to either write this union and when you do this ratification, like with the closure or without. I can always think of projective, right? Yes. Yeah, yeah, everything is projective. And then you can, of course, notice you didn't write the paper in the projective language because then you need to get it fixed. I mean we do the proof projectively, but the statements we try to not to project. Okay. I mean then we don't read it, so. Okay, so this is very nice. And then you can even, I mean, okay, so is it easy to understand which guys are actually contained in the function space that I'm looking at? And it is not, I mean, okay, this is not technical, but this is a list which machine learning people like, yeah? Because I mean, now this is. People like, yeah, because I mean, now this is very computable. I mean, this is just backdoors that have to satisfy some equalities, okay? So you can check it. Okay, I have an example. I have an example. Basically capital I for all I. So here I am summing up all layers, and here are only some starting from some. Yeah, this is a small electricity. Yeah, yeah, yeah. I mean, it sounds like it waves. I mean, the idea is just basically dominant order, but the partitions are di times. No, so the idea is in this example, yeah. In this example over here, so this is this is on the left. Again, this is the free. This is on the left. Again, this is degree four polynomials I'm using here my fancy notation. That factor lies like this. Let me rewrite this. You have ax squared. And then you have So this function space is just all polynomials of the before with this vectorization, okay? This vectorization, okay? And now, also, this criterion on the previous slide just says that you have sort of these interesting hyper rules, like these are not really moods, you know, but they are like in these x squared factors, and we have a single one of them. But if we now have more than expected, then we would Then we would get the singular locus. So the singular locus is just a single function space in this example, namely when you have two such factors. But this is the single locus of the whole Zariski closure. These these do not necessarily have to lie on the semi-algebraic set. On the semi-algebraic set. Those that lie on the semi-algebraic set have to be of this form and of this form at the same time. And that means that B has to be zero. You can always explicitly understand which singular points also lie on the semi-algebraic set. Okay. Then a little bit about the relative boundary. About the relative boundary, so now that we understand the single points, we do not have a closed-form description for these relative boundary points. We have it in the case of strike one networks from this previous article, and we have something for like two layer networks. But otherwise, we can only just say a little bit about their geometry. We understand when this relative boundary is empty, so we have precise but very technical criteria in terms of these numbers, D and S, when this is empty. Okay, but what is somehow cute, what I want to point out, is that there is somehow two different types of boundary points. So remember that we had this sort of idea that starting from any function space, I can do this reduction to get a larger function space, which is then contains a risky closure, which is the same. So the definition of relative boundary is that you consider points in the function space all the way here on the left. Function space all the way here on the left that are like limits of sequences in the risk-based project. But now I have this containment here, these two containments. So I might distinguish simply where the sequence goes. So I could either consider sequences in this complement or sequences in this complement. And this gives me somehow two different types of values. Somehow, two different types of boundary points. These we call reduced, because if I have a reduced architecture, this is just inequality, and only these yellow guys appear. If I have a non-reduced architecture, for instance a completely strike one architecture, then only those orange guys would appear if I only have strike one. Okay, and so what is typically happening is, as in this picture, so the reduced boundary points are small. Reduced boundary points are small. We have a better result, stronger result than what I've written here in the paper, like we can estimate the dimension of this boundary. But it's always at least co-dimension two, and that's the pinch point of the Whitney umbrella. It's the reduced boundary point. And these other that come from like being non reduced when you do this uh merging of layers, those are if it's they exist at all, they have codimension boards. Do you have four dimensions points? Okay. Is that because the reduced ones are limits along singular, along singular points of the complex right? Well, the reduced ones in this case, they're reduced to because they're limits along points that are singular. Right, so in fact, these reduced boundary points are completely contained in the singular locus. But the singular locus can have codimension one. Sure. But the reduced boundary points are then. The loose boundary points are then at least could imagine what in this single to understand this slide on boundary points they arise because you simplify, you combine their inequalities again. So this is like an easy thing of just taking a polynomial of degree D and then chopping it up into uh polynomials of smaller degree without thinking of funny exponents or something. Like the standard. And they're similarly discriminant in your looking outside of it, so that's not the idea. In this example, here this was we were thinking of degree two polynomials that you then want to factor into two linear polynomials. So with the real numbers, you cannot always do this. There is like some open set where you can do it. And then you have to discriminate which is the boundary. And that's And that's yeah, so the boundary is a little bit more complicated in this case. There's like some semi-algebraic subset of the discriminant, and so you can study which parts. Okay. So the boundary might not be normal. Well, things are normal. Nothing is normal. This is not normal. This is not normal. Yeah, this is definitely normal. It could be normal if the if this boundary sets that the body boundary points is anything. Yeah, yeah. So the boundary is empty and we do sometimes we have exact criteria when it happens. So but you can there is a we have precise criteria on just like really like the sort of table when we have a table which says oh there's like six different cases like the architecture could be full dimensional it could be or not it could be smooth except for zero point or not and Or not, and it could be the risky close or not. And so we have like a table where this happens. We'll show like minimal architectures in this case if you want to study them experimentally. So I wonder if this geometrical constraint, for example, would be some constraints. Yeah, so that would be my second question. What type of geometric features of the function space, well, what type of impact do we have on the optimization? And so now I want to. Optimization. And so now I want to do this. My last thing is exactly about this optimization, this result that I find very surprising from the machine learning perspective. So we did a theoretical study on training such networks with the squared error loss. So the squared error loss, if you can, you think of those as just matrices. Given data, you just want that the matrix times the input. That the matrix times the input data is the output data is to do many robust. But that would be quite sort of the optimization problem here, which is often studied in math papers. But now what you actually optimize is the squared error loss on this parameter space. So we eat all of these filter vectors and then apply the loss. Applies a loss. Okay. Okay, now finally, this theorem that I'm very excited about. What we show is that if you have sufficiently endless number of data points, let us assume for a moment that this architecture is reduced. It can also generalize it, but let's get more technical. So, for reduced architecture, and I have enough data points, enough is just the Points, enough is just the filter size of the final filter. Then, for almost all choices of data, almost all in the algebraic sense, except in some low-dimensional locus, every critical point here in this parameter space satisfies one of these two conditions. Either it maps to zero, and those you can easily detect. And those you can easily detect because that would mean that one of the polynomial factors is zero. Or, and this is the exciting part, this critical point has to be a regular point of the parametrization map and its image here is in fact a smooth interior point. And as I find super surprising that a single up, we put like a That it is simple, we put like a year and a half work to understand the boundary and the singularities, and then at the end we prove well, actually you don't see that. So, this is like a Burgini type of error to these linear functions again. That's my very question. It's a genericity. It says they're general at all. I mean, this theorem is false if you drop this reduced assumption. So, I mean, this is what I think Nixon. So, I mean, this is what I think Neeks is also really surprising, and why one has to be really. So, also why I want to point out why is it so nice that these are regular points of μ? Because that means that if you have a regular point of the parameterization, then you are an actual critical point of this function we like to study in math papers. But this is typically true for other false architectures that you study. So, here this theory was studied. Theorem was studied in different versions before for easier architectures, and then it's false. So, these famous linear fully connected networks, it's false. And for strike one case, we did in the previous paper, it's also false. Because here the function space is a variety. There, it's a full-dimensional single algebraic set, as I was drawing in the board. In this left example, critical points actually often are singular points. Actually, often are singular points. That's the Netflix problem. Here, critical points are often on the boundary, as I was drawing on the board. But once you do these reduced architectures, we don't want strike one anymore, neither of this happens. And also, in both of these well-studied architectures, critical points are often spurious, so they are very often not critical points. So, they are very often not critical points of view. And there's this whole thing. I mean, machine learning people like René, Vidal, I don't know who means in machine learning theory, in this sort of framework, you want to prove that linear networks converge. And there is the whole difficulty is that you have these sporious critical points, and people understand they are like saddles, and what type of saddles they are, and why gradient flow avoids them. And here, if you want to do this convergence analysis, we haven't done it. This convergence analysis, we haven't done it because people just on my curve since last week, but you don't even have to worry about it. So I find this result very surprising. In fact, now it does not sound shocking, but there was lots of questions already. Yes. 