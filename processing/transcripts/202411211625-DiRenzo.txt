I take advantage of many interesting talks and conversations that I had with you during this week to slightly reshape my contribution. Not the subject, but just the goal. I try to focus more on what prospects we have for 05 and what are the challenges that we need to address. Oh, by the way, Francesco, I do directive characterization for Virgo. Characterization for Virgo, and specifically, I, whose support for transient search is coordinated the activities of the rapid response team and the event validation. And this is what I'm going to talk about today. So, first off, I'll start defining, describing what I mean with data quality issue. We had many examples. Probably I'll try to give a more, a broader description briefly, still. And then I will describe how we do address this. How we do address this data quality issue. I will introduce the framework that we use, the data quality report, and I will consider specific case that is doing rapid response to gravitational wave transients and event valuation. And very briefly, touch which subtraction just in terms of demand of personal power, just not overlap to what already described there. Lastly, I will mention what Mention what this will scale up, will be needed to be rethought for four hundred five when we'll have a cascade of events and many of them having data quality issues. So let me start with Virgo O3 noise budget. This is just to scare a little bit people used to super smooth design sensitivity work for doing their analysis. For doing their analysis. But I'm not going to talk, sorry, Rabian, I'm not going to talk about the nice budget contribution. Second? We can talk about that already. How so? It is a three, or four, a little bit different story, sorry. So let me start focusing on this flattened curve out of the full the real director output. This represents the baseline noise. This represents the baseline noise and in many ways represent our upper limit to the sensitivity, our best sensitivity that we can achieve. So the best DNS range that we have from out of our manufacturer. On top of it, there are these, oops, spoiler, there are these features, these spectral noise, which is itself a sort of kind of data quality issue. And here I'm reporting a sort of equivalent definitions. Sort of equivalent definitions of what is a data quality issue. One definition that I think people used to do simulations on smooth core will appreciate is that everything not included in this blue curve is a data political issue. This comprises, of course, spectral noise, which often is non-Gaussian, is likely to be non-Gaussian, non-stationary, and of course glitches, transient noise that is not measured. Measure represented by sense amplitude spectral density is a little bit average out. So you don't see bleaches here unless they are very loud. So the data quality issue is everything not in this blue curve and which can reduce the performance of searches and in general in analysis. How do we address the quality issues? Well, we have a bunch of statistical tasks. Tasks, if you prefer, where we try to find this muscle in the actual noise. I will briefly cut some of them. It's a non-comprehensive list. I will try to just give you some classes of them. So we have this test which compares different estimates of the power spectral density in a super short, quite short window around the time of event. Event with longer estimates. And if there are statistical differences between the two, then this returns a p-value of having no stationary noise, the vicinity of the event. Similar, to some extent, we have this test which considers the normalized energy, which if we have special and Gaussian noise, this energy should be distributed like chi-squared. But if we have Squared. But if we have excess energy, for example, from a signal or from a glitch, then this test returns a p-value, a small p-value, so the test is failing. We have no stationary nodes. Particularly, there are two, actually three outputs. One is the full data, which includes the gravitational wave signal. The data with the subtracted template, the best match of the signal for the signal. The best match of the signal should be white. And another, which is the energy in the vicinity of the cherry track. There are other tests like the Raleigh test, or a test based on the band-limited area mass, which try to, which was purpose is to verify the hypothesis of stationarity and Gaussian in longer segments of data around the time of the events. Probably not. Next, I will not spend too much word on it because Mervyn already did a great presentation. There is this pinette tree, which is a multi-level classifier. So, in a single spectrum image of the data around the time of an event, or put at the time of an event, it classifies excess of energy of being a chirp or a glitch from some of the classes described by gravity supply. gravity supply. Let me also, yeah, put in action also the auxiliary channels, which are channels monitoring the detector environment, monitoring the subsystem, the parts of the detector. And a class of tests that we do is to compare the energy in the strain channel with that, the excessive energy in the strain channel, with that in the auxiliary channels. Auxiliary channels at the time of an event. And if both channels have excess energy, glitches, hopefully, glitches in the auxiliary channels, then we plague this Zirturnet and we examine whether this overlap can be this excess managing the oscillating channels can be transferred to the secret. In this case, we evaluate whether Whether what we see in the strain is from terrestrial origin or is actually astrophysical symbol. Similarly, we do this comparison using the coherence. We have a Brucco, again by Gabrielle, and we use it to compare the coherence of the strain with a series channel around the time of event before and after it. If there are differences, for example, excess of coherence at the time of event, well, this is a sort of At the time of event, well, this is a sort of red flag, and we want to examine in more details if what we see in the string is not gravitational waste at all. I think Derek also mentioned this test which is called pen check. There are at the various sites we estimate coupling function, coupling of some environmental noise to the strain. Environmental noise to the strain channel. And we have this task that evaluates if ballistic projection of environmental noise to the strain channel. If the projection, the contamination of the strain channel from the signal in auto-monitor by environmental channel is above a certain threshold, then red flag. And more analysis is done by environmental people inside. This task is mostly by Adrian and me. Mostly by Adrian Emmy Cornell, I think also Patrick doing it related to it. All of these tasks and many others actually are collected, let's say, in the data quality report framework. This framework is a sort of collection of tasks. It's typically triggered by gravitational grade alerts. And this is a scheme taken from the Gregor tree paper, the short paper. Paper, the shared paper. So, when an alert for a significant event is produced, a label, data point EQR request is applied to that alert, to that super event. This triggers the production of this data quality form with all of its check. And then these are sent back to the GASDB page of the super event are just some number for. Are just some number for virtual doing no three. I guess that main number is here. The prompt checks are completed within five minutes or so. Similar performance work. So there are actually two instances of the data quality report. As I was mentioning, there is the Virgo data quality report, more or less in the same shape that it used to be in the tree, except for some. Except for some optimization and improvement, improve in the speed that all tasks are completed. And the Iguini QR, which is the main information that we have on Superiment pages for checking the quality of the events. If you are a little bit redundant, in the case, for example, there is an issue at CAT or at T for Computing Center, one can check the the output of the task in one of the two. Of the task in one of the three. And the DQR is organized, the big one DQR is organized in sections, one for each detector, where you can see the name of the task, of course, this full result of that, and a summary label at the end of it if the task has passed or has failed. There is a few issue. And let me start with. And let me start with one of the first questions that I have in this presentation. So, would we like to add this thing a little bit more automatized? Because at the moment, for example, rapid response in shifter, we'll mention what is this person in a moment. We'll check the label and in the case this is this thread, this is a Q issue, it will contact the corresponding. Will contact the corresponding DECHA expert. There could be some reason to have more automatization of this process. For example, adding more automatization to reduce the burden on call DHR experts, reduce the activity, well, automatize the activity of level zero rapid response team experts. I will mention more, I will provide more. Mention more, I will provide more context for this soon. Oh, and additionally, yeah, I put here a sort of schematic neural network representation just because I had fun putting there too many lines, but this is just symbolic. I did not enforce. I mean, I don't know if I want to enforce a neural network structure. Following the approach by Gabriele, you just need to work. It just needs to work. If an algorithmic approach works better, awesome. Explainability is also important, I just said. No need to follow the fanciest, newest algorithm, as Marco mentioned yesterday. So whatever works, works. And how do we do, where do we use the quality reports? Well, they are mostly used for the validation of transit. Mostly used for the validation of transfiguration of wave events. Here I'm reporting a schematic block representation of what happened in Lolaten SIG from the identification of significant gravitational wave transfer candidate to the alerts sent out to the astronomical community to tell, oh, we have identified something of interest and we trust what we are sending out. In this process, an important part is represented by the act. The important part is represented by the activity of the rapid response team, which is the first in-loop human batting of diesel vehicles. So people from the rapid response team are from various groups like the LAF and C, the SHAR, pipelines, etc. They check that what has been produced automatically, up to a year, possibly, don't tap pipeline issue, data quality issue, and other kinds of issues, which will reduce the credit. Which will reduce the credibility of our learners. And if this is the case, they will bet it. And again, all of that, as represented by the pictures at the bottom and this title, all of that is done to enforce really weak and the alerts that we sent out to the astronomical community in order to be prepared promptly prepared to the next multi-massage observation or rotational waste signal. Here I briefly summarize what's the organization and timeline of the activities of the rapid response team. I guess that the two takeaway messages from these slides are the latency. Median time between the alert and the sign-off is half an hour to in O4B. Just be a little bit longer in O4A, but yeah. But yeah. A time that can be improved. Maybe astronomers would like to have it a little bit shorter. One reason to automatize more reactivity of their level zero shifters. And yeah, just to clarify what I mean with level zero shifters, I guess that most of you have already been exposed to a similar slide on other occasions, so no need to repeat it too much, but this rapid response team is organized in a three-tier system. Team is organized in a three-tier system with level zero shifters covering 24-7 for alerts. And up to now, we have had more than 500 participants to this activity, so a considerable amount of people from the FPTA collaboration. To support them, we have 70 experts, which are experts from pipelines, from Dechar, from Pipelines, from that chart, from various tasks that we don't see pipeline, which are on call. So, for example, there is a problem with GW salary. Somebody will call Cody or Roberto or somebody else in the middle of the night, as happened a couple of days ago. And yeah, I'd like to compress all of them. So as I was saying, take a take a message from these slides, the latency and the huge amount of people involved in this activity. In this theme, I guess it's a little bit tiny from back in the room, but this is just to represent what the level zero shifter is supposed to do. This flow chart, this workflow, doing various checks, specifically checks that the candidate event is not high profile, meaning that it doesn't require an additional level of attention. An additional level of attention scrutiny because maybe it contains a neutral star, so it's likely to have an automatic counterpart. So much more attention is required to that. And additionally, there are various other checks, including the check of the VQR for the presence of quality issues. If after all of this check, the RAP response team decides whether to Decide whether to confirm the alert, so send out the initial server, or redact it if something went wrong here. And here, yeah, it's an outcome of a discussion we had with Derek last dinner. So a general principle is that if everything can be codified so well in a workflow, then why we cannot automatize or automatize it to some extent? Standard. This will certainly reduce the burden on the 500 people that are serving on it. And there are other advantages. So we can move these 500 people doing other things, in other aspects of the reluctance, which are less, requires more personal power right now or in the near future. And additionally, automatization. Additionally, automatization will ensure greater speed and consistency, uniformity in the decision done during this rapid response. Potential drawback of that, I'm saying this as a person who did the training to about 200 of these participants, is that, well, there is a sort of educational aspect in people, early career scientists, students, participants. Career scientists, students participating in this activity, and because they get exposed to various aspects of relativity searches, HR, calibration, detector operation. So automatizing it will get away this positive educational aspect. Just to be fair in pros and cons. After the rapid response team, another another Another level of validation is the let me call it proper event validation, which is done after the candidate event is identified. And this is a sort of second level of scrutiny, a more thorough analysis of the presence and the effect of data quality issue. This takes advantage of additional DQ tasks, data quality tasks, DCR tasks, sorry, which run Which run RN latency, and for example, some correlation. It takes more time to be completed. And typical time scales for one person to do the validation of one event is one hour. But I must say that this number has a huge variance, and it's one of the problems that you write. Let me finish describing this, then I will. Describing this, then I will highlight the problem. Currently, the rota of event validators comprise 40 people plus, I will say, five involved in the review and organization of this activity. Of course, as already mentioned by Derek during the presentation, if a data quality issue is identified, if a bleach is identified, then we could demand for bleach subtraction, which has been one of the other topics discussed. Which has been one of the other topics uh discussed in this work workshop. Other question: Do we want to automatize more this process? Well, some of the reasons are that maybe we can avoid or partially avoid a second evaluation of the QR. This can be a little bit redundant, so we should be against redundancy. Second, um, well, um Well, organizing this activity requires a significant amount of resources in terms of time and training. Of course, who participate to event validation requires additional training in respect to what is required for rapid response schemes. So this is a significant amount of time. And also, review requires a lot of time, and this is because often the evaluation of the impact of data quality. Evaluation of the impact of data quality issue is a little bit difficult to determine, to address. And as I was mentioning in the slide before, typically evaluation of one event requires one hour, but another validation which are usually completed in five minutes, other in two hours, because maybe the specific event is more tricky or the validator is more thorough, is more precise in their choice. Precise in their job. But this creates sort of large disuniformity in the output, in the recommendation, final output of the validation, and additional time for the review. So automatizing, at least in part, this process will guarantee uniformity, greater speed, as I said, one hour for the validation, often up to Often half an hour, 15 minutes, depends on the quality and the application for the review. So resources can be allocated elsewhere. At the end of that, there is artifact mitigation, bleach structure. I will not describe this again. Let me just focus again on one take-away information. This is from Audrey, of course. I'm not making, I'm not mentioning OT. I'm not mentioning 04. And in 03, we had 60 events, 20%, about 20% of the events with data quality issues that require subtraction. And the time for doing the subtraction with bias wave, doing the review of the results, and often iterations of the process requires a lot of time. I roughly estimated a couple of weeks, but this can depend on the case. Can depend on the case. Again, time that can be reallocated elsewhere. And this, quite quickly, is a sort of timeline and summary of various validation activity, 2000 activity with some people icon on the basis that require in-loop human intervention, which are the rapid response team minutes after. Minutes after a significant event is identified. Online parameter estimation, of course, event validation, and glitch subtraction. So one of the messages that I would like to touch with this presentation is whether we can right now and of course in provision of five change something, automatize more in order to move resources where they will be moved. Resources where they will be more needed. And these are some final numbers to explain why this is pretty much needed. So as of now, you know, four, we have 160 unretracted alerts from candidate events from online searches. 178 total, which means one every 2.6 days. Considering the rate of mitigation. A rate of mitigation of twenty percent, this means that thirty two candidates right now need noise mitigation. And how this scale to 05, sorry. In 05 we'll have a rate of 1.3 to 3.2 events per day and one mitigation needed every 1.5 to 4 days. If they still require Four days. If they still require a few weeks, it will be problematic. And yeah, I'm the end of the presentation. This is my conclusion. So address, try to understand how we can distribute personal power for the next observational campaign. Understand how we can automatize many things. And yeah, we already started doing this with many people that started. Many people started thinking that to automatize the flowchart of the record response team. So let's hope we'll have some tests by the end of October. And another advantage of presenting the last day. Thank you all. I took the picture of this conference. So yeah, time for questions. Michael. Yeah. Again, still the right way for your team to be communicating workflow. I guess I asked this question because Twift is an example. And I'll also say, here's the first attribute, here is the link to the complex web page where you can find information about that. But there's a world where your sort of EQR stuff appears in the context of the CLTR but just going to be You can see also that just sort of click on a link and you can look to see what the DQR is. Yes, as we look towards SO5 and sort of ways we're going to communicate with our what you thought that should be looking like. Well, to have communication with your astronomer as fast as possible, I think that the elevation of data points should be automatized itself. Of data points should be automatized. So I think that, well, I'm not sure on the format, I think that the circular, well, notice in circular should be there, but if possible, have them with the least possible supervision by humans. Makes sense. I don't have yet an explanation, a clear idea, a clear view on to what extent this must be authoritarized because coming from Automatizing because coming from that chart, coming from when we spent months on the validation of a single event, I'm probably a little bit redundant in giving the whole process of decision to an algorithm or a machine learning algorithm. But ideally, this will be required because it will require a lot of human input, otherwise, to put. To address what were the numbers, like three events per day, three tellers per day. So, yeah, not a comprehensive answer, but just some feelings. Thank you, Francesco, for our next talk. I have a simple question about the workflow of inventory validations. So, maybe you mentioned it before, but if one before. But if one event was identified that you have an equal issue, the next step you request to use noise mitigation or the glitch subtraction. How does it work? You contact some people to there is first a sort of review of the okay. Event validators provide a recommendation of what to do, then there is a review of that recommendation in A review of that recommendation. If the each subtraction is really required, then we go to this branch, which starts the loop of noise subtraction, evaluation of the results, evaluation, and then decision of iterating again. So who will perform regulatory subtractions? No, like in or three. Sophie, Harien. So there is some people just don't perform. Some people are just the one part is past. Okay, let's say the bias wave team. So it is a bit automated than it was before, but there's still. It's a bit automated, more automated than it was before. I don't want it to spoil anything because it's open to everybody, but that's for a computer. Yeah, I mean, I don't know how automated things are on your end, but you know, this strikes me that the answer here is not. Me that the answer here is not machine learning per se, it's like robots, right? I mean, no, that's just what we talk a lot about: robots and key because we have a lot of automated jobs running on Git and in other places that are doing a lot of the processing we need to do automatically as the control in. So one could imagine that what one needs is as each of these different checks are coming in, instead of having a human check it, the bot is checking it, and then if there's an issue in the workflow, And if there's an issue in the workflow, it spits out an alert to a human. And then maybe DQR is concerning, and then maybe a genuine ML thing, like the Grand Beats SPY tree, runs and identifies a segment, right, to target, and then that should produce an automated, like automatically produce a Bayes wave config file with the appropriate config settings, and then launch that job. None of this is machine learning, but it is human work in creating and then maintaining these automated processes. And then maintaining these automated processes. That's hard, maybe sometimes boring, but this stuff should sort of flow through within seconds and be running without people having to think about it. When you start, I move into this page because I thought what you were saying were more, was more relevant for this part, but actually, no, maybe this part too, right? I mean, this is definitely easier, but maybe also for prevent validation, it makes a lot of sense what you said. I mean, something to take. Something to take in mind to consider for profile. Thank you for the feedback. Danger of being on time, so I'm going to do that. I don't want to spy the page on this.