Of medians and key means. So, suppose we have a data set, so a collection of points in a high-dimensional space, and we would like to partition it into groups of points, so k clusters. And for every cluster, we also want to find its center, ci, and assign every point to that center. And the cost of And the cost of k-means clustering equals to the sum of square distances from every point in our data set to the center of the cluster it is assigned to. And because we always want to assign every point to the closest center, we can say that, in other words, the cost of the clustering equals to the sum over like all data points, the square distance from that point to the closest center. That point to the closest center. And the definition for k-medians is similar, but instead of taking squared Euclidean distance from a point to its center, we will look at the distance from the point to that center. And in this particular case, we will be interested in key medians in L1. So we will look at the L1 distance from a The L1 distance from a point to the center. And so these are two very common objectives for clustering. They used a lot in theory and in practice, especially k-means. There are many interesting algorithms developed for this problem. So here is just some examples of, let's say, algorithms for key-median, recent works on that, and some recent works on key means. On key means, and this is just a very small snapshot of what's going on, like what research has done about these clustering methods. And while these algorithms are very popular and these clustering are very popular, there is some drawback that they have. And the problem is that it's not so easy to understand. Not so easy to understand these clusterings for a human. So, to illustrate this point, let's imagine that we have a 100-dimensional space and 300 centers in it. And now I give you a point, a data point in the space, and ask you which cluster the data point should belong to. Should belong to. And of course, in theory, answering that question is easy. So you take your point, you look at 300 clusters you have, and you compute distances from that point to each center and take the minimum distance. So that task is easy for computers, but it's not so easy for human beings. And that's why understanding a clustering for a human is not. For a human is not that easy. And so we can ask how to make key means and key medians easier to understand. And this is exactly the question that Dasgupta et al. considered in their work. And they suggested to look at explainable clustering, which is based on threshold decision trees. So what are threshold decision trees? Threshold decision trees. Threshold called decision trees are space partitioning trees that make the decisions based at every node based on one coordinate or feature in the data set. So let's look at an example. So here is an example of a decision tree with three coordinates, X, Y, Z. So what we do first, we compare So, what we do first, we compare the x-coordinate of the data set of every point with the threshold four. And based on the answer, we go into this left or right sub-trees, or maybe upper or lower subtrees, and then we compare, let's say, coordinate y, these two, and again, as now assign every point, like first cluster, second cluster, third, and fourth. And as you can see here, And as you can see here, this classification is much simpler for a human being than k-means or k-medians. So we can easily interpret this clustering, especially if the number of coordinates or maybe the depth of the tree is not too large. And we can also say that this decision tree assigns not clusters to every To every data point, but rather some center. So, center like C1, C2, C3, and C4. And so, another way of looking at decision trees, more geometric view on it, is that we partition our space using hyperplanes that are axis parallel. So, in this example, we can partition the space first with, let's say, a vertical line into Vertical line into parts and then partition each of these parts like recursively, maybe in two parts, and then the right part again using, let's say, horizontal parts into four parts. And so this is an example of a space partitioning using threshold cuts. And you can think about some other examples of such trees, like let's say KD trees. KD trees. And it is known that decision trees are easier to understand for humans. Again, there may be some fine print. So obviously it's easier to understand decision trees if the number of maybe decisions is not too large, and especially if every coordinatory feature has a certain meaning. For example, if you want to classify cars, you can first partition all cars based on their All cars based on their price, then on their size, on their speed, on their gas consumption, and so on. But now the question is, how do we measure the quality of decision trees? And Desgupta et al. proposed the following way. So they suggested that we combine decision trees with the classic Humidian. The classic key medians or key miss clustering. So, what they say is that let's take a center in every cluster assigned to the decision tree. So, every cluster corresponds to a leaf of the decision tree. And let's denote that cluster actually by T of X. So, T is the decision tree. Let's say T of X is the center of the cluster assigned to X. And now we will use basically the same formulas. Use basically the same formulas as before to measure the cost of a decision test. So, for k-medians, we are going to look at all points in our data set, and we add up costs which are equal to like the distance from x to the center assigned to x by the decision tree. For k-means, the formula will be almost the same, except we look at a squared Euclidean distance. And of course, after they propose the definition, there is a question: how much do we lose by restricting centers of clusters in our clustering? So what is the relative cost of explainable k-medians or k-means clustering with the optimal k-medians and k-means clustering? So we want to compare this. So, if we want to compare these two quantities, let's say for key medians. Above, you can see the cost of explainable clustering, where again, for every X, we look at the center assigned by the decision tree to X. And below, you can see the cost of so-called unconstrained clustering, so where we can assign to every X the closest center C. In the priority, it's not clear that this cost. Clear that this cost, this difference is somehow bounded, and maybe that bound may depend a priori by number of points in our data set or may even be infinite. And somewhat surprisingly, what this group data showed that actually that number is bounded by order of key for key medians. So it doesn't depend on the size of our data set. Size of our data set. It doesn't depend largely on the dimension of the space. It just depends on the number of clusters. So specifically what they showed is that if you look at any data set and look at explainable k-means clustering, its cost is going to be bounded by order of K times the cost of the best chemians clustering. Key medians cluster. And this result was later improved to order of low-key, log log key. So that key was improved to log key, log log key. This is by Fingari et al. and Lyran, Shan and me. And surprisingly, also the algorithm for this is very simple. So either I will show you the algorithm. Show you the algorithm for obtaining an explainable clustering a little later in this talk. But basically, it just randomly partitions our space and it turns out that it works well. But let's wait for a moment until we see that algorithm. So what is the situation for key means? For key means, kind of this ratio or the cost of explainability is somewhat Cost of explainability is somewhat worse. In the original work, Dasgupta Frost, Marshkevich, and Russian gave this price of explanation, bound on the price of explainability of order of k squared. And later, this was improved to order of K up to some logarithmic factors. And so, unfortunately, this bound is also tight, so it can't be improved. Let me also mention. Be improved. Let me also mention that for key medians, like on this slide, it's not hard to show a lower bound of order of flow key. So order of flow key is the best you can do. And like at least till this year, that was the best bound. And for k-means, the best you can do is basically order OK. And this is a little bit unfortunate. Little bit unfortunate because somehow order of k doesn't look like a good bound and a competitive ratio. So the good news. Are those bounds only for high-dimensional instances? Yeah, that's a great question. It's true that the dimension should be high-dimensional instance, and you can do better if the dimension is small. And I won't talk. And I won't talk about this today, but I will give you some reference. Thank you. And in general, of course, you can also say, okay, if you look like at, let's say, 200-dimensional space, then whether that can be understood by a human being, and that's not clear. So, this is not a theoretical question, but more applied. But this is a great question when actually even decision trees are easy to understand. Of course, in the picture where you have. To understand, of course, in the picture where you have three, four, five coordinates, perhaps that's not too hard to understand. But if you have like one million coordinates, like maybe in some neural network models, who knows? So, okay, but let me get back to k-means. And it is what Leran and I showed: that if you allow a sort of by criteria approximate. Now, a sort of by criteria approximation. So, if you compare your explainable instances, explainable clustering with like one plus delta k centers with unconstrained clustering with key centers, then we can actually get a much better competitive ratio. So, that competitive ratio drops to log squared k, which is polylogarithmic in k, a much nicer expression. Expression and the price you are paying for that is just additional delta key centers or delta key leaves in your tree. And of course, that competitive ratio depends on delta as well. So there is some dependence which is hidden here. So in general, if you are interested in k-means, you can still use explainable clustering, but you just need to get bounds. Bounds as by criterion bounds. So, as I said, after probably mentioned already, after Dasgupta et al. introduced the explainable clustering idea, there were a lot of works on this topic. And so, here are just some of them. And I wanted to mention particularly the work by Sherry Karanhu, who looked at low-dimensional spaces. At low-dimensional spaces, and so they provided better k-means algorithms for low-dimensional spaces. And also, several works like by Deng et al. and Alabor et al. on this slide look also at shallow plastic. So, of course, for a human being, it's easier to look at decision trees that are not too deep. And so, it turns out that. And so it turns out that for shallow clustering, we have some kind of mixed both good news and not so good news. So in some cases, you can't get as shallow tree. In some cases, in practice, you can. But I want and discuss that question today. So, and let me get back to the result for Kimidians and show you some actual. Some actual proofs. So, as I said in several works, the researchers showed a competitive ratio of log k log log k using a very simple randomized algorithm, which we will see now in a couple of slides. And the bound that we had is log k log log k. And the proofs weren't actually that simple. And just this year, that bound was improved to order of three k. So the improvement. improve to order of the key. So the improvement is not that large, but it's nice that now we know the exact bound for that very simple algorithm. And in a way, we know that it's the best possible. So in the work with Liran, I showed all the low-key competitive ratio and group tetal actually gave even better bounds. So they really got the exact constant in front of this low-key. So it turns out that the right answer is just low key. Turns out that the right answer is just locally without any. So, this is the best you can do. Yeah, there are like matching lower bounds. So, and also it's nice that you can use this algorithm alone as a very simple black box deduction to get k log k competitive ratio for k means. So, basically, the only thing you need to know is this algorithm, unless you want to. Algorithm, unless you want to get those shallow trees or by criteria approximations. So now let me, in the remaining time, give you some ideas about that algorithm. And this algorithm, as well as other algorithms, are data oblivious. So how do they work? We first run one of existing algorithms for key means or key medians, in this case, key medians, and obtain a clustering, obtain a set of. And obtain a clustering, obtain a set of center C1, et cetera, CK. Then we completely forget about the data set X, and we partition only our centers, C1, CK. And the guarantee that we have is that for every point in the space X, the expected distance from X to the center assigned to X by the decision tree is upper bounded by order of log times the minimum decision. Times the minimum distance from x to the center. And this is again, this is not very surprising, but this is nice. It makes our algorithms faster. And now let's look at a random cut algorithm. It takes as input C1, et cetera, CK and outputs 3T. And the algorithm is recursive. So it recursively partitions our Recursively partitions our region into parts. It starts with one cut, partitions the space into left and right part, and then each of those left and right parts are partitions recursive. So here is the picture. For simplicity, let's assume that we scaled our data set and it lies in the cube is unit cube of dimension D. So the algorithm picks a random coordinate i. picks a random coordinate i from one to g and then a random threshold t between zero and one like this and splits uh the data in two parts left and right and then it recursively partitions the left and the right part so also make certain cuts and it proceeds like that until every cell of the space contains exactly one center there is one small There is one small exception of fine print. So, if we want to make a cut, random cut that doesn't partition our cluster, our centers into parts, or one of those parts is empty, like in the top left corner, then we ignore this cut. So, we don't make cuts that don't partition the set of centers into non-empty groups. So, and this is it. This is the algorithm. So, and this is it. This is the algorithm for obtaining explainable clustering for key mediums. And what is the problem? Why it doesn't output the optimal partitioning all this? Of course, the problem is that, like in this picture, we can have some data point X. It has a closed center in the nearby cell, but the decision tree assigns a different center. Tree assigns a different center in the cell, which is much more further away than the closest center. And basically, we need to show that that happens not very often, or the distance, the extra distance is not very large. So, to do so, let's analyze the algorithm. Let's kind of think about it from a perspective of one particular data point. So, let's speak in RBI. Data point. So let's pick an arbitrary data point X and see what happens from its perspective. So initially, they pick a random cut, and by doing so, we are eliminating some set of centers, basically the centers that lie in the other cell from X. So that point X, I don't know if you can see it, it's depicted by a black point in the right cell. So if you make this cut, then we So, we make this cut, then we make another random cut, and by doing so, we are eliminating yet another center, and we continue eliminating centers until there is only one center remains like in the same cell as our point X. And this can be this view can be a, maybe we can look at a more general problem, not even more general. General problem, not even more general, but kind of take a more abstract view on it. So let's assume that we have just a collection of key stats in some measure set. And so this measure set contains just, in our example, various cuts that we can do, like coordinate cuts. And sets S1, etc., SK are those contain those cuts that separate the first, second, and so on k center. Second, and so on, key center from that fixed point X. So we have now measured space, sets S1, et cetera, SK. And in this game, we assume that we pick some element from our measured space, omega, and we eliminate all sets that contain this omega, like in this picture. And there is only one exception. We do not eliminate sets if omega contains. Omega contains all sets, all remaining sets. And this corresponds to the case when we make a cut that doesn't separate any set. So now we look at this game and we eliminate sets, still only one set remains, and we say that the cost of the game equals to the measure of this set. And again, this corresponds to the L1 distance from X to the remaining set. And it's easier, perhaps. And it's easier, perhaps, to analyze this game than the original problem. So, the theorem we show is that the measure of the winner of the remaining set is less than or equal in expectation to log my k, actually here, to log k, the measure of the minimum set in the game. So, how to prove it? So, let's use the idea of exponential clock. Of exponential clock, which have been used before in math and in our community. And the idea is as follows. So we're looking at this game, and this game is played in rounds. At every round, we pick some omega and eliminate some sets. Let's assign some time to each round. By doing so, we are not changing the game. We're just assigning a time to every round. Time to every round, and that time is some positive number, and the distance between two times, like ti plus one and ti, is distributed according to the exponential distribution, this parameter mu of the intense space omega. Okay, so this is what we do. And what are the benefits of this trick? So now the So now the time when we pick a particular element omega has also the exponential distribution, this parameter Î¼ of omega, where omega is an element. So for simplicity, let's kind of discretize the space. So assume that this measure space is finite. That's easy to do. And then again, for every element, the time we choose at omega has exponential distribution. Is has exponential distribution with parameter mu omega. And for every set, the time when we choose the first element in that set has exponential distribution with parameter mu x. And also conveniently, if you have two disjoint sets, then the hidden times for those sets, or the times when they pick the first elements in those sets, are independent. And now, for simplicity, again, for the purposes of this talk, Simplicity again for the purposes of this talk: assume that the smallest set is a set S1. Okay, that's that's easy, but furthermore that this set is disjoint from other sets. And this makes the problem a little simpler. If we assume that all sets are disjoint, that will make the problem much simpler. But this is some kind of intermediate problem. So again, S1 is disjoint from all other sets, but those sets can overlap. Sets can overlap. And what's good about this assumption is that now, before S1 gets eliminated, if it picks some element like this, then all sets containing this element, for instance, S1, gets eliminated. We don't need that exceptional clause. If we pick an element like this, then these two sets get eliminated. So how do we argue now? How do we argue now about the cost? So, consider the time when the first set, S1, like the smallest set, is eliminated. And if this is the last set that kind of gets eliminated, or maybe here more correctly, gets hit with an element or an element a mega is chosen from it, then we're in a good shape. What might happen, of course, is that some sets, other sets, are still in the game when we want to eliminate S1. Eliminate as one. And so let's, as a warm-up, let's just bound the cost of the game by measures of all those steps. So what is the probability that some set Si remains in the game after S1 gets hit? So in other words, the hitting time for Si is greater than the hitting time of S1. So because these exponential random variables, we can easily bound that. Variables, we can easily bound that by measure of S1 divided by measure of Si. And the cost we are paying if Si remains in the game is mu of Si. So conveniently, the total like cost, expected cost of the gain of that Si equals mu of Si. So that's good news. But of course, we have K stats. So the bound we get is K times mu of S1. So this is much worse. mu of s1 so this is much worse than i i promised you uh and uh can we do better so what what did we learn from uh this first attempt one thing is that it's it's it's it's probably a better idea to use the union bound or to to to bound the cost but the second thing is still we can get some bound from even this very basic idea and yeah i still have i have just two slides remaining so i'm almost on time so let's uh let's So let's try to modify this argument a little bit. So if you look at some set SI, what is the expected time when it gets hit with an element omega? So its expectation is equal to one over mu of SI. So the bigger the set, of course, the more likely it is hit earlier. And it's again, it's not hard to show that. Again, it's not hard to show that it's unlikely that a particular set Si is hit much later than its expected time. And much later means that like log n, log k times one over mu S i. Okay. And because of that, they say that a set SI is a surprise that if the polymer happens when the heat set. We hit set S1, the smallest set. Set Si remains in the game, and the time we hit S1 is greater than 1 over mu SI times OK. So basically, again, this expression on the right-hand side is some kind of expression where we expect that Zi should already be eliminated. So if it happens that S1 is eliminated at that point of time, One is eliminated at that point of time, but Si is not well above the time when we expect Si to be eliminated. We say that it's a surprise set. And an easy computation shows that the probability that Si is a surprise that is mu S1 over mu Si multiplied by this one over K factor. That one over K factor is here because of that log K addition. And then the same expressions as we had on the And then the same expressions as we had on the first slide show that the probability that like or expectation of cost from surprise stats is bounded by mu as well because we have this additional one over k factors. So we managed to deal with the surprise tests. And now let me again, let me just show very quickly what happens with these non-surprise tests. These non-surprise tests. With non-surprise tasks, we are going to argue as follows. So we have the set S1, we have the remaining sets, and let's run the game first without S1, ignoring it. So we are eliminating some sets, and we are not looking at what happens in S1 because it's disjoint from other sets. And in the end, we have one set remaining, like in the other part of our picture, not including S1. And so now we need just to. And so now we need just to see which set S1 or Si gets eliminated earlier, right? We have just one set Si. It's not that, so the time when Si is eliminated is no longer distributed according to the exponential distribution because we, of course, condition on SI being the winner. But if SI is a surprise winner, you already paid for it. So now we can assume that SI is not a surprise winner. Si is not a surprise winner. And then basically, if Si wins against S1, it means that the hitting time of S1 is less than that exactly like this bound on the heat time of SI we had previously. It's one over mu SI times log. And this happens with this probability mu s1 over mu si times log k. And so we get this log key bound. Okay, so let me finish. Okay, so let me finish now and just say that hopefully explainable k-means and k-medians clustering is easy to understand. For k-means, we can lose a factor of k up to logarithmic factors. We can improve that bound to log squared k if we allow by criteria approximation. And for k-medians, we now know the exact The exact ratio thanks to the work of Gupta et al. So that's low-key competitive ratio. And today this is all just order of low-key competitive ratio. Okay, thank you. Questions? Is it easy to Is it easy to derand them? I guess you have to look at the data set, but is it easy to derandomize? I don't quite remember whether this was started or how easy it is to do. So in this case, Easy it is to do. So, in this case, when we look, of course, at this oblivious algorithm, that's probably impossible only if you look at central C1, central CT. If you have the data set, probably that that's doable using some known techniques, but I'm not 100% sure. Like standard techniques, right? So, like maybe conditional expectations or something like that, but I'm not 100% sure. Thanks, Austrico. Yeah, well, then you got to get it set up. The coffee break is one hour long.