Today I'm talking about some inhomogeneous regulation in solving some inverse problem when the measurement data is limited or indirect data. And this work is that the joint work is Yunsung and supported by some ONR and NSF. Okay, so in this talk, we are interested in recovering the signal U in dimension N from the measurement data in Y in dimension M. In dimension M. And this measurement is from some linear measurement operator A, and that measurement could be a noise by the epsilon. So, depending on the application that the operator A could be some Gaussian kernel when we consider the debluring, or A could be just identity when we consider the denoising application, or A could be some discrete Fourier metrics when we consider the scenario of virtual later, SAR data. SAR data. Typically, the reconstruction is from solving some optimization problem finding some minimizer of the U, which minimizes some residual of the specific norm. And this, the P norm could be depending on that, how we assume that the distribution of the measurement error. So for the case of that, the error could be as a Gaussian, a type of Gaussian, then the p equal to. And in this world, we only consider that the Gaussian noise and peak. only consider the detection noise and vehicle too. And when the measurement data is limited or incomplete, that is the dimension of the measurement m is less than n, then system is underdetermined, then there are many possible solutions and actually the problem is ill-posed. So there's no sufficient information to recover the signal perfectly, right? So now we introduce the regular So now we introduce the regularization. So regularization is a law in stabilizing the G-pause problem. That is, it enforces the searching the signal to some particular spaces, which reflecting some prior information of the signal. So regularization is added to this optimization. And now I clarify some terminology that here the first term is called the fidelity term that measures the discrepancy of u for a given measurement of y. For a given measurement of y. And R of u is a regulatory term. And also, here's another positive parameter we call the regularization parameter, a balancing between that, that the fidelity term that cannot come from that regularization. Okay. Let's briefly introduce that the standard regularization. The first thing is the L0 regularization that algebra norm counts as non-zero element when we When we were given some constraint that L0 known, that it induces some sparse features. So searching the signal space is a sparse sparse. So typically when we're using the L0, that actually suffers some combinatory complexity in optimization problem. So alternatively, we use that airborneization. It also induces some sparse feature, but it is a convex. So it is used as a Is used as an alternative of L0 to capture the sparsity as a kind of convex relaxations. And another popular regulation is L2 is also called the tick and off. Then it is two norm and it's known that it's helping the recovering smooth region or smooth signals. We can understand with the Bayesian context. So actually one of the people, many people are interested in the sparse feature. In the sparse feature. So there is some generalization of the L1 legalization. So not just searching the signal spaces, but considering the gradient spaces. For instance, for total variation legalization, so we call the TV. Then TV has the L1 norm of the gradient. And then when it used that, it captures some sparsity in the gradient domain. The sparsity in the gradient, the most of the gradient is zero. That means that it is. Greater than zero. That means that it is that target space is piecewise constant. And also, when we consider, we can also consider the high order. So for the case of that the second order derivative, then most of the second of the derivative is zero. That means that piecewise linear reconstruction. So as we see that the different regulators have a different referable target spaces. Spaces. So it's very crucial to know about that prior information of the signal for choosing the regularization. So actually, we have two questions arise. First thing is, if you don't know about any prior information, then how we choose a regulation? Which regulation is good for recovery? The second question is, even we know about the prior information, but there is no explicit feature. But there is no explicit feature like some sparsity or smooth variation, or even that the feature is very explicit, but the features are mixed, then none of that the single standard regulation is not enough to cover the signal. So let's see some example that I bring some signal that has a two dominant feature. One left is a jump discontinuity, and on the right side is highly resolving. The right side is highly oscillating. And jump discontinuity is a sparse inner gradient. So we know that it is good for using the TB recovery. So when you apply the TB on this signal recovery, the left side, the jump display very well, the oscillation is not good because as you see here, there's some steering case effect for oscillation in peak below the oscillation. And for the case of the oscillation, the tickhunoff is good. So when you apply the tick-off, oscillation is good, but Chikanov oscillation is good, but that the left side has some gift phenomena, there is a jump discontinuity. Okay, so our goal of this project is actually twofold. One is we approximate some local characteristic signal from some single measurement, and that measurement is incomplete and indirect, and also it could be noisy. And second point is based on that, the estimated local. Based on that estimate locker behavior, we employ some regularization which prefers to recover individual locker features. Any question? All right. So the form of the regulation form is like that. So we consider some inhomogeneous in respect to that exponent of this gradient. So it is spatially. Gradient, so it is spatially varying power, and the power is between. We assume the power is between that one and two. And as we know, that the P list of one is very good for that capturing the sparsity, but actually it's not a convex. And for the case of P gold larger than two, P is larger than two, then it is good for recovering very smooth region, but it has some instability and or say some marginal gain. And say it's a marginal gain. So now in this world, we consider the P is between the one and two. And we can understand that the P, such P between one and two is like some interpolating balancing between that L1 and L2. So the changing that the P is like some change in the geometry of the constraint. So for the case of the L2 norm, that the constraint is like the circle. But for the case of P, But for the case of pickle one, it looks like the diamond, and the middle one that it likes some interpolating between the two features. And also, so it's compared to some weighted TV. So weighted TV is considering some weighted. And in that case, the geometry is like some diamond is invariant, but it changes the scale. So actually, there's some Um there there's some different different aspects of different different law in the weighted and the exponent, and we can just simply understand what is the difference in geometry, but it's actually it's not well-known study about that the impact of the weighting and exponent in the optimization. So in this work, we only consider that the exponent exponent and the interplay between And the interplay between the weighted and exponent is our future work. So, this inhomogeneous regularization in spatial variant P is actually already studied in the denoising application. And then in that case, because it's denoising, so the operator A is identity. So actually, it's a complete. And in there, they assign some exponent based on that. Some exponent based on that the first or second database of the true signal. Actually, we don't know about the true signal, but just noidy. So, from that, they approximate some first or second data bit. But in these works, actually, it is only work that when the signal to noise SNR ratio is low, then it is worse because it because it's a signal SNR row, then approximation of the gradient is working well. Of the gradient is working well, but for the case of that, what you consider that measurement is limited in indirect for the case of the incomplete Fury data, the VR data is indirect, right? And so some frequency is missing, but the case of incomplete, then actually we don't have information about the gradient. Okay. So our method is first we approximate the local view. The local VR VRS. First, we estimate the local behavior or local characteristic. How we do is first we generate some multiple reconstruction samples from the standard TV or TCOP regulation and it bearing that the regulation parameter and number is a C. And each reconstructed sample might be inaccurate, but we have a bunch of samples. And from that, we take in some. And from that, we're taking some averaging gradient to get us some, say, the G1 statistic and G2 statistic. And using that, these two statistics adaptively to capturing some, we estimate some local feature. So as we expect that the G1, because it is from the T-conoff, then we expect that it is good for covering some discontinuity, right? And for the case of G2, because it is from the T-conoff, To because it is from the ticker, then it's working well, some oscillation or smooth spark. So, we adaptively using that two statistics to capture, to estimate the local behavior. And how we do is actually, first we identify some, say, some feature. We identify feature within some small patches, some measure positive, not just single point or single pixel. The rational using the patches. The rationale of the pet using the patch is that some feature, say, some characteristic, like some sparsity or like some oscillation, is actually the interrelation between neighbor, not just a single value. So from that motivation, we identify the features in the small patches. So we divide the domain, the signal to the non-overlapping patches, and we identify the feature in each patch. By the feature in each patches, and then we assign the uniform exponent power to India regularization. And here we're using some pooling operator. And if you are familiar with some convolutional neural network, that is in the pooling operator appears in there. And here we use some variance pooling and average pooling to extract some local features. Okay, so actually. Okay, so actually we first we first predefine some category. So each patch could be one of the three classes. One is the discontinuity and the other is oscillation and third is a semoture region. So we have a predefined category and we classify each patches to one of the category using some variance pooling. I don't explain the detailed algorithm, but the key idea is The key idea is just imagine that just a patch on the smooth region, then you can expect that the variance of the gradient is very low. So we can identify the smooth region. For other two classes, discontinuity and smooth region, the variance is very high. And we distinguish that discontinuity and smooth region is by the characteristic that discontinued cases, so discontinued cases, the gradient is actually isolated. That means that Actually, isolated. That means that the nearby gradient is almost low, but that the one, the pixel or point in the discount is very high. But for the case of oscillation, the gradient around is very high. So we use some specialized filter to distinguish between discontinuity and smooth speed oscillations. So here is some classification for our method. For our method, so from the signal domain, we divide some patches and then we classify the each patches. So in the discontinuity, in the jump discontinuity, we classify as a discontinuity class. And for the case of oscillation, that oscillation power, we assign the oscillation class that other resistance region. And once we do the classification, then actually we assign the sum. Actually, we assign some preferable exponent and H patches. So, ideally, it's interpolate, as I say, bearing the P is like some interpolating between the L1 and L2. Then, as you imagine that the P just starting from one to move to the two, then actually it relax some sparsity features. And when getting close to two, then it captures more smoothness. So, based on that motivation, we assign some. assign some we employ some function phi depending on it it is a function of some average pooling and each patch we have a gradient value then we're taking some average then and using that function of the average pooling we use some increasing function so so assign of one of the examples is using some exponential function to get here so in the right hand side is the exponent distribution of the regular Exponent distribution of the regularization. So, for the case of the discontinuity, it is exactly one and nearby, nearby is close in what, but not exactly one. And for the case of oscillation, it's very close to two, but not exactly two. But it is from the proposed functions. So, we also test that how much sensitive our method in the In the sample generation. As I said, the sample is from the different regulation parameter. So we test that the thousand, like a thousand different trier and trial is from the randomly generated interval. And then the below is some distribution of the pooling operator. For the case of variance pooling is for the classification and average pooling is the assigning the exponent. assigning the exponent. And here, as you see, that for the case of the discontinuity, for the case of discontinuity, compared to that L1, sorry, for the case of that L1 and discontinuity compared to the L1 and L2, that L1 is much stable for identify the discontinuity and for the case of that oscillation, the L2 is more stable. two is more stable. So from here we can see that we can see that some of our classification and assigning some exponents a little bit stable, stable, a little bit insensitive to the choice of the reconstruction samples. So here is the results of our method. So as you see that when you apply that p equal one, it's better to, it is good to capture this continuity. Capture this continuity, but I put that oscillation and P2 is that. And also, we compare the other method, and the slow is our method. So, F is our method. And you might expect that if you know about the product information, one is discontinuity and the other is oscillation. How about just putting that the left side just that pull one and just right side is just two? And we also. Just go to. And we also try. And as you see, it's very good. And when you compare the point-wise editor, then actually our inhomogeneous distribution of the power is much better. And we also tried the synthetic 2D images. And here we have this continuity, like some edges in the circle. Patches in the circle, and inside the circle, there is this oscillation, and the outside is no sparing. And from that, our map, we give some path classification like that, the middle is oscillation, and we capture the discontinuity of the edges and the other resistance. And based on that classification, we put on some inhomogeneous distribution of the power is the right side. Is the right side, and this is from that 3A measurement. So, I forgot to mention that all of that example is from that incomplete 3A measurement in this work. So, measurement is just only the 25, and we also consider that the noisies are about that 18 is corresponding to 25 SNR. And this is approximate error, and both the case of the peak. The case of the pico one and pico two, that as expect that the pico one is very clean to capture that edges, but the that we expect that the pickle two is good at the oscillation, but actually in this figure we cannot figure out that exactly, right? And for the case of ours, is that it also capturing that the claim that edges and also it's similar to osulation. Let's see some point-wise error. And for the case of P1, then compared to the edges if P equals 2, then the white one is better. Yeah, so that P equal 1, as you see, capturing that edge is better. But oscillation compared to P2 is not good, but our process method is really better than it's good to capture the both of them. We can explicitly see that here. That here. So, this is that comparison between the point miz error for each trials. So, first thing is peak of one minus peak of two. So, that blue means that peak of one is better. The red means peak two is better. So, we can explicitly see that in this case, pico one is good at not just the disc continuous, but also good at the sumo region. But as expected. But but as expected that the pical two is better in oscillation and also we also compare to the others and the last four is comparison between our method and the standard method and the other method so as you see that most of the the other last four is subtract that error from the our method and as you see that most of the part is blue that means the better than the others Others. Okay. Yeah, this is more complicated real image. Is it actually the SAR image data from some Arctic region? As actually, it is the ice and some cracks. And this is from that ICX data. And from the R method, we have some patch classification. Actually, here it's not directly understanding, right? But based on that R method, they have some patch classification. They have some patch qualification, and from that, we put on some distribution of the exponent. And this is also depth-grade measurement. And here is a result. And this is our method and the others. But yeah, it actually is only one of the third measurement and it's very complicated. So actually, most of things are. Most of things are actually similar to the true image, but compared to that, we can see the point on the error that black one is better, but and especially in the region about that 800, the 800, then it's more black one, that means much better. Actually, identification of that part is very important in Arctic region, start Arctic region research that. Arctic region research that it identified that ice or size of the crag is very important. So even the small improvement is better, it is very worse. Okay, so conclusion. So in this world, we only consider some incomplete free measurement, but we can also try another measurement. But here I point is that free measurement is the global operator. So one measurement is depending on all of the signal or all of the peak. Of the signal or all of the peak region in the image, and actually, in this world, we only assign an exponent based on that only information of the patches. But actually, because that operator is global, that there might be some interrelation between that patch, interrelation between the patches. So, in this world, we do not consider that relation. So, if you consider that, maybe we could have some better, we can improve. Have some better, we can improve some of our methods, and also we can consider some size. We can consider the varying the size of the patches or shape because depending on that, that the scales or depending on the interest, we can change the size or shape. And the last thing is actually, as I mentioned, then there is a possibility then put on some weighting and put on some weighting and each component. So we can. Component. So we can actually, we don't understand exactly how that H weighting effect of the weight and exponent in the recovery. So we have to more study about that their inter H laws and how they interplay the weight and exponent. That is our future work. Yeah, that's it. Thank you for any questions? Yeah, any questions? We have four questions. Yeah, so your activities are ready to start.