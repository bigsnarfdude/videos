Okay, so great to be here. Such a wonderful company. Thanks a lot for the organizers putting this work out together. So I will take advantage of the opportunity and tell you some ongoing unfinished work with Kiev Khan, who was at the University of Santiago Chile and Matias Pales Ine, who's now a fast of the University of Barbie. And since it's unfinished, I'm glad to take any suggestions on how to. Michaels, please speak up. Okay, I'm glad to take any suggestion on how to improve this since this is an unfinished work. Okay, so this work is motivated by some combinatorial problems concerning words and also by the limit theory of combinatorial logics that has been developed over the last 20 or so years by Lovas and many other people. So just so we're So, just so we're in the same page, for me, a word is just an ordered sequence of characters, a finite number of characters. A character is just an element of a finite set. I call the set an alphabet. And if I have a word and I remove characters from it, it doesn't matter where, I get something I call a sub. The length of a word is just its number of characters. And so the problem. So, the problem, the combinatorial problem I've been interested in for many years is the longest common sub-sequence problem. So, here you have two strings, and a subword that appears in both strings is called a common sub-sequence. And if it's of maximum length, it's called the longest common sub-sequence. So, for the purpose of illustration, here you have two examples where there's a unique longest common subsequence, and it's a Sub-sequence and it's in red highlighted. So longest common sub-sequence is a measure of similarity and it appears in several contexts like text processing or genome comparison. And then a natural question arises if you have these two strands of DNA and I tell you 80%, you have a common subsequent which is 80% of their net. Is this a lot? Is this not a lot? Does this happen by chance or not? Happened by chance or not. So, in order to address that question, people have come up with this ridiculous model of what the genome looks like. It's just a random sequence of characters taken from an alphabet, or the same theater for text. And then you get a very natural, well-posed mathematical question that seems to be kind of hard because we don't know so much about it, which is define the random variable, which will be. Which will be the length of the longest common sequence of the two words generated according to this random word model. Now, although this is fairly ridiculous, it seems empirically people have noticed that it does make some sense in some context if you calibrate for the right size of the alphabet. This is very cryptic, but I will not go much more into it. So there are several outstanding questions about this. Outstanding questions about this. One thing that is known and is very easy to see is that this longest common suit sequence, the expectation grows linearly with n. But the constant of proportionality has a name, it's called the Schwatel-Samkov constant, and it's unknown. Schwatel-Sampov constant, okay? And the CS constant, okay? And it depends on the alphabet size. It's unknown. Good bounds exist, at least for binary alphabets. At least for binary atoms. So, in order to try to attack the problem, people have looked at the sim, you know, they have changed the model of the random model of words. And one thing that they've done is look at the independent point tosses, all of them probably coming out. Of coming out P, the same character. And especially they look at this equation in the range where P is very small. And there they can get some results. So I propose to generalize the random word model in the following way. So I'll take a function g, which is from the open-closed interval 0, 1, and taking values in 0, 1. interval 0, 1 and taking values in 0, 1. And then I'll choose sequentially an infinite sequence of values, uniformly at random in the open-closed interval, 0, 1. And now for each one of those values, we evaluate the function and add x of y and then flip a point with the probability g of xy comes out a point. G of x phi comes out of one. Okay? And that generates a character. But now we are not going to. So if we look at the end first coin flips, we're not going to order the characters in the way we flip them, but we're going to order them according to the values of x of i's that we're generating. So for each x of i, you evaluate the function, you flip a coin, you get a You evaluate a function, you flip a coin, you get a value, and now you read from left to right for every n, and you get an infinite sequence of words. So, this is that random word model I was talking about. And there, it's it formally said. So, is it clear what the model is? Yes. W is going to be a character or a word? No, W is going to be just a single character. Zero one. Okay. The result of a one. Is it clear? So you said that you get an infinite sequence of words. You get an infinite sequence of words? Yes, because I get an infinite sequence of values, but for every n, I get one word of length n. Perfect, okay? So like generalizing change from a constant function, arbitrary function, does it? Sorry. Yeah, it has other properties. Okay, but I will not discuss them too much. So where does the okay, so called the generalized LCS problem that when I LCS problem that when I generate this words according to some functions and they don't have to be the same function. Okay? So of course when g is a constant function I just get the binomial random world model almost surely because all the xy values will probably one will be different. And so where does this come from? Well this is kind of natural way Well, this is kind of natural when you do limit theory of convergent sequences of words. So, if you've heard about this limit theory of graphs that Lois and other people developed, so people have gone and looked at more and more complicated combinatorial objects and developed a limit here, and it gets technically more and more involved. In this paper, what we'll try to do is look at a simpler thing, get a more Third thing, get a more simplified theory with less technical issue, but still interesting and also maybe that will show, give some insight on problems that are still open for more complex combinatorial problems. Now, every limit object comes with a way of generating an infinite sequence, which is converted. And that's exactly the Random Hohmann model. Exactly the random board model that I just presented. Okay, so there is where it's coming. Merkel. Yes? Just one question. A slide before this, sorry. So if I have the first n characters already determined and I want to add the next one, and I'm not adding it to the end of the list. I'm going to add it somewhere in the middle. It's based on the order of the axes. Yes, that's that's not it's not according to the order of the context. Reason why G is not defined at zero? G is not defined at zero? Yeah, it doesn't make any difference. It's just will make the rest more technically correct. No relevance. Well, it's measurable, it's enough. For everything I'll be doing, I am just not even right, and it will be a measurable function. Okay. So now I want to cast this generalized longest common subsequence as even a more generalized. Commons of sequence is even a more general problem, which I'll call the general longest increasing chain problem. And for some that would be familiar, I take the unit square in the plane, and now I draw an infinite sequence of points here. I just didn't draw the infinite number of points, just n of them. And they might not even be independent of each other. And then I define a function m that depends on n. That depends on n. I look for every n at the first n points that have been generated. And I want to define a random variable, which is called the longest increasing chain of those points, which will be the length of the longest chain in the natural partial order in the plane. Okay? So some of you will recognize this thing if we take the uniform distribution. Distribution in the square of end points. The longest increasing chain, sorry, at uniform distributed, end points uniformly distributed in the unit square, it's equivalent to having a random permutation, picking a permutation one to n uniformly at random. Label the points from left to right in the x-axis and read the labels from up, deep down up. And then it turns out that the And then it turns out that the longest chain is exactly the size of the longest increasing sequence. And this is a well-known problem that has major achievement that we understand well how this random variable is distributed. In this context, I can cast the same generalized longest commercial sequence problem that I just mentioned. You know, you generate x values. To generate x values, this gives vertical lines. To generate y values, an infinite number, this generate horizontal values. Now we'll flip coins for each one of these lines. So for the line at x to i, we have a double g, and we flip a coin, and that gives a color to that line. Okay, one or two. And we do the same for the horizontal lines. And now we place points. Now we place points at the intersection of two lines with the same color. Just rephrasing the generalized longest common subsequence problem. And now the longest increasing sequence here, longest increasing chain, has exactly the same behavior of like the generalized longest common subsequence problem that I want to look at. So the points here are not in general position. So this is state. So is staying flat is that increasing for you or does it have to be strictly increasing? Strictly increasing. Yeah, yeah. So there's another model that I want to introduce here, which is I take a function now, which is defined in the unit square. I look at the same generation of infinite number of Generation of infinite number of x values, generation of infinite number of y values. This is the last model I'm going to introduce. And then for each intersection point, for each coordinate x, y, y, j, I evaluate the function f, I flip a coin that has that probability of coming out the one, and I place a point there if the coin flip comes out one. Okay? And then again, Okay? And then again, this gives I kind of speak about this increasing chain. So I have two models here that I want to play around with. Are the two models clear or so then I can skip this? Okay, so so I will need in a bit, so to perform this some notation. Some notation. If I look at a box which is defined by two, the partition product of two open-closed intervals, I want to define this random variable, which would be the longest increasing sequence, data chain, sorry, of points in this box when I've drawn n vertical lines, n horizontal lines. And that will depend on this I cross state. And of course, there are two, in the first model, in the generalized longest, that was inspired in the generalized longest common system, there are two functions, g and h, and in this case, there's only one function, f, and they define the same one there. I'll make this a bit. It's a bit of an operation. Can we say that the second the first one? This one is related to this one? F is from the square. It's not, yeah. No, it's not a special case because here, for example, you will have, as you can see, if I have these two points, these ones will always appear here. And here, I don't have that. And here I don't have that situation marks. Okay, so they're different models. So the second one is motivated by a paper that I really like and I don't understand at all. So it's by Itimosipan and he looked at the positive quadrant of the integer lattice. And for each IJ, he considered the problem of flipping. The problem of flipping a point with probability p independent of each other, and if the point came out x, he added a point in that position. And then he looks at the, and asks, you know, what is the, if you look at any direction here of growth, what is the, you look at a rectangle, keep the aspect ratio, make it grow. How does the length of the longest increasing Length of the longest increasing chain there grow. Okay? And in order to analyze that, he defines, and this motivates my previous life, he looks at the boxes and then he considers the family of random variables, which are the length of the longest increasing chain in that box. And this family does have very nice properties. Properties, which allow you to apply what is called the Kiemann's Varative-Bergodic theorem. And it tells you that if you move along a direction at a rectangle of aspect ratio x, y and make it grow, you normalize by n, you normalize by n, you converges to a specific function. The beauty of this is that The beauty of this is that he is able to compute this specific function. He gives it explicitly. And I ask people to explain me the paper, then I beg them to come up with a different proof. I didn't ask yesterday as an open problem because it's not an open problem. Here's the solution. But I appreciate anyone who put me out of my misery. Put me out of my misery and give me a proof that I can understand and hopefully more combatorial or based on a general. And I'm really, I give up after this conference, if not nobody. Okay. So what do we show? What we do is that this random variables that I define, if I leave this function f on the unit square, I leave it fixed like p, then Then you get exactly the same behavior of like the Mosetana and then you get the same limit expression. And this is not surprising and it's not so difficult to prove either. But the nice thing is also it works. Oops. Okay. There we go. Yeah. Okay. The same thing. You can the limit exists under the same normalization for this. For this model, that it's generalized longest common sub-sequence, say. Now, the Spatal, the famous Spatal sample constant, well it's a family of constants, but the one that really people are after, it will correspond to this value gamma one half, one half. Okay, the value of one one. The exact value is not known. Approximations, very good approximations are known. The value of this function psi at the Of this function psi at the one-half is this. This used to be the conjecture, not a very strong conjecture, of what the value of the Spada-Sankov constant was. It's a conjecture by Steele, and it was disproved by Reuter. Okay, this is not the value. This value is a little bit too high for this now from From okay, so from the work of Stepananin, we know everything we want here: all properties of this function. It has several properties, and although we don't know this function gamma, we wish to know it. We can prove the same properties. And again, none of this is very difficult. It's a monotone function, symmetric has a non-homogeneity, and it's also super addictive. And so our main kind of result that we're going to show is, I'll now explain it, the two slides. So under some conditions on the function f, which for lack of a better name I guess called psi adequate. Now, this condition is what it's saying essentially is the following. We have this function in We have this function in the unit square, and we have this diagonal. So, what it says is that if you move off the diagonal, your function f can grow, but not too fast. That's essentially what it's saying. Now, if it grows and not too fast, you get more density of points over here than in the diagonal. Diagonal. But this result is saying that, nevertheless, still the longest common sequence of, you know, if you throw many points, you will tend to follow the diagonal in. This is what it's saying. And essentially, it will be exactly the integral of the subanus function of f. The density would be given by f. The density would be given by F at the end of the diagram. Okay, and the second one is sort of treble because it's just, I mean, the second one is just them of Banana's work. In some sense, there's not possible to relax by much this condition, but I won't. And does not vapus the theorem? Constant functions satisfy that property. And functions satisfy that property. And there are others. You know, you can take any function defined along the diagonal and sort of you can make an f that will still satisfy the conditions and establish some closure properties. If you have two functions that satisfy the hypothesis, then you can make other functions that also satisfy the hypothesis. Something similar is also true for this gamma function of Gamma function of which we don't know what it is. So, part of our motivation is trying to get gain some insight into this function by looking at this generalization. And we get the same result. Here about the convergence, the longest common sequence, this is the generalized longest common sequence problem. Under some condition, it will converge to this integral along the diagonal. Along the diagonal, when you take this, this is true if I'm taking the same function g to generate the word one of the words and the other words. It's not true otherwise. Unlike before, this integral you cannot write in terms of explicit earlier, you were able to write it in terms of square root of f ah, those were. Ah, those were examples of functions. Yeah, here I cannot give it, I can only say that there are functions that satisfy this condition, constant functions. That's the only thing I can prove. Although I can, and this, okay, I conjecture, this is a two-day-old conjecture, which I haven't told my previous evening. Right, so you conjectured it on the airplane. No, no, not when I got here. No, no, not when I got here. When we were finishing the talk. No, no, not even. I don't have such a nice story. It's still my room preparing the talk. And it's a conjecture here that this holds for every function. Evidence? How much evidence? None. None of them. Very complex. Very convincing. Okay, that's sort of about it. We have some other results about some Lipschitz continuity properties of this gamma function. I wish, you know, the playing around with this G and H, we can learn something about this Shvata sample function, but I would embed on it. Another question that I thought, so don't put any conversations. Question: So, don't put any condition on F in the second model. You don't put any condition on F, there's still, intuitively, there should be a preferred path for the longest increasing chain going from here to here, depending on F. Can you give some at least for some class of functions F, can you kind of say how that the curve behaves? The curve behaves. And then the last one is: I mean, here what we're doing a little bit is, and people have done it for graphons, kind of combinatorics on generate something according to the limit object of a sequence of graphs. And you ask, you know, what is the size of the largest click there, for example. This depends on, will depend on the Will depend on the limit object that you use to generate. So we're doing the same for words. You can also do the same for permutations. The initial Okay, if here you just choose any to take any mass function. Any mass function in the unit square, which is not the uniform one, that's called a permutal. Okay? And that would be a limit object of a sequence of convergence permutations. Then you can ask, okay, as a function of that density that you have, for example, what can you say about the longest increasing sequence? The longest increasing sequence of those permutations, too. So there are some combinatories that. So there are some combinatories that might be able to want to do also there might be interesting at this and finish. Other questions for Marcus? Sorry. I'm familiar with graphons, but know very little about this model. I'm curious about Lensing. So for graphons, So, for graphons, if we do a measure preserving bijection of the function has generally kept graphs are the same as vertex relating. So, for your model, if you like your measure... You cannot, yeah. So, there's this equivalent relation for graphons, right? Here, no, we don't want to. You're not taking any quotients. So, the functions, the message. So the functions, the measure of functions from 0, 1 to 0, 1, those are the limit objects. And you don't have to take any. Because of course, a word is different if you reshuffle it. Because it's longest increasing terminology. So, for words, what you look is density of patterns. Patterns. Okay? And if the density of patterns, if for every pattern the density converges to a certain value, this is a convergent sequence of words. So if you permute the word, you lose the density, it doesn't make sense. So you're not doing, you are not, you don't have to do what it's done for graphons. And in that sense, also, it's simpler. So in many ways, it's simpler theory. Yeah, the advert is. Yeah, the adver is if you're gonna want to introductory to the theory, look at our paper if you want to lecture on the topic. You do get compactness, but that's paper also, yeah. There's a metric and you get a compact space, and it's just simpler than in the case of graphos. Or something page seventy third pointer, is it a question that you're asking to find C? Yeah, as a function of f and maybe you know natural. And maybe, you know, not for every sufficiently interesting class. So this side. This action principle, assuming a vast energy by the path C. I'm not familiar with it, so I'll ask you more. Thank you. 