So our next speaker is Xiao Shen from the University of Utah and he's going to tell us about temporal correlation in the inverse canal alone. Thank you for the organizers for this nice meeting. I think my talk will be a short, so I'm going to share a story with Timo. So this was like in the second year of my graduate school, so I was taking the year-long graduate probably cyclists. So my first probably class was Timo, and he did Class was Timo, and he delivered a fantastic lecture. So at the time, I was interested in LS and PDE. I thought maybe I'll try modulating. So at the end of the year, I went to Timo's office. As you can imagine, the meeting was very, very short. So at the time, I heard Timo works on a KPZ. But now if you search KPZ, you get the KPZ equation, and then it's a stochastic partial differential equation. I thought, oh, this is perfect. I like PD, and then I don't know the difference between the KPZ universality and all that stuff. Universality and all that stuff. So I walked in and I said, Oh, can I work on like stochastic analysis for you? And then Timo just looked at me and then I thought, okay, maybe I had to adjust by a little bit. So I thought, you know, I like PDE and analysis, but I don't like discrete probability. Because I didn't do too well in a comment. When I was undergrad, I didn't do well in a comment for a class. And the team also just said, you know, look at me. He just said, No, I don't work in silk ass analysis, but you can give this a try. And then he gave me this chapter in his random growth model book about the corner growth model. So then I started reading that. I was like, oh, this is very bad. I don't know if he was offended because we meet it. And then I think later on that summer Timo actually uh emailed me. He said, Oh, I can actually support you with like RA ship. So I really appreciate it and also Timo doesn't hate me. So, my talk is just going to be about the temporal correlation in the scammer polymer. This is drum work based on drumworks with Whitney Pasu and Timo Sebal. As it was seen here, the height function here is an object of study in the KPZ, a very important object in the study of the KPZ universality. It's often given by HX and T, where X is a spatial variable ranging over the real numbers and T. Ranging over the real numbers, and t is a time variable which is non-negative. So, one way to interpret the height function is that we can think of this as a model of a growing interface. So, imagine here we have two photos of a growing interface at time t1 and some layer time t2. And hx of t1 can be thought as the height of the growing interface at site x at time t1. And similarly, for h of x t2. And another way to interpret this height function here. To interpret this height function here, we can thought of this as some kind of length of the geodesic in some kind of random metric. So, in this case, here we have a space location x, and then we look at the geodesic between the origin to the point xt1, xt2. And then the height function here can be also thought as the length of these geodesics. Now, once we have the height function here, we can fix the time and vary the location, and this is the spatial sound. And this is the spatial status of the height function. And sometimes people call this profile, you know, free energy profile. And then we also have the temporal process, which is we fix the location and then we vary the time. And in general, a lot less is known about the temporal process. So we go along the time. And I guess one way I kind of think about it's not precisely accurate is that if we look at the Boozma increments, along the downright path, they're independent. But along the upright path, the Boosma increments, you know, the distribution. Path, the Boosman increments, you know, the distribution is hard to describe. Okay, so now let's, so for this up, we can look at the temporal process. So in this case, let's fix the location x0, t0. And one natural question is to ask, okay, so can we, do we know the joint distribution of HF0T1 and HF0T2? And this is yes. So there are the works from Baik, Liu, Johansson, and Rachman. And so they were able to prove the explicit formulas for this joint series. Formulas for this joint distribution. And I think in theory, you hands in it for several models, you know, Brownie, LPP, LPP, LPP. And however, at this moment, I believe it's still for us to, it's hard for us to actually utilize these formulas. For example, we can ask what is the correlation of these two random variables. And somehow it's not, yet we don't know how to compute the correlation out of these in very impressive formulas. Formulas. So now a little bit about the study of the time correlation. So this is a great interest in the physics community. Somehow, this is, so the study of this is done by several physicists. This is through experiments, numerical simulations, and non-reverse arguments. And in the mathematics field, this is really started by Patrick Ferrari and Herbert Schwong. So they essentially pose this problem, and in the zero temperature setting, for the In the zero temperature setting for the corner model. Then, this was studied by several other authors: so, this Reedy Rasu, Patrick Fari, Genghui, Alexandra Oceali, and Herbert Japon, and Ling Fujiang. So, this is for the corner growth model or last faster perpolation with various initial conditions, as well as the half-space LPP. And for the positive temperature model, And for the positive temperature model, there's the work for the KPZ equation from Haivon-Borwin, Fromet also, and Alan-Henman theory. And I'll talk a little bit more about these works. And the techniques involved for the zero temperature model, they can be classified into two cases. There's the integral probability and also the percolation methods. And the study for the K-P's equation is done using this Gibson-Reino sample, which is a technique introduced by Ivan. Technique introduced by Ivan and Alan. We're going to stop here. We're going to add another model to one of the lattice temperature models, which is known as the inverse gamma polymer. And the techniques we use here is the coupling methods combined with the percolation methods, the combination of the two. And as you notice, the coupling methods here is developed by Demo and many, many of his collaborators in the audience here. And then the coupling methods here enters in the picture in two important ways. Enters in the picture in two important ways. I guess the first one is that in order to apply these percolation methods, we need the precise, nice, moderate deviation estimates. This means we need the sharp bounds, like the upper and lower bounds for the left and right tail. And then this reason becomes available using the scalping methods, using the EJS formula that Phil just talked about. And there are also others work from Nico's. Others work from Nico and Jonash Artman, Wortman, Jonash Wortmann, as well as Melan Ashishendu about the lower bound of the left tail. So now these four estimates were become available. So we can apply the perpolition methods. And another reason that we do the combination of the two is because often in the propellition methods, they refer to this object called geodesic, which does not really exist in the Which does not really exist in the positive temperature. So sometimes, you know, it's not straightforward to have a, like, you know, to lift a percolation argument for the zero temperature model to the positive temperature. So I can give one example in the exact solid case. So, you know, in the zero temperature model, let's just say the corner world model, so one way we can think about one way to something interesting studies is that we can, for example, we have two semi-opener geodesic. Let's say they're in the same direction. They're in the same direction. And one interesting point when I ask you, what is the probability that these two semi-infinite GLA they coalesce too slow? So they would stay disjoint for a long time and then they coalesce. So this was done in the zero temperature model with both perpolation method and also the Doff method. But however, now in the positive temperature model, for example, inverse gamma polymer, we can ask a similar problem. So now I have these two points here, two vertex and vertices. And vertices. I have two semi-infinite polymer measures now. So, if they were coupled in a natural coupling, we can also ask: so, what's the probability that we see the sample path of the state is going for a long time? Or another way you can think about if the two measures are coupled independently, so then this would be what's the probability that these two, the first meeting time of the two sample passes would be far. And so I believe currently, so for the in-merg scammer polymer, so the purple. Polymer. So the percolation methods from there will not directly apply to the positive phase. Because over there, it involves this very nice results about the rarity of the disjoint geodesics. I think Alan is an expert on this type of argument. The disjointness of geodesics. I should say the existence of multiple disjoint geodesics is important. And however the and then and I think this problem can be solved using the the context of this. Next. I'm going to again talk about why we study the inverse gamma polymer. Well, first it's because it's introduced by Timo. And also, um so for the inverse gamma polymer there are fewer tools are available. Uh for example, the scaling limit of the profile to every two process is not known for the inverse gamma polymer. Is not known for the inverse gamma polymer. And also, we don't know any multi-time joint distributions for the free energy across two different kinds. And so, our here, so we're here. We're going to use the Copping technique plus the percolation technique. In particular, it does not involve any integral methods. And lastly, is that because the inverse gamma polymer is higher up in the hierarchy of the KPZ models, this is what Phil talked about, you know, the relation between the six-vertext model. The relation between the six-vertex model and ASAC. So, this means under some transformation or scaling limits from inverse gamma polymer, one can get to the corner rope model, Brownian LPP, O polymerial polymer, and also the KPZ equation. And then somehow we're hoping like our tools developed for the inverse gamma polymer, or the results that we obtained for the inverse gamma polymer can be translated to the rest of the models. That's this. So, next we're going to define the model. So, unfortunately, I actually have sticked my notation from Irick's talk. So, here, my W here is the boundary, and the Y here is the ways inside the bulk. So, WK here, we can think about as a two-sided multiplicative walk with W of 0 is 0, 1. And let's assume that W here is non-negative. Non-negative. So you can imagine these W's here that appear on this anti-diagonal origin. And in the bulk here, we have IID inverse gamma random variables with the shape parameter mu. And in this hot here, mu will just be fixed. So there's at least some fixed parameter. So now we can define the bulk partition function, which is just a partition function. You know, just say a partition function inside the block, which is not C is the boundary. So Z of AB is the sum of all collection of direct paths from A to B. And then here we do the product weights along this path, but we ignore the starting point. So we don't include the weight at the starting point. And the way to think about it is now z tilde of a is equal to 1 instead of y of a. So we ignore the weight at the starting point. So now with this, we can define the actual party functions of this model with the boundary. This will just be, you know, This will just be so we do the summation over all integer all integer k and then we do the w k times the bulk partition function. I don't think I'll be doing any calculations with this part functions. And now we can think about three different classes of the WK, which is they're sometimes called initial condition or the boundary condition. The first one is called the droplet. The first one is called the droplet initial condition. It's also narrow wedge or narrow wedge and step initial condition. And in this case, wk is equal to 1 if k is 0 and it's 0 everywhere else, so this is a drop initial condition. And then we also have the flat initial condition. wk is just equal to 1 for all the value of k. And lastly, we have this random initial condition, which is because w itself is multiplicative block, so I just know. Multiplicative log. So I just often just take the log of this, and the log of wk is now just going to be a two-sided additive random log. And then we fix the zero, that's zero, considering the algorithm. So we can think about log of wk because xi is okay, I'm just going to write this as a one-sided random line. So As a one-sided random line. So log of delta k when you think about this as del k, where xi is the increment of the log of delta k. And in particular, there's a very important random initial condition or boundary condition, which is called the ratio stationary initial condition. And this, I think, appeared in Martin's talk. Okay, so I just define a please, I won't spend too much time. Define a pleasure. I won't spend too much time on this. It's important for the paper, but not really for the class. So, this just means we make a particular choice of these xi's. So, these xi, there's going to be iid ratios of inverse gamma, independent inverse gamma distributions with mu, with shape parameter mu over 2 mu over 2. And the reason, as we're going to show, this is ratio stationary is because, so imagine we have this walk w over here on this antidepali cross. Here on this antenna line across the origin. And we can also define another WK here, I call it W total. This is going to be the ratio of the free energy between these two points. So this is n plus k, n minus k. And then so there's, so there's a w zero and w k w tilde zero, which is the profile, and then these two have the same distribution. So it's stationary. Okay. Let's see. So, next, this is our main result. So, for the droplet initial condition and the random, for large, I should say, for large class of random initial conditions, and then we have the following estimate, which is, you know, so the first case here is when r is small compared to n. So this is r is giving some constant, but less than n over 2. And then we see that the time correlation for the fertility for energy is upper and lower bounded by r over n to the one-third. By r over n to the one-third. And in the other case, when r here is close to n, and then the free energy here is bounded by, I should say, one minus the correlation is bounded by the term n minus r over n2 states. Okay, so now first for the droplet initial condition, so the same type of results were obtained by Reedybus. By Ridi Basu and Shishinju Genguly in the corner growth model, and also by Ivan Corwin, Comin Okolso, and Alvin Hammond for the K-B equation. And our work here is inspired by the corner growth model. And we also incorporated some of the stationary techniques to prove this result for the inverse. And there's also an asymptotic result. This is done by Patrick Ferrari and By Patrick Ferrari and Alexandria Ocelli. So, in their work, they set R to be epsilon n or 1 minus epsilon n, then they send n to infinity. And by sending n to infinity, they were able to give an explicit leading order constant in these two, in the time correlations. However, the constant here is expressed as a variance of the function of every two processes in Brampton motion. So it depends on the integral probability input. Depends on the integral probability input very strongly. And another, okay, another thing I'll talk about is when n goes to infinity. So when n goes to infinity, as you can see here, maybe I'll just point it out here. It's not only a condition on the end, it's also a restriction on the R. So when R is small, but R also has a road in it. And this is something that we'll talk a little bit more on in the next slide. So now with the tools that we have developed for the droplet initial condition, and we can also look at the random initial condition. And we can also look at the random inertial conditions. And for the random initial condition, the only work here is by Patrick Farre and Alexander Ocelli in the corner world model. And they only look at the regime where R is large, where R is very close to N, N to infinity. And again, they obtain this leading order constant, but because their techniques depend on integral probabilities roundly, so the class of random initial conditions they consider are only Initial condition reconsidered, only a constant multiple of the stationary inertial condition. Okay, and as you can see, the result, our result here covers the case R is also small, well, so over here. So here is the case, R can only be large, so there is no results for the smaller gene. And also because our techniques that we have developed does not involve integral methods, so we were able to prove this for a much larger class of. Much larger class of initial conditions. So, first is that the increments are just independent. I think currently the paper is written in an independent way, as with this assumption, but this is not really essential for estimates, so maybe I'll just write it down right here. So, with this type of definition. Okay, so basically, the first assumption just we need So, basically, the first assumption just we need the independent of the x i's. But it can be replaced by the following two conditions. So, the first one is a strong mixing condition for the x i's. So, basically, it just means if we look at the covariance of, let's say, summation i goes from 1 to n of xi, and summation of, let's say, j goes from n plus k to infinity of xj. If we look at the If we look at the covariance of these two, this should be bounded by some alpha of k for all n. And then the requirement we need here is that we need this alpha k to be some k equals 1 to infinity of alpha k to be less than 5. So there is some kind of strong mixing condition on the steps x5. And the second condition is FKG inequality for nested partial size. So imagine if we look at the probability of the two events, we're looking at the summation. Two events where we have a summation. I goes from 1 to, let's say, 10 of xi. I want this to be bigger than A, and then I intersect another event. Summation i goes from 1 to 100 of xj, to be bigger than b. So if I look at the intersection of these events, because this probably here should be bigger than the product. It's like i go to 0.10 x i. Okay. Probably x. So I'm mentioning j is 100 xj. I guess if the x i were independent, then this would just be the back page, not the inequality. So the independent just means we need these two precisely, not proof. All right, and the second condition is on the running maximum bound for this random log. And I think I was just preparing the talk. I don't think we need. Was just preparing the talk. I don't think we need exponential moment. Some high polynomial moment bound here should be enough. Maybe like 1 over t to the fourth power or something. But again, then this one, the format or the notation is a bit better because this exponential bound also matches with the one from the moderate deviation as a. And the last, the third one, sorry, not the last one. The third one is that we need, there has to be some small probability such as this random wall. Small probability such that this random log takes a large value, which is t0 squared. This is just something for some fixed t0 and epsilon zero. So basically, for every t0, there exists epsilon zeros that should disfold. And the last condition here is that we want the variance of this log wn to be of the correct order, just like a random block. So you have a random block of n steps, so the variance will be of order n. Okay, so these are the initial conditions, although it looks complicated, but now if we assume. Complicated, but now if we assume the steps are actually IID, and all the assumptions here just boil down to some moment assumption on the edge rate over on the steps of the random one. And lastly, this is the thing that I mentioned. So the estimate here we obtain is for finite n and r. So in this case here, r n, n minus r, it could be some quantity that is much smaller than n. So this is something that you don't quite see in the limiting case, where r has to grow. Case where R has to row with n, where the distance between R and also has to row with n. And maybe I'll say a little bit about number three here. Okay, so now imagine we can think about this like in the zero temperature or positive temperature. This is, we look at the geodesic or a path between zeros and nn. So we know it's rare for the geodesic to fluctuate on the scale to go away from the diagonal by tn to the two-thirds because just by the shape, by the curvature from the limit shape. Curvature from the limit shape, each side will lose order t squared into the one-third amount of free energy. So this would be rare. So now imagine if we lift this antidiagal line toward n, so this is the second picture here. So now imagine the distance of n minus r is much, much smaller than n, where n minus r is some constant, let's say it's 100. So now, again, looking at the, you know, from the curvature, we know the red part, on the red path, or red geodesic, we'd lose. Geodesic, we'd lose t squared minus r to the one-third non-free energy, given this high transversal fluctuation. But on the other side, this blue geodesic is a typical one, so it tends to fluctuate on the scale of r to the one-third. But now, if n minus r is very small, like n2051 or some constant, this fluctuation here is much bigger than this, you know, then this log loss on the red part. So one cannot just, so this is supposed to be the local fluctuation. So then what we can do is. So now, what we can do is, you know, we can just look at the GNS from 0, 0 to RR and also from R to N. Now, this is the part. Now, if we look at the, compare the blue geodesic and then the green geodesic, so I should write that. So the profile along the orange part, it should behave like a random block. So maybe I'll write the estimate, and then I'll just go better. So blue minus green. So this should be equal to some in zero random log. I go from 1 to t in the minus r to the 2 thirds. I'll just write xi. This is just coming from, you know, the locally, the profile should be a random block. And then this is very important for the network. And then this is very important for the analysis of the models in the KBZ model, in the KBZ. Universality, for example, there's a KBZ fixed point, and there's also the Brownian Rity of the RI2 process. Jeremy, Baland, and also Balin. And there's also the Browning-Gibbs property. This is developed by Ivan and Holland. And lastly, this is so this is this local, this red walk behavior in this work that we do is from the Coptic method, which is interesting. Copting method, which is introduced first by Eric Kakator and Pete Hunebu. And then this was further developed by a lot of the co-authors, collaborator of Timo, so I won't name everyone here in the crowd. So basically, it essentially says for any of the initial conditions, this orange part behaves like random walk. So now if the blue minus green is random walk, so this is a random walk of order t minus r to the third, then the fluctuation of random walk is a. Then the fluctuation when we walk is the square root of the stats. So this means we will gain at most square root of t n minus r to the one-third amount of free energy. So now we can see that now the loss on the top from the red is t squared and minus r to the one-third. So it rules out that this local translate, this kind of local high transversal fluctuation should be and this yeah. Right, so uh I guess I have five minutes. Yes, I have five minutes. Okay. In the last five minutes, we can look at the larger regime. And the key idea here is that just because of this random log comparison that we did, the idea is that we should be able to replace when r is large, we should be able to replace the free energy from 0 to n by two parts. The first part is the free energy from 0 to R, which sees the boundary, and also the bulk free energy from R to N. And as you can see here, I wrote this little And as you can see here, I wrote this little formula. This is identity for the variance. So on this side, we have an infum among some variance. And then the way to prove this identity is very simple. You just take the derivative of lambda, you set it to zero. And then you can solve for lambda, you plug it in, you get to the right side. So it's just a formula. And from this formula, what this gave us is essentially this d equality. So one minus the correlation of these two is upper and lower bounded by these two quantities here. And then we over here, well, Then, and then we over here, well, I highlight these terms. These terms are free energy from 0 to n, free energy from 0 to n, and then this is free energy from 0 to r. This appears lambda here, 0 to r. And the next what we can do is let's do the replacement. So, suppose if we replace this free energy from 0 to n by the sum there, and then we see there's a cancellation between this term here and this term here. So, what we end up here is just a log of z tilde from r to n. Of z tilde from r to n. And on the other side, we also do this replacement, and we replace this log 0 by the sum from 0 to r and r to n. Because here we also have a lambda 0 to r, so we can combine the 0 to r free energy. So I think I missed a log here. So it would be 1 minus lambda with the law of this, and then with this term here. But now these two terms, they're independent. So we can split the variance into two parts. But then to give the lower value, we just ignore this. So on the top, Just ignore this. So, on the top, we also just have log of z total of r to n. So, now we get this desired bound, which is one minus variance is upper and lower bounded by the two terms of the grand order. So, this is basically the idea from the proof for the large RP. So, for the smaller, it's basically the following. So, again, in this case here, r is small, so we're going from 0, 0 to n, n, and this is in the drop phase. And then, we imagine, so I'll be talking the 0. Imagine, so I'll be talking the zero temperature intuition. So there has to be an overlap of the GLSI of order R. If we look at the GLS10R, and there's a GLS100. So the overlap of order R. So now if you look at the covariance of the two, just imagine the part that they're not overlapping, assuming they're independent, then the covariance becomes the variance of overlap in the voltage. And because of the amount of overlap order, And because of the amount of lattice order r, then we know the variances of order r to the two-thirds. So this gives us, you know, the small routine. And then, yeah, this is r to the two-thirds. And if we divide it by sort of their two variances, we get like an r1 to the one-third, which is the same length. And on the other side, for the model with a boundary, it's because now if we're looking at two density, what happens is that they both overlap on the boundary. They both overlap on the boundary for r to the two-thirds number of steps. And as we assume, you know, on the boundary, the things are correct order. So, okay, now also imagine that these paths here are independent. So now the covariance just becomes the variance of overlap of the boundary. And then the overlap here is of order r to the two-thirds. And from one of our assumptions, now the variance of this is also r to the two-thirds. Okay, and that's the small RP. And that's the small RP uh button. Thank you so much. We can access uh joint moments at multiple times uh joint moments uh looking at covariance. You're looking at covariance of energy at two time points. Yes. With your technique of access if you have multiple points and time direction. Okay. So so sorry, you you want like the the joint moments of the case would be So, I mean your case would be when you are looking at two times, right? Fans power is one. Oh, I see. Okay. Or generally would would be. Yeah, sorry, I don't know like a direct way. I don't know a direct way to see. For example, you can do more than two points, or you have to maybe the higher moments might be possible, but for now I don't see a way for getting more than this might be a small point, Joe. So is there other conditions on these X's for it to make sense of that second term? Where it's just to make sense of that second term, this sum from j starting n plus k to infinity. I get what you're kind of saying about this, like it's a mixing property. What is making that sum to infinity make sense? Oh, sorry, yeah. Maybe, let's see. It's not related to infinity. Maybe to some large, large fixed constant. But it basically we're just saying, you know, like the free energy on this portion of the boundary and the free energy on that portion of the boundary, they should be. Person of the character. Because the endpoint is always fixed. So this always just goes to a finite subject. Thank you for the public. No further questions. I'll stay on. We'll meet back here at 4:15.