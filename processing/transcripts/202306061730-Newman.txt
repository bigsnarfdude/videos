 Our next talk is linear embeddings of random complexes by Andrew Niemann. Thanks. So, first, I want to thank the organizers for putting together this very nice workshop and for the invitation to give a talk. So, this paper is on the archive. Here's the archive identifier. So, okay, so I will get to some things that are more like classical theorems from geometry combinatoric topology in a bit. But first, some setup is some random simplicial complex background. So, our object of study will be multi-parameter random simplicial complexes. So, this was a model. So, this was a model of random complexes introduced by Costa and Farber, where you start with a ground set of invertices, and you first include each edge with probability P1 to construct a random graph. And then wherever you see a triangle in that random graph, you fill it in with probability P2. Then you get a two-complex. Wherever you see a tetrahedron boundary, you fill it in with a three-cell with probability P3. Cell with probability P3 and so on. And so the PI's are the conditional probabilities to include an I-dimensional face given that its boundary was included in the previous step. So this model generalizes a couple of well-studied random simplicial complex models that people are familiar with by now. So Linneal Meshulam model is a special case of this, where you have the full D minus one skeleton, the D-dimensional faces are random, and then you have nothing. Faces are random, and then you have nothing above dimension d. So you have, you know, one probability parameter that's interesting. The ones below it are all one, the ones above it are all zero. Matt Kale's random clique complex model is also a special case of this where you build a random graph and then just fill in all the cliques. So, various topological results have been established for this model over the past 10 or so years. In particular, This series of papers shows a critical dimension phenomenon where basically, as long as you're in a sparse regime where the probability parameters are going to zero as n goes to infinity, the homology is, at least the rational homology, is all concentrated in the dimension where the f vector attains its maximum order of magnitude. So, yeah, so this is this, you know, these are these. So, yeah, so this is this, you know, these homology results have been established. And so, today we're looking at a geometric property, namely embedding the random complex into Euclidean space. And so I'm going to be talking about linear embeddings. At the end, I will say a bit about other types of embeddings, some things that are known about, for instance, PL embeddings and some of the challenges to studying PL embeddings. The challenges to studying PL embeddability or topological embeddability for this model. But primarily for this talk, we'll be thinking about we just want to map the vertices and then have the simplices be the convex hulls of their vertex sets. Okay, so that's the setup. And so as a warm-up to this, let's do the two-dimensional case briefly. So the following can be easily proved. If you just take an Erdős-Rainey random graph, You just take an Erdős-Rainey random graph. So, if your probability parameter is much, much smaller than one over n, then with high probability, your graph can be embedded linearly into the plane. And if p is much, much bigger than one over n, then the graph is non-planar. So there's a brief sketch of how this goes. For p much, much smaller than one over n, we can count the expected number of cycles. So by linearity of expectations, So, by linearity of expectation, this is just the sum from k equals 3 to n of the number of cycles of length k. So, we pick our k vertices, we pick a cyclic arrangement of them, we need all the edges to be included, and so we get this, and then this is at most n times p to the k. n times p is going to zero in this regime, so the expected number of cycles is tending to zero, so with high probability, there are no cycles. There are no cycles, so in this regime, GNP is a forest and is hence planar. For the other side, the number of edges in an Erdos-Rainy random graph is binomial with n choose two trials, success probability p. And so by Charonoff bound, if p is much bigger than one over n, then we expect to have To have way more than order n edges, and so by Euler characteristic, the graph cannot be planar. And so, just to say a little bit more about this, so I just kind of sketched out a simple proof that gives bounds on the threshold. This problem has been studied in much more detail. So, it's known that the phase transition for planarity is at the so-called Erdős-Rainy phase transition. Phase transition. So at one over n, the behavior of a random graph changes quite drastically. So if p is equal to one minus epsilon over n for, say, epsilon fixed, then an Erdős-Rainy random graph will just be a disjoint union of small components that either each of which either has one cycle or no cycles. So this is a planar graph. But on the other side, if it's one flow, On the other side, if it's one plus epsilon over n, actually, so there's something even sharper than that known. That the graph is non-planar, and moreover, there's a giant component. And this is one of the classic results from the study of Erdős-Rainey random graphs is this space transition at 1 over n. And that's where there's also the shift from planarity to non-planarity. It's additionally a one-sided sharp threshold. A one-sided sharp threshold for the existence of cycles in the random graph. So before the threshold, there's a positive probability there's cycles and a positive probability that there's not. Above the threshold, the probability of g containing a cycle approaches one as n goes to infinity. And then there's this a giant component as well. Okay, so my main result that I state here gives the threshold in this multi-parameter. Multiparameter setting up to the right vector of exponents. So I'm going to take alpha one, alpha two to be just some vector of positive real numbers. And I want to think of my probability parameters as being n to the minus alpha one, n to the minus alpha two, and so on. And so to state this theorem, and I'll say on the next slide kind of like how we should, how we should think of this, but to just precisely state what the But to just precisely state what the threshold is, I have the precise statement. But in any case, we take v sub s to be the vector of binomial coefficients, leaving off s choose one because the vertices are all included, so the vertex set is not random. And so, if we take, if we want to know if our complex embeds into dimension D, then we take D plus one over two floor, D plus one over two over two over two. One over two floor, D plus one over two ceiling, and we have these hyperplanes in, I suppose, out to some number of these alpha i's. We don't have to really check. We don't have to worry about like having infinitely many of them because in practice, we're only considering finitely many at a time. But we have these hyperplanes in that space. In that space, where in between the two hyperplanes. Yeah, so okay, it'll be easier to see with an example, but we have these two inequalities. And so if this holds and this holds, then with high probability, our complex does not linearly embed into Rd. But if either one of those inequalities goes the other way, then Inequalities go the other way than it does in bed into RD. So, this gives the right threshold up to the exponent. It gives both sides. I'll say a little bit at the end about sharpening these to be more than just giving the right exponent. But okay, so yeah, so I mean, I have these inequalities that come from this, but basically, the way to think of this is what this theorem is saying is that if you're in 2K dimension, if you want to know if your complex embeds into 2K dimensional space, into an even. Into two K-dimensional space, into an even dimensional space, then if the number of K-dimensional faces, so the expected number of K-dimensional faces, is sublinear in the number of vertices, then it will embed. If it's superlinear in the number of vertices, then it will not embed. And for odd dimensions, there's a bit more, the condition is a bit more complicated, but not so much. It embeds into an odd-dimensional beds into an odd dimensional 2k plus one dimensional space if either the number of k-dimensional faces is sublinear or x is just at most k-dimensional. And then it obviously embeds by general position. And otherwise it does not. Okay. So here is just to kind of get a pick get a get a sense of like sort of geometrically what kind of this is saying. So if we have just two parameters, alpha one and Alpha one and alpha two, and then all the other parameters are zero. So we're just building a random two complex, selecting each edge with probability n to the minus alpha one, and then filling in each triangle with probability n to the minus alpha two. Then each of these regions, if you pick your alpha one and alpha two in this regime, you'll get something that embeds into R1. But then over here, this is a two complex, so it will always. Complex, so it will always embed an R5. And if your alpha i's are over there, then alpha, then five is the best that you can hope for. One thing I think that was a little bit surprising to me is that, so down here, this is where alpha two is equal to zero. So we are building a random graph and then just filling in all the triangles. And it actually jumps from embedding into two dimensions into. Embedding into two dimensions into embedding into four dimensions with probably a very small window where it does embed into R3. But basically, it almost always one thing that comes out of this is that, so if you didn't stop this at like two dimensions, if you just took the clique complex and built a random graph and then filled in all the cliques of every dimension, then other than when it embeds into R1, the embedding. It embeds into R1, the embedding dimension would actually always come out to be even, except at these countably many points. Okay, so to kind of give you an idea of how the proof of this goes, I'm just going to go through the proof of a special case. So for this talk, I'll show that when alpha is between two-thirds and one, the minimum embedding dimension of a random clique complex with n vertices and With n vertices and probability n to the minus alpha is four. So the random clique complex, we're building a random graph and then just filling in all the cliques to make it a random complex. And yeah, this is what I was saying, that the embedding dimension is almost always even for random clique complexes. Okay, so the like piece of this that involves Radon's theorem, which I will get to, is the dense side of this. Is the dense side of this phase transition? So the non-embeddability piece. But we'll first go through the sparse side. So showing that the embedding dimension in this particular case is at most four. So why can we embed this complex into R4 with high probability? Well, so first of all, and I mean, this just follows from standard things in random graphs that if alpha is bigger than two-thirds, Is bigger than two-thirds, then the expected number of four cliques in our random graph is n choose four times p to the sixths. So n to the four minus six alpha. Alpha is bigger than two-thirds, that goes to zero. So in this regime, the complex is two-dimensional. That's a good place to start if we want to try and embed it in R4, but there's still some things to do. So we can assume that this complex is at most two. This complex is at most two-dimensional. And so, what we want to do in order to come up with this embedding is we want to consider the complex that we get if we just first throw away all the edges that are not in any triangles. So we throw away all the edges that are not in any triangles, and then we look at the structure of the complex that remains. And we show that such a complex can be. Complex can be collapsed away by taking vertices that are only in one triangle, deleting them, and the unique triangle that contain them. Keep going, keep going until we get just a graph. And so why does this help us with the embedding? Well, what we'll do, we start with X. It's at most two-dimensional. We delete all the edges that don't belong to any triangle, and then we do this. And then we do this sequence of collapses until we get down to something that's one-dimensional. And that's where we also use. So, being able to show that we can really collapse down to something one-dimensional is also where we use the fact that alpha is greater than two-thirds. And then we can just reverse all of that. So we get down to a graph, and now we want to add these triangles back in, but we're in R4. So we just, we have our. So we just have our edge, we want to tack on a triangle, we find a plane through those two vertices, and we just cut out a triangle from that plane, and we can build that part back up. And then since we're in R4, so it kind of seems like from there we could do this in R3, and I guess we could, but then we want to add back in the isolated edges, but we're in R4, so we can add those in by general position, and they won't intersect any of the triangles that we. The triangles that we have. So, this gives us an embedding into R4. So, this is sort of the argument for the sparse side for this phase transition. Okay, so now we want to show the dense side. So, we want to show that for alpha smaller than one, Smaller than one, our random complex does not embed into R3. So, the proof of this will make use of Radon's theorem, which I've stated here, but we've seen this in lots of talks over the last couple of days. So we know that any set of D plus two points, if we have our random complex and we attempt to map it into, say, three-dimensional space, any set of points. Dimensional space, any set of five points will have a radon partition. And furthermore, we can assume that we have a general position embedding. And so the radon partition will be unique. And so in order to show this, what do we want to do? Well, we want to say that suppose we place the vertices in R3 first and then generate our random complex. Well, the placement of the vertices works. The placement of the vertices works if and only if, for every set of five vertices, the radon partition on those five vertices is not realized by the simplices on those five vertices. So if we have five vertices that are split three and two, we have an edge cutting through a triangle, that edge and triangle better not be included, or else we have a problem with the embedding. And so I just call that a I just call that a radon match if there's a radon partition into sets A and B, and the simplex on A and the simplex on B are both included. That's a radon match for the placement of the vertices into R3, followed by generating the random complex. Okay. And so for this particular case, where Where we are in our three, if we do have a two, three partition, so an edge cutting through a triangle, then the probability that that particular set of five vertices has a red unmatch is p to the fourth, because we have to include the three edges of the triangle and the one edge of the piercing edge. And so for a first moment argument, And so, for a first-moment argument, we need to know how many of these 2,3 radon partitions there are. Because the idea will be for each set of five vertices that are 2,3 split, the probability that we have a radon match there is n to the minus 4 alpha. So if we want to just compute the expected number of radon matches, this would be this p to the fourth times the number of two, three splits. And we, in this And in this particular case, we really want 2-3 splits because, like a 4-1 split, we wouldn't expect that to happen in this regime because we already showed this complex is at most two-dimensional. So, we won't have any tetrahedra that'll surround a vertex. So, we really do want these two, three splits. So, for that, we'll use this theorem approved by Pavla, Florian, and Google. By Pavla, Florian, and Gunther, that if you have D plus three points in D-dimensional space, then there is some way to remove a point from that set and have an even radon partition, as even as possible. And so from this, well, what do we do? Well, for every set of six vertices, there is some even split on those six vertices, but each one. Those six vertices, but each one can be counted like n times. So we have something like, actually, I guess we have, yeah, we have at least this many from this theorem. One, six times n choose five, two, three radon partitions with, given a fixed embedding, no matter what that embedding is, we always have at least this many radon partitions that are two, three split. And so then in expectation, Expectation: the number of radon matches is at least the number that are two, three split times the probability of a radon match on a particular one. And so because we pick alpha to be smaller than one, this comes out to be n to the one plus epsilon, or some epsilon that depends on how close alpha is to the one. And so I, you know, this. And so I, you know, this, this is. So then there's an application of Jansen's inequality, which I leave out. It's just some kind of tedious computation to show that not only do we have the expected number of Radon matches is large, but we actually have concentration of measure, that there really are about n to the one plus epsilon radon matches with probability going to one. It actually goes to one very quickly. So So, this is like the expected number of radon matches. So, kind of the best we would hope for is that the probability that we have no radon matches is like exponential and minus the expected value. And that's what we get from Jansen's inequality, which looks at like, because we have, you know, two sets of five vertices. Generally, they're going to be the simplesties we get on these five and the simplesties we get on these five are going to be end. The simplest things we get on these five are going to be independent unless they share, um, I guess, two vertices. And so there's just some computations to check to handle the question of the variance of this random variable, counting the number of random matches. Okay, so what does this show? This shows that if I put the vertices into R3 and then generate the complex, it's not going to, it's not going to. It's not going to give you an embedding. But of course, you can just try another way of placing the vertices. And so we have to take a union bound over all embeddings. And so, but for the purposes of this proof, two embeddings can be regarded as being the same if they induce the same radon partitions on every set of five vertices. And so how many. How many non-equivalent embeddings? So, we just have to count how many non-equivalent embeddings there are, which is clearly finite. And we can get a bound on it coming from this well-known fact about hyperplane arrangements, because we can inductively compute an upper bound on the number of non-equivalent embeddings. Because if we have an embedding of endpoints, that gives us a hyperplane arrangement from all n choose D hyperplane. All in choose D hyperplanes, and then the next point will induce the same radon partition. The radon partition induced will just depend on which chamber of this hyperplane arrangement our point lives in. Okay. Yeah, so then the number of non-equivalent embeddings of endpoints, well, we just take this. Points. Well, we just take this, which is at most m to the d. So we get it, we get like n factorial to the d, but that's just exponential n log n. And in fact, that is the, at least up to the logarithmic scale, the right rate of growth because we do have the n factorial ways we could arrange these points on the moment curve. And so we get, you know, just, we get. Know, just we get a lower bound on this, even though we need it. We, you know, for this proof, we only need the upper bound, but this is the best upper bound we can hope for. So we have like n to the n non-equivalent embeddings to check. And the probability that each one works is going to zero faster than the number of embeddings is going to infinity. So this, you know. This wins the race, and so with high probability, X does not embed into R3. So, okay, so okay, so I just did this for the case of the cleat complex because it's much easier to kind of like to talk about, but the general case works basically the same. It's the same idea. You show the sparse side by doing this kind of collapsing thing and then placing what you have in. Then placing what you have in D dimensions in general position and then building it back up. And then the other side, the dense side, is counting the number of is counting the number of radon matches, showing a concentration of measure inequality and counting the number of non-equivalent embeddings. And then there's some like basic linear optimization to make sure. like linear optimization to make sure that like these two these two inequalities are really the only two that we need um that the other ones that we kind of implicitly need are implied by by those so anyway so yeah uh there's not really much much more to the multi-parameter model that's that's uh more than just some like tedious kind of calculations okay so um what about other types of embeddings so um there's this uh result of uli wagner that Of Uli Wagner that if that deals with PL embeddings in the Linneal-Meshulam model. So in the Linneal-Meshulam model, you have your complete D minus one complex, and then you add in each face independently with probability P. And so he showed that there are two constants where. Where on C is smaller than the smaller constant, then if your probability parameter is C over N for C less than this constant, then the complex is P L embeddable. But on the other side, at least, well, there's a gap in these two constants. But for C bigger than the second constant, Y D N C over N is not PL embeddable in In R2D. And basically, I mean, for lineal Meshulam, since it's a fixed-dimension model, the question of embedding it into twice the dimension of the complex is basically the only interesting embedding question that we could ask. I suppose if P is very close to zero, if P is going to zero very quickly, I think maybe you could embed it into like 2D minus one dimensions. But anyway, so. Anyway, so this is the PL embedding result that exists in the literature. Since then, the dense side of this threshold could be proved by the Grunbaum-Kalai-Sarkaria conjecture settled by Adi Procito. So there's another way to prove this using that theorem. But yeah, but. Yeah, but okay, yeah, maybe I'll get to that in a second. Okay, so one thing that I like about this proof of this linear embedding is that it gives a probabilistic proof that Faire's theorem is not true in dimensions bigger than one. So Fairy's theorem is that a graph is planar if and only if it can be drawn with a straight line embedding. Here's a probabilistic proof that that doesn't work for higher dimensional complexes because the thresholds for linear embedding into R4 are different than the thresholds for P. Different than the thresholds for PL embedding in R4. So, anyway, yeah, so I wanted to say a little bit more about the PL embedding. So, I think this is an interesting question. So, even if you're using the Grunbaum-Kalai-Sarkaria conjecture, that would give you a that would give you a proof on what I would conjecture to be the, so you know, what you get from. So, you know, what you get from this theorem, the Greenbum Kalai-Sarkaria theorem, I would expect that would be the right answer for the threshold for PL embeddability. But actually, the sparse side there is more difficult than the sparse side here because the certificate for PL embeddability in this fixed dimension model is a result from a paper of Horvadik from like the 70s that basically says that if you have a complex Basically, it says that if you have a complex that's d-dimensional, but you can collapse away the d-dimensional faces by taking d minus one faces that have degree one and just doing an elementary collapse. Then such a complex P L embeds into 2D dimensional space. So a two complex that can be collapsed to a graph in this way embeds into R4. But these kind of situation where you have a D complex. Of situation where you have a D complex that's also D collapsible doesn't really happen anywhere else in the random multi-parameter model. So we can't rely on that to prove the sparse side of PL embeddability. I also thought a little bit about looking at the dense side of this from the point of view of these kind of deleted join test map kind of approaches to non-embeddability where you take the Where you take the deleted join and you prove that it's sufficiently connected. But the problem there is that for these random complexes, we have really good tools to show rational homology groups vanish and to show that they're simply connected. So you can show that, and actually would be a fairly short proof to show that it's like rationally homotopically K connected for the right K to give a non-embeddability result, but to actually get that to Z mod, you know. To Z mod, you know, with coefficients mod two, that's an open problem to prove things about vanishing homology for random complexes with Z coefficients or even with Z mod two coefficients. And so that presents a challenge to giving like a proof like this for a non-embeddability result. Okay, so like I said, this main result gives the right exponents for linear embeddability. Linear embeddability, but what about a more precise threshold? So, the precise course threshold, or even a sharp threshold, if one exists. And I'm not sure if a sharp threshold should exist or not. So, for planarity, it's known that there's a sharp threshold at one over n. For the case that I talked about here, if you have x and p. If you have x and p and p is like a constant over n, there's at least a positive probability that that complex will embed into R3 because standard results from random graphs tell us that in that regime, the number of triangles is Poisson distributed with constant mean. And so it depends on this C. And so there's a positive probability you just get a graph and you have no triangles at all. And so that embeds into R3. So there's a positive probability. Embeds into R3. So there's a positive lower bound for that probability. I might suspect that if C is like small, then the probability actually is one, but I haven't really tried to prove that yet. And then kind of the proof for the upper bound suggests maybe a threshold for non-embeddability at R3 and more like fourth root of log n over n. This kind of comes out of the computations. But again, this is open as far as like what the right answer is for the sharp threshold. Okay, so yeah. Okay, so yeah, so that's all. So, thank you for your attention. Questions or comments? So, the statement of the current data is that for the list of cases, I don't remember exactly how it went. So, what I'm wondering if I can get a lot of these all the patterns? Almost all. Almost all of them. I mean, almost all of them. If we're looking at this logarithmic scale where we only are interested in the where Pi is n to the minus alpha i, and we're only interested in the alpha i's, then it will partition R D into these convex regions. And if your AI is within a particular convex region, then you get the dimension of embeddability. If it's on the boundary, then this is kind of what's going on there. We get, we don't really know what happens. On there, we don't really know what happens, but uh, but yeah, within uh within these different these convex bodies, then you get the right answer. Well, it's not, we think not the other stuff.