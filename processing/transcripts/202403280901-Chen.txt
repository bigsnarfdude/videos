I had a great time just for the afternoon. We have an instructional list of presentations today, starting from our keynote, Zhao Hung Chen from Yeo. So she will talk about seafood estimation and interest. Okay, Zhang, John. Okay, thank you. I wish I were there. It's beautiful scenery there, and I can see snowing. And can see snowing? So here is raining. Thank you for listening to my Zoom talk. I mentioned earlier, I do have application to OT with Yanqing and Dao, but I didn't have a chance to put in the slides yet. So I can mention it. Yinji said, let me give an overview talk. So can we just give an overview? Can we just give an overview? Which I try to convince you. There are many ways to do distributional robust. One is just don't make functional form assumptions. That is also kind of distributional robust. That is what this semi non-parametric view. Okay. I will don't worry. I will not, I just mentioned quickly, just mention how this will first mention model and examples and just explain the procedure. Just explain the procedure, the PNR series stream procedure, which including all kinds of criterion functions like maximum algorithm, minimum distance, GAL, GMM, all kinds of machine learning criterion as well. Then we'll give you a general consistency results, which can apply to all kinds of sales. Then we'll mention the existing results for the The existing results for the property for CMS machine, then we'll mention the existing results for C minimum distance or GMM or GL type of results. Then also mentions the semi-parameter two-step, which is a lot of causal primary results belong to this semi-parameter two-step procedure. Then actually, I don't think I will have time to talk. Actually, I don't think I will have time to talk about partial identified models because I originally want to focus on that, but I always have trouble to delete material and I don't think I'm going to cover that. Okay, so first, I know nowadays everybody wants to jump into machine learning, don't want to have any models. And I actually still want to go back to model. Why? Because I think in economics. Why? Because I think in economic causal analysis, I think we really need both model and data. If just based on observation data known, without economic model, I mean, we can only identify SMS correlations, not causality. For example, because this was related to some very important policy question, right? Important policy question, right? Such as increasing import from China causes increasing unemployment rate in the USA. Is this correct or not? Doubtfully, many politicians will push for this causation. But to answer this kind of question, can I just look at the data, just look at the import data from China and I import rate data in USA? Just regress regress I import rate data. I import rate data against import from China. If you're doing that regression, you see the signs. So then you say, oh, the import rate in US is due to increased import from China. This is really just regression analysis, really just look at the science, look at the correlation. So that's why for answer questions like this, we really need economic model for trade. Model for trade, actually, we need the general equilibrium model to answer this kind of question. So generally, a model is a family of probability distributions of observed random variables, just corresponding data indexed by unknown parameters. So, the model you can call it parametrically if all the parameters, because usually we call the parameters, we classify into two kinds. Into two kinds. You have primary interest and nuisance parameter. So if both parameter interest and nuisance parameter belong to a finite dimensional Euclidean space, then the model is parametric, like a parametric likelihood model. Then the other extreme is non-parametric, right? You don't make any function form about both the nuisance parameter and the primary interest. What I want to argue is actually fully non-parametric. Actually, fully non-parametric models is rare, especially if you think about economic time source data. It's impossible fully non-parametric for time source will, I mean, how many lags you put there. So in most, in practice, we usually get into either semi-parametric model or semi-non-permanent model. These two are virtually the same, just if we say the model is semi-parametric, if it's primary. Semi-parameter if his parameter interest belongs to finite dimensional, the nuisance parameter belongs to infinite dimensional. For example, now sensing the unconditional generalized mass of moment is actually a needing example of semi-non-parameter. In some sense, a semi-parameter, in some sense, that's kind of trivial, right? Because that's an example you, when you want to estimate the finite dimensional parameter of genuine, you actually don't need to recover the You don't need to recover the law of motion, the distribution associated with the data. Of course, you could also estimate that that will be corresponding to EL or GEL, that will have a high-order refinement, but you actually don't need it for consistency and ruling normality, first order efficiency. But then the more interesting of class models is semi-non-parametric, where both the primary Both the primary interest will contain both finite dimensional and infinite dimensional unknown parameters. So these are the examples where you have many economic this natant hydrogeneity, which means natent distribution you actually want to recover. So this broadly you could classify into these four categories. And also this one point I think people didn't stress. I think people didn't stress. I want to stress a little bit here. Actually, any model can be classified locally into just identified or over-identified. You said, well, what is this about? This has nothing to do with parameters. This, I'm talking about model, just identified or over-identified. This really corresponds to whether or not the model is not clear. Is nuclear testable or not, whether or not you have trivial power or not. Because if a model is just identified, in some sense, not that interesting, especially for economic causal relationship. What kind of model is non-parametric, just identified? Just prediction, non-parametric prediction. If you observe y and x, you regress y. Y on X, you regress Y on X non-parametrically, you can use whatever machine learning. That is for condition mean, that's an example. If you don't make any functional form or assumption, that corresponding to just identified. But actually, once we have some endogeneity, economic relationship, then you probably could easily guess to over-identify it. So, So I want to argue it's more interesting to work on model which is locally over-identified. So usually the semi-non-permanent model kind of like that. So seven non-permanent model we already say is family probability distributions of observed random variables indexed by both finite and infinite dimensional and non-primary interest. And non-primary interest. And again, for this, we can compare to the primarial model, of course, it's more robust, right? And also, because I heard the talk on the past few days, because this whole conference about robust distribution robust, or this or that robust. So they also have this alternative approach corresponding to no cost sensitivity. So this. Sensitivity. So, this semen non-profit model, in some sense, is kind of global robust, not focused on local. No robust is more like kind of testing, like pinman drift test. So, basically, it's a different view to think about robustness because robust we can talk about in all kinds. Robots, we can talk about it in all kinds of different angles. Now, of course, compared to the fully non-parametric model, it is more informative for estimation inference and incoming models. And of course, there's advantages of semi-non-primary models. I want to stress robustness because this whole conference is about robust distribution robust. Then If you don't make enough assumptions, of course, you also have a disadvantage. What's a disadvantage? Why is because it's adaptation? The primary interest probably sometimes you don't get point adaptation. Even if you get point adaptation, the information content of primary interest, this means even for scalar parameter beta, even if it's point identified. Even if it's point identified, probably you'll no longer be able to estimate the ruler normal when any sample size. So, this corresponding to so-called information content. So, these are the things where if you're just doing parametric model or even doing local sensitivity, then this thing will not show up. Okay, then of course, because it's more flexible, of course, then will be nice. It's more flexible, of course, then will be less accurate, means the confidence interval will be wider. And also, to get the same accuracy, then you need a much larger data set. But still, will be more accurate than fully non-parametric. Okay, so still, I'm still talking about model. Now it's model and the parameters. So now there's a semi-non-parameter. The semi-non-parameter models can roughly classify into two big classes: one is likewise based, the other is conditional or uncontinuable restriction-based models. We will give example later on. Then, in terms of parameters, I should have two types of finite and infinite dimensional parameters. One is I will call reduced form parameters. One, I will call reduced form parameter. These are the parameters here because when we say parameter could be function, because it's infinite-dimensional parameters, the reduced form parameters are the ones that are non-majorable mappings of a data distribution. So since data, for data from data, when you observe data, you can estimate the data distribution, right? The probability distribution associated with the data. So reduced form parameter as a non-measurement mapping of the data distribution. Mergement mapping of the data distribution. So, but definition is point-identified from the probability distribution of observed variables, say y or x. All those could be vector-valued. These are the examples, say condition distribution or conditional density of y given x, or condition mean, y given x, or condition quantile, or conditional duration, or conditional choice, probability, this or that. So, this has a reduced form. So, this is a reduced form parameter. Roughly, these are the plus parameters. Now, you can all do whatever machine learning algorithm to estimate very well. You can do whatever black box machine learning algorithm. But for economics, the structure parameter, which is other ones is really. It really describes the agent's optimization behavior or equilibrium condition. These are the ones on offer endogeneity or natent heterogeneity or missing data, etc. This kind of parameters, like a causal parameter or welfare or policy or preference parameters. The example like in industry organization like firms, so you will think about demand or Demand or production function or trade elasticity. So these are the parameters where you actually need for point identification of these kind of parameters, we actually need some economic modeling assumption. So that will allow us to identify this from the data distribution or more from the reduced form parameters. So even though the reduced form parameter Even though the reduced form parameter are always poly identified, but the structure parameter, you need some actual modeling assumption. So that probably either poly identified or also could be partially identified. Okay. That's exciting. So for example, now give the example the first big class I've mentioned is based on likelihood, right? A big class is the semi-parametric mixture models. So this So, this is a leading example. This is a Hackman singer, Jim Hackman gets a Nobel Prize for his Nobel Prize contribution. This paper is mentioned one way. So, these are the ones where actually Hagman and Singer. Singer was a biostatistician in Yale. Hagman was at Yale, so they wrote this paper at Yale. At Yale. And Ishiwara was actually a PhD student at Yale in Stats Department. So he heard their problem. So he later also worked on this model. This is an example where the observed data is T and X. T is a duration observed, say unemployment duration. X is observed the individual characteristics. The individual characteristics. So, from the observed data, we can recover what? Conditional distribution or conditional density of T given X. This you can think about this P, the left-hand side. This will be what we call the reduced form parameter. Here, of course, will be a function if recover from the data non-parametrically. But then, the economic duration model for unemployment rate, this or that, and even actually. That and even actually, Singer is a biostatistician, they study a lot of survival analysis, so that's why this wave of distribution, all those things are come from survival analysis. Think about duration, unimportant duration is also kind of survival. So this you have this, this part is parameter model suggested by economic model. So this is the part So, this is the part corresponding to the observed duration, given observed characteristics, and given the individual unobserved characteristics, U. U. we don't know, and you is draw from some distribution, latent distribution. So, this is a, you can see now this is a conclusion, it's just semi-primary mixture model. So, this is the example where. This is the example where they original Hackman single they are interested in the finite dimensional beta parameter. But this example to recover the beta parameter you also need to recover this natan distribution. And actually if you want to doing this prediction, you actually need to both to know both beta and f the natent distribution, not just beta. So this is the example where So, this is an example where. So, this is an example where, if you're making semi-parametric assumptions, means you don't specify the natent distribution. Because you don't observe it, you don't know what kind of distribution assume. This coin mega the agents, the individual's latent ability, right? This kind of example where Where earlier literature were making parametrical assumption here. Once when you make parametric assumption here, if it's wrong, if misspecified, then your beta will not get consistently estimated. So that's why to be robust, to misspecification, you just leave it unspecified. In this sense, it's more robust. So, why I said this, in some sense, probably is more robust than you take. Probably it's more robust than you take a parametric distribution, say knock normal, whatever, then you're doing a delta ball around it. Because in that way, then the beta, you probably still get rudened as medic. In this paper, this class model is very tricky. Once when you don't specify the latent distribution, The latent distribution. In this papers, they give some regularity conditions using some identification and infinity argument to identify beta and the nature distribution. But this kind of identification argument means the information you use to identify beta is really local, so you couldn't estimate beta ruden normal anymore. Rude normal anymore means the information content for beta is actually very weak. And this is what we call not regularly point identified. So actually this paper. The thing is, once when you have the, you couldn't, once when you know, you couldn't estimate a red rate, then actually, even till this day, for the hype-man single original paper, what's the best achievable rate to record? Achievable rate to recover beta, we don't know. I mentioned these two papers. These two papers is for special cases. When you don't have X, then they show beta is equal to X meter root M. Okay, so I use this to illustrate many points. One to say for many in economics, we have the agent have native. We have the agent have natural ability, and for firms, they have a natural productivity, all these different things. If we don't observe, nowadays we want to be more robust, more flexible, we don't make parametric assumptions. We leave it as non-parametric assumptions. So, this is kind of, in this sense, is also a distribution robust because you don't make any parametrical assumption. It's not like doing a It's not like doing a draw as a radius around a surrounding benchmark model. Because since it's an ATI distribution, you don't know what kind of benchmark distribution you put it doing or delta ball around it, right? Okay, so this is actually not just this. Um, semi-prime mixture model and many random coefficient model, non-linear measurement error, and missing data can all be cast into this framework. And that was semi-prime mixture model. The next big class in economics is also entry game is again, I will not spend time talking about it. And of this, again, is mixture. So I was trying to say if you don't specify this equilibrium selection, just leave it a non-parametric. Just leave it a non-parametric and just be robust, then again, the parameter interest this for computation parameters will not get point identified in this case. This again is kind of like a mixture model because the left-hand side you can recover from data, but right-hand side, you need some model restriction. If you don't enough, impose enough. Impose enough model restriction, you don't get point adaptation. Okay, so the first two examples are corresponding to semi-parameter mixture corresponding to natent hydrogenity. Then the other big class is kind of like extension of Hansen's generalized method moment. Generalized method moment means I don't specify a distribution assumption. So, here you can generalize that to the cognition moment restriction. These are examples of cognition. Cognition moment restriction, these are examples of cognition moment restriction, where you have this unknown function. Whenever you see this edge corresponding to unknown functions, so why this is a big class? Because in economics, money, firm, agent, they're all doing optimization behavior so that you end up the observed such as observed prices. This or that is endogenous, means could be arbitrary correlated with the error term. Aperture coordinated with the error term. So that means the regressors are endogenous, like white could be aperture coordinated with error term. So this is a conditional moment restriction. This again, here you didn't specify the distribution. So in this sense, it's robust. And turns out all these iser pricing models belong to this. So basically, you have two big classes of models. Then Also, two sets of parameters, right? That's a reduced form parameter and a structure parameter. We are interested in structure parameter and structure parameter. So, if you how to estimate the primary interest, which is structure parameter here, and you again you have two big approaches. The first just directly imposing the semi-parametric structure, such as in original Hackerman Singer, they call non-parametric. Thing they call non-parametric MREs. They just approximate the latent heterogeneity using the just like a histogram. You can call that first order spine. Or you can also think about how basis wavelength. Okay, so then they did MRE. That is directly enforcing the semi-perverted mixture structure. Alternatively, you could do because I mentioned the Because I mentioned the one part, we always have reduced form parameter. Nowadays, reduced form parameter we can all use in machine learning estimate very well, right? Reduced form parameter, remember, these are normal measurable mapping of the data distribution, such as the conditional choice probability and the condition mean of y given x, etc. So these are the ones we actually don't need. ones we actually don't need to enforce any model restriction at all means in some sense this reduced form parameters are non-parameters just identified when in some sense you can have this reduced form parameter as a summary statistics you don't lose any information then you can use a model structure to to map between the reduced form and the structure parameter then using that inverse Then using that inverse mapping to recover the structure parameter. So that will be the second approach. This is usually doing the minimum distance or matching this or that. For example, like auction model in economics, so what we observe the beads data from bits data, then the naturally reduced form parameter is what? So the distribution associated with the bits, right? You observe bit, you can. Right, you observe it, you can recover its profit distribution, or you feel assume it's continuous, you can recover the density. But we want to know and underline the value distribution. So then the value and the bit, the relation is through the auction model. So you can use that equilibrium, the condition using that by P. Using that mapping, so that once when you recover base distribution, you can inverse back to recover the natent distribution. That's one way, which is the second approach. But alternatively, earlier on, or even now you can still do it. You can directly also do in MRE. But of course, earlier on, people are making parametric assumptions about natent value distribution that could be misspecified, which is not good. But you could also. Not good, but you could also just do non-parametric MRE in the sense I don't specify don't specify the nature value distribution, then just through the equilibrium condition from that map to the base. So then you can still write down that hood, you can and you can go that way. So basically, Basically, these are all the broadcasts of models, and they have these two approaches. For each approach, you can do many different things, right? Even for this reduced form, you can use all kinds of machine learning algorithm, your favorite. That works very much for estimation part. So far, I'm talking about model and the parameter. Now, the thing is I want to say is the method penalized save is a general principle that can be used for. Principle that can be used in both approaches to estimate any semi-non-primary models. And also, that's why once we can see the general idea, then also actually doesn't matter whether or not the primary interest is point identified or not. If it's not point identified, you will recover the set. Then, then, and then can do some set inference. Okay, so now the general extreme aspect. The general extreme estimation is really okay, given the data and given the parameter space, these are total parameters, unknown parameters, including both finite dimensional and infinite dimensional. And probably is in infinite dimensional space, probably is not compact. And then you have a sequence of simpler parameter spaces. Primary spaces, which is called seal. The only requirement about seal is they just become dense as the seal become more and more complex, roughly say seal dimension increases. What does dense mean? It depends on what kind of metric you want to recover, the parameter theta. If you want to use this metric or pseudometric. This metric or pseudometric, then you just need for any theta in the whole original parameter space. You can find some element in the save space such that the distance going to zero when the save space dimension grows. The Grand 81 book called Abstract Inference, he called Master Receives, but his Method of Saves initially just read. His method of sieve initially just really doing the initial notion of sieves is kind of like finite elements. So, here now I use sieves to mean any approximation parameter spaces. A lot is just like bases, like spine, wavelengths, neural nets, rigid nets, etc. Okay. And actually, Grandander using that is for solving for PDE or ODE. Or ODE using the finite element. Then you have any criterion function. Then I said you can have some pseudo-true parameter value. Why? Because even though we are semi-non-parametric models, the model could still be misspecified in some parametric dimension and some function format still be misspecified. Misspecified so doesn't matter. We can always define pseudo-true, which is just a maximizer of the population criterion. Then we have the data, then we have the sample analog of the population criterion. The series stream estimator, I didn't put penalty there because penalty just in the eta n here. So you can have all kinds of penalty there. So basically, compared to the standard extreme estimator, now it's just. Standard is true maximizer. Now it's just parameter space. Now it's simpler, it's a serial parameter space. So, this you have approximating error. Then, here we don't need to be exactly maximizer here. You can have all kinds of penalties there, or you can have approximate solution. Okay, so that's the definition. Then, basically, you can have all kinds of criterion, then all kinds of basically you have two big classes. Basically, you have two big classes again related to like food base. You can have maximum likelihood or cause the maximum likelihood. This M is really corresponding to Hubert M. Okay, it's not maximum likelihood, including maximum likelihood, the special case. Then you have a quantum regression, non-initiated square, etc. It's kind of almost all the roughly all the reduced form parameters that machine learning algorithm. Learning algorithm criterion kind of corresponding to M criterion. Then we have also this minimum distance like generalized measure moment or GL that were all corresponding to condition moment restriction or moment restriction, unconditional moment restriction or kind of doing matching. Okay, so then you have the you can you can now you can see the matching you can use in your cleaning space matching, you can in the function space. Space matching, you can in the function space, you can categorize space. You can basically, you have a lot of freedom to choose the criterion. Then, then the sieves, you also have a lot of freedom. You can choose a finite dimension, linear sieve, or non-linear sieve. You can using the, we're all familiar with polynomial, home, polynomial, cosine sign, sprine, wavelength, neural net, bridgeless, etc. Or you can go into infra-dimensional linear non-linearity, right? Once we go, Once we're going to infra dimensional, then you have to add in penalty. So you can do this. I just use as an example. Any function in the holder space, you can do what? You can have this Fourier series expansion, then you can take the derivative, fractional derivative, you can have all the infinite dimensional, infinite dimension expansion, right? You can add in the penalty here so that you basically. Here. So that you basically just put penalty in the safe space. So that's why you can show in whatever penalty there. Okay. And then you can doing aggrogen multiplier, just move the, you can move in this example, you can move this penalty to the criterion function. So then ends up with the regularized criterion. Now you're doing optimization in the whole function space. function space. In this paper, we have all kinds of penalty. You have general semi and no semi-compact penalty or convex penalty, whatever. So you can have nozzle, fuse, nozzle. So you can have also reproducing kernel cuber space or reproducing kernel Banach space. Because actually now neural net is doesn't belong to reproducing kernel cuber space, but can be rewrite in terms of reproducing kernel Banach space. Banana space. Okay, so basically, those are all not in the criteria, in the procedure. Now, basically, just want to say the procedure is flexible because you can choose all kinds of different criteria and you can choose all kinds of different save inside civil, you can have all kinds of different penalty. And the thing is, you can impose shift receipts. You can impose shift restrictions and you can use whatever optimization routine. Then, now I just talk quickly about a general consistency because again, this consistency for any criterion function, any you can throw in whatever penalty inside here. So Because I will just mention this quickly because then later on you can just tighten this up, you will give you rate this or that, then later on when you're doing inference. In terms of consistency, you just need, okay, here for simplicity you assume we have unique pseudo to alpha zero there. And this is the alpha zero you can think about maybe is About maybe is when define q bar n. What does this q bar n mean? n is the sample size. q bar, when you see bar, means something not random. Then you say why it still depends on n because here, q bar n is kind of you can think about is a regularized criterion, already adding the penalty inside it. So the penalty, the that tuning parameter needs to is depends on sample size and. Depends on semblance as n. So you think about that way. Or you can also think about it: time series is not strictly stationary. So even if you take expectations, it still can depend on semester size and okay. So, or you come triangular array, whatever. So the thing is, the first this condition is so-called severe identifiable uniqueness, which is here. Sorry, I changed it to YINF instead of SUF, but doesn't matter. Instead of south, but doesn't matter. Okay, so this for the inf thing, basically, so for any alpha outside this topology around it, alpha zero, just think of a no neighborhood around alpha zero under the taut topology. Outside of the ball, think about this ball. Outside of the ball, but it's in the safe space. This minimizer, of course, is bigger than the true minimizer in the whole. The two minimizer in the whole space, right? So the difference you can define, just define that. So for each seal space, for each seal dimension, for each the penalty, so this actually, this thing is greater than zero. But the key thing is what? As some of that is increasing, this thing can go to zero in the limit. Goes to zero in the limit. Means in the limit, you can lose identification. Means this really make it very difficult. This so-called giving the limit, this equal to zero means this problem is ill-posed. And but in the reason I mentioned this is both the semi-parametric mixture model or the high quantity. Model of the Hyperman singer and this non-homogeneous endogeneity all belong to this setup, where this thing in the limit going to zero means in the limit, the loose identification. So in some sense, the thief is also one way to do regularization. And here, you can also announce regularization. You can just using sieve, don't need to do penalty, or you can just. Do penalty, or you can just doing the infinite dimensional penalty, don't use the C. But anyway, these are not both. Then the other, of course, if you're doing C approximation, then you have this C approximation error, right? So C approximation error, again, is very flexible. That's why whatever C you can choose will be fine. You can use in spring wavenets or cosine sign or Hermet polynomial or URNet, whatever. What matters is the Is the alpha zero? You can find something in the save space, element in the save space, so that the criterion difference is small. This has to go into zero. Then the third thing is required because we are using data to estimate this population one, this non-random, this random. So this similar to uniform norm of knowledge number. But again, this uniform normal knowledge number only requires holding the seal space. Remember, seal space is much smaller. Space remember C space is much smaller than the whole function space, it's much less complex. So, this is much easier to satisfy. But, of course, now the trade of this thing going to zero. So, now you can see now we basically have what? These are tuning parameters. We basically have three sets of tuning parameters. What are they? Of course, they are related. The eta n is defined here. You can also think about eta n will be optimization error. Be optimization error. If you think about your C will be neural net or other non-linear C, you don't have global unique solutions. You have, you basically, any general non-linear optimization, you have some optimization error. This is also, which is also allowed here that you can choose. And that, of course, also depends on your choice of the CO dimensions. This CO approximation error in terms of C approximation error in terms of criterion, of course, also depends on how fast you're netting the C grow, right? If you're not in C grow, for example, if C just equal to the whole space, then this identical equal to zero, right? So then, of course, if you're setting this equals the whole space, then maybe this is harder to satisfy. You can see they have all this tension. So this will be now you need based choosing a tuning parameter to such that the To such as the worst one, this thing about optimization error. This is kind of corresponding to uniform of knowledge number error corresponding to the criterion. You can think about it as a random part. This corresponding to the bias of the criterion, because this is really kind of bias of the criterion difference. So you require all this, they have to what? Grow to zero faster than this. Remember, if the Than this. Remember, if the things is ill-posed, this thing is in the limit can go into zero. So if the problem is well-posed, means in the limit, this bounded away from zero, then you just need all this, just little O1. Otherwise, if this can grow to zero, then this have to go into zero faster than this going to zero to control your pulseness. Okay, so under those conditions, Okay, so under those conditions, then you get the consistency. The proof, just very simple proof. But of course, it's really how to formulate the conditions so that it is easy to apply to all kinds of situations. So that's why I spend all the time talking about the conditions because later on, once we have consistency, then later on, when you're doing the rate of convergence is all kind of when you're going to local neighborhoods, so that will tighten. Local neighborhood, so that will tighten up this and tighten up these. And okay, so then for the rate of convergence, then we have to decompose into depends on curvature, local curvature of the criterion function. So now we decompose either into the MS machine, this corresponding to MS machine, just the sample average of this. And the other will be minimum distance. I think I will run out of time. So basically for general MS. Basically, for general MS machines, then you have all those general rate results there. And then also have some normality, but normality is for ruling functional. So I have no time to talk about it. This just innovate. So for example, you can use a neural net to order Causon radio basis to estimate quantal regression. So this will be your check function criterion. So if I want to use L2. So, if I want to use an L2 known to measure how well I recover the office quantile, so I require the true error term conditional density given the x. When evaluate the zero, because this is a quantum restriction, has to be kind of bounded between all about. This is a condition. These are the, you can see this condition. We all also require even just for parametric. Even just for parametric counter regression. So not restricted at all. So basically, here, I just as an illustration, okay, I assume the two functions belong to this bumper algebra. So then for this bumper algebra function space, we know the linear basis will linear smoother will not achieve optimal rate. So these are using in illustration give you a simple Gaussian radial basis, which is a non-linear. Which is a nonlinear C. So, this corresponding to this, this will be the rate. Okay. So basically, it's we run on time. Basically, once you put your function belongs on function space, whatever C basis you choose, then that will give you some approximation error rates. Then, the other piece is about the Curving number in terms of the criterion function, incremental criterion function. So, this that will kind of give you the variance piece that will balance together will give you the rate. Okay, so that's why you can really doesn't matter what kind of basis you use. Then, you can also have many other results about the efficiency and then. Efficiency and then about the civil act ratio inference. I will quickly talk about civil act ratio because I think this is quite general. The advantage of this is you actually don't need to know precise convergence rate for your parameter of interest. So, original Shen and Xi, they show sigma ratio is chi-square for the final dimensional parameter when that parameter can be estimated. Parameter when that parameter can be estimated Jordan rate. So then in this paper, we just generalize their result to any could be irregular functional, means the functional doesn't need to estimate a Jordan rate. For example, if we want to test for this, this will be our null hypothesis, which is m-dimensional. They are linearly independent, the derivatives. So then, if you want to test this. So, then if you want to test this, so this will be the unconstrained. Think about this will be not like good unconstrained. This will be constraint C MRE because I've just imposed a constraint here. So then we just show regardless the estimation rate for this, you have this zero ratio is chi-square with degree freedom, just how many restrictions. Restrictions you are testing. So, this is very handy, why, especially when you apply to Hackman single example, where the beta as I till this day, nobody knows what's the minimax, the optimal rate to estimate beta. But doesn't matter, we are doing C1, just this, which is a C to approximate, so then we can we also, the people who also have self-normalized can get the worst statistics. Normalize can get the worst statistics, then we also have natural ratio statistics. I like like to ratio statistics because then you even don't need to estimate the variance. So this doing CRMI, so we use a Hermetic polynomial. So then you can see this we are doing the using the Kaisper as a base confidence interval. This because we also have this word statistic, T statistics, you can also get it. So that means even if you don't know the Even if you don't know what's the rate you estimate or whether or not the beta you can estimate is really normal, you can still do inference. And also, it doesn't matter what kind of C you use to approximate a norm latent distribution. So that corresponding to, I need to be fast, that corresponding to the MS machine, then analogous results that NATO will develop for all this C-minimum distance. I will don't say it. Basically, again, you have. Basically, again, you have convergence rates, then you also have this, because this is no longer like we call it quasi-laboratory statistics, again, you can have the inference. So then later on, you can actually extend this to partial identified case. But of course, in partial identified case, the degree of freedom is unknown. That's why we are doing the COSA-based version. So you can use in posterior. Are base version, so you can use a posterior draw to approximate it. Okay, so um now give you the use this as illustration. So I mentioned earlier, I said this neat example of this condition model is this MPIV, even though the model looks simple, the X is arbitrary coordinated with error term, you have the instruments W. Instruments W. This model actually included many other things. You have the demand and then demand for differential product, international trade, demand for asset and price and financial assets. Actually, now all those causal inference that proxy stuff and dating biostats is actually the underlying mass is the same as NPRB. The same as MPRB, then actually reinforcement learning is the same. Okay, so I just want to say even though it's a simple model, you cannot do machine learning toning parameter choice that will give you inconsistent estimate. So here, I want to illustrate why because what I want to say is right now people Right now, people feel like, oh, you have to do the robustness. This means you're doing a robust radial delta ball around something. Actually, I'm not sure that is always the right way to do it, such as this trade application. Trade applications, the earlier literature, the old trade application, the firm's productivity is unobserved, it's latent. Observed. It's natent. In the past, all the stationary grouping models assume that the natant, the firm productivity draws from some parametric family. So that will introduce, that will imply, you see this other example. They assume the natant distribution come from different parametric family, either from parietal or from the truncated parietal or non-normal or this other and more flexible, slightly more flexible parametric family. Flexible parametric family. But of course, I didn't draw the Nayton distribution. What I draw here is in the trade application, they all care about for trade policy, they care about elasticity of the intensive margin in terms of trade. So elasticity really just taking the partial derivative. You first estimate anyway, the partial derivative. Anyway, the partial derivative. The thing is, you can see this shape is completely dictated by the underlying latent distribution, you assume. Whoever writes the paper, they assume all those paper are published in the top five Economic journals. And you can see they get all this different shape. Then, based on this shape, they have all different kinds of trade policy counterfactuals. Policy counterfactual. The funny thing is, we take this data we take from my colleague in the and his colleague in Yale, get their data set. They actually no longer make an underline distribution assumption, the firm productivity assumption ends up the geocontinental equilibrium model, then the United States. Steed State have no shape at all. This is just non-parametric. So then we just get the data, we just estimated this is what we estimated. So I have no time to talk about how we estimate it. I just mentioned this black curve is non-parametric estimated using MPIV. This dotted one is assume what? Assume underline the firm hydrogenity. Heterogeneity is priorital distributed, then you end up constant in a statistic. So I'm not sure how effective it is just doing a delta radius ball around parietal. That is kind of effective corresponding doing a small neighborhood around the constant in a statistic. So it's it's it's It's kind of a different way to do sensitivity. I kind of feel like if economic theory really has no guidance about what latent distribution function form to assume, probably it's better don't assume it. That is, in some sense, is robust. I know I'm running out of time. So, actually, the quick thing I want to say is. Actually, the quick thing I want to say is related to all this double machine learning, double robustness, is occurring when you talk about semi-parametric two-step, which is the first step you estimate non-parametrics, and second step, you plug into some finite dimension unconditional moment to recover the primary interest. And for all this, most of the results, many so-far so-called double-robust, all corresponding actually the first step, non-parametric is. First step, non-parametric is so-called, is what I mentioned earlier, is non-parametricity just identified. It's all, you can think about just non-parametric reduced form, like a conditional mean or conditional probability, all those that we corresponding to observed data. In that sense, actually, that other results show that if that's the case, If that's the case, if the first step is just non-parametric, just identified, then second step, you plug in whatever estimator plug into it. Doing alternate weighted GMing and second step, you will get an efficient estimator. So in some sense, all those different kinds of double robots, essentially, they're all first order permanent for all those turning five parameters. They just, people just, if you really want to do argue, To do argue theoretically, probably should go through higher-order refinement, then have a better way to choose the tuning parameters. Because right now, it's everybody just one person writing a paper, we're using slightly different, either using this basis, that machine learning. So because then different tuning parameters will get something looks like different. But a large class of models actually are first-order equipment. So basically, So basically, a round of time I will stop here. Basically, you can many things, you can map it into this sequential moment restriction, or more generally, it can be just conditional different information set. So many missing data, you can all map into this framework. And like Yen Chung during Yen Chen's talk, she also has this missing data since of environmental conditions. I will Um, I will whoops. Now, I give you neural net application around time. Uh, these are innovative. This, this, this neural net, this one, the first step is not just identified because first step is MPRV endogenous. So, this corresponding estimate is the average price elasticity. So, the price is endogenous. So, in this case, actually, how to estimate the first step? The first step does matter because first step could be over-identified. So, this basically just inaustrate you can regard the neural net as a non-linear theorem. You can also still doing the inference. I think that's that's it. Like Yanji and I and with Wenda and we have a paper ongoing work in chart basically doing the C to estimate the OT. To estimate the OT non-parametrically, then use that to doing some tuning fact parameter estimation. But I think I will stop here. I should stop here. I'm just very curious to. I'm just very curious to understand a little more how optimal transport enters in this framework. Optimal transport just corresponds to different models, different criterion. So can still use this. In that application, optimal transport, we're using SPRINE to approximate. We're using the do formulation, then just estimate using SPRINE to approximate. What approximate the infinite dimensional parameter? Yeah, yeah, because all these things serve is all approximate infinite dimensional parameter. I didn't have time to put it into the slides. Yeah, you're just using. Um yeah, you're just using all in the other one we're also using neural net to approximate the um to do the reinforcement learning to there we need to estimate there we are using maximum mean discrepancy distance. So doesn't it's that in some sense is really just another uh constraint and constraint C because in the function space so just a known function you use in whatever basis to approximate. The the uh the informative talk. Uh I have a general question about how maybe it's a little bit off the topic, but how can we incorporate the semi-parametric model specification into the Bayesian framework? Oh, yes, I actually do have a paper on Bayesian. Have a paper on base here. I don't have time to put it here. Yeah, you can do it. So we have, we did the, you just put C prior. So you can, we do the quasi posterior. Just take all these criterion. You can just put throw into them into exponential. So then the then the C basis, those, you know, once we did C approximation. Once we did CU approximation, then you put the prior on the CU coefficient. Then you're doing quasi-posterior. Can you introduce the paper that you yeah, so I can actually show you one paper we did for the partial identification for the for auction model. There we're using the We're using Hermann polynomial to approximate underline value distribution. Then we put the using equilibrium condition right down corresponding to the likelihood. So that's the semi-permission like that's a model because we model is very complex. We don't have point identification. So we end up with partial identified models. Partial identified semi-parameter models. So there we So there we just put the CU and the on the put prior on the CU coefficient. So put prior on the CU coefficient, the first paper, as far as I know, is issued by Xiao Tung Shen as Nari Waterman. 205, maybe Two or five, maybe JASA paper. There they did for 0 MRE, semi-parametric. They have examples like a partial linear regression. I had a couple questions, but in the sake of time, my first question is very just philosophical. The definition of seed here is fairly general and it feels Which is fairly general, and it feels to me that it would actually encompass any estimator, estimated in a non-parametric setting, right? Because I always view any estimate of finite samples as effectively operating in a finite dimensional approximation of the planet dimensional space. Is that it? So, is that how you view the scene? The seed is just every algorithm? What algorithm? Sorry, I couldn't hear you. I couldn't hear you if I'm estimating a non-parametric parameter, so an entry-dimensional parameter with finite sample. And if I'm using any linear estimator, I'm effectively operating in a finite dimensional space that I'm hoping across the regional space. So, in that sense, In that sense, your definition of C would be any non-algode. Yes, it is very general. The definition, that's why I want to use that to innovate. You can have even infrared dimensional save. There I put infinite dimensional is really just adding the penalization. So that is right because you put the penalty in the save space, then you're just doing Lagrangian multiplier move to the criterion function, just become regularization. So that's what like a reproduction. So that's what like a reproducing kernel Kubernetes spacing would be an example of that. Just a quick question. That's a further question. You mentioned for a second reproducing kernel panel spaces as a way to model neural networks. So any further comments on this? I mean, there is tons of work on using RPA that reproducing kernel Hilbert spaces to model neural networks. To all neural networks, but I've never heard of banana spaces. So, the reproduce kind of human spaces only for the single hidden layer neural networking the way it goes to infinity, that in the limit, that make it become effective linear C, that's how gets into reproducing kernel Hilbert space. If you really want to keep the original non-linearity, so enzyme is like a reproducing kernel Ballock space. But there is tons of work on even multiple neural networks being represented. Yes. You're right. So it's not every neural net can be written like that way. But yeah. So that, okay, sorry, there are several things, right? One thing is one thing, what here I'm talking about for here to get the theoretical rate or to do inference all those things, we need theoretical approximation. Need theoretical approximation error rate for so for that. So that's why if I put it, if you have a reproducible kernel back space, so you have those colorizations, so you give some approximation error rate. But the problem is, what I should say is, I have tried for several years, at least in economical applications. At least in economical applications, such as with this endogeneity problem, the neural net is not that effective. It doesn't perform as well. Even though in theory, if you have a lot of covariance, neural net should be better than SPRINE. But actually, in all multicolor comparison and in real data application, the SPRINE actually does really well. Even though, in theory, in terms of approximation error rate, neural net is better. Neural net is better. So, I think because remember the proof, there have several kinds of error. One is the optimization error. I think non-linear neural light, maybe the optimization error actually is huge. For our problem, because the example I gave you is how the non-provisioning endogenetic is not the standard and reduced and machine learning algorithm. And machine learning algorithm doesn't apply, we need to modify it so then the choice of tuning primary becomes much harder than spring. Thank you. Yeah. Let's stop here. Thank you. Thanks. Thanks for coming. So the next session starts at the end. Sorry. Yeah, I'm sorry. I wish I'd be there. Now I can see your snow. Yeah, it is very beautiful. Okay. Okay, so enjoy. Okay, thanks. Bye-bye. Yeah, bye, thanks. The entities are possible.