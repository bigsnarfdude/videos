Okay, welcome everyone to the last talk of the day. My name is Fernando Vico and I will chair the session. It's a pleasure for me to introduce Hugo Bordeman. As he's very well known in this community, he is vice president of the Abunda Steven Community and he very greeted also Vice President of the International Interaleborough Society, also of course, editor-in-chief of the Editor-in-chief of the report and dataset. So, please, Kilo. Okay, thank you, Freiland, for a very nice introduction. And thank you to the organizers for inviting me to speak and participate in this very nice workshop. So I will talk about realizations for matrix valued rational functions. Rational functions in several variables. And I realize now, looking at my first slide, yesterday I was practicing my talk and I decided to throw some things out. But I didn't edit the title. So I threw out this determinant of representations. But the slides are still somewhere. So if you have a question about it, So, if you have a question about it, I can answer all. Okay, so let's start with this classical result. So, f is a matrix value function. It's a rational function, so all the entries are just rational functions of z. And I assume that it's analytic on the unit disk. And then there is this result by This result by Arov, and it gives it this representation for functions, for rational functions that are contractive on the unit disk. So our function f has sup norm one, so the norm here is just the spectrum. The spectral norm of a matrix, the largest singular value. If F is contractive for every value in the circle, then I can write F in this form where ABCD is a contractive matrix. And in this talk, I'm just talking about rational functions, and ABCD are always finite matrix. Matrix. So obviously, people know everybody here that that's called a realization of the function. Realizations show up in systems theory, of course, as the transfer function of the system. Some of you may are more familiar with this form, but you can go from here to there if you introduce lab as one over C. But we are interested in the unit disk, so we would like to plug in C equals zero, for instance. Okay, and one of the many reasons why one may be interested in such a kind of result is, for instance, in interpolation problems, where you have certain values assigned at certain points, but you also want the function Also, want the function to have this global property that it's contractive. And if you know that such a representation is going to exist, then it translates into just finding these matrices A, B, C, D. And in particular, in this case, there's an easy way, once with the given data, there's an easy way to find the ABC. The AC. Okay, so this talk is about exploring analog results for matrix-valued rational functions in several variables. Okay. And so there's one important thing that I want to point out that could happen when you When you either go from different domains or you go from, in this case, one or two variables to three or more variables. There's an important difference. So first, let me just, what we're going to do in this talk is instead of these these complex numbers, we're also going to allow that we can plug in matrices or operators. Matrices or operators. And it's very simple. If this is our matrix polynomial, then we just replace Z1 by T1 and Z2 by T2 and so forth. And then this product is replaced by a tensor product. And these tuples that we'll be looking at, they will always be commuting. So we don't have to worry whether this CY. Have to worry whether this says C1 times E2 or it says E2 times C1 because these matrices or these operators are commuting, so there's no ambiguity. Okay, and if you take a polynomial and you look at the supremum norm on the unit disk, now you can do the same instead of a The same instead of a scalar variable z, you can plug in a contraction operator and plug it in the polynomial. And potentially, you would get a larger norm that way. But Vin Neumann's inequality says you don't. It's the same. And Ando in 1963 proved the same results for two variables. Two variables. And these results extend to analytic functions. But what happens when you go to three or more variables, there can be a difference. So there is this example by Veropoulos and Kaiser that of a polynomial in three variables and a tuple of contractive make commuting contractive. Contractive matrices, commuting contractive matrices, that if you plug that in the polynomial, it's bigger than the supremum norm on the trident. Okay. So that's a big difference going up in the number of variables. And the same phenomena occurs in other situations as some of us just discovered this week. Covered this week. But right. So that's an important point. And the additional point is that in the way these realizations are set up, whether you plug in scalars or polynomials, you get the same upper bound. So what do I mean by that? So suppose I have my function here in My function here in realized form, and the z is now some linear pencil, zi times pi. But the main assumption is that this is a contraction. If f has its realized form with ABCD, a contraction, you can compute identity minus fz star fz. And if you use things like this. If you use things like this here, you can rewrite that as a bunch of positive semi-definite expressions plus a term like this. And then you see immediately that if C has norm one, at most one, then this is also positive semi-definite. So then f of z is of modulus less or equal norm. But if you replace the z i's by CIs by operators, and you insist also that this thing will have norm one, you can do the same computation essentially, and you get the conclusion that when you plug in these operators in F, you must also be a contraction. Okay, so and when we plug in these operators, we have to replace A by Have to replace A by identity tensor A and so on. Okay, so if we're looking for a realization where the ABCD is a contraction, we should take into account that the function needs to have the property that whenever I put in a tuple of commuting Hilbert space operators of the right form, then it should still be a contraction. It should still be a contraction. And by the previous slide, that's seen that that can be different. Agler, Jim Agler, is sort of the pioneer in this area and he characterized analytic functions on a poly disk with this type of realization. With this type of realization. So, but in this talk, my main way I understood my assignment was to get to open problems. So I'm not going to give a whole review of the areas. So there I could name many authors, but I won't. There's a lot of work done on this. Okay. So Okay, so what we're going to look at is we're going to look at domains in d variable space that are of this form. And let me write down some notations. So if I have a tuple of exponents and I have a tuple of variables, then when I take this to that tuple, by that I mean this product. By that I mean this product, z1 to the power n1, z2 n2, and so forth. And when I have a tuple of operators and I raise it to this tuple of exponents, I mean that. And so our domains are defined through some matrix-valued polynomial. And so I start with a matrix. I start with a matrix valid polynomial, and then this is the type of domain I'm looking at. And as I tried to convince you, we should also be looking at when we plug in tuples of commuting operators that also satisfy that when I plug it into this expression, I get a contraction. So with this. So with this matrix polynomial, there's a domain in the d variable complex space, and there is a domain among operators. Here are some, this D is the unit disk, this T is the unit circle, and this BM is the unit ball in CM. Yeah. Okay, and what are some examples that we're interested in? I already talked to you about the polydisc, so that fits into the scenario when P of Z is just a diagonal matrix. So then this being a contraction just means that each single ZI is a contraction. Then there are the Cartan domains, the irreducible Cartanom. Irreducible quarton domains, numbers one, two, three have this form. And what these examples have in common, the top four, is that they're all linear in the variables. One more problematic example is this one. If I just take the one variable polynomial c1 squared, C1 squared, then of course, when this is of modulus less than one, that's just the unit disk. But if I look at operators who square as modulus at most, as norm at most one, there can be operators of any of any bound. There's no bound on these operators. no bound on these operators. So for instance, if I take the matrix 0n00 and n some big number, that will be in our domain TP because the square is just a zero matrix and obviously that's... So that's the kind of example we don't want. Can we read anything into non-normality or Jordan blocks or anything with that example or this being? I guess it's a little more than right, but yeah, no, it doesn't go as deep as that. So we should be careful with what type of matrix polynomial we, the type of domains we're looking at. And so that's the next slide, which I think I'm going to gloss over. I'm going to gloss over. But there is this condition, and the effect of that is that whenever these operators are in that domain TP, they will all be bounded by this number C in norm. And I'll come back to this expression in a moment, but I don't want to spend a lot of time on that here. Lot of time on that here. So there are some technical conditions that have the effect that the kind of operators we're looking at will all be uniformly bounded. And another aspect of our assumptions are that the domain, the closure of the domain that we're looking at, is so-called polynomi convex. And what that implies. And what that implies, because it's also compact, what it implies is that any analytic functions can be uniformly approximated by polynomials. And when you're in that setting, you don't have a problem with plugging in operators into your analytic function, because we know how to plug in operators in polynomials. And then, if any analytic function is a uniform Any analytic function is a uniform limit of polynomials, we just take that as our f of t. So those are some technical things that I don't think I want to spend a lot of time on. Okay, so the realization results that I forgot to mention my co-authors earlier. They were on the fan slide. The names are too long to keep them, keep repeating. Them keeps repeating them, but they're Green Spann, Kalarishno Verbovesky and Vinnikov and myself. So we're introducing this norm. So whenever I have a function, I'm going to plug in these tuples of operators and then take the supremum over that domain. And so that's And so that's this norm associated with the function. We also can, of course, just plug in scalars, but in general, this number is smaller than that number. And the associated classes of functions are called the Schur class and the Sure Agua class. So the Schur agla class is a smaller class of functions. Smaller class of functions than mature ones. Okay, so this is our main realization results. I have a rational matrix function without any poles on the closure of this domain. And we're going to insist that when we plug in operators, the norm will be at most one. The norm will be at most one. Let me plug in operators in this domain of operators that we're looking at. If it's strictly less than one, then I can represent f in this in this realized form. So what does that mean? That means there is some contracted matrix ABCD and there is also some integer. There is also some integer, and with that integer, I take P of Z, which defines our domain, and I tensor it with the identity. So I have many copies of P of Z. And then F of Z I can represent in this form. So if F of Z has this. If f of z has this form, then this eigenorm must be less or equal than one. So the result is not quite if and only if, because we're not able to get this realization when this norm is equal to one. And the reason is that we don't have a bound on this number n. So if we say, okay, So, if we say, okay, the norm is one and we divide by epsilon, make it a little smaller, we get an n epsilon, right? We get an n that depends on epsilon, but we don't know whether that n stays bounded when we let epsilon go to one or zero or whatever. Yeah, to one. Okay, so so, but this is our realization result, and I want to explain a little. I wanna explain a little bit the ideas behind the proof. And before I put on the next slide, let me just put on this simple example. If I just have a polynomial in one variable with real coefficients, and that polynomial is non-negative on this. Is non-negative on this interval 0, 1, then I can represent a polynomial as a sum of squares of polynomials plus x times 1 minus x times the sum of squares of polynomials. And so these are polynomials. So one direction is obvious, right? If I can write p of x in this form, then Then this is going to be non-negative when x is between 0 and 1 because this is always non-negative, and this is non-negative on 0, 1, and this is also non-negative. So this direction is easy. And for the other direction, you have to do some work, right? And this is... And this is, and we can rewrite this expression a little bit. I can rewrite this as h1 of x times the transpose, right? And then this part I can write in this form as a row of g one of x and so forth times the diagonal. Times the diagonal and then a column G1 of X and so forth. Okay, and notice that both of these are cones, right? Polynomials that have this, if I take the sum of two of those polynomials, it's again in this class. If I multiply it times a positive number, it's again in this class. So polynomials. So, polynomials that satisfy this form a cone. Also, any polynomials I can represent in this form, they form a cone as well. And so, the statement is that these cones are the same thing. Okay, so that as a little introduction. So, in this context, this expression is now going to look like this. Like this. So here is essentially a sum of squares. My w is, you can think of that as z bar. But we can treat w and z as independent variables. That's why we prefer to write w, but you can just think of it as z bar. So this is a sum of Hermitian squared. A sum of Hermitian squares. This is always positive semi-definite. And then these pj's, they will form the defining set of the domain we're looking at. So we're only looking at z-bar and z's where these things are positive semi-definite. And so it's obvious that if p can be represented in this. Can be represented in this way, and pj of t star t is positive semi-definite, and this whole expression is a positive semi-definite. So p of t star t has to, oh, I have this thing, then p of t star t should also be positive semi-definite, right? That's the easy direction. And so this theorem has the, as the converse, As the converse. So, and it's referred to as a positive Stalensat in semi-algebraic algebraic semi-algebraic geometry. People call that the positive Stellensat. So, the theorem is as follows. There are some conditions. So, the condition is that this scalar value polynomial. Scalar value polynomial, I can write it in this form. So, and that's referred to as an Archimedean condition. And the only way we can prove the converse is if we insist on strict positivity here. So, obviously, if P has this If P has this form, then these will be positive semi-definite. But to get the reverse, we can only prove it when we assume that this is strictly positive. Okay? But, and this is a very useful result because you're going from a statement that says, okay, this thing is positive on some domain, and then I get an algebra. And then it I get an algebraic expression for it, right? So that's that's the main point of this. And the proof of this result, it's proven by a separation argument. Okay, so that's an important ingredient in our proof. And once you have that, then you do the following. So we start with this rational function. We start with this rational function f that has this aglonorm strictly less than one. So we're going to, this rational function, we're going to write it as q times r inverse, where q and r are matrix polynomials. And then this condition tells us that this expression is strictly positive. And it's also strictly positive whenever we Strictly positive whenever we plug in tuples of operators. And then the previous result says, okay, well, then this thing you can represent it in this form. So we set this equal to that. And then if we bring this q star q to the other side and we bring this p star p to the other side so that everything is plus now then it translates. Then it translates into this equation. So I introduced some notation here, but out of this equal that, you just do the algebra and you can write it in this form. And that tells you that v star v is bigger than x star x. And that tells you that there must be some contraction that if I apply it. That if I apply it to V, I get X. Right? And that's where our contraction ABCD comes from. So, and we call this, this is, Joe Ball coined that term. It's called the lurking contraction argument. Actually, in his setting, there was... In his setting, this part wasn't there, and then there is an isometry between them, and it's a lurking isometry argument. But now it's become a lurking. So that's how we find our matrices ABC. Okay, and then you just write down the equations. And we want to eliminate the H. So you get A times P. get A times P plus P times R equals H. You bring the H to the one side and the R B R to the other side and you solve for H, you get this expression. And then you plug that H into the bottom line and you also multiply both sides with R inverse and then you get your your realization for the function. Realization for the function. Okay? So that's the argument of the proof. Okay. Okay. So as I said, I thought my assignment was to get as fast as possible to open problems. Open problems. So that's where we are now. So, what I'm interested in is to generalize this theory to a setting where we don't have I minus P star P is positive, right? P of Z being a contraction is the same as I minus P star P positive. But I want to look now at domains defined. Now at domains defined through an inequality like that. So here P plus and P minus are some given matrix polynomials and then associated with those we can introduce this domain. And then the corresponding domain for the tuples of operators should have this form. And then our new norm on the function f should be: we take the supremum when we compute f of t, where t is in this class of commuting operators. Okay. And we always assume that we start with an f that's analytic on the domain. Analytic on the domain, actually on the closure of the domain. Okay, and if this norm on the function is less than one, the type of realization we want is of this, this may be of this form. I guess that's part of the question. Why do I say maybe? Because really the type of problems I'm interested now in is where the The P plus is not necessarily square. So in the previous thing, P plus was just the identity. But what I'm really interested in is P plus is not square. And so then you have here a rectangular matrix. So maybe we should take some sort of generalized inverse. I don't know. Or maybe there should be some. Or maybe there should be some indirect descriptions. So that's one thing I'm interested in exploring. So I say that the P plus is not necessarily square, so there are issues to be worked out. And why am I interested in non-square P plus? For instance, the Cartan four domain can be put in this. Can be put in this context, but RP is in this case a 3x3. Oh no, it's a 3 by 2, I should say. And so it's a diagonal of p plus 1 and p plus 2. And p plus 1 is just the number 1, but p plus 2 is this vector. This vector of this form. And the p minus, the p minus, it's the direct sum of this thing with that. But the p minus, we don't have to worry about because the p minus, we're multiplying with a, so if it's square or not, that doesn't, the p plus is the important part. So that, so to extend it to get it to cartan. To extend it together to Cartan domain. Or would be interesting. Another very interesting example, at least to me, is this example. Remember that I had the SURE class and the SURE Ago class. And the SURE class said we just plug in scalars and then require it to be contractive. And the SURE AGO. To be contractive, and the Surago class, we plug in operators and require it to be contractive. And they were different classes. But if we choose this particular choice for P plus, so P plus has first three ones on the diagonal, and then it has this thing, this thing, and this thing on the diagonal, but the diagonal blocks aren't. The diagonal, but the diagonal blocks aren't square, right? So that's um and what happens in that case is that the um the domain in C3 is just the tridisk. But this norm now, this norm actually reduces just to the infinity norm. So if we can make this work, we get a realization formula. Realization formula for just sure functions. Okay, so that's another motivating problem. Another interesting example is the annulus. So, and this is just one variable case. So, the issue with the annulus. So the issue with the annulus is that it's because it has a whole, it's not polynomially convex. So some of the techniques that we've been using, we can't. But still, you can define a functional calculus because you have Lauren series for analytic functions on the annual. But again, Again, it's not clear what so it would be of interest to find a realization form. And actually, some of us have been exploring this example last few days. So that's another very interesting example. Finally, I want to mention the so-called To mention the so-called symmetrized by this, and that's this domain in C2, which you get when you take two points in the unit disk, and then you take the sum of those two and the product. And everything you get in that way is called the symmetrized by disk. Symmetrized by disk. And Agler and Young are interested because of some new synthesis kind of problem. The issue with the synergized by this is that you can a point in this G2. How can you check membership? Well, one way to do it is to write down this expression. And check that that's positive definite. And so that looks really easy, but the difficulty is that the bar here is in the first matrix, while the bars here are in the second matrix. And for the way we set things up and the way we find our realization, that's Our realization that's a problem. We always need the stars to come first, and we always need the stars either always on one side or on the other side, but not like sometimes on this side and sometimes on that side. So that's another challenge. I did recently just figure out a way how to get the bars to the other side. Have to get the bars to the other side, but then the matrix sees, then this matrix is not going to be square anymore. So, but if you can figure out how to deal with the star on the other side, that would also be interesting. So, that's what I have. Thank you. Thank you for your attention. What is the obscure?