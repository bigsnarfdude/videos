Yes, I've been working for the past two years on COVID monitoring at Ohio State. So, one project I thought I would briefly mention: actually, we did an implementation of the DSA methodology that Wasir and Greg talked about yesterday for repeat testing data. So, the idea was every student tested every week. And we actually implemented the DSA using that sort of interval sensoring idea that Wasir talked about. I'm not going to talk about that today, but I thought I would mention it since it's relevant. If you're curious, feel free to ask me. I'll be happy to tell you about it. Be happy to tell you about it. Our current project we're working on right now is environmental surveillance using dust. So, we've probably heard a lot about wastewater surveillance. Another possible avenue is, you know, you go into the room, you vacuum up the dust, and people shed virus, and you can find virus particles in the dust. I'm going to talk about our work with that data today. So, this joint work with, so Colin Kfaus and Joe Tien are mathematicians that work with me on the COVID monitoring team. And then Karen, Dana Miller, and Ashley Bo are like lab scientists. Both are like lab scientists who are involved in actually extracting the virus from the dust and you know creating this data set that we're using. Okay, so motivation. I think about passive versus active surveillance. So we have a lot of surveillance and mitigation strategies that are very good, but they require people to actively kind of buy in. Like, you know, surveillance testing, contact tracing, behavioral restrictions, vaccination. You have to, and some to some extent, like somebody said yesterday, you have to have people's. Extent, like somebody said yesterday, you have to have people's consent to make them do these things or convince the government to force them, perhaps. And, like, we saw at Ohio State, we had this, you know, weekly testing regime, but eventually, you know, students stopped showing up to test because there was no real penalty beyond someone sent you an email saying, you know, you're being bad. And then the administration over time, they know the students don't like it. So they would like to get rid of it because, you know, they want the students to have a good experience and be happy because it brings in, you know, the money. So there's a lot of pressure to do things that are less invasive. And we've seen a lot of limited. Invasive, and we've seen a lot of limited will by population decision makers to sustain these kinds of strategies, even though we know they're effective at stopping the epidemic because they're, you know, people don't like doing them, they can be disruptive, et cetera, et cetera. So, an appealing alternative is passive strategies. So, essentially, things I think about where you can surveil or take activities where nobody individually really has to do anything, like the students per se. So, like collecting environmental data, it's less invasive. Nobody has to go sit down for a test or take a rat. We talked about this morning. A test or take a rat, we talked about this morning. More robust than non-compliance. You can't really opt out of, you know, shedding dust into your room or opt out of the sewer system. You guys, you could, but it would be greatly onerous. And I don't think anyone would think to do it. So potentially you can get signal from a broader segment of the population. You don't have to worry about so much, you know, are the people who are not showing up the test, are they somehow different? And then the lower social cost. Administrators like this because it's less unpopular. And so one strategy. And so, one strategy or group of strategies is environmental surveillance. The idea being that people who are sick, they shed virus particles into the environment around them. You can observe those particles, and then based on that, you can try and infer things like, you know, what was the prevalence in a given space over a given time period, or get at that somehow. So, you've probably heard a lot about wastewater surveillance. It's been pretty widespread. So, a few examples. One paper looked at wastewater in Ohio cases, five-day rolling average regression, and the quadratic regression. And the quadratic regression. Another study looking at wastewater in Connecticut, and there's a survey at the bottom noting widespread use of wastewater surveillance, even going back to before COVID to polio used wastewater surveillance as well. And a lot of in particular sort of regression or autoregression or time series kind of models are what I've seen implementing this kind of stuff. So another avenue that has not been explored as much is environmental dust. And there's sort of evidence from what they call the indoor monitoring lab science community that says it's sort of a Lab science community that says it's sort of a promising idea. So, our class, our NB is our collaborators. So, they collected some dust and they found that, yeah, there actually is like virus particle, there is signal in this dust. So, we got it from isolation rooms in Ohio State. And another paper looking at a relationship between viral load and patient versus environmental dust. So, it seems plausible there's a relationship there. Another paper saying that maybe you could estimate airborne concentrations of the virus from dust. And we know that the virus spreads via often airborne mechanisms, that sounds like it could be. Of mechanisms that sound like it could be worth looking at. And so here's the data we're working with. So it's kind of a pilot study data, I would say. We have very limited data at the moment, but the idea is that we had fall, in 2020 fall, we had isolation rooms. We tested positive, we get isolated for 10 days, you go to this room. At the end of that period, the cleaning staff would come in in vacuum within 18 hours of students leaving the room. And then the lab scientist, they would take a 50 milligram sample of this dust, they would sieve it to kind of, you know, try and get things more uniform. Try and get things more uniform. And then they would take a sample and extract the virus RNA from that sample. We had two collections on 1028 and 1104-2020. And isolation are kind of a nice case because you can sort of have a good idea that who's in there. You have a pretty good idea that they actually are there. And you have a pretty good idea that they're probably infected. We were using these PCR tests, and for the PCR tests, false positives don't seem to be very common. So it's probably likely these people were at least infected close to the time they were in isolation. Close to the time they were in isolation, and probably there's some overlap for most of them. For full details, you can see this paper, the RMB paper. They go through all the details. I'm not going to go through that here. So we wanted to create a model where we could try and link the environmental data to prevalence. The first thing that we thought about is sort of what do we want in our model? There's two things we wanted to do. So most of the previous approaches I've seen have focused on sort of regression models and large-scale trends. Nothing wrong with that. We wanted to have a mechanistic model. We wanted to have a mechanistic model. As several speakers have talked about, mechanistic models have a few advantages. So, the one is you have sort of biological interpretations. You can, assuming you have a mechanistic model where you've done a good job building it, you can maybe learn about the biology behind things. You can incorporate what's known about the biology. You can do sensitivity analysis. You can choose parameter values, perhaps, in a way that's harder to do with regression models. Where if I change some biological factor, it's really hard to say how exactly that affects things. And sort of likewise, you can do. Things and sort of likewise, you can do scenario modeling by the same logic. You know, if someone says there's a new variant, we found that viral loads for this variant are 50% higher. I can try and incorporate that information into my scenario modeling for our environmental surveillance model. So that was one major goal. The other one was we want to model inter-individual variation. There's a lot of inter-individual variation among individuals in viral load. It's well known. And so one would expect that would translate to environmental shedding as well in some sense. Environmental shedding as well, in some sense. However, the challenge is you're not going to directly observe that inter-individual variation. Like, I'm not actually going to observe the viral shedding from probably one person. Probably what I do is I go into a space and I vacuum a bunch of rooms where I have like a sewer catchment that serves a wide area of people. And I get sort of all the signal from that put together. But we know this inter-individual variation exists. So our model probably needs to account for it. Otherwise, we're going to have problems with over-dispersion. So. So, we decided to go with a Poisson process-based model. We actually originally thought about doing an ODE model, but a Poisson process model seemed like it would get at sort of the same core ideas, but it turned out you got sort of a lot of nice results out of it in terms of interpreting and working with the quantum. So, pretty simple. We'll start out sort of as a basic shedding model for one person, let's say. So, we'll say we have an individual shedding virus particles in the environment at time zero, and that goes on for a while, and then we observe the virus particles. Goes on for a while, and then we observe the virus particles in the environment present at time t. And so we'll assume the following: we'll assume particles arrive according to an inhomogeneous Poisson process with time-varying intensity. I'll call that lambda sub t. And then we'll say once a particle arrives, it kind of decays like the radioactive decay model, decays with some half-life, decays at an exponential rate, and then we'll say particles decay independently of each other. So a particle could be shed into the environment, but then it could also decay before you get to observe it. Also, decay before you get to observe it. So, you may not observe all the virus. So, here's an example of how you could sort of use this model to generate data, let's say. So, say I want to know how much virus is actually in the environment at time t when I observe. Well, I could first draw the total number of arrivals nt from my Poisson process. Then I could say conditional on nt, I can draw the arrival times by using the appropriate density given by the endomogenous sort of intensity function. The endomogenous sort of intensity function. And then once I know the arrival times, conditional on that, I could say, well, if I know the virus particle arrived at this time, what's the probability it's still there when I observe at time t? Flip the appropriate coin, and then I count all the ones that survived to time t. That's my data I observe at the end for one person. Question? So we're thinking of the RNA for particles from the virus. Like, how many copies of RNA are there? Like, how many copies of RNA are there? Yes, particles decay exponentially at rate c. So you essentially think about it as picking like a half-life for the particles. Like our collaborator suggested that we should maybe try at first a half-life of a week for virus particles. It's not really very well known, it seems from the literature, but we did a week. Okay. So the challenge here is that, of course, in reality, I don't observe. Of course, in reality, I don't observe the NT and I don't observe the SI, right? I don't actually get to observe when the particles arrive. I don't know. I just observe the data at the end. So the question is: what can I say about the distribution of Y if I don't know the arrival times? And it turns out what you can do is you can thin the Poisson process, and then you don't need to know the arrival times. So the idea is that if I condition just on the number of arrivals, but not the arrival times, well, I get sort of this Bernoulli mixture for the probability. Bernoulli mixture for the probability that a given particle survives, but a mixture of Bernoulli's is again Bernoulli. And essentially, I'm just thinning the Poisson process by the appropriate P given by this equation. And so it turns out that you can then characterize the distribution of y without actually needing to know the arrival times, which is good because you can't observe them. So what do you have? Well, I like to talk about it in terms of three quantities. So y is what we're going to observe, number of particles in the environment at time t. Number of particles in the environment at time t. We'll define u to be the integral from zero to t of lambda t. So mu is the expected number of particles you shed into the environment over the entire interval. They're not all going to survive. Then I'll define p to be this here, and p is the probability, sort of the marginal probability a particle survive, so not conditioning on its arrival time. U is the expected number of particles that arrive, and p is the unconditional probability an arriving particle survives till time t. So you can think of why. So you can think of y as a Poisson process thinned with probability v. And then, so what's the density or what's the probability distribution or likelihood of y? Well, it turns out that if you go through the math, it simply is Poisson. At the end of the day, you observe a Poisson random variable with Poisson mean mu q, which if you know about Poisson processes is not terribly surprising. But the nice thing about that is this is a very simple model to work with. Simple model to work with. So y is Poisson mu p. And if I had n independent individuals, some of Poisson's is again Poisson if they're independent. So I would simply get Poisson and mu p. And I could then use this for whatever estimation or statistical framework that I would like to do. So one sort of interesting realization you get from this, well, actually, let me go back one second. So our eventual goal is going to be to use this model sort of in a two-step process. So the idea is you have to have some data at first where you To have some data at first where you know something both about the virus in the environment and the prevalence. So, the idea is you will assume for the moment you know why and you know the number of infected people. You calibrate, you estimate UP from that. And then once you have that, you can then collect new observations of why where prevalence is unknown, and you use your calibrated parameters to estimate the new prevalence. And in this very simple setup, you could just do this with the Poisson likelihood. On likely. An interesting point is: if you do this, that you notice μ and P are not separately identifiable. You can only identify the quantity μp, and thus C itself is not estimable under this framework. So, because you can't distinguish between more shedding with faster decay and less shedding with slower decay. Okay, and another cool kind of thing about this is that. Another cool kind of thing about this is that, so it's the only identifiable quantity, which is bad news in one sense, but it's good news in another sense because if you fit the wrong model, if you get UP right, you can still do the statistical problem and sort of estimate N correctly, even if your model is wrong. Or maybe you don't need to know lambda t and c exactly. As long as you have enough information to get at what UP is supposed to be, you can probably do okay. So, what happens when you add inter-individual? So, what happens when you add inter-individual variation? And so, I mentioned inter-individual variation is well documented. Some papers here that talk about variation in viral loads, a wide range of environmental concentrations in vacuum dust. So, we would likewise expect to see that in viral shedding. And then also, you'd expect that there's going to be inter-individual variation in infection timing. If you isolate, say, on days, you know, day, I call it day zero, it doesn't mean you were infected on day zero. We detected. On day zero, we detected you and put you in isolation on day zero, but you could have been infected some other time in the past, and it's going to vary from individual to individual. So, we want to think about how do you take these things into account. So, here's kind of a graphic of where we're headed. So, you have a bunch of individuals shedding. They have different shedding curves, different infection timing. They shed pathogen in the environment. Some of it's removed, but some of it's not. And the surviving pathogen eventually comes over here. We observe that as Y is the idea. Is the idea. So, how are we going to do this? Well, we'll just sort of think in the general case for the moment. So, we'll say that each individual has a vector of individual level parameters, theta i. That might include things like when you got infected, you know, how high the peak of your shedding curve is, whatever. And we'll assume that they're drawn from some distribution G that characterizes the population distribution of those things. And then, once you have that, we'll say that our shedding curve, our That our shedding curve, our Poisson intensity, is now not just lambda t, it's lambda t of theta i. So it depends both on time and the individual. And then we'll define mu i and p i because now each individual has a different amount of expected shedding and has a different probability of particles surviving unconditionally because this depends on the shape and timing of their infection curve. So, what does this do to our model? Well, if you were able to condition on the Well, if you were able to condition on the theta i's, if you knew those things, then you could do the same sort of math as before, and you once again get that y is Poisson at time t, but now it's going to be Poisson, not n mu p, but summation mu i p i. Of course, you're probably not going to be able to observe all of the theta i things, especially the things that are sort of like, you know, internal, like, you know, what's the peak of my viral load? I'm probably not going to be able to observe that about an individual. So, unconditionally, what happens? Well, unconditionally, I Unconditionally, what happens? Well, unconditionally, I essentially just have to take that Poisson distribution and I mix it on the distribution of whatever that vector of parameters induces on summation mu IPI. So it's a Poisson mixture where the mixing distribution is a distribution of summation mu IPI that's induced by whatever inter-individual variation model I decide to put on this. And so, what's really interesting about this is, once again, if I assume, as I did before, that Assume as I did before that things are drawn IID from G, therefore, you know, the mu IPI are IID draws from somewhere. If I'm able to characterize the distribution of mu IPI well, then I should be able to do this problem statistically reasonably well, even if my model is wrong. Or, you know, we always talk about mechanistic models. People will say, you know, they're great. You get to learn a lot of things, true. But one of the downsides is people will disagree about what's important, right? We like to joke that people will say, you know, Taco Tuesday is really important. Know, Taco Tuesday is really important. If your model doesn't have Taco Tuesday in it, well, it's just not a good model because everyone loves Taco Tuesday, it has a massive effect on, you know, whatever. And so I say to them, well, yeah, you know, put Taco Tuesday in your model, but unless it's really helping you characterize the distribution of mu IPI, it's not helping you with the statistical problem. I mean, it still has value as far as the correct mechanistic model gives you the correct mechanistic understanding, but statistically, it may not be necessary to fit the correct model. So, a couple of special cases that I think are interesting. So, one sort of, let's say, sort of a simple case. We'll just say that theta i with a single inter individual level parameter theta i, it's a scaling of your curve. So, everyone's shedding curve is the same shape, but some are taller, some are shorter. So we'll define a shape, you know, for convenience, we'll say the shape as integral one, and we'll say that theta i is the scaling. So, your shedding curve now is simply theta i times lambda t. And you can observe here that then, of course, And you can observe here that then, of course, μi is theta i, so on average, we expect the individual i to shed theta i virus. And then pi is going to be the same for everyone because pi only depends on the shape of the curve, not the height, depends on like the relative heights. So everyone's going to have the same value p, p is not going to depend on theta i. And then what happens when I want to go and mix this? Well, one case where you can do things analytically very nicely is if I. Do things analytically very nicely is if I say that I further assume that these theta i's come from a gamma, then what I get is a Poisson-gamma mixture, which is well known to be negative binomial. And for those of you who do SATs, this probably sounds very familiar because this is exactly one way you can derive negative binomial regression, essentially the over-dispersion and over-dispersion model for Poisson regression. And so this is sort of saying that we can connect that idea to a mechanistic framework here if we want. To a mechanistic framework here, if we want. If you mechanistically assume that everyone has the same shape of shedding curve, the heights are random, drawn from a gamma, you get exactly to what is essentially negative binomial regression. I mean, there's not really any covariates in this setup here, but if you had them, then you could add that on top of it. So I kind of like that you have this connection between the mechanistic aspect of the model and statistical frameworks that you can use regression models. So a more complex case that was suggested by this Kistler. Case that was suggested by this Kistler et al. paper was the triangular hinge function. The idea is that people have variable length of time they're infected, they have variable peak heights, and there's going to be a lot of variation. So the idea was that log shedding is a triangle. So the idea is you're sort of then raw scale viral shedding should increase exponentially to a peak and then decrease exponentially back down to eventually zero. And so we have essentially straw parameters that are going to be what's your peak height, what's your onset time. Peak height? What's your onset time? How far is it from onset to peak? How far is it from peak to recovery? There's this complicated formula here that you can look at, but I'm just going to show you the picture instead. So here's an example of what this curve might look like. So individual I here, they were infected at time Ti supergroup zero. They increase exponentially to this peak, delta I, and then decrease exponentially down to zero. And as you can see here, the Ti zero is the four times zero. So we didn't, you know, they didn't start getting infected when they were isolated. didn't start getting infected when they were isolated, they were infected first and isolated after that. And so, is that the realistic model? I don't know. Kissler et al. claim that it is, but it's perhaps a more realistic model than just assuming everybody has some fixed shape. So we wanted to fit our data to this model and sort of see how it would go. Now, we have this sort of pilot study data where we only have two data points. So you got to use one for calibration and one for testing estimation. And therefore, with one data point, you can only fit. And therefore, with one data point, you can only fit a one-parameter model. That's just life. So we went ahead and took the values for the infection duration, time from onset to peak, time from peak to resolution from the Kistler paper. We're going to assume that infection time is uniformly distributed over the possible interval. We took our decay rate C to be log two over seven. The idea is that particles have a half-life of seven days, just by our collaborators in indoor monitoring. And our sort of unknown parameter here. And our sort of unknown parameter here is delta i, the peak height. And we're going to assume it comes from a one-parameter family, so exponential v. So, probably in reality, you'd hope to be able to collect more data, have maybe more parameters. You can estimate a more complicated, a more sophisticated model. Well, with two data points, this is really all that you can do. And the minus one in the uniform of the idea to represent. So, we saw at Ohio State, there was sort of a one-day lag between testing positive and actually getting isolated. They got to go, you know, find the first. In actually getting isolated, you got to go, you know, find the person and move them to the isolation room. That takes some time. So, we added the extra one day to think about that. So, how did we do this? Well, the idea is here's our data once again. Our observation is copies per milligram dust. We took that 50 milligrams of dust, kind of sieved it to create uniformity, observed how many copies per milligram. And then, because these are isolation rooms, we know who was there. We know that they were infected, you know, at least approximately. So, our two options. So, our two observations were on 1028, we had 173 copies from milligram from 39 people who were infected. And then on 1104, we had 283 copies from 39 infected. So, you can see in these two data points, at least, a lot of variation already from point to point. We use the Bayesian MCMC scheme to estimate beta, and then once you've estimated beta, plug that in and estimate N. So, the idea is you take the posterior of beta using it as the pure prior for the estimation scheme. I'm not going to get into that because I think it's actually not really the main point of this work. I think it's actually not really the main point of this work, and you know, it's relatively straightforward, I think, in this scenario to figure out how to do that. So, we used our 1028 data to calibrate, and then we estimated n for our 1104 data. And you can see how that turned out. And so you see that the posterior mean is significantly higher than the true prevalence. But you kind of expect that because the number of copies per milligram is way higher, even for the same number of people. That just looks like it might be with two data points, it's hard to say, but it may just be a feature. With two data points, it's hard to say, but it may just be a feature of this data that there's a lot of variation. That could be how things are. But you can see that if you look at sort of the 95% interval, you do sort of at least cover the correct value. And so one of the questions we're going to get at, we hope to get at is sort of when you're doing this kind of thing, what can you really expect to achieve? And I think that because of the sort of natural levels of variation we see in environmental data, pinpoint precision estimates of prevalence are probably not going to be possible. So the idea maybe is to. So the idea maybe is to go ahead. How does it take up the remote? Okay, I'll show you a picture in a second. I think it's a few slides down, but it is. So it's the number of copies of RNA per milligram of dust. So yeah, it's concentrations, but we kind of think about it as being a count because the analysis procedure, they first sieve the dust to sort of make everything useful. They first sieve the dust to sort of make everything uniform, and then they sample from that. So, that was the idea. So, it's not, it is technically a concentration, but I think the Poisson assumption is probably okay because of the sitting procedure. Although you can disagree with Poisson. So, anyways, that's what things look like for our real data. Kind of limited, so we wanted to explore things further with a simulation study. And so, things you could ask are: well, how much variation in shedding might exist? And shedding might exist? Or how does the estimated uncertainty change with n? Is the uncertainty coming from, is it coming from not knowing beta, or is this coming from the fact that there's a lot of variation in the data? And then sort of lastly, well, what if you, I've been saying you can fit the wrong model, what if you actually fit the wrong model? What happens? And so we estimated, we generated synthetic data using beta was 0.15. That was the math estimate from our real data calibration with two data points. Once again, I'm not going to strongly. With two data points, once again, I'm not going to strongly argue that that's really how reality is, but just to sort of get an idea of how this might go. We generated pairs of observed copies of milligrams for each of n equals 10, 40, 80, 200, 500. We used the first of each pair to estimate beta. We use the second of each pair to estimate n both when we say beta is known, we just know what it is, and when beta is unknown. And then we also fit the misspecified model. So here's just So, here's just if you simulate the distribution of how much shedding do you get from different numbers of people, you see if you take these parameter values in this Kistler model, you get quite a lot. Like most people shed very, very little virus into the environment. And there are sort of a few number, I guess instead of super spreaders, super shedders, we'll call them, right? This is consistent with what the other papers have been finding, like this Laram paper, this HISTOR paper. They found the, you know, that there's sort of You know, that there's sort of order of magnitude differences between people's viral loads. So it's not surprising that if that's the case, you'd see this kind of pattern of variation in environmental shedding. It makes a lot of sense. Of course, if you aggregate, say, 40 individuals, I mean, there's still a lot of variation. You can easily go from 100 to almost 300, but it's not quite so wild now. Like, I'm probably not going to say that 40 people are no people, whereas I could very easily say that one person is no people. Is no people. And it's probably the case that this is sort of an inherent challenge you have to deal with in environmental shedding models: that there just simply is a lot of variation for individual and individual and viral loads and viral shedding. So we fit our pairs. Here is the one where we said we didn't know what beta was, we estimated it. So the idea is you use your first data pair to estimate beta. Once you have that beta, you take that. That's your calibration step, you estimate it. You can see how we did. Step you estimate. You can see how we did for the different values at n. And you see that the numbers can be good. They can be pretty far off. Like the one for 40 in particular is kind of high. But it sort of just happened once again that for that draw, the simulated dust value was much higher than the simulated dust value for the calibration step. And that seems like it could happen based on the histogram we just showed. But you do also see that these models think there should be a lot of uncertainty, which I think is true. There's going to be a lot of uncertainty. True, there's going to be a lot of uncertainty. And we didn't totally miss the meaning of our intervals, although the one is really close. And then you can see, of course, that as n increases, you get sort of the prevalence is more precise as a ratio of n, right? So sort of as a percentage, I get better and better as you'd expect. But it seems like if you assume this sort of Kissler model situation and the isolation room data, which maybe is even optimistic, you're not going to be able to tell precisely the number. Able to tell precisely the number of people who are infected, maybe the best you can hope to do is you can hope to tell, sort of, you know, nobody from small outbreak from big outbreak, and that's maybe what our goal should be. So one question was, well, this uncertainty, is it coming from not knowing beta or is it coming from the data? And if you say, well, I know beta, you, of course, do have less uncertainty, but you still have quite a bit of uncertainty. And you can see that it looks like a substantial amount of the uncertainty is coming not from not knowing the beta. Uncertainty is coming not from not knowing the beta parameter, but from the inherent variation in the data. So, probably there's nothing you can really do to totally get rid of that, no matter what you do. So, we also fit a miss-specified model. So, if you're going to fit a miss-specified model, you probably want to be lazy and save yourself some work at least, right? That's the whole reason we do this. So, they say, what's the simplest thing you could do? Well, just make everybody's shedding curve a constant. You know, it's like a horizontal line. A horizontal line. We'll say it's a constant with random height. So we'll say that your shedding curve lambda t of theta i is simply the height delta i. And we'll take that delta i to come from, once again, exponential beta. So again, we'll use our first of each observations to estimate beta, take that calibration, use the second observation to estimate n. There's no point here in assuming that beta equals 0.15 is known, because if you say that beta is known, that's just wrong for this model, right? So Model right, so uh, but you can sort of see what's going to happen here. What would you expect to happen? Well, you'd think that if I get rid of all this variation in like timing and you know all that stuff, that I'm probably going to understate the uncertainty. And you see that is whoops, that is indeed what happens. So the mean estimates actually aren't that different, the means and medians, but you the uncertainty is smaller and perhaps smaller than it should be. Yeah, that's that's probably true. That's probably true, but I think for these situations, what you might do is you might like go in and like measure every week or every day or something like you know, they go into the Every day, or something like you know, they go into the dorms every week and vacuum and collect the dust. So, I think that if you're going to use it for like every six months, no, you shouldn't do this. I agree. But I think that in practice, that's not what we're trying to do. Yeah, just to build on that, and I think another thing that was a long time is the independent because you have people attaching each other and showing some time. I think you also get more and more. I think you also get more and more into the idea of like residual virus, too. There's going to be another concern that I'll talk about. For the isolation rooms, the idea we didn't consider this because, like, so, one, first of all, you can't unless you have more data. You have to have a time series of data to do anything. And two, they're professionally clean between people going into them. So it's probably maybe in that case, at least reasonable not to worry about it so much. But I think for more general applications, you're going to need to think about like sort of residual dust as a time series. But, anyways, do you see that? But, anyways, you see that the means don't really change. The uncertainty is lower. And that should make a lot of sense because if you only have one parameter, you essentially can at best estimate a mean sort of, right? If you want to have a mean and a variance freely, you have to have two different parameters. And so we'll get to that later. Here's the histograms that Laser was asking me about. So here's the posterior densities for a couple of examples for both the hinge and the constant function. Hinge and the constant function. So I think the posteriors look sort of like reasonable posteriors, something crazy is going on. And you see that the difference is mainly coming in the tails between the hinge and the constant function. So in the misspecified model, I somehow leave out sort of the very extreme outcomes that are rare but possible, but maybe I need to think, that maybe I need to think about. So you can see that going on then. The vertical line is the true value for these. So, what do we learn from this? Like I said, once again, this is kind of first of all, a pilot study. Second of all, you're probably not going to actually want to surveil isolation rooms because you already know who's in there, right? It's a nice test case because you know what's going on. You don't need to worry about things like residual dust, perhaps. And so maybe sort of this is sort of getting at, you shouldn't expect to do much better than this really with any application because this is sort of an idealized scenario in some ways. But there is substantial. But there is substantial, there's a lot of evidence for substantial variation in individual shedding. I think it's important to take that into account. Of course, larger ends are going to yield better prevalence estimates. If you're putting this into practice in like, you know, a single dorm versus like a sewer catchment serving the whole university, maybe the sewer catchment signal is going to give you more precise prevalence estimates as a percentage. Not surprising. For the scale we're working on, it seems like maybe you could tell, like, you know, healthy from a small outbreak from a big outbreak. From a small outbreak from a big outbreak, but I'm not going to tell you whether n is 10 or 15. In this scenario, we looked at, it doesn't seem like it's probably going to be possible to do that. There's less uncertainty if you know the model, duh, but actually not that much. It seems like a lot of the uncertainty, probably most of it, is coming from the inherent variation in the data in our setup. So even if you knew the model perfectly, you still have a lot of variation you have to deal with because you can't observe the individual level characteristics. Using the misspecified model. Using the misspecified model gives you okay point estimates, but underestimates uncertainty, but that's limited by a single parameter. If you want to fit a misspecified model and you have more parameters to work with, well, your goal really is, sort of in this framework, is to say, can I adequately describe the distribution of mu i p i with some number of parameters? And if you have, say, two parameters, you can describe the mean and variance. Three, you could do the mean, the variance, and the skew, and so on, like with the method of moment setup, which you know, people think that's dumb, right? But even if People think that's dumb, right? But even if you do something sort of very simple and dumb, you can do that. And so maybe at some point, if you have a good number of parameters, you can describe the distribution of mui pi well enough that it doesn't really matter if your mechanistic model is wrong, at least for the purposes of estimation. So we actually do have better data now where we've been sort of surveilling over time from week to week, but we don't have IRB for that data yet, so I can't talk about it beyond that it exists, I guess, whereas this fall data is covered by our IRB. So as you guys probably know, with human subject data, As you guys probably know, with human subject data, there can be a lot of hoops to jump through to get access to it. Everyone's privacy is protected. It's all important stuff. But our goal, sort of in the end, is we're in the process of implementing this at Ohio State. We're going to try and use it sort of as a trigger, a threshold triggered intervention strategy. So the idea is you do this environmental surveillance in the background. Mass testing has now been phased out, so we don't have that anymore. So you do environmental surveillance in the background, like you go to some dorm every week and you collect the dust, and you create some threshold. And if the dust virus goes over. Create some threshold, and if the dust virus goes over a certain threshold, you say, Okay, we've got to step and do something. We're going to pass people in the storm, we're going to do quarantining, whatever we're going to do. But that way, the idea is that we can have interventions that are hopefully more targeted and not sort of burdening the entire population unnecessarily. And then the idea behind this framework is you can state your threshold in terms of some belief about what you think it says about prevalence, but in a Bayesian sense. And then, of course, there are lots of things left to address. So, one of them is residual virus. I think that in a dorm, you're going to have to think about: well, not all the virus goes away from week to week. So, when I think about the virus I observe now, it's going to be not just virus that was shed that week, but also virus that was there some previous time that was decaying, you know, from other people who were there previously. Different spaces have different dynamics. Like, you know, an isolation room, you're isolated, you're there all the time. Even in a dorm, you might live there. Time. Even in a dorm, you might live there, but you're still not there all the time. And then, if you think about a dorm versus a library, it's going to be different again, or a dorm versus a dining hall. So, thinking about how we take this framework to different spaces. My thinking is probably you have to calibrate sort of on a per-space basis. Like you calibrate for dorms, you calibrate for whatever kind of building you're going to do, but you can't just take what you found in a dorm and transit that to a dining hall. It's probably not going to make sense because people's behavioral patterns are different. Is there Is there spatial dependence? I don't know, there might be. People might visit the dorm next to them more often. I mean, Evan saw with the Paris data, he looked at there were there was a lot of spreading, you know, with people that you lived with versus, you know, classes. There was more spreading in that kind of scenario. Threshold-based intervention, of course, the big correct question then is what should the threshold be, right? You say, I should have a threshold, then you got to decide what it is. So then you get into not only characterizing what you believe about prevalence, but trying to characterize what you think the cost and benefit. What you think the cost and benefit of doing things are. And that gets into questions that are perhaps, you know, not purely mathematical questions anymore. But I think we still have useful things to say about them as mathematicians. A few concluding thoughts. So I think the appeal of this framework is that it's mechanistic. You can incorporate mechanistic biology. So if you think you have a good mechanistic model, you can learn about the biological mechanism. But if you want to do predictions, you don't necessarily have to do that correctly. That correctly. As long as you characterize the distribution of μIPI in a sufficiently sort of rich way, your estimation and predictions are probably going to do okay, even if your model is not realistic. And you even have some theoretical guarantees. Like, for example, if you did method of moments, you can get, say, K moments right with K observations of calibration data. That's how method of moments works. And the key to all this is sort of that everything goes through this new IPI, which I think. That everything goes through this new IPI, which I think is sort of that's to me that's sort of the really cool part of this framework and sort of what I like about this. It connects the mechanistic ideas to the new IPI, to statistical methods that don't care about the mechanics per se. All right, thanks. I will open it up for questions.