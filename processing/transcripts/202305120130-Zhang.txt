I'm having some issues with the people online who they can't hear us, so anyway, so um the second topic by Li Kung Jiang. He will be talking about emulating complex batch models using some kind of a visual extreme model. So thanks for that introduction. And thanks for the invitation from the Hubble makers. So this is joint work with my PhD student, Xiaoyu, and then also my collaborator, Chris and the Raphael. Collaborator Chris and the Raphael. So, first, to motivate this talk, we all know that basic ACE simulation models are very computationally extensive, and they're very challenging in uncertainty quantification. Particularly, it's hard to do uncertainty quantification for forward simulations. And then it's also hard to do calibration. Also, inverse problem in current translation. Problem in kernel translation. And then in recent years, there have been some success using surrogate models to do uncertainty quantification. For example, Fausion process, polynomial chaos extensions, and some applications of neural network and generated models. One drawback of these ambulators is that they don't naturally account for Next, a path for extremely penetrating. However, in weather mass or in climate, usually it's the extreme simultaneous occurrence of extreme war weather that is more constructive. And we want to have a model that can accurately model those, simulate those simultaneous extremes. And then in our work, we want to pattern to an emulator that accounts for the spatial extremes and generate realistic ensembles of complex multi-class simulation models. And then we want to, at the same time, capture the dependent spatial streams and provide measures of uncertainty. And the last goal is still work under progress. Still work under progress. It's a bit more challenging, which is to facilitate calibration of complex computer models within a Bayesian non-survey quantification context. So our work is integrating a spatial extremes model with variational algorithm coder. So what is variational algorithm coder? So first, let's talk about dimensional reduction. So what dimensional reduction is trying to do What dimension reduction is trying to do is to reduce the dimensionality of your input data and try to represent the input data, which is in higher dimensional space, in lower dimensional, in the latent space. And then we can build some decoder to reconstruct the original data from the latent representation. And what we want to do is to try to minimize the information that's lost during reconstruction and then also to retain. To retain as much information as possible with the dimensionality that you fix. And then one famous example is principal component analysis. So what principal component analysis does is also like trying to reduce the dimensionality of the input. How he achieves that is to combine all the input and then you try to calculate. The input, and then you try to calculate the central covariance. What you do is to graph the first let's say n eigenvectors that correspond to the first n largest eigenvalues. And then the decoder is simply the natrix with the column vectors of the eigenvectors. So in this case, the information that's retained is highly dependent on the number of On the number of principal components that you choose. And then this is an example of application of the principal component analysis in phase recognition. And you can see that as you choose more and more principal components, you can reconstruct the data better and better. And then And a more complex version of that is to use neural network. So you build neural network for both the encoder and decoder. And that's what people call auto-encoder. And then what they try to do is try to minimize the loss, which is the distance between the input and your emulated output. And then the variational autoencoder is. Encoder is a bit more complex because we want to encode input as a distribution. And why do you want to do that? That is because when you, in either PCA or autoencoder, what's in your latent space is a fixed point. And then you don't have, there's no ability to generate new content. Let's say you have. Let's say you have a group of images in the input, and then you try to reconstruct this new image that is not in the input, that will fail, this other inputter will fail because it's not in the linear extension of the original input. So, what this variation of our input enables us to do. where it enables us to do is to encode the input as a distribution and then before we decode from the latent space we sample from the encoded distribution so that we can get a variational output that is that has some sort of uncertainty attached to it and then sometimes we call this latent space verticalization so the So, the encoder is trained to ensure the mean and variance and control of this encoder, which is QP. And then in this case, what we're trying to optimize is this evidence lower bound. So, evidence lower bound consists of this marginal likelihood of this input data, and then also the KL distance between the encoder and the. meaning the encoder and then the true posterior of the latent variable. And then it's easy to show that the evidence lower bound is simply a distance between the joint log-like with it and then subtract by the log of the encoder. So it's So, it's in the expectation of the encoder. So, what we can do is to utilize some Monte Carlo estimation of this expectation. So, in the machine learning community, people usually use the minibash version of this algorithm. Let's say that you have n different replicates of the data. What you do is, for each year duration, you For each iteration, you randomly sample a moon batch of the input, and then you calculate this of the buffalo values from each input. And then for each input, you do want to sample the latest variables many times so that you can estimate the expectation. And then you want to update the To update the beta, which is the decoder parameters, and then V, which is the encoder of parameters using the configurated descent. So that you do that iteratively until you get a convergence of optimum algorithm values. So this is an example of what various model inputter can do. So on the upper left, you have an input image. Left, you have an input image, and then what the variation model does is to try to animate the original image, but have some variation that is sort of different to the original image. So in analogy, what we want to do is to, given a climate model output, we want to emulate that so that we can get a variety of maps that. Of mass that looks similar to the original map. So, how we're going to do that is to utilize this spatial exchange model called MES Infinitely Divisible Spatial Model proposed by Ryan Rich, Benjamin Shady, and also Rafael John, who worked with Ben Student Gretchen Boff. So, what max infinite divisible Infinite divisibility means is that given the distribution function, any positive power of the distribution function is still a valid distribution function. This ensures that the component-wise mesma of a spatial process is still a valid, this still a valid stochastic process. So this formulation. So, this formulation of MaxID is a product of a wide voice process that has a Friche margin and then also a logicist representation process with the latent variables in Z that is exponentially tilted positive stable variables. And then those W, K, S are the basis functions. And then how the reason why this works is because this latent variable is exponentially tilted positive stable. So first, let's introduce what is positive stable. So positive stable is simply a specific example of stable variable that has many, which is a large family. Which is a large family of distributions. And this positive stable is simply a subset, which is like the support only covers the positive part of the real line and is controlled by this concentration parameter, alpha, and then also the scale parameter, delta. So if this variable is positive stable, then the Laplace transform. And the Laplace transform has a very clean, closed form, but it's sort of hard to express the fedio of this distribution. And another nice property about positive stable is that it's great or tail. The tail decay rate is in the order of alpha. And then what exponentially tailted positive variable is that this is doing is to try to It's doing is to try to slow down or increase, speed up the tail decay of the positive stable because vare tail is still sort of heavy. Once you exponentially tail the distribution, you will become less heavy-tailed. So this data will give you a low-basis mixture that is less heavy-tailed. Less heavy tail. So, as a result of this, the resulting process has different dependence structure when you vary the value of theta. So, this result is saying that when you have positive theta, which is exponentially tilted, positive stable, the process will be asymptotically independent. And then when it's exactly zero, then you have asymptotically dependent. Have asymptotically dependent process. And then you can simulate this model using a radial basis function, right? And then what we do is to pre-specify a series of knots in a spatial domain and then also specify basis function. And then you use different data values to get the Get the dependence structure that you want. However, this model has some unrealistic assumption, which is there's only one dependence class once you choose what data value to use. For example, if you choose beta value to zero, that means your process is asymptotically dependent everywhere. However, if you apply this model on a large spatial domain, On a large schedule domain like San Diego and Boston should not be asymptotically dependent. But if you choose a positive data value, then your process will be asymptotically independent everywhere. So for two points that are very close to each other, this should be asymptotically dependent in a lot of cases. So that's also not realistic. So if we want to Realistic. So we want to build a model that allows for long-range asymptotic inevenness and then the short-range asymptotic evenness at the same time. Another drawback from this model is that this alpha parameter controls both the tail decay of the marginal distribution of the spatial process and also the dependence property because once the data is exactly zero. Theta is exactly zero. Alpha controls the dependence strength of the pairwise dependence measure chi. So we want to, before we want to integrate this process in the variation modeling program, we want to modify this model so that it has more realistic extremal dependence structure. So, what we do is to separate So what we do is to separate the marginal behavior from the latent positive stable variables. What we do is to allow a different shape parameter in the white moist process to allow for the first shape variable to have a different shape parameter than the latent the latent positive stable variable. And then another modification is that Is that since this data will give you a different tail decay for the latent process? How about we let it vary across different mods, across different places, so that we get different dependence strength in different places? And then also, we allow this basis function to be compactly supported so that the effect of The defect of a certain moment, only radius to a distance. Once you reach that compact support radius, you will not affect other places. So large asymptotic independence is automatically ensured when you have a compactly supported basis function. And then also, because you are having different tail decay rates at different local places. At different local places, you get varying local dependent strength. And then, also, our white noise is better controlled because we allow the shape parameter and then the scale parameter to vary. So, a quick overview of the theoretical properties of this model is that the marginal distribution of this The marginal distribution of this process is still very go-tailed everywhere. And then it's controlled by alpha zero, which is the shape parameter of the first shape variable in the white noise process. And I didn't include the exact form of these constants that is multiplied on the parental terms. One thing to notice is that. One thing to notice is that, so I have these two sets. One is the basis weights at each location, and then I want to include the indices for the loop non-zero weights, and then also the indices for the knots where you have the zero values corresponding to asymptotic dependence. So, this is saying that if your if this location is not covered by any knots that has zero values, which means it's not covered by any local asymptotic dependent regions, then this term will become zero. Since we fixed alpha to be less than one, that means the tail decay of a location which is covered by local asymptotic dependence. by local asymptotic dependence will have a fast will have a faster tail decay. So it's lighter tail than the locations which is covered by non-zero positive stable variables. And then dependence property wise, it has also very flexible local dependence properties. And its properties. Let's focus on the two locations that are relatively close, which means that they are covered by the same basis. So the first two cases are asymptotic independent cases. The first one is saying that if both of the locations are not covered by asymptotic dependence local regions, which is in this toy example. Which is in this toy example of these two red local bases. If they're not covered by these two asymptotic-dependent regions, then they're asymptotically independent. And then the second case is saying that if one point is not covered but the other point is, they're still asymptotically independent. Sorry about that. Even though this Even though this region is covered by the local asymptotic dependence region. So in this case, we have local asymptotic independence and also dependence. And then for two locations that are covered by the local asymptotic deepenance region simultaneously, you do get local asymptotic. You do get local asymptotic dependence. So these three cases enables this NaxID model version 2 to have very flexible dependence properties, allows for local AI or AD, and also ensures long-range test and quite e-dependence. So this kind of flexible dependence property is very hard to come by. To come by, it's sort of hard to come up with a spatial model that has these flexible tail-dependencies properties. So now that we have this modified NASIE model, how can we integrate with the variational algorithm code? So, variational outlet coder is doing dimension reduction, which is exactly the same thing, to what this max. to what this max ID process is doing. It's trying to represent this process in paid dimensions using those patty-tailed exponentially tilted positive stable variables. So what we want to do is to integrate this model structure into the variational autoencoder. We want the encoded z variables to have the same property. Variables to have the same property as the exponentially tilted positive stable variables. And then we also want to incorporate the likelihood of this max ID model in the elbow so that once we optimize the elbow value, we also optimize the likelihood. And then a nice thing about this construct is that we don't have Construct is that we don't have to specify the weights. Since the weights is sort of linearly connects the process of Y and Z, we can sort of let the neural network to train the values of the weights. We don't have to specify what the weights are, where the knots are. So one thing we do have to specify is the dimensionality of the latency. The dimensionality of the latent space. So, this is a schematic diagram of what we call the extreme variation model encoder, in which both the encoder and decoder are neural networks with simply multi-layer perception. And then in the thin layers, we use as the activation function. activation function. And then here is how we calculate the algo function. We incorporate the likelihood of the max ID model and then we also want this latent z variable to have positive, exponential details, a positive stable variable distribution. And then we utilize the minute back version of Mini-batch version of the elbow and then utilize the passive brain descent to iteratively update the median beta parameters to get better and better algo values. So this is a quick review of how it works. And then we did a few simulation studies to test whether this works. Is it is it whether this works from the real data? So we simulate data from the MaxID model. So we use a uniformly distributed, uniformly sample 2,000 locations on this square, and then we simulate 100 replicas of the MaxIP process. And then when we simulate the data set, we use 25 knots. And then we use the basis of quarter two. Of order two with radius of three. And then the underlying theta values for the positive stable variables are in the center plot. So we have three zeros, which is circle. You have two in the second column, and then one in the last column. So these are the local asymptotic dependence regions, and then the rest are all asymptotically independent. Independent. And then on the right, you have a visualization of the first vertical of the simulated data. And then here are the values for the other parameters. And then we implement the extreme variation value for MR using the torch package, which is simply a sort of Sort of integrated through the PyTorch in Python. The nice thing about that is it has an auto-graph system that allows you to do a differentiation of the model parameters through back propagation. And then we simply train the extreme virus modeling filter. Various model encoder on my workstation without using GPU or any other multi-core computers. And then here is how many ad hocs it takes to converge. How we decide convergence is to compare the alpha values of the latest 100 iteration to the previous 100 iterations and then compare the absolute. Iterations and then compare the absolute difference until it reaches below the threshold. We decide whether it converges or not. And then we also compare the emulation result with the Gaussian process. So on the left, again, you have this original simulated replicate. In the center is the emulated replicate. Is the amulated replicate from the various Malbo encoder. And then on the right is simply an application of the heterostodastic adaptation process package. So what you can see is that the variational auto encoder generated a replicate that looks very similar to the original simulation. However, the emulation from the Emulation from the absolution process has the right pattern. It gets the local high values sort of right, but it didn't actually capture the high values accurately. And then when you simply do a piku plot, so what I do is to take the emulation value. I take the abilation values and then compare that to the simulated replicate and just do a peekup. I know that's not a good practice because there's spatial correlation, but this is a quick way to decide whether the emulation is doing a good job. So in the center is the preview plot from the emulation. You can see that it stays consistent from the lower tail to the upper table. From the lower tail to the upper tail. However, for the gunshot process, in the lower tail, it does a pretty good job. So since this original data set is very heavy-tailed, so this approximately corresponds to the median of the stimulated data set on the left. And then you can see that at the beginning, At the beginning, the Gaussian process is doing a good job, but once we get to the upper tail, it's underestimating the data set because of the restriction of the Daussian process. So Daussian process, Daushan normal distribution is always light-tailed. So when you have a happy-tailed data set with high dependence in certain regions, the Gaussian process will. Regions, the Daushing process will underestimate the high values. And here is the tag measure. Assumes this estimation of pair measure is estimated in the 1000s. Of time measure is estimated empirically. Even though our model has non-stationary dependence structure, which means that the time measure is not simply a function of the distance, it varies when you move a pair of locations with a similar distance across the spatial domain. You have different dependence properties. But what we wanted to do is simply assume that across. Assume that the process is stationary and try to compare the dependence strength in different distances. And on the left, you have the time measure from the pairs that are approximately 0.5 apart. And then in the center is the result from all the pairs that have From all the pairs that has approximately distance two. And then on the right, you have distance five. You can see that as distance increase, the chi measure, which is a measure of operative dependence, gradually decrease faster than the closer pairs. So that's intuitive. And also, intuitive and also what you can see that the confidence bands of the emulation almost overlaps not overlaps exactly but it stays consistent from the lower tail to the upper tail so that is very promising so you have model that captures the upper tail and the lower tail quite well simultaneously Well, simultaneously. Something happened. Yeah. Okay. So another thing that we did was to utilize different k. So the original data set was simulated using 25 positive stable variables. So what if we used a different k than the true number of positive star variable variable variables? The true number of positive single variables. So, what we did was we simply use Payne's clustering and then to decide how many clusters of high values there are, and then use that number to decide what the latent space dimension should be. And then the result is here. The ammunition is still good to the eye, and then also when you compare the And then also when you compare the QP plot, it becomes wiggly, but it's still within the confidence spans. And then again, this distribution, the distribution process is very heavy too. So here corresponds to 99 percentile of the original data set. So the distribution really concentrates from 0 to 3. 0 to 3. And then another practice that we did was to simulate fauction process and then try to see whether this very small audiencer does a good job. So you can see that the emulator result and the simulated result also looks similar. And then when you look at the pupil, When you look at the QP plot, it stays very close to the 11 line, but it overestimates a little bit when you get to the scale. But mostly, this scale stays very consistent. And we think that's still doing a very good job, considering that this is a model built for spatial extremes. And that the reason that it works is. And then the reason that it works is that we can have spatially variant dependence parameters theta. You can allow the theta to become a very large value, very positive, so that it's less heavy-tailed. So it behaves less like a nice table, but more like a Dausian process. So this has a lot of flexibility. Through controlling the value of videos, you can get a variant. Very independent structures in your annulation. And then a quick review of our application is on the REST data set, which you have seen a lot in this workshop. I just want to quickly point out the dimensionality of the original data set, which is like a very high resolution grid. Very high-resolution grid of satellite data on the sea surface temperature. And what we did was to extract a monthly maximum. So we have 16,703 locations and 372 time replicates. And then we run the VAE and try to annulate the original amount maxima from the C-surface temperature. And across the time. And across the time it does a pretty good job and it only takes like one hour for the very small length of our to convert and then and then some future work which I don't think I have time to maybe just a quick cover from that so our next step is to utilize conditional variation on the internal variants. conditional variation model encoder because when you impose conditions to the variational model encoder you can generate even more interesting samples, more interesting new content. For example, you can tell the variation model encoder to add hair colour and eyeglasses. So for example, to our case, what if we condition the input on climate conditions, whether it's rainy, whether it's sunny, or whether it's cold, or whether it's hot? It's cold, whether it's hot. And then this will allow us to do a model calibration of the input parameters within the Bayesian constant implementation constant. So this is still not done yet, but just our next stack. So this is all I have. Thank you.