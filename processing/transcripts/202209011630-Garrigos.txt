So, let me begin by joining the rest of the speakers and participants in thanking the organizers for setting this conference, and especially thank Dai and Claudia for the invitation to come and speak here. Okay, so I want to present some theoretical results about the greedy algorithms, which are more or less based on two recent works. Two recent works. One was done a few years ago and was published last year with Dirworth, Hernandez, Kuzarova, and Teryakov. And there is a more recent paper, a preprint, which you can find in my webpage. So I will mostly concentrate on the recent results, but also mention some of the old results. Okay, so I will start with a problem, which is really a problem of Fourier series. Maybe this is the reason why I. Series, maybe this is the reason why I got interested in this topic since my background comes from harmonic analysis. So, it is a problem in approximation theory, of course, and well, you can state it in this way. You have a function in Lp of the torus with a Fourier series. This is a Fourier series, and you have an integer n. You want to find the subset of the spectrum of cardinality n, which is closest possible so that the projection into this subset gives you the smallest. This subset gives you the smallest possible norm after you subtract them, right? So, um, well, this is a particular case, or say you can ask for a more general question. You ask the question in this way, you ask for the best n-term trigonometric polynomial, which approximates the function f, and then what could be a constructive procedure to produce such a best minimizer, right? Such a best minimizer, right? So, well, this can be interesting in the harmonic analysis community, right? So, one expects that some hard harmonic analysis should appear in this kind of problem. So, the only easy solution to this problem appears with p is equal to 2. When p is equal to 2, there is this trivial grid algorithm, which is you take the Fourier coefficients, you reorder by size, and you pick the largest. Okay, this bold phase. Okay, this uh bold phase G n denotes well the the choice of the first n elements uh the largest Fourier coefficients and the corresponding spectrum elements, right? So you take this choice, this norm will leave the remaining coefficients here and the L2 norm is equivalent to a little L2 norm, so that will be smallest. Okay, and the same for this problem. In this problem, you fix the coefficients in this, you allow to move the coefficients and move the spectrum. So this is. Multi-spectrum. So, this is the only case in which the question is trivial. But when p is equal to 2 is different than 2, then this is widely open. I mean, I don't know any constructive procedure which gives you the best result or near best result. So, one of the reasons, I mean, the first attempt that you could try if you want to understand this problem for P different and 2 is to apply this same algorithm in L P, but then it is known that this is not going to be good. Okay, so this is actually this, you consider. So this is actually this. You consider the infinite series, this is the rearranged Fourier series according to the size of the coefficients, and it is a known result that there are L P functions for which this rearranged Fourier series does not converge in the L P norm. And this was obtained independently by people in harmonic analysis, Cordoba and one of his PhD students, Fernando de Gallardo, and by Tenly Akov in the late 90s. There is also another result which is stronger than this. Is stronger than this, and what's recently obtained by Timur Reutbert that says that if you take any increasing sequence going to infinity, then not only this series is not going to converge in the L P node, but any subsequence, I mean, you pick a fixed subsequence, you will always find a function in L P so that the series does not converge. For instance, you take 2 to the n, I mean, you may think that maybe you take a lacinary series, maybe this is going to converge here, but this is not the case. There are always functions. But this is not the case. There are always functions which avoid this. And somehow, the reason what is behind these counter-examples is the fact that the LP-nor can be described with Fourier coefficients by this Lidelbut-Peley expression, but this expression is not an unconditional thing. It doesn't depend on the absolute value. So this greedy algorithm, which picks the largest, can be cheated, and then you can have a function that the greedy algorithm doesn't see. And then when you apply greedy algorithms, Don't see. And then, when you apply a grid algorithm, this function has a very large knob. For instance, you look at one of these blocks, you have many consecutive elements in the spectrum, and you take all the Fourier coefficients equal to one. That block is essentially going to be a Dirigler kernel with all the coefficients equal to one. The Dirigler kernel has a small L P norm if P is close to one. The L P norm of the Dirigler kernel is n to the one minus one over P. But you delete some or you change the signs randomly in a Dirty. The signs randomly in a DRA kernel, then the norm of this thing will be closer to square root of n, which can be very large. Okay, so change the signs or removing some of the coefficients is more or less the same, and this is what the 3D algorithm does. You can cheat the 3D algorithm, and by doing these projections, you can start with something with a small norm and construct something with a big norm. Okay, this is more or less what is behind this. And this is more or less well known. And then the question that one can ask is: is there any Is there some algorithm which is more natural than this one in order to approximate trigonometric functions with n-term trigonometric polynomials? So, okay, there is this nice result of Telyakov. I was very surprised when I saw this theorem for the first time some years ago, because, well, he found a constructive procedure, which is not so intuitive and natural as the previous one, but it actually, with this script GN, with this procedure. To this procedure, you iterate these procedures n log n times, then you are able to beat the best error of n-term approximation. Okay, you would like to have n iterations and beat it. You cannot do that, but with this, you are very close, n log n iterations in LP for p larger than 2. So when I saw this result, I thought, well, I have to understand how this algorithm works because this can be something interesting. The algorithm also gives results for p smaller than 2. For p is smaller than two, but the results are not that good because then you need more than n log n, you need a power of n log n. Okay, there is this symbol here means that there is a log. So, okay, if p is close to one, p prime is close to infinity, and you may need like n to the 100 iterations for approximation. But for n bigger than two, it is a very good thing, right? So, one would like to understand why, what is happening here. So, what is this algorithm? Well, this is the weak chevy check greedy algorithm, which is the Weak Chevy Chevridi algorithm, which is the title of the talk, and I will describe this in detail in the next slides. So, this doesn't come from nowhere. Actually, the motivation of the paper where this is proved was not to solve this problem, was to study properties of this algorithm. And this algorithm comes actually from the signal processing community and from other applications where in the Hilbert space situation is called the orthogonal matching pursuit. Okay, so there is an extension of this algorithm which was already popular in applications to the Banach setting. In applications to the Banach setting, which Ateliakov used to prove this result. So, okay, my goal at the moment was to try to have a better understanding of the algorithm and this theorem and see what one can do better in some cells. There are many open questions about this too. Okay, so okay, here I live the trigonometric system and I ask this in a general framework. So I will be working in a Banach space. Now, D is not necessarily a basis, it can be a dictionary. A basis, it can be a dictionary, which is essentially any collection of vectors that spans the full space. And if it doesn't span the full space, you just take the subspace generated by the vectors. And then you put some conditions that they are normalized, but this is not necessary. I mean, you can put semi-normalized, like the norms can be between two positive constants, and that will be okay for the results that I will mention here. The Banach space can be real or complex, and the results will be also true. So, this is the usual. True. So, this is the usual set of n-term polynomials, okay, linear combinations of n elements of the dictionary. So, this set lambda has cardinality n, and there are some coefficients, some scalars, which can be real or complex. And little sigma n is the usual best n-term approximation, which is the distance of the function f to this set of n-term polynomials, right? So the abstract question here is to find the reasonable algorithms when you have such thing, you are given a banana. You have such a thing, you are given a bank space in a dictionary, find reasonable algorithms so that the error of approximation is close to the best error. And then you can quantify this closeness in various ways. And the one I want to use here, the quantification I want to use, is this. So I want to find the smallest number of iterations so that when I apply the algorithm pn times, I can beat the best error of n-term approximation, except perhaps for a constant which is fixed. Except perhaps for a constant which is fixed. Okay, you can see C equal to 2 or something similar. So these are this, I call them Lebesgue-type inequalities. This is what we would like to understand for this specific algorithm. Of course, it could happen in some situations that there is no such fee. Like the pure Greedy algorithm I put at the beginning, the one of reordering the Fourier coefficients by size, you apply in LP by this result of. In Lp, by this result of Eugbert, this contra example, this may not go to zero for some functions. And this goes to zero when n goes to infinity because these functions are tense, right? So it may happen for some algorithms that you don't have such a fee. But the interesting thing is that for this orthonormal matching pulse to it, it is. They exist. So before describing the Chevy Chev grid algorithm, I consider the Hilbert situation, which is the one which is well known. I mean, probably, yeah, many people. I mean, probably many people in the audience know about this. So now X is a Hilbert space. This is a dictionary in the Hilbert space. Tau is a number, it's more than one. And how is this orthogonal matching pursuit working? So you start with a function f in H. And in the first step, when n is equal to one, what you're going to do is the inner product of f with all the dictionary elements. And you're going to pick the largest inner product because you think somehow that the largest inner product is going to give you the most relevant element in the dictionary. The most relevant element in the dictionary compared to the function, the one which is closest in some cells. So you pick that element and you call it phi j1. Okay, this is the first choice of the spectrum that you are going to take performing this. It doesn't have to be necessarily the largest. You have this tau here. If tau is one half, well, you can pick anything which is bigger than one half the swoop. Okay? Then once you have this element, you make an orthogonal projection into the space generated by phij1, and that gives you. By phi j1, and that gives you a coefficient. Okay, and then you this is going to be your approximant, j1 of f, and then this is the remainder, and then you repeat the process to with the remainder. Now, you take f1 here, which is the remainder, you put f1 against all the dictionary elements, you pick the largest, which will be different from the previous one because that inner product will be zero, and then from that largest now you project in phij1 and phij2 to get two new coefficients. To get two new coefficients. Okay, and remainder, and so on, and so forth. Okay, this is the process. The coefficients in this algorithm I'm using here change in each step. That's right. So it's more costly, but it's more efficient for the kind of theorems that one tries to prove. So this kind of algorithm was, well, appeared, I think, in the community of statistic regression. And some people consider without changing the coefficient, which is a variation also. Changing the coefficient, which there is a variation also. They prove the L2 convergence. So you have a function in the QR space and any dictionary, and this algorithm is always going to converge. No matter what the dictionary is, can be very, very general, but it's always going to converge. So there are some extensions, and this orthogonal version was introduced by Malad and some co-authors in a paper in the 90s. And this version with the tau, which is called the weak orthogonal matching pulse, it was introduced by Volodya Teleakov in a year. The Atelia Coffee in a year 2000. Okay, so they all proved these convergent results. And now there was a problem here, which, well, the complete solution was still to be understood, which is the optimal recovery. So what happens if I have a function f, not in H, but an entrant polynomial? So should I recover after n iterations the same function? So it turns out that this is not the case. You can construct dictionaries for which n iterations are not enough to recover f. Iterations are not enough to recover F. But then the question is: how many iterations do I need if the dictionary is reasonable? Okay, if it's not reasonable, you may need too many. But if it's reasonable, how many iterations I need? So, okay, there were some results in the literature about this, putting conditions in the dictionary based on coherence and so on. But the most important result, which seems to be a key in the theory, is this theorem of Tong Zhang from 2011. Okay, this is a very short paper, seven pages, but it's very Short paper, seven pages, but it's very deep. I mean, it's hard to understand what is written here, but it's all true. So he proved that under restricted isometry property. So if the dictionary satisfies the restricted isometric property, which, well, maybe everybody knows, but it means that when you take S sparse signals, which means S term polynomials or linear combinations of S elements in the dictionary, the norm is comparable to a little L2 norm. Okay, so if this property is true and there So, if this property is true, and there are many procedures to construct the dictionaries with this property, with the read property, then automatically Cn iterations are enough to recover all the signals even in a stable way. Okay, so for every F in H, if it's not exactly sigma n, but it's not far because this error is smaller than epsilon, then the recovery after C and operations has an error also smaller than C epsilon. Okay, and this will be true for every n, not for every n up to infinity, but for every n up to s divided by. But for every n up to s divided by some constant, okay, depending on the on the RIP condition. So, this is a key result. And even in a recent conference in Arkashan, there was Foucault giving a talk and he was asking to find a simpler proof of this result because this is very, very evolved. Okay, so I think it's an interesting result to understand. There are a few alternative proofs to the one of Tongjiang, one is by Coen and another by Foucault, but the one I find more interesting and more powerful is by Telyakov. Okay, so this is the one I. Teleakov. Okay, so this is the one I will study here. Okay, so now we move out of the Hilbert space situation and we go to Banach spaces. So I need to define this algorithm in Banach spaces. So I need something to replace the inner products and something to replace this orthogonal projection. So I need some structure in the Banach space. So the structure I'm going to use is first is the smoothness of the norm. So the norm is smooth, is this limit basically this incremental quotient limit? Basically, this incremental quotient limit exists for every x and every y in the unit sphere. So, if this is true, if the norm is smooth, then you can define in a unique way the norming functionals. Okay, these norming functionals are going to replace my inner product in the Banach space situation. So I tell you what they are. So the norming functional is an element from the dual of the Banach space x with norm one. And so that when you apply to x, you reach the norm. Okay, the norm one is reached in the point. Okay, the norm one is reached in the point x. Then there is a figure here which I to try to illustrate this graphically. So suppose this is the ball of your Banach space. It doesn't have to be round. It's a Banach space. And this is an element x, which is in the sphere, in the boundary of the ball. Then this norming functional is going to be, well, the kernel of the norming functional is going to be a hyperplane. And then you add x, this is going to be a hyperplane, which is going to be more. Be a hyperplane, which is going to be more or less tangent to the ball. It's tangent because it cannot touch the interior of the ball, because otherwise, the norm will be smaller than one. Okay, so it touches the ball maybe at one point or at more points if the ball is not strictly convex, if it has some segments. But the thing here is that if the Banach space is smooth, this is unique. If the Banach space is not smooth, like in the R1 norm, which you have a corner, then you may have many of these. Corner, then you may have many of these hyperplanes and many of these functionals. Okay, so the way the algorithm is going to work is that I'm going to take the dictionary elements, suppose the dictionary elements have norm one and are these three, and I'm going to project the dictionary elements with this functional here. Okay, so these projections are going to be parallel hyperplanes, and then this distance here is going to be the projection of phi2, which is a relatively large number. The projection of phi 3 is going to be 0. The projection of phi 1. Of phi three is going to be zero, the projection of phi one is even larger. Okay, so the greedy algorithm is going to pick the largest projection with this, so it's going to pick phi one in the first place. Okay, so this is how it's going is going to work more or less. Okay, so we've one later I will put examples of explicitly what are these functions in some examples of parameter spaces, but this is what I will need. Right, so by the way, this functional is also exactly, and this is one of the reasons for the unit. Exactly, and this is one of the reasons for the uniqueness, is exactly the derivative, the derivative, the catoderivative of the norm in the direction of y at the point x. Okay, then I need something to replace the orthogonal projection. The good thing here is that I only work with finite dimensional spaces, which are the elements that I pick from the spectrum. So when I have a finite dimensional space, I can always find a best minimizer of the distance from x to the space, because it's a finite dimensional problem. Space because it's a finite-dimensional problem. So, this projection that chooses one of the minimizers, I will call it a Cherisha projection. Okay, and they always exist, and hopefully, one can compute them in practice as well. So, this minimizer may not be unique, but it is unique if the norm is strictly convex. Okay, if the ball doesn't have segments, then you can make a drawing and more or less convince yourself that this is going to be unique. So, these are the tools, and here is the definition. So, this definition is due to Tenyakov. So, this definition is due to Tenyakov, and this is the generalization of the matching pulseuite algorithm to Banach spaces. So, now you have again a tau, you have a vector in x, and in the first step, you do what I explained before. You project the fx, the normal function of x, against all the elements of the dictionary, and you pick the largest, and this is phi j1, or something which is bigger, one half of the largest. Then you project, and once you have picked, say, n of these elements, you project with this project. Elements you project with this Chevich projection and you get the n-term approximant, and you define the residual as the difference and you apply again. Okay, so it's a similar strategy, but this is these are the tools that you need to use. So what Telyakov proved in his, well, he proved various things in this paper, besides giving the correct definition, is that if you have a little bit more structural smoothness, you have uniform smoothness, which means that this limit exists uniformly for all x and y in the sphere, then actually the algorithm converges. Then actually, the algorithm converges, converges for every X in the Vanach space and for every dictionary. Okay, so it's a good algorithm in that sense. And he proved also rates of convergence for certain subclasses of X, etc. Okay, there are several results related to this. But okay, so the theory was more or less about this algorithm, was more or less stuck here. There was a few papers afterwards, but not many more results. And then I will mention the result about the Lebesgue inequality. I'm interested. So let me first give the examples. So, let me first give the examples. I forgot of how this function is defined. So, we try to compare with the Fourier integral problem, the Fourier series problem I mentioned before. So, suppose x is an element in the Banach space with norm one. And suppose the Banach space is this big L P. For P is truly between one and infinity, this is going to be uniformly smooth and uniformly convex. And then the functional is just the inner product of f against this function. So, when p is equal to two, it's just the inner product. And then when p is not two, you have this nonlinear. And then, when p is not 2, you have this non-linear factor. So, what do you do? You want to apply this to the Fourier series? Well, you would compute the Fourier coefficients of this function and pick the largest. Okay, and that is going to do the algorithm. Then you do the Chevysha projection, then you do the remainder, and you apply the same to the remainder. This is the way that you are going to construct this spectrum set using this idea. But now, notice one thing: if I consider instead of big L P, I consider little L P, and I consider here. And I consider here the canonical basis, en, the canonical basis. So I have to take this inner product of the canonical basis with a vector x in little lp. So I get xn to the p minus one. Okay, I'm taking mobiles here. So if I had to pick the largest thing, I'm actually picking the largest element. So in little lp with the canonical basis, the Jechevichev algorithm is exactly the same as the usual greedy algorithm. It picks the largest elements. Okay, they coincide here a little. They coincide here, and little two in particular. They are the same thing. Okay, I will use this observation later. Okay, so let me now try to state the results, the result that Tenyakov obtained about the Lebesgue type inequalities for this algorithm. Okay, how to find this phi? So I need three properties. One property is a quantification of the smoothness. So I need a little bit more than uniform smoothness. I need to quantify the smoothness with some number. some with some number and that number is the power type so the power type of a norm i mean a norm is said to be smooth of power type s basically there are very equivalent there are various equivalent definitions but basically when you expand the norm in Taylor series so you have here for t equal to zero this is the derivative and this is the error term if the error term decays like t to the s okay for some s bigger than one um if this is done uniformly in x and y there is an equivalent version using what is An equivalent version using what is called the modulus of smoothness. Maybe people in barrack space here are more familiar with this, but they are equivalent. Okay, the modulus of smoothness is this two-difference operator, and then this has to be smaller than t to the s. Okay, this is the first property I need. In the case of a Banach space, this s is equal to 2. Okay, and this is something that you more or less expect in a Banach space, and actually in a Hilbert space, sorry, S is equal to 2. In a Hilbert space, sorry, s is equal to two, and you cannot find any Banach space with a power type larger than two. Okay, this is the best power type that you can get in Banach spaces. And this is a constraint, and this constraint will give a problem later in the theorem. Now I need to replace the RIP conditions, the restrictive isometric property conditions. So I need two properties, more or less, it's like the upper and the lower bound of the RIP condition. This one is the easiest. And this is saying that if I take an element, That if I take an element, an n-term polynomial, you know, an element like this, a combination of n elements of the dictionary, I can enlarge the set which is inside the set A by a set B with more numbers, okay, which is a larger set, and then paint with a constant K. Okay, so this is like an unconditionality property. This constant K can depend on N, and in the case of the RIP condition, this is easily verified because suppose you are in a Hilar space and you have the Suppose you are in a Hilar space and you have the RIP condition. This is bounded by the little two norm of A, summing over A coefficients. You can enlarge that to summing to B coefficients and the little two norm using the RIP property, you can make it here, even with a uniform number. Okay, so instead of passing through the RIP condition, I will state it directly like this. And for many examples of dictionary, especially for basis, this can be computed. This Kn at most is of the same. Kn at most is of the size n, okay, in general, it is not more than that. But it is a problem. This will appear in the number of iterations, multiplying some other n's. And the important thing of this theorem is to reduce the role played by this conditionality cost. Then I need something else. I need to replace somehow the lower bound of the rib condition. So instead of starting with a little l2 here, I start with little one. So suppose I apply here the Cauchiswaldz inequality. Here, the Cauchichalt inequality, I will get the Liberal L2 norm of this times the cardinality of the set to the one-half. Okay, then from the Lido L2 norm, you have the rib you can get to here. So, this number, so I will say that this, the enterpolynam satisfy the property A3 with a constant V and power R when the power that I put here in the cardinality is R. If I have a Hilbert space with a reproperty, R is one half. Okay? And in general, I will show you in examples what. I will show you in examples what these are, what is the form of these are, which for explicit basis usually can be computed in a nice way. Okay, so here's the theorem. The theorem of Telyakov, I mean, the general theorem that he proved, for which you can derive the result of trigonometric polynomials, is this. If these three properties hold, if you have a dictionary with these three properties, then you have this Lebesgue inequality with a number of iterations phi, which is a power, is n raised to the power r. is n raised to the power r times s prime times this constant. Okay, so if you are in a Hilbert space, s is two, r is one half and you have n and then you recover the result of tongue jung. But if you are in a banach space then depending on the geometry, depending on if you would like this as large as possible, so it's prime is small, and you would like this R as small as possible, then if the geometry is good enough, you have also any iterations. Okay, if not, you will have to pay with more. Not you will have to pay with more. Then, this constant appears also here. And then, the important thing of this proof: this is a deep proof, this is more general than the tension theorem: is that you can be an inductive process, a very smart inductive process, you can reduce the role of the constant kn to a log of kn. So it plays a very minor role here. And this is the main contribution probably in this theorem. All right. Well, I am interested in applying this not to general dictionary, but to. In applying this, not to general dictionary but to bases. So, in general, I will ask for these conditions for all A's and B's. But if you only know the conditions, like for D sparse elements, for D sparse B's, okay, then you can put a D here and this theorem will be true not for every N, but for N's which are controlled by this D number. Okay, but I will not really use that, but this is also true in this generality, and this is the statement which really generalizes the tangent theorem. We're putting this this. All right, so let me now tell you how to find this this A3 property. So if you have a basis, you have a basis, this is not hard to do. I mean, it's actually quite simple. So what you want to do is you are summing these coefficients and you want to compare this with something. So you take the dual basis and then you apply this thing to the vector that you want on the right-hand side. So in this A3 property, you maybe. So, in this A3 property, you maybe will not remember, but what you have to obtain on the right-hand side was the same vector as before with the same coefficient, but maybe a larger set B. But if you do this and you put here the signs, so this epsilon n are the signs of this a n, when you apply this by the biorthogonality, this applied to this gives you the absolute value and nothing else appears, only the coefficients. So, you can put an absolute value here because everything is positive, and this is smaller than the. And this is smaller than the norm of this guy times what you wanted to have on the right-hand side, okay? Which is the projection in the larger set B of the original coefficients, right? So actually, what controls, what you would like to have here in order to have this A3 property, is some control of this thing. And this thing is sometimes called the fundamental function of the basis. Okay, this is the dual basis, and this is the fundamental function of the basis, which is the super. Function of the basis, which is the supremum of the norm of n sums of the basis. These coefficients are just one or minus one, or they are in a complex case, so they can be controlled basically by that. Okay, so if you know that you know how to compute the fundamental function of the dual basis, and this is like a power type, then you can use the theorem of teleacom. You have all the all the powers. So, here are some examples. So, in the trigonometrics. In the trigonometric system in LP, this is a conditional basis. The conditionality constant is at most a power of n. In this case, it can be proved that it's n to this power, but it doesn't matter because I will take a log, so it doesn't matter what I put here. The h n, so now suppose you are in the trigonometric system in L P, and then you want to compute the L P norm of n elements and n trigonometric monomials. So there are two extreme situations. When they are in arithmetic progression, it's like a Dirichlet kernel. And this thing, And this thing, the L P norm of a Dirty La Kernel, is n to the one over P prime. Because you are looking in the dual space, it's like n to the one over P in this situation. And then the other extreme situation is when they are in geometric progression, then you have like a lacunary series, and then, well, you can prove that in that case, the L P norm is like a square root of n. So these are the two cases, and these are actually the worst possible situations that appear in the soup. So this A3 property holds with this power R, which is the maximum of 1P and 1. Which is the maximum of 1p and 1, 2. And then the smoothness of the LP spaces is well known, it was studied a long time ago. And then for p larger than 2, the balls are very smooth. And then you can put this error in the Taylor series with t squared. And with P is smaller, P is between 1 and 2, the error is just T to the P, okay, and cannot be improved. So if you plug those numbers in the theorem of Telyakov, you have to multiply this by this, then what you are going to get, well, this by the prime of this. Well, this by the prime of this when p is larger than two, you get two here, and you get two here, and you get power one times the log, which appears because of the conditionality. So n log n iterations for p larger than two recover the best and term error of approximation. And then in the other situation, you get np prime minus one, and this is the reason, okay, why you get it. So I give you another example, the hard system in LP or in. The hard system in Lp, or in general, you take any wavelet basis which is unconditional in Lp, so unconditionality gives the constant kn approximately one. Then, in the case of the hard system, if you take n elements of the hard system in the L P norm, this is like a little L P norm. So you are going to have n to the one over P, and then because this X star, n to the one over P prime, and then you have here the numbers, and you have here the modulus of smoothness because you are in O P. And then you plug it, and you get for P is more than two, just Get for P is more than two, just n iterations are sufficient to recover this. So this is basically optimal. And for P bigger than two, you plug this number, you get more. Okay? So something, we don't know if this number is optimal. We don't know if maybe a better proof can give something better here. And then let me put a third example, which is just a simple variation of the previous. Suppose the canonical basis is illegal L P, so it will be the same, it's unconditional, the Kn is just equal to one. The kn is just equal to one, h n is just equal to n to the one over p prime, so you are going to get the same things. But then this is telling you already that the theorem of Teleakov is not sharp. Because I said before that for the canonical basis, little L P, this algorithm coincides with the usual grid algorithm, which in n iterations recovers things. But here it's saying that you need n iterations for p smaller than two, but a power for p larger than two. So something is not optimal. To so something is not optimal in that proof of that theorem, right? So, this is this was the starting of the project. Um, what you have to change in the proof in order to optimize this, okay? So, so let me try to explain this. Actually, what is important here, and you will see now in a short video, I'm going to show, is that you are going to take the derivatives, the directional derivatives of the norm, only in the Of the norm only in the directions of the basis and the directions of the dictionary, not in all possible directions, which is what the modulus of smoothness does. So maybe if you are going in these directions, you can get a better power here. Okay, and this is the crucial point. So let me show some heuristic about this. So suppose you are given an element x and you want to find the closest phi from the dictionary. So you want to find phi so that the distance of x to the space spanned by phi is the smallest possible. Okay? Small as possible. Okay, so what will you do? Okay, the distance of this is the minimum between x and minus t phi. Then you expand using this Taylor series. So this is going to be smaller than x minus t times the derivative plus the error, which is controlled by t to the s. So if this t is a complex number, you can take a direction here and convert the real part, which is what appears here, into a modulus. Okay, and suppose that t is positive. Okay, and suppose that t is positive, so this is modest, and then you have to minimize in t positive, and then this is very easy to minimize because you take the derivative equal to zero, you solve this, and you get that. So you get that this is the smallest possible when this functional, this norming functional applied to phi is largest. When this is largest, then you will be in the with the smallest distance. So this is more or less the idea, but you realize that only the interesting directions are only considered when you apply them to phi. Consider when you apply them to phi. Okay, so maybe I can improve this thing here. So I'm going to define a new property. Okay, this is a theoretical thing, but maybe Banach space theory can make sense. I don't know if it has been studied before, but I would say that the norm and the dictionary satisfy the property dq for a function qt if the distance can be bounded as I did here. Okay, the distance of x to phi is one, assume that the x has norm one, one minus phi of x. one one minus phi of x applied to this function q okay typically this q will be a power okay sometimes we will be able to prove this for phi's in the dictionary with a better power than the one you obtain in there from the smoothness and these are two examples from the first paper if you take little of p with the canonical basis then you can get a better power you can prove this inequality with q t equal to t to the p prime so when p The p prime. So when p is larger than two, you don't have a t square here, you can have something better. Okay, and this is important. Okay, you use the modulus, you will get less. And the same will happen here if you use mixed norm spaces. This is interesting. If you think about wavelets in Bias of spaces, they are characterized with mixed norms. And then in this case, the Qt that you get is the maximum of P prime and Q prime. And there is no two. There is no maximum of this with two, which is what you would get if you use the modulus. Which is what you will get if you use the modules of smoothness. Okay, so somehow this is the property that is natural for this at least to consider these cases that were missing in the previous theorem. Let me see. Okay. So this is the property dq. And then I would like to have a description of the property in terms of things which are better known. And then there is this curious description, which I didn't notice, but now, okay, it's very elementary. Notice, but now, okay, it's very elementary actually. It turns out that the DQ property always holds. You put here the modulus of convexity of x star. Okay, and this is not surprising because, you know, when you had t to the s as modulus of smoothness, you had here t to the s prime. So somehow you would expect some sort of duality. But this is actually quite quite natural because somehow what you are doing here when you are taking this infimum, this is t and this is rho. This is the modulus of smoothness. Okay, so you take this infimum. Okay, so you take this infimum and you forget about the sign, you are taking the supremum of this minus this. So you take the supremum when t is positive of t times fx minus rho, rho of phi, rho, sorry, of t. This is more or less a Legendre transform, and the Legendre transform is going to give you something like this. Okay, and basically, it is not true in general that the Lagen transfer of the modulus of smoothness is going to be the modulus of convexity, but it's comparable. Let me say, it's comparable in some sense to the modulus of convexity on the dual. Okay, so actually, you don't need to do the minimization property. This is always going to be true. You can forget about the modules of smoothness and have directly here a modulus of convexity. So, in that case, I can state a theorem. So in that case, I can state a theorem which is slightly more general than the theorem of Telyakov. And what I'm interested in, this theorem is that it doesn't depend on the fact that these functions here are powers. Okay, the theorem of Telyakov depended very much that the functions were exactly powers to obtain his result. But now I can do it with no powers. So I have a dictionary with property dq, whatever dq is, for instance, this one, and then the number of iterations, there is this log of kn, which is the same, and one over q applied to one over h. Q applied to one over H. If you have powers, you have the power in Q and the power in H multiplying, and that's what you get. But in many examples, you actually have the H, which is not a power, there is a log factor, and the Q, which is not a power, and there is a log factor. And in those cases, the theorem of Feliacov will lose some epsilon, some epsilon power, and you will not get the sharp result. Here, you can get the sharp result if you do it in this way. Okay, so and then I claim that this is computable in certain cases. So, let me show some cases. So let me show some cases. And then the cases I was interested in especially was Lp log L raised to the power alpha. So I'm not defining these spaces, but these are the usual Lorentz sigma spaces. And well, for these spaces, it was studied from people in Banach space theory, like Maliv and Trojansky in the 70s, how to compute the modulus of convexity. So the modulus of convexity for P larger than 2 is T squared, like for the L P space, and then for P is more than 2 is something. For p is more than two is something different because you have t to the p prime and then the alpha appears somewhere. Okay, so you have to take this into account now and you can take into account this because of this new formulation of the theorem. So suppose you have the hard system, then we also know that this unconditional basis here, it ps is between one and infinity, and we know how to compute the age. Okay, well, this is not trivial, but we did this a long time ago, I mean, some years ago with Johann Hernandez and Martel, how to compute the fundamental functions. How to compute the fundamental functions using wavelets for these Lorentz spaces or Lix spaces. And it turns out to be n to the one over p prime, which is what you would get for p, times some log which also appears here. Okay, and you cannot get rid of this log. So you plug all these numbers in the new statement of the theorem, what you are going to obtain is that for the hard system in L P the log L to the alpha, if P is smaller than two, then the number of iterations that you need is N times this. It is n times this. So it's linear in n, and then there's a power here. And the nice thing is that this result is optimal. So I can construct an example of a function f, which exactly needs this number of iteration, modulo constants, in order to achieve this. Okay, so somehow the formulation in terms of these functions, q and h, is correct, right? It gives you a sharp result in this case, which is not necessarily trivial. If you have p larger than 2, there is a similar phenomenon as before. There is a similar phenomenon as before. You can also plug it in the theorem, but you will have this uncomfortable power n to the two over p prime. Because here, I don't know how to improve. I don't know if I restrict to these directions of the dictionary to get here t to the p prime. I don't know. I get the t to the two, so I'm lost with these two and I lose a power. And I don't know if this power is optimal, even for empty spaces. I mean, and in particular, for this case, you have an extra log. Okay, so in this case, there is this open question, which seems elementary, even for the hard system. Seems elementary even for the hard system, but is it optimal? I mean, we don't know about that. In this statement, the constants may depend on p and may depend on alpha. Okay? And in fact, when p gets close to one, this will blow up, probably like p minus one over p minus one, because this is the constant that will appear here. Okay, I didn't write you have a p there, but you have p minus one in front of something. But you have p minus one in front or something, and that you substitute and it will appear. Yeah, you can trace, you can trace those constants, yeah. And for instance, you can here you can put the two or something as close to one as you want. If it gets close to one, you pay with something here, okay? But it's always possible to do. I have three minutes, I think. So let me show the second example, which is the trigonometric system. So for the trigonometric system, one can also compute the H and the K. The h and the k in the case of these or leak spaces. The k is similar to the one before. We don't really care, it's a power times a log, but we don't really care exactly because we take the log of k n and this will be relevant if there is a log here or not. But the age, the age, when you compute it, you notice this thing. So you can have the maximum which is square root of n, which is what appears when you have this geometric series, these Lacinaris sequences. But then you have something here, which is basically the norm of the daily. Which is basically the norm of the D-Left kernel. And when alpha is positive, this is smaller than n to the one over p. So you gain something. And this is something that you gain, you would like to see reflected in the number of iterations. Okay, and this is one of, yeah, with the previous theorem, you wouldn't, right? So this is what you get when you apply the theorem in this case, n log n iterations for p larger than two. And if you are in p equal to two and alpha positive, then this is very nice because you have this. Because you have this divided by log n. Well, no, actually, you have this, okay, but you have some gain appearing from the modulus of convexity, then you get n log-log n iterations. So just basically n iterations is enough for this space, which is close to L2, but you would like to know how it approaches. Well, if p is smaller than 2, then you have the same problem as for the LP spaces. The power is p prime minus 1 times something. So we don't know here the optimality of this power. Optimality of this power, this power and will be optimal, but we don't know the optimality of the logs either. So, here it's very difficult to check the optimality because it's difficult to test this algorithm when the basis is not unconditional to test this own function explicitly. So, I still don't know how to do that. So, this is the result. So, there is a third example. Maybe, okay, if I have a minute, I can mention that. But this is from the other paper, from the earlier paper. I mentioned that before. I mentioned that before. This is the mixed norm space. So, in the mixed norm space, the mixed norm space, you first take the L P norm, this is double sequences, and then you take the LQ norm. And then in this case, the nice thing of this mixed norm space is that the functionals are explicit. You apply this to the canonical basis. And then what you take is the largest coefficient, but with this weight in front. Okay, so it's not exactly the greedy algorithm. It's a different algorithm, but this is how you choose things. things and then the DQ property holds with this maximum of P prime Q prime and there is no two and then the HM property you can compute and is this and then when you substitute you realize that the number of iterations that you need is the maximum of P prime Q prime and Q prime P prime. The good thing of this is that it gives you a better result than the usual greedy algorithm, which is not optimal for the mixed spaces. So in this region here, in this region here, which is P smaller than QP. Which is p smaller than q prime, the chevichev algorithm gives better, gives a number of iterations which is better. And here, the usual grid algorithm gives a better thing. Okay, and these are optimal. Okay, this number of iterations here is optimal. It cannot be improved in this case also. Okay, so it's another example to do this. So, okay, I think this is this is all. Thank you very much. Yeah. But you left your answer, then we will see. No, no, I would be involved. So this is about uniform smoothness with power type. Can you go back to this theorem of Temlyakov with three conditions? Yeah, okay. So no, no, no, no, no, no. No, no, yeah, no, yeah, no, this this one, yeah, yeah. Okay, so so I don't know if you are aware of that, there is a theorem of Gilles Pizier, which says that any space of any uniformly smooth space or uniform smooth norm, there is you can pass to an equivalent norm which is which has smoothness. The same thing also with uniform context, but here it's for uniformly smooth. But here it's for uniformly smooth, so you don't really need the first condition, you just need that it is uniformly convex, then you pass to an equivalent norm in which it is uniformly smooth. You just need that it is uniformly smooth, then you pass to an equivalent norm which is uniformly smooth of power type, and then you apply this. Of course, there is a constant which is not clear what it is, but it does not depend on n. This happened depends very much on the norm because the function of the fact is the norm. So you reach the normal So you reach the norm at the beginning and you want to use that functional that normally. You see with the same function, with the same to use that norm. So you cannot pass to an equivalent norm. I see. You know explicitly how this renorming is. No, that's this is complicated. It's not a it's not it's very it's defined in a very strange way for the LP spaces if you have a little bit characterization in the Little boot failing characterization in the case of power basis. I wouldn't use the norm of the LP, I would use the norm of the LittleBook free characterization, which is not simpler. And you can construct the function explicitly. Okay. But you need to know. I understand. You need to know something. Thank you. No, no, thank you for the question. It's clarifying actually. Any other questions for Ostava? Any questions on Zoom, maybe? Dmitry Gorbachev, I have a question. Dmitry, hello, tell me. Hello. Dimitri, hello, tell me. Hello. Hello, Gustava. I have missed. Why do you call Cheboshev? Why I call Chevyshev? Because the year I don't know. Yes. Okay. I call because Volodia told me to call, but I will tell you why, okay? And then you can argue with him. This is even the correct spelling. All right. So let me see. So I'm taking here. So now you see, I'm taking here a substitute of the orthogonal projection in the case of Banach spaces, and it's the best approximation of x from a finite-dimensional subspace. But this norm corresponds to infinity, p is equal to infinity or not. Okay, no, this is the norm of the space. So you take x equal to L infinity, it will be the L infinity norm, but if you take any other space, it will be. So what, what? It would be so. What I understood from what Bologna told me is that this, somehow, this thing that is the best approximant, he put this name, he baptized the algorithm because it's the best one. Because I know Cheboshev investigated the case of P is equal to infinity, probably two and one. That's all. Right, right, right. But he was interested in optimal, not in sub-optimal. He was interested in optimal, not in suboptimal, right? He was the best approximation battery animatic polynomial or something, right? So I think this is the relation. Thank you. The closest point to the convex set. I think there is a premise open problem in Binax space theory. Well, this is a convex set because it's a subspace, or maybe it's very strict. The problem is the following. If you have a assume you have a Banach space with the following property for So assume you have a Banach space with the following property for every convex set or strictly convex set, every point in the space has the unique best approximant to the in this in this convex set. Then the space is Hilbert. That's interesting. Yeah, and I think that's open. Yeah, I and I think that's open. I heard I heard this well, okay, I heard this from Ladimir Kadi probably 15 years ago or more when but back then it was open. Yeah, I think here it is unique, the bonus is typical. So, this may be simple because this may be a reason why how it's related to Chebyshev, because I think it's also Chebyshev's conjecture or something, or somehow related to Chebyshev's conjecture. Yes, yeah. Yes, it is. Okay, I'm sorry. Yes, any other questions? No, if not, let's thank Gustavo again. And we need to start the next talk extension.