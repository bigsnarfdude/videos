Neil Adam from Galway. And I want to say thanks to the organisers for, it's been a wonderful week, a wonderful workshop, and a particular shout-out to Jennifer Stana and for all her efforts in organising this. This is a talk about linear solvers for single-product problems. It's based on a paper that appeared last year in SIMAX and joint work with Einstein Jan from Santa Clara and Scott. Clara and Scotland Coughlin at one. I should have mentioned, by the way, that I'm really from Galway, but temporarily, officially for six months, I'm at Memorial University and I'll be there from tomorrow. Simon happens to be in St. John's in the next few months. Give me a shout. Now, so here's the outline of the talk. We're interested in developing preconditioners for convection fusion problems. Now, that's something that's been going on for a long, long time. A long, long time. So, I want to try and explain our spin on our motivation for want of a better term, because in some sense, the actual preconditioner itself is what to understand the problem, I think, fairly straightforward. So, the motivation is key. So, the first part of the introduction is actually going to take up most of the talk. And then I'm going to realize I've only five minutes left and rush through all the actual linear algebra. So, signal-pretty problems. Here's what a signal-pretty problem. Here's what a single-proter problem is. So we start off with a single-protective differential operator. So I'm only interested in this talk in solving very simple PDEs, and in fact, in some cases, an ODE, all linear, so very straightforward. So we've an operator that L of epsilon, it depends on some small parameter or some parameter, and we're interested in the case where this is smaller. Where this is smaller, in fact, over all possible possible ranges of it. And basically, we say that the problem is singular if, well, there's lots of definitions. The best, most formal definition will take too long to reproduce, is from the monograph of Horston Linz from 2010. But roughly speaking, what you want to think about is if you have a problem with epsilon, if you formally let that set that epsilon equal to zero, does bad stuff happen? That is, does problem become ill-posed? If it is, you're probably looking at. If it is, you're probably looking at a single problem. That's the first feature. The second feature is usually that solutions to these problems exhibit layers. Very often boundary layers, maybe interior layers, or other singular type features. Not guaranteed, you can construct artificial examples where, by sheer dumb luck, everything is consistent and doesn't happen, but usually it does. And it's well known that simple numerical methods making Make an impute physically unreasonable solutions to such problems. So, for the paper that this is based on, we considered three problems. A simple ODE, which was really there for motivation and also because don't worry about Zoofall the analysis for the 1D case. 2D case, the analysis is slightly more sketchy. But the really interesting case, since we're interested in solvers, is the two-dimensional case. Case. And within the 2D case, there's a bunch of different interesting solved problems that we can consider. One, which is the main one I'm going to focus on, is when the flow convective term is parallel to one of the boundaries as well as perpendicular to another. And so we've a mix of different boundary layers. The alternative case, where maybe we have flow that's not parallel to either boundary, is interesting. Not parallel to either boundary, it's interesting and it's alright for different reasons, but you can read out about that one in the paper. It's a little less technical from a solver's point of view. So, what are the computational challenges associated with this? Well, the first thing is that if you, as probably everybody here knows, if you try to solve this with your a simple unstable Azkelerkin method, or in our case, we're using finite differences, so you know this central differencing, you'll get just lots of possible. You'll get just lots of oscillations in your solution. So, it won't make sense. Secondly, even if you do apply some stabilization, you also need to capture the layers. And I'll make a case for that in a few minutes. But it's intuitive often when you're computing solutions to a problem that feature layers. Well, the layers are the interesting bits. And so you want to make sure you do that. Secondly, there's the mathematical challenge that has been perplexing people for quite some time, which is how do you design? Which is how do we design parameter robust methods, or I'm often when calling epsilon robust methods because it takes up less space on the slide. And by that, we mean that we want to be able to prove that our method works irrespective of the magnitude of that action. Now, I can make that arbitrarily small, and I'm still guaranteed the same error value, and also that error bound is useful. So, for example, you will find old papers in this where it guarantees that it's robust with respect to epsilon, but with the rate of conversions of, you know. Conversions of n to the minus 1 over 20. We're trying to do a bit sharper than that, and I think we did it, but the main thing is that the rate is independent of anti. I mean, this is neglecting implementational issues occurring. Let's say if eta is 10 to the minus 100, there's probably going to be some round off issues. So let's ignore that. So to exact arithmetic, we should have a reasonable error bound. Reasonable error about. Now, there's a large literature on this. So, the numbers that I put here, the 2,000 papers in the last decade, it's a bit rough. I didn't count them all. But I'm amazingly confident with that number. And there's about 100 a year on the specific subclass of problems that I'm, methods that I'm proposing. That is, I say, that I'm interested in coming up with solvers for. That is, these ones associated with APIRI. A priori constructed layer adapted meshes. So it's niche, but there's still a lot of stuff happening all around the world. If you're interested in reading into it, probably the best place to start is this paper by Sebastian Franz and Hansburg Rose from Dresden, appeared in Sciences Review. It's in the education section, so it's a very readable paper. Or rather, if you plan to get one of your grad students to do this, get them started there, maybe. Next, most readable. Next, most readable is this book by Steins and Steins 2018 on confection infusion problems. That actually grew out of a graduate course that was given in Halifax in 2017. So again, it's aimed at people who are trying to read their way into it. The remaining ones are more state-of-the-art ones. I particularly emphasise, though, that if the key reference is Rooster Steins and Tibiscuits from 2008, there is an update for a major part of that. A major part of that up on the archive by Hans Gargross from last year. So, most of the time we're going to be interested in this problem here. So, it is a linear convection diffusion problem. And the key facets we have is apart from our small parameter, multiplying a second derivative term, that I have a first derivative term, so I basically put the flow in the x direction. And this is going to lead. And this is going to lead to development of layers. The right-hand side, the choice that I've just made is irrelevant. Anything would do. And it's actually next to impossible to see on this figure, but trust me, we've got some layer-like behavior here down along y equals 0. And the width of that layer is all the roots axle. And the contour plot on the right, it's next to impossible to see, but there is another layer there of width axle. You can kind of see. There or with that clone. You can kind of see them both on the figure on the left, although it's difficult from that to appreciate the magnitude, but that's what it is. And there's lots of analysis, asymptotic analysis that's gone on to verify this. Excuse me, can I just ask, in the equation highlighting in yellow, should that one be multiplied by u? Give me that. Oh, absolutely, thanks. Thanks for pointing that out. That should definitely be you. So, although in some of our experiments, we've set that. So, although in some of our experiments we've said that we've annihilated that term. But you're right, yes, there shouldn't be a U there. So let's pretend that we were undergrad and tried to solve this PDE the way we had solved all our other PDEs with central differencing in a few lines of MATLAB. And this is what the solution would look like. So we're using standard central differencing on a uniform basis. And what we see is that Well, what we see is that, well, there's, I mean, this isn't noise, this is all signal, but the solution doesn't make physical sense. I'm going to rotate the left image a bit so we can get a sense for all those oscillations there. And so that suggests that the solution is not that physically meaningful. So at the very least, we need some stabilization. By the way, if you want to know a little bit more about these oscillations, there's several really nice papers on characterizing them, but this one is big. Of characterising them, but this one is particularly good by Ellen Maramish from 2003. So let's replace our central differencing with the next most sensible thing, so simple upwinding. So when I replace the operator with simple upwinding one, the resulting discrete problem now satisfies a discrete maximum principle. My original PDEs satisfies maximum principle, at least, but the title wasn't there. Least, but the tripod wasn't there. And so we can be sure that there are no oscillations present. But I want to stare at this picture for a little bit for a minute to try and tease out something. Because I've got a solution to two different problems. They're radically different problems. One of them, the diffusion coefficient, is 10 to the minus 4, the other one is 10 to the minus 8. They're very different numbers. And from where you're sitting, I'm pretty sure those two pictures look very, very similar. Maybe if a squint. Maybe if I squint, you might see that they're slightly different up here. And that's associated with the layer whose width is roughly 10 to the minus 2. This is computed at n equals 32. We're beginning to start to resolve it. Well, the main thing to take home is those two different differential equations that we're solving. If we were to look at the maximum normal difference between those, we would get something older what? And the difference between And the difference between these will not be otherwise. It'll be very small. So, two ways of thinking about this. One, the clear thing is we're not resolving the layer. We can't tell the widths of those layers, which is usually quite crucial. And in any case, it's still when we want to, when we say that we're interested in uniform convergence, often that's taken to mean maximum point-wise convergence with respect to maximum point-wise form. So that would suck it. Point way forward, so that would suck. So, what are we going to do? Well, you're not going to be shocked to learn that the way we try and resolve this is with a mesh that resolves those layers. Now, this is a stunningly obvious thing to do. So, I don't this yet when this mesh was proposed by Gushi Shishikin back in the late eighties, I think, maybe early nineties. 80s, I think, maybe early 90s, which was simply a tensor product group, or yeah, we call it a tensor product group, but we'll adopt the phenotyle terminology for a moment. And we choose to transition at points where we transition from a very coarse mesh, which happens to be uniform because why not, to something that's quite fine but still uniform. So these are piecewise uniform meshes. And encoded in these two transition forms. Encoded in these two transition points, excuse me, are the width of the layers. There's an ethanol term here, square width of the ethylene there. The other parameters, we see the sigma and the lower bound. That C should be a lower bound for a combination of the convection and reaction terms, just done from the analysis. The sigma term is just there, this relationship to the formal order of the scheme, so it's not that big a deal for a first-order scheme like we have. The log n term is there to Is there to essentially ensure that the discrete solution for the layer part is decayed enough at their point. So it's again coming from the analysis. So by the time Shishkin had proposed this, there were much better methods around. I mean, the first person to actually do this and arguably having essentially solved all the associated problems was Bakfalov, better known for his work in multigrid. His work in multi-grid in 1969. And even though the paper was in Russian, we still all knew about it. But the analysis was very, very tricky and it's very hard to generalise and factor in some sense. For even a minor variant of the problem I have today, it's still a number problem to prove that that converges. This one, the main thing that came with it was not the mesh, but the simplicity of the analysis with which it was accompanied. So, but, and it worked. So, you can prove now it's a So you can prove now it's a notwithstanding scheme, so expect it to be only a first-order conversion. So you'd expect that the error to be bounded by c to the n minus 1. In fact, it's spoiled slightly by a logarithmic factor of an n. That's just to do with the fact that we have this log n term for our analysis here. We can get rid of that if we use the back valve of mesh, but then we're doing that at the cost of, well, not being able to complete the analysis. I want to say though, in all this I want to say, though, in all this stuff I'll be presenting in terms of the solver, it actually all works for any of a wide class of measures. They will be looking at the shifting one just because it's the simplest to present and simplest to understand. But the paper covers a whole bunch of measures within reason. And we want to examine the approximate error. So the approximation error or our numerical error. So the main thing to Our numerical error. So the main thing to focus on is: first of all, for any value of epsilon, go left or right, and you do see convergence. And if you pull out your calculator, do some calculations, you will see it's exactly this. Secondly, for any fixed value of n, here I'm only looking at small values of epsilon. I could being a little bit lazy here and not including the larger ones, but it should convince you that the errors are essentially independent of epsilon. Is our slightly independent advanced. So we've got a good method. It seems to do what we think it should do. But there's a problem. So we've had during this week talk about evangelical type talks, and this is one of those. So to be evangelical about us, I'm not saying that absolutely everything that's published is wrong, but what I am saying is the not terribly controversial view to View to people when you're talking to linear algebraists, which is to say that if you think that your convergence or that the efficiency of your method is purely controlled by the number of degrees of freedom, you're probably missing something. And so almost all the single-perfect literature assume that the effort required to solve a problem is just based on that, just the number of degrees of freedom, and nothing else matters, which is unlikely. Else matters, which is unlikely. So on Wednesday, when we were having our discussion, Howard gave a really nice, in fact, introduction to this talk, but also explained maybe his own experience of having been working on solvers and then realizing that perhaps there's a bit more to learn about the discretization. I had some very similar experience except the other way around. I used to always work in. The other way around. I used to always work in discretizations, and then I learned that there was a bit more about solvers that I needed to do. So, to choose to steal some align from Howard, sometimes you have to learn the thing that you're doing. So, I decided to have to learn a little bit of linear algebra. And the thing that actually brought it to me, hope to me, was when I was working on a finite album method based on sparse grades. And we write a lovely paper here, 2009, published in. Paper here 2009, published IMA journal in numerical analysis. I followed up with some other papers. But essentially, what we were asking reviewers to believe was that if I had a linear system like this, I did some voodoo and replaced it with this one, which has much fewer degrees of freedom. So we've gone from 30,000 to 1500, that this is somehow better. And of course, we've sacrificed the sparsity in spite of The sparsity. In spite of it being called as sparse grids, not that sparse. And no, it turns out actually it's still worth doing. Certainly even with the direct solver. Actually, only with the direct solver. This is definitely worth doing. But that was the first time that I got involved in digging into what the solvers were doing, because otherwise the reviewers wouldn't accept the paper quite rightly. Writing. By the way, I also put this up as a way of advertisement. If anyone is interested in very structured but complicated looking matrices with extremely handsome sparsity patterns that you think you've got a good solver for talk to me. I've got lots of these. But no good solvers. Now, so I found out that, okay, for our single-eight pair problem, not this one up. Problem, not this one up, the Sparse Green stuff was on reaction diffusion. That, yeah, okay, if with the direct solver, the Sparse Good method was really good, much, much better. But something was nagging at me. But it did say I was running lots of tests, initially with backslash, but then getting a bit more sophisticated. It was using writing this from my own C code and both getting lots of different linear solvers. The one here is from a Linear solvers, the one here is remote fact out of the sweet sparse. We get this really unusual behavior. So just look at the last column. This is the solve time in seconds, and these are very carefully computed, so this is quite reproducible once you have the same architecture, which was just a simple desktop. So the last line had 10 to a man of safe, I guess. Sorry? The last row is 10 to a man of shape. Oh, yeah, thanks. Yeah. I've given this talk four times and I've got that. Thank you. So, That's possible. Thank you. So that's definitely 10 to the minus x. But what we see here is that the solve times are increasing initially anyway. Alecepts and get smaller. And the nice thing about that is that the linear system for, let's say, this one and this one is exactly the same, just with a different scaling on the diffusion. So that was a bit surprising. Direct solvers aren't supposed to work like that. And so that convinced me that I need to know a bit more about direct solvers. I need to know a bit more about direct solvers and solvers in general to understand this. Now, to steal another line that we've had this week, this time from David, that when you don't know what you're doing, it's a useful idea to talk to somebody who does. And so in that case, I was visiting the IMA in Minneapolis and Spatham, the person in the next office beside me, Scott McLaughlin, who asked him what he did. He said he worked on iterative solvers. And, you know, you try to ingrain yourself with somebody, particularly if they're an iterative solvers, a person might. Particularly if they're an iterative solvers, person by both mad, bad-mouthing direct solvers. So I said, hey, I've got a problem for which direct solvers are rubbish, and that led to a bunch of papers. And really it's the starting point for all this. But I guess it further lent evidence to the argument that we need to work with effort to solve the good preconditions. Now, so our goal, and this is kind of the So, our goal, and this is kind of the finished evangelical base, is to try to come up with a preconditioner for a convection diffusion problem solved with one of these provably good methods. So that was the simplest upwinding on a Shishkin mesh, in such a way as that the performance of the solver was parameter robust. Now, there's lots of, we're not the first people to try this, so I'll just. This. I'll just leave these references here for you, but the main emphasis I want to give that again is about problems on layer-adapted meshes. And so these are the main references that I could dig out on that. So what we're going to do? We're going to do convection diffusion 1 and 2D, as mentioned. It's always upwinding on layer adaptive meshes. We're going to use special boundary layer preconditioners. So this bit becomes a little bit obvious. So, this bit becomes a little bit obvious when you talk to people from the linear algebra community. We basically treat different regions differently. Because some places we've got a very coarse mesh, not much happening. Some place it's very, it's quite unsubtopic. Sorry, I'm talking about meshes because finite differences, but you get the idea. And from that, try to build up a robust preconditioner. We had actually done this years before, 10 years ago, for Years ago for reaction diffusion problems and kind of moved on to working on other stuff. And then last few years ago, this paper by Carlos Esivera came out where they kind of recognized our interest in doing this for convection diffusion. So let's talk about the 1D problem. I was a bit lazy in updating this, so for another talk I was again demonstrating that a standard A standard central differencing method with lead oscillations. And this is the 1D version of our Shishkin mesh. The 2D version is essentially just a Cartesian product of two things that look like this. So we choose our transition point that we talked about earlier and then we set up our finite difference scheme. So this is the stencil for the upward finite difference scheme. And the only thing I would then point out is just stir. Point out is just stare at the finite difference stencil for a minute, think about what that epsilon is, then let that epsilon be zero. And because, in some sense, at least over much of the domain where not much is happening, it's a sensible thing to at least get an intuition about the solver. And when you do that, you end up with just a very simple upper diagonal system, a bidiagonal system. Now, No, we're solving problems that are not symmetric, so it's certainly looking at condition numbers. Later, we'll be using flexible GMRS, so talking about condition numbers doesn't really tell you that much, but it's basically if your conditioning is bad, that does give you some information. So you can prove that the conditioning for this one looks like that expression at the top. So again, It's a little bit subtle. If epsilon is vanishingly small, things are not too bad, it's over m. But it's not simply epsilon over h. We have an epsilon over h squared, so it's a little bit more complicated to look. If we turn the handle and see what this means for a Shishkin mesh, it means that really, in some sense, that what we have to worry about is some kind of transitional regime between. Regime between when the layers are developing, but epsilon is not terribly small. So again, let's have a look at what our stencil is, and this time look at an approximation of what it is in each of our two regions, in the very fine region and in the coarse interior region. And these are what our two stencils look like. So, for example, So, for example, when we have a very small ep compared to n, we will see that our convection term dominates. And so, as mentioned earlier, it's in the upper bi diagram. And at our first step towards a preconditioner, we're going to do a block preconditioning. So, first thing that we want to know is that we have here basically L refers to the layer parts, I is, I call it the interior. Part I is, I call it the interior, even though obviously it includes one of the boundaries, the stuff that's not in the layer. And we approximate those two stacks separately in that kind of block structure. When we do that, essentially take an idealized preconditioner for the layer part, because in a while we're going to just swap that out for multi-grid. So initially, we just leave it alone and approximate the Approximate the interior part. And when we do that, this is sort of bad that we're able to prove through the precondition system. So when epsilon is very small, we have some very nicely banded eigenvalues. Proof is a little bit long, even in 1D. I'll refer you to the paper. And finally, get on to the results. So our results basically show Our results basically show that for when we pre-condition GM res with this, that, and this here I'm only showing you what iteration counts because it's 1D. I'll show in a second the iteration counts in 2D. But we do see that very good behavior with respect to epsilon. In fact, as epsilon gets smaller, the fewer iterations needed. Because in some sense, some structure of the problem is becoming even simpler. And so we're reasonably happy with that. I mean, if it's one iteration, it's a direct solvent. Yeah, that's the so it is, but admittedly, this is just the 1D case. So I mean essentially it's 100%. Effectively it is, yeah. Onto 2D, and so I'm going to skip through this because I've shown most of these pictures already. The idea for doing the preconditioner is analogous, just now more complicated. Just now more complicated because previously I had two regions, now I have four. So, this interior region, which is uniform, this one, which is very fine in both directions, but not equally so. It's still the aspect ratio would be compared to Neptune and a new heptal. So, and then here I have two anisotropic regions. Basically, what we don't want to do is apply multi-grid everywhere. That has been done on Shishi. Everywhere. That has been done on Shishka measures by Gaspar and others, and it works, but it's a very heavy tool. It's a little bit expensive. Basically, applying multi-grid everywhere where you don't need to. It's not what we're about. We save multi-grid for the corner region. And then, since I'm out of time, I'm just going to say that basically we do some kind of line solves on the On these two regions, a little bit of work and working out exactly how it should be done, but it's done and we do that. And if you're into multi-grid, you can read that really, really quickly. Or the file for this is already up on the Very's website as well. And then finally, we get on to the actual results. So here it's not quite a direct solver. It is a little bit more complicated, but it is still really good. Again, I'm only showing for smaller epsilon. only showing for smaller epsilon that it takes for n is 2 to the 11 5 iterations of the of f gmrs and the solve times are even better so remarkably we can even solve this problem more efficiently than when the problem is not significant third but that's in some sense because rather than having to do multi-grid everywhere we're just doing it in a quarter of the region so that's Is in a quarter of the region. So that speedup would be what you'd expect. And it speaks your direct solver, even when the direct solver fits in quote. So to finish, my argument is that for this quantization to be robust, you need a good solver. And when you're using their adaptive meshes, you definitely need a good preconditioner. And so if you're ever refereeing or reviewing. So, if you're ever refereeing or reviewing or editing a paper that does this, ask them about the solvents. That way, they might cite some of our papers. The next steps is we want to, the analysis in 2D isn't quite complete, it's quite heuristic. We want to do it less heuristically in 2D and 3D, maybe restricting ourselves to subclassical problems. The same idea has been extended very recently to problems on more complicated domains. More complicated domains. That is a finite element method for where we're doing, let's say, Delaunay over a region. It's not just a simple tensor product. And there's also work by one of my co-authors and a group at Lawrence Berkeley Lab on direct solvent straight. Thanks. Questions or comments? Finance. Two questions. One, you had a reaction term in most of your statement equations. Is that important for anything you're thinking about? No, I mean, we accept there's another variant on the problem where there's a second small parameter associated with the convection term. So multiply this one by mu, and then it becomes interesting because you have a whole set of different regimes. Set of different regimes. But in terms of what we were doing, no, a lot of the time we just set that to zero for pure convection diffusion. Sorry, one other thing. You mentioned reaction diffusion. That was not your concern here. But do you have any comments on differences between that problem and the convection diffusion problem? Yeah, the reaction diffusion is much easier. So is the main thing. So our discrepanizations naturally lead to SPD systems. So that immediately makes things easier. That immediately makes things easier, and we've been able to do a full analysis of the reaction fusion. The layers are simpler, also? The layers are, yes, they are extremely simple. So, rather than having layers of different magnitude, as we have in the convection diffusion problem, they're all of the same magnitude. I mean, okay, you can do some anisotropic diffusion problems where that would be different, and that might be interesting as well. But generally speaking, for the solvers, the reaction fusion is worth studying but easier. We're studying the easier. Yeah, so I mean the 1D was just there to fix up the error. And in fact, with another paper with a tie that came out of a time my co-author did his PhD with me, and one of the papers we have from it is showing that for certain convection diffusion or reaction diffusion problems, you can even treat incomplete Cholesky as a direct solver. And that's related to, remember I had this weird behavior with the direct solver? In some sense, that's because in the factor, huge amount in the Chalesky. Huge amount in the Chalesky factors, or in this case the LU factors, a lot of the terms are vanishingly small, and we run into problems with floating-point arithmetic in relation to sub-normals. That also tells you that a lot of entries and factories are vanishingly small, and in fact, in certain regimes, you can treat this as a direct solver. So, the one-day case is, for that reason, a lot less interesting. I'm sorry, by the way. I'm sorry, by the way, they're using incomplete GLS pieces of very solver, that's for the PVE case. We don't have that for the convection fusion model. But here those are still very efficient, but very few, but more than one iteration needed. Okay, thank you. Let's move on.  