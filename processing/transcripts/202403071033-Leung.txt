She's gonna tell us about purifying arbitrarily noisy quantum states. So, yeah, Ami, thank you for giving this talk and the stage is yours. Thank you. I want to thank the organizers for the invitation to join this meeting and especially for their various accommodation and help so that I can participate and enjoy a lot of the talks online. This talk is on purification of noisy quantum states. It smells very much. It smells very much like quantum information than quantum computation. But since this meeting is about fundamental limits to quantum computation, I think you will expect to see some intersection between these two subjects. This is joint work with Andrew Childs, Hong Hao Fu, Zhili, Marius Osos, and Verdan Fiers. The common threat to these authors is that at some point they are at Waterloo, whether at IQC or at Perimeter. The work has been The work has been written up and sent to the archive September last year, but there are two major revisions to be made. I'll mention what they are in the talk. I want to encourage you to ask questions if anything is unclear, because I cannot see your face quite clearly on the room view. I can see the room, but not individual faces. So if you think that something is unclear, I may not be able to get from your facial expression. So let me first define the problem. So, let me first define the problem. The quantum state purification problem starts with the specification of a certain noisy state. So you have this pure state that is unknown to you, and that is your precious quantum signal. It is mixed with some noise. The state lives in D-dimensional hubic space, so the noise is the maximally mixed state on d-dimension. And the convex combination is denoted œÅ delta for the rest of the talk. For the rest of the talk, the goal here is that you want a state similar to rho delta, but you want a version that has a lot better signal and fair little noise. So you want rho epsilon for epsilon much, much smaller than delta. For simplicity, assume you only want one copy. You're given many copies, as many copies as you wish of the noisy state, RoData, and you want to. And you want to devise an algorithm to convert these n copies of rho delta to rho epsilon. You know delta epsilon and d upfront, a very unrealistic assumption that quantum computation is perfect in this talk. So the question here is that how can you minimize the number of copies required? You will see that a lot of these protocols are stochastic in nature. It's a little probabilistic. Little probabilistic. So here we choose one measure of the resource, which is the expected number of the copies, and call it the sampling complexity. Sample complexity. There are other ways to define resource and success probability, but we just stick to this one convention here. If possible, we also want to minimize quantum memory and the circuit size to perform the purification. So this is the problem specification. Any question here? Yes. Question. What is the expectation of? Question: What is the expectation logo again? What is the expectation logo? Ah, um, you will see that there are a lot of noise in the problem. There are a lot of randomness in the process. Sometimes you succeed, sometimes you fail. And so we take expectation over all the possible fail and success outcomes. Basically, over the randomness of the algorithm. It is a randomness of the algorithm itself. Any other question on the statement of the problem? Then let's see what was known before our result. It's a paper that's from quite a while back now, more than a quarter century, in fact, by Sirek Eckhart and Machiavello. Machiavello for qubits. So there they find a very nice relation, an optimal relation between the sample complexity. The plus one is just there. Then the initial error delta and the final error epsilon. It's just this wonderfully simple expression relating the initial and final error and the sample complexity. And they prove that this is optimal. You can't do better. If you stare at If you stare at this, one thing you should marvel at is it has a very nice dependence on one minus delta. I just scroll back up one. One minus delta is your signal. So if you have very high noise, delta is close to one, one minus delta is very small, like one percent. Your sample complexity only depends on this very small signal fraction as an inverse square. It is a very gentle function. It is a fair gentle function. It also has nice dependence on the final error. It's just one over epsilon in the sample complexity. It is amazing result you have. You don't see this in error correction or in other aspects in quantum information. To make it even better, if you want more than one copy, you can have as many as roughly the correct fraction, n times 1 minus epsilon, number of copies. n times one minus epsilon number of copies with roughly the same average final error. It's amazing. And there's a follow-up work by Kale and Werner also on qubits on other varieties like with different error definitions and how you define the sample complexity and so on. Both papers relate to another problem that we all care about, that is cloning. Cloning happened to be a natural reverse of period. A natural reverse of purification because there you're given a good copy or some number of good copies of a quantum state, and you want more copies that you're willing to tolerate with more noise. So the purification problem is kind of an inverse with cloning here. And by studying the optimal solution for purification, they also obtain a nice optimal solution for cloning. Any question on the earlier work? If not, then If not, then what did we do? The work started around 10 years ago when Hong Hao was a master's student with me and Andrew. One thing that he obtained in his thesis is a nice expression, very similar to the one for QBIT, for Q-trides. Let me reassure you that the two becomes three here is probably misleading because you expect the Q-trid case to be harder. It may be hidden in. Harder. It may be hidden in the big O in the lower order terms. But essentially, the Q-trip result is similar to the QB result. Also, no threshold. This is also an optimal expression. Now, what about larger dimensions? There's some very hard workers in the team. We try to study the weak share sampling, which is a direct generalization of the original. Of the original Syria-Eckard McKeovolo result. They are nice expressions that we can simplify to express the fidelity as a linear program. Unfortunately, in this formulation, there are quantities in the expression that remain hard to compute. So we do not have an expression. And even the protocol itself, we have to propose, we have to assume certain steps being optimal. Assume certain steps being optimal. So it's not clear what the expressions are, like what you can obtain, what are the errors you can achieve given N copies or whether it is optimal at all. We are still not understanding this very well. Maris is working quite hard on this direction. Anyone interested in more detail, I encourage you to contact him to discuss further. So this is the good news for the talk. So, this is the good news for the talk. The difficult direction of the talk, since we, the subject, since it's so hard, you're not going to see much of it anymore for the rest of the talk. We're going to do something easier. We look at the easiest possible naive protocol you can imagine. We're going to try to use a swap test to perform quantum state purification and see how well or poor it performs. So, let me define what that means. This is the swap test. This is the SWAT test, probably familiar to most of you. There are two quantum inputs, I call that the two systems, S1 and S2. You have a potentially joint state tau in the two systems combined. You take in a qubit state, the plus state, and condition on the fact that this is a one state in the computational basis, you swap S1 and S2. Then you measure in the plus-minus basis, and your outcome is A. A. The first thing I will say is that this circuit is efficient in the problem size for dimension D of S1 and S2. Say D is 2 to the N qubits. Then this circuit has size all the log D. You can rewrite this control swap to be done qubit-wise, and then rewrite each control swap on the qubit in terms of the C naughts and the topilis. The C naughts and the topilies, and it would come out to be a small constant to perform the control swap on two qubits, and then you multiply that by log D. So this is a fair short circuit for D-dimension. Then if you get the outcome A being plus, your post-measurement state in S1 and S2 will be projected onto the symmetric space. If you get minus, then it will be projected onto the anti-symmetric space. If you choose a tensor product state to be in the inputs, rho and sigma, then the probability to get the plus and the minus outcome is given by this expression. The bias is given by this trace row time sigma. Now, it is simple from the inspection that if you have more different row sigma or you have very mixed rho and sigma, despite the fact that they're very Rho and sigma, despite the fact that they're very similar, if they're very mixed, then trace rho tim will be a small number. And so you will end up having an improved, a larger probability to get minus. And so our idea is to reject the minus outcome and keep the output only when the outcome is plus, in the hope that we postlect is pure and similar. And similar state, pre-measurement state, rho, and sigma. That's the intuition. We also need to discard S2 because we don't want the correlation between S1 and S2 to be carried on to the later part of the algorithm. This is the intuition for why we choose the swap test to be a tool-to-do purification. Then let me be more precise. Then let me be more precise. For the purpose of the next slide, it's useful to just write down the post-measurement state in S1. It's given by this expression. So if you choose to have two equal copies, so rho is equal to sigma, then your post-measurement state has the nice little form that respects the eigenvectors of the input. And it is a half-page, very quick calculation to show that the maximum eigenvectors. That the maximum eigenvalue, if it is non-degenerate, will be strictly increasing by the protocol. And so, for our state, clearly, this is the eigenvector corresponding to the maximum eigenvalue and it's not degenerate. Whenever delta is non, if delta is not one, then we will be able to purify the state. Let me just put say an aside. Say an aside, what happens if you don't have the same state here? It would depend on the dimension whether you get an improvement or not. If the noise is very low, they all work well. You can tolerate very different input state with very different noise level and you still get an improvement. If you work at a very high noise regime, then for low dimension, you can still get an improvement if the delta one and delta two are different. But when you take the But when you take the limit for infinitely large dimensions, then this peak is, this spike, the red spike here is infinitely sharp. So you do then need equal copies to obtain improvement using the SWAT test. So now let's see what we mean by recursive SWAT tests. It's roughly what you think it is. We take two copies of the noisy state, run them through the SWAT test. Noisy state, run them through the short task. And if it is a equals to plus, then I keep one out of the two copies. I get two more copies. This time I get a minus whoops, throw it away, then ask for another two copies and continuous. This time I get a plus. So I take the two better output states. I combine them with another round of the swap test and hope to get yet a better copy of the state that I want. And this is two levels down. I have done two rounds of short tests. So I want. Done two rounds of sour tests, so I want to combine comparable, comparably noisy state. So I continue this kind of purification elsewhere. And then by the time when I get another copy of the state of similar noise level, then I try to combine them together again on yet another round of swap tests in order to improve the noise. And then this time I, whoops, I lose everything. So everything turns red and I have to throw them away and start from scratch again. So this is kind of the cartoon way. So, this is kind of the cartoon way to think about this recursive SWAT test. I run out of space to draw it, so I throw away the red part. And starting from the initial noise of delta, after one run, let me call the next noise level, row delta one. Then, if I combine two noisy states with noise delta one, then the output if I get a good outcome, plus a equals to plus, then I call the new noise. Then I call the new noise delta 2 and so on. Then you can just keep recursing this protocol, combining two equally noisy states that you obtain to improve the noise from delta zero down to a delta n, that when it goes below epsilon, you say that I succeeded. You output the state here, and that is your output quantum state. Any question on this protocol? Very simple. The first thing to note is that if we allow to ask for this state, rho delta on demand, pair two at a time, two at a time, in a streaming fashion, it turns out that this protocol requires very little quantum memory in what we call a stack-based implementation. The memory that we need is n plus. That we need is n plus one. N is the number of iterations that you need. We need n plus one qubit. Remember that these are d-dimensional states together with the one copy for the one qubit for the plus state. We need that much quantum memory. I look up the Wikipedia version of what it means by a stack-based implementation. They said that it's a model after a stack of plates. So it's stacked up. And then the noise here, the input. The noisier, the input goes from the bottom and then the output goes from the top. And the noisier, two copies before you do the SWAT test can be stuck at the bottom. You do the SWAT test, and if it's good, you promote that state one level up. And then you kind of move things slowly up and down, up and down. And there's a pointer showing where your desirable data is. And eventually, you will hopefully have two copies of the delta n minus ones, and then you get the last. ones and then you get the last one and then you output it. And it is not totally obvious, but it is all the N quantum memory that you need for the process. Then in terms of other resources, the sample complexity, how many copies of rho delta do I need? You know from the tree that it is exponential in the number of iterations. The number of gates is roughly the number of tests, which is clearly also exponential in n, times the number of gates per test that you need. Okay, so the important question then is what is the number of iterations needed to go from delta to epsilon and how fast does delta i decrease? So let's do this analysis. So in one step at level i, I take two copies of rho delta i and hope to get the, if I get the plus outcome, then I get rho delta i plus one. I omit the superscript of d, but they depend on the dimension d. And this is the relation. If you have delta i for the i level, then the delta i plus one has this form. Several things to Several things to observe from this expression. If you rephrase it a little bit, the dimensional dependence always have a delta i in front of it, which is nice because it means that when delta i is small, say if you have one tenth of the noise, then the dimension does not bother you very much. So the for low noise situation, the dimension is not a an An important parameter in the problem, which is very promising if you want to generalize purification to a large dimension, which is the problem here. Before this, only two or three dimensional purification protocols are known. The second interesting thing to note is if the noise is low, roughly speaking, delta i plus one is delta i divided by two. Divided by two. So you have the noise roughly every iteration, which is very promising. It is also important to track the probability because if it fails too often, you will have to repeat the test too many times and that can ruin your complexity. Luckily, the probability of failure is never below a half. It doesn't change with dimension. It doesn't change with the noise in a big way. And it is at least a half. And it is at least a half. In fact, it is at least half plus one over d. And so I just repeated those two lines here. And this is some way to visualize how quickly the noise can be suppressed. I'm plotting this for initial error 0.99. It's a fair high number here. It's very close to one. So the good signal is only 1%. The reason to look at such high noise is that the lower Look at such high noise is that the lower noise regime. If you want to say delta being a half, then you can just look at the curve from delta being a half onwards. So your initial state will be here instead of up here. So usually I try to plot this curve as far back with a high noise as possible. And so here we plotted for some representative dimensions. One thing you should notice is that these curves look very similar. They just translate it to the left, mostly. To the left, mostly for the lower part because this recurrence is very insensitive to the dimension for low noise. Then another thing you will notice is that it drops very rapidly in the middle, in the middle part. So, where is the middle part? There is a threshold which is half delta, which will be a very important threshold for the problem. If you stay The problem. If you stay away from the half, like 40% is good, 45% is good, 33% is good, then you will be in the regime, in the lower regime, that will see exponential decrease. The other part is the top part, which is very flat, very slow to go down, but at some point it starts to really curve down. So there is another threshold up here, which again is not very important what it is. You can pick two-thirds, you can pick 15 over 16, just any good number around. Just any good number around here will be fine. One other thing to mention is that the larger than the dimension, with the same initial error, the slower is the noise suppression. So this curve, when you increase the dimension, always go further and further out, which allows us to use the limit when d equals to infinity as a worst case upper bound on the resource. With this, With this, we can conclude that it takes about five iterations at worst to suppress the noise from two-thirds to one-third. Basically, from the most noisy, from this noise, this higher threshold to the lower threshold. Takes about several iterations. The lowest part, as promised, we see the exact expression is exponentially decreasing with the number of iteration. And it is insensitive to D. And it is insensitive to D. So we don't need to tailor the analysis to dimension. And finally, we see more detail in this analysis is an overview of the result. For the very high noise region, things start to look tricky because it looks like it's very, very flat for large dimension. And you're not wrong, it is very flat. For the d equals to infinity limit, it is very flat until roughly one over. Till roughly one over one minus delta, then it starts to curve down and it hit whatever threshold you set roughly at one over one minus delta plus a little bit more. And we will present a proof of that. Any question on this highlight? Oh, there's a message. I have a question. Can you hear me? Yeah. Yeah, I was just wondering, where does this analysis differ from the D equals two and three cases that you showed at the beginning, by Paul? That you showed at the beginning, like on how and Sirak, and others. Oh, thank you so much for the question. Those are for a different protocol that use weak share sampling using the full permutation symmetry of the entire set of n inputs. Okay. Isn't the slot test a special case of weak short sampling then? It is a very weak case because you only use a very small portion of the permanent. Very small portion of the permutation symmetry. Okay, thank you. Yeah, you give up a lot to have a simple circuit and have analysis that you can actually perform. In other words, for D bigger than three, the similar analysis that I, the similar, we cannot do that. To the best of my knowledge, no one knows how to do the analysis for D bigger than four. So we'll start with. So we're stuck with what we can do, essentially. Any further question on the overview of the analysis before I show a little detail? I have a related question to previous question. So you said the previous protocols would... Is it true that they're trying to just project this n copy state into the symmetric subspace? Is that not really. We project two copies into the symmetric space. Project two copies into the symmetric space. We project another two copies into the symmetric space. Then, if they both succeed, then we take the two copies that come out and project them again to the two copy symmetric space. Right, right. But what happens if you just did it with many copies at a time? Like you just projected the symmetric stuff? Is that good? Not to the best of our knowledge. And it will heavily increase the complexity of both on the analysis and on the analysis. Both on the analysis and on the circuit. But you ask a question that we hope to understand a little further. I tend to think that the pairwise swap test is a little bit like a kind of a two-copy error detecting code. And the recursive swap test look a little bit like a concatenated code here. I do wonder if I can borrow some of the newer ideas in, say, quantum LDPC code to see if there is a LDPC code to see if there is a slightly stranger way to say, indeed, take three, four copies projected to the symmetric space, and see what you get in terms of the trade-off between the number of copies and the improvement. And then see how they can actually be combined in a way that we did not imagine before. We certainly look into one case in which we combine the outcome of different noise levels. That certainly does not help. That certainly does not help. But we're restricting to a very the most naive way you can imagine using the soft path here. Okay, any further question? Let me tell you why we stick to that naive version. Because one more thing to mention. If I start, if my noise level image is here, and I want to get down here. Is here, and I want to get down here. Then I have to analyze the complexity from this high noise to this threshold, say two-thirds. And then I combine it, multiply the complexity of the two-thirds down to, say, one-third, and then multiply the complexity from the one-third down to, say, 10%. I need to do the multiplication of the complexity because every copy comes with a number of copies that I need to get down there. And so it's a multiplication of resource. Multiplication of resource that I need to do here. But the reason why we do not bother with some of these other methods is because the recursive sort test works surprisingly well. If your noise is no more than if it's away from half, then you're very good. This is summarized by the statement that take any dimension, of course, has to be at least two. Noise has to be strictly small than a half. Strictly small than a half, any I, your noise is essentially exponentially suppressed with exponent two. Because of what I pointed out earlier, that there is roughly speaking, a one over half at every step, once your noise is not too high. And not too high really just means something like a third, a fourth onwards is not too high at all. And if you don't like this. And if you don't like this kind of expression, that looks a bit too ugly to look at. Pick one-third, then it is no more than two to the minus i. And which means that if you want epsilon as the output noise, take log one over epsilon. Iteration will get you there. So let's check what's the sample complexity then. Just a reuse of that diagram. Just a reuse of that diagram I drew earlier on this whole test. At the ith level, you need two copies per test, and you need how many tests? Roughly one over the probability to get plus at the ith level. So it's one over P to the I. Pi is the probability to get A equals to plus at the I-th level. Then you need to multiply the resource for all the levels together. So I range from one to So I range from one to the log one over epsilon. Then of course when I multiply two this number of times I get back one over epsilon. When I multiply all the one over probabilities, because the probabilities converge to one exponentially fast, it turns out that the product of them all together is upper bounded by one over one minus two delta. In fact, there is a delta up there that I found it is ugly, so I threw it away, but there is a delta on the numerator. Numerator and so here it is, is our bound. Let's compare with what we have with the qubit optimal trade-off. With delta no more than a third, these are essentially the same, up to a very small multiplicative constant, like three, four, five, eight kind of worth. So the sort test is competitive. The SOAR test is competitive. Let's also ask the circuit complexity. One way to find out how many gates we need is to stay at one of these binary trees and notice that the tests are given by the non-leaf nodes. And the number of non-leaf nodes is actually never, it is always the number of leaves minus one. Here I have one, two, three, four, one, two, three, and then I have four leaves. Two, three, and then I have four leaves and so on. So the number of tests is strictly smaller than the sample complexity. So I can just use the same upper bound here. And each takes all the log D. So the circuit complexity is the product of these three things together with all the exponents worked out. It's not a large number at all. So if I may convince you, the memory is also low, as mentioned earlier. So if I may convince you, So, if I may convince you, if your noise is not too high up front, then this recursive SWAT test, you can just use it. It's basically optimal and it works for any dimension. It needs very little memory and the operation is low and the complexity is pretty much optimal. So, any question on this? Then we're done with this. So, maybe I have a question. So, in this tree, you only keep the plus outcomes, right? Is it clear that if you go up on the tree and you get a minus outcome, it will always be worse than should never keep a minus? Yeah, we never keep the minus. This is also, I mean, it never helps you to keep a minus. We do not know, but because by being so. Because by being so generous and rough, we still get reasonably good outcomes that we know is close to being optimal by a factor of three to eight or something. We simply have not bothered. Okay. It sounds crazy, but life is short, so we do not spend time thinking about this very much. Are you saying that this is optimal in B equals two and three? In the equals two and three, or close up to another? There is nothing we can compare to for large dimensions because it's not known. It's basically a known. Which I hope will give me a little sympathy because I feel a little embarrassed presenting a result so simple here. But the truth is that, to the best of our knowledge, the optimal There is nothing known about dimension larger than three. Any further question on the simpler half of the problem? Then, let's see what happens if the noise is higher. Well, we had an overview, so you have some expectation what is coming. Medium initial noise, no problem. It takes a few iterations to go down from the higher threshold to the lower threshold. What does it mean? You multiply a pre-factor, it's a constant. Say if it is upper bounded by five iterations, then you have two to the fifth. This is the number of iterations, and then the probability to succeed is at least a half. So you divide by So, you divide by a half to the power five. This is our constant that we need to multiply. It just all it says here is that two to the ten copies of the noisy state will give you at least one copy of row one-third. If you want to have the error epsilon, not one-third, then you multiply this number of copies to what we work out earlier, but you set the delta to one-third. To one-third, and this is an upper bound: two to the ten and three over epsilon. We get you from rho delta to rho epsilon if delta is roughly this range. Two to the ten starts to look a little horrible. These are all constants. I know that all physicists say that constant is one, and all the computer scientists say that the constant is a big O. But let's see what it actually is. Let's see what it actually is, if you ever care about implementations. For d equals to 3, it is not 2 to the 10, it is 21. For a thousand dimensions, it is 121. So it rapidly increased, but it is actually not so bad for medium dimensions. Okay, so that's all to report on the mid-noise level. And the talk is on the very high noise. The very high noise regime. So let's take a look at what happens there. It does eventually get cleaned up. You can still perform purification using just the recursive swap test. You have a nice alleged upper bound on the number of iterations. But remember, the sample complexity is roughly exponential in the number of iterations. So we expect this in the exponent. Let me code the number of iterations. we call the number of iterations I star. If you have d-dimension, then I star D is the number of iterations needed to suppress the inertial noise to this higher threshold. As I said, it is not very sensitive to the exact value. It's all by a small constant, a small constant. And so how do we understand this parameter I star? It turns out to be very cute. Be very cute. The analysis is very cute in this regard. The delta i plus one as a function of the delta i is actually rather ugly and it doesn't it resists analysis. It goes from delta zero, delta one, delta two, and so on. And then it hit the one-third, and then we call it I star. Sorry, I think I mean two-thirds here. And then if you look at one minus delta, it is kind of equally unruly to analyze. Unwoody to analyze, but it goes up. The Q part is the recurrence starting from two-thirds here and inverse the recurrence for one minus delta admits very nice tight analysis. And so what we have done is to look at this other recurrence, start from two-thirds, and see when it starts to drop below one minus delta. And the number of iterations to go this direction is our I star. And so we actually. our I star. And so we actually can analyze I star. And this is what we have. We can just prove that I star for the worst case dimension is upper bounded by this expression. 1 over 1 minus delta is what we have seen numerically from the plots. And the small correction is log 1 over 1 minus delta. C is any constant bigger than 0. Take 1% if you want to, take 0.1%. 1% if you want to take 0.1% if you want to. And this I need to put into revision. It's not in the current write-up. The current write-up has an analysis, asymptotic analysis, which says that it is roughly this expression. So the upper band is tight. Then we also prove double-hole proof, and we actually prove an expression similar to this one that I show here. I picked this one because it is. I show here, I picked this one because it is easier to look at on the slide. We proved two expressions that look similar to this for a finite dimension. I'll let you stare at it for a moment to kind of absorb what it's trying to say. These are fairly simple expressions for all purposes in life, but you still actually need to find out what it means. Still actually to find out what it means in terms of the sample complexity. So let's do that. Sorry about the typo. I found it very close to the talk, so I just corrected with the big purple marks. If D is very large, which is, oh, I forgot to mention one thing. This expression for finite dimension approaches the upper one when dimension goes larger and larger. So these are related results. Results when d times one minus delta is very large, much larger than one. Roughly speaking, we're stuck with this kind of upper bound on the sample complexity. I think it may not be tight, but it is in terms of the dependence on one over one minus delta, maybe we can't improve it very much. And it is exponential in one over one minus delta. So for the example we have, it is one minus delta. It is one minus data is one percent, so we have four to the hundred in this parameter here. Of course, if you work with only one percent signal, you probably do not expect very good things in life. Likewise, if you have the other regime when d times one minus delta is much smaller than one, but dimension is large, then we have this kind of exponent, which is d instead of one over one minus delta. D is much smaller than one. Delta. D is much smaller than one over one minus delta when you have this regime. But you have this parameter inside as well. And d is in your exponent. But finally, in one regime when d times 1 minus delta is much smaller than 1 and d is moderate, then your sampling complexity is back to a kind of a polynomial of 1 over d and 1 over 1 minus delta. The exponent has this form. The exponent has this form, and when you plug d equals to 2, you get 4.8. You plug d equals 3, you get 6.2. If you get d equals to 100, you get 140 in the degree here. So that's kind of roughly what we get. And I think this is not necessarily tight-tight, but it is, you can't expect suddenly changing these things into lot of the problem. It's roughly speaking, this is the complexity here. The complexity here. Any question? Let me say that we got this kind of expression only around November and December after the initial submission to the archive. And we were initially extremely happy about getting the exact performance and working out the complexities until we see the exponents. The exponents. Last week, while I was preparing a board talk on the same subject, here comes a different idea. We think state tomography is the worst case scenario. Oh, yeah. At worst, you can do tomography. Learn everything about the state. If you learn everything about this state, you learn something about this pure state inside. You learn enough about this pure state inside. You bite the bullet and prepare a circuit to just prepare this state. Prepare this day, and let's see what happens. Let's see what happened here. So, first of all, if this is extremely small, like 1%, one in a million, then you first need to amplify the signal because tomography on this state will not be enough to give you enough good accuracy of the pure state component there. So, you expect the sample complexity to be a small polynomial of one over one minus delta. Then you output a description, psi pi. And you output a description, psi time, of a pure stick that hopefully is close to the actual one in trace distance. I forgot the one here. And just set it to be epsilon, that you want the accuracy to be for. And hopefully it's not too inefficient to prepare psi pi. And since you already have approximation, take one that is easy to prepare if need to. The complexity with this method is polynomial in D, polynomial in one over. Polynomic in one over delta and polynomial in one over epsilon with fairly small exponent. Thanks to David and Barbara for discussion last week at another meeting. So it is a little disheartening after all the hard work on the analysis of the recursive swap test for very high noise that state tomography seems to be slightly more optimal. Slightly more optimal. In fact, it's not more, it's not just slightly more optimal. The dependence on D if D is very large, or in one minus delta, if delta is very close to one, is a lot better with state tomography. There are other advantages with state tomography. The SOAP test relies heavily on having two states being exactly the same if the dimension is large and the noise is very high. I think tomography is fine if you have a little different. Is fine if you have a little different. I mean, if I have many copies of the state and they are just a little bit different, tomography still gives me something on the average correctly because it's always about averaging the signal to find out the classical description of the state. Also, you can do single copy measurement to perform tomography. It may run up the exponent for the Run up the exponent for the dimension by one. And so this is also perfectly streaming in the model. You don't need a lot of memory either. So there is some regime when tomography probably is the best protocol we have. Of course, we don't know whether it is optimal or not, but it is certainly due to our lack of imagination. It seems like currently we don't have anything that outperforms it. I'll perform it. It leaves an open problem that I did not manage to share on Tuesday. But I wonder whether state tomography can even be optimal in some way for purification. It ties back to this funny feature of the SIRA-Eckard Machiavello result that the optimal protocol with average single copy error. With average single copy error, seem to be slightly independent of the number of copies that they are preparing. And if tomography is optimal, we have the same feature that tomography, once you have done it, you can prepare as many copies as you want at the same local per copy average error. So I am very curious about this. Any question on this particular segment? I don't have a question on this particular statement, but more generally, if we how because how tensive is the other approach to let's say replacing the maximally mixed state by some other state or maybe even a more complicated sort of ansatz where you just know that the state you care about is the like eigenvector corresponding to the highest eigenvalue. Because I guess for the tomography of Because I guess for the tomography approach, this would still work in some sense. But I'm wondering if for the swap test approach, it's also the case. Because, for instance, I assume that if you, like, let's say, perturb with a very pure state, then the swap test approach will sort of break down, or am I wrong? In fact, that's one thing I want to find out. The analysis, the quick comment on The quick comment on having two copies, and you always have an improvement on the eigenvector corresponding to the maximum eigenvalue. It was done quickly two weeks ago. We did not ask that question earlier. I think it will be somewhat affected, but probably not too badly. I believe that the swap test is relatively robust if you have two identical copies of state. It does seem to be. It does seem to be somewhat unhappy when the state are slightly different. I do wonder how tolerant it is on that front as well, if the noise is not too severe. But let's say if we were in the setting where instead you get delta, let's say zero, right? Then the swap test might have some problems, though. It could be. I do not know up one. Okay. Yeah. Given that it projects to the symmetric space. Space. My just gut feeling is that if you have two states, and the difficulty with the analysis I showed earlier with that kind of leaf-like shape, is that you need to, when the deltas are different, you need to improve over the better copy, the better state out of the two. When the two states are of different noise, you need to use a noisier one to improve the noise of the less noisier. Improve the noise of the less noisy copy. I think that's roughly where the difficulty comes from. So I think it may be slightly more robust. The swap test may be slightly more robust than we worry, but I am hypothesizing here without enough checking. But hopefully this can be found out soon. Soon. The project seems to be evolving all the time. Every few months, if we think about it, then we find a few other aspects that we didn't think of before. So how are we doing with this? Oh, that's another question. Oh, yeah. So there's a method called virtual distillation mitigation. So basically, you can have like M copy of M copy of row, and then you can estimate expectation value by doing some global permutation operation, kind of similar to a swap test, but on like larger number of copies. And then you can get the expectation value of any, I'm not sure, some of you can get an expectation value of some of observable on rho to the power of m, that time. Rho to the power of m, that type of state. I'm not sure if that is somehow going to be helpful in this state tomography context or like this task in general. Not sure if there's any relation to that. I very much want to know myself. I suspect that there may be some regime when larger number of copies of state, and you try to perform a projection onto the symmetric space. Form a projection onto the symmetric space or a cyclic permutation inside of the swap may be useful. Can I say something? So I also asked myself the same question when Debbie proposed this. And I think maybe the difference is really that for virtual distillation, you are estimating. Can you say that again? Oh, sorry. I just wanted to say that for virtual distillation, you're estimating expectation values the same. Whereas here, your metric of success is actually outputting the actual. Success is actually like outputting the actual state. And I think classical shadows, or like shadow tomology in general, gives us reason to believe that estimating expectation values is like a strictly easier task than outputting the entire state. So I guess that's just my intuition. The output distribution just goes to be harder. I mean, sampling from the output distribution should be harder. So the curious thing is this. We have these three problems that seem to have very different complexity. The shadow, you want to have a prediction out of the shadow versus outputting a small number of copies versus tomography. We tend to think that these are widely different demands from easier to difficult. Too difficult. At the very high noise regime, I really wonder if somehow the separation of complexity actually collapse. Be very happy to discuss further with some of you. We can schedule a Zoom meeting or please discuss and let me know. So, open views and so on. And I may get. And I may get a little help on understanding just straightforward tomography of this type from David. And in a bit, he told me that he will send me some looks with some more exact detail as well. So we have more data points for comparison. Because currently, this kind of amplified tomography may be the best span we have for purification, knowing it also helps when we try to return to understanding which sampling. Share sample, which everyone seemed to agree that it ought to be optimal because you use the full permutation symmetry of the input. Let me finish the talk by a couple of minutes where how quantum computation meets quantum information here. And between me and Andrew Charles, you'll be slightly surprised that we have research overlap in that way. Overlap in that regard. It comes with a problem with query complexity with 40 oracles from about 14 years ago, roughly. As an example, for Simon's algorithm, when you have a dimension which is 2 to the m, m is the problem size for summons algorithm. I won't bother restating what it is here. What it is here. Your oracle, if it is noiseless, will give you the pure state, side S, when S is the output that you want to find in the algorithm. When it's noisy, if it has the kind of depolarized noise, you end up with a copy of the row delta that you have seen in the earlier part of the talk. Regoff, actually somehow put his first name here. Regoff in 2010 have reported on the fact that to Reported on the fact that, to the best of his knowledge, the best quantum algorithm requires exponential query complexity to solve Simon's algorithm, which means that the quantum advantage is lost if the kind of state that you have is of this form. The noise therefore he's looking at is not very high. It's something like 10% noise. Here, we provide a solution to his problem. For delta small than a half, you take that many copies of the noisy state, produce a copy of rho epsilon for epsilon, however clean you want, pick epsilon to be order one over m. It will suffice for the algorithm to give you a robust estimate of s. That gives back a polynomial. It goes from order m to order m square number of queries for Simon's algorithm. Number of queries for Simon's algorithm and only polyM size circuit. So at least we maintain that exponential improvement using quantum algorithm compared to classical. So that was the main drive over the project for many, many years. We knew it at the beginning. There are some more recent connections. One is the majority algorithm by Berman, Linden, Wenczinska, Montenero and Osos. Montenero and also. It says that if you have n bits that you don't know, and they are given to you in some kind of rotated version, but they're all rotated in the same basis, qubit by qubit. You have the output of majority in the same basis being rotated. And the problem for qubit turns out to be nearly equivalent to purification. If you know how. If you know how to perform purification for qubits, you have these n qubits given to you. You perform a random permutation, forget what permutation has been performed. You end up with the state that looks like the form that we have with the depolarized noise. Then you purify. And naturally, the correct eigenstate with the maximum eigenvalue is your majority output. And vice versa. If you know how to perform the majority. If you know how to perform the majority algorithm, you can just apply it to the convex combination with the purification problem. And your output should be the purity that you're looking for. So these two are very similar for qubits. And we hope our work provides a generalization for the quantum majority algorithm for larger than V, which is not very well understood right now. Maris and his students had some research on that from 2022. From 2022, there's an attack paper on that. And finally, the last problem is on cloning, which I mentioned earlier. And I think that's all I want to say. Thanks for your attention. Are there any other questions from the audience? Yes? Did you think about using Slightly larger version of which something to do the streaming, so going from two to three or four copies. Yes, I think they may actually give better performance. We have not sat down to done the analysis. Okay, yeah, for cottons, you have an unbiased estimate of the of the inbrush mid distance, which may be something very better for mixed rates than the darkness. Mixed rates than the parkers. In fact, if we have a similar problem and instead of purification, we have a mixed day signal here. The swap test will not give us the right output. The swap test will purify it to the maximum, the eigenvector corresponding to the maximum eigenvalue. Maximum eigenvalue. And we don't actually get the mixed output. For that, again, my best solution currently is tomography. And I'm not aware of other solutions currently. Yeah, maybe you don't just take the output corresponding to the symmetric subspace, but also the other ones, right? Yes, yes, probably. With the Schur transform, I think. With the SHUR transform, I think there may be a solution to make measurements and then combine the result back to a mixed data. Martin, please. Yeah, thanks for the very nice talk. I'm wondering why the high noise regime is so relevant because are there any applications? I would also expect that as soon as you have a little bit of noise in the measurements and the gates, then you. At the gates, then one would practically also be in trouble for other reasons. Comment on this. In fact, for a long time in the project, no one was looking at the high noise regime. Up until last year, when we were writing up the paper, we just realized that we can analyze it. Why not? It's more for curiosity, but it is if you come from. Uh, but it is if you come from the quantum information perspective, it is a very tempting question. How noisy it is for my life to be completely impossible. I think that again, oh yeah, no, thanks. Yeah, okay. Yeah, it is purely it's purely driven by curiosity. Yeah, well, that's not so good to be fair. To be fair, this question comes up in other purification problems. Metric state distillation is a different problem because there the circuit is noisy and your state is noisier. And entanglement purification is a threshold. You can't purify your entanglement if it's too noisy. Then you have classic communication. You know what state to purify it to, unlike in this case. But then it's a threshold. If there's too much noise in the entanglement, you can't purify. Can't purify, and here the problem is that you don't know this day. It is otherwise single, there's only one party, and there's no threshold. I think the fact that there's no threshold is a very curious thing. But I have fair little justification otherwise. Any other questions? Well, if not, then let's thank Debbie again for this great talk. Thank Debbie again for this great talk. And I guess we continue after lunch. So I just forgot the exact time. One second. So one. Okay, so we continue after lunch at one with pictures talk. Yeah, that's Yeah, that's true. Can we show this right proposal? That's it. Right, of course. But it looks not like we're gonna talk a little bit about the number sketch exactly on the top. Oh, yeah.  So now we want to get which