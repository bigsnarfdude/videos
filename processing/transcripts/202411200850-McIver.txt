coherence that embedded in the sky map, and the probability that these are truly astrophysical or terrestrial. So with that in mind, I want to give you an example from O4. And if you have an astronomer friend, it's probably likely that they've bugged you about this particular event. So this came through on the 22nd of April. So it had really promising qualities. Promising qualities. So, initial estimates gave it that it was highly likely it was a neutron star black hole. It was pretty nearby. The sky map looked pretty good, pretty well localized. And we have someone else in the crowd who had submitted this update to it. I think this came from Ryan. So this is an update informing the community that this is no longer consistent with an astrophysical event. It's now more likely to be noise. And this came through. To be noised, and this came through, I think it was 72 days later. So, this is something like three or so months after the fact, and there were over 90 circulars following up this one event from O'Court. All right, so here we introduce one possible solution. So, this is GW SkyMap. So, what it does is it takes in information that's encoded in a sky map, so a three-dimensional sky volume, and it's Volume and it assigns a score to it. This score is consistent across all of our pipelines. So we have a problem in the collaboration where it's difficult sometimes to interpret the information coming from different pipelines, but all pipelines will be issued a base star scan map, and all based on our scan maps can be used to issue a GW SkyMap score that can be consistently interpreted across all of the pipelines in our infrastructure. So zero is consistent with noise. Consistent with noise, one is consistent with an astrophysical signal. Keep that in mind. Now, what does it actually take in? So, it does have the limitation, the constraints, that we have to have more than one detector contributing to the sky map. Otherwise, as you might imagine, there's not coherence across the network, and G2B scanners not able to tell whether it's real or astrophysical. But here's one example: so, this is not confirmed in the Krabby 403, but also not retracted. Protracted very low score. So, this is GW skydot telling us it thinks this event is consistent with noise. And it's intaking three images. These are the sky volume, also the two-dimensional sky map, and we also feed it other metadata. So this is a multi-modal convolutional neural net. We're telling it which interferometers are contributing to the event, and we're also feeding it normalizations. So, in addition to the image data, we're telling it about the distance mean. We're telling it about the distance mean, we're telling it about the normalizations for the volume and other information that comes through with this base star schema. How does it work exactly? So it's a fairly traditional architecture for the convolutional neural net that's analyzing the images. And then again, we also treated these important normalization contexts. So, for example, you could get an image that looks pretty similar, but your need Similar, but you need this distance normalization to inform the algorithm whether it's near or far from the Earth, which is an important piece of information as it's making its decision. And I do want to say, so this is an initial prototype that was first put together by Marion Cabaro, who is now leveraging these same skills in industry, working for a satellite company in Vancouver. And this was trained on gravity spike glitches plus some threshold CBC candidates and simulated events. All right, and then enter Mervyn, our resident expert, who has taken this prototype and brought it into production and tuned it for O4. And I want to emphasize that this is not retrained with O4 data. So we had a lot of discussion yesterday about making models robust to changes in noise and changes in detector behavior. So this particular model, what Mervyn focused on was taking the difference between a two-detector network and a three-detector network up to where we knew we needed it. Up to where we knew we needed it to be for Virgo to join the run. So, this blue line that you see here, so this is the baseline model. This is Miriam's original prototype and a two-detector network. Essentially, with the MERVIN version, we have pushed this line of performance up. So we're getting that probability of a detection up to be consistent with what we saw for our three-detector network. So I'm kind of playing down how much work this took. What do you say, Mervyn? About six months. Mervyn, about six months of focus work. And we also trained this full four-era model on proprietary data. So we used the Mock Data Challenge 10 in particular to do some fine-tuning. We confirmed that it was performing well for three detectors. And probably the most time-consuming of all, again, to Marco's points about getting it running in production, is implementing it in the low-latency pipeline. And thank you so much to everyone who helps with that. This is a huge Everyone who helps with that. This is a huge team effort and a lot of time from the low latency team. Okay, so we've done all this labor. How well are we doing? So here we have GW Spanet score and probability for a few different, let's say, known types of signals. So the binary neutron star, neutron star black hole, and binary black hole. We are doing the best for the binary neutron star, which is sort of what you want to see for a tool that's meant to help astronomers. Tool that's meant to help astronomers follow up events. And you can see in these confusion matrices for each one of these types of sources that we're doing fairly well. And this is more than a score threshold of 50%. Is that right? So if this scares you, that these numbers are not 100%, don't worry. We've got you. So the score is not the only thing that's issued. We also tell you essentially the level of assumed risk. So if you want to see some examples. So, if you want to see some examples for how an astronomer can use this or how you with your tool can use this, you can check out this public output guide, which will walk you through step-by-step how to actually apply it. But the score is not the only thing that comes out. It's also corresponding to this false alarm probability and the false negative probability. And again, you can tune your risk. So, let's say I want to, so here we've got the points at which these two pairs. At the points at which these two pairs cross, I can reject 96% of noise, but then I introduce the risk that I'm going to miss 14% of my signals. If that's too much risk for me, I can just shift back on this line to wherever it is that I'm comfortable with. Let's say I only want to miss about 5% of signals. Now here, I'm rejecting again more than 75% of the noise. So you can scale this, tuned to your application, whatever your comfort level is, or how aggressive you want to be with your noise screening. To be with your noise screening, you can still be missing or rejecting about 50% of the noise with a very, very low risk with GW Sky9. All right, so how are we doing relative to some other common metrics? So Marco, I just want to confirm everybody here is part of the collaboration, right? Yes, but online. Online, I see not. I see you. Okay, in that case, we may just sit here and then skip the next few slides, just in case. I trust everybody online to not be taking screenshots, but just in case. So all of this is public. It's being recorded. Got it, yes. And we're being recorded. So stop. We can place some loud music. Everything we're about to say is fully public, so just feel free to take screenshots for the next little bit. But we've got this GDB screen. But we've got this GDB scanned score, so again, this is consistent with noise, this is consistent with a signal, and then we've got this P astro probability that you see here. These two stars, these are coherent waveburst PBH events. All the other dots that you see are from MASH filter searches projected onto this basis. And remember that GW Skynet needs more than one detector, so not every open public alert is going to be represented on this plot. So the places where Spot. So the places where it's interesting, where P Astro and G2B Scanet disagree, are in this region and would be over here, although happily we don't have too many events over there. And the events that have been retracted or updated to be low significance are in this pinkish red color. So GDB Skynet is doing a great job of predicting which events are going to be retracted. Even events that PAstra is very confident are astrophysical. G2B Skynet is nailing those events. So a couple of So, a couple events that maybe we'll leave for now. We'll see what happens in the catalog, let me put it that way. For this event, that's not retracted, and this event that's not retracted. And then we're going to drag by. There we go. Okay, and then coming back to our problem, so we mentioned this event that if you have an astronomer friend that probably bugged you about it, that we did see this updated version of the circular about. About months later, so the first SkyMap for this event was issued at 2136 on that day, and about 90 seconds later, GW SkyMaps had this very, very low score. So 90 seconds later, we already had this really strong indication that this wasn't real. So if you want to go back to your astronomer friend, you can say, well, we do have a metric now, it's running, it's in production, you can use it right now to try and screen these events. And screen these events. All right, and again to Marco's point from yesterday, so we had this prototype. So Mervyn pointed out that this paper was on the archive in 2020. So four years later, more than a year and a half after we got review sign-off for this algorithm to be in production in the LDK, and more than five months after we had all of the approvals for public annotations, we finally had cake day. So this is Mervyn with a cake and the date on the Mervyn with a cake, and the date on this cake, this is the very first automated public alert that contained public GW sky-net information. So, this is August of this year. And congratulations to Mervyn. I think it was a huge achievement and a show of determination to bring this all the way through into production in the low-latency software infrastructure. And we can also see, so this is an injected simulation, we can also see some potential. Simulation. We can also see some potential for using GW Skynet in terms of screening different events. So, which event do we choose? Is the super event? Is it the event with the best false alarm rate? Is it the event with best SNR? What about it's the event with the best sky map? So, here's an example. So, these are two G events that are associated with the same super events. And GW SkyMap has a very low score for the SkyMap that's not consistent with the true injected location. With a true injected location and a very high score for a scanner that is consistent with the real astrophysical event. So there's other potential that we haven't fully explored in terms of using GW SkyNet to help us to make some of these choices about what information we're displaying. Thank you, Cody. All right, so I have also promised you some, let's say, overview of explainability. Has anybody heard of explainability before in the context of machine learning? For in the context of machine learning? Okay, about half of us. So, the general idea here is that we want to be able to understand why machine learning algorithms are making the decisions that they're making. So, here is human understandability versus model complexity. And G2B Skynet is operating around here, the deep learning methods, the convolutional neural nets that are not particularly explainable, that we have to work hard in terms of trying to gain insights into what they're doing. Gain insights into what they're doing, particularly how a particular decision was made. It's very difficult, at least at the outsets, for CNNs. And then all the way over here on this side, we have linear regression, where there's one-tone mapping and you can understand how you arrived at a particular result. So here, I want to take you on a journey. So, this is a different version of GW Skynet. This is not running in the LVK sphere. This is a totally open source, open public project that was initially spearheaded by two. That was initially spearheaded by two undergraduate students at McGill. And as a project, this is for their honors thesis, they said, okay, what if we did GW Skyman, but we tried to also tell what the source was from the Skymap? To which I said, I don't think that makes a lot of sense, but you go for it. That sounds like a great project to explore convolutional neural nets. So this is what they did. They took GW Skynet, the glitch versus all, so glitch versus astrophysical, they made that They made that a model. They also have a model for, okay, if it's real, assuming that it's real, does it have a neutron star? And then finally, this BBH versus L model. So we're getting this, is it a glitch? Does it have a neutron star? Is it consistent with a binary black hole? And then at the last step, you get fed the class score that's the most consistent. So they ran this, they tested it, and similar to GW SkyMet, they did pretty well. So glitch versus all has this accuracy. All has this accuracy. We thought, great, okay, that makes sense. GDB Skynet also works very well. All is good so far. And then they heard the rest of it, and they also did really well. And the students were thinking, okay, hooray, it works. But my reaction was, okay, it works. Oh, no. Why are we seeing such high accuracies? How does it tell from the sky map what kind of a source it is? That doesn't make sense to me. I'm starting to get really nervous about what GW SkyMap proper. About what GW Skydown Proper is also doing. I'm also coming at this context as more of a machine learning skeptic. I really wanted to understand what was happening. So, the first thing that they did was this approach, this visualization approach called GradCam. We also use LIME, I think. So, essentially, what this is doing is it's taking this image, so this is an example of a SkyMap that's going to get fed to GWSkyMap Multi, and it will visualize for you where in the image is important for the machine learning algorithm to make its decision. For the machine learning algorithm to make its decision. So they did that, and we saw, okay, so it's looking at the sky map. Neat. And we didn't really gain a lot of insight. So we had to get very creative. We had to talk with some machine learning and computer science experts to do a totally new approach, something that was fully custom to what we were trying to figure out. So this is a study that was led by Noyar Gaza, who's at McGill. And the idea is that you take the inputs and you change it in some way that you control. Change it in some way that you control and you understand, and then you watch what the output is doing. So, for example, here's our original sky map. We can make it smaller in area, we can make it larger in area, we can scramble it, we can make it uniform, we can zero it out and take that information away and see what happens. And this is one of the ways you can tell what features are important and how they're being used. So, I'll give you an example. So, here is a GIF that should, yes, very good. That should, yes, very good. So, here we're changing the area of the sky map. So, it's going from smaller than reality to larger than reality. So, here we've got a small scaling factor to a large scaling factor. And this is telling us about the change in score. So, if the score is going down, this is in this framework more likely to be real, more likely to be a glitch. And so, for this particular example, this is the glitch model. So, it's telling us that a sky map that's smaller is more likely to be classified as. Is more likely to be classified as a real event. So, already we're getting some insights. And now I want to tell you the answer to the mystery. Does anybody think that they know already how JDB Skynet Multi is doing the classification between events? Okay, two people. Yes. You want to make the guess? Yes. Distance. Yes, you're absolutely right. I think, in fairness to myself, I did realize this before I saw this plot. But yes, that's exactly right. So, mystery solved. So, it's honing in on. So it's honing in on the distance. So here we've got sources that are injected glitches, injected neutron stars, binary neutron stars, and injected binary black hole sources. And we can see that, especially for the neutron star and binary black hole model, as we're changing the distance to be greater, we've got this divergence in terms of those two models. So it is the distance completely that's driving this. Five minutes. Thank you, Cody. All right. Alright, so another best practice that we learned through this process is the importance of a really well-understood training set, that you really need to understand absolutely everything that's in your data set when you're labeling it. So for example, does this image contain a glitch? So I want to quiz everybody here. Okay, does this image contain a glitch? What do we think? Yes. Okay, very good. I agree with that. Okay, how about this one? Yes. Okay, why do you say yes? Okay. Okay, so we think this is the glitch. What about here? Oh, it's bad. Yes, so this is actually. So Derek is smiling like they know what I'm... Yes, I recognize this about the time. So this is actually the camera shutter glitch event that Derek mentioned yesterday. And that camera shutter glitch happens right about here. So if you're a human and you're labeling this data set, If you're a human and you're labeling this data set, I might have said, okay, that's clean. And so would GravySpy, and so would gSpy.tree, which Margaret is going to talk about later today. So how sure are we that our data doesn't contain a glitch when we're labeling it as such? So this brings us to the need for fully described data sets where absolutely everything within the data is known, understood, down to the physical parameters, which we can tune in terms of the nature. Can tune in terms of the nature of the glitches. So, here I want to introduce two models. One is for common short-duration glitches. This is a paper by Versandra Bonderescu and her team. So, essentially, what they do is they model examples that are identified by gravity spy in the time frequency plane. And in time space, they will fit them in terms of the different parameters. So, for example, here is a series of distributions in frequency, and there are And frequency. And there are four parameters that are needed to completely describe these glitches. So, phase, which is constricted depending on the type of glitch, to be either minus pi, zero, or pi, to our discussion about phase yesterday. This is important for the model, but not varying a lot there. It's very modal. It's also frequency, which can range, vary through some range, the bandwidth, and the amplitude. But with these four parameters, you can fully describe. These four parameters, you can fully describe these common types of glitches. So, the blips, the tomtis, and the koi fish. There's also this model for scattered light. This is led by Rienna Nudal. So, nowadays, as Derek mentioned, we're seeing additional modulations to scattered light, but as a baseline, this is an excellent model that can reproduce these arches. So, you need to tell it how fast is the velocity of the surface that's scattering, and also how many arches. Scattering, and also how many arches that you want. And with those two physical parameters, you can model these glitches. So, what we've done, so this is an effort led by a summer student, Sean Collins, is incorporate these two models into what we call glitch pop, where you can simulate your Gaussian noise, your gravitational wave signals, as we have been doing for years and years and years, but now also you can draw the simulated glitch population. You can put a glitch exactly where you want it relative to. glitch exactly where you want it relative to a signal. You can slide it around. You can draw from a body of possible glitches. You can say I want 70% blips and 60%. That doesn't add up to 100. I would like some fraction of scattered glitches. I would like some fraction of blip glitches and do some Poissonian distribution. What we're left with is a time series that contains signals, glitches, and Gaussian noise that you completely know with an injection file that tells you absolutely everything about them. And you can characterize them down to the performance. them down to the performance on the flavor of different glitch, let's say high frequency scattering arguments. So this may be useful for a lot of the algorithms that we've seen mentioned already. If you'd like to check it out, take a look. This is a link to the documentation. And this is something that we're planning to leverage in future studies, especially to understand better what algorithms like GW Skynet do when there's a glitch in very close proximity to a signal. Okay, and then I'll leave you with the Okay, and then I'll leave you with the summary. So, we've learned the importance of these fully described data sets, the simulated universe where we understand absolutely everything about the time series. We've definitely learned the importance of understanding your model and especially trying to stress it out a little bit with something that could be very possible in practice, but maybe you haven't trained it on. So, for example, the center of an image or the center of a time series that you're feeding into it, how important is that? That you're feeding into it. How important is that? How can you make it robust? What does it do when it sees something that it's not expecting? That's within the realm of possibility. And also, how does it perform when you vary that background data? Okay, and then I'll leave you with one last point. So here's an overview of all the folks that have come in and out of the group since I arrived at UBC in 2019. I think to Marco's point and Jade's point, it's a lot easier to bring these things into production when you've got a big team. Our strategy is generally to have the junior folks. Generally, to have the junior folks, the students come in and try something new. And probably only about one out of every three or four projects will actually make it to production that reliably solve some kind of problem that we're trying to solve. And then the more senior folks in the group will then lead it on to production. So if there's something that you want to team up on on either one of those aspects, please don't hesitate to reach out. We'd be very happy to work with you. Okay, thank you. Piastro versus scoreboard, how does it fare against if you try to gradually lower the threshold of events that you're putting up? At what point does it start to get right? So when you say threshold, you mean the false alarm rate of the event? Yeah, I mean you said that the red dots were either retracted or updated too low. Tracted or updated to lower low significance. So, under the OPA threshold, yes. So, we adapt the OPA threshold. I will say, as you lower that, some of those dots that we didn't mention would go away. But a particular level of significance after which we can say that this is performing bad enough that we can't fulfill this threshold for this kind of cluster. No, actually, so GW Skynet, one of its greatest strengths is for the sub-threshold events, which I didn't show. So, GW Skynet in simulated studies outperforms Piastro, especially in indicating that something is real for the sub-threshold events. That's a great question. In the past, it's sort of been a tool is developed, it's sort of been decided on the science side that it's interesting, and then operations puts in production. I think that concept is going to change basically delivering the number of software species that we have. So it's a language where it develops tools and programming. Program. Thoughts on sort of how to get this science, but also from an early stage, we're just going to kind of discuss. Yeah, I think you're absolutely right. And I should say for the record that I agree that there should be a very, very tall threshold for review and production together. Review and production to get anything into the low latency sphere. So that's fair and appropriate, and I think we definitely want to keep that. I think the best place for something like GDB Scanit is probably within the architecture of the searches themselves, so that they're incorporating it into the likelihood or the down weighting, and that that information is already propagated by the time it hits ScriSDB. This is something that Mervyn, in particular, is starting to work towards. But yes, you're absolutely right. But, yes, you're absolutely right. We do want to find a better balance, especially with a global view from operations and also the data analysis side. Where are the biggest gaps? So, and I might make the case as somebody who's involved with GW Skydette that we're filling a critical need in terms of the time between the issued OPA, that's likely not real, and the retraction. That's something that we're bringing and that we're doing very well with. Although, it's, as you saw, it doesn't have an accuracy of 100%. As you saw, it doesn't have an accuracy of 100%. It will get it wrong at some point. We don't seem to have reached it yet, but it may. So, yes, I think we need it. And so, I'll also give a plug for Igwin as a possible way to have some kind of an oversight body that is well informed about what the resources are that we have to offer and has empowered to make this kind of prioritization decisions. But I completely agree with you that we need it. So, I forgot to take into account questions. So I forgot to take into account questions with timing. Oh, I think Sarah had a question too, and that's also the next speaker. Yeah, it was just a quick remark that I really like that you have to number, risk and opportunity, because it's something that we don't have for you, right? And as relevant, so we push a bit, we switch on that. We have this curve of risk and opportunity in the user guy, but I think it's important because we cannot make choice about astronomical. So this is all Mervyn's idea. This was all Mervyn's idea. Okay. So. All right, well, one last time, like Jess, please.