Okay, um good. So, good morning, everyone, and uh, thank you, John, for uh giving me an opportunity to speak. Um, so the work I'll present is, I think Whitney actually has done a bit of advertising, but um, a year ago, um, well, I've written two years ago with Chen Zhou, a colleague from Erasmus University, was invited to University was invited to write a review paper on risk and finance. And as you can imagine, that's a huge area. And so we had to narrow it down to something we knew about. So we kind of restricted the paper to extreme value analysis for risk and finance. And some of the illustrations I will show, they are in collaboration with my PhD student, Menlin. Uh, PhD student, Mandy and Joe. All right. Um, okay, so this is a slide I think I plagiarized from John. Um, so he, at the start of the workshop, he talked about this three themes for the workshop. And I'm going to make a disclaimer that in my talk, you will not hear anything about. In my talk, you will not hear anything about climate modeling or climate risk, but it will focus only on the bottom two boxes: the extreme events and financial risk. And so as I go along, so the reason why there's no climate modeling in this talk is financial data, if you work directly with financial data, it has, well, it's rather different from environmental. Well, it's rather different from environmental data. However, the risk measurement principles are often shared across disciplines. So it sort of allows you to bridge whether you are doing flood risk assessment or whether you are doing market risk assessment. The statistical concepts are often similar, but data properties are different. So you need sort of different modeling framework. Modeling framework. Okay, so speaking of risk and finance, well, that's just the possibility of an adverse scenario that will have an impact on financial stability of a financial institution like a bank or an insurance company, or we might be talking from maybe a regulatory perspective on financial impact on a market. And so, just from the definition of risk. Just from the definition of risk, it's clear that risk has to do with something in the tail of the underlying process or with extreme events that would ultimately lead to such adverse outcomes. And extreme value analysis in this case offers a natural modeling paradigm based on extreme value theory that gives you sort of mathematical models to describe extreme events. And on top of that, you And on top of that, you add modern statistical techniques to then try to solve various questions that arise in the context of financial risk assessment and management. So the goal for the paper I mentioned and for this talk is to share with you some of the advances in extreme value analysis that have provided solutions to some of the problems that we've encountered in financial risk management. Financial risk management. Now, trying to narrow the scope, I just wanted to show you a bit of landscape. And so when we speak about risk and finance, we often think of different risk classes or risk groups. The oldest one that banks have been concerned about is credit risk. Is credit risk. And then there is market risk and operational risk. And there are other risk categories such as climate risk. So maybe in a year or two, now that there's a lot of work going on, we'll add climate risk. But these groups, the reason I identify these ones is because they sort of lend themselves to the possibility of quantitative assessment. There are certain risks like reputation risk. Certain risks like reputation risks, which are harder to capture, and certainly not for someone who's with a statistics background. And so the top three credit market and operational, they are the risk classes that fall also under regulatory scrutiny. And I'm put an asterisk next to operational risk because, for, I don't know, a decade or almost two decades, a regulatory Regulators tried to make banks set aside capital to safeguard against operational risk, but this is very recently discontinued now. And we've done a project with Scotiabank where we help them with modeling. It's a difficult problem. And as I mentioned in the first day, this is something where we failed. It's just you don't have enough data and it's a very difficult modeling problem. And the other class of And the other class of risk is systemic risk, and something that has been on radar for a while, but really kind of sparked, came to the forefront in the aftermath of the 2007-2009 financial crisis. And I'll speak a little bit more about that. Now, you do have to, so this different risk categories, you basically have to treat differently. They have very different. Differently, they have very different data availability situations, and the data has different characteristics. So, in the case of operational risk, we're actually more dealing with something that we can borrow ideas from actual science modeling losses. We have rich data for market risk, and so I think a lot of this, so the first beginning, I'll focus on market. begin at the beginning i'll focus on market risk and you'll see there's there's reach data and credit risk is is um tricky so i think eva is not um it isn't really some extreme value analysis isn't something you can use for credit risk but um there are again uh we've done um quite um some projects for scotia bank and especially with my uh colleague professor harry joe uh where there is a need for sort of There is a need for sort of machine learning techniques. So the bank might be interested in, well, if they are considering giving loans to small companies, they have a lot of features of variables that come from the balance sheet of these companies, and they want to be able to classify whether a particular company will fall under high default risk or low default. High default risk or low default risk. So we're talking about classification problem, we say 11 categories. And so different statistical techniques would be needed to answer that. And so I'm not going to speak about that. And for when it comes to systemic risk, one prerequisite is we can do that, but we need companies that have some market indicators. So it's publicly traded company. Publicly traded company, so we have stock prices, or we could use CDS spreads. All right, so here is an outline for my talk. I'll start with a kind of a very basic illustrative or motivating example where I'll talk about measuring risk of an investment portfolio. This is a situation where we do have time, we have data, and so I will speak about properties. Will speak about properties of that kind of data and how does one go about measuring risk. And then there will be three sections where I'll speak about univariate extreme value analysis, multivariate extreme value analysis with two illustrative examples, and then I'll have one slide to tell you what happens when you have serially dependent data. So we obviously deal with serially dependent data. Zero dependence is different in nature from environmental data. From environmental data. So, if we talk about river flows or precipitation, financial data has very different properties and needs to be addressed differently. And then I'll speak about a few sort of open areas where we saw there's sort of short-term, medium-term needs that would be ideal to fill. All right. So, why does one actually, why would one be interested in? Why would one be interested in measuring risk of an investment portfolio? Well, Paul mentioned that banks sometimes do things because regulators tell them to do so. So it's true for market risk. So we need some sort of measure of portfolio risk in order for the bank to set the capital that is required by regulators. They're also interested in sort of using that information as guidance. Using that information as guidance for internal risk management decisions. You don't want your traders to take up too much risk. You want to set some boundaries. And while there are sort of some kind of non-stochastic, kind of basic ways to do that, which smaller banks might be doing. Bigger financial and bigger banks, they go for sort of quantitative methods. Methods, sort of statistically grounded methods to do that. And that's usually based on the loss distribution. So if I consider an investment portfolio of assets, think of a portfolio of containing stocks and bonds, I'll use X as a sort of generic random variable to denote loss in the portfolio over a fixed time horizon, say 10 days. And I assume that the portfolio composition doesn't change over that time. And then the change in That time. And then the change in the so the change in the value of the portfolio over that horizon would be your loss. And the distribution of x is known as the loss distribution or profit and loss, but I always put a minus sign. So for me, losses are in the upper tail and x is the loss random variable. Now, the way that risk is quantified in this context is by means of risk. Some risk functional. So it's some functional on the space of lost random variables. In finance, this is known also as risk measures. And popular risk measures used in practice are value at risk. So until, I want to say recently, until 2016, it was the standard measure of market risk in banking regulation. And essentially, it's And essentially, it's a high quantile, right? So, the bank wants to have that one number so that the probability that the loss on your portfolio will exceed that number is small. And this for those working with environmental data in sciences, we typically speak of return level. So, this is an equivalent of that for a fixed probability level P. probability level P. Now, this has been so value at risk came under criticism because effectively it tells you sort of the it's a frequency measure. It tells you that, so for that particular loss, you have a small probability of experiencing a loss larger than that, but it doesn't tell you whether the loss larger could be very, very large or is just barely large. So it doesn't really tell you anything about the tail. Tell you anything about the tail. And there are some kind of economical considerations like incoherence and other things that people have said about value at risk. And so a new measure was suggested known as expected shortfall. I guess that's a European notion. Conditional tail expectation is something more common in North America. But it's trying to take the whole tail into account. Take the whole tail into account. So, given that your loss is above a particular threshold, given by value at risk, quantile, what's the expected value of the losses then? Now, insurance companies were quick to adopt this, but banks have taken their time. And now, four years ago, they've adopted that. Well, actually, what is it now? Six years. However, there came a lot of However, there came a lot of statistical challenges with that. And one is that it actually turns out very hard to backtest expected shortfall. It's hard to check whether your expected shortfall forecasts are actually accurate. It's much harder to estimate expected shortfall than value at risk. So as of now, banks are required to use expected shortfall to come up with capital requirements, but they are backtesting using value at risk. Are backtesting using value at risk. So, but that's the situation. We're making some progress on helping with trying to backtest expected shortfall directly. Now, just to give you an idea how extreme this risk measures are. So, for market risk, we are speaking about P being 99%. So, one probability of 1% being more. Of 1% being more extreme than bar, and expected shortfall comes with a smaller probability level. It's actually computed under normal distribution to match that you would have the same capital. But that's just a little bit arbitrary. And in some situations, P has to be very large. And this is nothing we don't see in environmental applications. So, as you know, the dikes in Canada set to one to two. That's set to one to 200-year event, so that's 0.998. Landslide criterion is one in a 10,000-year event. And I think there were, well, okay, I will not speak about landslides, but Dutch dikes are set against one in a 10,000-year event. So we are talking about really estimating tail events here. Now, what does the data look like? So, what you The data looks like? So, what you see in Blue Day on the top right panel is a sort of a classical financial time series where you see periods of high volatility. And if you were to look at, say, normal QQ plot, you'll see immediately in this figure that the tails are not normal, they are much heavier than normal. And to sort of a useful way to look at the tail. Useful way to look at the tail is through a log-log plot. Now, if your data were sort of like a power function and see, so like a power law behavior, if you were to produce this log-lock plot, you would get a linear function with the slope minus alpha. And that's exactly what you see. So these are losses above a threshold of 2% loss. So this data is. So, this data is heavy-tailed. And mathematically, we want a convenient characterization, which is usually done by means of assuming so-called regular variation. So we say a distribution function has regularly varying tail if the survival function satisfies this limit relationship. And this kind of assumption is made only on the tail, and it includes a wide variety of distributions. Variety of distributions, the well-known students t distributions, QT, Pareto, Law Gamma, and others. Okay, so heavy-tailed law behavior. We see a lot in financial data. Debris flows actually also have heavy-tailed behavior. All right. So now why do, so this is regular variation is an assumption on the tail. Now, the reason why this is a Now, the reason why this is a convenient assumption is that it allows us to extrapolate from sort of not very extreme levels to very extreme levels. So I've written here a ratio. So if you think of the numerator, so 1 minus p is probability that my random variable x exceeds var, so quantile. And I do the same for level q in the denominator here. Here and this is assuming regular variation. If you think of f of x kind of being a power function, but only in the limit, so that's why there's the approximate sign, then you get this ratio. And from here, we see that we can compute value at risk at level P from value at risk at level Q. And I can say Q to be smaller, not as extreme. And then I just have to upscale it, push it into. Scale it, push it into the tail with this factor and the power one over alpha. So, and this is this is actually the whole premise of extreme value analysis. What we are trying to do is we are trying to come up with some mathematically justified mechanisms of extrapolating so we can do estimation sort of not very far into detail and then extrapolate beyond what we see in the data. So, a kind of So, a kind of central question in a lot of extreme value analysis is estimation of this parameter alpha of the tail index. And different ways you can do that. I'll just mention there is a Hill estimator, which is quite popular. It's derived on the assumption of regular variation. And typically, so it's estimated using K largest observation. In K, largest observations in your data. Well, the choice of K is sort of a Heel is heel in a lot of this analysis. It's kind of a tuning parameter. It's always there in extreme value analysis. It's either a threshold or how many largest observations, where does your tail start? So typically for estimation of tail index, we do so-called heel plot. We do so-called hill plot. We plot the values of the estimator for various k-values, so that's how many upper-order statistics here I'm going to use to compute my estimator. And then I look for that stability region where estimates don't change much. Fix that small k. So k has to be much smaller than the sample size so that we're really in detail, but it also shouldn't be too small, otherwise estimation is too volatile. Is too volatile, and then you get your estimate of the tail index. And there's been a lot of research that's gone and trying to help with choice of K. There are bootstrap techniques which work well in simulated data. I've talked to a number of people who say for real data, we found they don't really work. Choice of K is an issue. There are also estimators that try to address the bias. So, Hill estimator comes with sort of various. Serve various statistical challenges, but so that's just to say, kind of still an active area of research, and there's been already a lot of work done. Now, I showed you financial data and I demonstrated that it's heavy-tailed. I want to link this to some of the things that Whitney and Francis talked in the first day. They're sort of a, it's actually a kind of a Actually, a kind of a small sub-area of extreme value analysis. There's a bigger picture. And so I want to draw connections. And I think you were actually interested in generalized extreme value distribution. So here's another view of that. So if I have IID data and I consider the largest observation in my sample, then in extreme value theory, we use this definition. Um, theory, we use this definition of maximum domain of attraction, where if there is a non-degenerate distribution function g which will appear in the limit and normalizing constants, then we can normalize maximum and its distribution will converge to g. And so this is very much like central limit theorem, where we subtract the sample average, divide by sample standard deviation, and the distribution of Deviation and the distribution of the average will converge to normal distribution. And this is just an analogous result for the maximum. Now, the Fisher-Tibet theorem, again mentioned by Twitney, tells you that if such limit exists, it must be the generalized extreme value distribution up to type, up to location and scale. And here's an expression of the CDF. Now, if ψ is positive, then g of ψ corresponds to the first shade. Geoff psi corresponds to the Frochet distribution, so that's for all heavy tail, the limit you would get for heavy tail distributions, and regular variation of the tail is the necessary and sufficient condition for being in the first year domain of attraction. So that's your link. In finance, we kind of like to, if the data is heavy-tailed, it makes sense to restrict attention to that subclass. But in general, for many environmental applications, it's important that we sort of work with a general. Sort of work with a general generalized extreme value distribution or with real value side, not just site positive. All right. Now, this domain of attraction assumption, it's actually fairly mild for continuous distributions. I would challenge students here to give me a continuous distribution which doesn't satisfy that condition. So it's a very, very mild assumption on the distribution. On the distribution. So, in the case of central limit theorem, you know that you need the existence of the second moment, but so here the domain of attraction condition is fairly widely applicable. Now, if that is true, then you can also show that the conditional distribution of excesses of my random variable over a large threshold, that could be well approximated. So, there's a convergence in distribution to so-called generalized. Distribution to so-called generalized Pareto distribution with shape parameter xi. And this is known as Peekens-Baukama-De-Hunt theory. Now, when the Fischer-Tippet theorem motivates the use of block maxima approach, where we model block maxima for many environmental data, if you're interested in annual maxima, monthly maxima, then the Pickens Volcoma, the Hunt theorem and Balcoma-de-Hahn theorem and generalized Pareto distribution are used when you define extremes as not the largest over a particular period of time, but when you define extreme as the value of your process above a threshold. And yes, hydrologists have been actually using this for before mathematicians caught up and produced a lot of theory. Theory under the term peaks over threshold methods. So peaks over threshold. And so I put in a few here elementary probabilistic details, but so f bar is my survival function, and then I have conditional survival function. And then probability that my process, my random variable X is above lead. probable x is above little x could be decomposed as probability that I first exceed the threshold u times that given that I'm above the threshold that I'm then exceeding the excess is more than x minus u. Now putting that all together in the context of value at risk so this is my definition value at risk think of it as a quantile probability of being above is one over p, one minus p, and then I can decompose Minus p and then I can decompose this and then I will approximate this conditional distribution with generalized Pareto distribution. So, whatever the underlying distribution is, I don't know it. It's hard to figure out what the true model is. But in most cases, on the very, very mild assumptions on the underlying distribution, that the main of attraction condition, once I take a large enough threshold, I can approximate tail with generalized Pareto distribution. Distribution. And so then, if I want to compute value at risk, that's roughly given. So I'm going to invert things from the cumulative distribution function of generalized Pareto. And again, I'm sort of doing extrapolation here. So this F bar of U is probability that I'm above a threshold. And then there is a way to start at the threshold and extrapolate. And this is your peaks over threshold high-quantile estimator used in. Estimator used in many areas in hydrology, where you try to estimate return levels for peak flows or stage precipitation. So in many environmental applications, people use this estimator. And it's an alternative to the one I showed you for regular variation, except that it allows the psi to be some real valued, not just positive. And it's important. Positive, it's and it's important to keep it that way in environmental applications in finance, not as relevant. All right, any questions about this? Okay, so that's what I want. So I've shown you sort of univariate financial time series. I told you about how to compute risk measures, and I told you. Risk measures, and I told you how extreme value theory can be used to estimate those risk measures. So, I didn't speak about expected shortfall, but one can sort of derive similar results. And this can be estimated. So, the xi and the sigma, those are parameters. One can use maximum likelihood estimation, a Bayesian inference. In old days, people used methods of moments and variations on those. And variations on those, and you get your estimator of high quantile, which is going to work a lot, a lot better than if you were to try to use an empirical estimator, something banks actually still do, or if you try to use normal distribution. In early days of financial risk management, people just fitted normal distribution and took that quantile. This is going to work a lot better. Yeah. So far, I think I have a lot of um calculation here based on you know the core law assumption. I think it should probably spell this out on slide nine or something. Okay, go back. So I wonder, yeah, I think this is the power of I would call this power. So based on the data, is there ways to verify whether this is something called To verify whether there's something for basically, I'm asking whether there's some model that was good. Yes. So, I mean, there are, I think, so there are formal tests you can do, but there are also exploratory things you could do. So, you could try to even fit a generalized Pareto distribution above a threshold and see if your XI is positive. And if your XI is positive, the fact that you that means that your tail is regular. That means that your tail is regularly varying. Yeah. And also, actually, because here, you know, the R actually shows the power of position, right? So it's a precision actually matters a lot. So in the theoretical, in theory, is there a way to calculate or quantify the variation of the yeah, absolutely. Yes, yes, that's very important. So estimation of R. Very important. So, estimation of alpha is a hard problem, but you could see a confidence bound. So, the Hill estimator is consistent and asymptotically normal. So, you do get all the nice statistical theoretical properties that come with that. Now, in terms of sensitivity, so you could see the actual alpha is actually, well, so depending on where you pick your tail, you'll get different values. But on the Different values. But on the positive side, the quantile estimators actually not so volatile. So, whenever I do that, well, with value at risk or if I compute return levels, we always test for sensitivity to threshold choice. And for large enough values, you have quite a bit of stability. Yeah, so I think the fact that you, so if you look at what you're ultimately interested in, the sensitivity is less than the. Sensitivity is less than the estimates themselves. Yeah, the sampling variability is reduced. All right, so now how do we go from univariate to multivariate setting? Well, so when it comes to extreme value analysis, what we need is we need to be able to order observations, right? I need to be able to say what is small and what is large and what is extreme and what is not extreme. Extreme and what is not extreme. And for univariate data, there is sort of natural ordering, right? I could compare numbers on the real line and then I can extrapolate into detail along that line. In multi-period case, there is actually no natural ordering. There are different ways you can order your data. And what that means is that there are actually various ways to define what you mean by an extreme observation in your multidimensional space. Multi-dimensional space, and therefore, there have been many different approaches proposed for representation of multivariate tails. So, we're no longer looking at the representation for sort of univariate tail above a threshold, but so what is that multivariate threshold? And I think what you would notice, so the threshold shape would probably be dictated by an application, but in terms of But in terms of the model and representation, it's the teledependence that often determines what kind of representation you will use. Now, the regional, so in the kind of at the time of emergence of multivariate extreme value theory, people have concentrated. So, we went from modeling univariate maxima to modeling multivariate maxima where we define a multivariate maximum coordinate-wise. So, you take the Maximum coordinate wise. So you take the largest observation and your one variable, and then you take the largest observation in your second variable, and so on. And then we can study asymptotic behavior of this multivariate coordinate-wise maximum with suitable normalizations. And the limiting distribution, so unlike in the univariate case where we had generalized extreme value distribution, in multivariate case, you no longer have that one magic family. That one magic family that can approximate any tale. So the marginals would still be generalized extreme value, but the dependence can be many different things. And so the only thing we know that the limiting distribution when the marginals and in the mean of attraction, the dependent structure satisfied the assumption of multivariate regular variation. And again, that could encompass many, many different. That could encompass many, many different models. And what I'm going to mention here is that the block maxima approach may still be useful. So in environmental applications, we often have that natural block structure. So if you look at annual maxima. However, Harry was asking Whitney a question about the concern is when you compute this multivariate maxima, the extremes may not be concurrent. Streams may not be concurrent. So, the coordinate-wise maximum may not have the interpretation you want. So, if you're looking at that largest wind in a year and largest precipitation event, they may occur on different days, different storm events. So, the coordinate-wise maximum may not really be the quantity you're after. So, the block maximum approach in many situations would not really be applicable, but the assumption of Really, it be applicable, but the assumption of multi-virtual variation is still widely used, and I'm guilty of using that as well in many things I've done. So, I'm not going to dwell a lot on what that assumption is. It's a generalization of that heavy-tailed behavior to multivariate setting. It in particular implies that the direction of extremes is independent of the magnitude of extremes and along any ray. Of extremes and along any array or all marginals are regularly varying with the same tail index. That's roughly what it means. I'll skip this slide. So this is more of a remark, but so the multivariate regular variation is useful if your extremes are tail dependent, if they become large at the same time. Now, if that is not the case, as Whitney Is not the case, as Whitney mentioned, and you remember I asked him that question, but it seems like in some of the environmental applications, you don't have that terri-dependent. So you need some representation for asymptotic independence situation. And there are different approaches, different models, something known as hidden regular variation, where you restrict the space where you have a regular variation. Conditional extreme value models have been used. Value models have been used. I'm not going to talk about that because for the financial data, I'll show you the multi-tregular variation assumption is actually reasonable and we've done model diagnostic checks. So that's good. So I'll start with my first application is on measuring systemic risk. And so multivariate extremes, the study, what goes Extremes. The study goes back to the 1980s. British researchers have done a lot of work on coming up with parametric models for the tails. And in finance, I think systemic risk is probably kind of, well, we're talking about already 21st century. I mentioned in the financial crisis in the first decade of this century. So it sparked a lot of interest. And that's where we started. A lot of interest, and that's where we started seeing that multi-direct extreme value analysis could be useful. And so the financial crisis has kind of highlighted the extent to which systemic risk can affect global economy. It also identified inadequacies of the existing risk management techniques where regulators were happy with companies on their own. They seem to have sound risk management practices. However, when you However, when you look at your financial system together and when things started not going well and you have companies that when this, so regulators define them as systemically important financial institutions, when one of those goes under the water, it's going to affect the whole system. And so that's undesirable. We don't want to have those kind of players in the system. Okay, so how does one measure systemic risk? Well, there are many different ways. Many different ways. There are also many different definitions and many different aspects of that. So, as a statistician, I just, I guess, chose a convenient functional that has been introduced by Adrian and Brunemeier, known as COVAR, conditional value at risk. And COVAR is defined as, again, a high quantile. So, probability that why is the loss on the market, that the loss on the market is large. Is large given that financial institution is in distress. And distress we define as event that the loss on the financial institution exceeds some high quantile value, value at risk. So that's cobar. So how does one estimate this conditional quantile? So yes. Yeah, yeah. So you're looking at different what you're describing is a measure of vulnerability. So a bank may be interested to see how vulnerable they are to the market. But the regulator wants to know for they consider a particular bank and they want to see if that bank, if they were to be in distress, whether they're going to affect To be in distress whether they're going to affect the system. You don't want to have companies that are too big to fail, but if they fail, there will be a resonance in the system. So you can actually, I mean, from my perspective, I don't actually care what X and Y are, right? I will just go take the data and go about estimating that quantity. But you are quite right. There would be an interest in both directions. Yeah. yeah can i also think of x and y percent who different but dependent upon x yes yes so you could see how uh one company is affecting the other and i think in taking this to environmental applications you could see if there is a strong wind does that go together with uh large wave heights and so on yeah so you could you could absolutely study that yeah put different interpretations Put different interpretations. Yeah, and so for me, I'll just put the head over the decision and forget what X and Y are. But I'll try for this talk not to forget that. Now, co-var, so when Adrian and Brunima introduced this, they actually used a slightly different definition where they use the X is at var level, and then it allows you to use quantile regression techniques. And we've heard a little bit about that. Quantile regression has some Quantile regression has some challenges. But the interesting thing is that if you go with x at a particular value, there actually has been research shown that there are some inconsistency, independence. You could have stronger dependence, but that's not reflected in COVAR. So this is a much more robust definition, more appropriate definition to use. So then there was a paper by Gerardiner Gunn. Paper by Gerardiner Gun, where they used a fully parametric approach with a sort of bivariate garage filter, a very flexible distribution for innovations, by variety QT. So a fully parametric, very flexible model. Then I had a master's student, Jin Yu and Zhan, and we thought, oh, how about we try to use extreme value analysis? So we came up with an EVT-based approach where we use multivariate regulatory. Where we use multivariate regular variation, and we showed we do better. So, I want to mention when it comes to parametric modeling, if the model is correctly specified, you don't need extreme value analysis. Use maximum likelihood, use the correct model that captures your data, that's all fine. So extreme value analysis is really about cases where you may not be able to capture your data with one model entirely. One model entirely. So, we are then trying to only model the tail. We let the tail speak for itself. And so, in those cases, it's a useful framework. Now, what I wasn't happy about, we've been very careful selecting the data sets to make sure we do have multi-regular variation, but we did see that the market indices tend to have slightly lighter tails because of diversification than individual financial institutions. So, there was a kind of need to. So, there was a kind of need to can we do something where we relax the assumption of multi-vert regular variation and allow tails to have different tail indices. And so we've done that kind of extension with Men Lien, Joe, and Chen also joined us actually after this risk analytics day we've done, John. Remember, first time in Toronto, so we had a discussion on that. And so we modeled tail using upper tail dependence function. So it basically describes tail dependence. We assume the market index is regularly varying. And with these assumptions, we built an estimator. And I will not go into a lot of detail, but the trick was to note, to realize that covar is a conditional quantile. And you can probably match it to the univariate quantile at a different Univariate quantile at a different quantile level. And so we've came up with this idea of an adjustment factor, which we call eta P. And the eta P is the quantity that is going to capture the tail dependence or the strengths of tail dependence. The stronger the dependence, the more the bigger this adjustment. And if you have independence in the tail, then this conditioning will not affect. This conditioning will not affect that. The P will be one, and then there is no adjustment, and you can just compute COVAR using market index. All right. So just to illustrate, I will not go into any details. The estimator we've derived, we've proved consistency, and we've finally managed to prove asymptotic normality, which reviewers wanted. Anyway, so I'm going to look at 14 institutions. I've got I've got 20 years of data. We use SP as our market proxy. And so there's a list of institutions. But what I'm going to show is these are called traffic light matrices, and they were inspired by what the regulators of market risk also had this different lights in terms of is your company. In terms of is your company passing the back test or not passing the back test. So the way this works is I have a reference method across the columns and I have a proposed, think of any of this as kind of a competitive or proposed method across the rows of this matrix. And then I'm considering here, so this T asymmetric logistic bilog. Asymmetric logistic, biologistic, customerized and logistics. Those are different parametric models I'm assuming on the tail dependence function are. So these are models just for the tail. I'm not modeling the entire distribution, just assuming what parametric form for the tail, but here a tail. And that allows me to compute that adjustment factor eta. And the top to this FP is for fully parametric model, and EBT and Z is for the T and Z is for that multivariate regular variation-based extreme value model. And so the way you interpret this, you go across. So this green light tells me that this EVT and Z model outperforms, so has significantly better predictive performance than using our proposed method with the logistic dependence structure. It also outperforms the Hussley-Rice dependence model and biologistic tail dependence model. Logistic tail dependence model. It performs worse than our method with asymmetric logistic or with the bivariate t dependence. And that's not surprising. So these two models have two there they have two parameters versus a single parameter tele dependence function. So we kind of expect we do need we need a symmetry. We need at least two or three parameters to capture teledependence. And then And then this EVT model does better than the fully parametric. So, and this is for MBI institutions. Now, I think this was, so in many cases, backtesting, sort of showing statistically with significance that one method outperforms the other is very difficult for COVAR. It's easier to do when you look at value at risk. The way it's done is you, let's say. The way it's done is you essentially look at how many values exceed in your sample. So you do this out of sample, and then you try to see if with a loss function or scoring function, if you're outperforming, performing. But we have very few observations when it comes to covar to do back tests because at first we condition on being above var and then that leaves only a small fraction. So the results are not always statistically significant. So we often end up with this yellow. So we often end up with this yellow colored regions which tell me that the scores are not significantly different. So I can't tell you which method is actually better or worse. So what we've done is for those 14 institutions, we wanted to see if any method would kind of dominate. And it turns out that our approach with asymmetric logistics dependence model, asymmetry is actually not surprising. We do see asymmetry. Actually, not surprising. We do see a symmetry in the data outperforms for this sample of institutions, all the other methods. So, anyways, that's encouraging. The method seems to work. All right. And my final illustration is actually a new problem, which is I personally found very interesting. It's again something that came up with our work with Scotiabank. Is Scotia Bank, and they are interested in what is known as reverse stress testing. So for a while, banks were trying to make decisions based on, well, you can call it traditional stress testing because it's been used for a while. So you come up with scenarios and then you try to see what are consequences on your portfolio or your institution. Now, how do you come up with stress scenarios? Scenarios. So the choice is sometimes arbitrary, right? You go with this economic conditions, this inflation, and that. So how do you choose those values? And so you can also come up with scenarios which are perhaps so extreme that they're not going to be realized. So why even consider that kind of perfect storm scenario that may really not occur? And it may also exclude scenarios. It may also exclude scenarios which actually will lead to adverse outcomes, which are possible, plausible, and could have negative consequences. And so, again, I mentioned a lot of regulation, a lot of ideas come in the aftermath of different crises. And so, that was a time when limitations of traditional stress testing were revealed. So, the idea was to reverse the process rather than Reverse the process. Rather than you tell me scenario and I'm going to see what happens to my portfolio, you tell me what your adverse scenario is. And by the way, in environmental application, I can think of this given that there is a flood, what the consequences are. So here you tell me, so say, assuming that the loss will be above a particular high threshold, what scenarios, what values of risk factor could lead to that? Could lead to that outcome. And this is sort of something that supervisory authorities are now emphasizing. And in my discussion with Philip Ashby from Scotiabank, he mentioned they would be interested in using that in internal risk management decisions. Again, perhaps setting limits on trading and so on. Now, I think there are different ways you can define stress scenarios. So, from the regulatory documents, there are kind of two. Regulatory documents, there are kind of two criteria. One is you want them to be probable, right? You don't want to consider a scenario that is so unrealistic. Why even consider that? So we want this probable scenarios, and we want to condition that on that adverse outcome. So one way is to consider the mode of the conditional density. So if x here both face, it's some d-dimensional several. Some d-dimensional several changes in risk factors, and I'll give examples of risk factors. You have a portfolio loss, some real-valued function g. In academia, we like to think of g as linear. We, I guess, we're not creative enough to come up with other loss functions, but Scotia Bank shared with us real data. We don't know. We treat G as a, it's a real valued function, but it's black box. Bank evaluates, so can tell us what the value of. Tell us what the value of the portfolio is for given risk factors. And so we're interested in estimating this mode of the conditional distribution, and that defines the risk scenario for large L. So our starting point in this work was a nice paper by Glasserman, Khan, and Khan, where they've proposed to assume that you have joint elliptical distribution. Elliptical distribution. So elliptical distributions are kind of a natural extension from multivariate normal to something that still has elliptical symmetry. When you think of the level sets of, say, t-distribution or normal, they are the same, but you could have any tail behavior. And a lot of the time we're interested in heavy-tailed behavior. And so what they showed is that under this elliptical symmetry and linear portfolios, you can actually build a connection between conditional mode and conditional expectation. Conditional mode and conditional expectation. And conditional expectation is a lot simpler to estimate than mode of a density in several dimensions. So, which is nice, but then they assume they estimate this conditional mean empirically. So, you imagine if you push threshold, you have very few observations or none at all to estimate. So, felt like, oh, this is where we can use our hammer of extreme value analysis. And so, we've proposed. And so we've proposed an estimator with, you know, the same idea that we bring in threshold into the data, estimate conditional model, and we mode, and we use a non-parametric multivariate mode estimator, and then use multivariate regular variation to allow us to extrapolate into the tail. And just to show you a few plots here, so this is for simulated data from Bavira T, which is Which is elliptically symmetric and so satisfies all assumptions. We see fairly. So, this is bivariate case. So, I have a bivariate model mode, two components. So, I'm have a sample size of a thousand and I'm taking a threshold of 0.001. So, I expect about 10 observations above that threshold. And so, the methods. And so the methods are fairly comparable. However, if I go into for more extreme thresholds, and again, extreme here should be viewed in terms of the sample size. So if I have a very large sample size, then looking at 1 in 100 or 1 in 3000 is not so bad. But if the sample size relative to that probability level, the magnitude of the threshold is small, then we might. Is small, then we might be needing extrapolation techniques sooner. And so, what we see here is that the solid curves correspond to our method and the dotted ones at this JKK method. We get reduction in variability of our estimate, and we tend to capture the bias tends to be fairly small. Now, if we go to bivariates QT, so By variance qt, so it deviates from the assumption of elliptical symmetry. We find that under not very extreme thresholds, again, both methods seem to do well. You push the threshold further. There's a lot more spread for using the empirical mean, and we tend to stay more robust in that case. Now, so I want to finish. Now, so I want to finish off. This is real data. These are portfolios with currencies. So now, Philip started us off with a 60-dimensional portfolio. Then he gave us a 20-dimensional portfolio. And eventually, we got a collection. So, these are our smallest portfolios of, let's see, four and five currencies. One thing you will notice that a lot of them are. Is that a lot of them are not elliptically symmetric? So the sample clouds don't resemble ellipses. I mean, in some cases, they do, like Euro, but in other cases, not. So we've used our method to come up with scenarios. So again, not a very extreme threshold. We sort of our result, well, the two methods are kind of similar. Similar right. So, the GKK method has a nice way of computing confidence regions using empirical likelihood. We just use asymptotic normality of the estimator. That's why you see those ellipsoids, which with the covariance matrix, they estimated via bootstrap. But we also have portfolios like this. And so, what you see here is the Here is the blue plus signs there, those are the observations with the largest losses. And so they're kind of not on the edge of the risk factor cloud. So in this case, where we look at a high threshold, our method is sort of more robust, whereas the, if you mote, I think that point pulls away the The stress scenario for GKK method, so it's maybe less robust. But I think this is kind of a new problem. I think our assumptions have worked for the smaller data sets, smaller portfolios. Now, in large portfolios, we see risk factors. Some are tail-dependent, some are tail-independent. How do you construct scenarios in that setting? Is it possible that to lead so those points kind of this is, of course, this is. Of this is, of course, this is five-dimensional, so I'm only showing you know one view. So, what is not extreme in this dimension? Once you turn your five-dimensional cloud, it may be extreme. So, it's possible that a combination of perhaps non-extreme values and your risk factors would lead to large losses. So, there's a lot of still a lot of extrapolation, exploration to be done. Okay. Do you want me to stop? Do you want me to stop? Or do you want me to two minutes? Two minutes. Yeah. Go ahead. Okay. So, what I have really skipped here is that I haven't talked about zero dependence, which is there. And I would say there are two ways to deal with it. And one way that we like to do is kind of in a two-stage approach where we first pre-filter our data. We put the data through a garage-type models, which allow to capture. Models which allow to capture that particular dependent structure we see in serial data, and this is something that we also see in environmental applications. In fact, it's been shown. So, if you have seasonality, you could pre-whiten your data, you could also detrend. So, that's not uncommon. So, what we do is we put data through garage filter and then we do everything I've shown you. A lot of the time you see the assumptions were kind of IID, but I apply that at the second stage and then converts things. Page and then convert things back. So, where this matters is whether we're interested in sort of unconditional or dynamic risk. So, if I'm interested in forecasts, say one day I had forecasts, then I do need a dynamic model. Versus if I'm sort of interested in kind of average risk over a particular horizon, I can use extreme value techniques directly on the data. And there's a whole branch of extreme. And there's a whole branch of extreme value theory for stationary serially dependent data, where the techniques of declustering, say, if you have precipitation events corresponding to the same storm, you tend to see clusters. So you can still do extreme value theory by taking a maximum within a cluster. So there's a whole lot of theory and techniques in that area. So to conclude, I just want to mention a couple of things where I think. Mention a couple of things where I think this we might be heading at the intersection of extreme value analysis and finance. Risk is evolving over time, so I think there is a place for non-stationarity modeling. Sometimes I do need large time series. I know things haven't really stayed constant. The same tail index might have been evolving over time. So there's a need for that. You've probably, so there's this kind of dichotomy. So, there's this kind of dichotomy in multivariate extreme value theory where we tend to have models with tail dependence, and then we have representations with tail independence. There's sort of, they're kind of separate. You a priori decide in which paradigm you are. There's emerging some work that has started, and I think there's a lot more potential to go further to try to bridge the gap where we don't need to make that a priori assumption. And I think dimensionality. And I think dimensionality is a big issue. So we do want methods that are scalable, methods that I can use on 60-dimensional portfolio. And I'm afraid that when it comes to detail observations, it's a big challenge. So there are things to be done there. So if you're interested in more, there is this review paper. And well, I don't have a Well, I don't have a complete list of references, but some references I mentioned. So, thank you very much. I'm happy to answer questions.