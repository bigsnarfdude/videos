He's going to talk to us about global local digital processes for identifying plant cancers of population using both child and cancer-specific data. Thanks for the introduction and thanks. Thanks for the organizer for the invitation. Yeah, so Frontier Mm but this year I'm visiting UT for my sabbatical. And today I'm going to share work, fairly applied work. But I like this work because it demonstrates how a small tweak to the existing method can make inference that is not possible under existing method, especially compared to frequent test methods. So we're going to talk about, it's a clustering method for pan-cancer study, which means I'm going to look at multiple cancer at the same time and try and find subgroups or subtypes across cancer types. And the key here is we're going to use both data that share across cancers. Data that share across cancers, but also data that are specific to each cancer type. This is a joint work with my student OHID, who obviously did all the work, and also my colleague, former colleague, Deb Deep, and Barney Malik. All right, so yeah, so the motivation, the motivating application is in pan-cancer study. Pan-Cancer simply means you're looking at multiple cancers. So as we know, cancer is not a single disease. So even for two persons with the same Even for two persons with the same breast cancer, but they actually may be very, very different from each other. And for that reason, cancer is very hard to treat because the traditional one-size-fits-all treatment just doesn't work for everyone. And for that reason, it's very important to do a fine classification of cancer patients into different subtypes. So the idea is once you figure out the subtypes, you potentially can design a personalized treatment or precision medicine. So, traditionally, the classification of cancer or clustering of cancer patients into subtypes is restricted to tumor site of origin. For example, if you are having a breast cancer or have a colon cancer, you will divide those cancer populations into, or in this case, four subpopulations within each cancer type. And rarely people look at, at least in the past, rarely people look at cancer across or subtypes across cancer types. Cross-cancer types. So, for example, colon cancer, these are very well known for subtypes, and the same for breast cancers. And nowadays, because of these classifications, when you diagnose with cancers, you are also usually told which subtype you belong to. And based on which subtype you have, different treatment is assigned. But I would say this is a limitation because, in fact, there's a more recent study which shows that tumors from Shows that tumors from different sites of origin may have significant both clinical and molecular similarities. So, for example, in this 2014 cell publications, they actually, that's exactly what they did. They pull all the data, different cancer types together and do a joint analysis. They do clustering regardless of the tumor origin. And they do find for certain clusters, so the rows are different clusters, but I just showed two classes, two major clusters here. For example, the second cluster, Second cluster, they actually find out this is one type of lung cancer, this is a head neck, and the bladder cancers. They actually share a lot of molecular similarities, even though they form in different parts of the body. So that's sort of a more recent study suggesting that we should look at cancer jointly, not just one side at a time. So that's kind of one limitation we'll try to address. Another limitation is actually including this. Is actually including this paper or many other follow-up papers. What they do is they usually, when they classify patients, they usually only look at their molecular profile, for example, gene expression, their mutation profile, and so on, by ignoring the clinical data. Even though, actually, when you assign treatment, the physician usually look at both molecular subtypes, like which subtype you belong to, but also, for example, what a stage you're. But also, for example, what a stage you are in, right? So if you are in the later stage, surgery is not options. But if you are in the early stage, they might assign surgery. So even though the treatment is assigned according to both molecular and clinical profiles, but for some reason, at this moment, people usually, when they classify patients, they only use molecular information. So that's, in some sense, a limitation of existing pen cancer study in our point of view is we should, if there's a lot of different things. Is we should, if there's a clinical variable that is available, we should use that information when you classify patients. So, for example, one very well-known molecular markers people used in breast cancer is the homoreceptors. So, things you can test in your blood. So, but on the other hand, as I said, clinical prognostic markers are also very, very important. They actually inform the decision of treatment assignment, but they are under. Treatment assignment, but they are underutilized in the classification of cancer at this moment. So, one prominent example is this so-called CEA, which is a marker, it's a clinical marker that you can test easily with a blood test. It is known to be very, very important for colon cancer prognostics, but when you subtype or when you cluster colon cancer patients, this information is not used. So, therefore, the goal of this study or this paper. This study or this paper we have is to identify pan-cancer subpopulations, so which really just means cluster patients across cancer types. And we're going to use both molecular and the clinical data, right? Not just gene expression, but also whatever is available from the clinics. So we're going to focus on, at least for this application, we're going to focus on four cancers. They are in the GI tract, gastrointestinal tract, esophageal stomach, they're the upper tract. Esophageal stomach, they're the upper tract, upper GI tract cancer, then colon rectose, the lower tract, the GI tract cancer. As I said, we're going to look at both genomic data as well as clinical data. So this is just repeat the goal. So we're going to cluster patients with each cancer while we want the cluster to be shared across cancer so that we can say, okay, this subtype in colon cancer and this subtype in greptal cancer, they're actually very, very similar to each other. We want to do Future. We want to do cancer-specific clustering, but allowing the information to be shared, allow the cluster to be shared. There are many, many existing work, especially in the Bayesian non-parametric literature, starting from maybe one of the earliest hierarchical dish processes, all the way to a more recent development of common atom model or plaid atom model, and so on. So, I guess we could have just used any of this to do the analysis that we want, but one, I guess, features of I guess, features of, I guess, one limitation of using these methods to address the problem we want to address is that because we want to include clinical data, a lot of clinical data are actually cancer-specific. So, for example, the CEA for colonial cancer that I mentioned briefly just now is not collected for the upper track, upper GI tract cancer because it's just not useful. So, they never collect it in the clinics. Therefore, this CEA is Therefore, this CEA is completely missing for this cancer. But there are many, many covariates or clinical data that is only collected for specific data cancer type because it is important for only that cancer. So therefore, the challenge here for this application is if we really want to use clinical data for pan cancer clustering analysis, but unfortunately they're not available for every cancer type. So we want to develop a method. It's a simple tweak to actually the hierarchical dish process. To actually the hierarchical dish process, but I think there should be an opportunity to apply the same tweak to other more recent group clustering Bayesian number vector method as well. But with that simple tweak to the model, we will be able to basically incorporate whatever information is available for each specific instance type. Okay, so the outline of the methodology, it's a really, as you will see, it's a really simple tweak, but it seems to do what we want to do. Sims to do what we want to do. So, we want to incorporate group-specific local variable for clustering of group data. And we're going to provide some method we call this global local. We have some global variable that are available to all the groups, but then we also have local or group-specific variable that is only available for certain groups. That's why we call global local EP. And since it's a very small tweak to the hierarchical decision process, Tweak to the hierarchical dish process. As you would imagine, all the typical characterizations of HDP have a similar representation here. We have stick breaking, for example. We have infinite limit of finite processes representations, and they basically lead to a posterior inference algorithm later on. So, as you will see, very straightforwardly, HTTP is a special case when there's no local variable. If all the variables are global, Variable. If all the variables are global, then the proposed model is simply HDP. And yeah, so based on the infinite limit finite process representations, we use the block deep sampler. We developed blockage sampler based on just the finite global DP, global DP. Because HDP is a special case, obviously the blockage sampler can be used for its finite HDP as well. This is a little different from. As well. This is a little different from the typical algorithm in the literature for HTTP sampling. Typically, people use the Chinese restaurant franchise type of algorithm, which is basically a flapscape sampler or slice sampler. But we use slightly different inference strategy using a finite approximation. All right, so that's the outline of the rest of the. Okay, so we just need the minimal notation to the minimal notation to introduce the likelihood. So for group J, or in all cases cancer, and then patient I in the group J, we have two set of variables. One is the local variables. That is, so we call a variable the local as long as it is not available to all groups. So it may be available to more than one group, but it's not available to all. So those are what we call group local variables. We call group local variables. They're denoted by XJJIL. And then, just like any other group, group clustering methods, we have a set of variables that are shared across all the groups. And we call them global variables and denote by XJIG. And then we put together, let's call the collective variable as XJI for group J and patient, sorry, subject I. All right, so just like any mixed model, we associate You know, mixed model, we associate each XJI with a parameter theta JI, and later we have a observedly discrete measure on this theta JI to induce a cluster. That's a typical setting. Just like XJI, we sort of partition the parameters into the local parameters, that the parameters parametrize, the model for the local variables, and then the global parameter, which parametrizes the global variable. So to link this statement. So to link this theta to X, we consider, well, for this paper, we consider really simple decomposition of the joint likelihood, namely, we assume they're conditionally independent. But you don't really have to do so. So if you want to build connection between local and global, I would just rewrite the first term as a conditional distribution of the local given the global, since the global is common to everybody. The global is common to everybody, so it's pretty easy to do so. But for simplicity, we assume they are conditional independent, and of course, likelihood, you can pick your favorite likelihood. In our case, we pick multivariate Gaussian. Here it is. Like I said, it's a really simple tweak, a very, very minor tweak to the existing literature, well, particularly HDP. So we're gonna, well, I highlight the local part, which is basically new. If you just ignore the Every, if, if you just ignore the highlighted parameters, it is exactly HDP. So, anyway, so we assume that the joint distribution of theta GI, including both the local and global, follows some random measure, G, J, which is group specific. And then to induce, of course, clusters, we're going to assume it is discrete, almost surely discrete. Specifically, we assume it's a DP distributed. Is a dp distributed with some concentration parameter of and the baseline measure in this case, of course, is the key because gj is has support the same as theta. So we're going to assume the baseline is a product of two measure where the first measure is eating syncretic or unique to group J. So obviously UG will be different from group to group because even the parameter or the variable itself is different from group to group. And then for the global part, it's just a random. Part is just a random measure which has hierarchically has another DP prior, just like in the HDP. So again, UJ is idiosyncratic or unique to each group. GJs, therefore, technically, this random measure GJ, they do not share any atoms across groups because they have this unique part by UJ. But marginally, if you just look at the global atoms, they do share across. Sorry, across. Across groups, just like HDT. Okay, so with that, actually, we can sort of look at the clustering from two perspectives. One is if you just look at the global atoms, they defined the global clusters, just like HTT. But then you can also look at the, in some sense, a finer level, where you incorporate the local information. If you look at the atoms jointly, if you look at the local atoms and the global atoms jointly, you get a sort of a refinement of the global. you get a sort of a refinement of the global cluster by incorporating local information. So that's the gist of the sort of the features of this. But like I said, it's very easy to see that when you don't have local variables, it just reduces to HTTP. Since it's so closely connected to HTTP, everything you would expect from HTTP applies here, like you can derive the state braking. You can derive the state breaking representation, the Chinese restaurant type of representation, and so on and so forth. For here, I'm just giving you a very simple demonstration of the state breaking. If you don't recall the model, the model is here from the last slide. But since V is DP, obviously there's this stick breaking representation of it. And then since GJ is another DP, it has its own stick breaking. The only difference from this stick breaking versus the HT. stick breaking versus the HDP is in HDP you typically will collapse the waste because on the on the that's called local level you can collapse the waste because some of the global atoms they actually be have a positive probability to be to be the same because they draw from some common discrete measure here you cannot because there's always this well as long as you have some local variable for that group you cannot really collapse the atoms because they're not going to share the same local atoms if you atoms if UJ is continuous. If it's a UJ is a continuous measure, you're never going to share local atoms. Sorry, the local atoms will be unique from each other. So you cannot collapse those weights like HDP. So that's the only difference. What's that? UJ? Yeah, so this is like later we choose this to be normal inverse gamma for likelihood. Yeah, so it's a prior. Likelihoods? Yeah, so it's a prior that you pick. And we pick this to be conjugate for fast things. So it doesn't have a hyper prior. Unlike V, you have another DP on top of it. UJ is, yeah, in that sense, it's determinist. It's specified a priority. Okay, so this is probably pretty obvious. Probably pretty obvious. You can, for example, to make this connection to clustering more explicit, you can introduce latent variables which distribute it according to the weights on the last slide, the stick breaking weights. Particularly, I'm just in here using the same notation people using HGDP. So T is like a table in the Chinese franchise, Chinese restaurant franchise notations. So these are the basic Basically, in our case, it's the local level cluster label, and then K again using HTB notation, and that's kind of a dish you order from each on each table. And the dish in the HTTP case, they are shared across the restaurant. Therefore, that allows the clusters to be comparable across restaurants, across groups. So, once you're given this to the latent indicator, then of course, conditionally on these indicators, XJR. Similarly, on these indicators, XJI taking this form with parameters basically parametrized by the atoms in the stick breaking representation. Here, this K sub J T J is a bit of the complex notation, but it's really just you first figure out which table the customer will sit on, and then what dish the customer on that table order. That depends the global level cluster. Again, because the Global level cluster again because the dishes are shared across restaurants. This TGI is the local level cluster label. If you want to use the culinary analogy again here, you can think of for each table, you order a dish that is shared across the restaurant, but then each restaurant has its own unique, some unique dishes that each table can order. And those unique dishes are not shared across the restaurant. That defines a finer cluster with each. A finer cluster with each group. Because of this, probably it's already obvious from this notation, this nested notation, local cluster, which is TJI, is always nested in the global cluster. So it's really a refinement of the existing global clusters. So in other words, observation in the same local clusters, they are necessarily in the global cluster, but not necessarily vice versa. Necessarily, vice versa. So actually, we got the first submission, the reviewer came back with some comments about this local cluster, this local variable seems to only play a role in defining a local level cluster, which is not true because in fact, the local variable also affects the global clustering. So if you imagine two, let's say in the cancer application, if two patients have If two patients have a very similar local variable, say, for example, their CEA level is very, very, very, very similar, then there's a higher probability for these two patients to belong to the same global cost as well. The local variable also affects the global cost as well. So that's very different from, if you're from a non-Bayesian perspective, you may think, okay, a simple approach to apply for our application is you can simply cluster patients on the global level first, and then within the global cluster, you can further cluster them based on the local variable. Can further cluster them based on the local variable. That's a two-step approach would look like. In that particular, in that two-step approach, yes, your local level cluster will not affect the global cluster because there's a two-step. But in the Bayesian context, they affect each other. So the local variable not only refine a global cluster with each group, but also encourage patients who have similar local variables to co-cluster even on the global level. We will see one example using some. We will see one example using some illustrative simulation in a bit. Yeah, so local variable also affect global clusters, as I will demonstrate on the next few slides. So this is just a demonstration that a local variable is always nested within a global level. So this is a similar example. Yeah, so we could cancel one, cancer two, they're just two. cancer one cancer two they're they're just two groups and the left two panels are global clusters and the right two panels they are the global they are the local clusters as you will see for example uh cluster we call label five which is the blue cluster on the top left panel they are subdivided into three uh subpopulations or subtypes based on the local variables uh the next the the plot is on the global level so that's why i don't see much of separation among the local Much of separation among the local variables on a global level. But once you look at the local variables, they are well separate on the local level. So that was the first representation based on state breaking, and then you introduced the latent variables to make the clustering perspective more explicit. But as you would imagine, there's all sorts of representations. One of them is useful for our later posterior inference is the finite approximation. Is the finite approximation of that process. So instead of having the GEM, the typical steel breaking process, with a GEM distribution on the weights, the infinite weights, we can assume it's finite dimensional with either L levels or L dimension for the global weights and then T dimension for the local weights. And then probably it's pretty intuitive as L and T goes to infinity. intuitive as L and T goes to infinity that will converge to the original proposed global dp yeah but that's useful this actually we're gonna use posting inference based on this finite approximation it turns out to be very easy to implement with this finite finite approximation okay I'll not go through the actual post inference procedure but just want to show some simulation so usually I don't show simulations in talkative Usually I don't show simulations in talk because like many people said, it always works. But in this case, I want to use this example to demonstrate one of the earlier points I made that the local variables not only refine the global clusters, but also affects the global cluster as well. Maybe it's intuitive for Bayesian, but not necessary for non-Bayesian researchers. So, anyway, for this case, we have three populations or three cancers, if you will, and two global variables. Well, and two global variables, just for demonstration, and then population size is listed over there, about 100 each population. And we vary the number of local variables for each population, one, two, and three. So this is a demonstration because the scenario is actually very simple. So this is the global level. So these are the global variables that you see. Let's focus on this panel, the left panel for the moment. So we simulated from our model. So we simulate it from our model and then we fit our model to the data. As you can see, we plot a scatter plot with the cluster labeled by the color. And we compute the ARI, which is the adjusted render index. It is one means it's perfect clustering. So in this case, if you just look at the global variables, it's impossible to get perfect clustering because basically those clusters are on top of each other. So there's no way, whatever your method. So, there's no way, whatever your method, whatever method you employ, as long as only use the global information, there's no way you get ARI equal to one. The reason we can get AR1 equal to one is we design the simulations so that the local variable are very well separated. In this case, the local variable really dominates the global information that allows you to have, in this case, a perfect separation among the different clusters. So, again, that's just for demonstration that. So, again, that's just for demonstration that local variable can really help in this case. For example, let's say we look at this cluster, sorry, these populations. The cluster is really on top of each other on the global level. So there's no way you can get ARI equal to one if you just use global information, no matter which clustering method you use. But if you look at population two, this again, just for demonstration, the local variables are very, very well separated. So it's a very easy task if you just look at the local variables. At the local variables. It's very well separated. But of course, the key here is our method is able to use both information. In this case, recognize that local variable is more important than global variable so that even on the global level, you can still get a very good performance. So that demonstrates the point that the local variable not only refines the cluster if there are refinements to be made, but also it can increase the separability in the global cluster as well. As well. This is, of course, just demonstration because it's a very simple, simplistic scenario where the local variables are super well separated, where global level variables are on top of each other. But we, of course, do many more simulation that gets more realistic, where the local variables are not as well separated. We vary the separation between the local variable and the global variables and compare, for example, the method with HTTP, where it only applies to the global variables. So we see that as long as the variable. So we see that as long as there are some separations, the low separation to moderate separation to high separation, as you would expect it, as long as some separation, the local level, the proposed method is better than HTP. And with more separation, of course, the gap is bigger. And for example, even with each scenario, it also makes sense that in population three, global DP is much better than HDP because in population Is much better than HTTP because in population three, there were three local variables versus there's only one local variable in the first population in the simulation setting. But that's all, of course, very much expected because there are information in the local variables. Therefore, utilizing that information, of course, should help your clustering. All right, so next we're going to look at an application, the application that we actually consider in the paper. That we actually consider in the papers. So it has four populations of four cancer types: two upper GI tract and two lower GI tract. We select those local variables based on literature, clinical literature. So for esophageal cancer, there's 92 patients. And the only local variable that we find that is useful for prognostic purpose for this type of cancer is smoking status. And in fact, it is collected in the data. And in fact, it is collected in the data that we use. Oh, by the way, the data is coming from TCGA database. But that variable is not prognostic for the other cancer. Therefore, they're not used. For stomach cancer, we didn't find anything. So in this case, for stomach cancer, there's no local variable. There's only a global variable. Then for cola and erectile, they're actually pretty similar cancers. For example, this CEA, the antigen. CA, the antigen that I talked about at the beginning of the talk, actually, this variable or this clinical information is important for both colon and rectal cancers. So we're going to use this as a local variable. As I said, local variable is just any variable that is not available for all cancers. Even though they are shared across colon and rectal cancers, they're still considered to be local. But in addition, there's a slight difference that there is literature support that BMI index, the BMI is useful for prognostic. The BMI is useful for prognostic for colon, but not necessarily for rectal. So, therefore, we also include BMI as the local variable for colon, but not for any other cancer type. So, those are the four local variables we use according to literature. And for global variable, these are the common variable people use for clustering patients in cancer study, which is the gene expression profile. So, they are available for all. So, they are available for all the cancer types from TCGA database. So, they are shared. For illustration, we're going to reduce this gene expression. So, we have like 10 or 20,000 genes. The method is not that scalable yet to that high dimension. So, we're going to do dimensional deductions. There are many choices we can make, PCA or other nonlinear dimensional deductions. We chose UMAC because it's a non-linear dimensional deduction, particularly useful for visualization. Particularly useful for visualization because it brings everything down to two dimensions so we can view it easily. But in principle, it can be applied to the original gene expression data as well. All right. So this is a global cluster. So the U map is applied to the gene expression. So it's just two-dimensional. And these are the four cancer that we analyze. So first of all, something that is very, very much expected is. Very, very much expected: is the upper tri, the upper GI track cancer are more similar to each other than the lower. The two lower GI track is also similar to each other, but across the upper and the lower, not much similarity. So that confirms that if you look at the global level, the left panels are the lower GI track, the right panel are the upper GI track. They obviously are more similar within each track than across. So they share, for example, the stomach cancer, they share two. example the stomach cancer they share two clusters but then also on the other hand they also have some specific cancer specific cluster that is unique to each cancer type uh similar goes for the the lower gi track so those are on the global cluster global level and then we can also look um the local we can we can sort of zoom in a little bit and see how the local variables uh redefine or refine the global cluster we just identified so for example we can look at colon canc So, for example, we can look at colon cancer. This again, these are the x and y-axis are the global level variables. So, therefore, you don't really see much separation within each subcluster that we identify based on local variable. But on the next slide, I'll show the local variable. But, for example, the colon cancer in this case, this was one big cluster on the global level. We label seven. But if you look closer by utilizing the clinical variables, there are actually three subclusters we identified. Subcluster we identify, which doesn't show on the global level because they're not separable on the global level. So, yes, actually, let's just look at the bottom part, which is corresponding to these guys, the colon cancer. So, for example, 7B, which is one of the local clusters that is not separated from the rest of the cluster 7, their CEA level is actually much higher than, for example, 7A. Higher than, for example, 7a, which are this dot, which are very, very, very close to zero. So, um, yeah, so this, of course, if your local level variables are so well separated, and then therefore on a global level, you will see the effects, how it affects the global level clusters. So without, of course, without CEA, you will never be able to find a cluster that is not separable on the global level, but separable on the local level. So, but how do we know those clusters are any good, right? This is unsupervised learning, you don't know the truth. So, we look at different plots. So, after we get a cluster, we do some post hoc analysis. So, one of the things you can look at is, let's say, if you have happened to have other clinical data on those patients, which you didn't use in your analysis, you can look at those clinical data across the cluster you find. And hopefully, you find they are different across classes. find they are different across class. Another way to do it is you look at the genes because we have gene expression data which is common to all the patients, all the cancer type. We can look at how genes are different expressed across this class and see whether those differentially expressed genes, whether they match the expectation in terms of biological knowledge, whether they match the expectation or not. So we do we did both. So several classes does make a difference between in terms of Between in terms of survival, for example. So we can look at the survival curves. Again, we have these two levels of cluster, global level and local level. So we can look at the capitalized survival curve across global clusters as well as local clusters. So for example, it looks like for stomach cancer, there's not much difference in survival from on a global level. But there are some difference, for example, in the lower tract, especially in a colon cancer, for example, this cluster. So, this cluster cluster, orange and the purple cluster, these two clusters, first of all, they're pretty big clusters, and second, they have a significant difference in survival curve. We can also look at the local clusters, sort of the same story. Some of them don't show much difference, but then again, colon cancer for some reason, there is a difference, even on the local level. Again, this is one of the subtype in cluster. We call cluster 7, but 7B versus 7a. Versus 7a, actually. So, these yellow and orange clusters will not be able to, they cannot be identified with global variable alone, but they are identifiable with the help of local variable. All right, so that's one way to sort of validate what cluster you identify are actually meaningful, what clusters are not. Another way to look at, as I said briefly, there is you can look at the expression, or you can do some differential gene. Or you can do some differential gene expression analysis with the class A identifier. So, the idea is you look at one gene at a time and trying to see whether these genes are really different across the class A identifier. If they are different, those are called differential expressed genes. And then you go back to the literature and see what's the function this gene is known to play. And hopefully, they are related to colon cancer, to whatever cancer you're looking at. So, in this case, we again have two levels of classes. So, we can look at differential expressions, express genes. Expressions express genes on the global level as well as on the local level. So, on a global level, that's something that typically you would do for any type of cluster analysis. For example, here we have top six differential expressed genes. Some of them are interesting. For example, let's say if you look at the first gene, it is only highly expressed in one of the cancer, sorry, subtype or clusters. So, therefore, these genes can be potentially used as a marker. Can be potentially used as a marker because you can just test it. And if it's highly elevated, you know it belongs to that particular cluster. Okay, so that's one use of such differential gene expression analysis. But what's interesting here, I really want to emphasize is, because we also have this so-called local level classes, which use the clinical variables, they're not genomic information. We can repeat the same analysis on the local levels. We can look at genes that are differentially expresses. Genes that are differentially expressed across the local classes. So, what is interesting is here, for example, this guy, this particular gene, and it has a pretty different expression level for this particular gene for a subtypes or the clusters on the local level only. For example, this one is 7A and this is 7B. So that means this gene is actually differentially expressed in this 7A cluster and 7B cluster. This 7A class and 7B class, the local class. That is interesting because for two reasons. One is you will not be able to identify this just based on the global level cluster. So that will actually get and try to see whether this expression is that much difference across global clusters. It's not. But in the local level, it is significantly differentially expressed. And what is interesting here, we find interesting is because the local level variables, sorry, the local. level variables or sorry the local cluster is refined by incorporating the local variables but the local variables is not gene expression it's not genomic information it's pure clinical information uh so that shows even though you incorporate source in some sense orthogonal information to the gene expression the the clinical variables the cluster it finds or the cluster it allows you to identify can be used to in a reverse way so you use clinical variable to define clusters Clinical variable to define clusters, and then those clusters tell you about some genomic difference between those patients. So, that's something we find that is interesting. And then, like I said, so how do we know this gene is really important? Well, it is important by, well, ideally, you want to do experiment on it, but it's hard for statisticians to do so. So, all we can do is look at the literatures and see if that gene. And see if that genes is really, well, in this case, it was a colon cancer, so we will focus on colon cancer. So we can see whether this gene has been found important in colon cancer in the literature. Indeed, for this particular genes, it is. So the genes has actually highly expression from our analysis, has a high expression in this cluster called 7b. We look at their local variables for their clusters. Those are the clusters with extremely high CEA values. Extremely high CEA values and high BMI as well. And it has a lower, much lower mean expression, as you can see, much lower mean expression in 7A, which is characterized by low CEA values and low BMI. So those are the, I guess, the local variables, how they separate these two local clusters based on CEA and BMI. So why is it relevant? Well, Well, recently this particular gene has been identified as a key driver specifically in colon cancers. And it seems like the direction of like 7B, the expression is higher than 7A, seems to match the biological knowledge because S-APA regulation is known to significantly increase this endothelial cell proliferation rates in colon cancer, which in turn is reflected by the patient's high C. Reflected by the patient's high CEA value because high CEA value means more advanced colon can. So it seems like the direction of that up-regulation of the gene expression in 7B matches the intuition on the clinical side. I have five more minutes. Okay. So, yeah, so to conclude, like I said, the model is really a simple modification of HTTP, but it seems to be a model. But it seems to do what we really wanted to do is incorporate local variables in clustering group data. So, with that, with the help of local variables, we have basically two type of clusters. One is a more common global level cluster that is common to all groups. And then the local level clusters sort of divide or refine the global cluster into smaller. There are, of course, limitations, future direction. One of them I already mentioned. Future direction, one of them I already mentioned, it's not really scalable in terms of number of number of variables. That's why we had to do dimension reductions before we apply the method. The dependence between global local variables, obviously gene expression and clinical variables, they are dependent. In our case, we assume they're to be conditionally independent, but there should be a way to model dependence if you want. There might be even dependencies between groups, between the Dependencies between groups between the cancer patients, for example, we know colon cancer, rectal cancer, they are a priori more similar to each other, then the upper tract, same thing. If esophageal and stomach cancer, they are more similar to each other. So there are some dependencies or prior knowledge about the dependencies between groups. So is there any way to build dependencies directly in the model? And then lastly, there are other ways to incorporate local variables. Maybe, for example, can extend. Maybe, for example, can extend the more recent group clustering method, CAM, with local variables that should be possible. Or extending, for example, PBMX, which allows covariate to affects partition. Maybe we can extend this to the hierarchical setting where you have different groups. And with that, that's all I want to talk, and happy to take any questions. Any questions? A beautiful discussion, I mean, I love it. But I like one quick reaction is: where does it stop? Like, you're starting with molecular markers, now you're adding clinical markers. Maybe we could add endlessly, right? Yeah. Same thing in different worlds. Maybe it's worth to think of the local clustering in a different way, like maybe like subgroup analysis almost, because. Group analysis almost because you're including clinical covariance. It would be really natural now to also start thinking of the clinical outcome. And then you could think of subgroup analysis, like you showed the Kabel-Meyer plot. And then you can build something like only search for local clusters if you really make a difference in terms of outcome. Yeah, there'll be a more direct approach if you really care about the clean outcome. We didn't choose to do so is we want to use the outcome as sort of validation. If we don't use that, Validation. If we don't use that clinical outcome, is our cost meaningful in a clinical way? Yes, but if your goal is to differentiate patients with different survival or prognostics, yes, incorporating survival directly in the model would make a lot of sense. Thank you very much for the talk. It was really great. I would love to talk more about the campaigns. And one thing that I really appreciate of that is that you say, okay, I have this very large data set I perform when I try to production, which is something, you know, with the Disney U map that I something that I really like to do as well. I have some very principled colleagues, and of course maybe some review words that say, you know, this is just for visualization purposes, and you should really use, you know, they always sent me to, I think it was like a web page. Oh, you should not use this. I think it was. Should not use this. I think it was more for Disney and less for UMAP, but do you have a good reply? Yeah, so this paper is now, we are providing the revision. We get almost exact comments you receive, point to that website that says, ah, UMAP or TSNI is just for, usually just for visualizations. Well, we're preparing the report. So the way we'll argue is, at least for UMAP, I don't know, I'm not sure about Disney. For UMAP, there's About Disney. For UMAP, there's a parameter in the UMAP that controls how basically how much global features you want to preserve after the actions. So in our case, we're lucky that the referee want exactly, he told us exactly what he wants. He wants us to tune this parameter so that the maximum global features are preserved. So we're just going to do exactly what the referee wants in this case. But in general, it's a very difficult question because you don't know the cluster. It's a very difficult question because you don't know the clusters, you don't know the real clustering. So that parameter is making, well, in our case, we choose that global, sorry, that tuning parameter so that it preserves the global clusters as exactly requested by reviewer. But if you choose that parameter, make it much smaller, you will find that the clusters are much closer to each other on this two-dimensional plot. So the cluster result will, unfortunately, will be sensitive to that parameter that you use in the. To that parameter that you use in the UMAP dimensional deductions. So, again, in our case, we're lucky that the referee has a very specific instruction. They want this parameter to be big so that it preserves the global information. But in general, yes, I don't know what's the best way to tune that parameter for UMAP dimensional reduction. But if you perform another UMAP reduction, would it change? Or at least, you know, as long as. Or at least you know as long as you don't care where the clusters are but those uh observations are close to each other, uh in high dimension, low dimension. Yeah, to some degree there's robust. It will bring cluster all the points closest to each other, but still preserve the overall, I guess, the layout of the points. But again, U map, because it's a non-linear dimensional reduction, so it's less clear how this variable. Less clear how these variables ultimately will decide how points are layout on this two-dimensional space. Yeah. So I guess in principle, we can, if you're not for visualization, you can, in principle, apply your methods to the original data set if it's scalable, or use a PCA to, say, 20 PCs, and then only use this for visualization. So you do the cluster on the original gene expressions, and then only use UMAP for visualization. That probably will be less. And that probably will be less controversial. On the interest of time, we leave further questions. And we're going to move to the next talk.