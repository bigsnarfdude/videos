So, as many of us know, Brian wrote his PhD at Cornell with Len, with Len Gross, and he's now at Notre Dame and continuing his research in math physics, specifically math problems motivated by quantum mechanics and quantum field theory. Today, he's going to be telling us about some partial differential equations in random matrix theory. Thanks, Brian. Okay, thank you. To the organizers, I hope still I've never. I hope, still have never been to Banth in person, so I'm hoping that will come to pass at some point. But still glad to be able to be here and tell you about some of the work that I've been doing. So the main part of the talk will be describing joint work with Ching Wei Ho. And it also builds on work of Ching Wei Ho with Ping Zhang and the And the broad method that we're using is one that was developed in a paper of mine with Bruce Driver and Todd Kemp. And there's also some related results by different methods, but related results in the physics literature that are listed there. And I will finally mention that there is an expository paper about this sort of stuff. It gives kind of a non-technical introduction to this PDE method that's on the archive there, that if you look up, The archive there that if you look up that or go to my web page, you can find this. It's sort of a gentle introduction to this PDE method, which I will describe. Okay, so I want to start by just talking about the types of random matrix problems. I'm going to mostly just show a lot of pictures, tell you different kinds of random matrix models that this method can be used for, and just show you some pictures of what the eigenvalues look like. So, I need to start by telling you that. So, I need to start by telling you, reminding you, if you don't already know, of the Gaussian unitary ensemble and the Genebra ensemble. These are the two most basic models, random matrix models. So they're both unitary, excuse me, Gaussian measures on different spaces of matrices. So GE, it's a Gaussian measure on the space of n by n Hermitian matrices. And you make the Gaussian measure in the most natural way using. Natural way using the trace of x squared as your inner product. And the only part of it that's perhaps not obvious is that you need to put this n here. You need to scale the variance basically by 1 over n. The reason for doing that is if you don't do that, then the eigenvalues will just get really, really big. If you keep that n there, it keeps the sort of eigenvalue distribution of fixed size as n goes to infinity. And you do the same sort of Brownian motion or Gaussian measure on space of all matrices. On space of all matrices, again with an n in the exponent. And the two most basic results in random matrix theory are the semicircular law and the circular law. The semicircular law says that if you take a GUE for a very large n and you pick this matrix at random using that probability measure there, and then you plot the eigenvalues that the histogram of eigenvalues, the eigenvalues, of course, are real. Hermitian matrices have real eigenvalues that the histogram of eigenvalues That the histogram of eigenvalues of that matrix with high probability when n is large will resemble a semicircle shape. So, this is a simulation here of a GUE, and you see the semicircular shape. And you keep doing multiple simulations with very high probability, you will get the semicircle histogram. The circular law says that if you use the Geneva ensemble, so then the matrix is not Hermitian. There's no reason for the eigenvalues to be real. The claim is that, again, The claim is that, again, with high probability, almost all the eigenvalues will live in the unit disk here and they will be uniformly distributed, with high probability, almost uniformly distributed on the disk. So you can see this picture. Hopefully the points look like they're kind of uniformly distributed around this disk. Okay, so we will use this GUE and the Genebra in various points as we go along, but the main topic of Go along, but the main topic of discussion for today is sums and products of independent random matrices. So, the basic problem is you give me two independent random matrices, X and Y, and let's say I know and understand the limiting eigenvalue distribution of X. As the size goes to infinity, I understand the limiting eigenvalue distribution of Y. As the size goes to infinity, can I compute the limiting eigenvalue distribution? Limiting eigenvalue distribution of x plus y or x times y. Okay, very natural, fundamental question. Now, this question was answered using the technique of free probability and free convolution by Voigalescu and then further developed by Philippe Bian in two special cases, which is in the case x plus y, if both x and y are Hermitian. So Hermitian plus Hermitian, which is Hermitian, or in the product case, Or, in the product case, if you're doing x times y, unitary times unitary, which is unitary. Okay, the point is you want to stay within the very nice class of Hermitian matrices or the very nice class of unitary matrices. And it's a beautiful theory that we definitely draw upon in our work also. But we also then would like to go beyond that, those special cases. In particular, interested in cases where the X plus Y. Cases where the x plus y or the x times y are not Hermitian or unitary or normal or anything, just a sort of generic type of sum or product. So I'm now going to tell you several examples and show you some pictures of what the eigenvalue distribution looks like. So the first one is Brownian motion in the general linear group. And how do you construct it? Well, there's many ways to do it, but the way I'm going to describe it is you take a bunch of Geneva matrices independent. Bunch of Ginebra matrices, independent Ginebra matrices, Z1 through ZK. Here they are. Okay. And then you form this product, which is you take the identity matrix plus this small multiple of Zj. Okay, K is some large number, which eventually has to tend to infinity. Okay, and here T is the positive parameter. It's the time parameter in the Brownian motion. Okay, so this is a construction of, you know. Of, you know, like in distribution, if you do this and you let k go to infinity, then in distribution, you will get the distribution of the Brownian motion at time t. Okay, and the simulations that I'm showing you there on the bottom of the screen are exactly done by just plugging this formula into Mathematica. It's like five lines of Mathematica code. It may take a while to run if K is 200 or 300 or something, but that's just how you do it. You're multiplying. Just how you do it. You're multiplying. So you're multiplying, right? Products of independent random matrices. This is a big product, a sort of infinite product of a whole bunch of independent random matrices. And this was studied in a paper of mine from a couple years ago with Bruce Driver and Todd Kemp. And we were able to find the domain in which the eigenvalues live, which is drawn there in the black boundary, and also to Boundary and also to describe what the distribution of eigenvalues is inside. And one notable feature of this, so this one over here on the right is t equals 4. Everything depends on the time parameter. The left-hand side is, I think, t is 3.9 or so. And this is t equals 4. And what's notable is that the domain sort of pinches off and forms a hole at t equals 4. And for all larger times, there's a hole there around the origin. Okay. Okay, so that's one example of products of independent random matrices. Now, the next example is due to Ching Wei Ho and Ping Zhang, and they did the same Brownian motion in the general linear group, Bt. That's the same Bt as in the previous slide, but they extend it by multiplying by an arbitrary unitary element that's independent, of course, of the Bt. So it's another product of independent randomity. It's another product of independent random matrices. The Bt is one guy that we just talked about, and then you multiply by a totally arbitrary unitary. So that U doesn't have to be a Brownie motion or anything. You can just pick the eigenvalue distribution of the U any way you like. So in this picture, the U is chosen to have eigenvalues of plus and minus one and plus and minus i, which are here, like there's one, i and one and i and minus one and minus. And i and minus one and minus i. So, like when t is zero, the eigenvalues would just live at those four points. And then, as t goes on, the bt sort of spreads things out. And the time parameter here is one. And you get this nice four-petal flower picture. Okay, and they were able to extend our results, and it really opened the door a lot. Like Bruce and Todd and I just had no idea to ever thought of doing such a thing. To ever thought of doing such a thing or would have known how to do it. And it really extended a lot the range of applicability of the method that we were using. Okay, another example, which is also in the same paper of Ho and Zhang, is you take a Genebra matrix. Remember, that's just the one with the circular law here. Scale it, convenient to scale it by the square root of t, and then you just add a totally arbitrary. You just add a totally arbitrary Hermitian matrix. Okay, and you try to again find the eigenvalue distribution. So, in this example, the eigenvalues of the x are plus or minus one with probability one half and one half. So, again, you can pick the eigenvalue distribution of x any way you like. So, let's say half one and half minus one. And the t is 1.05. And see, you get sort of two blobs: the one and the minus one are here, right? Here, right, and then you get sort of looks like a circular distribution around one and a circular distribution around minus one, but not quite. That they sort of start spreading into the middle, and the two blobs have just kind of joined here up, I think, at time one. And this is a little beyond that. Okay, another example is a very natural one, which is you take just an arbitrary Hermitian matrix X and And then you take x plus i square root of t y, where y is a gu e. So it's basically Hermitian plus i times a G U E. X plus Iy kind of problem. One of them is a GUE and the other is just totally arbitrary Hermitian. And here I've used the same axis in the previous example, which is the time parameter is 1.05 and the eigenvalues of x are plus and minus 1. Values of x are plus and minus one. Same x in the same t is 1.05 in both pictures. And somehow you get a different picture. It's certainly not the same picture as on the previous slide, but somehow they're related. And they sort of the blob, the two blobs just kind of meet up in the middle. Okay, next problem, which is you do a Brownian motion in the unitary group, that's UT. All of this, of course. All of this, of course, means in the unitary group, u n and n eventually tends to infinity in the problem. You do Brownie motion in the unitary group, and then you multiply by a positive matrix, independent positive matrix. If it were unitary times unitary, then this would fall under the Voicholescu-Bion method. But if it's unitary times positive, then it's a different problem. And this one was studied in a paper of In a paper of Demni and Hamdi from last year. And what's notable about this example is that Demni and Hamdi were able to identify this domain that I've drawn here in which the eigenvalues live, but there is no known formula for how the eigenvalues are distributed inside the domain. And some of us have tried to say, oh, can we find the actual distribution of eigenvalues? Distribution of eigenguises is somehow harder. This problem, for some reason, is harder than some of the other problems that I mentioned. Okay, so unitary times a positive, unitary Browning motion times a positive. Here's another one, and this is in a project of mine with Ho that is not quite finished yet. We hope to have it up on the archive in another few weeks, probably. And it's similar to the previous. The previous work of Ho and Zhang, which is you do Brownian motion with an arbitrary unitary initial condition, but it's this three-parameter Brownian motion. And what is three parameters? What are these three parameters? Well, you can adjust the rate of diffusion in the unitary direction inside GLN. You can adjust the rate of diffusion in the positive direction inside GLN. And then the third parameter is there's a kind of correlation between the positive diffusion and the Positive diffusion and the unitary diffusion. So, this is kind of a twisted version here of slide six. You have that nice, pretty four-petaled flower. This is also a four-petal flower, but it gets sort of twisted around. That particularly, that correlation parameter kind of twists the picture around. Okay, so these are several examples of random matrix problems involving sums or products of independent. Are products of independent random matrices that can be analyzed using this PDE method. So I'm now going to work my way toward telling you what is this PDE method. And in order to do that, I need to tell you briefly about Girkow's method and the concept of brown measure, which are actually just two words for the same thing. So Girkow was studying the circular law in the context of matrices with independent identically distributed entries, but not necessarily normal. Entries, but not necessarily normal. Sorry, not necessarily normal distribution to the entries, a Gaussian, not necessarily Gaussian or normal distribution. And then like, well, how do you compute the eigenvalues? If the matrix is not Hermitian, you need a method for calculating the eigenvalues of this matrix. So just take a matrix A. It should be thought of really as a random matrix, but at the moment, it could be just one fixed matrix. And what Girko said, how you should compute the eigenvalues, is you make this. eigenvalues is you make this quantity here, which is you take a minus lambda star a minus lambda, which is a positive definite Hermitian matrix, right? A is just arbitrary, not necessarily Hermitian or normal or anything. But this A minus lambda star, A minus lambda, is explicitly Hermitian and positive definite, except when lambda is an eigenvalue. So you can take the log in the matrix. You can take the log in the matrix sense. You have a positive definite Hermitian matrix, it has a matrix logarithm, and then you take the trace of the logarithm and you divide by n, and you get this function s. Now you can compute, it's a little, it's not quite trivial, but it's a simple exercise in linear algebra to compute this s function in terms of the eigenvalues. Like this is the sort of this formula is the one you would actually use to compute s. Like if a is a random matrix, then you try to use that formula. And you try to use that formula. But to understand what it really means, you compute it in terms of the eigenvalues and you compute it like this: it's the sum of the log of lambda, absolute value of lambda minus lambda j. And if you look at that formula, log of absolute value of lambda minus lambda j, you may recognize that as being the Green's function for the Laplacian in the plane. So that means that function is as a function of lambda is harmonic, except Lambda is harmonic except right at the singularity where lambda equals lambda j. And so if you take the Laplacian in the distribution sense of this function s, you're just going to get a mass right at each of the eigenvalues. So properly normalized is saying you take this s function and you take its Laplacian, it'll give you exactly like a mass of one over n at each of the eigenvalues. So it's what we call the empirical eigenvalue distribution, is computed this way. Okay, so here's a little picture of the s function. You have five eigenvalues. I've plotted minus s because it's easier to see the singularities that way. And you say, all right, where are the eigenvalues? Well, of course, they're there where the singularities are. So you can sort of see from the picture where the eigenvalues are. But even better than that is that the function is harmonic, except at those points. So if you take the Laplacian, you really get only a contribution exactly at the eigenvalues. Distribution exactly at the eigenvalues and zero everywhere else. And that was Gierko's proposed method to understand the eigenvalues. Okay, now one more bit of preparation is to say, I want to do what Gurkha did, but I want to do it sort of in the large n limit. And to do that, you have to say, well, okay, if you have a random matrix model and you want to. Model, and you want to take the Large N limit of it, like what kind of objects do you get in the Large N limit? And one way to answer that question is to use the operator algebra formalism. So let's say I have a random matrix Xn. It's not necessarily normal or Hermitian, just totally arbitrary sort of random matrix. I can make this so-called star moments, which is I take some word in Xn and Xn star. They don't commute, right? I don't assume Xn commute. I don't assume Xn commutes with Xn star. So you can have Xn, Xn, and then an Xn star, and then three more Xns, and just all any sort of word in Xn and Xn star. Then I take the normalized trace, one over n times the trace, and then I take the expectation value. That's the star moment. There's so many of them because you can put stars and non-stars in any order you like. There's this huge collection of these star moments. And then you can take the large n limit. And hopefully, you've set things up in such a way that the limit is. things up in such a way that the limit as n goes to infinity of these star moments exists and then what am i going to do i'm going to try to find an operator algebra that sort of captures the large n limit of these star moments so i'm going to try to find an operator algebra a with a trace pow okay what is this trace it's not literally the trace in the sense of a trace class operator it's a it's a linear functional on the algebra from the The algebra from the operator algebra into C that behaves like the normalized trace of a matrix. Okay? So it's a linear functional that behaves like a trace. And then I'm going to also try to find an element A in the operator algebra so that when I take the large n limit of these star moments here, that what I get can be computed as tau of As tau of the corresponding word in the element A and its adjoint A star. Okay, so basically, what is this operator algebra formalism doing? It's just capturing the large n limit. It's sort of encoding the large n limit of these star moments into this expression here. Tau of A, A star, A star, A, some combination of A's and A stars. Stars. Okay, now it's an important thing that the notation sort of obscures is that I want to emphasize is the Xn is not a matrix. It's a random matrix, meaning it's a random variable with values in the matrix. Like I can take any, in theory, any value could be any matrix, right? It's random. It's a random variable. But this A is not an operator-valued random variable. An operator-valued random variable. The A is just one single operator in the operator algebra. And it somehow reflects, I guess, the idea that in all the examples I know about, those star moments somehow become non-random in the limit, which is like the expectation has a limit and the variance goes to zero. So they somehow become deterministic in the limit. So in the limit, there's just one single operator A. Now, I have no time to discuss like how you would build an operator algebra like this. Would build an operator algebra like this, right? That's a whole other talk. So I just want to say that there is a machinery in free probability for constructing the kind of operator algebra and the element A, yet there's a well-developed machinery for building such a thing. Okay, any questions before I press onward? Ryan, is it just a, not just a, but is it a matter of Not just a, but is it a matter of scaling? Are there results where they use a different scaling so the variance doesn't go to zero? So you end up with some random operator-valued thing? Yeah, I mean, people do want to somehow try to keep track of some of the randomness still. In a sense, it's a disadvantage of the theory that all the randomness goes away. So there is some like second-order freeness that tries to keep track of the fluctuations. But for our purposes, we're really just We're really just looking at the sort of deterministic aspect of the theory, which is the eigenvalue distribution becomes deterministic. The bulk eigenvalue distribution becomes deterministic. And so for us, this is sufficient. Okay, so then we say, okay, look, if you give me an operator A in an operator algebra script A with a trace, you say, well, what's the eigenvalue? You say, well, what's the eigenvalue distribution? Is there something like an eigenvalue distribution of A? Now, A is not necessarily normal, so you can't use the spectral theorem to define an eigenvalue distribution. There's no spectral theorem. Instead, what you do is you just imitate Girkho's formula. You know, I've tried to set it up in a way that it's sort of self-evident how to proceed. So, you just do the same thing as what Gierko said. And if you want to be cautious, so this is the same formula, except instead of writing the normalized trace. Writing the normalized phrase here, I've got this tau is I take a minus lambda star a minus lambda. And then just to be safe, you know, we're mathematicians, we like to be sure that everything is well defined. And so if you want to really be sure it's well defined, you can put in a little regularization, epsilon squared. That guarantees that this whole operator here is strictly positive definite. So it has a well-defined log. And then you take the trace of it. Then you take the trace of it, and but then you have to let epsilon tend to zero, you can't leave that epsilon in there. So you first put it in and then you take it out, okay? And you get basically the counterpart of Beyond's S function is this. And the theory, the general theory developed by Brown says that that limit exists as an almost everywhere defined subharmonic function. And you can then take the Laplacian and get the Brown measure. Okay, and the theory says that this brown measure, you give me an A in an operator algebra, just any element of such an operator algebra, that this brown measure is a probability measure on the plane, and it's supported on the spectrum of the operator A. So it really is a natural, it is the sort of natural way of defining something like an operator eigenvalue distribution of a operator in an operator algebra. You know, Gierko and Brown. You know, Gierko and Bran were using essentially the same idea. They definitely, I'm sure, did not know about each other's work from totally different perspectives. Okay, so now I would like to talk about this PDE method, which is, well, how do you actually compute the Brown measure? Okay, and for the sake of definiteness, I'm going to specialize to this case that I mentioned before, where you take a Hermitian element x. element x and you add i squared of t y where the y is a g v. And this particular result is in a paper with Ching Wei Ho, which you can find on archive from last year. Okay, so let's just have a couple more pictures for fun because we haven't seen too many pictures lately. I said there'd be lots of pictures in the talk. So here's a couple more examples. more examples. So this example, remember it's x plus i squared of ty. The y is gue, the x can be any Hermitian matrix you like. The eigenvalues of x can be distributed any way you like. Pick your favorite probability measure and form a matrix having that distribution of eigenvalues. So this one on the left is the distribution of eigenvalues of x, which in this case is quadratic from 0 to 1. Okay, so x, just choose x where the Just choose X where the histogram of eigenvalues of X is like this. And then make the X plus I squared of Ty, and you get this nice teardrop shape, which has a cute little sort of cusp here at the end, which somehow has to do with this cusp at the end of this distribution. Anyhow, just one more example. You can make infinitely many examples of this sort. And then here's a slightly simpler one, which is. Slightly simpler one, which is the eigenvalues are plus or minus one. And I had a picture like that previously. The only difference is this time I'm taking the probabilities to be unequal: one-third here and two-thirds here. Okay, and that's what that's what the eigenvalues of x plus i times a gv look like. Okay, so now on to try to, uh, yes, well, one more important picture. Sorry, before I start finding our PV. Before I start finding our PVE, which is, if you look at this picture, it's the same picture as on the previous slide, except I've changed the aspect ratio and I've just stretched out the picture so that you can see a little better what's happening. Is that you may believe, okay, it's just a simulation, so like you have to trust the simulation, is that inside the domain, the density of eigenvalues is constant in the vertical. Values is constant in the vertical direction. Okay, is that plausible? That the density of points, whatever that means along that vertical strip, is constant. That's the claim. Okay, of course, we have some theoretical justification of this. So our goal is to compute the brown measure of this x plus i squared of ty. And in particular, we want to understand this phenomenon that the density is constant in the vertical direction. And of course, we also want to know what is the domain. And of course, we also want to know what is the domain in which it lives, and you know, what is the density as a function of the horizontal variable? Okay, so now, Brian, before you go, I think there was a question in the chat, and I think it was about when you wrote the brown measure definition, but I'm not sure. So, if you can can you read that? The um what could be delta as lambda epsilon. The epsilon. What could be delta? Maybe the question is about if you do not first take epsilon to zero and you just do the Laplacian and Lambda, what is that? Yeah, I think there is some kind of formula for if you leave the epsilon in there and take the Laplacian, like you can do the calculation and you could try to do the things in the opposite order. And I believe you always get the same answer. You always get the same answer. If you first take a Laplacian, then take epsilon to zero, I believe it gives the same answer. But we're going to follow this convention. Okay. My question is, before taking epsilon goes to zero, I mean, only if you take S1 diapsin and then we apply the laplatin, then we will get certainly something like bronyan major and aided with some perturbation, but then Aided with some perturbation, but then what could be this major? It's not Brownian, should not be, I guess. Or it would be Brownian motion shifted because you have epsilon? Well, there's a sort of conflict of notation. The Brown measure is not really related to Brownian motion. It's a different term. So just the epsilon just mean this epsilon really is just there to make everything smooth. So like the brown. So, like, the brown measure could have like point masses. It could be like just some discrete set of delta functions, but the epsilon sort of smooths it out and gives you like something with a nice smooth density. So it's just smoothing it out. That's really what the epsilon is doing. Yeah, it's okay. But I need only to know 1 over 4π, for example, delta epsilon, delta s lambda epsilon, what could be, which kind. What could be, which kind of measure you can get with the epsilon as positive? Yeah, with epsilon strictly positive. I'm not sure exactly, but it's going to give you some positive probability measure, probability measure on the plane with a smooth, strictly positive density. Okay, but that is exactly this probability measure. It's a smoothed out version of the Brown measure. Okay. Brown measure. Okay, okay. That's, I don't know any better to answer than that. Okay, thank you. Thank you. Okay, so yes, we said that the density is constant in the vertical direction, and now we're going to try to compute the Brown measure. So in the large n limit, this GuE becomes, well, the square root of t times the GuE becomes the so-called semicircular Browning motion. And the X becomes just some. X becomes just some self-adjoint element X in the operator algebra that's independent or strictly freely independent of the sigma t. And then one important bit of notation here is that the mu, we're going to let mu be the law of x, okay, which really just means the limiting eigenvalue distribution of the random matrix capital X. Okay, so like in the previous example, the mu was example the the mu was like one-third mass at minus one and two-thirds mass at one that that that kind of thing okay and then our goal is to compute the brown measure of this x plus i sigma t with the i there right if you didn't have the i then it would be Hermitian plus Hermitian is Hermitian and then you could use the the method of Voikolescu but with X plus Iy it's not Hermitian it's not normal it's just why It's just whatever, and we try to compute it. And I will mention that we believe, it's not a theorem, but we believe that this Brown measure is the limiting eigenvalue distribution of the corresponding random matrix model, but that's not a theorem. Simulations certainly indicate that, but that's a separate issue. Okay, now the question is: how are we going to compute this brown measure? So here's the, I've just now repeated the definition of Repeated the definition of the Brown measure from a few slides back, except I put in this particular element x plus i sigma t, x plus i sigma t minus lambda star, and then we have this epsilon. Okay, and the first main result that we have that allows us to get going is that this function s of t in lambda and epsilon satisfies a PDE. Okay, here's the PDE for Okay, here's the PDE for that thing as a function of t and lambda epsilon. Okay, lambda we write as a plus ib. So a and b are always the real and the imaginary parts of lambda. Okay, and note that this is not, even when I was looking at my own slides, I saw it. I say, oh, it's something like a heat equation, but it's not because this is not d squared s d epsilon squared. It's d s d epsilon quantity squared. Okay, so it's a first. Any square, okay? So it's a first-order non-linear, it's the square, it's the sum of squares of the first derivatives, all right? First-order non-linear PDE with some obvious sort of initial condition. Okay, so that's our first step is that there is a PDE. The first part of the PDE method is that this function satisfies a PDE. Okay, now I will point out that the epsilon, previously I sort of gave you a false impression that the epsilon is just A false impression that the epsilon is just this regularization parameter, and its only role in life is you put it equal to zero. But actually, for us, the epsilon is a variable in the PDE, right? And so you can't just put epsilon equal to zero. So what you have to do is solve the PDE as a function of t in lambda and epsilon. And then after you've solved it, then you put epsilon equal to zero. And then you take the Laplacian to get the Brown measure. Okay, that's our general. That's our general strategy. Okay, now we need a method for solving this type of first-order nonlinear PDE, and it's of the so-called Hamilton-Jacobi type. And so, like, I've said this before, but I'll say it again, the best method for solving a PDE like this is to have a co-author who knows more than you do about such things. And in this case, it was Bruce Driver who taught Todd Kemp and me about. Taught Todd Kemp and me about the Hamilton-Jacobi method, and then Xing Wei and I also use that same method. So, what is the Hamilton-Jacobi method? Well, you take the PDE and you make a so-called Hamiltonian function where you change every derivative into a momentum variable, and then there's an overall sign change. So, I make this Hamiltonian, it's a function of a, b, and epsilon, and then these new variables p a, p b, and p epsilon, and I just read it off on the P D E. Those derivative. Read it off on the PDE, those derivatives become like that ds d epsilon squared became this p epsilon squared. Okay, and it just happens in our case that it doesn't that the Hamiltonian happens not to depend on a b and epsilon because the ab and epsilon didn't show up on the right-hand side of the of the PDE. So you form this Hamiltonian and then you set up an auxiliary system of ordinary differential equations, which is the so-called Hamilton. Equations, which is the so-called Hamilton's equations. It's just ODEs that you'd get from the Hamiltonian like this. Now, in our case, we're fortunate that this Hamiltonian here is a very simple thing, particularly because it doesn't depend on A or B or epsilon. It's pretty easy to solve the system of ODEs, and that's part of what allows us to make progress. Okay, and then you say, okay, great, you form this Hamiltonian, you form this auxiliary system of ODEs. Like, what does it really tell you? It really tells you well, what it tells you, the Hamilton-Jacobi formula, says that if you evaluate the function s along the solutions of that auxiliary system of ODE, so you take s of lambda t and epsilon t, where those solve those ODEs, then there's some kind of formula for that, which is, well, there's the value at time zero, and then there's this Hamiltonian piece here. So it's a pretty Piece here. So it's a pretty simple formula, relatively simple formula for this. H0 means the value of the Hamiltonian at time zero. Like you just plug in A0, B0, and so on into the Hamiltonian. Okay, so the upshot of that is that, well, you know, you get the solution along, you get the value of S along the solutions of the ODE is sort of computable. Okay, so. Okay, so that's the Hamilton-Jacoby method. And now we're going to try to apply it to compute the Brown measure. And I'm going to start by talking about: well, where, what do you get when the Brown measure is zero? Like computing, I want to find the region where the Brown measure is zero and then find the region where the Brown measure is not zero. And they're sort of separate. So the Hamilton-Jacobi method tells us how to compute s of t and lambda of t and epsilon of t. But in the definition of the Brown measure, you But in the definition of the Brown measure, you want epsilon to be zero. So, what I need to do is I need to try to choose my initial conditions, the lambda naught and the epsilon naught, if possible, so that lambda t will be my favorite complex number, lambda, and the epsilon of t will be zero. That's the key thing, right? We want to evaluate at epsilon equals zero. So I want to solve the equations so they come down and hit epsilon equals zero, okay? Okay, so the question is: well, how are you going to choose your initial conditions that would give you epsilon of t to be zero? And one simple idea, if you look at the ODEs, you'll see this idea is sort of reasonable, is if you want epsilon t to be very close to zero, then you take epsilon off very close to zero. Again, you have to look at the formulas, but it's pretty easy to see that that is a sort of plausible idea. You want epsilon of t, you try taking epsilon of t to be zero, you try making epsilon naught equal to zero. And in the formula, so you see, yeah, if you start at epsilon naught equals zero, the solution will just run along with epsilon of t equals zero the whole way. And so you say, oh, this idea is great. I'll just put epsilon not equals zero, and that's it. The difficulty is that if you put epsilon not equal to zero, then the solution. Epsilon not equal to zero, then the solution of the whole system may cease to exist before the time that you're interested in, and then the Hamilton-Jacobi method doesn't apply. So, in order to find out, well, when does this simple idea work? When can I just put epsilon naught equals zero? And that's going to kind of give me the answer. Well, what you have to do is you have to look at the existence time of the solution with your lambda naught is arbitrary and the epsilon naught is zero. Okay, which you can compute this. Okay, which you can compute. There's a formula there for what is this short, small epsilon naught existence time. And then the simple method is going to work where you just take epsilon naught to be zero. It's going to work provided that that existence time is bigger than the time t you're interested in. Okay, if the solution blows up, the solution blows up before time t, then you cannot apply the Hamilton-Jacobi method. Okay, so what does this say? Okay, so what does this say? What's the result? What do you get from this sort of simple-minded method where I say, I'm just going to take epsilon naught to be zero and see if the solution will work? Well, when it works, what you find, you know, you have to plug in and do some calculations, is you find that lambda of t depends holomorphically on lambda naught, and then you can compute the Brown measure from that, and you find that it's harmonic. That it's harmonic. So, the upshot of this is this simple-minded method where I'm just going to try to take epsilon naught to be zero. If it works, it's going to tell you where it works. It's going to tell you that the brown measure is zero in that region. To the extent that it works, it's telling you the brown measure is zero. So, it's basically telling you the region in which the brown measure is zero. So, we have what I call the good set in the lambda plane, in the lambda not plane. In the lambda naught plane is the one where this existence time is bigger than t. And then we have the good set in the lambda plane, which is you take a lambda naught in the good set in the lambda naught plane, and then you solve the equation and you get your lambda t, and that's the good set in the lambda plane. And the upshot of this calculation is the Brown measure is zero, zero in the good set in the Lambda plane. Okay, so it basically this method tells you where the brown measure is zero, which in a sense is boring because we're more interested where the brown measure is not zero. But in this picture, the good set is the outside part, okay? The gray part is currently unknown terra incognita. Is that on the outside part, you can just put epsilon naught equals zero. So you start with it, epsilon, start out here and you solve your equation here, and you run. Solve your equation here and you run over here, and you get a lambda over here, and that tells you the good set in the lambda plane, and that's where the brown measure is zero. So basically, it's telling you that the region where the bad set in the lambda plane is where the brown measure is going to live, right? So you've basically found the complement of the support of the brown measure. You still need to find what's happening in here. We've just found out here where the brown measure is zero. Okay, so now comes the interesting part, in a sense, is where the brown measure is not zero inside the domain. Now, what we've done so far is what I call the ground assault method, because like I like to picture the lambda plane as being sort of horizontal and the epsilon as going vertically. And the previous this method was basically saying that we were basically going to try to make Saying that we were basically going to try to make epsilon of t just identically equal to zero, that the epsilon of t was just going to run along the horizontal plane, just epsilon is almost identically zero. Okay, but that only tells you what's happening in the region where the brown measure is zero. So now we need to say, well, what's how do we get into the region where the brown measure is not zero? And we use the error salt method, like you really have to make epsilon positive. So I'm really gonna start up here. So I'm really going to start up here with an epsilon as positive, and I'm going to try to run along a solution. And only at the very end of the, only at time t does the epsilon come down to zero. Okay, and you try to land on your favorite spot inside this region. Okay, so I'm running out of time. And this is the part of the paper that's, of course, occupies a bulk of the paper, but it's hard for me to explain all the details. Paper, but it's hard for me to explain all the details of the calculation. But I'll just tell you, first of all, that there's a surjectivity theorem, that in theory, you can land a cannonball on any point inside that domain, and therefore you can try to compute the Brown measure. And then there's some kind of nice decoupling when you solve the equations that you see the A's and the B's decouple. Like this line only involves the A, which is the real part, and this part only involves the B. Sorry, one involves just the imaginary. The imaginary part of lambda one involves only the real part. So the real and the imaginary part somehow decouple, and that helps explain the simple nature of some of the formulas. Okay, so here's just a picture, right? We're now trying to go inside these domains. So I start in this so-called bad set in the lambda-naught plane with some positive value of epsilon that you have to figure out, and then you run. That you have to figure out, and then you run over here into this bad set in the lambda plane. And this map has the map that you get when you solve the equations has the property that the vertical line segments over here map the vertical line segments over here. That's sort of the decoupling that I was telling you about. Okay, now what's the theorem? All right, you know, there's, of course, a lot of details that I have no time to go into, but you can use this method. But you can use this method and you can actually compute the Brown measure inside the domain. And you find what we mentioned before: that the Brown measure depends only on A inside the domain, right? Outside the domain, the Brown measure is zero. Inside the domain, the density depends only on A, which is the real part, which means the density, therefore, is constant in the vertical directions. And there's some kind of formula for the density here in terms of, you know, you go back to the lambda. Of you know, you go back to the lambda naught from which you got this lambda, and you get a formula. Okay, so we get a more or less explicit formula for the Brown measure. Here's an example, same example I showed before. So remember, the density is supposed to be constant in the vertical direction. And what is that constant density? Like, how does the density vary on the horizontal axis? Well, this graph down here. Axis. Well, this graph down here is showing you that. So, you want to know what the density is here. You just go down here and you find this height here that's telling you the density of points along this whole vertical segment. And, you know, we have, you know, we can compute that bottom graph. Okay, so very quickly out of time, but I want to mention a kind of interesting connection between x plus i sigma t and x plus sigma t. And x plus sigma t. Okay, so I'm going to skip that slide and say we're going to define a map from this domain here where the brown measure lives to the real line. And this map is going to take it's going to take this whole vertical segment here to a single point over here. Okay, so it squashes the domain down. Okay, so it squashes the domain down onto the real line. And the theorem is that if you take the brown measure of x plus i sigma t with the i there, and you push that forward under this map q t, what you get is the distribution of x plus sigma t without the i. So there's some kind of mysterious connection. Like, I mean, we have a theorem, but like, I don't quite say I understand cosmically why it's. say I understand cosmically why it's true that somehow the behavior x plus i sigma t and x plus sigma t are related by this map. Okay, so I believe that is an excellent place for me to stop and thank you very much. All right maybe everybody can take the opportunity then maybe we Maybe we can ask some questions before.