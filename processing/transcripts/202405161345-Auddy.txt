And this is some very recent work, and in fact, like I'm learning this whole topic very recently. So, if you have feedback and questions, please stop me. So, I'll tell you today about single-cell genomics, which is this new technology of getting RNA sequencing profiles at the level of individual cells. And gene sequencing, this sequencing has been around for quite a while, but pre-2010. But pre-2010 it was mostly at the bulk level, where we looked at tissues and had the gene expression level only at the levels of tissues. And this led us to just an average, like we did not have the resolution at the levels of cells. So for every tissue, we had one gene expression. With some recent advances, mostly in isolating cells and what's called this next generation massively parallel sequencing technology, this single cell has signed. This single cell has single cell genomics has recently brought out a recognition. In fact, it was recognized as the method of the year in 2013. And why is this very exciting for us from bulk sequencing to single-cell sequencing? It's of course because we get down to the resolution. We are no longer restricted to just the mean. We can talk about individual observations at the levels of cells. I tell you this in more detail in this slide. This is in more detail in this slide, where you have an example where bulk sequencing simply does not give you the answer you're looking for. Maybe there are some patterns in the data, but bulk sequencing would miss them. So for example, if I had, let's say this example, where I have this bulk sequencing example where I look at this tissue and look at the total RNA and the run extra sequencing process, since everything is Process, since everything is reduced to sort of this mean effect, maybe the gene X indeed has like more or less similar expressions. But once I go down to the single cells, once I go down to the distribution level, then the gene X might have two different expressions depending on what cell type we are looking at. I want you to look at the very right panel of the figure, where up top you have just the average and below you still have the average, but it's grouped into two heterogeneous groups. Grouped into two heterogeneous groups. And single cell allows you to take care of this heterogeneous. So I should tell you now what this single cell RNA sequencing measure. It's simply a count. So you can think of this as x, i, j, k. For individual i, you have the measurement for cell j and g. I will, in fact, write this down here because this would be very important, and I will come back to this from time to time. So i is your individual who you supply whose data you're working on, j would be the cell, and k is the gene. So you can also think of this as a vector index by i and j which lives in, let's say, partial. So for every individual and every cell, I have one such vector. What does that allow me to do? I can talk about distribution. That allows me to do? I can talk about distributions now that I've been mentioning previously. So, for example, I can have this empirical measure, right? So, I could describe this, let's say, nu hat ij, which would be of the form one over sum of mi sum of the delta, the data quite term this the cell J you you can assess the cell chemistry. Assess the same cell, the same type of cell individuals? Yeah, so that's a great question. So I'm assuming that I have G running from 1 through MI. It does depend on the individual. And everything depends on the individual. I could possibly look at different cell types. The number of cell types could be different and so on. That's right, yeah. You're averaging here over genes, like and with that. I'm averaging over. I'm averaging over cents. I actually don't write this way. And then it's open UITA, right? Oh. No. Oh, let's say I have just this. Sorry. Sorry about that. I have just this expression. Right, so that's a great question. Like this SEN, JH SEN, it does depend on what the individual is, and the SEN type could be. Individual is, and the cell type could be varying from one individual to another. What that says is, I could not look at this as a third-order tension, right? Because I don't have the exact mapping from cell one from individual one to cell one from individual two. Those mappings don't really make sense. They could mean very different things. So instead, we would work with this average over cells. And what people try to do is, once they have this distribution, they try to relate this with other information that's available about the patients. With other information that's available about the patient, for example, how this distribution evolves over time, with age, and so on. Another point here is that we are working at the distribution level. I cannot repeat this experiment as in, like, I cannot measure the same cells from the same patient again and again. Once I measure them, those cells are gone. And each time I'm looking at a new set of cells. So, that's the question we would try to understand here. Let's say you have this flow over time. You have this flow over time, and I want to track down some sort of a blue curve out there, which is the true evolution over time. Of course, we do not have that, we have some estimate. And the way we would do this is by regressing these distributions that we have over some covariance. The specific framework that I'll use today is called the Frey regression, and it has some so given a distribution between So, given a distribution between any two objects, any two random objects, I can define fresher regression in this property form. So, it's motivated by the fresher mean, which is the minimizer of this distance over the space of all y. And if you have covariates instead of the mean, you would minimize the conditional mean. And note here that I have some weights which are denoted by s with the two arguments: a small x and a capital X. The small x is the point. The small x is the point you are measuring it at, and x is your observed information. There is a diver here where I have this small x and capital x, one of them should be a small x in the left or right. And this fresher regression framework is motivated by these first. If a linear model were inde true and if I had the standard delta distance, this Fraser conditional mean would s be the same as Pikmin. Being the same as white energies. So we were working with distributions, so I have to give you a distance between distributions, and we would work with this Wascher sign distance. The motivation for using Washer Sign distance also comes from this picture I had before, where you are trying to track down this flow, and people try to understand that, like, as you move from one point to another, they would take some sort of optimal path, as in the least fast path, and that motivates the use of Washington least. That motivates the use of Wascherstein distance. So, I won't go into detail about how Wascherstein distance comes about. It has to do with this optimal transport and everything. But given two distributions, mu1 and mu2, this is how you could sort of calculate this Wascherstein distance. I say this sort of because it's very hard to calculate in practice. Like, given two d-dimensional distributions, a standard form of the Wascherstein distance doesn't really give you anything. It's not easy to compute. Anything. It's not easy to compute. However, this is the definition. You want to find the minimizer of this quantity over the set of all possible couplings with marginals mu1 and mu2. And for one-dimensional case, you have this standard expression. If, let's say, both mu1 and mu2 have densities with respect to the levic measure, then you could write this as the difference of the inverse function squared. And they're And they're integral taken from 0 to 1. In case of the Gaussian density as well, you have some sort of an expression, but it's still complicated. If you have, let's say, two distributions with mean theta 1 and radian sigma 1 and mean theta 2 and radiance sigma 2, you could write this as A2 difference of the means squared plus a trace of a complicated expression. So you have sigma 1 plus sigma 2 minus sigma 1/2. minus sigma one half, sigma two, sigma one half, the whole thing raised to the power of half. And by the way, I want to mention here that the half I am using here is, this is the symmetric square root, which makes things a little bit more difficult. I'm not using the standard like sigma equals L L transpose. I want sigma equals L square. That complicates things. Okay. So going back to the question that we had before, how does gene co-expression depend on a? Does gene co-expression depend on age, or how does gene distribution depend on age? So instead of the mean effect, we want to, let's say, capture the variance. So I have this distribution, and I want to look at its variance. So for single-cell data, I have for each patient, I have this gene co-expression, which is essentially a variance matrix, or let's say a correlation matrix that I've obtained for each individual. I have observed that by averaging across all the different space. And I want to. And I want to ask the question: like, how does this variance matrix depend on each? If the data were indeed normal, the Waterstein thing that I just motivated, it would be exactly if you will. Doing the Waterstein regression and tracking these variances would be exactly the same thing. So that's what I say here. If I have, let's say, a normal data, theta sigma i for either patient, and my null hypothesis is that the sigma i does not depend on the covidence. For example, it doesn't depend on H. So I would. So I would use a fracial regression framework that already supplies what those weights are. So for the jth individual, I have this SIN weight. And I would compare two things. One is this conditional friction, which is used, which uses these weights depending on the covariates. And the other is the standard, which depends on only X bar, or let's say the equal weights at all the different data points. So, this would take me to the Waterstein vary centers because remember, we were working with pressure regression with Waterstein distance, and given a set of weights, I want to minimize Waterstein distance, the expectation of the Waterstein distance. And that is, in fact, minimized at the Waterstein barycenter, which can be tracked down as follows. And now I'm entirely working with Gaussian densities. Let's say I have random Gaussian densities with different mean. Nausian densities with different means and different covariances, as in the means and the covariances are random. Then I have some sort of barycenter weights. Then the mean expression is really standard. It's just your standard mean. But the variance is given by a fixed point in version. It's more complicated and it would have a solution like this. Given a set of weights w, you could write sigma star to be the solution of this fixed point inversion. Once again, this is where the symmetric Once again, this is where the symmetric square root creates problems. If it were not the symmetric square root, we could have factorized it and done something. And under the null hypothesis of no effects, we want to say that it doesn't depend which freshening you are looking at. You could look at the conditional freshening, but it would not depend on the coefficients. So, this is my test statistic. It starts to look more complicated, but it essentially looks at the Essentially, it looks at the washer strength version of what we do for standard least squares. So instead of least squares, well, you would have just end to distance, I now have the washer strain distance squared, and that looks like those two quantities, right? One is your prediction from xi, and one is your just your sample prediction, sample mean prediction. So it's sort of like the sum of yi hat minus y1 in our standard linear integration system. And this also could be broken down into different terms. Could be broken down into different terms with the mean and the covariance dependence there. And now things are more complicated because, in practice, you would never observe what the true theta e and sigma i are. The theta e and sigma e are random, but what you have seen is observations from a normal distribution with those uneven invariances. So that creates randomness at two levels. First is I have the random sigma i, which depends on covariates or it might not depend on covariates, and given those sigma i and theta i, I'm Given those sigma i and theta i, I'm sampling from those Gaussian distributions. Right, so that creates the two sources of randomness. So I cannot use this f star up top directly because all of those theta i and sigma i are unknown. And so I cannot compute, let's say, theta star, sigma star using those like two theta i, sigma i directly. So I have to plug in the sample versions in. So we would make life So we would make life simple here and just work under the simultaneous diagonalizability assumption. This would be the key assumption of T stock, where I'll say that, well, I do not want to go into trouble with the matrix square root. I assume that all the covariance matrices are simultaneously dragging. And that simplifies things significantly, right? It says that you have different covariance matrices, but the eigenvector matrix always stays the same. What is changing possibly is the eigenvalues. So I have sigma equals u lambda u transpose. The u does not depend on the individual at all. I have passed on all the covariant information onto the eigenpartures. You could also look at this as a third order tensor. If I could stack those covariance matrices along the third mode, then I would have a d by d by n tensor, where the first commodes would be just the singular vectors or just eigenvectors ej and the last would be a non-negative mode, right? So that would A non-negative mode, right? So that would attain the non-negative eigenvalues. So what we do here is we do not work with the general case and we would compute pressure depression under this common PCA assumption. Common PCA, by the way, goes back to this work of Flori all the way back in 1984, who worked out the asymptotics of common PCA in the fixed dimension case. So in our case, we do allow the dimension to grow, but we allow the Do allow the dimension to grow, but we allow the frequency n to just be under the CPC assumption. And now you can write down a closed-form solution. I do not have to solve a fixed point equation, I have this nice closed-comp solution. So if all of those eigenvectors are u, in terms of that eigenvector matrix, I can pass on the weights inside and compute what the true signal is, what the true fresh and significant. So the idea here is that SINX is something you know from the covariates, u is something you can sort of. U is something you can sort of estimate, pulling all the covenants matrices together, and we pass on the rest of the information to the base. So, this will be our test statistic under CBCA. First of all, we get rid of the means because I could just center them. And remember, the fresh mean of the means was simply a weighted average of the means. So, I could just take them out of the data by centering. I could not do that for covariances, which is why we go to all this trouble. But, however, under this Uh but however under the CPC assumption I can write down the very practical form of the of the covariances. So x star tau simply becomes the Froving Islam squared of sigma star Xtreme minus sigma star X bar, I squared the alpha and sum from all the different individuals georenic components. So covariates can only affect eigenvalues of sigma i and now f star can be written as this short expression. I do not have to work with Waterstein. I do not have to work with Watts time distances anymore. I've totally reduced it to an eigenvalue problem. So this simply becomes gamma hat times Z transpose rhyme V sum squared, where gamma hat is the square root of the eigenvalues of the sample combination matrices. And Vx's are the eigenvectors of Zx transpose Zx. The exact form of this is not important. The important thing is, like, I would work entirely conditional on the covariates. So, Vxs is something known for us. I do not have to compute it. Known force. I don't attack to compute it. It's like standard. It's given from the code. I don't attempt to estimate it. So this is a sample version, and how do I go about it? Go back, please. If you're taking some sort of mean, then the covariance of the covariance is very independent. Right, yeah. Right, yeah, but here it seems you you're taking average of the sending issue because it's very rich and um so okay so here I'm taking it uh yeah I'm just saying it very because I don't completely understand so uh what I have in mind is uh let's say I observe the true mu i and sigma i right with the true mean and the true sectors Right, with the true mean and the true centers of all the Gaussian distributions, those are random, and let's say I observe them, then this is how I would compute the fresh air. So I'm not really saying that the variance goes down or something. I'm just saying that the fresh air mean would be looking like this. Maybe I did not answer your question completely. It's probably a different type of mean than from what I'm thinking about. But thank you. Okay. Okay. So yeah, so what this notation sigma star xi essentially means is that I go to that covariate and see, using that covariate, I use its information to come up with a local conditional. So that's what the fresh medium does. And I'm saying that if I indeed observe the true density at that covariate, for that's higher than usual, then this would be my estimate. But yeah, so I'm not talking about sampling error here. Yeah, so I'm not talking about sampling error here. Let's say I know the true sigma i. That's that's what we are talking about. With sampling error, it's indeed more complicated and we would see a sample size difference. Okay, so right, so I want to use the sample version of the if statistic right under the CPCA assumption. So this was our common principal components assumption which like simplified things quite a bit. And what I would do is I would estimate the Would do is I would estimate the eigenvectors using the pooled covariance matrix. Here, for example, pooling would help, and I come up with this estimator h sigma hand is just taking all of those like individual, all of those yij's, yij transpose, and looking at the u component functions. Now I can estimate u by u hat, which would be, for example, I'm calling this the left singular vector, but it doesn't matter, it's the same for the left and the right, and I can estimate from u. I do have a sample splitting set here, which I do have a sample splitting set here, which helps me theoretically. So, for example, I use the first half of the samples to compute this u, and in the second half, I multiply all of those eigenvectors with whatever I've observed. So, the second half essentially computes the variance in the uf direction, right? I project the yij's in the direction of uf hand, I compute the variance. Okay, so our test statistic, we get some estimates of the eigenvalues. This is our sort of improved estimate of eigenvalues, which Sort of improved estimate of eigenvalues, which takes into account the principal components. This is our extra happiness. So I can tell you what the null distribution is now. Under the null, there is no dependence on the covariance. So essentially speaking, I would assume that all the sigma i's they are random, but they come from the same distribution. Their distribution does not depend on the that's why null hypothesis. So, under the null hypothesis, I have this sort of So, under the null hypothesis, I have this sort of a null distribution, which is a mixture of the chi-squares. So, all of those are independent chi-squares random variables, and the weights are given by eigenvalues of a certain matrix. The eigenvalues have two sorts of dependence. So, first is I have this matrix dk star, which is the expectation of lambda k under the true, under the identical model, where I've assumed that all of the sigma i's come from the same distribution. It's the expectation with respect to that. And the second. Respect to that. And the second is variance of square root of lambda k, which is the variance with respect to that eigenvalue with respect to that column structure. Here you would indeed see dependence on the sample sizes, M1 frame. Yeah, as Han was previously asking, right? So here you would indeed see, once I use the pooling of the data, I would indeed see a sample size. But the key point here is I do. The key point here is I do see the two sources of randomness. First is at the sample level, since I have sampling from the distribution, from the Gaussian distribution, I see that sample size dependence. And second, the true things are indeed random, so I have the variance going on. So these are the important features. You have this has a distribution and so on. A very good question to ask here is: what could we do to remove the normality assumption, or how critical is this to this entire framework? Or how critical is this to this entire framework? Of course, if you removed normality, the connection with Washerstein distances would go away. But you could say that I don't care about tracking distributions anymore. I would just track the, let's say, co-expressions or covariance differences. And that would be sort of, that would be a limited question, but that could still be answered by the framework that we just looked at. In terms of the asymptotics, the asymptotics would still work, but you would have to move from a fixed design to a random design. Fixed design to a bit like the random design setting because what we essentially need is Zx transpose Zx are incoherent eigenvectors. Under a random design setting, this would be probably satisfied and we can get a problem with the transaction. So null distribution does indeed depend on some quantities which you do not know. For example, the expectation of the lambda k and the variance of lambda k. Under the null, I can plug in the sample quantities. This should be a variance of square root lambda k. So under the null, where everything is the same, where all the sigma is are coming. Is the same where all the sigma i's are coming from the same distribution, I can plug in those quantities and come up with an estimate. And what quantity computes the power or like determines the power? It would be this thing, which is like mu lambda transpose zx transpose zx mu lambda. And mu lambda is the mean vector of the square root eigenvectors. So if this quantity is diverting, then I would see power over any one. So I'll show you some quick simulations to. So, I'll show you some quick simulations to convince you that this is indeed our theoretical results are backed up. So, like backed up by actual experiments. So, here on the left, I have some simulation from the non-distribution and plot the QQ plot. And you have a very good fit. The theoretical quantities that I've called, those are also sort of Monte Carlo because it's simulated from a chi-square distribution. So, we take repeated samples from that mixture of chi-square distribution and uh compute theoretical quantiles. And computing theoretical quantities. And on the right, I have this power of the f-test, which depends on the quantity delta. So here, the alternative I'm considering is given on the model here. Lambda 1 depends on, so let's say delta times x1 plus x2, and they have the mean difference, right? So 5 plus delta times x1 plus x2, and lambda 2 is 2 times x1 plus x2. And I repeat this eigenvalues for, let's say, the first 95 and the last 25. You would see I've taken x1. You would see I've taken x1, x2 to be uniform between 21 and 70. This is to mimic sort of the age distribution that we have in our data. This is what I have in mind. In general, you could have any covariate range and you could plug it. So I could ask the same question now for partial effects. The thing that I just mentioned was like null effects, where the covariates have no effect at all. They could say that only a subset of the covariates are now useful. So let's say p equals p1 plus p2. So let's say p equals p1 plus p2 and only the first p1 would be useful and sigma i depends only on let's say xi1. And now I can motivate a partial f-statistic which is given by sort of the difference between looking at all the covariates minus what you would get looking at whatever you're laying out. I'm sorry, whatever your talbot is set. So the partial depth statistic earlier was this probability Here was this Robenius norm squared of gamma hat CX transpose, but now I'm subtracting a quantity from it. So this also has asymptotically a chi-squared distribution. Now the null hypothesis has changed to like partial effects, where I'm saying that sigma i can depend on the covariates, but only on the first subset of covariates, not on this thing. I'm not going into too many details about like the asymptotic distribution. Of course, you would imagine the eigenvalues of this chi-square, of this asymptotic. Of this chi-square, of this asymptotic equilibrium distribution, they are different. They are sort of depending on the conditional distribution of x2 given x1. I'm taking out the effect of x1 completely from the covariance, and whatever is remaining, that's what determines those problems. Okay, so like, of course, I had some, I had like one slide there where I said that you could stack this up into a tensor, and instead I did not go to the And instead, I did not go to that route and we just took the matrix version of methods where I just pooled the data and everything. So I should tell you why this is important, looking at it as a stack matrix versus a pooled matrix. So of course this will determine the speed of convergence to the asymptotic distribution depending on what's really going on in the background. First of all, there are eigenvalue issues as in identifier issues. So if you have the pooled covariance matrix which does Matrix which doesn't have eigengap. Now I'm using that to compute eigenvectors. It could mess up, like, it could perter all the different eigenvectors in a way which in a way which completely destroys or results, right? So what I do is I assume eigencap at every level. I say that sigma i are indeed random, but all of those sigma i have non-trivial eigencap at the level of 100. So hopefully I have enough samples. So with Hopefully, I have enough samples. So, with this 1 over t eigen gap, I can assume push the results. And since we are using matrix version of results, I can use eigenvectors from the cooled covariance matrix. It does depend on the eigengap. So, I have depends on, you can say lambda ik minus lambda ik plus 1, but I have a shift that's of the order 1 over g. So that contributes to a power of d squared, right? So, I have d squared divided by the number of total number of samples. That's what determines my speed of convergence to the power. So, there is a So, there is a question that if this can be improved by looking at it as a tensor, it is attempting to do some sort of a power iteration method here, but I would say that that would require slightly more assumptions. This is, of course, some stuff that I previously worked with Professor Minyon, and also this is the stuff Johan was presenting a couple of days back. It's tempting to do that, but unless you have more assumptions. Unless you have more assumptions on the eigenvalues, for example, I can treat it as, let's say, if I take out the mean of the eigenvalues, maybe they behave sort of orthogonally or they have some sort of an incoherence within the centered eigenvalues. This kind of a method would not really give me much of an advantage. I would end up with the same method. But if indeed that happens, let's say all those lambda i's have some fixed expectation, and once I take out that expectation, everything is mean zero, and I can treat it as a mean zero. Everything is mean zero, and I can treat it as a mean zero factor, then certain advantages are possible. Okay, so how much time do I have? Like 10, 5, 10 minutes, 12 minutes? Okay, that's a lot. So I go back to the single cell application that I started with. This is what our motivating example was, that I have this gene co-expression for every individual, and I want to track whether that gene co-expression depends on the To expression depends on the age of the people. So, this is 982 healthy individuals whose data we have collected. We had just age as appropriate, and the rest was alternative. So, the framework that this study has had in mind, this Diaz study, whose data we are using, so they started to try to understand how this nutrient power. How this nutrient pathways change as we age, right? So, for example, our body processes insulin, how that thing changes with age. And so, these genes only concentrate on that nutrient pathway. So, you have 61 genes which talk about that stuff. And for the cell type, they did have many different cell types in their data. We looked at only at the specific type of cell type because we would be averaging more of the cells, we need some sort of homogeneity there. So, the type of cells that we use there are called like naive and long memory CD4 T cells, which are a type of white blood cells. And these naive and long memory, those are terms used to describe how they act. So, naive is just like those white blood cells which just attack whenever your body is in trouble. And long memory is something like sort of if you had an infection long back, they remember exactly what works and only that's the. What works and only impacts them. So it's sort of, I don't know, your usual soldier versus special forces, neither longer consider stuff. So, what we do here is for each of these cells, I will take the average as before and look at the various matrices across these genes, these protein-pathy genes. And that reduces our data to, let's say, 734 individuals. We need a certain amount of sample size, so we had 200 cells here, and of course, these genes are very sparse. These genes are very sparse. This gene expression data is very sparse. So we chose only those 26 genes which are represented in, let's say, 3% of them. And this is the question we want to answer if the sigma i changes or not. Okay, so we do indeed use this common PC assumption. And in the fixed dimension case, there are likelihood pressure tests you could run to confirm it. That is, we did not do that. So here is like three different figures to sort of convince you that. Different figures to sort of convince you that it's not too bad, at least for the first three eigenvalues, it seems to hold. If I look at the true eigenvalues and look at this eigenvalues computed under the Kauf PC assumption, there seems to be a good match. The line that I'm plotting is against C sub equals H line, and at least for the first three eigenvalues, I have a completely good match. So let's say this assumption holds. Of course, I am assuming the complexity assumption. Under this assumption, I would have this kind of results. Assumption: I would have this kind of results. The like null effects test, we had this like one covariate, right? H. So that null effects test was indeed rejected. The Frecher F test that I've told you about, that was rejected at the point p-value of 0.03. And these were the figures. So, of course, you see some sort of change as people age. So, for example, there seems to be like more variance, but this line that I had over there. This line that I have over there tracks the sort of mean effect, and that also seems to grow somewhat. It's not a very pronounced effect, but, for example, for the first PC, you start out at 0.2 and end up with something around 0.23. So, some sort of an effect is there. And if our model is indeed true, we get a reduction of the null hypothesis. So, the next questions would, of course, be: can we predict using this? Or this was just a rejection of the null hypothesis. Just a reduction of the null hypothesis, which is to say that the gene co-expression does in fact depend on age. Of course, you would want to ask how we can predict and stuff. Using this pressure regression model, you can push it somewhat using essentially the same things, right? So all of this pressure means I could now compute it at new values and say that these are my predictions for that age. But I don't know, maybe something better can be done using more structured configuration effect ones. So I'll finish up now saying that this was the motivation of my talk today. That's here single cell data, and single cell data is exciting because instead of the average, I can work with the entire distribution of regressions. So now I can track how the distribution depends on different covariates, how it evolves with time, and things like that. So, fresh regression under Gaussian T assumption is something which is somewhat tractable, but then we ran into this matrix square root issue. So, we again reduced. So, we again reduce your CPC assumption. So, going beyond Gaussianity would mean that you can use this framework to, let's say, other kinds of densities. You can definitely do this for univariate densities, for example, but beyond that, I think there are not too many results using this Washerstein regression beyond one dimension and Gaussian densities. Okay, so the future directions. Of course, one big roadblock here was we use this common PCS option. So, how would you go beyond that? So, how do you go beyond that? What do you need are sort of these perturbation bounds of Washington Valley centers, which are not very tractable in practice because of this symmetric square. So, recently, there has been another work which is also in our group that Li there is Hao Je Li. So, he worked with a student to track down what the distribution would be. Well, you have infinite samples. So, you no longer have the sampling error. Let's say you have just two sigma i. Say you have just two sigma i's and that no longer follows the CTCA assumption. But what they did is they did not work with this statistic itself, they looked at some modification of it, some first order modification of it, and they also got an asymptotic high-square distribution, sort of mixture of high-square distribution. Yeah, so but yeah, beyond that, I guess you need more, more structured assumptions. Of course, you could reduce the CPCA assumption to say instead of all Assumption to say instead of all the eigenvectors being the same, you could reduce this to, let's say, just the top rank k eigenvectors being the same, those kind of stuff. Yeah, so that's it. So these are some references. Of course, this pressure regression framework was developed by Hans Müller in this 2019 paper, and we heavily use that. Our framework is heavily based on that. And the second framework, second one, mostly talks about one dimension. One dimension. If you had, let's say, one dimensional densities, what happens there? And the last two papers, the third paper is mostly tells you how to go about computing wash-stand bias interest in case of Gaussians. And the last one basically supplies our data. This was the study that we had used for. Okay, thank you very much. Never mind, go ahead. I assume so you use Waster Stein2 distance because you can compute everything? Or is the reason? At least Washer Seng2 gives closed form expression, yeah. And so there's no intrinsic reason for like I guess V1 or V3 would work as well and it's just a constant that changes well. So Wasserstein 2 or Wasterstein 3? Using Wasserstein 2. But if I if I so I would I would guess the results did all go through for Wasserstein 3 for example or 2.5 just the constants change or I'm not sure like how the Washerstein 3 between dodge has looked like. I guess the Washerstein 2 has this Watching 2 has decided. It is possible that there are results like that. I don't have an opinion. I think just it's much harder to do the computations, but I think in the end, at least asymptotically, the constants just change. Okay. Thanks for the big talk. So before you started talking about this common PC assumption, you had like some format for what this form for this in general any general weed thing right so can you say a bit more about that like what the challenges are in computing this for a general weight profile like if i did some sort of weater is that regulation scheme or something uh yes so uh this this paper uh this second paper here like sino tv at quarters uh they do exactly that they take a pretty decent approach to this problem but uh Approach to this problem. But I think it's still a work in progress, like how the error converges, for example. But you have some convexity in Mosher Status, please? Yes, yeah, yeah. So, yeah, in the general case, that's the route if you would take ideas. Any more questions? Maybe we should take a shot. And so kind of discussion, maybe we should take a short break. Make sure we're speaking again. Five minutes. Perfect. There's one extra talk to say. Well, that's okay. Thank you. You have a T to a building part, it's so like a T. 