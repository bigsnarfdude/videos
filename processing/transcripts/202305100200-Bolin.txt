Okay, so yeah, you also have 30 minutes. So, David will be talking about Gauss matter and fields on metric graphs. Thank you. All right, good morning, everyone. So, this will also be, I guess, in the theme of this morning session on complex spatial models, but more Complex spatial models, but more not so much on non-stationarity, but more on complexity of the spatial domain itself. So, what I'm interested in is to model data on networks such as ribbon networks or road networks. And this is actually quite common in stream value analysis. I borrowed this figure on the left here for some river discharge. Some river discharge data in the DNU river network from paper by people at this workshop. And another example of this is actually what Rafael was, I think your first example in your course on Monday was also river discharge data in the extremes conference. Another classical example that is not in extremes is this Shaskel colour outbreak in London, where you have measurements. Outbreak in London, where you have measurements along street networks. All right, so what I'm interested in is to model data of this type using Gaussian processes. An important thing is that we want to measure distance here in the network. So, if you talk about the kind of the covariance between two points in this river network, this should be a function not of the Euclidean distance, but of the kind of distance that you have to walk along the network. You have to walk along the network itself. Also, we want these models to be defined in continuous space, so not only say at the vertices of these graphs, because we want to be able to do things like 3D. Right, so that's the setup. Now so why metric graph then? So the metric graph is our well not our but that's kind of a common alternative name for for network. Name for network. So the mathematical model of the domain itself is that we use a compact metric graph. So this is just like a normal graph. We have a finite set of vertices. We have a finite set of edges that are connecting these vertices. But what makes this now a metric graph is that each edge is actually a curve that is parametrized by some arc. That is parametrized by some arc length, and this is like we define the process now not only in the vertices but on the vertices and on these edges. So these curves, they, I mean, they might be straight lines, but they might also be standard curves. And then we equip this graph with the shortest path or Euclidean the geodesic metric. So the distance between two points is simply the shortest the distance of the shortest path. The distance of the shortest path that you can have to go work to get between the points in the network. Right. So if you want to define a Gaussian process on an object like this, kind of the, if you work with Gaussian processes, the first thing that you might think of is to do this in a covariance-based fashion. So you can simple idea is to. Simple idea is to take some isotropic covariance function R and then define the covariance function of your Gaussian process like this. So I guess R of this of the metric V on the graph. The problem with that is you then have to make sure that your covariance function is positive definite in that metric. And that is a very difficult thing to verify. It turns out that most standard covariance functions that you use in Rd are not valid, are not positive definite when you measure distances in this non-Euclidean metric. There are some exceptions. One exception is that if you restrict, so this is an example by this nice paper. An example by this nice paper by Anders et al. from three years ago now, where they show that if you restrict this not all metric graphs, but you look at the subset of graphs that have Euclidean edges, not so important what that means, but yes, subset of all graphs, then this Matern covariance is valid if you do not use the geodesic metric, but you use something called the resistance metric. Resistance metric, and if you restrict the smoothness parameter to be at most a half. Now, as far as I know, there are no constructions whatsoever to define differentiable Gaussian processes in this setup. So, this you can kind of see here as well, this new less than or equal to a half, that means essentially that you have continuous sample paths, but not differentiable if you know the role of this Moon triangle. Know the role of this mood-friendly in your maternal covariance. Also, and this is maybe more of a discussion for the after the coffee break, but in my opinion, isotropy is not something we want in this setup. And this is maybe a question you can think about: is isotropy a realistic feature if the topology of the domain itself is changing? I would. I would argue no. Now, another kind of more well, less subjective reason is that, as we have recently shown, there are no Gaussian processes on general metric graphs that are both isotropic and Markov. So, you would have to choose. If you want to have a Markov property, you cannot have isotropy. And then And then I think then, if you want to choose, I think Markov properties are usually more realistic. What do you mean by Markov there? Are you conditioning on the value of the nodes? Or something in any way? So, I mean, it is the kind of in the random field sense of marker properties. So, basically, yeah, if you condition on, yeah, for instance, if you want. For instance, if you want you condition on two vertices at the endpoint of an edge, the process on that edge should be conditionally independent of the process outside the edge given those values. Is it also for higher order? Yes, so also for, I mean, so if you have higher order, then you condition on process and derivatives in devices. So, I mean, one, what that, for instance, means is that even though if you're in this setting, you take Though, if you're in this setting, you take the exponential covariance and u equal to one-half, that is not the Markov process, which is kind of one of the key features of models with exponential covariances on R. But this is in general not Markov on graphs. And a simple example of that is actually as the circle. Model with an exponential covariance on the circle is not a Markov process. All right, so I will not. So, I will not go too much into this, but this Euclidean edges is also a bit of a restriction. So, here you, for instance, have a graph with Euclidean edges. Here's a graph that does not have Euclidean edges. So, essentially, one restriction is that you cannot have two edges connecting two vertices that have different lengths. And this is also kind of a restriction, especially if you think of things like street networks that might actually not be such a good idea. Right, so the goal that we had throughout this work was to create then possibly differentiable Gaussian processes on general metric graphs. And we also wanted to come up with efficient methods for things like statistical inference, simulation, rigging, and so on. And the idea is to extend this SPD approach. So, as we have heard. As we have heard also before in this workshop, we have this well-known connection by Whittle that a Gaussian field on Rd with maternal covariance can be viewed as a solution to this fractional order stochastic PD. And Finn and co-authors proposed in their 2011 paper to define maternal like fields on manifolds simply by defining the process. Simply by defining the process through the stochastic PD defined when you replace them R D by the manifolds. So that was kind of the idea. Our main idea was to extend this approach to the graph setting. And the takeaway message here is that if you do that, you can actually, so this will actually allow us to compute finite dimensional distributions exactly. Finite dimensional distributions exactly as long as this smoothness parameter is an integer. So, in other words, as long as this is a Markov process, we will not need any finite elements. So, we can actually work with this model exactly without any approximations in this case. So, that is the main results. Now, to get there, the first question is: well, First question is: Well, if you want to define this on the graph, then you have to define what you mean by the Laplacian on the graph. And that is actually not so straightforward. So if you have to look at what this operator would be on a single edge, then well, it should act like a second derivative on the edge. That's clear. But then there are basically the question is what happens when you have intersections of edge. Is what happens when you have intersections of ages? What happens in the intersections of your network? And then there are many possible choices of boundary conditions or vertex conditions on how you tie these things together. And one common choice in physics is to use the sort of Kirchhoff conditions. So these are vertex conditions that enforce continuity as well as a sum to zero constraint on the directional derivatives of the process at the vertex. The process at the vertices, if they exist. So, this is actually a natural generalization of Neumann boundary conditions. So, if you have a vertex of just degree one, then this is nothing else than a Neumann condition that says that the derivative is zero. So, this is what we started with. So, we will let delta gamma denote these operators, the Laplacian, that acts as a second. Operators, the Laplacian that acts as a second derivative with these Kirchhoff conditions. And this is actually a quite well-studied operator in so-called quantum graph theory. So there's a lot known about this operator. One nice thing is that the domain of this operator is quite nice. So basically where you can apply the functions you can apply this operator to are basically those that are twice differentiable on the end. Twice differentiable on the edges, they are continuous over the graph and they satisfy this sum to zero constraints. Yes. Can you interpret the Kirchhoff condition as sort of like making sure the flow coming into a vertex is the same as going out? Yes, exactly. Exactly. So, so this is actually a great model for hydrologic. Yeah, so that's kind of where, so this is really where this. Of where, so this is really where this is coming from. It's coming from this. Uh, I think, I mean, Kirchhoff worked a lot in kind of this electrical stuff where this is, I forgot the name, but some kind of potential electricity networks where this is really a natural physical kind of condition. So, yeah, I mean, that's why these are actually often called the standard vertex conditions in that kind of physics literature. In that kind of piece of literature as well, because they are, I guess, if you should guess on some, if you should use something without knowing anything else, I think that is natural. Right, so to define the model then, we define, we take this Laplacian and then we shift it by kappa, as in the standard Matern case, and then we define our model, our Gaussian process on the network as a solution. The network as a solution to this fractional order equation. Now, as I said, we know a lot about this Kirchhoff-Laplacian, so it's actually not difficult to show that we have a unique solution to this equation, which is an L2, as long as the smoothness parameter is at least a F. Yeah, so this is the solution. It's a centered Gaussian process. The solution: it's a centered Gaussian process, it has well that's a covariance operator, and it has some corresponding covariance function denote by rho. So, this is this is easy. Now, we can say a lot more about this. So, for instance, one, I would say, crucial property of this model is the following: that says that this is indifferent to if you. Indifferent to if you add or remove vertices of degree two in your network. So if you have you have your river network like this, and you have here are the three four vertices, and you define the Gaussian pulses on this network. Now, if I add some say observation locations as vertices and I define And I define now this Wicklamatern model on this new graph where I have added these vertices, that doesn't change the model, so we get the same covariance function. And also the other way around that you start with this and you remove this one by basically merging these two edges, again you get the same covariance. And that is very useful later on because that really means that you can basically add your observation locations as vertices. So that would seem to be fun. As vertices, so that will simplify computation. Another thing that we know is that this parameter alpha, as for the standard material covariance, that determines sample path properties. So this alpha parameter will determine the Hulder continuity of sample paths. This is what this says. Now, if you let this parameter alpha become bigger, then you This parameter alpha becomes bigger, then you also get differentiable sample paths. So if alpha is greater than one and a half, you have differentiable sample paths. And then if you continue increasing alpha, you will get twice differentiable and so forth. And you don't have to read history, but what it's saying is that you have also Hudder continuity of those derivatives. So, here you have two examples of this Gaussian process on a very simple network. So, it's this with four vertices and four edges. So, this is, well, if you start down here, this is a simulation of this model with alpha equal to one, and this is a simulation with alpha equal to two. And you can kind of see from these figures that alpha equal to two looks smoother. I think it is. And up here, you have the examples of the covariance. So, this is the covariance between the So, this is the covariance between the process in this location and all other locations on the network. We can also say things about the statistical properties of these models. So, one very important thing when you do, say, prove consistency of maximum likelihood estimators, for instance, or any type of essentially many theoretical results about Gaussian processes, somehow comes down to. Somehow comes down to the equivalence of Gaussian measures. So, this is what this theorem is saying: that if you take two of these Gaussian processes and you let mu and mu tilde be the corresponding Gaussian measures to this process with different parameters. So, mu is the Gaussian measure with these parameters and mu tilde has these parameters. Then these two measures are equivalent if and only if the smoothness parameters are the same and this precision parameters. And these precision parameters are the same. And that tells us a lot, actually. For instance, it tells us that we will not be able to estimate the range parameter kappa consistently. So this is exactly as for what is well known for the matern covariance. So this is what this theorem is saying. Basically, if you do maximum likelihood, so this is now just for a fixed value of alpha, if you do maximum likelihood estimation of kappa and tau. The estimation of kappa and tau jointly, then we will not get consistency for the estimator of kappa under infill asymptotics, but we will get consistency of the estimator of tau, and furthermore, it is we have also asymptotic normality of that estimator. And finally, another kind of statistical property is Krieging prediction. Is Krieging prediction. So, suppose now, since you have results like this, suppose that you use this model for prediction after you have estimated your parameters, then you're probably using incorrect parameters when you do your spatial prediction. And then you might ask, well, can we say something about the error that we're making in those spatial predictions due to the fact that we're using the wrong parameters? And so, here, basically, what this theorem is saying is that asymptotically, Theorem is saying is that asymptotically, again under infill asymptotics, you will get asymptotically optimal predictions as long as you have the correct smoothness parameter. So the range parameter and the variance parameter actually do not matter asymptotically for the quality of the predictions. Right, so to sum up, we know a lot about these types of rich and turn fields on metric graphs, geometry. Fields on metric graphs from a theoretical point of view. So, the question then is: can you do something with these practically as well? And the first thing is: well, how do you, how, for instance, if you want to do maximum likelihood, then you have to be able to compute the likelihood. So, for this, we have to be able to evaluate finite dimensional distributions of these Randall fields. And then, here is the key result is that these models are Markov if and only if alpha is an integer. only alpha is an integer and in that case these fields are mark of order alpha so meaning that in this conditioning step you if you condition on the process and all its derivatives that exist then condition on that the process is independent disjoint sets so because of this marker property we have the following idea that Had the following idea that now, if you want to evaluate or simulate this process, we should be able to use this markup property for doing that. So, we don't really want to use the SPD itself because that is difficult. So, maybe there is an alternative way we could think about this process. And the idea is the following: so, we should be able to define an independent Gaussian process for each edge in the network. So, suppose Edge in the network. So suppose that you have this square network here, then we define an independent process on each edge, and then we tie them together by simply conditioning on that these independent processes should now satisfy the Kirchhoff conditions. So for alpha equal to one, this would just be conditioning on continuity in the edges, and for alpha equal to two, it would be continuity plus this Kirchhoff condition. And then the question is: well, how should you then? How should you then define the processes UI on the individual edges so that after your condition, you get the solution to this SPD? And well, here is the result. So it looks a bit ugly. So essentially, what you have to do is that you start, say, with just the matern. So if you have alpha equal to an integer, you take the corresponding matern. Corresponding matern process. So this is R, it's just the matern covariance. You take this bold phase R is the covariance of the maternal process on R and all its derivatives. So it's a multivariate process. So you define U tilde as a multivariate process with a covariance that is the same as this kind of multivariate covariance of the Matern process and its derivatives, except that we do a correction. That we do a correction of this form. That sort of looks like a conditioning, but it has the wrong signs. And then, so if you then define this multivariate Gaussian process with this covariance function, and then you just glue them together, the independent process on each edge with this covariance, you glue them together, and then you condition on this Kirchhoff conditions. Then, if you look at the first entry of this glued together process, that will be a solution. Together process that will be a solution to the stochastic PD. So, this looks quite involved, but the key thing here is that we know the covariance here. So, now it's actually easy to evaluate finite dimension distributions because we know what the covariance function is on every edge. So, another way of viewing this is that, well, first of all, this condition. This, the conditioning step, we can write this just as a linear constraint on the random field. So it's, and then since we know that this is a Markov random field, what we actually have is that this is a Gaussian Marker random field under linear constraints. And that is things that we very well know how to deal with from both practical and numerical point of view. So, for instance, in the alpha equal to one case, this is the result that you get. So, this is the precision matrix. So, this is the precision matrix of the process evaluated at the vertices. So, you have an explicit precision matrix, it looks like this. It looks rather involved, but we're just summing essentially over all edges that are connected to each vertex. And because of this, this would be very sparse if the graph is not very highly connected. And again, recall that we can add vertices of degree two without changing the model, so by this result, then you can evaluate any finite dimension distributions. Evaluate any finite dimension distributions to this process. So, just a quick example of this. So, here's an application to interpolation of traffic data. So, we took actually data from this paper by Borovitski and all, where they looked at traffic speed data from the highways in San Jose. And then we fixed. Say and then we fitted different models that have been proposed for data like this. So, these isotropic models, our Witten-Maturn models, and also some discrete models based on the graph Racian. And long story short, if you fit all of these models, you do some cross-validation to check the predictive quality of these models, you see that these Witten Matern fields with alpha equals to two are by far the best in well, best in all metrics. Metrics that we considered, and it's quite a big improvement over, say, the Wittlerman turn with alpha equal to one or the isentropic exponential, which basically means that differentiability here really improves the predictive quality. David, in this, are you modeling the velocities directly, or is it the density of traffic? Of traffic? No, so the measurements here are. So these are the dots, so you can probably not see them. These are average measured traffic speeds at traffic cameras. So this is the like the we took like the average speed over one hour, one afternoon, sort of, and that's that's what we're modeling. All right. So it's not density, it's really just the speed itself. So if you're matching the radiance of that, Of that, it's acceleration. I'm trying to work backwards to the Kirchhoff condition and see what that means in terms of, because obviously the cars are not piling up, but that would just say the velocity is actually consistent as opposed to gradients. I haven't thought about what the Kirchhoff condition would actually. The Kirchhoff condition would actually imply for these velocities. Yeah, that's a good question. I don't have a good answer. All right, so just to okay, so some comments here. So, first of all, this model does not have, so this network does not have Euclid and Edges, so this is the topic explanation models are actually not guaranteed to be valid. I have a Guaranteed to be valid. I haven't mentioned these models based on the graph Laptacian, but those are the models that Porvitsky worked with here. So these can essentially be viewed as finite difference approximations of the Whittier maternity. All right, so last couple of minutes, just want to say that all of this application and everything I've been talking about, we have implemented this in our package. It's called MetricGraph. It's called MetricGraph and it contains a lot of functionality for dealing with data on networks. And it also implements these models. It has interfaces to our Inland Inlab rule. So if you have data, where you have data on a network like this, you can just take these Gaussian fields now and incorporate them in any patient hierarchical model that you can fit within our group. We can also combine. We can also combine this with another package that we have, the RSPD package, to actually not only work with models with integer smoothness but with general smoothness. You can also estimate the smoothness from data with this package. So just briefly kind of to show you how this works. So this is the code that I use for this application. So here is kind of the way it works. So we load the data, we create this metric graph object. This metric graph object. So, this is the graph objects. You can add the data to this graph, and then there are some plot functions. So, this is the plot of that traffic data. And then this is the entire code for defining and fitting the model with inlabru. So, you define this SVD model. So, this is now by default this function has alpha equal to one, but you can change that. And then you fit this model with. And then you fit this model with Inlabrue, and then you can extract the results. You can also do prediction. So, this is again how you do predictions. You can just use the predict function based on this and do plots. So, here you can might not see so clearly, but these are now the Euclidian predictions on the network. And this is, I think, a very nice illustration of why you should not use Euclidean distance, because if you look at these different lanes, Because if you look at these different lanes, for instance, in the two directions of the way, the traffic speed is very different. You could not model this by a covariance using Euclidean distance. That would just not work. All right, so to summarize, as far as I know, this is the first construction of differentiable Gaussian fields and general metric graphs. Gaussian fields and general metric graphs, and we have known sample path and statistical properties of these models. In the Markov cases, we can actually evaluate, we can work with these exactly without any approximations. I haven't discussed that at all, but in general, we can do inference and simulation and everything based on finite element approximations. That's also implemented in the R package. In the R package. So, if you're interested, if you want to try this out, we have a homepage. So, this is the homepage to the package. It has a lot of more documentation also of how to work with models like this. And what we're currently working on is mainly extending this to space-time models and also writing up a paper about Logaus and Cox processes using these models. So, actually, if you go to the homepage, there is a big. Models. So, actually, if you go to the homepage, there is a big net on how to do Long Gauss and Cox processes on metric wraps, but I guess we should also write a paper about that. And if you want more details about the theory, so here are the papers. The first one is about the sample path properties. This one is about marker properties. The last one is about the statistical inference. About the statistical inference, and the second one is about extensions to models with non-stationary parameters. So, kind of having modeling non-stationarity, but say, say, by having kappa be a function of space over the graph, and also it contains the theory for definite elements. Right, that's it. Thank you.