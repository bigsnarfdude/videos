Is George, and he will talk on Kernstein dispensing for measures on Hilbert spaces. Please. All right, thank you very much. So yes, this is joint work with my collaborators, Nikolai Kastruzak and Andrew Duncan, who's my supervisor. I'm a PhD student at Imperial College London. So this talk is going to be based upon how this is going to work. Okay, I'll point it somehow here. This talk is going to be based on this paper, whose title is pretty long now that I look at it. So the idea is we're going to be applying something called kernelstein discrepancy, which is a technique that was initially found in statistical machine learning, and we're going to be applying it to measures on Hilbert spaces, i. e. random functions where the function is lives in a Hilbert space. Is lives in a Hilbert space. And we're going to be doing things with kernel sign discrepancy. And the main thing we're going to be doing is assessing sampling algorithms for functions. Okay, so the motivation. This work is intersecting different fields here. Like I said, this technique initiated in statistical machine learning. It's being applied to sample quality assessment, a topic in computational statistics. And we're going to be using random functions and generated market processes to do some theory. So that's the stochastic. To do some theory, so that's this stochastic analysis bit here, somehow. So, like I said, in general, sampling functions is an important topic. We use it for a wide variety of things, in particular, modeling, random functions, of course, uncertainty quantification is a huge one. So, in a Bayesian framework, for example, you will be getting a posterior over some unknown. And in a functional framework, that unknown will be a function. So, you want to be sampling functions. And then, for emulators in particular, if you've got some real Particular, if you've got some real-life process and you want to model it, and the thing you initially used to model is really expensive, you might want to do a surrogate of that thing or an emulator. And to do uncertainty quantification for that, you again want to be using some sampling to understand its distribution. The issue is this can be quite hard to sample different random functions. In particular, if they are non-Gaussian, so if they're not a Gaussian process, it can be tricky. In the formulation of some of the In the formulation of some of the things I mentioned before, like emulators, they can have non-tractable distributions, which means that you can hit run on some computer code, you can't really write it down in maths. And as a result, you end up needing quite tailored sampling methods, not general approaches. And in particular, for general approaches, it's hard to actually even know how good they are, which is really what this is the thrust of the main application, what I'm going to be talking about. How good are some of the samples we have for random functions? Samplers we have for random functions. What things do we have that can reassure us of this? Theory. So if you have some random function, a sampler, you might say, oh, in the limit, this thing tends to this in some kind of a got it limit, which might give you cause to think it work, and it might give you unrest because you don't know, because it's an asymptotic result. For some things, we know the marginal distributions, particularly for Gaussian things and some non-Gaussian, but in general, not so much. We might have some. We might have some true data of the thing we're trying to model, but we might not have much true data. So it's hard to make a proper comparison there. And also, knowing the impact of parameters of these sampling algorithms that we have, that's important, and it's really hard to tell. And in particular, that relates to the theory statement here, because often you have like this asymptotic result, but you might have some constants under the hood which depend on certain parameters you're using for your sampling algorithm. Okay, so our Okay, so our problem statement, to put some math into this, is going to be our base space, curly X, is going to be a separable Hilbert space. Think L2 or Sobolev space, something like this. And we will have observed some samples, which will be capital X's, and we've observed n of them from some distribution q. Q is going to be known as the candidate distribution. So this is maybe the output of a sampling algorithm that we have, something like this. And our target distribution is going to be. And our target distribution is going to be P. So we want to know how far Q is from P. And we're going to assume P is of this form here, where NC is a mean zero Gaussian measure on our space with covariance operator C. And this is its density with respect to C, also its radium Nicodem derivative of dp density. Okay, so U is going to be some function here, so this is also known as a potential in certain literatures. So this is our setup. We've got samples from Q. Is our setup. We've got samples from Q, want to know how different Q is from our things from our sampler algorithm or our emulator. Want to know how different that is from a target measure P, and we only have samples from Q. We don't have samples from P. Okay? So here's some example use cases. Imagine we want to sample from a conditioned SDE. So here we have a non-linear SDE. Oh, I'll start from the top. Our base space is going to be L2 of 0 time interval 0 to 50 because I kind of wind. 0 to 50, because kind of why not? And we've got a nonlinear SDE here, and we're going to condition it to start at minus pi and to end at 3 pi. So this example is taken from a paper by Joris Bjerkins and his collaborators that was in statistics and computation recently. And they developed an algorithm to do this kind of sampling procedure. These are samples from this algorithm here. Algorithm here. And what we have here, so our drift term is a sinusoid, so it has wells at multiples of pi. So we see these kind of flat lines here at minus pi, pi, and 3 pi where it ends here. So we have these wells here. The thing is, this is a hard sampling problem. Like I said, this algorithm can do this sampling, but this is a very recent algorithm, and even then, it's not particularly, it's not perfect, right? This is a very, very, even this toy example, really, which can run it very simply. So in this case, We can write it very simply. So, in this case, our Gaussian measure, the base Gaussian measure is a Brownian bridge, and you can derive U from Gassanov's theorem quite straightforwardly. Another example application, Bayesian inverse problems. So here, our base space is going to be L2 over some compact domain D. And this is an elliptic equation, and this might be used for some kind of groundwater flow type problem. And the unknown in this example is this kappa here, which is a function. So the idea is that given a kappa, So, the idea is that given a kappa and a forcing term f, you would solve for p, and you could also have a boundary condition. So, you've got all sorts of non-linear things going on here. And then, once you solve for p, sorry, the way you use this in practice in this Bayesian inverse setting is you place a prior on your unknown. So you assume it's Gaussian for some covariance operator. You would observe some data, which is the solution given a kappa at some input point SN, some input point S. Sn, some input point, yeah, some input points in your domain, and u would be the likelihood term in this Bayesian formulation. So this is a very, very mainstream technique used in applied maths for all sorts of engineering problems. So the aim here is, given my prior and observed data, I'd have a posterior distribution on kappa, and I'd want to sample from that. And the posterior is of this form, e to the minus u nc, but it's damn hard to sample from it. So, given your sampler, you'd want to know: is it actually any good as well? Any good as well. Okay, so that's the motivation out of the way. Well, to summarise, what we want to do is derive a distance between distributions over functions, and we want to only require samples from one of the distributions. So we have a target P, and we've got samples from a Q. The idea is to use Konstein discrepancy, which has been used as a methodology for finite dimensional data, or to adapt it for infinite dimensional data. Okay, so data in this separable Hilbert space. Hilbert space. So, background covering kernelstein discrepancy. As the name suggests, we've got a couple of ingredients going on. And the first ingredient is kernels. So, kernels are simply put a measure of similarity between two inputs. They're denoted with a k, and they take two inputs from your space and spit out a real number. That gives a notion of similarity between them. And I'm not going to go into the exact definition of what a kernel is. Think a covariance kernel. Think a covariance kernel for a Gaussian process, same kind of kernels. But the main thing I'm going to be talking about today is the spectral interpretation of kernels. So a result is that if you were to take a measure on our space calligraphic X and take the Fourier transform of it, so mu hat, also known as the characteristic function to some literatures, then if you set your kernel kxy to be mu hat x minus y, mu hat x minus y, then this results in a valid kernel. And an example of this is the squared exponential SE kernel here, where if you take a Gaussian measure and take its Fourier transform, this is what you'd get, and this would be a valid kernel. So what was going to happen looking ahead is our KSD, kernel sign discrepancy, is going to involve a spectral representation, which is going to involve this measure mu here. Okay? So the idea is you give me a mu. So, the idea is: you give me a mu, and then I get a curve. And if I plug that into KSD, I can express my KSD, which I'll define in a second, in terms of that mu. Then we'll use properties of mu to tell us about properties of our discrepancy. Okay. So the next ingredient is a reproducing kernel Hilbert space. These are special spaces that you get from a kernel. And I'm not going to go too much into depth with that. The idea is there is a one-to-one relationship between these spaces. One-to-one relationship between these spaces and the kernel. And they have a very special property. So they're function spaces, which have functions which act on our space curly X and map to R. And the special property is the reproducing property that they have, which says that for every F in our reproducing kernel Hilbert space or RKHS, we can write pointwise evaluation as an inner product with the kernel. And then under some other basic assumptions, you can write expectations as inner product of F with an expectation. As inner products of f with an expectation of the kernel, or derivatives of f as f inner product with the derivative of the kernel. The idea is that we can reproduce point-wise evaluation, expectations, and derivatives in this space. So, Stein's method, the S in KSD. Stein's method is a very, very amazing idea of probability theory that started about 50 years ago. It's a way of characterizing distributions given how they integrate under certain operators. Under certain operators. So Stein's method is all about finding what's called a Stein operator A and a Stein class such that if you have a measure P in mind and you take a measure Q, and Q equals P if and only if the integral under Q of the operator applied to F is zero for all F's in a class. So this might look extremely abstract for the time being. There are very simple ways of obtaining these operators and these classes for a lot. And these classes for a lot of measures that we want. And the way we actually find these operators and these classes is an ingenious method called the generator method. So the generator method takes, it loosely says, you take your favorite Markov process whose invariant measure is the target measure P, then if you take the generator of that process, then that can be used as a Stein operator. So the main example of this is the Langevin diffusion, which is currently which is really used in practice a lot. Which is really used in practice a lot in statistical machine learning. And the way that this works is essentially integration by parts, because if you integrate this thing with respect to p, then one of the derivatives here goes to here, and you get a minus. So you have two derivatives of f minus, and then you have two derivatives of f here, and it cancels out to get zero. So that's like the kind of trick, is to get generators from a Markov process. So with that in mind, and looking back at the Stein. Mind and looking back at the Stein's method, if you know that this thing satisfies, if you know that this is a Stein class and Stein operator, a way to measure how different Q is from P is to see how much this value is different from zero over your Stein class. So this is what the Stein discrepancy is here. The discrepancy between Q and P only involves an expectation with respect to Q. It doesn't involve expectation with respect to P at all. So this is a thing I studied in probability theory in a very theoretical way. A thing that studied probability theory in a very theoretical sense, but for practicalities, we have an issue. It's really hard to calculate a lot of these supremums. But here's the trick: if you take your supremum, instead of being the Stein class you might have got from probability theory, and replace it with an RKHS, then you're in business. Because KSD, which is this expression here where you apply the unit board of your RKHS as your supremum, will be able to rewrite KSD in a lot of easier ways. So I like to state so far. So, I like to state so far: the idea of KSD was done back in 2016 in system machine learning. Nothing I've talked about here is new so far. The novelty will come with really showing this to be done in infinite dimensions. And then for the rest of the talk in the next 12 minutes or so, I won't be giving direct conditions granularly. I'll be giving kind of the moral of the story. I'll check the paper if you want the direct conditions and everything. But this is the overall idea. We're going to adapt. But this is the overall idea. We're going to adapt this to infinite dimensions. So we're going to need to choose the right operator and make sure we choose a kernel that makes this thing work, essentially. Okay. So here comes the theory for this infinite dimensional adaptation. So our basic assumptions are as follows. Bounded second derivatives of the kernel K, so the kernels acting on a potentially infinite dimensional space. So we're talking On a potentially infinite-dimensional space, so we're talking Frechet derivatives. This isn't too bad, pretty mild. Mild integrability and differentiability conditions on the potential U. Again, it's kind of fair game. And second moments of the candidate Q. Some might argue that's not so mild, but I would argue otherwise, to be honest. So the reason we're going to have these assumptions is because the main thing we're going to do is we're going to choose an operator that's going to involve two derivatives, and we're going to need to reproduce the operator. And we're going to need to reproduce the operator. So remember how I said if the derivative of a function is the inner product of the function with the derivative of the kernel. But if you're going to have two derivatives, then you're going to be able to differentiate your kernel twice. So here's our choice of operator. This looks very similar to the Langevin I showed you earlier, if you squint a little bit. The idea here is that this is the generator of an infinite-dimensional Langevin diffusion. So this kind of thing was studied in So, this kind of thing was studied in the context of path sampling algorithms. So, some of the first papers on the algorithms I was talking about for things like Bayesian inverse problems. And we're talking work by Andrew Stewart, who's at Caltech, used to be at Warwick, and he did a lot of this stuff in Martin Heiler. And they really did a lot of work on looking at the infinite-dimensional large-band, its solutions, its generator, and things like this. But in fact, a lot of the work we built upon for this was actually in the 90s by people like Vladimir Bogachev or Rockner. Bogachev or Rockner or Giuseppe DiPrato, where they really laid the groundwork of these infinite-dimensional diffusions. So, this is the generator we're going to be using, and it's similar to what we had before. We've got kind of second-order derivative here, as we had in the first time, one derivative of the function here, as we had the last time. But rather than grab log p, where p is a density, we have this thing here. So, the idea is that this is the generator, I should be a bit more specific, of the preconditioned. Of the preconditioned Langevin in infinite dimensions. So the pre-conditioned Langevin means that you're pre-multiplying everything by your covariance operator here. And that's really important, otherwise you'd have a c to the minus one term here, and that becomes a bit grim to have to deal with. So this term here is essentially the grab log p of our target p. So it's kind of the gradient of its density with respect to the threat to the Gaussian. We were using analogies. We were using analogies here. We've got this analogy of the finite to the infinite-dimensional larger band, the other generator to this generator. So the idea is we're going to plug this into the KSD and hope we can do the derivations. So if we just plug it in and define this, you hope it's well defined, and with the assumptions I mentioned, it is well defined. We show this in the paper. Then the first step is to make sure you can rewrite KSD in this what's called double expectation form. So this has been done in functional. form. So this has been done in finite dimensions and this is what actually makes KSD very usable in practice. So the proof of this is very simple. You start with the definition of KSD that we had here. You reproduce the action of the operator. You reproduce the expectation. And now we're just going to use Cauchy Schwartz. You've got supremum over the unit ball of an inner product here. So you just get the norm of the other thing. Simple as that. And obviously the step. As that. And obviously, the steps to make sure these reproductions are valid, those are the theoretical details we had to deal with in the paper. Yep, quickly. K has no relationship to P and Q. Exactly. Okay, and the reproducing property that you've used there is independent. You can choose any K and that will be true. As long as it's got the derivative. It's just a differential operator. Yeah, exactly. So as long as it can reproduce each bit of the operator. So operator had two derivatives. Had two derivatives here, and so our colour has to have two derivatives, pretty much. Okay, so we've got that, and now we just square it. If we square it, we've got an inner product, and then it kind of reproduces each other. So we get two expectations and two actions of the operator. And the thing is, is that this expectation here is just your operator acting on your kernel. And you can derive that by hand or use autodiff on a computer. Then, given your samples from Q, you just take an empirical estimator. You just take an empirical estimator of this thing. And that's how you estimate this notion of discrepancy, and you can use it for testing and distances and things like that. So the idea is: this is the thing you'd use in your numerics, but there's an issue so far. So yeah, this is easily estimated in the next K practical, KSD practical. I haven't actually told you this is a valid distance, right? Because we want this to be true, but I haven't actually said it is true or convinced. Said it is true or convinced you it's true. So we need to make sure this is true, otherwise, our notion of distance is: well, it's not a distance. So, in finite dimensions, this has been done, and the way this is done is you usually pick an operator that makes it easier, that makes it easy to do the derivation I just mentioned, where you do two applications of it. You want a nice operator, but then you've got to pick a kernel that satisfies this. So it's been done in finite dimensions, but the issue is, is Dimensions, but the issue is that if you take a little bit broken down memory in the history of these kernel methods, in the early 2000s or so, kernel methods were kind of state-of-the-art for a lot of statistical tasks and statistical machine learning tasks, and they cared a lot about approximation of functions. So when this method was first used in statistical machine learning around 2016-17, the theoretical toolbox they had at their disposal was all about approximating functions. Was all about approximating functions. So, what they did was they applied that toolbox to the PDFs of the distributions they were looking at. So, all the proofs that exist for KSD to show this, which is a critical property, all of them require having a PDF. And we don't have PDFs for this. So, kind of have to choose a different track. So, when you say distance, do you mean metric? Yeah. Okay. Yeah. Okay. Whoops. Okay. Whoops. Um yeah, so uh metric, as in this is zero, and this, and it applies I think it abides by some notion of triangle inequality. In general, you would have some sort of semi-metric, right? Yeah, you don't really care about the fact that you care about this. You care about this, yeah, sure, that's what I mean. Sorry, yeah. Okay, where was it? Yeah, so we don't have PDFs, Maxwell drawing board, but the thing we can do is there is another notion of, in a previous work I did, which relates very much to some of Harold's work he did with Ilia, there's a different notion of. With ILIA, there's a different notion of discrepancy that uses kernels called MND, maximum mean discrepancy. And with that, that has a spectral representation where you can write it in a form which depends only on the characteristic functions of the things you care about. So, characteristic functions, you can have that for infinite dimensions. The target is, the plan is, try and rewrite our KSD in a kind of spectral way that only depends on you're looking for a complex. You're looking for complex exponentials or things like that. That's the kind of idea we're going for. And it turns out we could do that. So, this is one of the two main results of our paper. It's rewriting KSD in what I will call a spectral representation here. So, remember this character, mu, like I said, if you choose your kernel such that it equals this, which is really common in practice, then what we're doing. Then, what we're doing is our KSD is really doing a Stein discrepancy where our test functions now are complex exponentials, and rather than taking a supremum, we're taking an average with respect to this mu here. So, this is a really important result because it's showing that we can rewrite this thing in a spectral way, like we said. And now we can start to apply some stochastic analysis results, some results about measure equations. Some results about measure equations and Markov generators in terms of if we know that this is zero, can we then conclude p equals q? So, this is one of the two main results of our work. And this form was not known in finite dimensions. And really, the proof is very straightforward. The trick is knowing what you want to end up with. That was the hard part. And we did that by this kind of analogy to this other notion of discrepancy that we've used before. So, the proof, this. So, the proof is very much a sketch proof. To do it in the paper, you have to be very careful about sorting integrals, yes? So, that chi, you have inner product with respect to chi here, and chi is the space of functions you want to. Yeah, the curly x is the space of functions, yeah. Sure, yeah. So this is like L2 or a sobola space or something like this. Yeah, and it's supposed to be a Hilbert space. Yes, separable Hilbert space. Essential, absolutely essential. Yeah, and you. Absolutely essential. And mu shouldn't point this in. So mu is your favourite measure on your central Hilbert space. So you can choose a Gaussian measure, then you get the squared exponential kernel. There are other ones you can choose as well, like if you use a Gaussian measure and then multiply that by a Gaussian scalar, then that gives you another kernel called inverse multi-quadric kernel, which is also used in practice. Just a quick when you say measure, you mean probability measure. Or no. Measure. Yeah. It doesn't have to probability measure. Because you could multiply this by like five, let's say, and it would still be a kernel. And it would also be a case if it's an infinite measure. That I'm not too sure off the top of my head. I can get back to you on that one, to be honest, yeah. Just some more bit. So there's no reason to expect that if you choose a generic K that has second derivatives, that it will possess. That has second derivatives, that it will possess this property. Absolutely. And it is not going to be the case. We didn't get to the punchline here, but I guess. Yeah, exactly. So, but if you go and you find a K, you then have to, and it's somehow related to what B and Q are, you have to check the regularity conditions, right? Yeah. And that's probably... Well, the usual framework of this is me as a user will look at the theory, say, these set of K's obey these conditions. I'm going to use that. And they will work on every P and Q. Okay, so that's the way this comes. Yeah, that's the way this kind of works. You very rarely tailor K towards your P and Q because it's really hard to do that. Because it's a very all sorts of non-linear relationships, yeah. Okay, so this is the form we've got it in, which is a new form for KST, and it holds in this infinite dimensional context as well as finite dimensional. So now, can we use this to prove this KST equals zero if and only if P equals Q? So as I was saying, the proof is swapping in. Swapping integrals and operators, really. The hard bit of this is knowing what to end up with rather than getting there. So, yeah, this is just stuff I've already said. One nice thing about this is that it separates the action of the kernel and the operator. So, the operator only happens in the integrand, and the kernel, i.e., mu, is only occurring as the integrating measure. Whereas a previous form. Measure, whereas the previous formulations of KSD, they're kind of intertwining quite a lot. So that's a nice property of this. Okay, so how do we do this? So this integrand here is what's called a measure equation in the 90s by these papers by Bogachev and Rockner and such like this. And I couldn't really believe my eyes when I found it. When I was reading these papers, they expressed their stuff exactly in terms of complex exponentials. And it just so happens that this integrand is exactly the thing that appears. Is exactly the thing that appears in their papers. Because the idea of what's happening here is a kind of Stein equation where your test functions are complex exponentials. And complex exponentials are essentially dense in every space you could really feasibly hope for. So the idea is we've taken supremum over the RKHS. That's now an average over mu. So the idea is here, the kind of results we get from these measure equation papers say if this integrand is zero for all s, sorry, oh, this integrand. Sorry, oh, this integrand is zero for all s, if and only if p equals q. So now this translates into a very, very simple condition to make sure ksd is zero if and only if p equals q. And that is that if mu has full support, then ksd is zero if and only if p equals q. Because if mu has full support, then it's integrating over every s. There are no gaps in the space. So it's checking every s. The integrand is non-negative. So if it's zero, it has to be zero for every s. Zero, it has to be zero for every s. Therefore, its expectation is zero for every complex exponential. Therefore, we're doing all the test functions you could possibly think of. So, if it's zero for all those test functions, p must equal q. That's the logic that's going on here. And in the last couple of minutes, I want to show you some numerical examples of this applying to this function sampling problem. So, what we're going to do is we're going to simulate some conditioned SDEs. So, one of the ones I mentioned earlier. SDEs, so one of the ones I mentioned earlier, and another one. And what we're going to do is we're going to vary the terminal conditioning time. So in the first example, it was like time 50. So as you change that terminal conditioning time, that sampling problem becomes easier and harder. It's easier if it's a short terminal conditioning time, harder in the future. And what I'm going to do is I'm going to look at two different methods. One of them is this piecewise terminated market process method by Bierkins et al. And the other one And the other one is a very recent neural net sampling method using score functions, which is by Valentine Debotelli and Arnold Dusset at Oxford and some other collaborators. My point, I'd like to emphasize before I start showing any results here. These are extremely distinct methods. And the real reason I'm using these two methods is because code is available and easy to use. Putting my hands up there, they're extremely distinct methods that do very, very different things. So the first one projects to a HA basis, like a HAR wavelet basis. So it projects to like. Wavelet basis, so it projects to like piecewise constant functions. A second one uses a neural net to like learn things, so it's very, very different things, but they're very nice to compare to have different properties to see. Okay, so we're going to look at this example again. What we're going to do is vary the terminal time. And in the next plot, I'm going to show you on the table, you're going to have different terminal times, then you're going to have rejection rates using different kernels. Okay, so these are different terminal times, time going 20, 30, 40, 50. These are different kernel and hyperparameters. These are different kernel and hyperparameter choices. I haven't told you T1 and T2 are different hyperparameters in the kernel. SE, squared exponential. IMQ, inverse multi-quadric. I mentioned it. It doesn't really matter for this table. The point is that we're investigating these properties. So what we see is for the PDMP method, as terminal time increases. So one thing I should say, sorry, we're doing a goodness of fit test of 5% error rate. So if the test, if KSD thinks, So, if the test, if KSD thinks these look like true samples, it's going to be around 5%, 0.05%. If it's higher than that, then it thinks absolutely not true samples, it's rejecting them. So, here, lower number is better, pretty much. Yep. So, the PDMP, so the zigzag sampler? Yes, infinite dimensional zigzag sampler, yeah. So, what we have here is we see this PDMP is, as time increases, is like getting more and more stable. And that the reason for that is, And the reason for that is, you've kind of guessed that, because this guy is essentially piecewise constant, because it falls into these wells. Whereas the score matching sample, based on the neural net, is kind of all over the place. It's really struggling as the time goes larger because it's a lot harder trajectory to learn, pretty much. Button the bell for me? Yes. Okay. We'll wrap up in a couple of minutes. Then we look at another one, which is a conditioned Unstein Ulenberg, and then the Condition Unstein Ulmer, and then the idea is that we're going to condition it and it will hit minus five, then revert back. And now, for these results, we actually get something very interesting because the PDMP sampler, as time increases, now struggles more. And the reason for this is because this trajectory becomes more curved as time gets longer, whereas if it's shorter, it just kind of goes straight to the terminal endpoint. And the neural net sampler, as time gets longer, actually suddenly gets better as well, which is surprising. Well, which is surprising. I think that's also because of this curving property. So, the point is that we can assess these samples for the first time. When you look at any papers about functional samplers, they only look at efficiency of the change or mixing. There's no numeric quantity of a notion of distance of saying how good these things are at actually sampling the target. So that's the kind of main novel contribution of this work. Okay, to wrap up, spectra representation of KSD, broad class of target measures, like I said, conditioned SDEs, beta inverse problems, things like this. Lots of questions remain. This. Lots of questions remain. I'll probably talk about those in coffee. I just want to go past a couple of references. The different Kernstein Screpency and other papers, 2016, Giolami-Mackey, Oates, Stein's review papers, again I can talk about these otherwise. And these were measure equation papers, which were like, had god-send, had all the results I could want. They're really fantastic to read. And finally, the colour methods and paths. This is kind of continuing the line of work that I think Ilya. Continuing the line of work that I think Ilya and Howard really started here with their paper using the signature. This is a paper I did with my supervisor on this other type of discrepancy before. Okay, that's it. Thank you very much. Sorry for going over time.