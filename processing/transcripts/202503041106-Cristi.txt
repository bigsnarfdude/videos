Yes, um yeah, hi everyone. Um this is joint uh work with uh Bruno Sidioto and uh um yeah sorry it's not about like such a hot topic as LLMs, but I think it's interesting. So all right. Yeah, and also yeah, I'm from BP Fed, so I think I'm the most jet-lagged person in the workshop. So sorry if I say some nonsense. This is gonna be all right, so so I'm All right, so I guess all of you know what the profit inequality is. You can think of it as the problem of allocating some resource, a single item, to a bunch of agents that come one by one. And they have some valuation for this item, a number, and we assume these numbers are independent and drawn from certain distributions. So we have the sequence x1, x2, x3. Sequence x1, x2, x3, and x1 comes from a sim share one, x2 from a sim share two, and so on. And in this classic model, the objective is to design a stopping policy that maximizes the expected selected value. So we have to make this decision on the fly. We have to decide what to stop and what to maximize the expected selected value. And the classic result from the 70s is that this is a stopping box. Is that there is a stopping policy that guarantees one half of the expected answer. Now, in this talk, what we care about is this other version of the problem. So, let's say instead of knowledge distributions, we only have sample access to these distributions. They still exist, but we only have access to a few independent samples of each of these distributions. And now we still want to design. And now we still want to design a stopping policy on the actual values. And again, we don't have access to the distribution, just to this. So, what can we do? And then the main question we ask is, let's say we can guarantee in a certain variant of the profit inequality, we can guarantee C star times the expected maximum when we know all the distributions. How many samples would we need from each distribution? Would we need from each distribution to guarantee almost the same thing? So C star minus epsilon group times the expected mass. Okay, so in previous work, this question was solved for the fixed order version, this classic version, where you can get the one half. Actually, it's a very beautiful result that says that actually just one sample from each distribution is enough. From each distribution is enough to get a TD one-half, nothing dependent on X. However, when you move to the IUD version, all the distributions are the same, where you can get a 0.745 approximation, one sample from each distribution, which would be like n in total from each distribution, is actually not enough to get close to this point zero for pi. And you need one over epsilon essentially times n. Essentially, times n samples to get close to this 0.745. And in this work, what we did was to solve this question for the other two classic variants of the single selection reference inequality, which are the random order variants and the free order variants. So the random order variant, instead of coming in a predetermined order, In a predetermined order, they come according to a random permutation, uniformly random permutation of the values. It's a random order, and the free order would be that we see the distributions and we can decide in which order we inspect each of these distributions. And interestingly, the tithe guarantees are not known for these two values. However, we still prove that Prove that to get whatever that type guarantee is, minus epsilon, it's enough to have the constant number of samples per distribution. And so one good thing about what we did is that this is kind of a black box result in a sense, because we don't know what C star is, right? And these previous results I showed you were basically, okay, here is an. We're basically, okay, here is an algorithm that uses samples, and it turns out it attains this same guarantee that we knew was the title. These were not derived by taking those algorithms and constructing a sample-based version of them. We're just, oh, here's another algorithm that happens to be this correct, happens to have the same. Why is the, can you go back to the previous slide? Why is the fixed order? Why is the fixed order case easier with known distributions, but seems to work harder in an unknown case? Right, no, yeah, it's a great question. The thing is that the number is low. So, to reach this number, I mean, in the IED, of course, with one sub, you still can reach the one half, right? But the thing is that you're aiming for something higher. So, that's a reason for you. Yes, that's a reason. I think you mentioned this, but I think I missed some of the details. So, when you say once hand. Else. So, what's when you say one sample, that's one sample per distribution. Yes, yeah, yeah, yeah, yes, one sample per distribution. Everything is per distribution. And when I say IID, since it's n times the same, it's this times n. Got it. And so then on the next slide, when you've got the two different orders, this. So here, if it was IID, then these would be the same. It wouldn't matter which of these models you were in, right? So this also, you're imagining N is growing. Also, you're imagining n is growing by some x number of distributions or something like this. Or is that everyone has a different distribution? So, so, okay, so it could be the same, but I don't know, right? So, so I do this without knowing the distributions whether, I mean, either, I mean, I, in this version, I don't know if they're the same or not. They could be, but I only see something, so I cannot guess that. So, this is still like N over. If you, it's still like n over epsilon to the five, total descent. Yes, yes, total descent, time second. Yes. Is it a discrete distribution with a fixed number of possible values? Is the distribution discrete? Or can I have continuous? Continuous. Oh, you can have continuous. Yeah, yeah, sorry, sorry, yes, yes, you can have. What random order is? What random order in free order mean to things? Yeah, so random order is that there is an independent uniform permutation of the variables, and you see them in that order. So in the fixed order, this works for the worst possible order you could have, the one half. Here, there is an extra random permutation that actually has, and the free order you can choose. So you see in the usual Choose. So you see in the usual version, you see the distributions, then you can choose which order inspected. Here, of course, you only see samples, but then you say like, oh, this corresponds to F1, this samples corresponds to F1, and then you decide which order you will see the actual. Okay, so what is difficult about this question? About this question. So imagine we see this situation. We see that almost every variable is always 10 in our samples. There is one that is always zero. You think we should do? We need more data, right? Yes. So, I mean, of course. So, I mean, of course, you could say, like, okay, that's how we see those, and these are all 10, so let's just take a 10, right? But if we're aiming for maximizing the expectation, of course, in this example, the problem is that zero carries all the expectation, because with tiny probability, it's huge, right? So it actually contributes almost all the expected maximum. So, in this instance, if it's like that, we should aim for the last one, right? Aim for the last one, right? Skip all the tens. And even though we might get a zero, sometimes we get some huge values, so we should aim for this. However, it could be that this is just zero, right? So we still want to take the ten sometimes. So that's the complication. This essentially details of distributions that we cannot see using science. So, what do we do to handle that? So, this means that essentially the algorithm The algorithm should reserve some probability, right, to get to observe even the COs, right? Things that seem useless. And we actually proved this, I think it's a strong guarantee that says that if there is a C-star competitive algorithm, that means when I know the distributions, if there is an algorithm, a stopping policy. If there is an algorithm, a stopping policy that for every instance satisfies that I get at least C star times the expected maximum, then there is also an algorithm that might not be the same that guarantees first its C star competitive, but also it reserves enough probability to observe each of the distributions. So if there is a C star competitive algorithm, there's also one that, on top of that, guarantees for each of the distributions. That guarantees for each i observing xi before stopping would probably be at least C stars. Right? So there is such an algorithm, right? So, I mean, that's essential, because as I said, if we want to aim for a C star comparative algorithm, it should reserve C star probability for this zero, right? So that's the first essential step we need to prove. But this exists. Sorry, just to clarify, is the Sorry, just to clarify, is this for the free order and the random order setting? Exclusively? That's a great question. So, this is very general, actually. It works for all four parents, and it even works for more general combinatorial variants of the perfect inequality. So, if you're doing OLED matching, say you have an offline and an online site, if you can guarantee C star, then there is also an algorithm that guarantees that when Guarantees that when node I arrives, J is available, we probably decide. And this is for every person. And if you are in an online commuter auction where we have a bunch of items, and each person has a valuation function over the items, if there is a systar computer algorithm, it is also one that guarantees that for each agent, for each possible valuation in their support. What I can offer What I can offer this person when this person arrives is that it's C star times the maximum possible value. And if this monotone is the value for the entire state. So yeah, this is a strong result. And it works very regenerative. So let's see what we prove such a result in the single idea case. Let's say we fix an instance, and let's say we normalize it, so the expected maximum is one. We can write the following main-max problem. An adversary, so remember, the distributions are fixed, right? So, given distributions, an adversary will choose this parameter beta, some weights. And we will choose an algorithm, and we want to maximize this. So, beta zero times this. So, beta 0 times the expectation of the algorithm, which is like the competitive ratio because the expected mass is 1, right? Plus beta i times the probability of observing xi. All right. So now notice that everything is linear. So, if we allow for randomized algorithms, so we can exchange the mean with the max and then And then the adverse, I mean, when max is outside, that means that there is an algorithm that guarantees the same as the value of this problem for any beta. And notice that then we can choose beta to be either beta 0 equals 1, so then we get the complete region, or 1 beta i to be 1, so we guarantee that it will be equal. So what we want to prove is that the value of this min-max problem is at least C star. If there is a C star comparative algorithm, If there is a system compared to algorithm, we will prove that this mean max power m has a value for this system. And then that's enough to conclude the code. And then how we prove that? But it doesn't say that the CS star computing algorithm is the one that achieves. No, exactly. No, it's not necessarily the same. But if you can compute everything, then you get CSL for injury problem. If it exists, then the value of this is. Then the value of this is. I mean, if you can compute the equilibrium of this problem, then you can. Yeah, yeah. I mean, right now I'm not saying anything about computation, I'm just talking about existence. Yes. So, how we prove that this is a DC star? Well, take the instance and I construct an auxiliary instance y for some delta that's very small. And then most of the time, Most of the time, yi is just xi times beta zero. And with small probability, it's beta i times something that balances with probability. And now, let's say I take this algorithm, but I pass it this other instance, the original instance. So, first I construct this other instance, an algorithm this C stars there, and then I C starts there, and then I take that same algorithm and I pass it X. So, what will happen? So, in this new instance, the expected max again is roughly one, okay? Plus delta, but delta will tend to zero in the analysis. If it's system competitive, then the expectation of the algorithm on these variables y is at least system. On these variables y, it's at least c star, right? Because the expected max is c star. But the expectation of y is roughly the same as beta zero times the expectation of the algorithm when we apply it to x, right? Because most of the time we'll only see the beta zero times x, right? So most of the time we just get that, right? Beta zero times is the expectation of the algorithm on x. And whenever we get that, And whenever we get that, right, if we get to see it, we'll get beta i times essentially one. So the probability that we'll serve xi times one. And so that's it, basically. So the value of this, when delta tends to zero, we get that the value of this min-max product is on this system. Just apply by the rational of the group, so you fix the instance to define the gain? Yes. Yes. But now in order to analyze the value of the game, you would say for any choice of beta, I know that there is an algorithm. Yes, yes. Because I construct for any choice of beta, I can construct this other instance. Yes. But then, because everything is linear, I can exchange the model. So it was a third thing that y is a function of x. That's the kind of refix that's. Yes, yeah, it's like a little round kit. Is really good. It's like you kind of maximize and then keep this variable and just reject that duality or min-max. Okay, so yeah, so this algorithm exists. And essentially, it means that we don't need to know the upper test. If we have access to this algorithm that exists. That exists. We don't need to know the apertures, and then we, as a consequence, we can prove this. We only need to know the distributions in some range such that beyond that range, things happen with very small probability, right? Because, okay, let's say nothing is above mi with already more than epsilon in total. So I just say I take the algorithm, and whenever I see xi larger than mi, I just take it. I just take it. Overall, this only happens with probability epsilon, so I don't hurt myself by doing that, and I get everything that is beyond my time system. Okay, so that's great. We don't need to know the upper text, so we don't have this problem with the zeros. So for instance, we could take Mi to be the top epsilon n epsilon over n quantite of its distribution, and then actually with this one can show that a polynomial number of samples are enough. A polynomial number of samples are enough, which is not immediate when we see the example at the beginning, right? But yeah, I mean, we can do it with polynomially many samples. And then to go to the constant number of samples, we need a bit more, right? And we use this second step, which is inspired in some observations by this other paper, that is specific. Specific, this other thing was very general. This is specific to the random order and the free order, because in free order we can always choose to see things in random order. So this says that we can actually pretend almost all variables are IID. And almost all means something like n minus 1 over epsilon squared. So almost all of them are, I pretend they're IID, and a small chunk. And a small chunk, a constant number of variables, are maybe not ID, but they are very few. So that would be very nice later on. Okay, so why can I pretend they are ID? We can do the following. So let's say we eliminate the bottom epsilon quantile of the distribution of the maximum. That means I would make everything that is above. I would make everything that is above that zero, sorry, below that zero. This cannot hurt me a lot, right? Because this happens, this is the maximum small probability, so this doesn't contribute much to the maximum. But most variables will become zero, right? Most of the time. If I had a lot of variables that were larger than that, That were larger than that with large probability, then this wouldn't be the bottom epsilon quantile of the mass, right? So again, when I do this truncation, most variables are like this. They're zero with a probability at least one minus epsilon, and maybe something else with probability epsilon. Also, in the in the previous when you used the Neo et al paper, so you actually reduced to the case a bit not a So you actually reduce to the case with not IID but also small probabilities. I mean I mean so to get that we first have to do this. So we do this truncation first so that doesn't hurt us actually, right? Because it's just the bottom quantum at the maximum. And then everything becomes like zero almost always, right? Except for a small number of Except for a small number of elements that might not behave like this, but we'll handle them differently. Alright? I'm trying to understand what you mean by bottom, epsilon, quantile. Like if epsilon is 0.1, you're removing the bottom up to the 90th percentile or? No, no, so yeah. So let's say it's 10 with probability epsilon and 100 with probability 1 max epsilon. I'll remove everything below 10. The bottom is epsilon. The bottom is epsilon. So the top would be 1 minus epsilon. Right. So I so I preserve a bit because right I want to preserve most of the high values, right? The high values, right? I'm just removing a small fraction of the bottom part. How can you do that without knowing the distribution? Right, so that's a great question because I can just estimate this quantile with one over epsilon samples, roughly. That's what you mean by constant. Like one of the set constants. Yeah, yeah, yeah. One over epsilon is constant. Yeah, it doesn't depend on n. That's what I mean with constant samples. No, okay, so let's pretend for a second that everything behaves like that. So they're zero with probability at least one minus epsilon. How will it look like? So when we see the sequence, if we look at the specific time, and let's say they come in random order, what's the probability that I see something larger than x? Well, if I assume all of them look like that, like they're All of them look like that, like they're one over one minus epsilon with zero with probability one minus epsilon, then at an arbitrary time, most of the time I would just see a zero. And actually, each particular variable has more probability of being about x, right? So intuitively, it makes sense that they sort of behave like an ID. And this is not the full, this is not a full proof, of course, but here's a bit deeper intuition. Deeper intuition. So let's think again. The probability that at a particular time, if they come in random order, it's a randomly chosen variable. What's the probability that this variable is above x? If it's randomly chosen, it's just the average of the probability that it's above x, right? So 1 over n times the sum of the probability that xi is above x. Now, if it's at zero with probability one minus epsilon, then this f has to be at least one minus epsilon, right, for any x larger than zero. So this f is close to one, right? But when f is close to one, one minus f is very close to to the logarithm of f, minus the logarithm. Here is a plot, right? So so close to one is almost the same thing. Okay? It's almost the same thing. And then I can put everything inside the logarithm. It turns out that this is almost the same as just taking a sample from the geometric mean of the distributions. And then this means, I mean, this is not whole proof, it's just some intuition, right? But this means essentially that I can pretend that all the variables that have that kind of behavior after this presentation. After this presentation, our IED samples from the geometric. Okay, so that's intuition. Again, it's not a full proof. It requires more details, but that's the basic idea. When things are zero, most of the time, they behave, and when they come in random order, they behave like IAD samples converging. Okay, and then we're almost done because now, with a few samples, we do this split between things that behave like AID and things that don't by sampling this bottom epsilon quantile. Then, so we estimate very well this constantly many distributions that do not behave like IID, but there are very few, so we can estimate it very well. Estimate it very well. And for the rest, I just have to estimate one distribution. Right, so and then I have enough size. Of course, I never estimate well the takes, but for that we have the observability lemma, and we use that algorithm. Which turns out that can be found for another time. By solving the Bmax problems. Actually, so actually, I just actually we can. I just actually, we can just write an LP that encodes the algorithm. Because, so we're just using a few samples, right? Constantly many for each, so a total polynomially many samples. We just used empirical distributions. So there are very few points in the support. So we can just encode all algorithms in an LP that has polynomial size. And we just write constraints that encode the. And we just write constraints that encode the observerity lemma. And we know that the best one has a competitive ratio of an we can just encode the dilemma at this min-max because we are only looking at things with polynomial size support. We can encode that in an LP of polynomial size. So yeah, it exists, but also when the instance has polynomial size support, Instance as polynomial size support, you can encode it in an LP. That's polynomials based. Okay, so yeah, so to summarize, we conclude that for all four variants on the single choice profit inequality, a constant number of samples per distribution gives the optimal guarantee, and we can construct the algorithms in polynomial prime because of what I just said. It's based on two things. First is the observability lemma. Is the observability lemma that can be generalized to many variants of the total inequality? But unfortunately, the second step is very specific to the random order single selection problem. But again, it's key to go from polynomially many samples to understand. Yeah. Very nice talk. I think I'm still missing something basic, but I think the last thing you said might have helped. So if just step on its own already gives you, like, would give you some algorithm that would sample, would it be something like one over epsilon to the, to the, one over polynomial and epsilon times like log n samples or something like this per distribution per distribution? Would it it's just Just this level? Yeah, just remote stuff. Would you get something like that? Or, you know, other stuff on your head? I think you still need to make it point. So the thing is that you need to estimate well each variable, but also like at the same time. So because of union bounding. Yeah, yeah, yeah. Yes. So I see. So the key is that you want to be union bounding over only constantly. Over only constantly. Yes, yeah, yes, yes. Okay. Everything makes a lot more sense. Thanks. And constant again is actually what the equilibrium of terms. Yes, yeah, yeah. Yeah, yeah, yeah. But constant, I mean, it doesn't depend on n. You want to make sure that you don't need to make that? Alright, that's this is a