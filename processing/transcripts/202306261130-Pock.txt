Thank you very much. Sotiana, thank you very much for the invitation. Hello, everyone. Yeah, I'm talking about the recent work, which is mainly the work of my PhD student, Dominic, who's now finishing, and it's also joint work with Andreas Harping and Martin Roller from the University of Kraz. So the ground for this work is a Bayesian formulation for inverse problems. I guess these kind of formulations you will These kinds of formulations you will see quite a few times in this week. So, the basic idea is that we would like to recover a true signal X, let's say X is an image, from a corrupted or noisy observation set. And the connection between our measurement and the true signal is basically due to this formula. So, it's a linear, nonlinear inverse problem. or non-linear inverse problem. F is a so-called measurement or forward operator. In most cases, it's a linear operator. For example, in MRI, it's Fourier transform or in computer tomography, it's a Radon transform, could be a blur kernel or in image denoising F is just the identity mapping. And this new here is just measurement noise, which we assume here to be additive. And most often, it is just zero mean Gaussian noise IID. Noise IID with some specific standard deviation. So, when we move to a probabilistic framework, and actually, what we're doing here is probabilistic. You know, the images, the deterministic quantities X and Z are replaced by random variables written as with capital letters X and Z. And of course, they follow a specific joint distribution. So, you have a joint distribution between Measurement set and images X, for example, so which means that for a specific measurement, there's no one true image you can recover, but you can recover a whole family, a whole distribution of different images, and just simply a distribution connecting the measurements and the images. So, a very fundamental statement is that the Statement is that the famous Bayes theorem, which tells you how you should treat conditional distributions, and for example, give them some likelihood p of z give them x and the prior p of x, it tells you how the posterior should look like. So the likelihood of x give them a measurement. And this is basically derived from the product rule of probabilities with marginals and conditional. Marginals and conditional distributions. Okay, so I think this should be well known to you and just fundamental fact. So now the question is, how can we compute actually a solution based on Bayes' theorem? And there are two common strategies. One strategy is, when I just may go back, is that you look at the likelihood of the posterior and you say, okay, I would like to. Posterior, and you say, Okay, I would like to recover that one, that X that has the highest probability, and then this leads to maximum a posteriori estimation. And instead of you know maximizing the right-hand side of Bayes' theorem, you minimize the negative log, and then this leads to classically variational approaches. This is an energy minimization problem, then. But more stable estimator, actually, I mean, it depends on how your distribution looks like, but in the easier. Like, but in the easier cases where the posterior is a unimodal distribution, the more stable form of inference is given by computing the so-called mean squared estimate. And the mean squared estimate is that x that minimizes the quadratic distance to all possible reconstructions x weighted by the posterior. And when you, you know, compute the derivative of this least squares estimation, set it to zero, then basically you see that the solution. To zero, then basically you see that the solution is nothing less than kind of a weighted average, and this is nothing less than the posterior expectation. Because what we see here is nothing less than the average with respect to the posterior distribution, or it's also called because it's the minimum of the least squares functional, the minimum mean squared estimation. You can write it in this form. Here you see it's an average, or you can write it just with an expectation with respect to the posterior distribution where the random variable set is. Where you know the random variable set is fixed to a certain observation set. So, what you can also do, since we have in mind that we don't have a point estimate, we have the whole distribution, the posterior, and the whole distribution gives you more information than just a point estimate, like the map or the MMSE. It also allows you to compute something connected to uncertainty. And this is, for example, the variance. So, the variance, the intuition is if the posterior is very wide. The intuition is if the posterior is very wide, then there are a lot of different reconstructions with almost the same probability, and then there's a lot of variance. And if the posterior is, you know, is quite peaky, then clearly then we say there's maybe one solution, and this one is the best one, and all around is you can forget about it. And here's the definition of the variance: it's again nothing else than expectation of the quadratic deviation from the posterior expectation written in this form. Written in this form, or I can write it in this form. So, this expectation here is this guy here, and here you have the quadratic deviation, and the expectation is again with respect to the posterior distribution. Okay, so and the basic thing of this work is to study the posterior variance and see how this can be connected to the true error. So, in uncertainty quantification, if you say, okay, I have a solution, and for example, I have a solution, and for example, pixel wise, I can compute the variance. But what does the variance tell you? What we would really like to be interested is what is the true error with respect to an unknown ground truth solution. And if you assume that we have also a ground truth realization x, and x of z is the posterior expectation, then a common measurement of discrepancies, again, just a quadratic error. So x minus. Again, just a quadratic error. So x minus x of z squared. And since x of z was the posterior expectation, you can write it in this form x minus expectation of x fixing z. Now, when you compare this last formula with this formula here, then you see these guys are, you know, have some similarity. The only thing is here, we have one x, which is the ground truth. And here in this formulation, we have all x and we weight them with respect to the probability in the posterior. And therefore, And therefore, based on this observation that we have a functional similarity between the posterior variance and the mean squared error, we would like to see how much information about the quadratic error is actually contained in this posterior variance. So does the posterior variance tell us something about what is the true error? And this is exactly what we would like to study and also give some guarantees on how much you can. You know, how much you can say about the true error. So, as I said already, traditionally, the variance is used to quantify uncertainty in a prediction. This is something very natural, as I said. If the posterior is white, then you're very uncertain. If the posterior is more peaky, then you're more certain. But does this really carry the information we are interested in? What is the really the error we are doing when you inspect the medical image? When you inspect a medic image, you would really like to know if I look at this group of pixels or this pixel, how certain am I that this pixel value I've reconstructed is correct or not. And actually, here you see a comparison for a simple image noising experiment. So here, left with the noise observation, then the next one is the ground truth. This is some estimate based on the mean squared estimator, the posterior expectation with some method. And when you Method. And when you compute the pixel-wise quadratic distance, then you get an error image like this one here. And when you compute the pixel-wise variance, so this is the marginalized variance, of course, then it looks like this. And now we see, okay, I mean, they are not the same. This is clear. So this is on the left, you have the true error, on the right, you have the variance. But we are interested in how much information about the true error is contained here and can be counted. Contained here and can be quantified and maybe give theoretical guarantees. So here's a toy problem setup, which basically tells you everything. We have a joint distribution between our x and z, and both x and z are just scalars. Okay, so here on the horizontal axis, you have x, different values of x, and here on the vertical axis, you have different values. And here on the vertical axis, you have different values of z. So when we look at the prior p of x, we see this is just three Gaussians. So this is the likelihood of the true signals. So it's very likely to be minus one, zero, or one with some small variance about it. And then we have a noise model, which tells us, okay, our observations are the axis contaminated with noise. And the noise is Gaussian. And when you do this, you get this 2D distribution. So now we have these three Gaussian peaks here. Three Gaussian peaks here, and the you know, the vertical width of the Gaussian is the more or less the noise level you see. Now, when you marginalize to the left, what you get is the marginals of the noises observations, P of Z. And it looks like this. And you see, it's nothing else than convolving P of X with a Gaussian. So, this is a well-known thing. Okay, now what we can do is we can plot the posterior. Posterior means we have we fixed Have a we fix the observation set, for example, we fix set to be zero, and we are at this line here, and then we can draw the distribution along this line, and then we get this curve here. Okay, so this is the posterior, when we fix that to be zero, when we fix that to be 0.55, then we get this line, and we see now the posterior has two peaks, one smaller and one larger peak. Computing the posterior expectation means Computing the posterior expectation means computing here the expected value. And it's clear that it's just one peak, and the expected value will be zero. That's clear. Okay. And when we look here to this graph here for set equals zero, also the posterior expectation is zero. For this curve here above, we see that the posterior expectation actually is not very likely. So it's between the left and the right peak, and it will be maybe a little bit more to the right peak because the right peak has more mass. Red peak because the right peak has more mass. And this is exactly what we see. If Z is 0.55, then the posterior expectation will be 0.63. What we can also do is we can compute the posterior variance. This is just one narrow peak. So the variance is expected to small. We see it has the negative, the log value of the variance is minus 1.964, relatively small. And the variance here is much larger because we have two peaks and the expectation is in the middle. Peaks and the expectation is in the middle, and we get this value. Okay, so this basically tells us everything. We can now do an experiment, actually, where we simulate ground truth examples X at noise, compute from the noise observation set, the posterior, compute from the posterior the expectation, and compare this expectation to the X that has generated. Compare this expectation to the X that has generated the noise sample. Okay? So this is the inverse problem. It's what commonly is called inverse crime. So if you have ground truth, you add noise and then you know exactly the noise model and you recover the solution. What we can now do is we can take this joint distribution p of x and z, this is this one. And since we have explicit formulas for the ground truth, quadratic error, and for the posterior expectation, we can just make a change of variable. Expectation: We can just make a change of variables and compute a new distribution. Now, in the variables s and t, whereas s is the error, the quadratic error with respect to the ground truth, and t is the variance. And this is exactly what we did here on the left-hand side. Okay, so what you see basically is if I have a result where the variance, the log variance is minus one. Variance, the log variance is minus 1.42, then there's a distribution of different errors I generate. If the variance or the log variance is minus 0.6, then we see that the distribution is quite narrow. Okay, so there's no distribution. When do errors and variances occur jointly in my invest problems. In my invest problems. And what we then now do is we can compute the cumulative distribution with respect to the error, which simply means I integrate along the vertical axis for each t independently. Then I get the plot on the right-hand side. Of course, the cumulative distribution always, you know, it starts, it's a monotone function, starts from zero and set the rates at one. And this is now the distribution we see on the right-hand side. So here it's again the log 10 variant. So, here it's again the log 10 variance, and here we have the probability now that the error is less than or equal to a certain value s, and the value s you can read of here. And now from this plot, actually, what we can, which information can be derived? If I see in my posterior expectation a particular variance, like minus 1.42, then I can, for example, plot the 0.9 quantile. 0.9 quantal, and I know with 90 percent, my error will be less than what is this minus 1.90 or almost two, right? This is the information I can get. If I see a certain variance, I know that the error with 90 percent probability will be less than a particular threshold. So, this is the basic, that's the basic principle of the entire talk. Okay? That would be ideal. That would be ideal. Then it would say that the posterior carries so nice information that there's a monotone dependency between variance and error, but this isn't in truth. When you look at this distribution here, it's a multimodal distribution. So you see here from the variance that it goes up and down. It goes up and down. But we see actually in imaging examples, it's more monotone. It's just in this example, it's not monotone. And it turns out that this is, this is at least what I also believe is that the maximum you can expect from error quantification. That you say with a certain probability, it can tell you this intensity value doesn't have an error more than this and this. If I move up with the quantile, when I take the Move up with the quantile when I take the 0.99 quantile, I would be somewhere like this. Then, of course, with 99% accuracy, the error will be smaller than now, then a higher value than before. That's clear. Okay. And now the problem is that this distribution we were able to compute in closed form because we had nice Gaussian distributions, closed form formulas, and with some tricks, we were able to compute this distribution here. Here. And in practice, it's not because we will never have access to all different observations. I mean, here in 1D, it's easy, this is just this distribution. But if we are in medical imaging, then we have multi-dimensional data and we will never have enough training data. So what we have to rely on, rely on sampling, that means we will only be able to compute this distribution here, you know, in an empirical sense and also the right-hand side only empirically. The question is: how can we give The question is: How can we give theoretic guarantees still only if you have access to the empirical distributions? So, this nice video here basically tells you how these curves develop over time if I sample. So, this is now a sampling process instead of doing everything with closed formulas. And actually, what you see is that the curves of quantiles, you know, they change. The more samples they have, the more they approach the true quantile. Okay, the question here is: now can we give theoretical guarantees? Still in the case where we are only able to produce samples, not to come up with the entire distribution, just to be able to sample from the distribution. So, this is based on something that is called inflation of quantiles, and this basically makes use. Basically, makes use of a very fundamental research of a paper of Camdes. But let's first define what's a quantile and what's an empirical quantile. So let's assume we have a real-valued random variable, just a scalar random variable y. And what you see here is the definition of the Q quantile. Q is a number between 0 and 1. And as you see here, it's just a basic definition. It's the small Basic definition: it's the smallest real number y for which the probability that the random variable y is less equal to this number y is greater equal than q. In some cases, you have explicit formulas for the q quantile. For example, if y is just standard Gaussian distributed, then it's known that the q quantile is square root of two times the inverse of the error function of qq minus one. Of the error function of qq minus one. So, what happens if we just have samples from our random distribution? At this point, there's no, you know, we don't ask the distribution to be a particular distribution, just we say we have IID random variables from some distribution, we have n1s, and then we can define the empirically Q quantile with this formula. So, it's now the, you know, everything that is empirical. Now, everything that is empirically gets this hat here. The empirical q quant L y is the smallest number y, for which this quantity is greater than equal to q, and this quantity is nothing else than the probability that y i, the i th random variable is less equal than y. And how do you compute it in practice? You take all your samples, you sort them, you compute q times n is n is the number of samples, and you round them to the You round them to the ceiling. And so, in the end, it's the kth smallest value of the sequence y1 to yn. Just sort them and take that one that has an index qm. C. Okay, so the question is now, this is something we will be definitely be able to compute in practice. We will be able to produce samples and we can compute those instances of the random. Instances of the random variables. But what does this quantity tell you about the true quantile? That's the question. And for this, we use the lemma of Romano-Peters and Candes, and it's called the inflation of quantiles. So suppose we have n plus one random variables, and we pick a value q between zero and one, then it turns out that the probability that That the probability that the n plus thirst of the sequence, I mean, this is arbitrary, you can take any if you want, that this one is less equal, the inflated quantile based on the first n observations is greater equal than q. So what are you doing? You take the q and you enlarge the q, the value a little bit, and the factor of enlargement is 1 plus 1 over n. So you see, if n gets a large, is a large number, then this approaches 1. This approaches one. So you inflate it a little bit, and then the probability that this random variable is less than equal, this quantity here is greater than equal in q. So what you can say in conclusion, a new random variable y n plus 1 will be smaller than the inflated empirical quantile y hat 1 plus 1 random q with at least probability q. But there's a catch which makes Which makes it a bit tricky sometimes to think about it. Namely, the point is the following: the previous result is in probability. So, here's a nice example. This is a standard Gaussian distribution, and I've plotted the empirical distribution. I've just produced samples and made a histogram of the samples. So, what you see here in this black bar, this is the true quantal based on this formula with the arrow function. Formula with the error function. And the red one is the empirical 0.9 quantal. And then the blue one is the corrected one, so the inflated one. And actually, if these guys are larger than the black one, then you're fine, basically, right? Because then when you produce many samples, then they will be below because they are larger than the black one. If I have more samples, then actually you see that all these bars are. Actually, you see that all these bars are at the same place, there's almost no difference because this one converges to the true one. But what can happen is the following: that you are unlucky and you have random samples so that the red line is below the black line. Okay. And the reason for why this actually does not work is so this basically tells you if I take this one as a quant inflated, there's no guarantee that, you know, if I select You know, if I select many different random samples, that there will be below. The thing is the following: that this formula here is on probability. It also means you have to compute these empirically inflated quantiles infinitely often. So everything is on probability. So when you do this, I've programmed this in Python. And then you easily see that, okay, you have to randomize. randomize randomize not only on these values here but also on the way how you compute the bars here and then it holds so it means if we would do just one training based on on our samples and then compute the quantiles it's no guarantee that they will hold for all future examples so you also have to randomize over your training data sets you have to have in principle infinitely many training data sets that give you the quant inflated quantiles and then you know You know, this gives you the true results in practice. So, this is something you have to keep in mind, but in the end, if you know, if n is large enough, then everything is so close that actually there's no problem. Yep. How many trends are we talking about there? Because if you say, like, well, with n equals 100, you have to do many versions, and then it will be good enough. If you take n al, it will always be good. You have to do more than 10 experiments, then why not just take that? Why not just pick the end of thousands? Of course, there's a trade-off between how many samples you have because then, or you randomize over it, yeah. Actually, we didn't see that we need too many different trainings. Actually, in practice, we just had one training and then just made sure that we have that n is large enough and then everything was fine. And then the actual mistake we are doing is so small that it doesn't make any problem in practice. But you're right. Yeah. Does the end depend on how heavy tail the errors are? Actually, in this formula, not does not end at all. This is a fundamental universal research. The only thing is you need IID. That's important. Okay, so it doesn't matter how heavy they hold. Yes. I mean, of course, in practice, it might change, might change, of course. But here for this formula, you don't need anything. It doesn't appear anywhere in this formula. It doesn't appear anywhere in this formula. IID is important. And this is sometimes hard to make sure in practice that you have IID samples. For example, later we will see that we will apply it to different pixels. And pixels that are close with respect to each other, they are not IID. If this pixel has this color, the next one depends on this pixel has the same colour, so they are not IID. But you have to see that you have experiments, right? Right, uh yes, uh, but here yeah, okay, it's not it's not really used. So when you look to the proof, it's really very fundamental. Yeah, don't lose anything. Thank you. Okay, so now what we are basically doing now, we already know everything. We assume that we will be able to compute this joint distribution of errors and Is this joint distribution of errors and variances for our imaging problems? So, actually, just think you do it for one pixel. That's also maybe can be seen as a limitation of this approach that in principle, all your measurement data is the input, but you can make, you know, you can say only anything, something about one pixel, actually. So, this is the variance of this pixel, and this is the error of this pixel. And let's assume. Pixel and let's assume we will be able to compute this thing. Then we make the cumulative distribution and then we apply the inflation of quantiles lemma actually to each of those quantities here. That's basically everything. Okay, so here SI are the errors between observations z i and xi. So s of x and zeta is actually, you know, the arrow based on when zeta is the observation, you compute the solution xi and so on. And together on the random variable t hat i, which is the posterior variance. So in practice, actually, it doesn't make sense to consider the variance to be, you know, a real number. So we rather apply it to intervals that we say, okay, we bin the different variants. We bin the different variances into intervals, and you are free to choose the size of the intervals. And you make, you know, this you give the guarantee for the interval. So let's assume we have an interval, tau k, and for this we defined the estimated conditional error quantile as q hat tau k as 1 plus 1 over n tau k. So n tau k is the number of samples that fall in this interval of variances times q in. Of variances times q, and q is a user chosen value between 0 and 1. And s cow 2k is this times q empirical quantile of our errors that fall into this bin of variances. And actually, what we have in the paper is the following coverage guarantee. That we have just defined this empirical quantile here. That the probability that the error is less than this empirical quantile here for all these variances in this bin is greater equal than Q. So in practice, it means you choose Q and then in your reconstruction, you see a certain variance, and this tells you in which bin you are. The bin tells you. You are the bin tells you what is the empirical quantity, and you know that the probability that the error is less equivalent in this value s is greater equivalent q. This is what it in practice. So in summary, the algorithm is as well as you need a lot of reconstructions on your data. All these reconstructions gives you these combinations between variances and errors. From those values, you compute actually these values s hat q tau k and then for any future. And then, for any future reconstruction, you can just look up where am I in which bin, and I can give a probabilistic guarantee of how big my error can be in worst case. So, some intermediate remarks for high-dimensional data, like images, the sample set I are the given observations, as I said, and the samples XI are the pixel intensities. And in order to satisfy the ID, And in order to satisfy the IID assumption, we must apply the method to each pixel separately. So, if you want to really follow the theory, you would have to do this separately for every pixel. I mean, the sampling is the same, it's just the values S have be computed for each pixel separately. In practice, however, and this is what we have seen, is we can consider all pixel intensities together. It's not entirely clear for us why we can do that or if it is allowed. Can do that, or if it is allowed. So, you know, if you think in terms of probabilities, it can be very tricky sometimes. You can easily trick your mind with this. But it turns out we have made comparison that you really can put all the pixel intensities together, which gives you much more samples for computing the variance and error connections. And as I said, this guarantee only holds in probability, so no really strict guarantee can be given for just one individual pixel. For just one individual pixel, so what you have to say is: okay, if one reconstruction looks at this pixel, you cannot exactly say that the error will be less equal at a certain threshold. But if you say, okay, I have infinitely many reconstruction, look at the same pixel, and for those together, the guarantee will hold. But in practice, we have seen since the thresholds are so tight that actually it works very well in practice. Okay, maybe I should also mention that this estimation function, the posterior expectation, and the conditioning, which was the variance, this is arbitrary. So, in this sense, the method is universal. You can also have a neural network that tells you the uncertainty, and you can have a neural network that computes the prediction. So, this did not enter the approach at all. We have just seen this nice functional relationship between posterior variance and quadratic error. And therefore, we said, okay, let's try out these two guys. We said, okay, let's try out these two guys. Okay. So let's apply it to TVL2 denoising. So in TVL2 denoising, we don't have an operator. So we know that the observations are just images plus noise. Here we assume IIT additive Gaussian noise. Therefore, the likelihood looks like this: Gaussian distribution. And as a prior, we assume here for simplicity a total variation prior. Simplicity: a total variation prior. By the way, it's not a real prior because you cannot integrate it. That doesn't make sense at all without likelihood. You would need to, you know, cut it off at the boundaries or do something else because the normalization constant you cannot compute because TV is zero for our constant images. But still, I mean, when you combine it with likelihood, then it makes sense. And here we used a simple form of the total variation, which is. A simple form of the total variation, which is not here taking the L2 norm, but just the sum of absolute values, so L1 norm, because later we will compute the marginals based on dynamic programming, which only works for this form of the total version. Here I have the negative log posterior, which is just a Rudy-Nossa-Fatemi model, well known. And now the question is, how can we compute the posterior expectation variance from this model? So actually, we can make use of two. Actually, we can make use of two different approaches. One is based on dynamic programming, and the other one is based on Langemouth sampling. So, let's first talk about dynamic programming. Actually, the method that we use is coming from image labeling, so which is a field of discrete optimization, has been used a lot in computer vision around 2000 and later works in the field of graph cards and In the field of graph cards and macro random fields and belief propagation and so on. So, all these nice works in this field. Here, you assume that these rectangles here are image pixels, and every image pixel can take a different value. And the value could be just the intensity value, but you can also have depth values, motion vectors, or image labels. And then every pixel is connected to neighboring pixels based on these edges here, and they define the neighborhood. These edges here, and they define the neighborhood, uh, yeah, straight, uh, the neighborhood relation between the pixels. And then, what you do, you set up a certain energy that has two terms. One term is based on every pixel, so you have costs for selecting a particular label in every pixel, and the second term is the pairwise terms that assign costs to certain edges between pixels. So, if you choose in one pixel label one, in the second one, label one, One in the second one, label one, then the cost is zero. And if the labels are different, for example, you can realize the total variation prior here. So, this is how the energy usually looks like. Here you have the unitary terms, the per pixel cost, and here have the perverse terms. And actually, what you see here is that minimization of this energy can be done by choosing a path now here from left to right on the pixel grid. From left to right on the pixel grid, where the cost is minimal. So, this has close relations to the Bellman algorithm or also to shortest path algorithms. Okay, good. Oops. So, what about Markov-Randofield? So, this is more the thing we are interested in, in Markov-Randofield. We are not interested in minimizing this energy, but we rather say that we have a distribution where this energy is exponential minus this distribution. Distribution with some variance on a temperature parameter here, t that tells us how wide that distribution is, and the normalization constant here is set. So does anybody know why it's good to have exponential distributions? It's just because everyone is doing it. No, there's a very good reason. So when you have, for example, constraints on the marginals. For example, constraints on the marginals, and you would like to find out that distribution that has the highest degree of freedom, let's say a distribution that has maximum entropy, then when you come up with a Lagrangian approach with your constraints, you always find as a solution exponential distribution. So the exponential comes from the fact you're looking for a max entropy distribution. And if you have just a constraint on the variance, then the distribution with highest entropy is clearly the Gaussian. It's that distribution that has the highest entropy among all other distributions with fixed variance, is the Gaussian distribution. Okay, so therefore, it's quite common to make use of this kind of distribution. So, there's old work of Mamford and Zu, Max entropy, and so on. So, it's a quite nice paper where this is exactly derived by this makes sense. As I said, energy minimization and map estimation course. Energy minimization and map estimation corresponds to minimizing charge energy, but here we are actually more interested in computing the posterior expectation and the variance. And for this, we need to compute the marginals. So the marginals in this case would be what's the probability of selecting in a particular pixel eye a certain label or intensity value where I have averaged out all others. That's the margin. I've integrated. That's the margin. I've integrated or averaged or summed out all other pixels and just want to have the distribution for this particular pixel here. When I'm able to do that, from this, I can easily compute the expectation and the variance. So this function is just a proper normalized distribution over the labels for one pixel. Okay, so I just would like to, I don't have so much time here. Would like to, I don't have so much time here, but let me just outline. I think I have 10 minutes or something like this. Yeah. Let's just do it on a chain. Not an image, but a chain and see how this looks like. On a chain, everything is easier. So the graph is just n nodes with numbers from one to n, and the edge set is just the tuples between neighboring nodes on the chain. On the chain. Each node can take a label out of a given label set. Why? In our case, when we do TV denoising, it's just these great intensity values. And the energy looks like this as an easier structure. Now, coming back again to the macrofrandom field, actually, we are dealing now with so-called potential functions. We are the exponentials of minus those pixel and pairwise energy terms. And then the end. And then the entire posterior distribution looks like this. We have a product over the unitary terms times a product over the pairwise terms. And here in between, the phi's are exponential functions of minus theta and the psi's are exponential functions of exponential minus theta, ii plus 1, and so on. So here also these potential functions can be, you know, identified with unary terms and pairwise terms. So now let's see how we can compute the marginals based on. Can compute the marginals based on dynamic programming. And actually, all you need is this formula here: AB plus AC is A times B plus C. That's all behind it. And when you now consider this pixel marginals, this is this huge sum over all other pixels except that one that is here in between. And this is the formula from before. Actually, it sees that you can, you know. Before actually, you see that you can, you know, start to organize and group the sums so that here you just have this unary term here in between. And on the left-hand side, you have some, sum, sum, sum, sum. And on the right-hand side, you have some, sum, sum, sum, sum. And now it turns out that these sums, F tilde I and B tilde I, these are nothing less than the format and backward messages in the good old Bellman algorithm, and you can compute them by means of dynamic programming. So these are the functions, and you see, okay, in order to compute f tilde i, you must need to know f tilde i minus one plus unary and pairwise terms, which you know. And basically for the backup messages, in order to compute bi, you must need to know b i plus one. So you start computing these messages from the left and the right-hand side. And actually, based on dynamic programming, it tells you that I can compute all the marginal. All the marginals at the same time with just going from right to left and left to right at the same time. So this is quite low complexity. And then for numerical reasons, you replace those messages. That's the reason why I put the tilde here with the log messages. Now they are called, you know, Fi and Bi. And here's the normalization constant, actually, which you don't know and you don't need it. And it turns out that the formula to compute those. To compute those fi and b as nothing else than a log sum x formula, and it's a soft max or a soft minimum, it depends on how you would like to see it. So, actually, this is a soft max function. And it turns out that the energy minimization is exactly the same algorithm. You just replace the soft max with max. But the difference between marginalization and minimization is max versus soft max. That's all. Okay, so as I said, you don't need a partition function because. You don't need a partition function because whenever you have computed the pixel marginals, you can just normalize them over the labels, and it's just simple normalization. And as I said, in the log domain, we see that we have the soft max function and in energy minimization, it's just replaced by the max function or when you do. So it is energy maximization versus minimization if you do max or mean. Okay. Okay, so here I have a result to see what is the difference between marginalization and minimization. So this is the T V plus L2 model on a chain. The black function is the noisy thing. The blue one is then the inference. So the blue line here is the posterior expectation. And on the right hand side, this is the minimum. And one of the problems, the staircasing effect of T V is, for example, gone in the marginals here. And this black shadow, basically, this is the marginal distribution you see, and based on this marginal distribution, you can easily compute the variance. How can we go to 2D grids? This is now heuristics, and there are many different algorithms. For example, loop people leave propagation. We are decompose the image into trees like this form. And on the trees, you can compute the dynamic programming exactly. And then you start alternating different trees and combine them. Different trees and combine them, but in practice, it works very well. Usually, five iterations or so, and you get very good results. Next one, I need to speed up is other sampling algorithms. And the kind of algorithms which are very modern nowadays are the good old Uhla algorithm, for example, the unadjusted Launchmer algorithm. And this is basically nothing else than gradient descent plus noise. Gradient descent plus noise, and there are some theoretical research for that. That you need your energy to be continuously differentiable with Lipschitz continuous gradient, and the step size have to be chosen in a way that the sum diverges, but they have to go to zero. And then you can show that actually the distribution of iterates you are generating here approaches the distribution of the posterior. But this is the But this is the simplified version of the research you can get for this kind of algorithm. Unfortunately for the TVL2 model, the gradient of this term, you know, it does not exist. You have a sub-gradient actually, and therefore it's not Lipschitz continuous, and therefore the Uhl algorithm cannot apply it here. What I can do, you can smooth it, or I can try to apply a different algorithm. And what we did in the paper without What we did in the paper without any theoretical guarantees that we used a primal dual algorithm, which in our case is quite natural. And so it's a primal dual algorithm that can solve the ROF model based on the settle point formulation. And we add here noise in the primal step. So we are currently trying to get conversions guarantees, but it's very tricky, I have to say. So if you have a good idea, that's maybe something interesting to study. Interesting to study. This is a simple comparison between the result you're getting for dynamic programming, which gives you almost the exact solution and different algorithms. And here, this is the error that gives us a threshold based on the discretization on the discrete problem, which we solve with dynamic programming. And then we see how long it takes with different algorithms. So this is a smoothing of the total. This is a smoothing of the total variation term based on a Huber function and just then the ULA algorithm and then different primal dual algorithms with different primary and dual step sizes. And we see that the primal dual algorithm works very well. So it's the fastest algorithm. And of course, it depends a little bit on how you set the step size. Oops. We also did the same for the variants, and then we see there's a trade-off for the step sizes where there's A trade-off for the step sizes where the variance converges slower and the expectation converges faster. Something happened. Should I share it again? Yeah. But now it stopped. Screen share stop? Yeah. Yeah, just share it again, I guess. Yeah, but mess zoom joint meeting. Oh, okay. It's oh my zoom. Okay, is it um in the internet? Yeah that's gone. Were you on Eduro or the Edu Rom is there another uh visitor? Yeah, but no, I think it works again. So, what about the recording? I think okay. Recording is in yes. Yeah, recording is behind. Okay. Okay, sorry for that. Yeah, here we have just a simple comparison between the pixel marginals based on dynamic programming and on Lauschmann. And we see actually they are almost the same. I mean, up to a very small error. Good. So now let's apply it to different algorithms. So for the total variation, we have already seen it that we can sample and for filter. Uh, that we can sample, and for fields of experts, which is a better prior, we need to resort to a standard LaSchwart algorithm. But this is usually continuously differentiable, so no problem. And also, we used a more expressive prior based on the total deep variation of Eric Kobler. So, here are the final results. So, on the left-hand side, actually, you see now for image denoising over a big training database, the BSDS 500. Here on the left-hand side, the joint distribution of variants. Left-hand side, the joint distribution of variance and errors. And on the right-hand side, then you see the cumulative distribution with the 0.9 quantile. And now, as you said, it's almost a monotone function. So it tells you, okay, really, if the variance becomes higher, then also the threshold for the error will be higher. So that's a very nice relationship. I think this is for the total variation. Now, this is for the total variation. This is how it looks for the fields of experts prior, even better, as we see. Spray even better as we see almost monotone, and this is how it looks for that total deep variation. So, this actually tells that it works because there's a nice monotone relationship. So, here's a comparison for this one image here that we have already seen. On the left-hand side here, you see the true error, and on the right-hand side, we see the estimated error. Of course, it overestimates. It has to overestimate. This is how it looks. This is how it looks for the fields of experts prior, which is already better. The estimated error is a little bit more blurry. And for the total deviation, then it's a bit more crisp. Maybe this tells us a little bit more. Here we have the PSNR for the reconstruction. And what is interesting, we also computed a neutral information between, you know, the Based on the joint distribution, the mutual information is the connection between the factorized distribution and the joint distribution. And we have seen the better the prior, the larger the mutual information. So the better the prior, the more information about the error is carried in the posterior. That was quite interesting. And we also checked for the user-chosen different quantiles, actually, what is the Chosen different quantiles, actually, what is the empirical quantile we then can compute? And we see that it's always very, very close. And if it's below the user-chosen quantile, actually, it tells us that it works quite well. Panel result, this is now for medical imaging, MRI reconstruction. And here, of course, it's a bit more complicated because we have the linear operator and the data fidelity term, but it also works. And but it also worked quite well. So, what would be ideal if you know the variance or the estimated error really predicts the error on the left-hand side and it overestimates the error, of course, but in regions where there's more error, there's also more error here in the estimated error. Okay, so I was talking about the method how we can predict the mean squared error based on the posterior variance, and we have derived some. And we have derived some theoretical guarantees that, of course, holds only in probability. And what is important is that the method is universal, so you can try it out with your favorite inference algorithm and your favorite uncertainty quantification algorithm. And one limitation is still this trade-off between IIT. So that in principle you can only apply it for single pixel. And we applied it to image noising, and also briefly I showed it for one inverse problem in medical imaging. This problem in medical imaging. For future work, of course, now it would be interesting: how we can learn a better prior that leads to a better prediction of the estimated error based on the posterior variance, for example. So, thank you very much.