I'll just say Ben is a profit you see birthday in E. So can I ask the organizers when you said you have provocation, there are multiple provocations, what was the idea? Or were we just supposed to interpret that? No, it was just sort of stating your position on the topic and then allowing sort of just shaping the conversation, the panel discussion. Conversation, panel discussion, in a very, and you can be as provocative as you want to be. We picked individuals with stronger things. I might want to rewrite the talk now. No, I'm not. We can do it on the fly. We're going to do it on the fly. Let's do it live. All right, so I have, this is a copy of the title page of a book by Paul Meal from 1954 that I have mildly edited called Clinical versus Statistical Prediction. Statistical prediction. What was this gentleman after? Who was Paul Meal? Paul Meal was a psychologist at the University of Minnesota. He was chair of the psychology department. He was president of the American Psychology Association. He also was one of the founding members of the Minnesota Center for the Philosophy of Science. You might know Paul Neal for having invented the term construct validity. The term construct validity with Lee Kronbach. He also was a pioneer in this idea of clinical versus statistical prediction, which I'll mention in a second, and also probably has some of the most famous and unassailable arguments against null hypothesis testing. And yet, nobody's ever been able to refute me on, and yet we go on. So, a really interesting person, very deeply, was a Started off his career. He thought he wanted to go be a psychoanalyst. He was like enthralled with Freud. He went to Minnesota. He majored in psychology, had minors in philosophy and biometry. And that's quickly what led him to this problem because he wanted to be a psychoanalyst. At Minnesota, they had a very analytic, science-heavy, rigor-based psychology department. And Mule was torn. And Mil was torn. You have this issue that he likes psychoanalysis, he liked the advancement of what could happen in the clinical hour. How could you make these arguments that everything could be reduced to these behavioralist, very like, you know, rigid, rule-based diagnoses? And the debate in the, let's see, this will work, though it does work, the debate in the 40s in psychology and in the 30s before that was the idea of whether or not you should be making decisions. You should be making decisions or making clinical treatments based on ideographic treatment or nomathetic treatment. Ideographic, meaning more or less individualized. While we would think that every patient is unique, you have to approach each case uniquely. Nomathetic, meaning that you take in a holistic set of cases and use those as the basis of your evidence. And there was a push and pull. And there was evidence in both directions. The other thing that was arising, I think, is hard to appreciate. Was arising, I think it's hard to appreciate. Is there by the end of World War II, there really wasn't systematized statistics. There was a lot of different ad hoc versions of what we now know as statistics, but everything got, and I'm going to get more into this provocation in a minute, everything got systematized after that period of World War II. So it's interesting to look at some of the ways that people were thinking about how should we think. Should we think nomathetically? What is the right way to think nomathetically? So, I put up some quotes that I think were provocative. Neil was a big fan of thinking that the way that you should make judgments, and he came across, this took him 10 years to write this book. So this was over 10 years of research. He said, he agreed with Reichenbach that the right way to make predictions was to find the smallest reference class for which you have stable frequencies. So to come up with small, stable. Small stable associations such that you could have good production. How you do that, not always clear, but at least that would be the goal and straightforward. And if you look at statisticians, they were kind of, when they were trying to convince everybody else to do statistics, this is more or less what they were arguing. So Bart would say, you know, the persons are unique or ill-defined. If that's true, statistics has no value. But if things are similar and definite, and they can be counted, if you know what to count, If you know what to count, then we can do some physics. Similarly, Bradford Hill, the father of modern biometry, said that he, in a lecture, two physicians made the argument that really deep down, what you're doing, all of these physicians, is statistical inference without saying it. Because it can't be that every patient is unique. You're basing it on previous encounters and whether that previous. Previous encounters, and whether that previous encounter just grew on one patient, two patients, three patients, or four, what he says is you base your method of choice upon what you've seen happen before, whether it be in only two or three cases or in the score. You're a bridge, so keep Brian things. So I think there was at least that was like the pull. What happened slowly after this, when Meal wrote his book, Collecting Evidence, which at the time there were about 20 different cases, showing that this kind of counting output. Counting outperformed experts. He found 20 different examples. I'll go through some of those evidence. And he also found a lot of different arguments and a lot of different philosophy. He wrote this book in 1954, arguing that perhaps the right approach to clinical psychology is statistical prediction. If you build actuarial tables, remember them. So, what is actuarial prediction? Let's go through what Meal thought. Meal thought he had two definitions. First, the mechanical prediction. Two definitions. First, the mechanical prediction, I'm just now, you can see the background change color because we're not in the present. So, let me now do a translation from 1954 to 1924, 2024. Mechanical prediction, he just means an algorithm. That's all he means. By actuarial prediction, he just means machine learning. Very clear, that's what he means. Or you have statistical prediction or pattern recognition, or AI. They're all the same. I think, as we now know, they've all collapsed with each other. That reference class has collapsed. And actuarial prediction is a subset of mechanical prediction. Is a subset of mechanical prediction, but he also argues that mechanical prediction is probably also better than clinical prediction. So, an example this morning, Eli's talk, the what's the name of this or the PSA? The PSA is a mechanical prediction, not an actuarial prediction. Maybe they did some statistics, but they just added some numbers. You rank things at absolute numbers, right? There are lots of people who've looked at clinical risk scores numbers. There are some that you just rank things, you scale six things on the scale of zero to two and add them. So that would be a mechanical risk. And at them. So that would be a mechanical prediction. If you had instead had weights that you could fit by logistic regression, that would be an actual real prediction. And so Meal is arguing, and he will say this in 54, he says this again in papers throughout into the 80s when he was still writing about it, that you find the best features. And here's what's his argument for why astruarial prediction or when actuarial prediction is better than statistic. He was saying that if you have the same set of features and you give them to a clinician, Set of features, and you give them to a clinician, or you give them to an algorithm. And the algorithm had built actuarial tables on these features in the past and then made maximize outcomes based on the counts. That that should be better. His evidence and his philosophical argument was that should be better than most clinical predictions. And it's kind of weird. Like, does everything really fit into that? I guess this is the provocation. Is everything really just statistical prediction? Is everything really just statistical prediction? I don't entirely buy this argument that everything can be algorithmic. Certainly, if we're at a casino, we all agree that you should just play odds, which usually just means you go home. I do always think, actually, what's funny is that casinos are the way that we always argue for probabilities, that people always use them as ways to motivate utility maximization. And if if it's the clearest evidence that people don't utility maximize. People don't utility maximons, that casinos exist. And in fact, my favorite footnote in von Neumann and Morgenstern is the one where they cannot understand the letter. And they're like, we're going to leave that for future work. Insurance, that seems like that should be statistical. It has been for hundreds of years. But are decisions in high-risk surgery statistical? Might already be statistical. I feel like this is where it gets complicated. We can talk about this more later. Where it gets complicated, we can talk about this more later. I don't want to go into this so much. What Miel says is this: He's like, Look, here's the thing: you go to a doctor and you say, What are my chances if I can do this surgery? And the doctor says back to you, you're getting me. I can't tell you anything. Now, it's kind of funny, that's kind of a joke, but honestly, I think that's that you have to think through that. Because in some sense, that's true. Others says maybe you do want to know how many people have died on that particular certain stable. So, there's some. Certain stable. So there's something to be evaluated. Alright, here's the quick evidence for the superiority of the clinical method. First is a Burgess in 1928 did a fairly simple, almost the exact same algorithm that we talked about this morning, 1928. Took seven factors, scored them on scale 0 to 1, added them up, and this outperformed a bunch of psychiatrists in predictions of the series. Sarman, for example, Sarmin wrote a pretty scathing review where he was looking at predicting educational outcomes in 1943, and he came to the conclusion that a competent statistical clerk can make predictions as well as a highly trained clinical worker, which we do with that. Neal, by 1986, had collected all sorts of different evidence, and he is right, which is one of the most interesting things, one of the things that I don't, it's weird that we're in this conference where we keep talking about problems with algorithm and prediction, is that it is true. Is that it is true if you look at the literature and go into the 2000s later, that there are a few things in social science that have withstood the test of time: that clinical predictions are never better than statistical predictions. That's crazy. Social science, and that's funny. Like, social science is true. It's hard to have robust, robust things that last over, you know, this is now 70 years. I guess when we go back to birds, it's hundreds of years. So it's something to think about. It's something that takes seriously. So, it's something to think about, and it's something to take seriously, and it's something to think about moving forward. And now, so if we are moving forward, are we in a different position than we were not too forward? That's my question. A, shouldn't I put this together right beforehand? I don't know. The top three maybe were the ones that were a little more serious than the others, but you know, maybe there's something to do with the causal effect of situation. But I do think it's very interesting. I think the thing you want to also put yourself in the mindset of is in 54, we did not have systematized rules, we didn't have RCTs, we didn't have a regulatory system. RCTs, we didn't have the regulatory system we have now. Now we have lots of rules. And I wonder if this community, really, what's happening is that we're just having a really bad reaction to the fact that we've oversystematized. That perhaps we're looking at what happens if you have too many rules. This is something maybe that you'll didn't see. Maybe our problem isn't just that rules are bad, or that statistical rules are bad. It's just that if you overload us all with them, we could find lots of works. I don't know. We could find lots of woods, which I don't know. I'm provoking that up. But here's the thing: we can talk about why Will's right and end with this. But I think that the reason why statistical prediction outperforms clinical prediction is because if you accept a utilitarian lens, you can't beat statistical prediction. If you accept that you measure success by maximizing expected reward of some kind, well, then you said that I want a statistical. Well, then you said that I want a statistical algorithm to make my decision form. Because the optimal thing to do in that case is to choose something that dues is the optimal decision form. So, in some sense, it's the framing. And the way you would think about this is if you're going to evaluate a surgeon on the rate of people who died on their table, right, it's the rate, right? You turn it into a problem, you turn it into a probability. As soon as you decide that you're going to evaluate people based on probabilities, then statistics will win because you decide that statistics is often. You decide that statistics is often off. The same thing happens with regret. You can't fix it by going to regret. So, if you ask what is best and then you check on average, you've lost the algorithms. And so, again, just for the last few slides, I'm just going to put this one last thing out there where I put it to my left level, is that what we're doing by buying into that utilitarian frame is buttressing a big nation of tongue rules. Just a big bureaucratic system of rules. And by buying into the frame, I feel like that's all we do because I don't support anymore. Anyway, is there a way out? Well, are you done? Yeah. Oh, it's your last slide? It's my last slide. Okay, cool for it. This might be the way out. I don't know the way out. But something I've been fascinated by, and I have not been able to come up with any solution for, is that, or like any way for us to engage with. I think another problem that this room has is that I think a lot of the solutions or the things that we would like to be solutions are the Or the things that we would like to be solutions, or the way out of the algorithmic death trap, is to throw away technological solutionism. And so there is this axis of the hazard versus hero. Is a human a hazard or human a hero? There's a great book called Human Error by Jim Reason, where he lays out this axis. I think of it as not really an axis, it's more. And where up here, the human is a hazard that you have these things where I could put in procedures. I could patch things with procedures. Procedures. I could like patch things with procedures that are repeatable and measurable. I do think that there's something, and I don't know how we can deal with this, and so why is the qualification? Exactly. There is something down here at the bottom, whereas like, you know, at some point, and I know we're bad at it, technological people are bad at it, but we engage on a daily basis with the singular and very fun. And I don't know if anything that we do can say anything nice about it. And that's where I guess that's the kind of like weird question to hold. That's the kind of like weird question the whole thing has to do with. It's like, you know, I don't, I'm not sure that math and statistics would say anything down here, but like we can't avoid these kinds of things in our world. So, Jessica's going to tell us how to fix it, right? Thank you, Jessica. Where is Jessica's? I've got it. I've got using these translations.