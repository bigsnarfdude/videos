Thanks very much to the organizers for putting together this really nice workshop and inviting me to participate. I really appreciate it. We've heard some really fascinating talks throughout the day, today, yesterday, as well, and technical brilliance, lots of contributions. Really interesting to see the various connections among the topics. This is not going to be like that. This is a very applied talk. And it's the last talk of the day. And we're cutting into happy hours. So, this is actually a story about fishing. About fishing. So let me get started here. I worked many years ago on a review of recreational fishery surveys methods for NOAA's National Marine Fisheries Service. So this is an agency within the United States that oversees fisheries in saltwater, basically. And one of those is recreational fisheries. And recreational fisheries are hugely important. It's a multi-billion dollar industry. It competes with a commercial industry. It has big biological. It has big biological impacts. And so we wound up doing this review of recreational fishery survey methods. And ever since then, there's been this industry of looking at these surveys conducted by NOAA Fisheries because they're enormously complex. So surveys are complex and fisheries surveys are complex squared because they use two different surveys. They use one survey to estimate effort. So fishing effort is how many trips are people taking? Is how many trips are people taking? And they do another survey to estimate catch per unit effort. So the catch rate. And each survey is complex. So complex times complex is complex squared. And these are really messy. They all involve surveying humans, which we know is a miserable business. And these are done in two different ways. The effort survey is done off-site. So this is typically done with a mail survey or a telephone survey. While the catch-per-unit effort is done with an on-site survey. So they actually. Done with an on-site survey, so they actually send interviewers out to the field. Okay, and there are many such surveys, and this one is going to be focused on one particular survey called the Large Pelagics Intercept Survey. And a pelagic species is a highly migratory species. So these are big, interesting-looking fish that swim up and down the coast. And they're subject to international treaties, for example, on how many you can catch. So they're really important. These are things like tunas and sharks and marlins. And sharks and marlins, billfish, things like that. So, a typical question of interest would be something like: how many wahoo were caught by recreational anglers along the U.S. Atlantic coast in 2021? Okay, so there's a real problem that you might want to address. So, to do that, we want to be making inference about numerical characteristics of a real, well-defined, finite population. And there's nothing random here. This already happened. This was back in 2021. Happened. This was back in 2021. So there were Capital N trips that targeted pelagic species in the Atlantic in 2021. And if you could go to every one of those trips and record how many Wahoo were caught, you could just add up those numbers and that's the answer. But of course, that's infeasible. So we're going to draw a sample and try to make inference about that number using information contained in that sample. And immediately we run into a problem, which is there isn't anywhere where you. Which is, there isn't anywhere where you can find a list of all these large pelagic strips. And instead, what NOAA does is they create a frame of what they call site days. And the sites are fishing access sites. Where can you put a boat in the water to go out off the coast and fish for Wahoo? And the days are the days in the fishing season. So if you cross those two together, you get a frame of site days. They take a sample of site days. They send an interviewer out to the field like this. Interviewer out to the field, like this lady here, who is intercepting a boat trip and recording the catch by species for that particular site day. Count up all this, all the trips on that site day and record the catch for each of those species of interest. Okay, so excuse me. So we've got a universe of elements. We've got variables of interest. YK would be a generic catch for some species. Usually there are many, many species, not just one. Many, many species, not just one. So, yk is just generic for one particular one. ZK will consistently denote the number of trips on that site day. And we're going to draw this probability sample using known positive inclusion probabilities for all of the elements of this frame and use that as the basis to make inference about finite population parameters, which include the y total, the z total, but most importantly, the ratio of the y total to the z total. Total to the z total. So, a little bit of notation, the usual notation for indicators for sample membership, one, if k is in the sample, zero otherwise, and with expectation of an indicator being pi k. Okay, and we all know the Horvitz-Thompson estimator. We just take those inverse probability weights, we assign them to the yk's, we add them up, and that is an unbiased estimator under. Is an unbiased estimator under no assumptions on properties of the Y case. It's just properties of the design under repeated sampling. Same for the ZK. And we're really, again, most interested in that rate. So if you take that ratio, it's asymptotically unbiased for the corresponding finite population ratio under mild conditions on the sequence of populations, designs, and properties of the Wiseman disease. Okay, so we're going to use this kind of approach for the Large Pelagics Intercept Survey targeting the rate. And the problem that And the problem that we immediately encounter is that you go to all this trouble and you send the field crew out there, and they get to the site and they spend all day there, and there are no pelagics trips at all. So this makes them really angry because they wasted an entire day. And they really want to choose their own site days. They think they have the expertise that they could do a better job of choosing where to locate their samples. And of course, we don't really want to do that because we have no. To do that because we have no inferential basis to make extrapolation to the finite population. But we figure: okay, let's be nice people about this. Let's see if we can come up with a design compromise, which is to say we'll select an initial probability sample and we'll divide it at random. And part of that sample is going to be maintained as a strict probability sample with its known inclusion probabilities. And the other part, we're going to allow the field crew to mess with it. They can change that if they want to do so. Change that if they want to do so. So, the problem there is: we'll call this the A sample and the B sample. The A sample will be the strict probability sample, known inclusion probabilities, strictly maintained, and the B sample has unknown inclusion probabilities. Okay, so this may seem a little idiosyncratic, but there's a lot of surveys where we do this initial screening step. So, we might do household surveys where we're looking for kind of a rare demographic characteristic, like maybe age-eligible children for a vaccination study, okay? For a vaccination study, okay. And you go to a lot of households, there's no age-eligible children. So, if there are a way to kind of build out your sample, start with a probability sample, maybe supplement it in some way, that could be potentially useful. And there's a lot of other contexts, maybe an establishment survey where you're looking for a rare occupation or something like that. Okay, so the problem is this expert judgment selection mechanism is unknown. And if we're going to make some inference, we need to recognize that this BC. We need to recognize that this B sample is no longer a probability sample. And the one thing we do know about the B sample is that the field crew chooses the B sample after they see the A sample. So they're not going to revisit the A sample site days. That doesn't make any sense. It's the same site on the same day. So we can at least factor pi KB into two pieces. The part that says, I'm not in the A sample, and then the part that says, okay, given that I'm not in the A sample, which one did I choose? And that's actually important. And that's actually important in some of the fine strata that we look at because this 1 minus pi ka is essentially like a finite population correction. We have pretty large sampling fractions in some of these strata. So we do want to keep this into account and not ignore it. Then we're going to have to estimate this row K. And one of the interesting things about row K is that it can depend on site day characteristics, including the number of trips and the catch, because we will have observed that at the point where we try to estimate. Observe that at the point where we try to estimate. So we're going to specify a parametric model for the row case and fit it using the A and the B sample. So what we're going to do here is make the usual assumption that this is behaving like a non-response model. So it's like independent Bernoulli's with different probabilities are okay that we will parameterize and estimate using the data that we've obtained. And this is Data that we've obtained. And this is a big assumption. The judgment model, or you know, people are flipping coins and we don't know the probabilities. So we know this is not the right model, and we want to keep that in mind as we go forward. We're going to just do logistic regression here. And as you probably know, in this setting, you can't just do plain old logistic regression because in the B sample, you only see successes. You don't get to see what's happening on the failure. So you have to add on this other piece which you can estimate from. To add on this other piece which you can estimate from the A sample, the probability sample that we've maintained. So we do that pseudo-log likelihood approach. And we've also done the same thing with kind of a width replacement likelihood instead of a Poisson likelihood. It doesn't really make any difference. So maximize with respect to the parameters in row k, get some initial estimates, row k tilde, and then we normalize them so they kind of match up to the sample size of the B sample. We then have a set of estimated inclusion probabilities, which take into account both what we are. Which takes into account both what we already knew from the A sample that we weren't in it and then the part that we just estimated. And now that we're at this point, we're in a situation that's kind of similar to probability sampling from two frames. So in dual frame estimation, there are multiple valid estimators, and they kind of depend on what you know and about the design properties for one design applied to the other sample. So here we have Okay, so here we have options, and we'll just try them out. So, one possibility is what we'll call a separate estimator. Just compute the Horbitz-Thompson estimator from the A sample, because there's nothing wrong with that. That's the usual unbiased Horbitz-Thompson estimator, pure probability sampling. And then compute this approximate Horbitz-Thompson estimator from the B sample by just plugging in estimated probabilities, and then take some convex combination. It could be entirely ad hoc, or it could be determined by the relative sample sizes, or you could try to optimize it somehow. Or you can try to optimize it somehow, but we'll just do something simple. Another possibility is what we'll call the combined estimator. For the combined estimator, we will just take the two samples, the A and the B, we'll stick them together, and we'll figure out what's the probability that you wound up in this combined sample, either from A or from B. Okay, not from both, because that's excluded. So we just do this computation, plug in row k hat, and now we can construct a Horbitz-Thompson-like estimator using these estimated. Like estimator using these estimated joint inclusion probabilities, not joint inclusion probabilities, but the probability that you're in the combined sample. And that has a nice form. It's just yk over this combined inclusion probability. And it's nice because we know that that denominator is bounded below by pi ka, which we know to be positive by design. And so this is never going to be the case that we're going to get tiny little estimated ROH K's that are going to blow up our weights and have bad behavior. So we kind of like Have bad behavior. So we kind of like it for that reason. So you can prove stuff about this. You know, you can make the usual sets of assumptions about properties of the strict probability design. And then we've got Poisson sampling on the non-probability design. You can stick that together in order to get usual kinds of properties like mean square consistency and consistent variance estimator and asymptotic normality of your estimator and so on. And the concern here is that, yeah, you can do that. Here is that, yeah, you can do that. That's a useful thing to do. But all of that relies on that stochastic assumption about how expert judgment is working, which we don't believe. You know, it's probably not the case that people are sitting around flipping coins. So we want to think about robustness of this methodology because this is for a real problem. You know, no officiaries want to send folks out into the field, allow them this judgment behavior, and then get an estimator at the end of the day that's defensible. And, you know, defensible. Defensible and defensible in a pretty broad sense because these numbers feed into international treaties, they feed into the division of counts of species between recreational commercial fisheries and so on. So it's pretty important that they get it right and believe it. So we've got this possible misspecification, not just the parametric specification of row k, but the whole idea that these things are sampled by Poisson sampling. So one possibility that might at least address the parametric That might at least address the parametric specification would be to think of something doubly robust. So you might, we've already got a model for the selection probability, and we could say, well, let's stick in an outcome model as well, some kind of regression model for yk given xk's. And you can do that. You can write down a doubly robust estimator and it will have exactly the kinds of properties you want that if you have gotten this probability essentially correct, then you have properties like a model-assisted estimator. A model-assisted estimator, and this is going to work regardless of the quality of the regression model. Whereas, if you screwed up the probability but you have this model essentially correct, then the residuals will be unbiased and this sum of the predictions will be approximately unbiased for the total of the Y. So that would work. And in simulation, it works exactly as advertised, right? But in the real application, we don't have great covariates available. Have great covariates available at the frame level that are going to predict catch by species. You know, imagine that you have this. You could sell this for a fortune to people who are out fishing, right? You can sell them this information. They'd be happy to have it. So we don't have great covariates available for the frame, but we have a saving grace, which is that we're really interested in the catch rate. And the estimated catch rate is going to be the ratio of these weighted estimators. So whether we're doing separate or combined, we're going to. So, whether we're doing separate or combined, we're going to have the sum of the weighted Y's divided by the sum of the weighted Z's. And those weights, and the way we construct them, we could allow them to depend on the catch, but we don't because there's a whole bunch of catches and we don't want it to do that. But we will allow it to depend on trips. That's not a problem, and it actually works quite well. And the rate is actually has a little bit of double robustness just by construction. So, under a pretty plausible outcome model. So, if you just say that the expected value of yk given zk. The expected value of yk given zk is proportional to zk. That's not crazy because it's regression through the origin. If you don't take any trips, you're not going to catch any fish. Okay, and then it sort of increases linearly, you know, so that's not crazy. And if that model holds exactly and the weights depend on the z's, but not on the y's, then this is exactly unbiased. Okay, so that's kind of a nice property. And that's whatever the quality of the probability model. You could screw up the probability model. That'll still be unbiased. And if the probability model is correct, And if the probability model is correct, then you get the usual design properties giving you consistency of the ratio just by the usual argument, whatever the quality of the outcome model. So that's kind of reassuring. We have some hope that we can get away with screwing up the specification, right? And so, but we don't trust any of this. So we want to really try it out with extensive simulations. And so what we did, and I should say what Chien Men did, may have. Being flustered trying to get the thing going, I may have forgotten to acknowledge Jin Nin Wang, who is my PhD student who just graduated a few weeks ago. And this is joint work with her and then with lots of collaboration with folks from there. So we used historical large pelagic data to create an artificial population. It's stratified in the way these things really are stratified, which is by region, by month, by kind of day. Month by kind of day, like there's different fishing behaviors on weekends and holidays than there is during the week, and so on. And each of these, there's over 57,000 site days, each have a known pressure. And pressure here means an index of expected phishing activity. So no one knows this. They've gone through every site day and they say, this is how much phishing activity we expect. And that's based on historical data and what they know about the sites. So we've got a pretty good, a decent covariate there at the site. There at the site day level, we simulate trips for each site day that are realistic. There's zero inflated Poisson models. So a lot of times there's zero trips and then sometimes there are positive trips. So we simulate that. And then given the simulated trips, we also simulate catch. And we do this for 11 different species. So these are made-up species, but what we do is take Poisson models and we might zero inflate them and we might truncate them. So for example, if we truncate them at one, If we truncate them at one, that says this is a kind of fish where you're only allowed to keep one. So that's a bag limit. Or maybe you're allowed to keep four. We might have zero inflation. So some of these fish, you go out there, you get nothing, nothing, nothing, and suddenly you hit a school of them and you catch a bunch of them. So you might get zero inflation. We have different kinds of truncation. And we have different relationships between catch and trips. It could be linear, but it could also be square root. It could also be quadratic. So we just create these sort of artificial species, but these are. Of artificial species, but these are reflective of real species out there in the world. Okay, so we then use the traditional LPIS sampling design, which is stratified sampling, to select a stratified sample of 865 site days. And within each stratum, we then divide it at random into 75% that has to be maintained as a strict probability sample and 25% that the field crew can move. And we allow two different movement methods. So one Movement methods. So, one of the movement methods is more strict than the other. The stratum method says you can move, but you have to stay strictly within your original stratum. And the bucket method says you can move a little more broadly. So it just loosens up that restriction. And that was something that the field crew wanted to have as an option. So then we have to simulate the judgment behavior. And we did this in many different ways, none of which are Poisson flipping a coin. Okay, so that's never the case. That's never the case. The first is no move with judgment. So that means the field crew looked at the sites that they were able to move and said, no, thank you. I'll keep them where they are. But they made that judgment. Okay, so this is not a probability sample because they had to make a judgment. Unskilled says they are useless. They make random moves. So they started off with probability proportional to pressure. They just screwed this up by just randomly sampling outside of that. Of that. The next two types are expert jump and skilled jump. This says that the field crew are really good at avoiding zero trip sight days. Expert jump says they completely avoid them. Skilled jump says they're pretty good at it. So I think they miss half of them. And in fact, in the pilot study, skilled jump is pretty close to what they see in practice. The next one's a little weird. It says it doesn't affect the distribution of zeros at all, but Of zeros at all, but if there are positive trips, it just shifts the distribution towards higher numbers, higher positive numbers of trips. Okay, and then the next two are combinations of those that affect both the distribution of the zeros and the distribution of the non-zeros. There's two more. They each start with these row k's, which are modeled as logistic functions of trips, and then they either use those probability. Use those probabilities to draw a without replacement sample using approximately these unequal probabilities, or drawing a with replacement sample using exactly these probabilities. Neither one of those is Poisson sampling, but they're close. So these are the designs that are as close as possible to what we actually use for estimation. And then finally, there's no move without judgment. So this is, forget all this judgment stuff, just do the no move strategy. That's just probability sampling with probability weights. So that's the basic. With probability weights. So that's the baseline strategy, the classic estimation strategy. So for all nine of the judgment behaviors, we have to estimate the row k's. And we do that through this whole simulation study. We do a thousand replicated original samples. We generate all of the nine judgment samples under both of the movement methods. We model the row K's as functions of trips. And then we estimate catch rate for 11 different species using four different estimators. So they are the combined. Estimators. So they are the combined or the separate with the Poisson or the with replacement likelihood. Okay. So, and then all of that gets compared to the no move without judgment, which is the baseline probability design. So this is maybe a little hard to see, but the most important part of this thing to see is that there's a blue line, horizontal line, and most of the mass is below it. So, because these are ratios, these are box plots of root mean square error ratios. All of them. Square error ratios, all of them relative to the baseline strategy of probability sampling only. So being below the line means expert judgment helps. Okay, and they're divided across the top into nine chunks. So no move, unskilled, expert jump, and so on. So those are the nine different judgment strategies. Within each one of those, there are eight box plots. So down here, there's a total of eight box plots. The first four are combined, the next four. Combined, the next four are separate. Okay, and the four combined correspond to the two movement methods, stratum and bucket, and the two likelihoods. Okay, but whoops, sorry. And the things to see from this figure, sorry, first of all, each one of these box plots only consists of 11 points because they're just the 11 species. But just to avoid having a zillion dots on here, they're summarized. Zillion dots on here. They're summarized as box plots. And what you can see is that combined is consistently better than separate. Both are consistently, almost consistently better than probability, classic probability. So expert judgment works. Combined is better than separate. The bucket movement method gives you a little more, a little freer movement. So it does a little bit better. And those are kind of the highlights. Okay. So it holds up a Heights. Okay. So it holds up across a wide range of behaviors. And I'll just repeat that none of those judgment behaviors is consistent with the Poisson model that we use to estimate our probabilities and construct the estimator. Okay, we also looked at variance estimation. We started off with what we call V hat one. We just treat the final combined weights as if they're traditional survey weights. We plug them into traditional survey software with the usual linearization. And then we thought, oh, that can't possibly work. Let's try all this other stuff. We derived things with. This other stuff we derived things with Poisson and with replacement sampling, we get jackknife and we get grouped BRR and blah blah blah. And VHAT1 worked best. So it has best mean square error properties among the ones we considered across this range of simulated characteristics and the best confidence interval coverage. So not bad at all and easy to implement. So the summary for the expert judgment is that this simple and wrong model for judgment behavior works in almost all cases. Works in almost all cases. It fixes up the bias. You get a combined estimator that beats the classic strategy, the probability sampling strategy, in almost all cases, across a range of catch characteristics, across a range of judgment behaviors, across these different sets of movement constraints. And it doesn't really matter if you do post-on or with replacement likelihood. It's the same thing. Combined meets separate in almost all cases, and this simple variance estimator gives good confidence interval coverage. So we show this to NOAA, and they're quite happy about this, and the field group. Quite happy about this, and the field grew. They're actually pilot testing it. They did it last season, they're doing it this season. First season, they had some problem because nobody moved anything, and now they're getting a little more brave. So that to be determined, you know, there's data coming in. Okay, so this dual frame estimation approach worked quite well in our specific context of expert judgment sampling. And we had done a lot of kicking the tires. You know, we really, I'm not even showing you all this. I'm not even showing you all the simulation results that we tried. So, we tried this in lots of different contexts and it continued to work pretty well. So, we had built this hammer, so we started looking around for nails and decided to try it out on a couple of other problems, one of which is respondent-driven sampling, where you've got an initial probability sample of seeds and a non-probability sample of sprouts. And then another is just probability sampling with a supplemental convenience sample. Okay, so we're seeing. Okay, so respondent-driven sampling: the idea is we're trying to do a link tracing design in order to look at a hidden or rare population and use the links among the elements of that population to try to find more cases. When they're rare, if you just went out sampling, you rarely find them. So we start off with a set of initial respondents or the seeds, and that's going to be a strict probability sample. And then they get to recruit their peers, and their peers might get to do additional waves of recruitment. Do additional waves of recruitment, and all of those additional waves of recruitment are a non-probability sample. So, everything beyond the initial probability sample is non-probability. So, we'll want to be able to estimate the unknown probability of that recruitment process. And many existing methods make some assumptions about how this process diffuses through that network and kind of achieves some nice properties as it kind of walks around the network. And we're not going to do that, we're just going to take this dual frame. Do that, we're just going to take this dual-frame estimation methodology and just apply it directly to this problem. This is purely by simulation, just to see what it looks like. And to do this, we created an artificial population using a very famous data set called Project 90. These are network data from a study in Colorado Springs, Colorado, of heterosexuals, transmission of HIV. There's lots of individuals in this data set with their links to their peers. Links to their peers, and each of the people in that network has a set of binary attributes, including: are you retired? Are you female? Are you a pimp? Are you a drug dealer? Are you, you know, lots of different binary attributes? So 13 of them. And what we're going to do here is simulate a respondent-driven sampling design. And if you look across the literature of some of these designs, some of them have no choice but to start with a very small. Start with a very small initial sample and maybe non-random, and then just wait, wait for this thing to diffuse. So, no choice there. But others actually do draw an initial probability sample and then allow waves of recruitment. And that's the case that we have some hope on. So, that's what we're simulating here. And that's actually based on a real study that's published in Journal Official Statistics. Starts with 100 random seeds. The seeds, we will either select them just randomly or we'll select. Select them just randomly, or we'll select them with probability proportional to the number of connections that those individual seats have. We'll have a target sample size of 130 or 150, which means this is going to, we're going to allow this to start, we're going to stop it pretty quickly, which is realistic in a lot of real problems. You can't wait around forever. You have a time stamp to meet. So our target sample size is 130 or 150, and each respondent is allowed to recruit up to three peers. To recruit up to three peers, we compare it to various estimators in the literature. And I'll just highlight one of these, this Woltz and Hickathorn approach, which does something that is quite intuitive. If you imagine this thing diffusing through the population, then you would imagine that people are sampling from among their peers, and the peers with more connections are more likely to be selected. So, we want to downweight those folks with more connections. That's the degree of DK. Degree of decay. So we will also do our combined estimator using this dual frame approach, and then we'll just do a simple convex combination of our combined estimator with the VH estimator. And the reason for doing that is because we know that if the probability sample size gets very small, we're going to suffer. And so that's where VH has more ability to do stuff. So let's trade. To do stuff, so let's trade off by taking the relative sample sizes of these two types of sample and form that convex combination. That won't do much in our simulation study because all I'm going to show you here is the case where we have a pretty large initial probability sample. But if you change that dial, then this would have more impact. Okay, so once again, we wanted to really kick this around by trying lots of different simulated behaviors of the recruitment. And the standard assumption is that. And the standard assumption is that acquaintances are just recruited at random. So, if I have five acquaintances, I just pick three of them at random and I recruit them, and then that continues. So, that's the standard assumption. The next one we tried is what we call recruit fraction, which means we will do the same thing. We'll recruit at random, but not necessarily three of them. We'll pick a random number between zero and three, and then we'll recruit that many. Next up was degree, which says pick your popular friends, and then inverse degree. And then inverse degree says pick your least popular trends. So it's either proportional degree or inversely proportional degree. Prefer female says that females have to recruit females, males have to recruit males. So we have this structure. Prefer pimp, same idea with pimps. Expert female, everybody who knows a female has to recruit a female. Expert pimp, everybody who knows a pimp has to recruit a pimp. Okay, so all of these quite different from the random mechanism initially. And once again, we'll just make a simple. And once again, we'll just make a simple model for the recruitment behavior, the probabilities in the recruitment behavior, but we'll just make it on a logit scale, linear in degree, fit that by maximizing the pseudo-lag likelihood, just like we did before. And then for each of a thousand replicated probability samples, generate all eight versions of the recruited sample, estimate the rates and the variances for the 13 attributes using the classic estimators. These are just in an R package, so we just use that directly. And then That directly. And then for the remaining cases, we'll estimate the row k's using the pseudolog likelihood and use a variance estimate that's computed by assuming the final combined weights are traditional survey weights, I should say variance estimates, and form confidence intervals. So these are results, and this time the box plots are for the 13 attributes, and they are relative to the combined. The combined estimator. So, box plots less than one favor the combined estimator. The SH estimator is, though it actually works here, it doesn't work very well. So it's not so interesting. But the combined estimator dominates the others across this range of recruitment behaviors, including the convex estimator. And again, this is a real example of how people might do such a study, but it's not the only way people might do such a study. Not the only way people might do such a study. And as you shrink that probability sample size and let the waves continue farther out, the others will do much better. So we do a lot better here. These are confidence interval coverage. So now the combined estimator appears there explicitly. It's this light green pair of box plots, again, across the various attributes and across the various kinds of recruitment behaviors. And consistently, we're doing better than the corresponding confidence. Than the corresponding confidence interval coverage for the other classic methods, a little bit better than the convex combination, and pretty close to nominal, though not quite there. Okay, and this is not too unexpected, but just out of the box without any special treatment, it does not too badly. Okay, so in our fairly limited simulation study, the combined estimator dominates the existing estimators. It's robust across this range of attributes and across a range of recruitment behaviors. And across a range of recruitment behaviors that we've just made up, a variety of recruitment behaviors. It doesn't really require strong assumptions. And that simple variance estimator, once again, works quite well, including giving good confidence interval coverage. And again, in other settings, like if you have fewer initial random seeds or if you have longer waves of recruitment, these other methods are going to be much more competitive. So just want to put that big asterisk there. So I want to describe one more kind of study. This is a convenient sample. These are increasingly common. Convenient sample. These are increasingly common in real surveys. So, responses to surveys are decreasing, the cost of obtaining probability samples is increasing. So, this is a hard problem. And it's becoming more and more common that you'll start off with a small representative probability sample, and then you will supplement it somehow with a convenient sample of various types. And these can be purchased from vendors. You know, you can just go to a vendor and say, give me a thousand of this type. And so this is becoming really common. Something to be aware of. Something to be aware of, if not worry about. And without a probability sample alongside this non-probability sample, there's kind of no hope of making appropriate inference from those convenient sample data, but we have some hope in this setting. And the particular example here is a real study called Culture and Community in a Time of Crisis, which studied things like, well, culture and community during the pandemic. Okay, so it had So it had a probability sample from U.S. households with known inclusion probabilities. So a well-designed research grade probability sample. And alongside that was a non-probability sample from an art organization's mailing list. So does not have complete coverage, does not certainly have complete coverage. And not known inclusion probabilities. So we want to combine these two research. So, we want to combine these two resources using a dual-frame method. And this is a little different than the earlier problem because the earlier problem we started off with design all the way through. So, we knew all there was to know about the B sample before we allowed expert judgment to affect it. But here, we don't know anything about the B sample. So we don't know what frame it's coming from. We don't know what initial probabilities are. So, what we did here is to, we have a rich set of covariates. We'll use those covariates. Those covariates available in both samples to do a step of matching so that we can assign a probability, an A sample probability to every B sample element. And then we've got a matched part of the A sample and an unmatched part of the A sample. The matched part of the A sample plus the B sample, we'll use the dual frame estimator. And the unmatched part, we'll just use a single frame estimator. And we'll stick those two pieces together. So that's what we do. And the reason I wanted to present this is to let all of you know. To present this is to let all of you know that there's a nice platform, a simulation platform that you might have interest in that was created by some of my colleagues at NORC for a sort of a competition at the joint statistical meetings in 2021. It took these real data, the CCTC data, and created an artificial population of 113,000 records, a subpopulation that you don't know, consisting of 74,000 records. Records, and there are a whole bunch of binary variables of interest along with a whole bunch of covariates. Okay, and the binary variables of interest are things that have to do with culture and community. Like, do you want to see a play? Do you want to celebrate your heritage? You know, what are you missing during these pandemic conditions? So, there's a whole bunch of these binary variables. And then Norc went on to simulate a thousand probability samples with known inclusion probabilities, so A samples. And then they simulated a thousand non-probability samples. Non-probability samples. And those are various sizes, but the one I'll focus on is of size 4000. Okay, so a big, convenient sample, a smaller probability sample. We know the inclusion probabilities for the A sample. We don't know the inclusion probabilities for the B sample. We've got lots of possible covariates for matching and for propensity estimation. And we do this and we do the matching step, as I've described, and we do a propensity estimation using a really simple model. Intensity estimation using a really simple model, and then just try out our combined estimator once again. And what I'm showing here are the, there are 22 variables, 1,000 realizations each. I'm only showing the best five and the worst five just to fit them on the plot. Okay, so the best five are the top. Each set of box plots is for one variable. The first three box plots here are for the binary variable see a play. The first box plot is the same. A play. The first box plot corresponds to only using the probability sample. So there you have known inclusion probabilities, you've got the unbiased Torvitz-Thompson estimator, no problem. The next one is the combined estimator. Sorry, it's hard to work this thing. The dark blue one is the combined estimator, and the sort of golden rod color is the separate estimator. So top five are the best cases, the bottom five are the worst cases. The bottom five are the worst cases. The bottom five, you know, the Horbitz-Thompson estimator doesn't care about this, it's unbiased no matter what. But you'll see a little bit of bias in some of these binary characteristics across these worst five. It's not huge, but it's there. And so what it turns out across these 22 different binary attributes is that combined has lower mean square error than the separate in almost all cases. Using the non-probability data dominates using the probability data. Dominates using the probability data only, which is not too surprising. But and the simple variance estimator actually yields pretty good confidence intervals once again. So remember, what we're doing here is we're taking an initial sample of 1,000 probability sample. We're adding into it a 4,000 non-probability sample. If the non-probability sample had all the information of the probability sample and you could fully efficiently extract that information, then you have. That information, then you have just increased your sample size by a factor of five. You went from a thousand to five thousand, right? But the relative, the effective sample size ratio is nowhere near five, it's closer to like a little more than two. So there's diminishing returns. You know, it helps to add that convenient sample, but you don't want to do it forever because you're just learning more and more about that subpopulation and you're not really learning about the unmatched part of that population. So, and once again, if you look in terms And once again, if you look in terms of effective sample size ratio, you can see how that combined estimator pretty much dominates the separate estimator. And either one dominates using the probability sample only. Okay, so let me just finish up. This dual frame approach seemed to be a pretty simple and effective method for combining probability and non-probability samples in our original application where we needed something that would work for the NOAA application. We do get a single set of weights that we can apply to any catch characteristics. That we can apply to any catch characteristics. We get some weight stability by construction, and because we're looking at rates, we get a little bit of sort of double robustness. We have a simple variance estimator and pretty good confidence intervals right out of the box with a very simple computation. And this just seems to work pretty well across a range of non-probability types, including expert judgment and the convenient samples and respondent-driven sampling, over a wide variety of simulated settings within each one of those. Each one of those. And, you know, not to oversell this, but this is like: if you want to try out a method, you should beat this one, right? So, this is kind of a, it's quite simple and pretty effective across quite a range of conditions. So, I will stop there and thank you very much for your attention.