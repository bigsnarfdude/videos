And so, the project I'm going to talk about today is about instrumental variables. And it's joint work with Suravit Saengyongam. I don't know how to pronounce his name, unfortunately, Nicholas Fister and Jonas Peters. And before I start, I want to first, I think we heard a little bit about causal inference throughout this workshop, but I would like to kind of give another quick overview of our instrumental variables because I think that'll be helpful to understand what we're kind of doing. Helpful to understand what we're kind of doing on top. And I think the easiest way to think about them is usually to think of simple examples. So here I have a very, very simple linear sum. So linear in the sense that all the nodes are linear function of their parents. So y is a linear function of u and x, x is a linear function of z and u, and u and z are just noise. And in causal inference, we're usually interested in like edge coefficients and such simple graphs. So in this case, In such simple graphs. So, in this case, we're interested in this edge coefficient between the x and y, which is 2. And the reason why we're interested is because the thinking, like explained by Lindbo, is that if you do an intervention, it corresponds to replacing the generating equation of whatever you're intervening on with that new value you're setting it to. So, in this case, if x is our treatment, not assigning to treatment means we replace x with zero, and assigning to treatment means we replace x with one. means we replace x with one and then you can see that if we replace x with one versus replacing x with zero the change to the expected value of y is two so that's our average treatment effect so it's both this edge coefficient but also the average treatment effect which is why we're interested in it and because everything is nice and linear so the first thing you try out obviously is you just do an all s so here's the population level all s uh i'm not sure whether most of you are used to thinking like that but if the all s is consistent If the all s is consistent for always for this value, the covariance of x and y divided by the variance, if the unconditional all s. And what we see here is that we kind of have this problem because if you plug in the equation for y into the second equation, then you have this 2x term. The variance of x is 3. You already see this here. Then you get a 3 times 2, so 6. 6 divided by 3 is 2, so that's good. But you actually have an extra term because you have a covariance between x and u term. Have a covariance between x and u term, so that's a plus one. So the value your unconditional s converges to is seven-thirds, meaning you're biased. And the standard way or the easiest way of solving this is just conditioning on u. So that just means basically setting ones in for the and like just ignoring these u's basically simply spoke spoken. And what you can see then is that then the covariance of x becomes two and the covariance of x and y becomes four. And the covariance of x and y becomes 4. And then it's actually converging to this value, 2. But now, instrumental variables, obviously. So, what's the thing with them? So, the idea is: what if you have unmeasured confounding? So, we cannot do that. Well, then, if we have a variable with two properties, which is that it's independent of the outcome, it doesn't affect the outcome directly. It's independent of the unmeasured confounding and it does not affect the outcome directly. Then you get, so if you turn this into properties in this graph. You turn this into properties in this graph. This means you have some z that's independent of that the covariance of u is zero, and the covariance of epsilon y is also zero, where you think of epsilon y just being whatever is in y that's not caused by x and u. So if there was an edge between z and y would pop up in epsilon y. Then you can plug this in and you can see that the covariance with whatever is left in y after I've removed this causal bit of x, this u plus epsilon y bit, that that is zero. And now this equation here is And now, this equation here is obviously only depends on observational things, right? On y, x, and z. So I can solve for the two in this case. So I get what is called an instrumental variable estimator, like what's the two stationary squares estimator, which is one special case of an instrumental variable estimator. And you can see it's actually kind of a special case of the ordinary squares estimator, because the ordinary squares estimator was the variance of x and the covariance of x and y. And in this case, you basically have these z's replacing two of the x's in the previous equation. The axis in the previous equation. Okay, now I've said a lot. So just quickly showing that this is not made up. So if you run this in R, then you will see that exactly what I claimed happens, right? So the regression of Y on X, the coefficient for X converges to this seven over three. The regression of Y on X and U, the coefficient for X actually converges to two. And then the two stagely squares, there's these two interpretations of it, right? Which is the literal two-stage squares where you run a first stage. Stage T squares where you run a first stage of x, the treatment on the instrument, then get the fitted values, and then run a regression of y on the fitted values. And that is completely equivalent to this other equation I showed, which is the covariance of y with z divided by the covariance of x and z. In this simple case, where there's one instrument and one treatment. Okay? Now, so now let's try to maybe think a little bit what does this mean and like more generally. Like more generally. So, more generally, in an arbitrary setting, you can think of this instrument often as explained with these three conditions. But I think this is a little bit cleaner way to think about it, which is you can think there's actually arguably two main conditions. The first condition is you have this covariance condition that that has to be equal to zero. And you can think of this as causal because it's implied by the instrument being independent of the unmeasured confounding and not causing y directly. But you can also for a second. But you can also, for a second, forget causal. And you can think of the two sets of scariers as an M estimator. So it's an estimator that's finding trying to find the solution to some function setting equals zero, the root set. And that's the defining equation. So covariance of y minus beta x, where it's an arbitrary beta, z equals zero. Use 260 squares is trying to find that equation. And then the causal claim basically is that your true causal function lies in the space of that, in the root set. Of that, in the root set of that equation, right? And that is then implied by these two conditions, but strictly speaking, all you want is that this is true. And if you have some very weird dependence between Z and U, technically, you could have an edge between Z and U, and this would still be true. But the simplest way of thinking about it is that for this condition to be true, it corresponds to Z being independent of Y in the graph where I delete the edge between X and Y. And that means no edge between. And that means no edge between Z and U, and no edge between Z and Y. In addition, no direct edge, right? And now, then the second condition, which is kind of again, you can forget the causality almost. This one has nothing to do with the causality. The second condition is I now have an estimator, an M estimator, that's trying to solve this function, trying to find a solution to this equation. And obviously, it can only do that if that solution is unique. Otherwise, it doesn't converge. It's not consistent for anything. It converges to a random variable. And in this case, And in this case, this is very straightforward because it again, everything is our singleton, this is a simplified assumption. But in this case, if the covariance of x and z is zero, we run into a problem because yes, my causal function, which is just tau x, lies in the space of this y minus tau x, z is equal to zero, the covariance. But so does any other linear function of x, if this covariance is zero. So this equation is true for the causal function, but it doesn't tell me actually anything for the causal function. Function, but it doesn't tell me actually anything for the causal function, right? But if the covariance of xz is unequal to zero, then this solution space suddenly only has one solution, and I assume it's the causal function, and it only depends on observation, things we can observe. So you can solve this equation, and you, by assumption, that your instrument was valid, it will be your causal parameter in this case. Okay, so you can think of it as always consisting of two conditions: you have this instrumental validity. Of two conditions. You have this instrumental validity condition, which in the graph corresponds to these red edges not existing. And actually, it doesn't strictly, but the red edges not existing is enough to imply it. And then you have a secondary condition, which is something about the relationship between Z and X. And this is a little bit more complicated than the relationship is there. In this case, it's just this edge coefficient is non-zero. But more general, it becomes a bit more complicated. Okay. And if I want to do this more non-parametrically, the basic structure of this doesn't change. The basic structure of this doesn't change in the traditional literature on how to do the two sites squares non-parametrically. Because the idea there is usually you just assume that you have some basis, which you let be like in practice, it's finite, but theoretically it could be arbitrary, like go to infinity the number of basis functions. And you hope that your causal function can be written as some, in this case, simplifying as a linear combination of this basis functions. And then instead of just requiring that the covariance of z with y minus your f. With y minus your f of x is zero, you actually say that it has to be true for any measurable function of z. And then in practice, you will impose as many constraints as you need to solve for the number of basis functions you have, because each of these equations for each eta, it imposes one constraint. So if, for example, if I have a binary instrument, there are only two such etas, the identity and the zero function. So then I can actually only identify one basis. So I can't really do non-parametric two. Really, do non-parametric two-shd squares with a binary instrument. And now, with this condition, again, so now my causal assumption is my true solutions in the space. This corresponds again to assuming these two edges don't exist. And then again, I need to assume that also that the space is unique, as in there's only one element in it. And one way you can think about it is that the kernel, so the things I can add to it without actually leaving the space, some sort of kernel has to be only. Has to be only the zero vector. And that's the second condition. And it's actually exactly equivalent to the other one. You can think of it as the first, this covariance being invertible, it's a matrix. And obviously, previously we had a number, and a number being invertible is equivalent to not being equal to zero. So nothing really has happened. It's the exact same principle. So once again, you have some equation. You know, this equation is always what the solution of your two statistics square, your two-stage squares estimator will always converge to something in this space. In this space. Then you assume that the causal bit is: I assume that my true solution, the thing I'm looking for, the causal function, lies in this space. And then the second step is: I assume that this space only contains one element, and then I can solve for it. Okay. Now, so I think also when Lin Bo talked about it, I think there were some questions as to what was the, like, are these very strong conditions? How did this help? And so, I think I like to usually give one example when I hear this question. I like to usually give one example when I hear this question where it's very plausible that something is an instrument. And that is this case where you have a treatment assignment. So you're running an experiment, but you cannot actually control that people take what you've assigned to them. So the only thing you've actually randomized is the treatment assignment. And then in this case, the treatment assignment is an extremely natural instrument because you know it's random, you controlled it. You know it, or you hope it has an effect on people actually taking the treatment. Otherwise, you're in trouble. Otherwise, you're in trouble. And it seems plausible to say that it doesn't affect the outcome except via the treatment, especially if it's like a randomized double controlled trial with double blind, right? Like with a placebo. Sorry. And so this is a perfect example of where you can then recover, use that as an instrument. And even more so, and so the reason why I'm talking a little bit about this is because I think, like Linbo mentioned, these instruments are. Like Lynn Bo mentioned, these instruments are extremely valuable in the way that they, because you can pick them, right? So the treatment you've got given, you're interested in some effect, and the instrument you can pick. So you can always tell, obviously it's easy to tell a story, but you can, the world is pretty big. So there's lots of places to look for instruments. And they do give a little bit of a more plausible promise of getting anywhere close to causality than this no unobserved confounding assumption. And one thing I've noticed, for example, here at this workshop is that I don't think. For example, here at this workshop, is that I don't think they're very prominent yet in climatology in general, and I think this is something definitely worth thinking about. And so, for some other examples that are extremely popular in econometrics, you have this one famous study where somebody used college proximity as an instrument to investigate the effect of education on wages. There's another one where somebody used the draft number for the Vietnam War draft to estimate the effect of having been at the army on future. Of having been at the army on future income. Then there is a lot of studies where the quarter of birth is used as an instrument because it seems plausible that it's kind of random, but it has an effect on all kinds of things, including like athletic success, right? And then another one, which is a huge, huge area, is this Mendelian randomization where people use these little parts of your genome that encode whether the next bit of your genome is read out or not, which are called SNPs. And again, it seems possible. And again, it seems possible it's your genome, it's set at birth. It's kind of random that this is akin to an excellent instrument. And then this has an effect on some treatment. And then you can use an instrument for this treatment. So for example, if you want to look at the effect of cigarettes on all kinds of things, this seems like a possible treatment. And again, the important thing is you can pick the instrument, which is why it's even though the assumptions are not free lunch, right? It's not easier than unmeasured confounding, arguably. Than unmeasured confounding, arguably, it's some other kind of unmeasured confounding, but you pick the instrument and you don't pick the treatment, right, at least most of the time. Um, okay, so now that I've got the setup out of the way, I actually want to dive into the actual project. So the actual idea of the project is very simple, and it's an idea that's been floating around for a while, because you'll notice something. All these assumptions that we make, they're always independence assumptions, right? So we actually say that is independent of the unmeasured function. That is independent, it does not affect the It does not affect the outcome in other ways so it's independent of this epsilon y. And all of the things I previously did didn't technically use that, they all use zero covariance assumptions. So, which is why these edges, for example, could have existed and it could have still theoretically worked. It's unlikely, but if the functional relationship is super special, the covariance might have been zero, even though they're not independent. But any argument you make for why an instrument is valid is almost always arguing for independence, right? So, even though independence is obviously a strong So, even though independence is obviously a stronger condition, so now I'm assuming y minus f of x is independent of z rather than just has zero covariance, it's kind of a free assumption because everybody's already making it anyway, right? And once again, you get the same structure as previously, right? So now I have this constraint. I hope my causal function is in this constraint. I need to argue for that with some subject matter knowledge or because I know something about the instrument. But all these instruments I've mentioned previously, people already argued for this. Mentioned previously, people already argued for this condition, actually, and not just zero covariance. And then, out of this comes an identifiability condition, which is again that there's some kernel that's only the zero vector, so that there's only one solution in this space. And this condition is a bit weirder because independence is generally a bit weirder than covariance, but basically nothing really happened. It's still a condition just like previously. And if it holds, then I can again use this independence constraint to find my causal function. To find my causal function. And so to summarize again, so this first condition is stronger than the previous conditions made in the literature, but it's stronger in a way that was already implicitly argued for almost whenever instruments are used. And as a word, the second condition is a bit less likely because think about it this way. The first condition now defines a new space. This space is a subspace of the previous space with the zero covariance condition. So obviously, it's more likely that there only be one element in this subspace, right? Be one element in this subspace, right, rather than in the larger space. Okay, so the likelihood of us having identifiability with the second condition is a bit higher. So to illustrate a simple example of that, let's return basically back to one of these linear steps. And let's assume that your instrument's effect on your X isn't just linear, but there's a linear term, and then there's an interaction term. And the interaction term is with this unmeasured confounding, which has expectation zero, so it doesn't affect the covariance. So, it doesn't affect the covariance between z and x. So, what I see here is that the covariance of x and z only depends on this alpha coefficient in front of the term in the generating equation for x. Whereas in the so I have failure for alpha equals zero. So, in practice, you might argue, well, so there's two things. So, if alpha equals zero, it fails. I don't have identifiability. My two safety squares will not converge to anything. But more importantly, as alpha goes to zero, Importantly, as alpha goes to zero, my two-stage scarce will start behaving really badly, right? Because it will have a really hard time finding that one element in my space because my loss is really, really, really flat around the minima. And then with noise and everything. And in the independence condition, I have this new identifiable. So again, this independent, the first condition is obviously true because I assumed this graph, right? So it's also true that y minus tau x is independent of z, not just zero covariance. But this independence condition is a little bit weaker. So in particular, it only starts failing. So, in particular, it only starts failing once alpha and gamma are zero. Okay. So, and again, a secondary, important secondary implication of that is that if I use this, exploit this condition well, then if alpha is small, but gamma is still quite large, it should also do better, right? So even if alpha is not equal to zero, but just small, this should have an advantage. Now, so how do we exploit this? So, obviously, the big price for this little game is that in the Gain is that independence constraints are much harder to work on. So, the way we did it is we worked with this Hubert-Schmidt independence criterion because it's very, very broad and relatively easy to compute. And the idea between this Hubert-Schmidt independence criterion is that you map like probability measures on some space into some function space, which is this reproducing kernel-Hilbert space. And because it is a reproducing kernel-Hilbert space, it has a very cheap scalar product to compute from a To compute from a computational perspective. And if you pick the right kernel, this mapping is injective. And then in this space, you can just look at the distance between the product of the distribution of X and Y together with versus the joint distribution. And if the distance is zero and my mapping is injective, I know they're independent. And in practice, obviously, I have to estimate this. This is relatively easy just from a. Just from a computational perspective, and then I can use it as an independence test as well. And so, our idea basically underpinning this project is: can we use this loss? Can we use this h sig as a loss to find this causal function? And the way we did this, so this is a little bit complicated. I will try not to go too much into it. But the basic idea is you use hstick as a loss. The problem with hstick is obviously it's quite a complicated loss. It's not analytical solution. Boss, it's not analytical solution for sure. But then the idea was to use some, it's a very machine learning project at this point, is to do stochastic gradient descent and then use all these advances with stochastic gradient descent that have occurred over the past years in machine learning. And also use a few little tricks which make it feasible. Because the problem with this HSIC loss is obviously, even if you do statistic gradient descent, it's also very, it's non-convex. We barely know. We barely know anything about it. You only really know that if it's zero, then you have independence. And if it's close to zero, it indicates independence, but we don't know anything about its shape. But there's a few tricks. So one is you can kind of start at good locations. So for example, you can start at the OLS solution, which was obviously easy to compute. So you hope you're already closer to the true minimum. And then also, because you have the ATSIC is also an independence test, right? You can let it converge and then you can look at your solution and see whether it's actually independent. Your solution and see whether it's actually independent. And that helps you. And then, if it's not, you can discard it and rerun it. So, again, very machine learning. But basically, you have to compute a few hyperparameters where there's also been advances recently in finding some good ones. And then you do a gradient descent step and you keep repeating that. And then you see if it converges. And then you look at it and see whether it's actually independent. If not, you discard it and run it again and initialize it a random start. And run it again and initialize at a random starting point. And then, in the end, once you're happy, usually because it either converges and you actually get an independent solution, or because you give up, because you have some counter, then that's your final solution. And you have to add an intercept term because it doesn't matter to the h6, the intercept, of course, because it doesn't do anything to the loss. And the loss, by the way, is just this here. This is the h6 loss, which you can see is really simple to compute. It's just four matrices and then a trace multiplied. Matrices and then a trace multiplied, and only this K and this L actually depend on the data, and it's all just evaluating the kernels at the data points. So now to illustrate this, let's take a simple example, which is again almost, oh, almost linear SEM, where we have two cases. Well, look at four cases. One is we assume the instrument is Gaussian, one is this instrument is binomial. And we have two functions: one a linear causal functions, and one's a non-linear causal function. Causal functions and one's a non-linear causal functions. And then, if you look at it and see it as a function of alpha, right? Because remember, alpha is this term with the linear term here. Then what you see is exactly what we hope for when we start this project, which is you have that in the simple case with linear and Gaussian, what you see is that as the alpha becomes reasonably large, the two stationary squares becomes as good as the h stick. The good thing about that is it means the h stick. The good thing about that is means the HSIC, even though it's so much more complicated, doesn't seem to do worse in the setting where it's relatively good for the two-station scripts, right? It's truly linear and the alpha is large. But if the alpha goes to zero, your two-station screen starts basically failing, right? Because this is on a log scale. So the mean squared error starts exploding. And the so also for a quick explanation, we have a purple and a red line. These are both HSIG. One is initialized at the true solution to see whether the convection. True solution to see whether the convexity was a problem. And the red one is initialized at the OLS solution. And you see, most of the time, this is not a problem. This is only a problem down here. But so you see, this HSIC really seems to protect you against the alpha going to zero. So if we go back to the equation, the fact that there's this interaction term that doesn't affect the expectation, but does affect the variance seems to the H6 can still just with that identify the causal function, even though there's no effect on the expectation. Even though there is no effect on the expected value of the x by z, and the binary case looks exactly similar, and then in the non-linear case, you also see a few other advantages, which is then the, it seems like this really becomes helpful to have this much stronger identifiability assumption, because it's just you're trying to identify a lot, a lot of basis functions with only one instrument, which seems to be hard with just these covariance constraints. And especially over here, you see with this binary instrument, it can actually only, it can never identify more than one of the basis functions. Identify more than one of the basis functions leading coefficient because it only imposes one covariance constraint, right? It can only impose one covariance constraint. So here you basically have failure. The two statistics case has no chance of recovering the true causal function because this identifiability condition never holds. Whereas once again, the HSIC does it just fine. And this green one, by the way, it's just the noise in the outcome to kind of have as a baseline. You can't get below that, right? So it's not a better method. It's just to give an idea of what the base variance is. Of what the base variance is if you had measured confounding. And then, yeah, and then we also derived a little bit of theory for it. In particularly, we showed that because the HSIC, even though it looks a bit scary at first, under some assumptions of the kernel, it's actually relatively well behaved, which is you have on compact sets uniform convergence of the estimator to the true ASIC. And then you can use that to show that the minimizes actually converts to the minimum. And the only And the only limitation of that consistency is like a pseudo-consistency, is obviously the non-convexity, which is yes, the minimizes do converge to this minimum, the true causal function, but you don't actually get the minimizes up here in practice, right? You get some local minimum. So to summarize, we kind of gave an overview on these gains and identifiability, because it's important to note that this kind of existed, like econometricians have known this for a while. So the main actually contribution is the show. Is the showing that how to exploit this with a bit of a machine learning approach, but also with a little bit of theory. And we also did a little bit on conditional instrumental variables and this case where you have to add an MSE loss because it's under-identified. And regarding limitations, so obviously this is a little bit computationally, can be computationally expensive. Technically, the independence is a more stringent assumption, so it's important to keep that in mind. And obviously, we know very little about. And obviously, we know very little about the finite sample behavior, and we don't even know the asymptotics, right? So, regarding infrared, the APSIC, again, it's an independence test. So, you could try to invert it, but obviously that gets a little bit harder when you're in higher dimensions. But at least you could try to get inference this way. And regarding future work, we're thinking a little bit about looking at a double robust version of it for the conditional case. And also, another interesting thing about this APSIC is if your final solution is not independent, it'll kind of tell you, right? Is not independent, it'll kind of tell you, right? It does an heteroscatasticity test on the residuals automatically because it also puts out a p-value with that final solution and it'll complain, so to say, if it's not a very good solution. And just as a last point, I don't know how much time I have left, but one and a half minutes. One and a half minutes, okay. So I just sort of, because none of this obviously had this much to do with extreme events. So just a little bit, what one thing that's nice about it is because it's set up in such a general way. If you look at the under-identified If you look at the under-identified case, which I talked a little bit about, but not properly, like you have one instrument but two outcome, two treatments, then what you can see is that this space that this condition imposes, which you two station scarce estimator is trying to find a solution in, it's actually an affine variety, right? You said that, right? A fine variety. Oh no, a fine space, a fine vector space, a fine vector space, because it's a solution space to this equation in this example. And then you can one. And then you can one, you can do plug an MSE loss in on and to find a good solution to this. But the most important thing is, how do you interpret this solution space? So, one way to think about the solution space is that this condition is also equivalent, obviously, to the square residuals being independent of Z. So that your loss of your estimator is independent of Z. And this condition is interesting because you can think of Z as some sort of environment. And then independence from Z means that your model is robust to changes in this environment. So I think. Changes in this environment. So, I think a simple example is: think of Z as hospitals. Then, if you want to devise a policy and generalize it to other hospitals, you would like it to not depend heavily on your hospitals, your solution. So you impose this independence on the laws. And that somehow is like a way to talk about causality because causality is kind of robustness to all kinds of interventions, except on the outcome itself. And in this case, it's kind of a partial robustness against not robustness. Case is kind of a partial robustness against not robustness in the statistical sense, different kind of robustness. And the neat thing about that is obviously this doesn't kind of depend on the loss you use. So you could very easily use this framework and think about it also for other losses, such as the quantile loss. And if you go back to my algorithm or our algorithm, basically the loss only matters in one step, everything else. And you could easily use the Hilbert-Schmidt independence crypto because it's so. Use the Hilbert-Schmidt independence current because it's so general, also for all other kinds of losses. So, you have to pick a different kernel because, for example, I think the quantile loss is on the upper reels, right, and not on all the reels, as opposed to the loss we used here. But then you pick a different kernel and you have this whole machinery of the Super-Schmidt independence criterion and this static gradient descent, so that you can look for solutions in these spaces as well. And yeah, that's it. So, thank you very much.