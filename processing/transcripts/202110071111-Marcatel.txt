The FPLL library to her mission lattices. Thanks. So, hello, and thank you everyone for attending to my presentation. So, I'm a PhD student with the Professor Philippe Belgenson, founded by ATOS and in collaboration with the University of Anne Verpe. So, today I'm going to talk about our extension of the FPLLL library. The FPLLL library, so to air emission analysis. So we call it FPLLLH. So the main idea is to convert the LLL on FPLLL to emission latences, so lattices over Eisenstein or Gaussian integers. So emission LLL has numerous Has numerous applications, for example, in computational number theory or cryptanalysis, and also in NEMOs, multi-inputs, multi-inputs. So this is not the first work to study emission AL. The first one was by Git Napias in 1996. But we are the very first to present a To present a full implementation relying on floating point numbers with a proved reduction instead of exact arithmetic for the quotients. So first we will see what are the objects that we will consider. So first let K be an imaginary quadratic number field and now K be its integer ring. It's integer ring. So, an algebraic lattice over OK of rank n will basically be a free OK module. So, as you may know, this is not a definition that is perfect for the general picture when K is not necessarily multi-nequalatic, etc. But since we will force OK to be We will force OK to be to be Eclipian in the next slide. This definition is enough. So for LLL we also need to install metrics. So we will rely on the air mission scatter product and also the crunch metoconalization process. The Grangemidocanization process. So, with the definitions of the BI star and the mu IG as very classically. And so now we can introduce the definition of a delta eta allisis for over OK. So we just like the So we just like the original reduction, we want the module of new IG to be bounded by eta. And we also want the Lovatz condition. So that basically says that we don't want the norm of the GI star to decrease too rapidly. So it is important to note that for the proof to work we need eta squared to be strictly lower than delta n itself strictly lower than one and thus it forces okay to be Euclidean and there exist only five imaginary quadic Euclidean rings but fortunately the Gaussian and Isenstein editors are So we wanted to rely on an existing implementation of LLL. So we have two main choices. So the first one was FP LLL, which is based on the L12 version by NBL and Stellich. And there was also the Galatread software made by Thomas Piteau and Andre. Thomas Pitot and Antoine Jou, which is based on interval arithmetic. But since FPLL is faster in most of the cases, we prefer this one. And so we extended the L2 algorithm to the case of the read integers over OK. And we kept the three main points. We kept three main points to have this extension. So the first one is to have the gram matrix computed and kept in exact representation. And so using exact arithmetic. The GSO coefficients are computed and stored in floating point numbers and with a precision linear in the lattice dimension. And then we perform the lazy reduction, which is the idea that Which is the idea that when you perform a size reduction, so you might have a very large Î¼, which in fact is bounded by the size of the input matrix, by the bit size. And so if you want to perform the self-deduction at once, you will need a very large coefficient. So the idea of the lazy. So the idea of the lazy reduction is to just reduce the helper bits of mu. And this is very natural when you use a floating point arithmetic. So it's quite simple. And then you iterate the reduction until the coefficients are indeed reduced. So our main result is the following theorem. So basically says that So basically say that if you choose a valid per delta and eta and you feed our outcalorum with a dimensional lattice of OK and using a precision that is as follows, so linear in the dimension, our algorithm will output delta analytic basis in polynomial time. So a very good point of our theorem is that we were able to have a very similar result than the L2 algorithm. And we have this exact same coefficient that depends on the rings that we work on. And that is the coefficient here for the linearity. coefficient here for the energy. And so the smaller this coefficient is, the less precision we will need. And so we will see that we need less precision for Eisenstein and Gaussian integers compared to other classical integers. So basically we took the limit of the Of the C that we saw earlier, and also the limit for the classical case. And we computed the ratio and as an algebraic lattice over, for example, the Gaussian integers is can be represented as can be represented as a lattices. As a lattices over Z of dimension 2D, we can see that since the ratio is lower than 2, we will need this less precision to perform the reduction. And so we plotted the needed precision for algebraic dimension 100. And so we see that for the Gaussian and the Eisenstein integers. The Eisenstein integers, we need less precision than the classical one. So our idea is to be able to use the architecture of FPLL at most so that when you want to try it, you don't have to have a whole new Have new things to learn. You just use FPLLL as you would classically, almost. And so we implemented new metatypes that relies on the existing metatypes, so that simulate the arithmetic of the Gaussian and the Eisenstein integers. And so now we will see some benchmarks. So we computed the ratio to the reduction of random algebraic lattices over Gaussian integers. And we just time saw when reducing with the L2 algorithm and with the L2K algorithm and for various dimensions. A variety dimension and various bit size for the integers. So we can see that we have a huge spike here. It's mainly because here I just forgot to precise. This is the results were for approved LLL. So basically we directly use the prescribed precision to perform our To perform our reduction. And so, when the dimension is sufficiently small, we can rely on double precision. But since we need less precision for equivalent dimensions, we are able to rely on double precision a bit longer than the classical one. So, this is where we have this spike. And for the rest, when the dimension grows bigger, When the dimension go bigger, we are near twice faster than the FPL L. So then we can perform the reduction directly with double, so whatever the dimension and the PressQ precision. So the reduction will not be proved, but it will be fast. So in this case, we have even better results. Particularly for the very large matrices, and then we have the wrapper mode. The wrapper mod, which is a nice feature of FPLL, which basically run a fast mode of LLL and followed by a proved mode of LLL. So it combines efficiency with probability. It's much faster than the proved error. than the proved uh the proved the error and uh an output proved a proved result even if it's not necessarily the same um the same output as if it was uh the directory the program that it's not necessarily a problem um and so this time we were approximately uh a bit lower than twice uh twice slower than twice uh twice factor for for almost all the the samples so then we tested another algorithm so we also ported the bkz algorithm to to gaussian integers and we tested it on uh falcon so the crypto analysis of the falcon public key the falcon is a finalist uh a signature scheme of uh the mist pqc standards Of the MIST PQC standards. It relies on towards two cyclometric rings, so the idea of Falcon is that you have two small polynomials and then so small with Gaussian coefficients then you compute the the The quotients of those two polynomials, modulo some steps, and finally you solve the entry relation for F and G. So here is the entry relation. And so now you have the public key that is the H polynomial and the secret key that. And the secret key that is composed with all post coefficients. And the idea is that this lattice is exactly the same as this one. And of course, the coefficients of the right-hand lattice are much larger. So the idea of the capital analysis is then to perform a lattice reduction. Reduction and fortunately, we will be able to find the coefficients of Fg. So, we tested the catalysis for the K where we have the ring Z of X over X power 64 plus 1, which is a lot smaller than the ones that are proposed. Smaller than the ones that are proposed in the competition. But since we want our result to finish, we had to choose a smaller rings. And so we were able to perform the computation of FNG twice faster using our airmission BKC. So to conclude, we propose. We presented the FPLLLH library, which implements a generalization of the L2 algorithm for the Gaussian and the identities. It is fully compatible with FPLLL and you just have to add the option minus LI and minus LG for LLL enumeration and BKZ just for negotiation. And BKZ just for the Gaussian integers. And you can find a binary at the following address. So I believe I have quite time now. Do I have the time to do a little demonstration? Yes, yes, please. You have plenty of time. Oh okay. So yeah, I think if I please so I forgot to prepare it, but if we don't have time, it's not necessarily a problem. Just have to Fortunately, I've done it sometimes. So, here is the lattice that we want to reduce. So, as you can see, it is quite ill-shaped and very hard to use. And so, we just have to run the the classical command, we just add the minus uh Z i. Is that i and we have this very nice matrix are you able to yes impressive I think it uh it's okay I said what I had to say okay so so thanks Eleatien so uh let's thank again for the talk first. Let's thank again for the talk first. Okay. And any comments, questions? Yeah, I have a question. Because then you only work with basis matrices, yeah? So you only consider sub-lattices of the standard lattice. Yes, yes, exactly. Yeah? So over the integers, you would never treat the hexagonal lattice. The hexagonal lattice, I believe this is the isotal integers. The Eisenstein integers, yeah, yeah, over the integers, yeah. So, just to give you an example, because you cannot embed it into Z2, and you would not find this lattice. But I think that these lattices might also be important. So, it's much more general if you start with the gram matrix. So, in your case, B times B transposed, if you work with columns, and then do the LLL on the gram matrix. The LLL on the gram matrix. I'm not sure I understood your question. Well, there are lattices that cannot be embedded into the standard lattice, not in the same dimension. Okay. Or are you allowing lower dimensional lattices? So you have basis coordinates of size 100, but the rank is just. 100, but the rank is just 50 or so. I don't work with those kinds of lattices. I'm only considering very simple lattices where basically when we have a dimension 100 lattices over the Gaussian integers, so you have square matrices, yeah? You're in this square matrices, I use just I just work with square momentators. But I believe the software is able to compute with yes, yes. So if it was your question, you are able to not have full rank lattices. Is it worth your question? Yeah, that's possible to do it just with non-full rank. But why are you more interested in? Why are you more interested in these in these basis matrices and not in the inner products? I believe this is a deformation from the cryptography point of view. Yeah, I think so, yeah. So these cryptographers are more interested in these basis matrices. Okay, yeah, thank you. I also have a question. Can you hear me? Can you hear me? Yes, yes. Okay, so at the beginning, you said you only consider the Euclidean field. Imagine your field Euclidean, right? So it's Gaussian or ISO 10. So the other field, other Euclidean, you have imaginary Euclidean phi Euclidean field among all the imaginary varieties. Only imaginary molecule, right? So, how about the other? So, do you mean the other rings that are Euclidean or the rings that are not Euclidean when you say the other? I mean, like Q root minus two or other, yeah, because I think if I remember correctly, you consider the field Q. Oh, I see it here. The QI. Oh, I see. So, all of them here. I see. So all of them here. How about the the real? The real incorporate field which are so Euclidean or necessarily to be imaginary? So this is much easier to have an imaginary quantity field since it has a finite number of irreversible elements. So, I was mostly interested in those elements. And mostly, since I'm working more with a cryptographic point of view, the Gaussian integers are my main focus. If you want to have finitely many units and Euclidean and everything nice. Euclidean and everything nice, but you are not so much interested in commutativity. Quaternion orders sometimes also have the property to be Euclidean. So the one of discriminant two or three, these work. Yes. So you could apply the same methods that you have also to these more general things. So I think that is quite it's quite it's quite hard to prove the theorems and to have a bounded a bounded precision. So when you look at a higher dimension of ring, it is much it is uh harder to have uh exactly this um this inequality. Uh most of the time you will uh rely on uh the time you will rely on the algebraic algebraic norm and it's not easy to bound one BI star with another and so the the proofs are impossible to perform. This is what I saw when I was When I was trying to use a bigger cyclotomic field, for example, when we use Z of X over X over 4 plus 1, which is still very useful in cryptography. But I was not able to perform the reduction as I was. The reduction as I was for the question integers If I can add a comment, so maybe I mean you can say that the proof is far from trivial and it's a little bit more tricky than the standard case. And so it could be done maybe for also some even cyclotomic fields. But I do agree with Gabby that the case of quaternion orders will be interesting, but also some work. In fact, in theory, we should be able to deal with all different cases. It is just that indeed the focus was really maybe more on a cryptographic setting, but the goal was to have a tool which could be used for general setting. I think that even Ugate Napias already did it for quaternion orders. Yes. Yeah. But not for FPL. Not in Point. So, in fact, tomorrow Thomas Pito will present another approach to this, which is easier to deal with the Quaternum case. Okay, are there any other comments? So I have questions from Paul, but I will just answer to Paul. I have a silly question. What kind of sampling did you use to generate your mattresses, your test matrices? Even restricting to those basis matrix. Since we heard that we had some kind of We had some kind of nice bias. No, no, no, no. The idea was just to have a simple generation. So I choose to have every coefficient pick at random between zero and the certain bound. So for example, to support sixteen. And I picked every element. So every element of the samples are The samples are sample matrices are positive, but it allows also to be sure that there is a reduction to perform. Sometimes you might have some lattices that are naturally reduced when you sample them. And using this form you're sure that you have at least some work to do. Some work to do. I'm not sure I understood. So, did you use some kind of rejection method or just multiplying unipotons as before? Oh, no, no, no. These are not inversible matrices. These are just every so independent unit for me within. Okay. Yes, I don't need to compliment to to to But to complement to link with the the previous talks, I mean we we do not have a robust random matrix generation which is integrated in the grant, but it's something that we will do. So it's why it was very interesting to have this talk. So it looks like we've come to the end of comments and questions. So let's thank all the speakers and Etienne in particular for this afternoon's talks. And tomorrow we start 