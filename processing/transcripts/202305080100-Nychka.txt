It's that one. Absolutely. Yeah. So a lot of familiar facing for me, this is really fun. I feel a little bit embarrassed in giving this because most of these topics, there are people in the room that know much more about them than I do. So the fact that I'm giving you a course, I think through this week, I think through this week, actually, you'll be giving me a course. Maybe the reason I was asked to do this is I'm the only one who's foolish enough who would actually undertake this and speak to all these experts about things I don't quite understand. But anyway, I'm going to go through some spatial statistics, the way Shotir and I think about it. I asked. I asked Shotir if he wanted to do part of the presentation. And anybody that knows Shotir knows that he's actually very diplomatic and always sort of very, very sort of smooth about things. But this was one time when he just said, no, no, you present. So I'll go ahead and go through this. So let's see here. Let me just figure out. Figure out great. So, four topics: a little bit about Gaussian processes, and then the challenge that I see are in two ways. Making these Gaussian process models work for large data. And as part of that, we'll talk about two different strategies here. One, basically, focusing on the covariances. Focusing on the covariances and the other focusing more on the precision of it and basis function expansions. And then I want to talk a little bit about non-Gaussian things, which is totally appropriate for this workshop. When we're thinking about extremes, things we've left a Gaussian world and we have much more complicated distributions. And so I hope some of the remarks here will be helpful. Will be helpful. So there's this great, great quote that I like by Waldo Tobler, a geographer who I think spent a long time at Santa Barbara. Everything is related to everything else, but near things are more related than distant things, which sort of brings us into spatial statistics and why we're thinking about trying to use things that are close to predict locations that are also. Locations that are also close by. I added this other phrase to this, but the distant things become less important given the near things. And the emphasis here is on the given. As I tell my students, whenever you see that vertical line, you pronounce it as given. It's conditional. There's so much that we do in spatial statistics where we're conditioning. And in fact, also that this phrase, distant things become less important given the near things. This is also the screening effect in spatial statistics, which says that even if you have very highly correlated observations, if you condition on the observations that are local, those highly correlated ones at a distance no longer matter. It's that screening effect that we're seeing. We're seeing. So it turns out I won't be talking about too much throughout the talk, but in the background, that's really what makes things work. Okay, so I want to give an example, and I'll sort of use this as a way just to sort of explain the basic points here. But what I wanted to do was immediately throw in this sort of non-linear twist and this whole thing. Twist and this whole theme that really modern spatial statistics, we rarely have things in closed form, and basically everything is sort of evaluated in a Monte Carlo kind of computational way. So we have some data here. What we would like to do is fit a smooth curve to this, and we would like to make, say, an inference for the maximum of this curve. Not just to predict the function, but we want to say something about. function but we want to say something about where the function is is largest. Okay the way we're going to get into that is we write down a model for that unknown function. And it's at School of Mines our STAT department has partnered with the computer science department on a master's. And so the students take a statistical learning core. Statistical learning core and also a machine learning core. And it seems like over and over again, I'm sort of running interference to say, you know, machine learning is great for large data, but when you have smaller data sets or you want to actually talk about the uncertainty in them, you need a model. And while statisticians are a little bit not as savvy about very highly efficient algorithms, we do. Efficient algorithms, we do know how to model data. And I find that computer science is short on that. And so the point of this is that there's sort of a collaboration there. But just to kick things off here, I'm going to talk about a model. So we're trying to estimate this unknown curve. We're assuming that it's a Gaussian process just to make things easier. Process just to make things easy. We'll assume it's mean zero. We know the covariance function. I've put up a few examples here. Let's just see. So D here is just the pairwise distance between these two locations. Oh, great. Oh, that's nice. Okay. So D is the pairwise distance scaled by this range parameter alpha. Just some some uh some very uh common common forms of this um all of these if you plot them what you'll see is that as distance increases the uh covariance decreases um so um is it not recording you are not sure your windows so that you have to share your screen oh oh i see Oh, oh, I see. Okay. Did I hit the wrong button? Was that someone? Okay. Is that good? All right. Okay. This last covariance function is a little bit exotic. It's the Wendler. It's the Wendelum and it has compact support. So it's still going to be one when the distance is zero, but it's going to fall off. It actually looks pretty close to a Gaussian shape, but it's actually identically zero when D is one. You can make up lots of kinds of curves that look like this. The magic of the windlin is that it's also positive definite, which is sort of Also, positive definite, which is sort of an important feature here. Okay, so a whole group of these. What do they look like? So what I did was I just generated some realizations here, and you can see that this is the exponential. I put up an element of the, let's see here, this is the blue here is the Wendlin. Here is the Wendlin, and this green is the matern. This matern realization is actually the true function underlying my example. So just a little bit of this. Going back to my diatribe against machine learning, where they don't have a model. So, you know, they would throw an algorithm at a particular data set, and you'd say, so, what are you assuming? And you'd say, so what are you assuming about this function? And then they would start to talk about, well, there's all these standard test data sets that they've tried this out on and it works. But, you know, the advantage of the statistical approach is that we actually have a model in mind behind this. So it turns out to be powerful. Okay, so the next part of the model is we have this idea of a smooth. Have this idea of a smooth function. We're going to add some white noise to it. This is often called measurement error, although, really, truth to tell, it's really all the stuff that is unrelated to the smooth function. It's all the stuff that is not predictable based on things being close. And we lump all that into this thing called the error. Okay, really, what I wanted to emphasize. Really, what I wanted to emphasize here, as I said, this is sort of an overview course. It's a little bit of Doug and Shotier's guide to our view of spatial statistics. When I teach this, I really want my students to understand that there's this big three in terms of the spatial model. And so we have a correlation range. And you can sort of, when you look at these examples here, you can sort of Look at these examples here. You can sort of get a feeling for the distance that things are correlated versus being independent. So, for example, we could say that this green curve, by the time we're over here, we're probably close to being independent of values over here. So that's our alpha. We also have just a variance in terms of this function, and then we also have. function and then we also have as i said this catch-all this measurement being added on that also has a a variance um so we we have those those three things um the main point i want to make is that if someone has a model and they have not identified these um they're sweeping something under the rug that is um as far as i know any sort of Any sort of spatial approach, any kind of curve-fitting approach has to address these three parameters in some way. It's sort of intrinsic to the problem. Brings us back to, say, like a pure machine learning approach. We would ask the question, so how are you finding these? And exactly, what are the big three in your problem? Now, there are more complicated covariance functions, and there can be additional. And there can be additional parameters besides these, but I really just wanted to extract these as sort of the essence of the problem. Okay, the goal here, again, is we want to say something about this underlying smooth function, but in the process, we're going to have to estimate these parameters. And I should say that in terms of just additive models, when we're Just additive models when we're observing a Gaussian process directly, this is fairly straightforward. I would say this is pretty much a solved problem for small data sets. When we're talking about non-Gaussian problems, which I would say is more germane to this week that we're here, this is difficult. And I'll show a little bit of that at the end. And as I said, there are people in the room that know a lot. There are people in the room that know a lot more about this than I do. Okay, so to get this off the ground, we're sort of on our way to sort of trying to actually fit this curve and make inference about the maxima. We have our data y. It's the smooth function f plus e. So those are vectors now. And I think for this particular application, I have like maybe 80 ops. Maybe 80 observations, or maybe close to 100. Okay, this is really useful notation. So remember, the bar we read is given. So our assumptions here are building this up in a conditional format. And this is a very simple start to this. To this, it's probably the simplest version of a hierarchical model. And I suspect we'll also see a lot more complicated hierarchical models this week, too. So the way you think about a hierarchical model is you describe the data knowing the true function. And often that turns out to be a very nice dialogue with a science. Nice dialogue with a scientist because typically a scientist or an engineer they know if they actually knew the true process what their instrument would be finding. But then below that, there's the process is actually unknown. And so at that point, you want to put a model on that. And I should say if we were doing a Bayesian analysis here, we would also have another level where we'd be putting priors. We're be putting priors on my big three: on the correlation range, the marginal variance of the process, and also the white noise. I'll just mention this briefly because it's just really fun. If you're bored sometime this week, do a web search on red paperclip. And so there's a And so there's a, this is a little bit of an old story, but there was a Canadian, a young man whose dad told him, you know, you're just sort of floundering here. You're done with college. You need to sort of figure out what to do with your life. And so he took this to heart and he looked in his wallet and he saw that in his wallet was a red paper clip. I guess no, no. I guess no money, no dollars, no Euros. So he took this red paper clip and he went onto the web and he traded it for a pen. And then with the pen, he traded it for something else. And eventually, after about more than a dozen trades, and you can look this on the web, his very last trade, he actually ended up with a house, a house. Up with a house, a house in Saskatchewan. And so my point is that conditioning is very important. And in terms of this process of going from a paperclip to a house, if you understand how each of the trades work, it all makes sense. But if I say, well, red paperclip and eventually I'm going to get a house, it's much harder to understand. Harder to understand. So that's about the most complicated hierarchical model I can think of, and that this is the easiest one. The second to the last trade, I think, was a role in a movie. There was sort of a B-grade movie that he was eventually able to trade to, and then he traded that movie role for the house in Saskatchewan. Anyway, so red paper clip, go ahead and look that up. All right. Okay, what I want to do is write down the Krieging formulas. And if this was a real short course, we'd go through all the details of the linear algebra here. But I really just want to make a simple point and get to really the essence of this. So, again, our job is to try to predict this curve. Try to predict this curve. The most interesting places to predict it are places where we don't observe it. And there's the formula. It's actually a coefficient times the covariance function. And we actually create the covariance function. You know, we could recognize this as a set of n basis functions where these covariances are basically referenced at the locations. Referenced at the locations, but then this is the free parameter which allows us to predict. So we have basis functions, times coefficients. Now, from a geostatistics point of view, people get all excited about this and they say, well, this is the best linear unbiased estimate of F. And I'm not as keen on that because at some point we're going to have to do. Point, we're going to have to do prediction errors or talk about inference, and we're going to need more than just sort of mumbling things about best linear unbiased. So, what I like to think about is that this is actually the formula when we assume everything in sight is multivariate normal. And this is really just the conditional of a multivariate normal distribution. Okay, so in this second part here, I'm telling you about the Telling you about the coefficients. And so, what we do is we find the covariance among the data, which is including the process here and the white noise, the observations. Notice that I have my big three buried in here. We're assuming these are fixed now that we know them, but of course, eventually we're going to have to estimate them. So, two things. First of all, First of all, the big three are intrinsic to this. The other thing is that, you know, I'm writing this in fairly dehydrated form, but this is a big matrix. This is a vector we're solving for. This is our observed vector. You can imagine that as the number of locations gets large, these matrices get very big. And so Very big. And so, you know, I would say, in terms of sort of the current challenges in spatial statistics, a lot of the most interesting problems are with large data sets, remotely sensed data sets, data sets coming off the internet. That's a large matrix, and we have to solve that linear system. And so that's a challenge. And I'll be talking about sort of the strategy. I'll be talking about sort of the strategies for doing that. This is a little bit of a digression here. I came up into statistics with splines and thinking about smoothers. And it turns out that if we're simply trying to go from our observations to the predicted function at our observed points, this whole Krieging just turns out. This whole Krieging just turns out to be a smoother, and so this is the actual matrix form of that. And what's important about this is that when we think about the smoother, this ratio of variances turns out to be the key quantity here. It turns out that the individual variance between the noise and the Gaussian process are not important by themselves. Important by themselves as a smoother. It's their ratio. Lambda's a little bit backwards. It's the noise to signal ratio. I mean, usually in engineering, you think about the signal to noise ratio, but lambda is this ratio of these two different components. Okay, so that's creaking. So that's my single slide. We've covered almost. Single slide. We've covered almost half a course in spatial statistics there. And so there's our fit to the data. And what I've done in orange is also indicate the true curve here. I'm doing a lot of cheating here. The way I typically describe this to my students is that this is an oracle estimate where we've had some divine inspiration. Had some divine inspiration where we actually know what the true covariance parameters are. So the only thing that is varying here is we're applying this to this random data. In the next part of this, I want to talk about how we estimate those covariance parameters. Okay, so there it is. Nice curve, looks pretty accurate. Okay. Accurate. Okay, so as I said, geostatistics starts with deriving this estimate as being the best linear unbiased estimate. We need to add something more to it to actually do inference. And as I said, I'm assuming everything in sight is multivariate normal. And because of that, and I'm not going to go through the details of this, we can actually work out the conditional distribution. The conditional distribution of the function given the data. And what I've done is done five draws from that distribution. The interpretation of each of these draws, let me see. I'm going to, you know, sometimes, well, I don't want to mess with this. I don't want to mess with this. On my laptop, I can often zoom in on the PDF to show detail. But anyway, I think you can see that they're a little bit different, especially up at the top here. The interpretation of these curves is actually very convenient. All of these curves are equally plausible given the data. And so, in terms of setting confidence intervals or setting inferences, we simply need to generate a Is we simply need to generate a lot of these curves and we can take quantiles of them. And in particular, we can, let's see, is this coming? It's a little bit hard to see here. In particular, instead of five, I generated, I think I went a little bit wild on this. I think I generated 10,000 of these root realizations. These realizations. Out of the 10,000 for each of these points, I took the inner 95% range of them. And that's what I'm reporting here. So these are pointwise confidence intervals, or if we can use these as a 95% level inference for the curve. So very nice. So, very nice envelope, very easy to do. Notice that we're already getting into this idea where we're doing Monte Carlo. We're not really writing down exact formulas for things, simply able to sample. In this particular case, sample a lot. I mean, I was sampling 10,000, and I think that just takes a few seconds on my laptop. So, there it is. If we wanted, we could actually modify. If we wanted, we could actually modify these bands so that they were simultaneous coverage instead of pointwise 95, but there it is. So very, very convenient. The thing that I wanted to emphasize here, though, is we can also do an inference for the maximum. And so this is a non-linear problem. The reason I wanted to showcase this is that I Is that I'm not really aware of any closed forms for this. So at this point, we really need to resort to Monte Carlo to compute this solution. So the idea here is that I want to find the maximum of this. I want to estimate the maximum of this function. And I also want to estimate where the maximum occurs. So there's actually two points. Curves. So there's actually two points here: the X and the Y. The way you do this is actually very simple. I take my 10,000 curves. For each of those curves, I find where it has the maximum, where the maximum is located. And it may be a little bit hard to see, but all these small points here are my 10,000 different results of that. So you can see that there's actually some down here. So you can see that there's actually some down here where these are very low maximum and they're centered about in the middle. But this is a 95% confidence region based on a density estimate of these points. And we can see that the orange curve is the true one, and it's contained. Both the maximum and its location are contained. Are contained within this envelope. This 95% envelope for this, as I said, is something I don't know how to do except to do Monte Carlo. I wanted to give a 1D example because it's just simple to do. I was computing this on the plane, sort of easy to do too. But what's really valuable about this is that if we think about using Is that if we think about using these methods to fit a spatial surface, often we want to interrogate that surface just by creating contours. For example, we can estimate rainfall over a certain region. We can ask the question, where do we see rainfall that's above a certain amount, nine inches, ten inches, ten centimeters? To ask that question, To ask that question and to do the inference for it, we have to do Monte Carlo. And that inference for those contour lines is actually very similar to the kind of example that I did here. Okay. Let's see here. Gosh. So how much time do I have, Shotir? Maybe technically 15 minutes, but you can take another 20 minutes. Okay, okay. Okay, so let me backtrack here. The example that I went through was, as I would say, an Oracle example. Okay. Okay. Let me go through this and then I'll open it up for questions since we have a little bit more. So the other thing I wanted to emphasize. Other thing I wanted to emphasize in this talk is this my feeling that in working on a spatial problem, there's sort of these two separate parts to it. So there's the estimating the curve and doing the draws of the conditional samples that I just showed you, and then dealing with these covariance parameters, dealing with the big three. With the big three. And what I'm my impression is for large data, for nonlinear problems, this often ends up being separate. Okay, so how do we do this? We start with our joint distribution. So we have our observed data, we have our Gaussian process, the The white noise here is just the difference between these. So, in some sense, we're including those. What I want to do is integrate out this, integrate out the Gaussian process. So, basically, I've concentrated our distribution around what we see, and then we have the parameters left over. Now, if we wanted to do a Bayesian analysis, we would Analysis, we would then multiply this by a prior on these three parameters. But for the moment, I just want to focus on maximum likelihood. So what we have here is we've taken a problem where we have our observed data and we say that this is following a likelihood based on a multivariate normal. And notice that the three covariance parameters, surprise, surprise, they end up Surprise, surprise, they end up in the covariance in the covariance matrix. This is the likelihood. The actual form of it is not too important, except I want to point out we have an inverse of the covariance matrix in this first term, and then we have a determinant of the covariance in the second part. This is a lot more work to compute than simply doing the spatial prediction. What I showed you in terms of the spatial prediction, I purposely wrote that as a linear system. When we go to large data sets, that simply becomes solving a large linear system, which is a well-studied problem in numerical analysis or in computational linear algebra. For those of you that do computations, you realize that this term here with the inverse, you don't really ever invert this matrix. You simply solve a linear system. So that's fine. That'll go very fast. The deal breaker here is this determinant. There's really no simple way to get around actually getting this determinant, and it turns out to be a fairly expensive. Turns out to be a fairly expensive computation. So, the bottom line here is that if we want to do maximum likelihood to estimate the parameters, or if we want to use this likelihood as the start of a Bayesian calculation, we've bought into finding that determinant. For large sample sizes, that's finding the determinant of a large matrix. Yeah, so. So there you have it. Now, it turns out that there's a trick here where you can take this likelihood and concentrate it over just two parameters, the range parameter here, the alpha. And you can actually work out the MLE in closed form for sigma squared. And then you're just left with the ratio of these two. With the ratio of these two in the likelihood. And I only bring that up because there's our noise to signal ratio coming through. So there's something very intrinsic about this spatial problem, this curve fitting problem, that is directly connected to what engineers think about, signal-to-noise ratio. It also explains why this parameter lambda is hard to find, because basically we're trying to separate these two components. Separate these two components when we're only observing the sum of them. Okay, so for small problems, as I said, this is sort of a solved thing within the fields package, which I work on. There's simply just a simple one-liner where we're giving it the locations, the data. The covariance that we're looking at is actually. Covariance that we're looking at is actually a member of the matern family, smoothness two and a half. And so this single fit will sort of do the whole fit for you, and you can get the results. You can take this object that's returned and subsequently do the conditional simulation for it. And besides the fields package, there's other good software to do this. And I want to mention. And I want to mention SP Bayes, which is a nice Bayesian version of this, which runs very fast. And then also there's other packages that can handle larger data. Okay. Let me just ask if there's any questions at this point. Anything at all, or any comments? Any comments? Yeah. In your simulation of the maxima location of the value, your best guess would prioritize from truth. Is that a natural consequence of this approach? I think I'd like to think that that's just sampling variation for this particular example. So my plane flight wasn't long enough. My plane flight wasn't long enough to actually do a Monte Carlo check of this, but I think that's the case. Is that this is just one data set? That's where we ended up. Yeah, good point there. Yeah, it's sort of interesting how that ellipse is a little bit skewed relative to the center of the. To the center of the blue line. Yeah. Yeah. Anyway, good, good question. Yeah, yeah. And Anthony? Let me do the simulation soon. You're fixing the big three, aren't you? Yes, I am. This is an Oracle. Yeah. So do you know how much you have to and Lambda is between the ball science? Yes, that's right. Do you know to what extent, you know, if you Extent, you know, if you allowed for the variation of trick 3, how different do you think that over? Another good question. I don't know. I don't know. Yeah. Yeah. So the yeah, all of that's possible. You know, you can, you can, so the So, the way I have this set up is these are two separate steps. So, you could imagine doing a Bayesian analysis for the big three, doing draws of the posterior for alpha, sigma, and tau, and then each of those then doing these predictive things. And if you wanted to go completely wild, you could do 10,000 for every draw of those. For every draw of those. But yes, I don't know how the uncertainty would increase here. Yeah, yeah. Yeah, Michael? I'm going to ask a question that reflects how little I know. It seems that people use Gaussian processes to find spatial screens and things like temperatures. How well is that going to work for extreme temperatures where we know that the extreme temperature? Whether we know that the speed temperatures are not, um is the spatial aspect dependent on that kind of thing or not, or do we just hold our nose and go and do it? Yeah, yeah. So I there's there's met there's people in the room that know a lot more than I do, but but but let me just make the yeah, yeah, let me let me let me make Yeah, let me make the basic statement that the role of the Gaussian process here is not to characterize, say, the extreme temperature. What its job is to say two locations that are close, how similar are those extremal distributions? So it's describing how the distribution would vary over space. And it's not to say Over space. And it's not to say that that distribution is normal. And we could say, well, this distribution over this particular region is about the same. We see the same kind of extreme values. Or we could say it's really different. And that's sort of how we would be modeling that. Am I making sense at all, Michael? Yeah. I would add that this sort of ties into a So you would have to be able to think about that. When you have two stations that are near each other, they're both likely to have a hot day at the same time. Right. You know, that obviously you can farm and find your brother with listening. But for heat, it could be very large. Yeah. Yeah. Yeah. Yeah, yeah, Francis. I think I'll follow up on the same question. So, this ref, as you show here, if you think of the heat build of the Pacific Northwest in June 2021, it's a big spatial extent and very high elevated temperatures. So, one challenge that's associated with that event is simply the define. With that event, is simply to define what it was. What's the spatial extent of the region where we consider temperatures to be extreme? How does that correspond with our understanding of physics of frequency? So would you use a technique like the one that you're showing on the screen as a means perhaps for simply defining the But subsequently, we're going to use an animal's choosing approach to doing extreme death definition. Yeah. Yeah. So using that definition. I feel like this is a trick question, Francis, that there's either a right answer, and I'm not sure if it's yes or no, but yeah, let me just point out something about this sort of non-linear. About this sort of non-linear sort of way of thinking about stuff. So here's the temperature distribution over Western Canada. And your heat dome might be defined as everything above a certain level, maybe. So, you know, the kind of methods that we're talking about would be. Would be useful to find that range and attach uncertainties to it. So whether that's physically appropriate or not, I'm not sure, but that would be the way to do that. So let's see here. So I'm showing you inference for the maximum. This would be sort of inference for the Sort of inference for the range of a particular level set. And so the only point I want to make here is that we're estimating surfaces and we can manipulate them and interrogate them in different ways. We can ask non-linear questions and be able to get uncertainties to them. Yeah. Based on your expression, I think I should have answered no. I don't know. I don't know. You're looking doubtful here. Yeah. Okay. Let me go through this fairly quickly. So as I mentioned, I want to make a transition now to large data. And whenever people, the first thing they say about doing standard spatial Doing standard spatial statistics with large data sets. They mumble something now about Vecchia. And so, what this is, is it's a way of working with the covariance matrix and reducing the size of that so that it's computationally manageable. And let me just walk you through this. So, if this is the first time you're If this is the first time you're seeing this, it's a little bit mysterious. But again, we see this vertical line here, which we're saying is given. And so this is the joint probability of our entire data set. And we can write that as a product of conditional probabilities, where this would be the first observation. The first observation given all the remaining ones, and then times the second observation given all the remaining ones. And it turns out this is an exact expression. What we're doing here is we're taking a joint distribution, which can be sort of computationally expensive to find or just complicated, and we're breaking it up into a sequence of conditional ones. I guess there's an analogy with the red paper clip here. Analogy with the red paper clip here, too. Okay, so the trick here and the Vecchi approximation at this high level is actually very simple. We're saying, okay, so here's a particular observation, and we're going to condition on all the ones that are remaining. So if I is 10,000, we're actually saying we're going to write down the conditional distribution of one up through 10,000. Through 10,000 minus one. So the question I would ask, given I tend to be fairly lazy and are always looking for shortcuts, is, do I have to really use all 10,000 of these? I mean, certainly I can get by with fewer. And that's exactly what the Vecchia approximation is. It's to say, well, we want a condition on this particular element. Particular element, and we're only going to use a subset of these, and that's how this whole thing works. So we're going to look at a product across all our data, but we're only going to be conditioning on subsets. Since we're working in a multivariate normal world here, all of these conditional distributions are also normals. If the subset is small, then all the matrix is small. Subset is small, then all the matrices are small. And the, you know, I should say that there are several different methods that sort of come off of this basic idea. And Abby has some nice work with nearest neighbor Gaussian processes. Matthias Katfus and Joe Guinness have sort of also worked on this. There's a There's many, many sort of variations on this that are very useful. But the basic idea is that we're conditioning on a small group of observations. And so when we think about this thing, this is still going to be a multivariate normal distribution, but the matrices are going to be small. And that And what that ends up with is that when we put all these together, we're actually getting an expression that is easy to work with. Okay. So just a little bit of history here. And I guess I put in this history because Vecchia was actually at Colorado School of Mines. So it's our pride that the whole sort of source of this was initially a faculty at our. Initially, a faculty at our university. But then Michael Stein really realized the importance of this. And what he did was he changed this from conditioning on each observation separately to groups of observations. And then finally, as I mentioned, Joe, Guinness, and Matias generalized this to sort of general formats and following on Stein's work. And they also pointed out that a lot of good. I also pointed out that a lot of good ideas, for example, Avi's work in terms of Gaussian process nearest neighbor, are actually a specific case of this. Okay, so that's the covariance approach. It basically is just taking your current covariance model and making it and approximating it. Approximating it. Okay, here is another approach. And let me focus. I want to get to some other stuff here and also have time for discussion. Let me focus in on just this expression here. So, again, we're confronted by large data. We're looking at that likelihood, and we see we have to find this determinant. To find this determinant nominally of a very large matrix, and we're trying to find shortcuts to that. We're trying to find shortcuts though, but in a way where we still retain this whole nice sort of Gaussian modeling, and we're actually modeling processes that are reasonable, that we actually expect might be good models for our data. The way we do that is we find a set of basic. We find a set of basis functions and we multiply them times coefficients, and that is the model for our function. The key here is that these coefficients now are going to inherit all the randomness. The coefficients C are going to be multivariate normal, and they will then induce F of S to be also multivariate normal. Multivariate normal. The whole key here, though, is C only has length L, where L we're assuming is going to be much smaller than N. So that's the basic idea here. If we look at the covariance of the data, it's going to be n by n. So let's just say n is 10,000. We'll suppose we're going to be using a thousand basis function. A thousand basis functions. And so, this fixed-rank Krieging approach will basically then end up with a thousand by thousand covariance matrix. And then we have these H matrices based on the basis functions, which sort of can convert this to the full size. Notice that we also have some error in here, our tau squared. And this is usually important to include anyway. Important to include anyway, but it also has the role of being a regularizer here: that this full covariance matrix is now going to have full rank because of that tau squared times the identity. So there are many variations on this. And again, I want to mention Andrew's fixed rank Krieging package, which sort of does this very, very, very nicely. Nicely. If you're in the fixed-rank Krieging world, there's this wonderful matrix identity called the Sherman-Morrison Woodbury formula. And what that allows us to do is to basically all the calculations where we're confronted with this large n by n matrix, we can use this identity to actually convert those in terms of the linear algebra to the smaller elements. algebra to the smaller L by L one. And I would say that's the first big deal in terms of fixed rank Krieging is doing this finessing, say, from a 10,000 by 10,000 matrix to something that's just 1,000 by 1,000. Okay. All right. So that's the first step. The The other variation on fixed-rank Krieging is to do consider basis functions that form a multi-resolution. And this has two advantages. One is sort of technical in that I find that it gives better approximations to ordinary covariances. So, one goal here of fixed-rank Krieging is to try to do the exact process. The exact problem, but have an accurate enough approximation where you don't really see the difference. This multi-resolution has that effect. And so, what I'm doing here is putting down some basis functions at one scale and then making half the scale here and showing 16 of these basis functions. So, you can imagine these are our fixed functions. These are all getting hit by this random coefficient. By this random coefficient that is multivariate normal, and by choosing the right covariance matrix for these coefficients, we can actually generate these. And based on my earlier speech, too, we still have the big three. You're not going to get around that. There's still going to be a white noise component, and then these coefficients now, which we're modeling as multivariate. Modeling as multivariate normal, have the variance and the correlation range out there. Okay, so that's the way that works. There's another version of this called predictive process, and I don't want to spend too much time on this. These slides will be posted, and you can look at them. I just want to say that the predictive process is very clever because it goes back. Because it goes back to our very original form here. The easy way to think about predictive process. Boy, I've talked about a lot of stuff here. Let's see here. Yeah, yeah. Okay. So remember that I'm basically a lazy person and always looking for shortcuts. Here's our original Krieging estimate: it's the covariance function. It's the covariance function where we're creating basis functions now. And we can now interpret these basis functions as thinking, this sort of looks like fixed-ranked Krieging, doesn't it? The predictive process just comes about saying, you know, if I have 10,000 basis functions here, do I really need to do the sum over these 10,000? Can I get by with, say, maybe 1,000 or maybe 50 if there's not a lot of structure? 50, if there's not a lot of structure in the function? And the answer is yes. And the method is called predictive processes. So it's just a way of cutting down the size of this sum. And the magic of this is that in cutting down that sum, we're using all of these linear algebra tricks to reduce the amount of computation. Okay. I want to get to non-Gaussian observations. I want to get to non-Gaussian observations. And again, there are many people in the room that know a lot more about this. What I want to do is basically give you enough to launch you to be able to follow some of the more complicated stuff connected with INLA. And you'll see that my understanding of this, we're really starting with very. This, we're really starting with very simple things. So I want to motivate this by an example. What I've done is throughout these 70 years, I've gone to our National Weather Service and collected all of the locations of tornadoes that are on the Fujita scale three or above. three or above um if you if you experience an f3 or higher torn tornado um you're you're you're very lucky um these these winds are above um yeah i apologize i'm not going to do this in metric they're above 100 miles an hour and just just looking at things um if the winds are above 60 miles an hour um you Miles an hour, you cannot walk. The winds are so strong that you cannot stand up against the winds. So you can imagine when you get up to an F3 tornado, this is something where you can't make any progress. You're just laying on the ground. An F5 tornado, which is the highest, those are the kind where the winds are so strong they're picking up cars, throwing them into houses and things. And things. And so, anyway, we're interested. I figured that this would be a good example of extremes. What I've done here to simplify the problem is I've created a grid over the Midwest of the United States, and I've simply counted the number of tornadoes that are in this record, F3, and higher. And higher. And so the color scale here, it's a little bit hard to read. I apologize for that, sort of washed out. But really, the whole point here is what we would like to do is estimate the Poisson intensity over this region. So we're at a point now where our data is no longer normal. The observations are Poisson counts. And in fact, the reason that I'm focusing in on That I'm focusing in on this particular area is beyond this, where you see just white, these are counts that are zero. So you really can't finesse this and say, well, you know, the Poisson for large intensities is essentially normal. We're really confronted with the fact that we have non-Gaussian data. The model here is very simple, however. simple however what i'm what i'm saying is that the the log of of the intensity is going to be a gaussian process with um here's our our marginal variance and our correlation range i'm also including a fixed linear modeling in here um but there there they are now you might ask well where is the tau squared the tau squared is actually coming in now because these are poissons Coming in now because these are Poisson counts. And so that's actually intrinsic to the Poisson random variable. If you have a Poisson random variable where the expected counts is 10, then the variance is also 10. So you've sort of built in that randomness there, you know, as part of the Poisson assumption. Okay. It turns out that my analysis Out that my analysis, while correct, is in some sense incorrect. So, this is a little bit like Francis was asking me the yes-no question. And you might think about what am I doing wrong here? All the statistics is correct. There's something a little bit more foundational that I'm missing. Okay, so as I said, what I wanted to do was to provide. What I wanted to do was to provide a little bit of an introduction to this whole world of INLA and approximations for non-Gaussian distributions. And the underlying idea here is that we want to try to avoid doing a fully Bayes calculation using Markov-Chainmont-Monte Carlo. These take too long, and if you ever want some arguments for that, just read these very Arguments for that, just read these very nice papers by Harvard Rue and his group and Finn, where they explain how, you know, for these large problems, doing the MCMC is really not as time effective as doing these approximations. Okay, so what do we have here? The F here is our log intensity of the Poisson. Intensity of the Poisson distribution over space, and we have our parameters in here. The beta here now is new, that this is our regression parameter. It's just a linear model in terms of longitude and latitude and elevation. The actual form of these expressions is not too important, but really, what I want to say is we can. Really, what I want to say is we can write down the joint density function here. This is the Poisson part that describes the connection between the intensities and the data. And then what we have over here is our whole spatial model for the intensities. And the main point you might recognize here is that this is a multivariate normal because this is a Gaussian process. And here's our inverse covariance matrix. Our inverse covariance matrix in the midst of that. Okay, so what we would like to do is simply do maximum likelihood to try to estimate these parameters. And in order to do that, we have to integrate out our Gaussian process. The way that works sort of in In terms of the details, we basically have to write out this functional form. f is a vector which we then want to integrate over all the all the dimensions of f so what I'm describing here is a multi-dimensional integral to my knowledge this has no no closed form and not only that numeric And not only that, numerically, it would be difficult to do because remember, where this is a spatial problem where F in this, in my tornado example, we have 900 locations. For a sort of other cases, we could think about having several thousand locations. It's really not feasible numerically to do these integrals, even as a Even as a computation. So, what we're gonna do is do a different kind of strategy where we're gonna approximate this expression and come up with an approximate expression where we can actually do the integral. And that's the basic trick here: we're taking this density, which I just showed you, and you've stared at a while. Showed you, and you've stared at a while. And we're going to approximate it in the following way. And I see that, let's see, sorry, I missed something here. We're basically going to do a Taylor series expansion in our function, in our vector f. And we're going to do that Taylor series expansion around the maximum value of this expression at f. So we have to compute that maximum. So, we have to compute that maximum. And then we're going to do up to a second-order Taylor series expansion. The first order term is going to be zero because we're evaluating at the maximum value. The second order Taylor series expansion, well, if this was just a one-dimensional case, this would just be a quadratic term. It would just be a square in F. It's not a square in F because this is a vector, but it has the form. Vector, but it has the form of a multivariate normal density. And so the trick here is think about what's varying here. So we want to do this integral over F. We're saying it's proportional to just the density at the maximum. And then, well, let's see here. We have F here. There should be an F here. The μ and the Here, the μ and the Q depend on these unknown parameters of our covariance and our regression model. But the point is that that second term is what we have to do the integral over. And as I said, all STAC graduate students know how to do this integral because it's just the multivariate normal density. And so what I've done is written that in blue here. Written that in blue here. So, what we have finally is a likelihood that only depends on the parameters of the covariance and the data. The other part, so this is one part of the INLA strategy. There's another part where we do this similar kind of trick. And I should say, this is sort of an analytic. Know, I should say, this is sort of an analytic approximation, which we're then going to use for computing. The other part of the INLA strategy is where we were doing this integration for theta, we can also sort of find approximations for the conditional distribution of F given the data and the parameters. And that's important to do in inference. So INWA stands for iterated nest. So, INWA stands for Iterated Nested Laplace Approximations. The nesting here refers to simply not just doing a simple Laplace approximation to this, but doing something more complicated. And I've now told you everything I know about INLA, and you can ask Finn and other experts in the group here over the week if you want to know more about that. Okay, the other part of this is we still let me let me go back here and I'm just about to wrap up. The other part of this is remember that we still have this big data problem. And despite all these fancy tricks of using the Laplace approximation, we still have to deal with this covariance matrix. This is still very, very big. Very, very big. And so, part of the way these problems are worked is we inherit one of those versions of dealing with large Gaussian problems. So, we talked about the Vecchi approximation. There's fixed-rank Krieging, there's also predictive processes. You can take your pick of those. The INLA folks also have a more particular version. Have a more particular version that's related to Markov random fields and also stochastic partial differential equations. You can choose among those and then apply those into evaluating these likelihoods. Okay, so here's my tornado example. So So I've concentrated this over just the marginal variance. Oh gosh, this doesn't come out very well either. I've concentrated this over the marginal variance of the process and the correlation range. It's a little bit hard to see here, but the likelihood surfaces actually have quite a bit of, what would you say? What would you say, correlation among them, even though this is my maximum likelihood estimate at about three degrees correlation range and a variance of about three and a half. There's really a lot of choice here. You can sort of entertain quite a bit of a range. When we take that maximum likelihood estimate and evaluate our solution, this is what we. Solution, this is what we get. Now, we can also go to the next step of actually generating, applying a Bayesian solution to this to generate draws from this posterior for these parameters. And then that would allow us to quantify the uncertainty in those estimated parameters. And we can also do conditional draws from this. But again, I apologize. But again, I apologize that this is sort of washed out here. But this is my predicted tornado surface. So what's really interesting about this, and I'm just about to wrap up here, is that there's this hole in Missouri where there's just not a lot of these extreme tornadoes recorded. And Chris. Chris Weichel, who works a lot in spatial statistics, is at Columbia, and he's sort of right in the middle here. So I don't know if that has anything to do with it. But so this is the result. You know, I've sort of checked this. I think it's correct. I don't really believe it. And yeah, yeah, Francis is shaking his head too. Francis is shaking his head too. My wife is actually going to go to Bulgaria this summer. And it turns out that in Bulgaria, when you shake your head like this, it means yes. And when you shake your head like this, it means no. But I'm assuming Francis is saying no. Yeah. So what have I missed in this? Classic statistician taking the data. Yeah. Yeah, Michael? Yeah, yeah, Michael? Non-stationarity? There is non-stationarity because it's fairly well publicized that there's an eastward shift away from Tornado Alley towards the east in tornadoes. It may or may not be due to climate change. Okay. But it's important because it is more optimally. Okay, okay. So that's speaking to the fact that I just took the Speaking to the fact that I just took these 70 years and just clumped them together and sort of had a more general question. I mean, how would you deal with constellation area? We shouldn't necessarily, along with private seal that spatial relationships. Right. Right, right, right. So, you know, when initially I was talking about a hierarchical model, and a hierarchical model would do a good job building that in. A good job building that in. Whether for this particular data set we can actually tease out those time trends is another thing, but at least we can write them down. Yeah, but I'm thinking about something else. I just don't like the patchiness of this. Yeah, Francis? You haven't told us how it's sampled. Right, right, because I don't know how these are sampled, but. Know how these are sampled, but I'm thinking that the patchiness here. So I just sort of realized this last night as I was awake due to jet lag is these are all reported tornadoes. These are not the tornadoes. And I think that what we're seeing here is probably a bias towards population centers. I think that would be true if you were. I think that would be true. But F three it makes it off damage on the ground. The Natural Weather Service goes there. So yeah, there's a really well known bias in tornado trends. We're all tested. We'll have to bring the data. Okay, okay. Well, it still, you know, it still doesn't explain why there's, you know, we're seeing. Um, you know, we're seeing a concentration here, and you know, on either side, I'm thinking these are sort of synoptic scale events. Pardon? So so this is that this is on the order of 14. Out of the 70, we're estimating on the average about 14 here, and then down here, this would be on the order of about two. Order of about two over the 70 years. Are you thinking of any terrain differences? Party? Right, right. So again, the southern Missouri, it's a pretty rugged area. Yeah, so so would that fit, Michael? Would the terrain factor in it? Okay. Yeah. I would think it would, but that's not really, I would think that that spot would be a little lower. Yeah. A little more space. Be a little lower, yeah, a little more south, yeah, yeah. Anyway, so, um, yeah, so that's um, yeah, so we can, we've we can talk about this some more. So, that was my um, let's see here. So, I feel like I covered quite quite a bit and thanks. Um, the that that tornado example, I think, is an example of looking at extremes. Um, Extremes. As I said, I'm a little bit surprised at the results. I feel like I worked this as a textbook problem and I got all these patchy things, which I don't really believe as a climatology of tornadoes. But anyway, it's a good showcase of sort of getting into non-Gaussian things. And as I said, I think that this week we're going to be talking a lot. This week, we're going to be talking a lot about non-Gaussian stuff, and so these kinds of tools are going to be useful as a starting point. Okay. So I think I'm, yeah, thank you.