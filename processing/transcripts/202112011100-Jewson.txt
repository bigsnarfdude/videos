Thank you very much to the organisers and particularly to the Junior ISBA session for inviting me. And thanks to Veronica as well for switching session because I had a flight arriving into the UK and I was very worried about quarantine restrictions and delays. Okay, so this is some work I've been doing with David, my advisor at David, my advisor at UPF in Barcelona. And yeah, so loss function selection and the use of improper models. And my subtitle is this data-driven, robust Bayesian inference. And that's going to sort of hint towards one of the applications for some of the methodology we've produced. Okay, so this is sort of a summary of the talk. So we're going to consider the problem of selecting between sort of traditional statistical methods using probability models and then the sort of more modern array of. More modern array of algorithmic inferences that are often sort of you have some complex procedure that's going to produce a prediction and you sort of estimate the parameters of that using loss functions. We're going to consider some way to select between these two methods. A sort of key point of this is that you can always turn a probability model into a loss function by considering the log score, but you can't necessarily always turn a loss function into a probability model, or at least not a normalizable probability model. A normalizable probability model. It could have, for example, flat tails, and then it wouldn't be normalizable, and then you don't have a probability model. To sort of alleviate that, we're going to use this Hiver and then score, or sometimes called score matching, that's going to allow us to compare loss functions and probability models in a way that's scale-in-varying. And I'm going to show this on an application. The sort of main application is going to be this robust regression example where we're going to select between doing robust regression with Tuki's loss, which does. With Tukey's loss, which doesn't correspond to a proper probability model, and also sort of standard linear regression with a Gaussian model. And yeah, everything I'm going to talk about today is in this paper that we archived at the start of the summer. Yes. Okay, so the motivation. As I've sort of summarized, so models assign probabilities to observations. We have a likelihood. We have to have Wabesian, so we have priorities on parameters. And this is sort of the standard way to go about doing statistical inference. Then, sort of more. Statistical inference. Then, sort of more recently, we have these algorithms. These are often more complicated deterministic mechanisms to think about making predictions. And then, these are often estimated using loss functions. And there's been sort of over the last, I think, 20 years, these references are, there's some sort of conflict between these two methods. So, as Bremen, in this quite influential paper in 2001, sort of saying, well, actually, models are never realistic enough to actually represent reality in any useful manner. Represent reality in any useful manner or sort of flexible enough to actually do what we want to. And then you've probably all seen this: the George Box quote that models are wrong, sometimes useful. And yeah, some of the examples of this are robust logistics or these really modern machine learning algorithms. On the contrary, everyone much more recently said, well, actually, yeah, this is fine, but if we abandon the model, then we sort of abandon our goal of understanding the truth, right? We're trying to, or the truth, the Trying to the nature that generated the data. So, models provide much more intuition about understanding the relationship between the data, and also you can produce probabilistic forecasts and do nice things like quantify uncertainty. So, our goal here is to try and address this conflict. And this is my sort of OBES link: we're going to try and address this conflict by learning from the data. So, we're going to try and use the data to learn whether we should use. To learn whether we should use a probability model or an algorithmic approach. You might think about trying to do this by cross-validation. So, if two methods produce predictions, you can compare their cross-validated losses, but that raises two issues. Firstly, in what loss do you choose them by? The Gaussian model is going to be best by the squared error loss. The Laplace model is going to be best by the absolute loss. And Tukey's model, I guess, is going to be best by Tukey's loss. And also, this is a predictive approach. And we want to do an inferential. Approach and we want to do an inferential approach. We want to see, yeah, sort of look in an inferential manner at this. And yes, so consistency is going to be quite key to what we do. So we're going to take the standard paradigm away from just considering proper statistical models, but we do want this guarantee that if the data generating proper probability model is under consideration, that we will recover it. And that's going to be quite key to what we and we're going to be Bayesian firstly because we, well, at least I believe it's correct. We well, at least I believe it's correct, and then also we already know that Bayesian model selection has lots of these nice ock and raiser properties. What's the challenge? Well, I've sort of hinted at this. So models are defined using the units of probability. They must integrate to one. An algorithm's loss can be completely arbitrary. Okay. We, yeah, or the units of that loss can be completely charged. The scale can be completely arbitrary. We, pardon me. We, pardon me, we know how to select between proper probability models very well, base factors, being less likelihood AIC, BIC, we could keep going. But these methods fail when the loss, the sort of loss we consider no longer corresponds to a proper probability model. This non-integrability, and I'll show you some examples of this next, this non-integrability makes the scale arbitrary and makes it impossible to sort of, yeah, one, not impossible, but using standard. One, not impossible, but using standard methods, you can no longer compare the losses. Yeah, you can no longer compare losses to probabilities. Yes, so from losses to probability model, so I said this at the start that we can always turn our model into a loss function by taking the log score, but the reverse transformation or the negative log score, but the reverse transformation of the exponential of the negative loss doesn't necessarily give us an integral probability model. Often, this is not a problem. Probability model. Often this is not a problem, so we can turn the squared error loss into a Gaussian model, we can turn the absolute loss into a Laplace model. And if we can do that, then it's fine. We can compare to algorithms minimizing Gaussian loss and absolute loss by turning them into Gaussian models and Laplace distributions and then using standard probabilistic framework to choose between those. But there are going to be some interesting cases where this isn't the case, okay, where this transformation E of the exponential of the negative loss function gives you something that doesn't integrate or integrates together. integrate or integrates together okay so this is the the main uh motivating example from uh from robust from robust regression so we want to uh regress a response y uh on a p-dimensional set of predictors x so think about well yes and so we can think about this from sort of an algorithmic point of view so we define some prediction function here i've considered just a simple linear predictor but this could actually in general be anything so we have some y hat At the end of things, we have some y hat that's going to be a function of our x and beta to produce a prediction, and then we come up with estimates of those parameters by minimizing some loss between the observations y and the predictions y hat. So that's a sort of algorithmic perspective to this regression example. The model perspective is to say, well, actually, let's define a data generating density for y given x, and call that f, and then we can estimate this either for fences by maximum likelihood or by specifying some prior in the parameters and then conducting data together. Parameters and then conducting patients again. So, as I sort of mentioned, for least squares, you can think about that as either a model or an algorithm. Okay, so you can either think about the parameters as being the minimizers of those squared error loss, or you can think about the least squares parameters as being the maximum likelihood estimates under the Gaussian model. But we sort of, it's hopefully quite well known now that this procedure is not robust to art lies because you have that squared error loss. Any one observation that makes a big error, you get a big loss. That makes a big error, you get a big loss for that, and that sort of skews the inference to take to give sort of disproportionately high weight to outlines. If I look at the sort of example of this, so here I generate, this is a sort of super simple example, but here I generate 90% of my data from a Gaussian with mean zero and variance one, and then 10% from this Gaussian with mean five and variance nine. And the red is my posterior predictive under. Predictive under a Gaussian model, and this has sort of been shifted away from the majority of the data in order to take account of those. Okay, so a standard algorithm to, or at least what we're calling an algorithm to robustify linear regression against such outliers is to use Chuki's loss. So if you see Chuke's loss, which I give here, we still have this squared error term. So we have the least squares divided by the variance, but then we have these two extra terms. Variants, but then we have these two extra terms: we have a quartic and a whatever to the power six is called, and this is going to be if our if our residual is less than some value that's going to depend on kappa, and then it's going to be flat afterwards. The important sort of what why these are how they are is that this is designed such that you are both first, you are both continuously differentiable, you are continuously differentiable twice. So, this is why this is this is why you get these this quartic and this this to the power of six term and the Six term and the idea is that the hyperparameter kappa controls this robustness efficiency trade-off associated with robust statistics. So if I set kappa equals to infinity, you can see that this term and this term will both cancel and you end up back with the squared error loss. And as I sort of take kappa away from infinity, the auto is cutting my squared error loss off at this value kappa. And then after the value kappa, I'm just giving sort of equal weight to all observations. Okay, so this is the red is this squared error loss. This squared error loss, and then I see that for different values of kappa, my Tuki's loss becomes flat, becomes flat after those, or for my for residuals over that should be, I think, just sigma or sigma squared, but becomes flat for residuals after that. Okay, but if I then try and do, so I started with Chuke's loss. If I try and turn Chuke's loss into a sort of pseudo-probability model by taking the exponential of the negative loss function, I get that it integrates to infinity. And you can sort of see. Integrates to infinity. And you can sort of see this. I flip those flat, that flat loss, and I end up with flat tails. Okay, so here I'm comparing the Gaussian again to those same Tukey's losses, and I can see that around the sort of for small residuals, the shape of this pseudo-probability model looks something like the Gaussian, but then it sort of starts to move away and becomes flat. Okay, so why is this a problem? Shuki's loss is strictly decreasing in this value. In this value, you can sort of see as we increase kappa, it's getting lower. So we can see GG's loss is strictly decreasing in this value, kappa. So therefore, if I just try to naively learn kappa from the data, the loss minimizer, the loss minimizing value kappa is zero independent of the data. This is because, so it's not integrable, and thus kappa is controlling the scale of this improper probability model. This improper probability model. So, if I just try and learn this in something that looks a bit like maximum likelihood, say I just find kappa to be the argument and rise of my loss, I'll get zero independent of the data. Again, I'm yet to come across any coherent way to learn kappa for Tukey's loss in a data-driven manner. And so this sort of motivates our problem. We want to be able to use the data to select between a Gaussian model and Tukey's loss. So you want to select from the data, do we need to be robust or not? Do we need to be robust or not? And then, further, if we select Tukey's loss, so if we decide we want to be robust, we want to be able to use the data to also select Tukey's hyperparameter and decide how robust we want to be. I have a second example that I probably won't have time to cover too much, but from kernel density estimation. So, here, this is sort of imagine the in-sample likelihood of a kernel density estimate. So, you're estimating the kernel between each observation. Between each observation and every other observation. And so, in so doing, you end up with a kernel between the observation and itself. And that sort of also ends up with a very similar idea to having these flat tails. And this also doesn't, this also sort of isn't possible to normalize this into a proper probability model. And that's why there aren't standard likelihood-based methods to estimate kernel densities. Okay, so this is, I've sort of most. So this is, I've sort of motivated the problems again. We want to be able to choose between a loss function that may not define a proper problem, that may not be able, we're not able to transform that to define a proper probability model. And we're sort of at least mainly motivating that for a robustness point of view and a sort of standard likelihood framework. Okay, so the first question we ask is: well, can we do Bayesian inference with something that's not a probability model? Can we do Bayesian inference with a loss function? Can we do Bayesian inference with a loss function, or can I do Bayesian robust linear regression with Chuki's pass? Luckily, Pierre Vissery, Chris, and Stephen Walker in 2016 came up with a framework that allowed us to consider so doing. So if we are defining this parameter theta star as the argument arising this loss function, where g here is the data generating process, we can come up with this general Bayesian updating, which they sort of argue is a coherent and principled prior to posterior update. Principled prior to posterior update given for the minimizer of this loss given the data that's observed. So you take the prior and then you multiply this by the exponential of the negative loss function. And you can see that this is sort of a strict generalization of Bayes with standard Bayes being recovered using the negative logoizer. Okay, but to start with this general Bayesian series only principled incoherent, as I call it, for Principled incoherent, as I call it, for parameters that actually minimize the loss. So remember, I said that if you tried to minimize the loss over kappa, over kappa two of the Tukey's loss, you would recover zero. And therefore, sort of as a result, trying to use this generalization approach to either construct a joint posterior over sort of model parameters theta and this hyperparameter kappa, or trying to maybe integrate out those model parameters and estimating kappa in as a sort of marginal, a marginal minimizer of this. That's probably going to. Marginal minimizer of this, that's probably a maximizer, a marginal maximizer of this term, that also, but that also will end up with kappa zero. So we sort of we need some sort of framework beyond standard statistical tools in order to be able to. Okay, so an important part to this framework is: so, what does it, I suppose, I want to be able to choose between a proper probability model and an improper probability model using the data. So, I need some way to. Some way to be able to define some way in which an improper probability model can actually capture the data generated pressure. So the data must have been generated by a proper probability distribution. So we need to think about some notion of how an improper model could possibly capture the behavior of this proper property. Because clearly, the improper model could not itself have generated the data. However, the argument is that the improper However, the argument is that the improper model could provide a better representation of the Day COX process than any of the proper probability models under sort of under consideration. So if you have outliers and you're considering either a Tukey's loss or a Gaussian model and they're the only models you're considering, then it may be the case. And I'll hopefully try and convince you that actually Tukey's loss is a preferable model, even though Tukey's loss could itself not have generated data. Okay, yes. So how can Okay, yes. So, how can we interpret statements made by an unnormalizable probability model? Well, we can no longer make statements about absolute probabilities because there's this unknown normalizing constant that it doesn't exist, right? It's infinity. So it could be anything, okay? So instead, we can't make statements about absolute probabilities. So we're going to make statements about relative probabilities. So if we consider this ratio, then whatever that normalizing constant was, it then it cancels, it disappears. So we can consider the ratio. disappears so we can consider the ratio of two unnormalizable probabilities of our unnormalizable probability that's our improper density evaluated at y naught and y1 as describing how much more likely it is to observe y equals y naught than y equals y1. If we consider this for our Chuke's loss case, then for residuals close to zero, or for y is giving residuals close to zero, then okay, their relative properties behave more or less like Properties behave more or less like a Gauss. Okay. However, for residuals that are outside of this of our Tukey's loss cutoff, we're then saying that these relative probabilities are then one. So it's sort of those two observations are equally likely if they're both outside. So we're sort of in some way saying nothing about the tails. We're saying two observations, as long as there are two observations past the cutoff, then yeah, we're just saying that. then yeah we're just saying they are equally they're equally as light this so if we if we're going to interpret the the statements given by our improper model in terms of relative probabilities then we want to fit them or compare them to the data generating process also using relative probabilities and we're we're lucky that uh fisher's divergence actually gives us a way to do that so fisher's divergence for for two uh for two densities g uh is g uh g and f is a half times the um l2 the squared l2 norm of the that should that should be a negative sorry there's some tripods here so that should be a negative of the difference between the log gradients okay so i just expand the definition of the gradient as sort of assuming everything everything is nice and everything exists so if i expand the definition of the gradient and i see that i'm worried about this or i'm scoring i'm comparing f to g I'm comparing f to g based on its infinitesimal relative probability. So y plus epsilon y. Okay, so we're interpreting our unnormalizable probability model in terms of its relative probability statement about the data generating process. And then Fisher's divergence gives us a way to compare how those relative probabilities made by our improper model can compare to the data generating process. So then we need, once we So then we need, once we motivate fission divergence, we then need, we obviously don't know G, we don't know the data charity process, so we need something convenient to allow us to actually think about minimizing the distance between F to G. And luckily, if you consider, so we're going to try and estimate the parameters of our unnormalized improper model by minimizing Fischer's divergence to G. And luckily, this minimization is the same as minimizing the Hivarian score. Minimizing the Hevarian score or the score match or the score matching criterion, I think it's often called. So, this Hevarian score takes twice times the Laplacian, the likelihood, where if we're in a univariate case where we are for regression, the Laplacian is just the second derivative, and then plus the first derivative squared. Okay, and so yeah, so I have this here, but for the univariate y, this simplifies nicely. So, why is this super useful? Why is this super useful? Well, the two arguments of this Heverian score depend on gradients or first and second derivatives of the log likelihood. So when I take the log, my normalizing constant, whatever it was, becomes an additive term. And then an additive term, it doesn't depend on data. And then when I take gradients, they disappear. So using the Havarian score, this allows us to score our improper models invariantly to what they're. Invariantly to what their normalizing constant is, or invariant to the fact that their normalizing constant was infinite. So, this then gives us this scale-invariant criteria that we can now use to consider comparing the Gaussian model with Chukey's loss. Okay, so yeah, we're now sort of ready to introduce our methodology. So, we're going to extend the general Bayesian. The general Bayesian posterior to what I've called the H posterior. It turns out I was just reading some reviews, and this did actually exist slightly before us, but was used in a way to score proper probability models. So here we're taking the general Bayesian posterior, but rather than just using directly using our Chuki's loss function or directly using the loss function of our algorithm, we're going to first apply the Heverinen score to the improper model associated with that loss function. Associated with that loss function. So that's going to, that will be taking, that's taking derivatives of the log of that improper score. So we're taking derivatives of the loss function. And we are then invariant to what that scale was, invariant to what that normalizing constant was. And yes, this then allows us, whereas we couldn't produce a general Bayesian posterior, a joint general Bayesian posterior over theta and kappa for that Tukey's loss case, we can now use this H posterior to produce a general posterior posterior. H posterior to produce a joint posterior over H. We prove in our paper that this is consistent. So you can do this consistency if this combination, theta and kappa, minimize the Fischer's divergence to the data generating process, and these tilde values are our H posterior map estimates. Then we prove under some fairly standard regularity conditions that as n stands to infinity, this is big. As n to infinity, this is big OP one over square root n. So we first proved posterior consistency. So that then shows we can use this as a way to think about learning what that kappa cutoff value is. And then we can further think about producing a loss selection criteria that is analogous to our standard model selection criteria by integrating out those parameters. So here I have the integral of the So here I have the integral of the prior times the exponential of the covariance score applied to the improper model, or it could also be a proper model, so I can produce these for either. And this is an exact analog to our sort of standard Bayesian model selection with the margin likelihood where I've taken out the log likelihood for a proper model, because the log likelihood does depend on the scale of this guy F, and I've replaced it with the Heverian script. I've replaced it with the Hivarian score, which is going to be scale invariance. This then turns my H score into a criteria that I can use to select between an improper model and a proper model. We consider, I said that consistency was going to be key and as part of that, we consider a sort of two-stage select a sort of two-stage inference process where you would, for example, choosing between a Gaussian and Chuki's loss, you would first run this H-score selection. So you would. H score selection, so you'd uh you would calculate h for the Gaussian model, and you calculate h for the Chuki's loss. And if the Gaussian model was chosen, you would then drop back down to Bayes. Okay, so you would then, if the Gaussian model is chosen, there's then no reason to worry about using the Hevarian score anymore, and you can just go ahead and do your standard Bayesian inference using the Gaussian model. But the Hevarian score gives us the possibility to also select Juki's Voss model if you have outliers, if you have model specifications. Modeling specification. Okay, so yes, we have this criterion. We're going to do this loss selection through the sort of H base factors. In our paper, we consider a tractable approximation to these H base factors. We're going to use Laplace approximations, and this basically gives us the vessel to actually prove. This allows us to prove, this allows us to prove posterior consistency. I give the forms here, but in the interest of time, I think it's probably. Of interest of time, I think it's probably not interesting to go through these too much. Um, yes, and I think I have yes. So, this is the this is sort of our main theorem. And so we, while I'm deliberately skipping over some details, under some fairly standard regularity conditions, we get that this selection using these H-base factors or this evaluation score selection gives us a consistent selection. Consistent selection. Consistent selection, consistent selection to the Fisher's divergence minimizing model, which can be proper or improper. What this means is that if you are selecting between a group of proper models and improper models, if the data generating process is under consideration, you will recover that. That will minimize Fisher's divergence and you will still recover that. Sort of, this is key, right? We don't want to be considering improper models if we don't have to. If our set of probability models are good enough, brilliant, that's what we want to do. Brilliant, that's what we want to do. But in the case where the set of probability models that we have available to us is not good enough to correctly characterize the data generating presses, then we also have this ability to select an improper model. Yeah, the rate, so we get the fairly standard in the nest, so we get exponential rates if for example the Gaussian, the get this right, the. Gaussian, the get this right. The Gaussian model is nested within Tukey's loss. So, if I send that kappa parameter up to infinity, I recover the Gaussian model from Tukey's loss. So, we get the standard asymmetry. So, if where if it's very easy if you have outliers to detect Tukey's loss, because that's sort of the more complicated model, you get a slower rate if you select, if you want to select, you get a slower rate to select the simpler of the two nesting models. In our experiments, we're going to use some of David's won local prize in order to. Some of Bavid's men local prize in order to improve the select. I just wanted to comment on some related methods. So in 2011, Hibernan and some of his colleagues proposed a score matching information criteria. So here they estimate the fission divergence by correcting the in-sample revariant score using its asymptotic biases, sort of an analog to AIC. Analog to AIC, but for the Heveridin score. So two main distinctions are: firstly, they're doing so out of sort of for computational reasons. So they're considering this in a case where the normalizer of the model is hard to calculate, is sort of computationally expensive. We're doing so to allow us to actually consider improper models. And then also, they're sort of AIC. They're sort of AIC, there's this cross-validation framework, it's a sort of predictive criterion. Okay, and we actually prove, is that right? I think we have it, we don't, well, no, not quite prove, we have a counterexample in our paper where their method does not come up with consistent model selection where we do. Sort of similar to AIC versus BIC ideas. The other one is this paper from Lewis, McKekin, and Lee that was read in Bayesian analysis, and David and I submitted a small discussion. Submitted a small discussion on. So these guys consider doing Bayesian inference, but conditioning on insufficient statistics from the data. And in particular, they consider Bayesian inference, Bayesian updating, conditioning on Trukey's loss minimizing parameters. Okay, so we this is sort of really nice and is somewhat different to what we do because they're sort of they're fitting their proper probability model. So their example is that they fit a Gaussian. Probability model. So, their example is that they fit a Gaussian model for regression, but they condition on these robust summaries. And so, they're, yeah, they're sort of still fitting a proper probability model, but in a robust fashion, whereas we're actually considering selecting between a proper probability model and an improper probability, proper probability model. And it may be, our argument is that the improper probability model may, in some situations, provide a better approximation to the basic factors. They also, as a sort of They also, as a sort of part of their paper, they talk about how important it is when you use Juki's loss to set hyperparameters. And yeah, we think actually our method to do so could complement theirs very nicely. So you estimate your hyperparameters using our method, and then you could go ahead and do your Bayesian updating condition. Okay, so making quite a good time. So to finish off, we'll look at our experiments. So, to start with, sort of sanity check, or at least to justify this notion I have of data-driven robustness. So, I generate some data. So, 90% of this data is going to be from a Gaussian with mean zero and variance one, and then 10% is from a Gaussian with mean five and standard, I think it's variance nine, standard deviation three. Okay, and we're going to just look at how this margin. And we're going to just look at how this marginal, this marginal integrals. So here I'm going to fix, I'm going to fix kappa 2 and just integrate out of each because I want to plot what this score looks like for different values of kappa. Okay, so on the x-axis, we have this value of kappa. This dark black line is then plotting our H-score criteria for different values of kappa. Okay, and then what I have. And then, what I have on top of this are bootstrap parameter. Yeah, no, sorry, it's not even bootstrapping. I'm just sampling again from the data generating process and then plotting a box plot to try and represent the distribution of the regression parameters. Okay, so it's probably easiest to start off here. So, the idea is 10 standard deviations away from the mean is quite big. So, this is our sort of non-robust version. As we become more robust, As we become more robust, our age score is increasing because we're sort of excluding some of those outliers and we are better fitting the majority of the data. So this is happening up to a point. And then after this point, we become the idea is we've become too robust. So we're sort of getting rid of too much of the data. And yeah, and then it starts to sort of harm our H score. And we compare this to these approximate. Approximations to the distribution of the parameters. There's sort of a standard thing to worry about when doing robust statistics is optimizing the bias efficiency trainer. So if you do, if you sort of fit Emily, Emily of a Gaussian model is the most efficient way to do inference for the mean of something. But if there's outliers, it can be arbitrarily biased. And the idea is if you use Chuki's loss, you are limiting the bias. You are limiting the bias somehow at a cost of efficiency. So you can see if I work again from right to left, here I can see that my estimate, the sort of true value of mu was zero. So here I can see that for kappa, pardon me, for kappa too big, I am biased. So I have these biased. And as I decrease kappa, I'm shrinking that bias. I'm shrinking that bias. It goes below because it tries to shift the mean to reject the outlier. So it sort of. Shift the mean to reject the outlier. So it sort of shifts a bit negative, which is why it actually goes below the line. So you can see, but I'm decreasing the bias as I come from here until I get somewhere around about here. This is not sort of formal, this is just a comparison. But as I get into the middle here, I'm just about in the stage where I'm unbiased-ish. And that's where our H score is maximized. And you can see that this, based on the upper and lower points of my box plot, this has actually got the Of my box plot, this has actually got the smallest variance. So I'm selecting five, and here I'm trying to argue that we're doing a good job of optimizing this bias variance trade-off. So I am almost unbiased here, and I have a much smaller variance. If I go up here, you can see suddenly I'm rejecting too much data. I'm using too little of the data, and the distribution of my mean half becomes very wide. And to look at the value was chosen to the octavella, I think, was kappa two is five. October value, I think, was kappa two is five. And so you can see here that my sort of Chuki's loss model is doing a much better job of approximating the majority of the data and then has these flat tails as I come out here relative to the Gaussian. One thing to note here that I haven't mentioned before. So, of course, I'm plotting you here the improper density of Juckel's mass. Okay, so that doesn't have a normalizing constant, and that's I'm free to choose that normalizing. And thus, I'm free to choose that normalizing constant. So, here I have chosen that normalizing constant to match that. I think it matches a kernel density estimate data. Remember, the argument is that's only making statements about relative probabilities, and thus that normalizing consequences. I've also then chosen the mode of the Gaussian to also do the same to sort of try and provide a fair comparison. And you can see that the Gaussian provides a much worse expression. Provides a much worse explanation of the sort of definitely the mode of the data or the majority of the data than cheaper clusters. Okay, so now we're going to, that was just, that was actually just for estimating the mean of the Gaussian population under outliers. Now we're going to, that was sort of trying to convince you that our H score does go some way to achieving this data-driven robustness. The next element was this consistency that I said was going to be really. this consistency that i said was going to be really important because if our if we have proper problem if we have proper probability models that are good we want to use those okay and we we prove this uh consistency result so here i'm i am this time just going to generate from a gaussian model uh i've got six i've got five covariates and an intercept my residual variance is one and we're just going to consider increasing the sample size and i'm going to do this for the score matching information criteria that was that paper of hevarinen and some colleagues Hibaronen and some colleagues. And then I'm going to do it through our criteria under a local and a non-local product. So I said that kappa equals infinity was going to recover, was going to recover was going to recover the Gaussian from Chuki's loss. I reparameters this. So we take mu is one over kappa squared. And thus, if I have a half Gaussian with mean zero, that's local. That's got its sort of prime mode at mu equals. At nu equals zero, and I'm going to, or an inverse gamma, which has got zero density at zero. So that's then that's an inverse gamma prior. We choose our prior in sort of an objective manner in order to put most of our prior mass in the region kappa is one and three. So kappa has the units of standard deviation. So we're saying we ideally, if we want to get rid of outliers, we want to be getting rid of them somewhere between one and three standard deviations away. Okay, so firstly, this is box plots of the SMIC, score matching information criteria as n increases. So negative values are correctly selecting the Gaussian model. Okay, so for all n, most of the observations, most of the time SMIC is correctly selecting the Gaussian model. But certainly from this, there's no evidence, I'm not saying it's inconsistent, you can't show that from this, but there's no evidence of it being consistent. There's no evidence of even the median, for example, decreasing with some. For example, decreasing with sample size, and we still have this, even for n is 10 to the 5, we still have this heavy, this heavy right tail for the local prior. Okay, there's at least some evidence that the median is decreasing with sample size to start, but we still have this heavy right tail under local prize. And this is, as I'm sure David has told you all many times before, this is a sort of manifestation of the polynomial rates for selecting the simpler model versus the exponential rates. Simpler model versus the exponential rates for selecting the more complicated model. And David luckily came up with a super clever way to fix that using his non-local priors. And if we now consider a sort of non-local prior specification, as I talked about before, we now have that, well, after for any is 10 to the 3, for n is 1,000. Okay, we're wrong once in my 100 repeats, and for n equals 10,000, we're then right all of the time. So this is then demonstrating that consistency. I will just wrap up quickly. I will just wrap up quickly, very quickly, with some real data experiments. This is a data set on some colon cancer or some gene expressions associated with colon cancer. So this is a the first example is one where the data looks pretty Gaussian. The data is pretty well approximated by the Gaussian. And so in this case, our H score is able to select the Gaussian. The SMIC also selected the Gaussian. And David, in his paper with Javier Rubio, also In his paper with Javier Rubio, also select the Gaussian over the plastic. Here, our methods are agreeing. We're not offering anything, just sort of showing a sanity check. And if you look at the data, it seems like the Gaussian approximation looks good. Okay, the Tuki's loss doesn't do that much worse. This is a normal QQ plot suggesting actually, yeah, the data is pretty well approximated by the Gaussian. And just again, to draw on that bias variance idea, the black. The black line plots the difference between the parameter estimates at the index is 0 to 7 from the Gaussian model and the Tukey's loss. So the parameter estimates are almost exactly the same. The intercept is slightly different, but all of these parameter estimates are almost exactly the same under the Gaussian and Tukey's loss. But we have inflated variance under Tukey's loss. So here this is a case where the parameter estimates are the same. The bias is the same. We don't know what it is, but the bias is the same. They're the same parameters. Same, they're the same parameters, but the Tukies has inflated parameter variance. And in this case, we want to choose the Gaussian model, right? It has similar bias, lower variance. The Gaussian model was what we wanted. In contrary, we look at this data, which is some RNA sequencing data, again, that David and Javier studied in their paper. We take a subset of the variables because at the moment we're not considering sort of really high-dimensional predictions. Dimensional predictor sets. And so in this case, we get that Chuki's loss is massively favored by our model. This agrees well. Javier and David selected a Laplace model over Gaussian tail, so there's evidence of non-Gaussianity. And so we find, yeah, very strong evidence of non-Gaussianity. Interestingly, this score matching information criteria in this case still selected the Gaussian model. My gut feel was that this is because they're estimating this asymptotic bias. Asymptotic bias, and they you sort of have 192 data points and 15 or 16 parameters, and maybe that's somewhat unstable. That was my sort of gut feel for that. But this is an example where, yeah, we select Duke's loss. Previously, the Gaussian model had been rejected. And then, if we look at the actual model fit, so here I'm comparing the model to the standardized residuals. So, this is the fit of the Gaussian model. So, you can see these are standardized residuals. So, the Gaussian model has estimated a variance that. So, the Gaussian model has estimated a variance that is too big and has thus overshrunk, has overshrunk those residuals. That's where you see sort of a lot of mass here, and you can sort of see evidence that maybe some of these were outlying. Guys, if I compare that to the fit under the Chuki's loss, I'm hopefully providing a much better fit of this mode, and then we still sort of still have this ability, we have this flat tail. We can still, in our relative probabilities, capture the idea that we have some unlikely values. Some unlikely facts. And again, looking at the normal Quubot, that seems to support this, at least supports the assumption that we're the conclusion that the data is not Gaussian. And the last plot, yeah, the last plot I'll do, again, we're looking at this bias and this variance. Now, in this case, we actually see, okay, so there are different parameters. The Chukis and the Gaussian are fitting different parameters in different cases. And actually, we also see that some of the Chukis parameters are also achieving. Chuki's parameters are also achieving lower variance. This is the case again, we're arguing that we're somewhat optimizing this biance variance trade-off associated with robust regression. I think that's probably a good time to stop. So we do have another example looking at kernel density estimation in the paper, which you are, which you're welcome to read. I'll skip to the just to yes. So just to wrap up, it was previously difficult to firstly learn whether. Difficult to firstly learn whether you should use a robust method that might use a sort of heavy-tailed flat-tailed loss function that was going to correspond to an improper model. We were able to interpret these improper models as providing information about the relative probabilities of the data generating process. This was naturally led to the convenient Heverian score as a way to evaluate how they perform relative to the data generating process. And yes, and that then allows us to select between proper and improper probability models. And this notion of data-driven robustness that I'm hoping to explore further. So yes, thank you very much. Can you hear me now? Yes. Thank you, Check. Wonderful talk. Nice presentation. Any comments here from the floor live? Christian? Okay, Jack, and I'm happy when I hear about the evariant score. Score, so that's great. Uh, but I have a question about these loss-based uh distributions. Uh, namely, that I mean, that's my generic question I ask at every talk, I think, that in front of the of the H, you could add the constant and not modify the spirit. So, do you have a way to scale this weight in the exponential? So, we actually in this paper, we don't Actually, in this paper, we don't deal with this problem at all. I think we talked about it, and we didn't want to clutter it too much. You're absolutely right. You changed the confidence, and that will have some notion over selection. I think my answer to this question is that Chris and Stephen have a good solution to that. They have this bootstrap, sort of comparing it to the Bayesian bootstrap using sandwich covariances and Hessians. And yeah, at least in the current presentation, we have. At least in the current presentation, we haven't used that, but that would be if I was sort of recommending something, that would be what I would do. And if I may ask you a second question, how do you set an objective prior on Kappa 2? Yes, so we were sort of saying completely objective might not be. So you, I think you, did we prove, did we prove this? You would have to be super careful about having a improper prior because you've got an improper model. Improper prior because you've got an improper model, and it's possibly that the tails of that will not decay at all. So, I certainly didn't consider this. I think you could probably prove it quite easily, but you wouldn't want to have a completely improper prior. That would be very dangerous. We just have this notion of, okay, so where do you want to cut outliers? So we know if the data is Gaussian, so have that as H0, if the data is Gaussian, then most of the data lies within three standard deviations. So we probably want to cut before then. probably want to cut before then and you also still want to you sort of still want to capture the mode of the vector you don't ideally want to be cutting observations less than one standard deviation we sort of consider this as a as a reasonable range to put sort of a lot of our prior probability on any other comments on the floor here uh should it uh hi thanks it's a very interesting Hi, thanks. It's very interesting. I was wondering, so you had this sort of asymptotic result, like a bit like Bunchein family at some stage. Can you say something about the variance? Is that related to sort of the some kind of optimal variance you can expect or facial information if the model are correct? Or is there something you can say about the variance? I might have missed what you said, but. So we haven't here proved asymptotic normality. We proved consistency and then we proved the rates of the rate. rates of the rates of the model selection and what's your opinion and what is your opinion sorry Judith I don't yeah I don't quite understand the the question I I guess you're gone yeah maybe you can so I think your answer is you didn't do it and you guess it could be done like what do you get do you think that you actually uh get um Get a correct variance in a sense, or are you inflating the asymptotic variance? How much inflation do you get? Do you get some sense there? So it's going to be a sandwich co-variance matrix, which in these, nobody's proven anything, but they're usually bigger than, or sorry, it's going to be a Hessian. Your base muster is going to be a Hessian, probably, rather than a sandwich, which the MLE will be. And that's usually inflating. And that's usually inflating. So, yeah, so I don't have much more intuition beyond that. Any other comments, questions? Any questions from the online audience? No, then let's... Sorry, Peter. Sorry, I just want to... You're not allowed to ask questions, David. That's not how this works. No, no, I didn't want to. No, I'm sorry. I'm joking. I'm joking. I'm not about. Okay. No, of course. I didn't want to ask. I didn't want to ask, I just wanted to add a remark to Judin's question. I think what happens here is: so we didn't look at this, so we don't know what the variance is going to be like, but it's not very easy because, see, first we're in a misspecified setting, right? So you're looking at the asymptotics for this Kappa parameter for Tukey's laws, but the truth is a proper model, so it's misspecified. But also, if the truth But also, if the truth is, for instance, a normal, then the mode occurs at the boundary. So kappa goes to infinity. So we have this combination of things that we have misspecification and conversions to the boundary. So all we've done is prove that the rate is one over square root n, but that's it. We then actually look at what the asympetic distribution would be like, but I don't think it would be normal. And I have no idea of whether the variance would be. The variance would be efficient even under the usual misspecified sense of being efficient. I don't know if that helps, but that's. Thank you for that additional comment. Any other questions here online? If not, then let's thank Jack again.