Thank you very much for the introduction and for the invitation. I'm very glad to be here, even though I mean, it would have been much, certainly much better to be in Banfort together. But anyway, that's what it is. So I apologize, I'm not going to use statistics a lot, even though I will mention some aspects of statistical approaches. I will be mentioning compressed sensing where statistics. Sensing where statistics does play a role, as well as something about machine learning, where indeed statistics is very important. So, we'll try to draw a connection to the main theme of this workshop. So, let me just start with a very common example. We've seen it a few times already in this workshop. So, I will try to be quick, just as a prototypical example of the inverse problems I consider. So, this is the Calderan problem. We have a bounded domain, omega. We have a bounded domain omega and the conductivity function sigma, like the one that is shown here on the right, and we suppose that sigma is bounded by above and below. The physical model here is this elliptic PDE, the so-called conductivity equation, and for simplicity, I take Neumann boundary condition. So it means that I put, I prescribe a current on the boundary. So G is prescribed on this boundary here. And the forward problem is modeled by the Neumann-to-Diracle map. So for each current G that So for each current G that I apply, I measure the corresponding potential on the boundary of the domain here, and the corresponding potential is just U, so the solution to this elliptic D on the boundary. And so this is the so-called Neumann to Dirichlet map, because from the Neumann data, it associates the Dirichlet data. And this map is linear, namely for each fixed sigma, since this PDE is linear, the Neumann to Dirichlet map is itself linear. Neumann to the Reclaim map is itself linear. However, the map that from sigma goes to n sigma is highly non-linear because the solution to this ellipticity depends in a very complicated way on sigma. So somehow the forward map is a non-linear map, even though n of sigma is indeed belongs to a space of linear and bounded operators. So this is the forward problem, so from here to here, and the inverse problem, the so-called Calderon problem, concerns. Problem, the so-called Calderon problem, concerns going to the opposite in the opposite direction. So, from the non-metroid nuclear map, we want to recover the conductivity sigma. Right, so this has been mentioned already. In many situations, at least, these inverse problems is uniquely solvable. It means that f is an injective map, which means that given the data, you can recover a unique sigma. This is certainly true in 2D or in 3D if the conductivity is smooth enough, but I don't want to go. Smooth enough, but I don't want to go into these technical details here. So, the key point, however, the key difficulty in solving these inverse problems lies in its instability. Because as it's been mentioned already, the inverse map F minus 1 is only logarithmic stable, which means that even if N of sigma 1 is very close to N of sigma 2, this means that, yeah. two. This means that yes, sigma one will become close to sigma two but very slowly because this log factor here means that the speed of convergence becomes exponentially slow in the variable sigma. And so this of course creates issues in the practical inversion. Here I just show an example of what happens in or what can happen in practice. I mean I could have shown many other I mean, I could have shown many other examples, but anyway, in this case, you see the true sigma here, and what you can recover. I don't want to go into the details here of this algorithm, but anyway, what you can recover is only a very smooth approximation of sigma. So in some sense, we recover a low frequency approximation of sigma. This is not precisely correct, but at first order it is correct. And as it was detailed in the talk by Nico on Monday, this means On Monday, this means this corresponds to the fact that in the linearized setting, the singular values of this operator n of sigmas are exponentially decaying. So this inverse problem is severely posed. And so in some sense, the high frequencies are completely washed away. You cannot recover them because the noise kills those pieces of information. This is, of course, no surprise because this PDE here is infinitely smoothing, which means that even if Sigma has a means that even if sigma has a discontinuity here, even if it had a Dirac delta here, but anyway, a discontinuity, so in this case, the function sigma is not even continuous, as soon as you move away from this discontinuity, and certainly on the boundary where you make your measurements, your function u is even analytic. So somehow your singularities are completely destroyed, which means that going in the other direction is very complicated. Very complicated. So, the question that we can address is: how do we solve this problem in general? Of course, I could speak of regularization Bayesian approach and all those approaches. In this talk, I'm going to focus on one particular type of either regularization or prior, that is assuming some prior knowledge on the sigmas. Before going into that, let me just discuss a general model for inverse problems. So, instead of considering the specific Instead of considering the specific Calderon problem, we consider any inverse problem model between two Banach spaces X and Y. The forward map is a possibly non-linear map F that goes from an open set of X to Y. And since often there is a PDE that appears in F, like the conductivity equation in the Calderon problem, typically F is a complicated map, so the analysis often is not so simple. So the inverse problem, as I mentioned, is the recovery of X from the knowledge of F of X. From the knowledge of f of x, and it often happens, at least in many inverse problems in P that one has instability. It can be either a logarithmic instability or only like a mildly imposed problem, but still some kind of regularization or prior knowledge on the X has to be used in order to have a stable inversion. So, this type of framework appears in many different domains with many different Different domains with many different applications, with many different PDEs. So, what I'm going to say is not specific to the Calderon problem, even if I'm going to use that example a few times. It applies to many other examples. At some point, I will also discuss a scattering application. Giovanni, can I ask you a quick question? In the previous slide, what are the axes on the right picture? What am I looking at? Right, yes. Yes. So this is just an arbitrary, let me say, an arbitrary visualization of the Diracleton-Neumann map. And the arbitrary visualization is the following. The Diricleton, sorry, the Neumann to Dirichlet map is a linear map. So I represent it with a matrix. And this matrix is determined by fixing a basis of L2 of the boundary of omega with trigonometric polynomials. So basically, I chose sines and cosines. So basically I chose sines and cosines up to a certain frequencies, and then I use that basis to represent that operator up to a certain frequency. So basically here you see the lowest frequencies and when you go up you see higher frequencies. So basically in, you know, you shouldn't think as a radon example where these guys are end up here and these are just a smooth versions, a sinogram of these conductivities because in the radon case, it's just a low. A low, a lowered order smoothing. Here it's infinitely smoothing. So basically, those three things are disappear here somehow. They are somewhere, but I wouldn't be able to tell you where because it's really, they are everywhere. So that's the problem. Yeah. But that's the representation. Maybe you'd see something geometrically interpretable if you did the Schwarzkern kind of. Yeah, that's something. Yeah, yeah. Yeah, it was just a quick intuition to see exactly. A quick intuition to see X and Y somehow. Yeah. Right. Thank you. Yeah. So the key point I want to use here is that the object, the signals that we want to recover are not arbitrary signals like the one I show on the left. So I'm not aiming to recover a white Gaussian noise, but I always look for Always look for some kind of structure in the signals. So, the objects I want to recover always have a structure, as we see in these examples here. It's like lungs and the heart in a particular and idealized setting. Here, these are maybe cracks in concrete. This is the structure of the earth, and similarly here. So, anyway, I always had a structure, and this, of course, is a key point in the Bayesian approach. Uh, in the Bayesian approach, in even though here I will just take a more deterministic point of view, and I will just say from the mathematical point of view that X belongs to a certain lower dimensional object of my high or infinite dimensional space X, and W is either a low dimensional subspace or submanifold of X. Furthermore, often X is sparse in that space, which means that it can be represented only by few non-zero elements of a suitable Non-zero elements of a suitable fixed family of atoms. So that's my assumption, and I want to recover, and I want to exploit this assumption in the recovery. So this, of course, is something that I'm not discovering myself. It's a very common strategy. For example, in this paper, if you restrict your unknowns to piecewise constant functions on some unknown polygon, so you don't know the polygon here, you have to find it. Polygon here, you have to find it. So, this gives rise to a finite-dimensional sub-manifold of the space, because somehow you can parameterize those conductivities with the vertices of the polygons. Then, in that case, you can prove Lipschitz stability. So you can move from logarithmic stability to Lipschitz stability, so a much better stability property. And you see in the reconstruction that the results are not comparable at all to what we saw before with the low-dimensional. We saw before with the low-dimensional reconstruction, with the low-frequency reconstruction of the conductivity. Here, the results are much, much better. So, the question is: why can we find a general and abstract explanation of this phenomenon of why reducing to lower dimensional subspaces or submanifolds, or possibly using sparsity, helps in achieving a better reconstruction? So, that's the key topic of the talk. The key topic of the talk, and this first result is a relatively old result, it's quite actually simple, and simply says that in the case of linear subspaces, so assuming that I know that my function lives in a finite dimensional subspace, so it's a linear subspace, then things are fine, provided that I have a couple of assumptions on f. So, f, first of all, has to be injective on the whole space. So, you need to have a global. So you need to have a global injectivity assumption, which, for example, in the Calderon problem is granted thanks to the global uniqueness results. And you need to have injectivity of the Frechet derivative. So basically, the linearized problem has to be injective. If you have these two conditions and you restrict your unknowns to a compact and convex subset of this finite-dimensional subspace W, but this is easy in the conductivity, it's just enough to Is just enough to prescribe the lower and upper values of the conductivities, for example. Then, in this case, you always have Lipschitz stability. So, you forget the bad log type stability estimates. If you assume that your unknowns live in a finite dimensional subspace, Lipschitz stability is almost free. Let me just go through the two main steps of the proof just to understand the ingredients here. The proof is basically divided into steps. The proof is basically divided in two steps. The first one is when x1 and x2 are large. In this case, the result is easy thanks to global injectivity, because somehow this quantity here cannot become too close, f of x1 minus f of x2, and you use compactness as well. The second step is when x1 and x2 are small. Well, when an x1 and x2 are small, you use somehow the inverse function theorem. So you approximate f with the Fouchet derivative, but then the Foucher derivative B. But then the Fourier derivative being injective and being between finite dimensional spaces, it's invertible with a bounded inverse. And so basically, you prove the result by this simple trick. Let me mention that here, I'm just repeating the statement. Let me mention that many more precise estimates, so not this abstract estimate in this very abstract framework, but many Lipschitz stability estimates for the Calderon problem or for other inverse problems for PD. Inverse problems for PD have been derived under many different assumptions on W, either piecewise linear, piecewise constant, piecewise anisotropic. In many of these cases, these estimates have been found. And all of these results, as soon as you specialize to particular problems, come with specific estimates on how this constant C behaves depending on W and especially depending on the dimension of W. Of W. However, the annoying part of all these estimates is that they all require the full Neumann-to-degree claim map. So here you always have the full knowledge of f of x. And this is a little bit counterintuitive since you are trying to recover only finitely many degrees of freedom because we are assuming that x lives in a finite dimensional subspace. So it's a little bit counterintuitive that you still recover, you still need, sorry. Cover, you still need infinite-dimensional measurements on the right. So, in the case of the Neumann-to-Diraclema, this would still mean that I have to input all possible input currents, at least infinitely many of them, and measure all the corresponding potentials. So the question is, why infinite measurements? Do we really need them? And the answer is no, and that's what I'm going to show you next. And first of all, we need to understand how to model those finite measurements. How to model those finite measurements, and the easiest way to model them. If in the infinite-dimensional setup I require f of x to belong to an infinite-dimensional Banach space Y, in the finite measurement setup, I require the knowledge only of a certain projection of f of x. So these projections are these maps qn, and these qn's will be finite rank operators, and they will, as n goes to infinity, they will need to converge to the identity of y. Converge to the identity of Y in a suitable sense. Just to avoid technicalities, I will skip the precise definition. I will just mention two examples. The first example is when Y is a Hilbert space. And so in this case, it's just enough to consider Qn as the orthogonal projection onto the first capital N component elements of this Hilbert space. So, for example, if this ultronormal basis is the Fourier basis, Qn is just a low-pass filter. Is just a low-pass filter. So it means that I only need to measure my unknown up to a certain frequency. The higher frequencies, I can discard them because they're not useful. However, in the case like in the Calderon problem, where my space Y is not a Hilbert space, but it's a space of operators. So this will be in general just a Banach space. In that case, a natural structure is to proceed as follows. Structure is to proceed as follows. Basically, you consider this kind of projections where you project first in the domain and then in the codomain. So, basically, instead of applying all possible input currents, you apply those only up to frequency capital N, and then you measure the corresponding potential only up to frequency capital N. So, that's the idea. Of course, if one wants to use more, if you want to put a Hilbert structure on the operator's Structure on the operator space, then of course it would be possible to use some kind of projections like in this case. But this is the more natural projections you can do in this setup. All right, so the result in this case is relatively simple. And basically, it says that anytime you have a Lipschitz stability in the infinite-dimensional measurement setup, so with infinitely many measurements, and basically these kind of estimates arise as a consequence of both. Estimates arise as a consequence of the previous results that I mentioned. So every time you have this, then for free, you get basically the same estimate up to a slightly larger constant here, but with finitely many measurements. So instead of measuring the full f of x, you just need to measure qn of f of x. So this finite dimensional projection. And the number n, which can be read as the number of measurements, is not like it's not. Like it's not, I mean, it's explicitly constructed. It's not that we know it exists, but I don't know how large it is. It's explicitly constructed, and it has to satisfy this inequality. And this inequality can actually be checked in the examples. And I will show you what happens, for example, in the Calderon problem if you check this inequality here. The way I see this result is basically an sampling theorem, so an extension of the sampling theorem for An extension of the sampling theorem for non-linear and deep posed problems. Why? Because somehow the classical, if you want, the Shannon-Nyquis sampling theorem is telling us that if you have a bun-limited function, so you have a strong a priori information on what you're looking for, then it's enough to sample them at precise points. Of course, those are infinite-dimensional spaces, so you still need infinite-dimensional measurements. But in practice, in principle, if you principle it if you have a a simple space you just need to measure at a finite number of points and it tells you exactly how far these points need to be and this is a result in the similar spirit because it says that if I start with an object that lives in a low dimensional space then I can recover it from a finite number of measurements and I tell you exactly how many of these measurements I need and which and which measurements I need. Which measurements I need. Let me mention that there are a few related results to this one, both in the deterministic settings, so without any probability involved, so again with deterministic measurements, and also in the statistical settings, even though in that scenario the results look a little bit different. And Richard discussed about spoke about those on Monday. And in the statistical setting, usually the number of measurements goes to infinity, and you are guaranteed. infinity and you are guaranteed that somehow your posterior distribution concentrates along concentrates on your original unknown x dagger. Let's say those are the typical results that you see in the statistical setting here. The results is somehow similar because we always aim again at finding something with finite measurements, even though the precise estimates are slightly different. Let me see how this can be applied to the Calderon problem as an example. Can be applied to the Calderon problem as an example. Here, what happens is that basically, I'm just repeating what I discussed before, exactly the same notation, more or less, hopefully. And the oops, what did I do? Wrong direction, sorry. So the corollary you get in this case is the following. Anytime you have a ellipsis stability estimate for the Calderon problem, and many of these estimates have been actually obtained in the literature with many. Actually, obtained in the literature with many different types of priors, so many different finite-dimensional subspaces, W's. And every time you have these kinds of estimates with infinitely many measurements, so with the full Neumann-to-Dirucle map, then you get the same estimate with a slightly larger constant with a finite number of measurements. Again, here the finite number of measurements means that you don't need to apply infinitely many currents, you need to apply only capital N. To apply only capital N of these currents, and you need to measure the corresponding potentials only up to a certain frequency, let's say. In the specific case where you actually consider the unit disk, so the example I showed at the beginning, and you take as orthonormal basis on the boundary, those trigonometric polynomials, you take Pn as the projection on the first capital N of those, well, here it's going to be 2n, but anyway, then the number of measurements it can be proven by using. It can be proven by using this inequality here. It can be proven that the number of measurements is of the order of c squared. So basically, it grows quadratically with the Lipschitz constant. And recall that for EIP, this Lipschitz constant behaves exponentially in the dimension of W's. So anyway, it's a bad behavior, but somehow there's nothing we can do about it because the inverse problem is originally severely posed. So it's perfectly consistent. Perfectly consistent, then as the dimension of the space increases, the situation becomes worse and worse, exponentially fast, let's say. And this is why these kind of approaches, these kind of methods work well, provided that those dimensions are actually small. So you need W to be really a low-dimensional space. So you need a relatively strong prior on your unknown. The second example is inverse scattering. Here I'm just Scattering. Here, I'm just slightly faster because I just want to show you the result without going into the details. Here it's an inverse medium problem. We want to recover N, the refractive index here, from the knowledge of the corresponding far-field pattern. The far-field pattern is computed for all possible incoming directions D. Here, the incoming field is just a playing wave in the direction D, and I measure the far-field pattern in... And I measure the far-field pattern in all possible directions x hat on the sphere S2. Here, k is fixed. So from the far-field pattern, we want to recover n. And in this case, basically, if you have again a finite dimensional subspace where n lives, instead of the theorem says that the corollary says that instead of measuring the far-field pattern for all possible incoming direction D of the incoming waves and all possible positions. Waves and all possible positions on the boundary, it's enough to measure these far-filled patterns at a finite number of incoming directions and positions on the boundary. So basically, you get a finite dimensional, finitely many measurements in this sense that you need to input only finitely many plane waves and you need to measure the corresponding far-field patterns only at finitely many points on the boundary. You see here these are. You see here, these are not exactly projections because those are sampled values. And so, to move from projections to samples, here we use the theory of reproducing kernel Huber spaces. And it's possible to do that because somehow these smoothness assumptions allow us to state that these sampling procedures are actually linear operators on suitable Hilbert spaces, and so basically they can be seen as projects. And so basically, they can be seen as projections. All right. So, the next bit is going beyond sampling. Going beyond sampling means try to use par CP. So, in this, in what we saw before, we just use a linear structure in W without assuming any additional structure on the function, on the object's x. However, it's possible that using compressed sensing should help us lower the number of measurements. Help us lower the number of measurements even further. So, this is just a very quick slide on classical compress sensing. Here we want to, we consider linear forward map from a Hilbert space to little L2, and it's defined by a family of functions psi L. So for each component, I take the colour product of my unknown U with a certain function psi L. The key assumption, of course, is the sparsity of U, and this is measured with respect to a finite family phi of. A finite family phi of j. So basically, u with respect to this family sparse, namely, it's represented by a few components here, and the sparsity is S. So the cardinality of this set, capital J. So the idea of compressing is that I do not have, if I have a nice forward map F, still I don't have to measure all of these color products here. I don't have to measure all these measurements. However, I'm okay even with a finite Even with a finite subsample, with a small subsample. So I take not all little L in L, but I choose a set capital L where I take my measurements. And I suppose the number of measurements is little m. So the main theorem here, I'm simplifying things very much. So I apologize for this. The main theorem says basically that if my measurements are substantially proportional to the sparsity of the unknown, then I can recover my unknown. Then I can recover my unknown by solving an L1 type minimization problem. So, recovery is possible if my measurements are proportional to the sparsity. Note that in the typical scenario, the number of measurements in the linear setting would be proportional to the number of unknowns, so to the dimension of sigma of W. Here, the dimension is replaced by the sparsity by using exactly the theory of compressed sensing and the price you pay. Sensing and the price you pay is first you solve an L1 type minimization problem, which is complicated to solve, and then you pay a price of log factors here. So, what happens if you want to apply compressed sensing for inverse problems in PDE? Well, it turns out that the main ingredient here is the relationship between these two families, the families of the measurements, so the psi L, and the family of the sparsity, so the phi j's. This problem is very well understood and known. Is very well understood and known. For example, in the classical case where your image, say, is parsing a wavelet basis and you take Fourier measurements. In this case, this is very well understood. Our setup is slightly different because we do not have the Fourier basis. We have a more complicated problem. And in one example, we studied the linearized Galfa-Calderon problem, which is very similar to the Calderon problem I discussed before. I don't want to go into the details, but for that particular Into the details, but for that particular inverse problem, instead of measuring infinitely many currents, instead of measuring a finite number of currents that is somehow proportional to the dimension of the space, we are able to reduce the number of measurements actually to the sparsity of the potential in the wavelet basis. Here, provided the measurements are chosen at random. And so, here, of course, the statistical aspects of compressed sensing do play an important role, however much. play an important role. However, I'm afraid I don't have time to go into those details. The next step is considering nonlinear priors, which means that so far everything was linear. We had a linear subspace and we have sparsity in a linear dictionary. Can we move to non-linear priors? And of course, non-linear spaces are just manifolds. So we just consider instead of a space w, Space W, we consider a finite dimensional subspace. Instead of considering a linear subspace W, we consider M to be an n-dimensional manifold inside my space X. So this will be a low-dimensional space in a high-dimensional space X. This is just a technical assumption, but in the interest of time, I will just keep comments. And of course, many examples arise here because, for example, in the Because, for example, in the vector in the linear case, you could have piecewise constant functions defined on a finite part, on a sorry, on a known partition, on a fixed partition. Here, as soon as you allow non-linearity in the manifolds, you can also move the partition. So, for example, you can consider indicator functions on balls with variable centers, radii, and intensities, and you can have as many balls as you like. You can also consider instead of balls like Also, consider instead of balls like polygons, like simplexes, or and I will come to this to this case at the end, you can also consider a generative prior that are very common nowadays. So instead of assuming a priori that your unknown lives in a known manifold, you assume you learn this prior by using, for example, a GAN or an open coder, which basically allows you to Basically, allows you to represent your complicated object X with a few parameters. And again, this gives rise in general to a manifold in your space X. So what happens in the general case of a manifold? Basically, the result is exactly as before. So here, that's why I'm going to be relatively quick on this slide. It's still a long paper. It's still very technical. Many things have to be taken care of because the many. Taken care of because the manifolds means that you need to take about to consider the change of charts and everything. But still, once you write everything down, basically the result is as before. The two assumptions on the injectivity of F and on the Fresh derivatives are simply replaced by still the injective global injectivity of the map F and the injectivity of the differential. So instead of considering the Fourier derivative, here we consider the. Fresh derivative, here we consider the injectivity of the differential. And the statement in this case is that still you need Lipschitz stability. So even if or holder stability depending on the regularity of the manifold. So even in the manifold case, if you start from an inverse problem that is a very little posed, you can reach older or Lipschitz stability if you assume that your unknowns live in a finite dimensional manifold under these assumptions. These assumptions. We have exactly the analogous result with finite measurements. So here you can put qn of f of x, qn of f of y as before. I just didn't want to go through it, to go through the details again. And just on the technical side, there is an important bit because in the previous result, we used the convexity of k because the convexity was used to go from x1 to x2. Basically, we consider the segment between x1 and x2. Between x1 and x2, and along this segment, we approximated with this function with the Frechet derivative. Here, this is not possible, and you have to find a workaround. It's not possible because as soon as you have a manifold, this argument, the manifold, somehow the complexity is lost. All right, so the final step is going exactly towards machine learning approach in the case when the manifold is not known. And this comes the auto-encoder business, let's say. Autoencoder business, let's say. So, what do you do? What do we do in this case? This is still a work in progress, so I will just mention a few things. What we would like to do is somehow to obtain a result of this kind when M is a manifold generated by a note encoder. So let's see. Not necessarily a note encoder, by some kind of generative model for a prior. So let can. So, that can be an odd encoder, but not necessarily. So, just to mention what a generative model is: well, this will just take many object X that are of interest in my space, capital X, and we'll try to understand the structure of those objects. So, we'll try to understand whether there is an intrinsic manifold that can represent them all. So, like in this. Represent them all. So, like in this website here, it's very famous. You can just generate random pictures, random faces. So, from the mathematical point of view, basically, this is just a low, it can represent, this allows us to represent a high-dimensional object that you see on the right with a low-dimensional vector here with a generator of G. And basically, this can be seen as the parameterization of the manifold in some sense. So, this will be just the inverse of a chart of the manifold. Of a chart of the manifold. This is a low-dimensional object, and you generate a high-dimensional object. Why this helps in solving inverse problems? Well, in the classical approach of solving inverse problems, you look directly for the x here. If you know that x comes from a generative model, then you can write the inverse problems in this way. And instead of recovering x, you just need to recover z. So it's exactly as we did before when we said x lives in a manifold. So here I'm just making. So, here I'm just making explicit this parameterization of the manifold. There are many, many papers on this topic. So, I apologize if I'm not citing, providing a comprehensive list of references. I'm just providing one. And you see, again, in the Calderon problem, if this is the ground truth, and if you solve without using a strong prior, if you solve this problem just looking for an X. Just looking for an X here, you just recover a low-dimensional approximation of these functions here, which is exactly what we expect, and that's what we get. If, however, you use a strong prior like this one, which somehow encodes the fact that you know that you are looking for lungs somehow, so you know there's a strong structure inside, then the reconstruction would be much better. So, that's the idea. So, the question is, can we put this into the frame? Can we put this into the framework we discussed before of stability? And so, to do this, we need to go a little bit more into the details of how these networks are constructed. Usually, the architecture of these networks is divided into structured as follows. In the first layer, which is this one, you have a fully connected layer. So, basically, you just have a linear map, an affine map here that transforms your low-dimensional space into a much higher-dimensional space here. And then you have Dimensional space here, and then you apply a usual non-linearity. And next, all the others are typically convolutional layers. So in each step, you apply convolutions. Usually, the convolution is done with a stride here. So basically, you move slowly the filter here in order to expand the vectors. More precisely, let's see how these convolutions behave because somehow this will be key to understanding how. This will be key to understanding the structure of our generator. If you look at these pictures, you see that at the beginning, so if you move horizontally here, you see the number of channels. So here you will have 1024 images that are 4x4 here in the first layer. And then you move up and you reduce the number of channels, you divide it by two, and you increase the resolution of those images. So when you go and then you keep going. So when you go and then you keep going at the end you will have only one image when here it will be three you see free because it's a color image with high resolution. So that's what happens and you see it here in the structure of the generator. When you keep going the number of channels decreases up to one and the resolution, so the number the resolution of these spaces, so the dimension of these spaces increases. So alpha one, alpha two, and becomes bigger and bigger. And becomes bigger and bigger. Now, our aim is writing this in a continuous setting because we want to apply this in infinite-dimensional spaces. So, we want to do exactly the same thing in a continuous setting. We are not the first ones to study convolutional networks and in general neural networks in infinite dimensional spaces. Here I put two references. The main idea, I do not have time to go into the details, but the main idea is that, of course, the number of channels remain the same. Number of channels remain the same, the non-linearities here remain the same. What you need to change is these discretizations here. Instead of using just simple discretizations as in the discrete case, here we use a multi-resolution analysis coming from a wavelet basis. So basically, these VJs are nothing but the multi-resolutional, the spaces that are in a multi-resolutional analysis. So every time you go up in the layers, Up in the layers, every time you go forward, these pieces will be of finer and finer scales. So, this is just a technical slide showing that to make these things work, every time you need to project onto the right spaces. Otherwise, as soon as you convolve, you would go out immediately from the right space VJs. So, every time you need to project to make this work, but this is a technical step. And the key property of And the key property of all this construction is injectivity of the map G. Why do we need injectivity? Well, if we want a manifold, of course, if all these maps are smooth, then smoothness is easy. However, if we want a manifold, this parameterization, the key property is the injectivity of this map G. So our main result is how to obtain this injectivity. And it's not as easy because every time you need to do these projections, Time you need to do these projections, and every time the number of channels decreases. So, basically, you are taking smaller and smaller spaces because the number of channels decreases. And so, somehow, injectivity in principle may be lost. So, the decrease in the number of channels has to be compensated by the higher scales that you put in the multi-resolutional analysis. So, this has to be studied. It's not that simple, but that's the result is as follows, basically. The result is as follows: basically, that if f is injective, so the first map of course has to be injective, the fully connected step, then the nonlinearity has to be injective, but there are many non-linearities that are injective, so that's okay. And then the third condition is some sort of linear independence on the convolutional filters. I do not have time to go into the details, but basically in each convolutional steps, the convolutional filters that you use need to be. Convolutional filters that you use need to be somehow linearly independent. And the fourth condition is that all the parameters that are involved here, namely the stride in the convolutions and the number of channels and how these spaces in the multi-resolution analysis change, all this has to be well written somehow. All these dependencies need to be correct. Once you have this, then you get indeed injectivity of the full map. So basically, once you have this, you can still Have this, you can still apply the results, the Lipschitz stability with infinite or finite measurements with a manifold that is generated by a neural network of this architecture. All right, my time is up. Let me just conclude with this simple slide that just says that in my perspective, it's very useful if in inverse problems for PDE, we use techniques that come from different domains. And in this talk, I just use this. I just discussed some approaches that are based on sampling, compressed sensing, and machine learning, even though there is no learning part, it's only the architecture part. And of course, in many of these problems, statistics does play a crucial role. All right. Thank you very much.