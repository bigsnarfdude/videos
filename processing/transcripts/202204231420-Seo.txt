My name is Msun Ha. I'm from Joseon University and which is Wangzhou is located here, you know, the Seoul is here, and the Wangzhou is here, and Wangzhou is nearby, the Kentech basically, if Nazhou is nearby, it's a Wangzhou basis. So I am now in the physics education department, but I'm as I mentioned that I specialise in statistical physics and the modeling things of today. And the modeling things today, I would like to talk about the temperamental issue. So, why I am talking about this is like that part, a little bit delayed, this workshop. And then it's my university has some special like building like that, is like long this way. And this is the big mountain in Wangzhou. So, this is a winter season, and this is not you. And I'm also work on a little bit of synchronization, that's why I have this one. This one and then I honor like a thank you for the three organizers for being here. So Amari is this one? Okay, so I forgot actually put on this like the view graph the other day. So that's why I'm showing. And basically I used to be like UW or UC or CEO, something like that. I still like that. And then the summer. Do you like that? And then the summer normally I was in Canada, and then winter normally in the Seattle, something like that. I realized that too. That's why I showed you. And then my specialty or interest is like how to apply the minimum model things. That is the issue. So today I will talk about that way. And since I am in physics education department, so I can't forget that. So, since I'm in the physics education department, you know, at the beginning I always say, you know, thinking is happy, but doing something is very tough. And also, like, you know, I like this thing is open to like word, but I realize that this is very. But I realize that this is where it's much better depending on the where you are, you know. Like here, Canadian, is hockey stee is popular, so they know how we eat it. And we're also like this, it's caribou, or the reindeer's eyes is matter. Or husky, or the hero is Seattle. Like at the time, I was in Seattle Airport to go back to Korea, and then this Chinook is like Samoa is important there somehow as a school friend. Data somehow. It's a school friend should know this. So maybe the terminology is sometimes better. So this workshop is quite interesting, but sometimes I couldn't follow well because I'm not good at that brain networking somehow. But I will still keep learning that part. So basically, but I'm focused on like Avalanche or the 1D system or some kind of Zamming or network. Kind of Zamming or network things, I try to understand like using this kind of word complexity or the universality. And I prefer kind of like a non-banan current thing. I'm interested in that. And then I look at my title for the paper, and it's more like a dynamic or process or network, somehow tempered. So that's why I put on this. So I tried to meet. So I tried to maybe find out the link to this workshop with my model. And I got this one email reminder says my paper is like cited. And this paper and I will get these two. My paper is later on is related. And one more paper I will find to do to that paper. Actually the we discus discuss about this issue, you know. This is about this issue, you know, critical brain hypothesis thing issue is I I didn't I totally didn't know this paper before, but I look at that means like there is a universal power, but it's not critical, something like that happens. So that's my starting part of this my talk. So basically, I'm talking about some simple modern network things, and actually that is based on popular active activities. Popular active-driven model. So maybe Naoki knows about this model. But we put the memory. And that define the memory. That is another big issue. So I will explain how I just say it's memory here. And why? Why temper? Or what is temporary the same as time going or time obligation? That kind of thing I have to explain. And also, like, active drift, temper, net. So, like, activity temporal network, and for what, how to modify this one is interesting, and then how to like the analysis. So, because I'm interested in special temporal method means like extended set, finite size scaling or dynamic scaling people usually call. So, it's our and then I'll put the like random and then look at the how to draw the like the uh Distinct nodes or giant clusters, I will measure that way. And I will explain a little bit more and then summarize, and then with the pushion maybe. So, let me go on. And you know well about these. And then at the beginning of this work, we are focused on this kind of social network type things. So I will tell, and this is like time. So there are many data sets like There are many data sets like a lot, detail. So and maybe it's like some conference they meet each other, then they connect it, something like that. So you have like time unit and then there are connection. So basically it's an end node, but it's not always connected somehow. So I will explain that way. So this is normally you got this kind of static recall. This has like a two friend, this one is like Brand, this one is like double link, something like that. But templar is like slightly different, like that. Okay, so basically depending on what kind of window size you are looking at, it's like important somehow. And I would look at the paper normally, it's like time manufacturing issue. Two lots. It's kind of like similar when you like fit the data, how to be in. The data, how to be in the size. This is important I learned. So I will explain with this window-like time issue. So to last key, you can call here static. This is highly dynamic and in between is might be it might be there is like optimal window size depending on the your system. Be nice. So I will explain this way. So Explain this way. So, if you think about how make like the typical temporal network, and people already think about the node itself has a different activity. So, activity distribution is important. Kind of like the fitness to make, or the natural frequency is like part of the model. And how to connect, like Like is like measure the distance or the how like long we talk each other, something like that. That part is like considered and put with memory and without memory. So I will explain more. And then memory here is like ever connect or never connect. I will explain everything. So I say new and old. So if you have a time, you want to call the new friend or the old. The new friend or the old friend, something like that. So at the beginning, is there the paper? I'm not sure this is the first one, but it's like pretty much the first one, I guess. So this is like they measure their empirical data pass on scale and power and something like that. And then this time series, they like propose, and this is what we know, and they're done. And there's another issue here is like uh they put on the diffusion and then is consider which one is important, structure or the timing. And depending on them. And this is kind of related to that last plot I did or like a neural firing is also like people keep using. I realize that. But today I just focus on three papers, which is I work on this. Which is, I work on this like home through, and this is like how Tim is now is like different. Like K is with nearby kiosk, she air now. But at the time, this is like series paper got together because this is like at the beginning, it's like 2015 was like kind of become popular. So the whole So the Petro Holme mentioned that there is only one time to study that kind of model would be interesting. And then this is like special topical like paper they collect. So that time we start this idea and then we then paper later. And then second paper is PRE, is more likely model focus on and the lap last part is like Last part is like uh to learn the like academic dynamics and then look at some different. But today I I only focus on this model issue. So how to like modify with the memory. So basically each node has own activity and with the power law with power law exponent, then that activity is similar to the strength. So it's it So it's later on it's become this comma is like formed of strength, distribution. And the other one is linked is, as I mentioned, that one node has like measure some strength add up and then they look at the like which one is connected. So once it's like chosen activity and it choose the other day or from the new friend, new node or that different like one of all Different like one of old one. So, this is with the referencer attachment issue. So, this is like a sorry again. I'll put the wrong button on the end. So, as a candle, and this is basically this I is activated due to this AI is bigger than any others. Bigger than any others, and the more probable is selected, and then the other one is like either this way and that way. Bad times like zero means random. So one minus like new is like old with the different like the probability. So beta is zero mean memories, the memory, there is no memory, so everyone is the same probability, and beta with 0.5 is with memory. With 0.5 is with memory. So, as I mentioned, that this is coming from the heterogeneity activity, and this one is how we make a tie and how network is developed different way. So, then basically, due to these two exponents, your gamma k is changed with in terms of like 2.5 or in terms of like 2.5 or 4.0 which means you have a different like structure structure for how low as well as and look at the like temporal behavior so that's why i considered the random so basically static is all connected like you can go everywhere but here is like depending on which window it is there's no way to go and then stay for a while so So basically if you look at the like decide which size of TW you might got different like the number of things each node you can visit. So if we measure sorry here yeah if we like make a window like doesn't matter this is not it's a different sample issue you can measure and You can measure, and we got this kind of view is like sorry, TW is one and this is n. This is like a well-known user for static, and in between there you can argue with some dynamic scaling answers with the power. And this is the I'm sorry, this is number should be two. So case one is with the method comma is same. The activity distribution is the same, but it's like that is different. And then has little difference, but you hardly see somehow. So you can look at the like the degree distribution. Sorry. How many menu do you have? How many minutes do you add? So let me oh so one part is like the power is different but later on it looks similar at the end. So I will explain the main thing is like how many links you can make if you enlarge like your window. So this is kind of important point This is a kind of important quantity. How many distinguished nodes you may have if you put the TW? So that is the issue you might think about the largest giant connected component sizes change so that you can like this scale different like M is like cluster last cluster size divide by a number of distinct nodes and window divide by n then And window divide by n, then you might have a minimum slightly. So red curve is memory disk attached zero, and then blue, blue one is 0.0 with the memory. So then you might see there are kind of three regimes, only regime, middle, and the last part is static. So this is somehow interesting, but it's not that clean. So we some We some summary have, okay, there's something minimum is slice delay. So, what? So we measure slice in different way and care about the scaling unjust. Then we realize that there is three regions clearly with the different power law behavior. In the middle, it has its own exponent, and that relates to what we call the temporal population issue. So, if you like. So if you like the properly divide by the N D, which is like depending on the time window size, then you have a minimum as I showed before like collapse very well somehow. So, but at the end is the same. This is like the last static part, the initial part, and there are exponent eta is kind of initial, it's foreign to actually, I didn't explain in detail, but you might look at the paper and you will see. So, basically. So basically both case is with the if you divide by here is like I T and tau is T divide by N and there are extra power but what if you you put divide by the N V here then it's like without skip they did the other you didn't put any other thing and then well collapse both case realize that is Both case realize that it's ND is important, which means it's like the number of sizes differently grows, I guess. So, so we little bit change the measure for this thing, it's like a type of growing weight, and then it looks similarly have that kind of thing. So, what I want to like deliver you guys is like, you know, as I mentioned, that I showed that paper. I showed that paper without special criticality. Basically, there is like how low I put in already, gamma. And then so the beta is a little bit changed due to that, related to the gamma k is in terms of gamma and beta. And then you know about the size of network and the population issue. Then giant clusters show that power itself, basically, I guess. Basically, I guess. So, depending on the window, it's too big or too low, then you didn't see that way, but in between, it follows very well. So, what I've done is here is like maybe this paper I didn't know until yesterday, and then I realized that there is an issue here, similar thing. So, maybe it's now explained or if you know well. And the other thing is, like, I'm kind of looking at the Like, I'm kind of look at the data issue, then it's still people interested that way. I'm done. Yeah, thank you. Do you do like the coffee things I slice the nervous today? No. So that's why I All right. Thank you very much. Thank you. We have time for questions. Yeah. Yeah, we'll go for Yeah, well, for you. Yeah. So then, what is the girl or mentality? How does this happen for us? A message A just It's a strength basically, so that means like uh if you activate, once it's activate, then you can make a link. It's kind of history, yeah, only born something, but it's like get rid of after time Windows I just changed and it's it's like delete all earlier link issue. I mean, do you I mean, do you pick up the one node with in terms of that activity distribution function? So, yeah, A equals 3, but there is power. So, that if I have like a half type of like A is large or small, that is different, like the A is assigned to every note. Every node. AI is the everyone has owned like fitness or the popularity or the activity. I mean, but outgoing type. So I want to go like hubia and then want to talk first, something like that. So yeah, thanks, Dandy. I'm curious about how the memory and new models change in dynamical properties, like for example if you Dynamical property, like for example, if you consider imaging random modes, let's say random modes on your temporary model, 3D model, or you have DVD dynamics. So does this have accelerated or decelerate dynamics? Basically memory it means free for the meet non-node, already connected node, which means it's like the bonding is strong, but the number of nodes itself is small. Is small. So that is, they are kind of hold until make certain thing, and then everyone knows the same, then you make a new node or something like that. So this makes me think that the dynamics would be slowed down by this because they have less connections, stronger connections for longer time. So the that the speed of slowness is I I'm a little bit confused because the size is small so that it value itself is like smo small. Steph is like small, but divide by that thing is fast within the community, I guess. That's true. So, so I slice different language sometimes because big or small or strong and because this is like not single connection, like multiple connection is possible. I met you thirty times means I pick you more than others. You more than others, something like that. So that means like plus itself, size is small, but it's like the only difference. Yeah, but when we talk about random hooks or processes, and probably the most exciting thing is how the information or the random expresses the entire network or the entire network. So the if you look at local say part, then yes I mean memory makes things local, so the within local range, yes I mean the Within local range, yes, I mean uh it may make uh USA even faster. But when it comes to we ask a question like what is the time uh before the USA spreads out to be your large part of the world. Yeah, that's but I then if I understand correctly, then it's like you know, I stuck somewhere for a while. That's the issue. I because like the window is so small, then I have only two friends and the backup board and things like that. So never problem. So never propagate anywhere, but until certain time. But it's weight enough, then it eventually goes the same way. So question is what time is make that kind of thing? So beta is matter, but we didn't actually automatically solve it exactly, but we know how to control. So that means is depending on the issue, you want to propagate fast, everyone happy, or Fast, everyone's happy, or something like that, and then you choose a different way. So, so that preference is issue, I guess, is interesting somehow, but I know all the people calculate the different quantities, so I couldn't compare the detail. But this issue is like I had like three people together, something like that is tweak issue probably also is there too. Do we have any questions from the remote participants? Questions from the remote participants? That's not the case. Okay, let's thank Michon again. Okay, now we have slight changes.