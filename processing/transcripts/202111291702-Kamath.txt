Thank you to the organizers from Luka Gabor for organizing this. It's really cool to be here with all these folks. And I'm going to tell you about efficient mean estimation with pure differential privacy via sum of squares exponential mechanism. I expect you to have no idea what any of those terms mean, except maybe efficient mean estimation. We'll get into everything later. I set your alarm for 20 to 3. Yeah, this is joint work with Sam Hopkins, who's fantastic, and my very, very talented master's student, Mahbad Majid. Let me say that private estimation is. Let me say that private estimation has been a sort of topic of my research for the last maybe three years or so. And in particular, the question I'm going to talk to you about today, it's a question that's been on my mind for about two years now, so I'm really happy to share this with you. The paper is up on archive as of 23 hours ago, so it's fresh off the presses. So, yeah, as a sort of sneak preview as to what we're going to see today, I'm going to tell you about an all-in-one private mean estimator. That is, as the title suggests, something which is That is, as the title suggests, something which is efficient, private, and with near-optimal sample complexity. But also, some other desiderata that people really look for in these settings, including robustness with sub-Gaussian rates as well. And more broadly, like I said, there's a specific problem that I've really been focused on, but I think there's a more broad picture that I'm going to allude to here. This addresses a fundamental deficiency in our understanding of DP estimation, differentially private estimation. And I think we really didn't know how to do efficient. didn't know how to do efficient statistics in high dimensions under privacy. So I think this really helps us understand a bit better. And this is yet another success story of the algorithmic high-dimensional statistics paradigm, which has really done very, very well in the last couple of years. As Salil mentioned earlier, like algorithmic robust statistics. This is really the next work in that line in my mind. And more broadly, we also give a framework for private estimation using SOS optimization. SOS optimization, which I'm not actually going to say too much about the specifics of this framework in SOS optimization. But really, I think this is a big chance for a lot of different communities to work on one type of area. In particular, hopefully this invites the robust statistics as well as the sum of squares community to the table. So I hope any of you in those areas or anyone watching takes note. So let's start with privacy, first of all. Like I said, I didn't expect you to know any of the words in the title. And generally, when we talk about privacy, we ask, And generally, when we talk about privacy, we ask how do we ensure that statistics don't leak information about individual data points. Now, I could give you a few slides about, like, oh, here's his machine learning model that leaks stuff about his training data, but let's just go straight to the sort of definition of what sort of privacy notion we're going to use. And we're going to use something called differential privacy, which is very well accepted and used right now. I'm going to tell you a sort of game which you can think of differential privacy as being. I'll flash the definition. The definition isn't really going to be too important, but this is the picture you should have in mind. We imagine In mind, we imagine that there are two data sets which differ in one entry. We call these neighboring data sets. One of these two neighboring data sets is fed into an algorithm. That algorithm produces an output. An adversary, looking at the output of this algorithm, has to determine which one was fed into it, was it x or was it x prime. If we say that the adversary can't really decide whether it was x or x prime with chance much better than sort of random guessing, then we say the algorithm is differentially. Then we say the algorithm is differentially private. That's sort of very informal, but that's sort of the idea you should have. They can't tell whether it was x or x prime. And in particular, if they can't even tell if your data point within the data set at all, then they can't really tell anything more substantiative about your data point as well. So that's the idea you should have in your mind. And here is the definition of what it means for an algorithm formally to be differentially private. We say it's epsilon delta differentially private if we're all data sets which it differ on. Data sets which differ on one entry, we have the following kind of similarity of the output distribution of the algorithm on these neighboring data sets. So we say that the probability of all events is similar in some sense under x and x prime being fed into the algorithm. And a little bit more formally, it's like you have this multiplicative term and this additive term. But again, the specific definition is not really too, too important for this talk. Just that people know. Just that people know it, people like it, it's accepted. Cool. And generally, in terms of parameters, what should you think of epsilon and delta as being? This quantitative definition. Generally, think of epsilon as being a small constant, like 1, and delta as being something which is pretty small, like sort of cryptographically small is often how we describe it, but at least less than 1 over n. Kind of a very, very small term. Okay, so this is epsilon delta differential privacy. And one interpretation of it is that One interpretation of it is that we can think of an algorithm being differentially private as saying that the privacy loss random variable, whatever that is, I'm not going to say it's like some sort of leakage, is bounded by epsilon with probability 1 minus delta. That's one way of viewing it. And so there's two primary notions of differential privacy that people care about, which we're going to distinguish today, which is delta equals zero, pure differential privacy as it's sometimes called, where this is like a very strong bound. You say that the leakage is always The leakage is always bounded by epsilon with probability 1. This is in comparison and contrast with approximate differential privacy, which is any delta greater than 0. And this sort of allows some small, potentially small chance of privacy leakage. And so these are qualitatively different notions. There's things you can do under approximate differential privacy that you can't under pure differential privacy. And so it's easier to design an algorithm for approx DP, but we're going to be focused mostly today on pure DP. Mostly today on pure DP. Because let me just sort of comment on the cost of privacy under Prox DP. This is often going to be polylog 1 over delta. And you might think, okay, this is not so bad. But if you, like I said, choose delta to be cryptographically small, then this is going to manifest as additional polynomial factors and something you'd really want to avoid. So but we're going to go for the best possible thing we can get, which is pure differential privacy, so that we don't have to even worry about this cost at all. So that we don't have to even worry about this cost at all. Cool. So that's differential privacy. And our goal is pure differential privacy. I'll say that this is well accepted in both theory and practice. In theory, they won the Girdle Prize in 2017 for this work. And in practice, it's deployed in many organizations, including, for example, it was deployed in the 2020 U.S. Census. Anyone who was in the United States on April 1st, 2020, had differential privacy applied to your data. So, yeah. Let's talk about the specific setting that. Let's talk about the specific setting that we're going to be focused on, privacy and statistics. And for the first part of this, this will look pretty familiar. So we imagine that there's some unknown distribution d. Via random sampling, we generate some data set x1 through xn, and this is fed into an algorithm which produces an output y, which has some utility, which is measured with respect to the unknown distribution, not with respect to this particular data set. So we're trying to learn something about the distribution. Pretty familiar, nothing fancy so far. And there's two data. R. And there's two desiderata. One is that we want the algorithm to be accurate. This is the same as we always want. And today we're going to be focusing on estimating the mean of the distribution p. The new thing is that we're going to want the algorithm to be private always. And so this is kind of like a worst case over whatever data set we're given in, whether it's like a zero probability, like a very, very low probability data set. We don't really want to compromise. So you'll notice that it's like stochastic accuracy, but always we want privacy. We want privacy. So that's kind of the setup. Yeah, and the general question we ask is: what's the additional cost of privacy? How much more data do we need to guarantee two in addition to one? Questions about the setup? Cool. All right, let's talk about mean estimation, a problem near and dear to my heart. So we're going to be focused on the following type of setting. Suppose we're given x1 through xn from some distribution p with bounded covariance. We want to output a mu hat which is close to Put a Î¼ hat which is close to the true mean of the distribution with probability 99%. I'm hiding a few dependents on parameters here just to keep things simple for now. But in this case, if you just want, say, 99% accuracy or probability of success, then the empirical mean works really, really well in this setting. So like the basic, most basic possible algorithm you could possibly want. And in particular, linearly many samples suffice. Now, I'll comment that fancier techniques. I'll comment that fancier techniques allow you to get a success probability one minus delta with this number of samples. I guess alpha is the sort of, if you replace this with alpha, this sort of a familiar term. This was done in Gabor's work with Shahar Mendelssohn a few years ago, and then this fusion work. I'll touch on this a bit later in the talk. But for now, at least for the time being, we're just going to focus on, say, 99% probability of success and constant accuracy, just for simplicity. And the last point I want to make before I continue is that we're going to pretend we start with a coarse estimate of the mean. This is for technical reasons that it's kind of, I can talk about this offline, but suppose we just start with some sort of warm start. We have some estimate of the mean, which is accurate up to, say, square root of the dimension, and we're going to go from there. Does the setup make your sense? Cool. I'll revisit this assumption. It's not really strictly necessary, but you need something. It's not really strictly necessary, but you need something. It's a technical detail, which we'll get into. Okay, so I'm going to tell you three deficient algorithms before we get to our algorithm. And with apologies to Tolstoy, every flawed algorithm we'll see is flawed in its own way, a different way. So the first algorithm will tell you first differential privacy 101. If you've ever heard of differential privacy, you've probably seen this algorithm before, which is the Laplace mechanism, but just in case. So let's start with f being some sort of vector-valued function of interest. Vector-valued function of interest, for example, like some non-private mean estimation algorithm. So it takes in a data set of size n, outputs a vector of size d. And a key quantity when doing differentially private analysis is what's known as the sensitivity of a function. So this might look kind of intimidating. This is the Hemming distance here. But just to parse it in words, you can maybe mull over that definition. It basically asks, how much can the function change if you've modified just one data point in your data set? So you look at the max possible. So, you look at the max possible value it could change if you just add or remove one point or swap one point, and then you take the L1 distance of that difference because it's a vector. So, that's what's known as the L1 sensitivity, pretty intuitive notion. And the claim is the most basic algorithm in differential privacy is the Laplace mechanism, where you take your non-private statistic and then you add Laplace noise to each coordinate of its output, and that'll be differentially private. And yeah, the add Laplace noise to each coordinate proportional to. Laplace noise to each coordinate proportional to the L1 sensitivity. Cool. Laplace mechanism. Sorry, what's Laplace noise mechanism? Oh, sure, sorry, I had a picture, but I removed it. It's basically the exponential distribution, except two-sided. So tails decay as, you know, e to the minus x. Two-sided to make it symmetric. Cool, good question. Any other questions? All right, yeah, and so let's see what happens if we try to apply the Laplace mechanism. What happens if we try to apply the Laplace mechanism to this problem? Well, the first problem is probably, if you think about it a little bit, what happens when you take the empirical mean? Well, just move one point out to infinity, you have infinite sensitivity. So the first kind of obvious thing we have to do is we have to clip our data, and then it'll be okay, so to speak. So we're going to average our clipped data, where in particular we're going to limit the sensitivity by clipping to an L2 ball of radius O of square root D. I'm hiding some other dependencies here, which aren't too major here, but yeah, we're going to. To major here, but yeah, we're going to clip our data to square root D, which should contain most of the data. There's going to be some bias that's introduced if you're looking at just a second-moment bound. But trust me when I say we can take care of this. I kind of did this in a previous work with Vic Run Signal, who's my postdoc, and a student, was previously a student of John Allman at Northeastern. Where do you center the both? Like I said, we kind of have a bound on the mean. We know it's kind of near the origin. It's O of root D of the origin. So just like take a... Because you have the initialization. Exactly, yeah. It's kind of already near it. Exactly, yeah, it's kind of already in your it, so just pick like a thousand square root D and clip it around that. Probably you'll get all the points, and yeah. So the resulting L1 sensitivity will be D o grand. This is not too hard to see just because the L2 sensitivity is going to be root D, normalization, and you get an extra root D when you convert from L2 to L1. And therefore, the Laplace mechanism would prescribe this sort of output. And if you kind of look at how much noise there is, how big is this noise in terms of the error? How big is this noise in terms of the error it introduces? It's going to be d over epsilon n times square root d, because we're in d dimensions and we're looking at the l2 error. So, in other words, if you want to cancel out the error that we get due to privacy, that's going to need n to be d to the 1.5. And this is bad, right? Because we had an algorithm before, which is order d. And now we have d to the 1.5. That's no good. So let's put this in the table. You can see that we have the Laplace mechanism. It's going to give strong privacy notion. It's going to be polynomial. Going to get a strong privacy notion, it's going to be polynomial time, but the sample complexity is no good. Cool. Questions about the first approach? Pretty standard, straightforward. The second approach is the Gaussian mechanism, which I think maybe you can guess where this is going, what this is going to be. So I'm just going to tell you the diffs. You add Gaussian noise instead of Laplace noise, naturally. The main differences are twofold. One is that you scale the noise according to the L2 sensitivity instead of the L1 sensitivity. Instead of the L1 sensitivity. So, this is one difference, and in particular, because the L2 sensitivity is going to be square root D, we said we clip it to a square root D ball, essentially you can save a square root D factor in the sample complexity as a result. This is just the natural relationship between L1 and L2 sensitivity. So, nothing too fancy that I've hidden there. So, that's great. We get the right sample complexity. The downside is that this only gives approximate DP instead of pure DP. So, like I said, this is a weakness. So like I said, this is a weakness. It's because Gaussian noise has lighter tails than Laplace noise, 1 to k e to the minus x squared. This is first of the e to the minus x. So it gives kind of qualitatively less privacy, and that's inherent to Gaussian noise. Let's go back to our table. I skipped some details, but essentially it gives the right sample complexity, the right running time, but it's going to be at proxy p. So that's deficient for our purposes. And let's move on to the third approach. Let's move on to the third approach. The third approach is going to be qualitatively different, and this is going to be kind of important to understanding our method. And it's going to be based on something called the exponential mechanism. So the exponential mechanism is a bit different in the sense that it's like a selection algorithm rather than a noise addition algorithm. And so we're going to basically define a bunch of objects, and we want to pick one based on which one has a high score. And we'll make this a bit more concrete, but imagine we have some set of objects. But imagine we have some set of objects. For example, as we're going to see in our instantiation, this is going to be a set of candidate points, which could be the mean. We're going to use a score function. The score function is going to take in a data set of size n, as well as an object, and tell you how good is this object with respect to this data set. So essentially, we're going to be using it to measure how high quality is this mean candidate. Is it a good mean or is it a bad mean? And the third thing, of course, we need the data set itself, naturally. Naturally. And the idea is we're going to output an object, one of these object, which approximately maximizes the quality over the set of possible objects. So for example, if this was non-private, then it would be super easy. Just compute the score of each object in the set, and then pick the best one. But that's not really private, I claim. But fortunately, something fairly simple is private. This is a beautiful algorithm, which is just the exponential mechanism, as it's called. You just sample a Q. As it's called, you just sample a q, and you sample an object q with probability proportional to e to the epsilon times the score function. So you can see this is basically picking things with higher scores, but you kind of smoothen things out based on how private you want to be. For example, if epsilon was zero, perfect privacy would just be uniform. So yeah, that's kind of what the exponential mechanism is. And let us, and the key thing is that this is going to give us a strong notion of privacy. Cool. Question about the exponential mechanism. Cool. Question about the exponential mechanism. I guess what assumption do you have about the score function? Yeah, this is something I kind of hit. You're going to need the score function with bounded sensitivity. Like you want if you change one of the data points, then it's not going to change much. I just kind of elided it to make it a bit more simple. But yeah, great point. I should have probably mentioned it. Oh, I see. So the scaling is going to be in the sensitivity. Yeah, yeah. It should be divided by delta, where delta is the sensitivity. Divided by delta, where delta is the sensitivity. But yeah, I didn't want to get too pinned down by notation. Cool, great question. Anything else? Cool. So let's talk about how we'll use the exponential mechanism here. And one of the kind of intuition is something which has been a pervasive idea nowadays in kind of modern robust statistics, which is kind of that the empirical mean should be close to the true mean in every 1D projection. I just want to draw you a quick picture of kind of A quick picture of kind of what this maybe looks like. So, like, a high-dimensional distribution we could frequently picture out of being like in a ball, all with like roughly, you know, square root d away from the mean. This is kind of like the picture of a Gaussian you've seen maybe many times. And my claim is that, you know, if you have the true, if you have the true mean, then in every sort of 1D projection, it'll look kind of like the middle, no matter which one you look at. But the claim is if you have a point over here, say, here. But the claim is if you have a point over here, say here, then maybe in this projection, for example, it would look like it's in the middle. Whereas on the other hand, if you project into this direction, then you'll see that most of the data is on this side. So that's kind of a wrong point. So maybe, I don't know if this helped people in general, but the idea is that the true mean should be kind of right in every 1D projection. And that's a very exploited idea. And that's a very exploited idea nowadays in all sorts of robust and multivariate statistics. And we're going to try to use that same idea. In particular, I'm going to present to you like a kind of algorithm which appeared in some of our work, but I feel like there's many, many different ways. And I'm still thinking about the simplest way to kind of do this. So okay, let's try to instantiate. The first thing we're going to need is a set of objects, and we're going to just cover the space. We're going to cover the set of all possible means within this ball of radius root d. Root d. You'll notice that it is exponentially large. Maybe not a good sign, but okay. The next thing we're going to need is a score function for every candidate in our set. And the score function, I've kind of said it in words here. How many points have to be changed to have Q be far from the empirical clip mean in some projection? That's kind of like, if you think about it, it's kind of like a sensitivity-bounded version of just check if every 1D projection looks right. Projection looks right. I don't really want to focus too much on that. The main thing I want to get in out of you to get out of this is the fact that we're looking at kind of every projection and finding if they all look right. So that seems kind of like a bad news. How are we going to do that? But like I said, if you at least plug it through the analysis, the standard analysis, we'll see that O tilde of D samples suffice. So the sample complexity is right. But the thing is, we've paid a lot in terms of the running time here. Terms of the running time here, like both of these steps is not clear how to do, at least at first glance. So that filled up the third spot in our table where it gets the right sample complexity, the right privacy notion, but the running time is exponential. And my claim is that this isn't really just, I've shown you three algorithms very, very quickly, but it's not just like I showed you three bad algorithms, nobody, and like, you know, there's another one that works, that's obvious. Like, people have looked at this a fair bit, and it seems like there's some sort of gap here in terms of what we can do. In terms of what we can do. But our result, I bet you could have guessed what was coming. Yeah, we managed to get the black term in all of these. We get linear sample complexity roughly, pure DP, and polynomial running time. In particular, I'll flash up a simplified version of our theorem, which is kind of for this setting so far, and we can stare at it for a bit. So suppose we're given a data set from a distribution P with bounded covariance, and we have this sort of warm start in terms of where the mean is. Warm start in terms of where the mean is, then there exists an efficient pure DP algorithm which outputs an estimate of the mean such that the L2 error is less than alpha with probability 1 minus beta, and it's linear sample complexity, and you can see the dependence in the other terms. So now I'll pause for an awkwardly long time in order to see if there's any questions demystifying the statement. Because if you get this theorem statement, that's, I think, my job of done and how we beat the previous thing. So i is the dependence on epsilon uh? I mean, you said you should think about it as one or two or something. Is it the correct? Oh, yeah, yeah. Sorry, let me claim, I'll also flash it later, but this is a tight dependence up to polylog. Oh, there should be a no tilde here, but up to polylog factors, this is the right bound. Yeah, and yes, like basically one over epsilon is the standard cost of privacy in pretty much everything. Well, good question. Anything else that we want to demystify? Otherwise, I can talk a bit about how we kind of get around the issues and that exponential mechanism setup that I mentioned. All right, that's awkward enough. Let's continue. So, why was the exponential mechanism slow? There were two problems which I really, really pointed out, but let's say it one, I mean, I might even say it again, but the one problem. Even say it again, but the one problem was that we had to compute the score function for exponentially many candidates, right? We built this cover. Cover-based algorithms are, as we all know, generally slow. So, but we didn't really need to compute the score function for everything. All we needed to do was sample from this distribution. Will we be able to do it efficiently? And we'll see when we can. The other thing is that even computing the score function for a single candidate was slow, right? We were looking over all possible projections and the sort of trick. And the sort of trick here is that we're going to be able to use recent developments from robust and high-dimensional statistics to make this a lot faster. So, those are the two kinds of things that we're going to do. And let's talk about the first one first. How do we deal with the fact that we're dealing with this cover? And how do we get around that? Well, again, we need to run this. And the idea is the following. It's a very simple observation, but if we can say that if at But if we can say that if f, if the score function f is concave with respect to the objects, with respect to the candidates, then this resulting distribution will be log-concave. And so the alarm bell should be going off for some of you who know that, you know, if you have distributions which are log-concave, we have a very, very rich understanding of how to sample from log-concave distributions. In particular, we know tools to do this privately for log-concave distributions. You might be thinking, what does it mean to do private sampling? Well, this sort of Do private sampling? Well, the sort of classical analysis when you do log-concave sampling, this is usually total variation type of guarantees. But we actually need something a little bit stronger, which fortunately some very smart folks have already worked out for us to get this type of multiplicative guarantee you want for pure differential privacy. That's great. There's some notes you need to switch to a continuous analog of the exponential mechanism. In particular, you're not just running it for a discrete set of points, you run it over the entire space. And rather than paying And rather than paying according to how many candidates you have, you also need to make some volume arguments about how many good candidates. But yeah, I'm not going to get too much into the detail of that. And additionally, you're going to need some sort of Lipschitz property on the score function. So that's kind of the, to kind of summarize this slide, if we have the score function be log concave as well as Lipschitz, then we can apply the deficient samplers and yeah, and then it'll be fast. And yeah, and then it'll be fast. So that's addressing the first problem. The second problem, efficiently computing the score function, like I said, there's been a very recent line of work on this type of problem, understanding how to look in all directions, find which ones are kind of interesting in some rigorous sense, and especially doing this efficiently. So, for example, Gabor and Shahar, like I mentioned before, they had a work which kind of is really the backbone of our algorithm. Our algorithm a few works down the line. That was made efficient by Sam Hopkins' work using SDPs and SOS-based techniques. And that's kind of like the line of work we fall into. In particular, we're going to use a privatization of Yeshwant, Chera, Panam, Jerry, Flammarion, and Peter Bartlett. And in particular, the years are a bit weird, but I guess this was first, this was second, this was third. Journals are, I guess, slow. But yeah, so it can really be seen as a type of privatization of their approach. And I'm not going to get into the full, full detail, but let me at least tell you kind of, you know, the main idea behind what we do. So what we do is before, so the difference between what I'm going to tell you now and what I told you before is rather than like using the candidates being a cover of points, now we're going to look for a direction from at a specific point. In particular, we're at some point and we want In particular, we're at some point and we want to kind of figure out how interesting is a given projection in the setting, and it sense kind of like that. Essentially, we're looking for an interesting direction in which the empirical mean and the current point differ in that projection. So basically, go back to one of these pictures. Suppose, like, suppose, I don't know, suppose we're right here right now, which is a bit of a caricature, but we'll see the most interesting direction is this one, because of the fact there's a lot of points in that direction versus. Points in that direction versus where we are right now. And there's more of a picture you can draw inside it. But very roughly, that's the idea. We're going to try to look for directions that point towards the mean using some of this work, which finds interesting directions. And of course, just doing that by itself would not be private, but we're going to plug it into the exponential mechanism to say we don't find exactly the best direction, but we find one which is pretty good. That's the first step, and then basically we do a gradient descent type algorithm, which, like I said, is. With gradient descent type algorithm, which, like I said, is almost exactly like they do it in their work. Of course, there's some other parts you have to privatize, like how do you pick how big a step to do. That has to be privatized as well, and repeat. So, yeah, that's kind of the... I'll point out that this slide basically covers most of the technical work proving that this SDP has the requisite properties. I'm just not getting into it right now, though. And there's some technical changes that have to be done in terms of how we read off the solution. But I won't get into that. Off the solution. But I won't get into that because my time is almost up. But let me say that more broadly than sort of what I've described here, we kind of, there's kind of a recipe that we've provided here, and we explicitly say it. Suppose you have a score function with all the following properties. It's things like if a high score function applies high utility, bound sensitivity, Lipschitzness, concaveness, and a large volume of high utility candidates. Basically, if you have a bunch of nice properties of your problem and a specific score function, then you Specific score function, then you can find privately find a high utility point efficiently. So this like right here is basically all the things that I said we're going for. And you can give a specific meta theorem for SOS-based score functions, which looks something like that. Don't read that, please. It's a bit of a mouthful to actually go through and state formally, but precisely, but yeah, we kind of try to make this a user-friendly framework, hopefully, so that this could be applied to other types of problems. This could be applied to other types of problems. I'll point out that the framework also works for coarse estimation, which I didn't really mention today. But I'll say the final overall theorem statement, which looks very similar to what we had before. I've added a few more things in here to sort of tell you all the bells and whistles. Now, suppose the differences from before are, let's suppose we have an eta corrupted set of samples where this is sort of the standard robust estimation setting. And now we know that the L2 norm of the mean is just bounded by some parameter. Of the mean is just bounded by some parameter r. Then we're going to get a robust estimate of the mean, and this is the sample complexity. You can see there's sort of like an additional term where you have to do this chorus estimation, which I didn't get into at all today. And that's also pretty much optimal. So I'll let you stare at this for about 15, 20 seconds and ask any questions you want. And then I've got about one more minute, so I'll wrap up after that. Questions about this? So sometimes the square root of eta can be improved to something smaller if you have light tails. Great observation. Yes, all we have right now is a covariance-based algorithm. It seems harder, Salil would be able to say much more about this than I would, but it seems like it's generally hard to use these higher order moments without getting things which get like exponential in the. In the number of moments you use. In particular, an interesting question which is currently open, which I don't know, we might think about it a bit more, is how to get the right answer for Gaussianity. In particular, if you, the right answer for the Gaussian setting should be this plus this. That wasn't very informative. Basically, get rid of the squared and add in the non-private term. That's what it should be for a Gaussian. But we don't know how to get that right now. But that's easily. Get that right now. But that's an interesting question. Cool. And yeah, just in conclusion, we get the first private algorithm, a pure DP algorithm with the linear sample complexity for mean estimation, everything else you could want for free. And just to say some open directions, I think it's really, really cool that this is like, I think, the tip of the iceberg in terms of connection between robust and private estimation. They sound so similar, but there's some other work in this direction as well, using things like resilience, two comedians and stuff. Like resilience to comedians and stuff like that. But I think you can see on the years of all of them, except this work by Cynthia Dork and Steven Lay, which is, of course, years before anyone else thinks about this, as many things by Cynthia. Yeah, but the rest of this is all 2021, 2021, 2021. This is the time that's ripe to get into this area. I think this is the first application I know of SOS and DP estimation, so I'm guessing this is not the only place. And there's more specific open questions I can talk about more offline. For example, this Gaussian. Can talk about more offline. For example, the Skausian case, which Gabor alluded to. Anyway, thank you. I'm happy to take questions now at this time, or over dinner, or over beer. Yeah, I have a question. So I didn't understand why is the score function, I mean, I didn't really understand the definition of the score function, but why is it concave? Like, what's the where is it? I don't see the convexity in what you're saying. Yeah, yeah, that's that's sort of one thing I kind of skipped over in the sense that I kind of skipped over in the sense that it's kind of if you define the SOS score function in a very specific way, where kind of the score of a specific point, you have a specific constraint which makes it sort of linear in that, then it works out okay. I can show you the specific condition offline. I guess I won't. It's really like if you set up your SOS function and the score optimization problem the right way, then it's really not immediate. And did I understand? And did I understand correctly that to define, like to compute the score function, you do a gradient descent step to compute it itself? Uh no, no. So can you show like the slide again of the definition of the score function? Yeah, what's uh this right here? The score function here is measuring uh how good a specific uh direction is. Like, we're kind of gonna run the uh we're gonna run the exponential mechanism not just once, but we're gonna run it, let's say, logarithmically many times. Runs, let's say, logarithmically many times. And so, at each step, you're at some point, you kind of find which direction is interesting enough, and then you take a step in that direction after you run the exponential magnum. The step is with respect to the mean, or what is being moved? It's created in this sense. Your candidate. Like, you always have a candidate of where you think the mean is. You're gonna start with something which is very poor, of course. But you try to find something using the sort of SDP and exponential mechanism to find which direction to go in. Mechanism to find which direction to go in. So, this is kind of like the main expensive part of the whole algorithm: finding which direction to move in, but then you just take a step and repeat. I don't understand, like, how do you interleave the sampling? I mean, I don't understand how is the computation of the score function and the sampling from the exponential mechanism, how they are interleaved. Yeah, it kind of just goes into the, like, into the exponential mechanism. I mean, I guess you. Into the exponential mechanism. I mean, I guess you know the standard log concave sampling type of things. You basically imagine that you compute the score function for a specific direct. Yeah, yeah, okay. I see why it's a bit confusing because we're taking steps in two things. Let's zoom in only on this first part, where you're running the exponential mechanism. So now we're inside the exponential mechanism world. Within that, you have some direction, and then you compute the score function with respect to that direction, and then you use the, like, well, you use these samplers to get. You use these samplers to basically find, okay, what's the next direction that I should look at? I see. Okay, okay, okay, okay. Does that make sense? Yeah, so there's like an internal kind of like log-concave sampling, and then you zoom out, and then you're doing a gradient descent. But that's like, you can just picture it in one within that we're doing log-concave sampling. Cool. Yeah, happy to answer more questions, like I said, at any time. Yeah, happy to answer more questions, like I said, anytime now. Thank you. Thank you.