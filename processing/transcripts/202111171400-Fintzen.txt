Thank you very much for the invitation to speak at this conference in honor of the customer's birthday. It's a pleasure to tell you about the story of representations of PRD groups today with a small twist. So what is the setup? The setup is that F denotes a finite extension of Qp or the Laurent series of a finite field which contains the ring of integer O, a maximal ideal. Of integer O, maximal ideal P, and the residue field is FQ. And then the main player for today is denoted by G, which is a connected reductive group over this local field F. So a lot of people like to study groups like GLN and classical groups, but you're also welcome to use all kinds of exotic exceptional groups. And so the big motivation, the long-term goal, is that we want to classify all the representations of this. All the representations of this periodic group G. And by all representations, I mean irreducible, smooth, and complex representations. Smooth is always just means the stabilizer of every vector should contain an open subgroup. And instead of complex representations, I also allow FL bar representations so that the coefficient field is FL bar, where L is a prime number different from P. And we have already seen this occurring in other talks, so that has applications to all kinds of other questions. Of other questions, among others, Lauren Spice quoted my talk already, and we also see all kinds of global applications. But how do we construct actually these representations? We know that there's certain building blocks which are called supercasp representations for complex coefficients and caspital representations for mod L coefficients. For mod L, there's a difference between super caspital and caspital. I'm happy to talk about that at some point if anyone is interested. That at some point, if anyone is interested, we can construct both actually. All right, so we want to construct these representations because for applications, it's really helpful to be able to write down these applications and to know how they look like. But how do we do this? How do we construct these building blocks? So, we know that there's, or let me start like this. We know how to do parabolic inductions, we know how to start with a building block and get all kinds. To start with a building block and get all kinds of other representations, but these supercastle representations are very mysterious, and mathematicians have tried to write them down for a long time. It started, I think, more than 50 years ago with work of Howe and Moy and others. And then people started special cases, started with just small groups, GO2, SO2, and then GLN. And so the case of GLN was solved around 25 years ago, thanks to Grushner and Kutsko. We know now. Minkutsko, we know now how to write down all the representations. And microphones, we never showed later how to do that model as well. And classical groups, as long as P is Naughty Critch 2, we know how to deal with as well, thanks to Sean Stevens. And then Stevens and Corinchok did it model. And we also know how to deal with inner forms of GLN. So classical groups are fixed points of an involution. And inner forms of GLN, like groups like GLM or by division algebra, I mean know how to construct supercars. And we know how to construct supercast representations for those as well, thanks to the share and Stevens. And so these are all groups that somehow relate back to GLN. So we can use the methods for GLN and make them, adjust them so that they work also when we have an involution, take fixed points, or when we look at inner forms. Excuse me. When you say construction, do you mean also like the exhaustation? Do you need to worry about the prime P that, for example, P divide? Prime P that, for example, P dy then? That's an excellent question. So, yes, in that case, I mean, provide a construction of all superclass representations. So, provide a construction that is exhaustive, that provides us with all the representations. And so, in these cases, we have a construction that provides us with all the superclassical representations. Thanks. Further questions? All right, so that means in this case. All right, so that means in this classical setting, we know how to write down the representations, at least in theory. There is an explicit recipe, there's a book that we can read, and then we know how to do it. So the question that I'd like to answer is how to do this in a more general set. So what do we know for general groups G? So there the work started much later because people naturally somehow started with GLN and then it was much more difficult to get a hold of all the redactive group as well. Get a hold of all the reductive groups at once. Groups like Hielen, but also exceptional groups. And one of the big breakthroughs was actually work of Moyen Persant in the 90s, where they defined the notion of depth. So to a representation, they associated depth, which is an invariant that could be either zero or a positive real number. And so I like to draw the depths here because I want in one direction to draw the depths and in the other direction the prime number p, the residue field characteristic, because some results. Residual field characteristic because some results depend on that. And so Mohen-Passett said: give us a representation, and we give you an invariant, which is called the depth, and actually slightly more. And what Mohen-Passett then did is they studied the depth zero representations and related them to representations of finite groups of V-type. And Morris did the same roughly around the same time. So now we know how to get these depth zero representations, because they come from representations of finite groups of V-type, which in itself is. Groups of V-type, which in itself is, of course, very complicated and very difficult story. But up to this big black box, we know what to do. But now we are mathematicians. We are interested in what happens at higher depths as well. And in 1998, Jeff Adler gave a first construction that works rather general for general reductive groups. And then three years later, JKU generalized this construction vastly and provided us with a lot of supercomputers. And provided us with a lot of supercast representations. And I've drawn these supercast representations as dots here. So these are representations that JKU constructed based on a construction of Jeff Adler. And that's the most general construction we know today. So you might recall Lauren's talk. Lauren Spice talked about supercastful representations and how to parametrize them in order to calculate the characters. And he used JK use construction. But a lot of people were trying to write down. People were like people who tried to write down an explicit local Langnons correspondence or study the Hauer or theta correspondence, study distinction questions. Most of them refer back to use construction because that's the only way we know how to write down explicitly these representations in a rather broad general setting. So, since I said this is the construction that is so widely used, we might wonder what do we get out of it? I mean, I was just asked for GLN, is this construction? For GLN, is this construction exhaustive? And I said yes. So we want to get a similar result for JKU's construction. We want to know that everything arises from that construction. And Julie Kim actually proved in 2007 that this is the case under the following assumption. If we assume that the prime number P is very large, so we are in this region here of P very large, and we assume that the field has characteristic zero, so the local fields. The piodic field F is a finite extension of QP. It is not allowed to be. Extension of QP. It is not allowed to be in or CSO finite field. Then we get everything from JKU's construction. So this construction is exhaustive. So that means all the results we prove using this explicit construction are results about all the supercascular representations out there. This large depends on the group, right? I mean, given, I mean, there is no. Yes, so large depends on the group, but not only the group, large depends on the field as well. So for example, for GLN over field F. For GLN over field F, large means that P is greater than N, but P also should be greater than E, the absolute ramification index of F. So if F is very ramified, highly ramified, then the prime P has to be very large. Or more precisely, I think P should be larger than something like the product of N times E, but it's difficult to write it down because Julie Kim's conditions are a bit more complicated. She writes down a list of things she wants to be satisfied, and then you have to work out when this is the case. Satisfied, and then you have to work out when this is the case. Okay, thank you. Thanks. Further questions? So, yeah, this very large can be very, it's really in some situation very large. If the field depth is highly ramified, it's as large as you want, essentially. Okay, so, but it was still a big breakthrough back then to actually know that it is exhaustive in some region. And then you might wonder, is this? And then you might wonder: is this everything? Maybe we can prove exhausting. Can I ask another question? Yes, please. These constructions of Sean, and do they agree with you? I mean, are they? Yes, so I mentioned on the previous slide the construction for GLN and classical groups. And in the case where the prime doesn't invite the order of the while group, which we'll see in a moment show up, these two constructions agree with each other. It's not immediately obvious, but you can see. It's not immediately obvious, but you can sit down and show that they actually agree with each other. And there have been students who have actually worked out how to translate from one to the other in the setting where both exist. In the case of classical groups, of course, we know everything except for characteristic two, right? Correct. So you lacks that. You lack having as much as Sean gets or you lack something. So yeah. So let me, I mean, you'll see that in a moment. What precisely. What precisely we get out of you and whatnot? That's an excellent question. But the main thing is that you can work with general reductive groups, at least those that split over time extension. So that's much more general. But then on the other hand, we lack something for small crimes. And that's actually what, oops, wrong direction. And that's what we see here. So shortly afterwards, Vita and Vu, they gave a construction of some representations that they called epipilages. Representations that they call epipelagic representations, because the representation they constructed are very close to depth zero, they have the smallest positive depth. And Margarita thinks of a representation as a fish that swims in an ocean, and the upper zone of the ocean is the epipelagic zone. So the representation they constructed are some of the epipelagic representations here. And so what they did is they said, take a certain input, a nice element, so-called stable vector in the Moi-Passett filtration. vector and the moi-cosat filtration, then they perform the construction and the output are these epipelagic representations, some of the epipelagic representations. And so, what's very interesting is that while they could only show that the input exists for large primes, I showed in a special case, it's a joint paper with Spetromano, that actually the input for Rita and here exists also for small primes p, in particular for the prime p equals 2, which some Casimon, I think, asked about after someone else's talk. So that means Else's talk. So that means we actually get new representations for small primes. In particular, we get some that I think we have seen thanks to the work in classical groups that you couldn't see yet. But we also get genuine new representations. So for example, I think for G2 with the prime is equal to 2 or 3, you get new representations. So yes, use construction is excellent because it works for so many groups in such a general setting. In such a general setting, but it somehow misses out some small prime case. It can't deal with the wild setting yet. So, somehow, there are advantages of the classical group approach and advantages of that approach. And so, then the natural question is, what can we get out of use construction? And so, what we can get out of use construction is actually everything under the assumption that the prime p is large. So, there's a region where the prime p is large where we get actually exhaustion, and moreover, we get the exhaustion. And moreover, we don't need to assume that the resident the characteristic of the local field is equal to zero. So we can remove that assumption as well. So let me write this down as a precise theorem. So the statement is that suppose G splits over a time extension of our local field F, which is just what JKU works with. I mean, that's, of course, given for GLN and classical groups that have been dealt with. That have been dealt with, and assume that p doesn't divide the order of the while group. And that's what I mean by p large. So, what is the order of the while group? For GLN, the order of the while group is n factorial, so you exclude all the primes up to n. And in general, for classical groups, you see the while groups have huge order, but the primes dividing the order are rather small. It's usually for classical groups up to n. And for the exceptional groups, in the worst case, in the worst case, is VA, where we exclude the primes 2, 3, 5. We exclude the primes 2, 3, 5, and 7. So we don't actually exclude much. We only exclude four primes, even for these huge groups like E8. So that's the statement. That's where we get exhaustion. In this setting, we know everything. And I should say that the same also works model. So if you like number theory, if you like to study, for example, the local languages and families, then it's important to also work model. The situation is more. Also, work model. The situation is more complicated, model, in particular because representations of finite groups are no longer seemingly simple in that setting. So that causes a lot of complication, but still the same construction essentially works. The proof is just a bit more complicated. And let me make one more remark that this condition is essentially optimal. So as I said already earlier, there are more representations here that do not arise from use construction. So that means if we want So, that means if we want to go beyond GLN and classical groups and want to get everything for all primes, we really have to work harder. So, that's essentially up to a small star because I would have to tell you about more details. It's essentially as good as it can be. Can I ask one more question? I mean, could you tell me how does the order of the wild group come into its construction? Why is that so prohibiting? Oh, yeah, that's an excellent question. Why the order of the wild group? Order of the while group. It's because if P does not divide the order of the Y group, then all the maximal, then all the tori that occur in the group G are actually split over a tame extension. And so the other way around, if P does divide the order of the Ry group, there's some inner form at least where you can find the torus that is not split over a tame extension. And JK, you can only deal with Tori that's split over Tame extension. That split over time extension, okay? All right, good. Yeah, and also, I'm just checking my memory too. Of the yeah, so that's the reason because um, we want all the tori to be tame. And if we're in the setting that p doesn't really write the order of the wild good, not only are all the tori tame, we also have enough of nice generic elements. So, that's actually in the same paper where I proved that the optimality condition. So, I think Lawrence Bice mentioned in his talk also that we need a In his talk, also, that we need enough nice elements, and that's guaranteed if P doesn't divide the order of the value. Thanks. That's an excellent question. Further questions? All right. Well, so that's the state of the art, except there was the problem. So some people might have heard about that already. The problem was that JK wrote down the You, he wrote down this construction of representation and proved these are supercast representations. And everyone went away and said, Oh, yeah, now I know how to construct supercast representations. Let's work with them and assume they are supercaspital. But his proof that these representations are supercaspital did not work. Or more precisely, what he did is he used a reference, an old paper by Jared, but there was a typo in this reference. There was a plus minus one missing, just the plus minus. Thing. Just the plus minus one, nothing else. And I think it's from the days where you handwrote a paper, gave it to a secretary to time it. And I don't know, somewhere in the process, the plus minus one dropped out of the equation. The problem is, if you put this plus minus one back into the proof and JKU's proof where it should be, the whole proof breaks down. Like to the extent that there exists a counterexample to the key ingredient in JKU's proof. So these are proposition 14.1 and theory. Preposition 14.1 and theorem 14.2, it doesn't really matter what they are, these are certain intertwining properties, and they are just false. And that means if you put the plus minus one back in, you can't just tweak the proof a bit and make everything work. So there was a problem. So that was scary for some moment, but I guess, well, I wouldn't talk about this if we couldn't fix it. So, luckily, use construction still works. So, everyone who used use construction, believing these are supercaspital representations, didn't do a mistake. They are indeed supercaspital. So, I provided an alternative proof in which I use the, there's a question. GA2 has four maximum towards sky. Okay, I don't understand what sky means. What sky means. If you want to ask the question, please unmute. Otherwise, I move on for now. Okay, no one unmutes themselves. He means that if P is two, GL2 can have as many maximal tori as you want. P is two, GL2 can. Okay. Oh, sky, you mean infra? Okay, so this is just a comment that if we work with GA2 and P is equal to 2, you can have as many Tori as you want. Yeah, thanks. All right, so that's a good remark. So yeah, sequence to a lot of things are crazy. So what was I saying? So I told you about we don't have to worry, use construction still works. What I did is I took What I did is I took the first half of JKU's proof and substituted the second half that relied on these wrong statements by just three pages of an alternative proof to make everything work. So that was a relief. We can still work with it. So actually, this is really the state of the art. That's the picture we have. If P doesn't even write the order of the while group and work with the same group, then we know everything. The problem is that actually, well, not the problem, but the siding. Not a problem, but the side effect is that these proposition 14.1 and Ethereum 14.2, it's a pity that they are false because we really like to use them for some application. And so, well, that's a long story short. And what we did is actually, in joint work with Tasha Khalifa and Lawrence Bice, we actually suggest that instead of using JKU's construction the way he has written it, we should twist JKU's construction. So basically, we should twist it by this plus minus. So basically, we should twist it by this plus minus one that was in the way. And then we can make this proposition 14.1 and theorem 14.2 work. And then we can use use original proof to actually show that these twisted representations are supercaspular representations. So I say this is a vague version because I'm not really telling you yet how to twist things. Before I tell you that, I need to just give you a better idea of what are the objects that we're working with, anyways. Anyways. But before I dive into that and just give you an idea of how things look like, I wanted to tell you a bit about why do we care about this result. And so there are several applications that already occurred partially in Lauren Spice's talk. For example, it was necessary for the formula of Harish Gendra characters of these supercastle representations to show that this twist exists. So Lawrence Weiss told you about all his work that he put into it, but it's This work that you put into it, but at some point there was a credit character plus minus one in the Y. And the work with the joint work with Tasha Kalita and Lauren Spice allowed us to twist this character away and make everything work. Similarly, for Tasha Kalita, in order to write down the local Langnans correspondence for non-singular representations, there was also plus minus one in the way, and our work allowed it to be spared away. And also, if you want to prove character identities for the You want to prove character identities for the local Langlands correspondence for regular supercastimo representations, this was important. And I put a dot dot dot here because I think this will also have applications, for example, if you want to work with HECA algebras. And there might be more applications. In general, if you go back and look at what people did in the past, people often put in a plus-minus one to rectify things and to make things work, to make a certain L packet stable and things like that. Things like that. And so it seems that there is some reason why they had to put it in because maybe the twisted version of use construction is actually the one we should work with and not use initial construction. All right, so what I want to do next is to give you some idea of how do these representations look like and then tell you a bit more about what is this, what do I mean by twisting? What is this mysterious plus minus one? Are there any more questions at this point? Any more questions at this point? I have a quick question, Jessica. Sure. So you didn't, well, it was not. So are these sign issues related to fixing Heke algebra isomorphisms or it's something different? So what do you mean by fix? So when you want to identify, when you want to have some Heck algebra isomorphisms like Some Hecke algebra isomorphisms, like when you have a type or a cover, there is a Hecke algebra and you want to identify that explicitly, let's say with the affine Hecke algebra, the super-cuspital case, that isomorphism usually is not unique. And there are some signs that come up there. Is the sign issue? That's interesting. Let's talk about it afterwards. I'm not sure what signs come up there. What I know is that if you want to, for example, relate the Hacker algebra of a complicated representation. Of a complicated representation to a hacker algebra, often easier like of a depth zero representation, these problems come up actually. And you might have to twist the I mean, you might have to insert the twist in order to prove that there are nice identities between different hacker algebras. I'm curious to. Okay, thanks. Let's talk about that later. I'm curious to learn more. Further questions on this point? All right. All right, so how do we construct these mysterious representations? So there is a folklore conjecture that all representations, all supercast representations, should be constructed in a two-step process. The first one is you construct a representation, which I call rho sub k, of a compact or compact mud center subgroup k inside g. So of sln of O inside sln of s, for example. And then the second step is that you build a representation of g from this representation. Of G from this representation of the compact Ohm circuit via compact induction. So let me give you an explicit example so you have a feeling for how this looks like. So let's take as our coefficient field either C or F L bar, whichever you like, and let's take the group SL2 of F, where you take your favorite field F. Let's take a compact open subgroup consisting of these matrices here inside SL2. So on the diagonal, that's one plus maximal D. So on the diagonal, that's one plus maximum ideal of diagonals, either p in the upper right comma or the ring of integer in the lower left. And then we add the plus minus one to it for technical reasons. So then what do we need to do? We need to construct a representation. And I want to construct a one-dimensional representation for you. So that's just a map to GL1 of the field. So just to the invertible elements. So it's just a character. How can we do that? The plus minus one, let's just send it to one and forget about it. So what I'm really About it. So, what I'm really interested in is where do we send this compact open subgroup? And so, this compact open subgroup is not an arbitrary one. It's actually one that has this funny name here. It's called Gx one half. Why is it called that? It's called that because of Moi-Prassat theory and Brad-Tietz theory. So, let me start to explain. What is X? X is a point in the Bridge-Tietzbelt. Yes. This is really meant to be SL bar. This is really meant to be FL bar and not QL bar values? Yes, you're welcome to use FL bar, do it model, yes. Okay, sorry. Yeah. You could also use QL bar that would be isomorphic to C, so there's nothing. So you can use characteristic zero or model coefficients. Thanks. Okay. So what is the red sheets building? The red sheets building is a lot of The word sheets building is a lot of real vector spaces glued together. In the case of SL2, it's a lot of lines that are glued together. So it looks like an infinite tree, like that. So where are the lines? So this is one line, this is another line, another line. A lot of lines grew together. And then the group acts on it. So it's a building and the group acts on it. And then X is a point in this building. So say that's the point X. And so if you have a point in... If you have a point in a space on which a group acts, it's natural to look at all the elements of the group that fix that point. And so that's the stabilizer of X. In that case, it looks like just O on the diagonal, O on the lower left corner, P in the upper right corner. And instead of calling this a stabilizer, I also like to call it GX0 because it fixes the point X and the ball of radius zero around X. The ball of radius zero around x. I must say, I'm going to tell you what the Moi-Prasat filtration is, and I'm you have to be a bit careful. The precise definition is slightly more technical, but I want to give you a feeling for what this is. The precise definition, you have to be a bit careful with the center and with finite order subgroups. But so, this is the this is the depth zero piece. It's also called the powerhoic subgroup. And now it's natural instead of just fixing the ball of radius zero on. Fixing the ball of radius zero on it to fix a ball of larger radius. So we can fix a ball of radius one-half, or a ball of radius one, or one and a half, or two. And by fixing larger and larger balls, the subgroup that fixes larger balls is getting smaller and smaller and smaller. And that's how you get a filtration. So let me write the next filtration subgroup. So the next interesting piece is the one at depth one ha. And this one is slightly smaller. Is slightly smaller on the diagonal, but the off-diagonal stays the same. And then the next interesting piece happens at steps one, where the diagonal stays the same, and the off-diagonal gets slightly smaller. And then you can keep going and always alternatingly make the diagonal smaller, make the off-diagonal smaller, make the diagonal smaller, off-diagonal, and so on. And that's the moi-persat filtration. Another moiety filtration would be if you Moi-perside filtration would be if you start with the vertex of this graph, you would just get the standard congruence filtration. So you get all the matrices congruent to one mod p to the n for some integer. But what Moy and President did is they observed that this congruence filtration, something natural you like to work with, in particular if you work with automorphic forms as well. But they observed that actually from constructing representations, it's not everything. And it's helpful to have this other filtration, whereas you see, you're alternatingly. see you you you alternatingly do jumps on the diagonal jumps on the off diagonal diagonal off diagonal alternatingly and in general the moi per side filtration is constructed by for example gln by having the diagonal the torus jump at integer points and having the off-diagonal entries jump at different depths where the depth depends where in the building you lie. But for SO2, these are the two main examples to keep in mind. Main examples to keep in mind. And so that means this group here, Gx one half, that we work with is actually appearing in this Moi-Passat filtration. And this filtration has a lot of nice properties. In particular, if you look at the quotient between two successive groups of positive depths, so the quotient of one half and the next one at depth one, these are all vector spaces. These are all abelian. And the zeros quotient is a reductive group that acts on all the others. So there's a lot of structure involved here. A lot of structure involved, but okay, that was just the tip of the iceberg SEM advertisement. Why it's interesting? Let's go back to the construction. We start with this group at depth one half here. So that's the real numbers are called the depths here. And what we can do is quotient out by the next one. I call it one half plus. This is just the next filtration step group we have seen. And this quotient just corresponds to anti-diagonal matrices over the finite field. Over the finite field, because the diagonal stays the same, only the off-diagonal changes. And so, what can we do? Just take a matrix, send it to A plus B. That's just an element in our finite field FQ. Well, and we can just send this to C star, to the invertible elements in our field, so either C star or the invertible elements in FL bar, via some non-trivial character. Choose your favorite one. If this was Fp and you send it to the complex numbers, just send it to the piece roots of unity. And so, if you combine this whole thing, this gives us now a character, a map from this compact open subgroup to the invertible elements of our coefficient field. And now, how do we get a supercastle representation out of that? That's just via compact induction. So it means, as normal, we take the induction, so all these functions on the group G to our coefficient field that transform on the left via this character, via this representation for elements H inside this compact open set group. Elements H inside this compact open subgroup K. But these functions should be compactly supported. That's why it's called compact induction. And then we act on it via right translation. So this is roughly the space of functions on the quotient g mod k up to the fact that k pulls out via this representation. So this is a huge infinite dimensional space, but the theorem is that this is actually an irreducible representation. This is particularly an example of a representation constructed by Wider and Euro. Constructed by Vida and U, an epiphelagic one. It's even an example of a simple supercarceral representation that was constructed earlier by Gross and Vider. And in general, JKU, the JKU's construction is in the same spirit. You also construct a compact open subgroup and a representation of it, and then induce it compactly. So you build this space of functions on it. The only difference is that this compact open subgroup will get much more complicated, and this representation row will get much more complicated. Will get much more complicated. All right, so that's an example so that you have a feeling for how these representations look like. Let me get back to the statement and make the statements a bit more precise now. So I said that, okay, there was this problem with use construction because it relied on a misprint, but we don't have to worry. I showed that the construction nevertheless gives us representations. And these representations are of this form. They are compact induction from some compact. They are compact induction from some compact subgroup to the whole group of some representation constructed by JKU, which is a bit more complicated than the one I've shown you before, but in the same spirit. And so what we now did in joint work with Scaletha and SPICE is we knew that there was this plus minus one in the proof that was missing. And if you put it in, then everything breaks down. So what we said is, how about we put the plus minus one back into the construction? So we showed, and that's non-trivial. So we showed, and that's the non-trivial part, it's much more difficult than it sounds. We showed there exists the quadratic character, just the math from this compact open subgroup to plus minus one, such that JK used proposition 14.1 and theorem 14.2, these tricky parts that failed earlier, are satisfied for this twisted representation. So instead of taking the representation constructed by you, we twist it by just a plus minus one, just a plus minus one. And then everything works. So in particular, Works. So, in particular, if we now compactly induce it, we get a supercastable representation. So, well, that's the result. And as I said, this quadratic character was super important because that allowed us to calculate these character formulas, right? On local language correspondence and all these things. So, somehow it seems like this plus minus one should have been there already. It seems like this twisted representation. Seems like this twisted representation is the much more natural one to look at. And also, if you try to construct these representations geometrically, try to construct the local Langman's correspondence geometrically, somehow you have to twist by this quadratic character. So it somehow should be there. It seems more natural that it's supposed to be there. All right. So that's the more precise statement. It's still not very precise because I haven't really told you what the plus minus one is. So let me. Minus one is. So let me dive into that on the next slide now. So let me try to give you a more precise version of the statement. And so in order to tell you what this plus minus one is, let's stick to the case that the group G is just adjoint. And inside there, we have an M, which is a twisted Levy subgroup. So what could this be? So for example, I guess a fake example, if G is equal to SA2 or whatever. To SA2 over say QP, then we could take M inside there to be all the matrices of the shape AAB B times P. So inside SA2. So this is actually just the twist, the turf. So this group becomes isomorphic to the diagonal matrices over a field extension. So if you adjoin the square root of p, this becomes isomorphic. Of p, this becomes isomorphic to the diagonal matrices, conjugate to the diagonal matrices, and so it's a twisted Lavy subgroup, which means it becomes a Lavis subgroup over a field extension. In this case, it becomes a torus over field extension. Well, I say two is not adjoint, but let's ignore that for now. So, that's a setting. G is our group. Inside there is a Levy subgroup that comes from JKU's construction. And now we assume that. Now we assume that p is not equal to 2 and we take a point in the Wertheets building. As we had before, I started also for the example as a point in the building. And then the statement is that there exists an explicitly constructed sign character, so plus minus one. What is it character of? It's a character of m sub x. M sub x is all the elements in m that fix the point x that factors through this quotient here. This quotient here, and then it's a map from there to plus minus one. So, what is this quotient here? So, this quotient is just a reductive group over, well, rather the fq points of a reductive group. So this is, well, this is the zeros filtration quotient I like to talk about. So, that's in the market filtration. The zeros piece are actually one piece above it divided by the next filtration piece. And this turns out to be the Please. And this turns out to be the FQ points of a reductive group. So this is something like Gl2 over FQ. But it can be disconnected. So in the example before, in this example, we have SL2 and inside there this tame torus that is anisotropic. This quotient here, this quotient turns out to be just plus minus one. So it's just the disconnected group. It's just Z mod two. It's just Z mod 2, just plus minus mod. Well, and so we construct a character of this reductive group. Well, you can easily construct a character of this reductive group, but the point is that this character has the following properties. And so let me first say what it is and then motivate it. So it satisfies the property that for every tan maximal torus t inside our twisted Levy subgroup. Inside our twisted levy subgroup, and every point inside the building of the tourists, I'll say in a moment a bit more what this means. The restriction of our character to this intersection of this team of the F points of our Taurus with Mx equals the character that is given to us. Well, here's the character. I will say in a moment a bit more what this character is. But basically, the point. Basically, the point what I want to make is we construct a character of a finite group of V times, so of Gl2 over Fq, but it might be disconnected as well. And this character is given by us on all the Troy of this group. So for every Torus of the group, not just the diagonalizable ones, but every Tori, also elliptic Troy, we are prescribed, it's told to us what this is. But not only for all the Tories of the finite reductive group, but also for But also for the intersections that come from just tame Toring of the bigger group with this finite reductive group in some sense. And this intersection might, for example, just see the disconnected part. So that means we are really told what this credit character should be on a large part of this group, but it's not at all clear that everything is compatible and that everything fits together. And that everything fits together into one character of the whole group. And so, maybe just to tell a bit of the story before I actually tell you what these weird symbols are. So, this is a product of several characters. And this first character, this sharp character, this is actually a character that came from Lauren's construction, when Lauren tried to write down a character formula. He had an obspection that was in the Y. And so, what he did is he wrote down a plus minus one and told us, oh, if this. Told us, oh, if this plus minus one, that I can tell you what it should be on all the tame tori, if that comes from a plus minus one on the whole group, on this whole group here, mx mod mx0, then all the obstruction vanish and then he can actually calculate the things. And so he tried then to see that this plus minus one on the torus actually comes from the plus minus one on the whole group. But it turned out actually it didn't work. If I remember correctly, there were counter examples where he just couldn't. Correctly, there were counter-examples where he just couldn't lift it from the tourist to the whole group. Well, and then there was Tasha on the other hand, Tasha Kalita, who wanted to write down a local Langnans correspondence for non-singular representations. But he ran into a problem as well. There was some mysterious plus minus one, which is essentially this epsilon thad here, which split up here for technical reasons. And he was thinking that, oh, wouldn't it be great if this epsilon thad, which he knew how it That which she knew how it looked like had to look like on the tourist actually comes from a character of this whole group, of this whole MX, this compact group here. So, MX, or this finite group of V-type, this quotient here. And well, that's right. The counterexample is to Tasha, not to me. Okay. So Lauren Spice just said that the counterexample to his, so he Lauren Spice were down a character which he wanted to come from, like a character on the tourist which he wanted. From, like, a character on the tourist, which he wanted to come from a character on MX, but couldn't prove that it works. And then Tasha Khalita came along and said, Well, it seems not to lift. Well, but then Tasha, on the other hand, had his own character which he wanted to lift. And it seemed not to come from a character from MX either. So, well, what's more natural than having two characters that we know how they look like a notorious? None of them actually comes from the group we wanted to come from. How about we try the product? How about we try the product? So then there was the idea to try to actually see that the product comes from a character of this subgroup MX. And so that's actually what turns out to be true. If you take the product of these characters, the product of what Lern wanted to lift for his obstruction to disappear, the character that Tasha Khalita wanted to lift for his obstruction to disappear. If you take the product and actually some other natural piece that Some other natural piece that should have been there, then suddenly this comes from a character of the group Max. And so what I okay, let me say two more things. I'll show you in a moment how this character looks. Okay, maybe let's do this now. Just to scare you. Sorry, I show you how this character looks like. This is just part of it. Well, I didn't really want to scare you. I think we started a few minutes late. I wanted to. A few minutes late. I wanted to tell you a bit what this character is. So, but let me maybe, since you have the slides, leave it at this by just saying, how do we get characters on a Taurus? So on a Taurus, we have roots. And so we can just evaluate a root on an element of the Taurus. And then these characters are just constructed by taking into account which route we should choose and how to actually get a How to actually get a character out of it. So, if you maybe I should say a few more words: if you take an element in the trails, you apply a character, you end up in the ring of integers. You can mod down to obtain an element of a finite field. The finite field it lands in is denoted k of alpha because that depends on where this root is defined. And so then you can apply the sign character to it. And that's how you get a plus minus one. If you know that there's a lot of symmetry, you actually take the sign character for the elements of node. Character for the elements of norm one, if you know it's a norm one element, and then you take a product over whatever characters you need to take into consideration. I'm happy to tell you more details for everyone who is interested in afterwards. I just wanted to give you a feeling. That's how they look like. That's where it's coming from by using these roots and the trores that are prescribed by the applications we need them for. And so, this is this mysterious character that we try to lift. And the way in the end we lift it is. And the way in the end we lift it is actually by taking this product and splitting it up into three different pieces. So, what we do is we write this product as three different pieces, epsilon one, epsilon two, epsilon three. And these three pieces have nothing to do with the three pieces we started with, but each of them we can lift. And so, one we lift by using an explicit construction using the Moi-Pasat filtration I showed you earlier and an action of the group on the. And an action of the group on this Moi-Perside filtration of this quotient group on it. And once you have an action, then you can take the determinant and the sign character again. Another, that's the first character. The second character, we use some hypercommoder deconstruction. And for the third character, we use the spinner norm. And anyone who's interested, I'm happy to say more about this afterwards. But I just wanted to give you an idea, a glimpse of how the character looks like. Idea, a glimpse of how the character looks like, and the way we actually lift it, the way we construct this big character is by splitting it into different pieces and dealing with each piece separately using all kinds of somehow crazy construction and in the end magically work out and give us this magic character that does everything. All right. So I think the last slide, the last comment I'd like to make is I want to tell you something about the culture I grew up. And so in the culture, the way I grew up, I was told that it brings Where I grew up, I was told that it brings bad luck if one wishes someone a happy birthday before it's their actual birthday. And since I think Bill Castleman's birthday hasn't happened yet, I will not wish any happy birthday at this moment. So I'm just ending the talk with a silent, awkward silence. Thank you.