Thanks everybody for coming. I said it before, but it is so awesome to have a three-dimensional conference. I'm really enjoying it. And all the talks so far have been amazing. I also want to, before I give my talk, say special thanks to Anna and Matt and Charles for helping organize the whole thing. And John Braun, who you've all met but is not in the room, I want to also throw a special thanks out to him. He is at least 50%. At least 50%, probably more, responsible for the funding that we got for this conference. He pushed a lot of buttons and pulled a lot of strings, and he is the reason that all of your accommodations got covered. So thanks to him as well. So I'm going to talk about positive bases, which we already saw a little bit about back on Monday with Sebastian and Carlos' talk. So that means I can skip a bunch and maybe catch up the three minutes that St√©phane wants to steal. Minutes that Stefan wants to steal away from me. This is joint work. The first half is going to be joint work with Gabrielle, who's back there in the audience somewhere, I think. Where's he hiding? He's not. He was. I swear I saw him a second ago. But he spoke earlier in the week. And the latter half of the work is joint work with Gabrielle and Shane Planedon, who some of you know and some of you don't. Shane was invited, but decided he. Invited, but decided he'd rather vacation in Australia. Go figure. That's the abstract. I'm going to skip that. I just wanted to show you how similar my talk is to every other talk. So I'm going to have a beginning and then a middle, and then I'll do a little more middle, some more middle, and then I'll do the end. Okay. So we'll start where all good talk starts at the beginning, which is our definition. So again. Our definition. So, like I said, we've seen these ones before because Sebastian introduced them all to us. So, really, I'm introducing notation here. So, the positive span is the conic convex hull of a set. We say it's positive spanning if the positive span is Rn. We say it's positive linearly independent if, when you take away any vector from the set, you change the positive span. And then a positive basis means that your span. Basis means that you're spanning and you can't take away any vectors and still remain positive spanning. So, I'm going to push this ever so slightly into quote new notation, but I think you'll agree that there's nothing really new here. All I'm going to do is I'm going to change the words positive spanning to say positive spanning for S, where S is a subspace. And it means exactly what you think it means. It means it spans the subspace. Positive linear independent doesn't change. Positive linear independent doesn't change, and then positive basis for S means it gets a positive basis for that subspace. So there's really nothing extending or powerful or confusing here. We're going to need it near the end of the talk when we talk about how to look at optimal positive bases. I'm going to just skip through these examples because I think Sebastian did a great job of explaining these concepts. So, our top two are positive. So, our top two are positive bases. The bottom one, bottom left, is not positive spanning. The bottom right is not positive linearly independent because I can make this vector through a combination of these two. Okay, so top good, bottom bad. I'm also going to fast forward through this because this is a classical theorem. This is a theorem that, if I were not speaking to this particular group, might be. Particular group might be exciting and novel. But to this group, we recognize that the reason we're interested in positive bases is because if you have a positive bases and you're not at a critical point, then that positive bases contains a descent direction. And the proof, I redid it partly to show that it's a math talk. Otherwise, you'd be like, was that a math talk? Not sure. Was there a proof? There's a proof. It's a math talk. There's a proof. It's a math talk. Also, because, well, technically, the classical proof doesn't involve a subset. So, but it's really easy to adapt to a subset. It's just a matter of saying, I'm going to take the projection first. So, if you're not at a critical point with respect to the subspace you're interested in, then there's a descent direction in your positive basis. So, one last theorem that we're going to need at the end of the talk, also covered by Sebastian wonderfully, is gen wonderfully is positive bases have at least m plus one elements where m is the dimension of the subspace you're interested in and have at most two m elements where m is a dimension of the subspace you're interested in okay the proofs for these are trivial extensions to the proofs for when you're working in rn you just do an isomorphism over to rm and then you say well it's true there and you come back and you're done okay so Okay, so that's handy because we need to, we're later going to need the idea of a minimal positive basis for a subspace. And so a positive basis is going to be minimal for the subspace if it has m plus one elements where m is a dimension. So all of this generalization is stuff where you'd go, well, if somebody asked me how it generalized, I'm going to write that down. I don't even need to have actually seen these theorems and definitions. There's the beginning done. There's the beginning done. Let's go on to the middle. So, so far, in the beginning. Oh, I like that. In the beginning, we saw what a positive basis is, and we see that it's useful in DFL. The questions we're going to look at today are, how do we decide if a positive basis is good or bad? Okay. And Sebastian definitely hinted at that very, very strongly when he defined his. When he defined his k cosine measure definition, that question is going to lead to a new question, which is: okay, can I actually compute this cosine measure? Once I've got that, we're going to go to the question of, okay, suppose I want to design a positive basis. Can I make one that is in some way as good as it can be? We can't really answer this question here until we have this question answered because we need to define what we mean. Because we need to define what we mean by good before we can say, I want it to be as good as possible. So let's start on that good question. How do we decide if a positive basin is good? So here's two positive bases. They both span R2 positively. They're both positively neutral independent. Which of these would you rather use in your pattern search? This guy, right? So all you have to do is imagine. So, all you have to do is imagine the idea that your descent direction is in the positive x direction. In fact, you have a linear function that just goes down that way. This guy's going to take forever to get there. He's going to zigzag like this. This guy is not going to do great, but he's going to do pretty well, right? So, you want a positive basis where you point not in every direction, but you point kind of strongly in every direction. How do you? How do you measure that? So, if I were doing a longer talk, I might show you how you don't measure that, but we're going to jump to the punchline here, especially because a lot of you have seen this. You're going to measure that with what's called the cosine measure. So we're going to take the minimum over all vectors of length one that are in our subspace. So, cosine measure on a subspace. And then I'm going to take the dot product between those vectors and everything in the basis. The basis. So when I normalize that dot product, then I have the cosine of the angle. So if that angle is small, that cosine is big. So when I take this max here, I'm finding the vector that is as close to possible as the u that I'm checking out. And I want that to be as far away as possible. Now remember, I'm doing the cosine. So when I take the min, that means I'm making the angle bigger to minimize the cosine. So I'm trying to. Bigger to minimize the cosine. So I'm trying to find the u that makes the biggest angle with every other possible vector in my positive basis. I'm also going to define the cosine vector set, which is just the argument. Okay, so this is the set of vectors that give me that solution, the ones that point far enough away. Immediately, we see I can ditch the u on the bottom because I've got a norm u equal one right here. And if you want, you can. And if you want, you can even say, I'm going to assume my basis is normalized. So I could ditch the D on the bottom, making this even clear. Okay, so how do we compute this? So norm D is a constant. This guy's not a problem. This is a maximum. Maximums are okay in minimization problems. These are all linear functions because D is a finite set. So I'm taking the maximum of a set of linear functions. That seems like it should be okay to work with. I'm doing a minimum. To work with, I'm doing a minimum of that. Hey, I've got a linear program, right? Well, no, spoiler, I've got this norm u equals one, right? So now I've taken my linear program and I've killed it. I've said you've got to minimize on the ball, the sphere of radius one. If it was normally less than or equal to one, that wouldn't be so bad. I could do that. But that's going to cause a problem because then, since I'm minimizing, and these guys are always going to turn. And these guys are always going to turn out to be positive because I have a positive basis. So one of these, one of these max is always positive. I'm just going to set u equal to zero, and that'll give me my min. So I can't just use my standard relaxation trick to solve this. I have, well, a really hard problem. Before we solve it, let's do a little intuition. So here's a positive basis. What we do is we pick a U, we check the Pick a u, we check the angle, we find the vector that's closest to it, we check the angle, it's 15 degrees in this case, cosine is 0.97, that's the number we're trying to minimize. Here's a different U. Cosine, so the angle is 30, cosine is 0.87, so that's better. You can see if I stick it halfway between two vectors, then I'm going to get both this and this are active. My cosine's gone down again to 0.71, and you can imagine if I go a little. 71, and you can imagine if I go a little further up, instead of this angle, I'll start looking at that angle, it's going to go up again. So, in two dimensions, it's not too hard. You can just kind of stare at it a little bit and say, oh, I want this, right? I want the one. I'm going to find the two vectors that give me the biggest angle. I'm going to stick a vector in between it. That's going to give me a cosine. So, the cosine measure of this particular basis, positive basis, is 0.38 approximately. And the cosine vector set. The cosine vector set consists of two vectors. I picked this guy, but alternately, I could have picked this guy, right? I could have split this angle into or this angle in two. I end up with the same minimum. So what we're trying to do is find these vectors. Might be one, might be two of them, might be n plus one, who knows. And then once we have those vectors, we find the cosine attribute. All right. All right, middle part two. So here's our new one: our new line. The cosine measure can be used to measure the quality of the basis. Good. We want smaller cosine measures. That's better quality. Sorry, bigger cosine measures is better quality. We're minimizing to find cosine measures. Which now brings up the next question: how do we find a cosine measure? So, this is work with Gabrielle. With Gabrielle. And as I said, we looked at it before. You can't just write this out as a non-linear program. Can, turns out, but it's hard to solve. So back at this picture. What was our insight when we were looking at this? Our insight was you find the two vectors that have the biggest angle between them, and you stick a potential cosine vector in the middle of that angle. Vector in the middle of that angle. That's a great insight. How do we do that in three dimensions or four or five or 22? That's what we really want to generalize because we're pretty sure that that's the trick, right? That's what we're trying to find is the collection of vectors that has the largest open space, and we want to stick a vector in the middle of that open space. So there's a classic theorem. Theorem when I teach it. It's called Graham's Lemma from linear algebra that says if you're given a basis, then there is a unique vector, which I'm going to call UB, where the angle between UB and each element of the basis, BI, is the same. Okay, and you can visualize that. Take your basis in Rn, your coordinate bases. It's the vector of all ones, right? Okay. If I give you another basis, how do you... If I give you another basis, how do you find it? It's not too hard. You just slap around some change of directions and it'll tell you what the new one is. All right. You can also, you're visualizing this, you go, oh, well, actually, haha, I got you. There's two vectors. You said unique, but there's two. I could take the vector of all negative ones. So I have to throw in this. That angle is acute. That's what makes it unique. So there's one vector that makes an acute angle with everything that's equal. And there's another vector, which is the negative of that, which would be an object. Which is the negative of that, which would be an obtuse angle with everything. Okay, so let's mathify that a bit more. This is what it looks like when you actually dig down into the details. So this says that the angle is the same between that vector and every other vector because that measures the cosine of the angle. I set a unit basis just to make get rid of that normalization. This says that that angle is acute. And this is actually the formula how to find the vector. Actually, the formula how to find the vector. Okay, so you build what's called the gram matrix, which is big and messy, but it's actually just an n by n matrix, so that's not so bad. You invert it. This is the vector of all ones. So you've got an inversion of an n by n matrix, a couple of vectors of all ones, take a square root, and you've got the angle. Once you've got the angle, that's how you get the vector. Great. How does that help us? Well, this is the Us well, this is the cool part, and this is what that insight from before I hope kind of hinted towards: is it turns out that if you give me a vector in the cosine vector set, then that vector u star is the gram vector for one of the bases in the positive basis. Okay? Wow. You go back to the picture and you go, all right, so. The picture, and you go, all right. So, what were the three possible ways to build a basis out of that up, left, and down vector? You could take the coordinate directions, and the grand vector would have been that vector of all ones. It's not the one we're looking for. You could take straight up and diagonally down, and then you got the first vector we found. You could take straight up and diagonally that, or straight, sorry, straight across and diagonally down, and then you get that vector, and it's the one we were looking for. So now we have our. For so now we have our algorithm, right? You it's pretty straightforward. There it is. You just try every possible basis. Okay, there's a huge flaw in this algorithm. I'm suspecting everybody's spotting it immediately. Select every possible subset that's a basis out of your set of positive bases. So if you have a positive basis that's size n plus one, that's not. n plus one that's not so bad there are n possible ways to do that right or n plus one sorry if you have a positive basis that's size 2n then there's 2n choose n ways to do that it turns out in the 2n case you actually there's a because of the properties of positive basis you can skip a whole lot of them and it it reduces a lot so i'm not going to cover that today but you can take a shortcut but if you take something in between there's no shortcut There's no shortcut, so you've got to check a lot of things. So, technically, this is a finite time algorithm. There's a finite list to choose from. You check everything in the list, you're going to find it. Unfortunately, that finite time grows at n choose s, where s is the number of elements and n is the dimension, so or s choose n. So that could get really big really fast. So, that could get really big really fast. The good thing is this is just an n by n matrix. So, in the world of DFO, we don't typically go with n bigger than 20-ish. Inverting a 20 by 20 matrix is not difficult. It's super fast. Everything else here is just vector multiplication, again, in dimension 20. So, this algorithm is crazy fast per iteration. The issue comes in how many iterations you can do. Many iterations you can do. We have tried it in dimensions up to about 20 and it's worked fine. We haven't pushed it to the breaking point to see where it falls apart. You can do some simple math and see that it's definitely not going to work if you have dimension 100 and a basis of size 150. No matter how fast you are with modern computers, it's not going to work. Nonetheless, it feels like a reasonably satisfiable answer given. Answer given we're going to keep dimensions low. Open question. I'll throw it out to everyone here. To me, this entire concept kind of reminds me of the simplex method. We've proven that a solution has to occur when you pick the right basis out of your set of variables. And in the simplex method, a lot of creative methods to say, well, here's the fast forward way to get to that basis that gives you the vertex of the. Basis that gives you the vertex of the polytope that solves the problem. Is there a fast-forward way here? Right? Is there something where if I give you a dimension, 100 problem with 150 elements in my positive basis, is there a, even if it's heuristical, fast forward way to get to that correct one in a fast time? I don't know yet. So many problems, so little time. I'm also going to point out this line here. There's a little quirk. I'm also going to point out this line here. There's a little quirk when you're working things out. The reason for that's in the paper, or I can tell you about it after the talk, but I gotta stay on time. Yep. Okay, more middle, even more middle. So now we've shown cosine measure is the way to measure things. And we've shown there's a finite time, finite time algorithm to compute the cosine measure. I'd be remiss if I didn't point out that shortly after we published this paper, Rommel Regis pointed, published another paper. Regis published another paper that presented an alternate approach using nonlinear programming. So he writes out the nonlinear program exactly as you would expect to write it out. And then he works out the crushed Kentucker conditions, which is very nice. And, you know, if you're looking for ways to speed up our algorithm, those crushed Kentucky conditions are probably the trick you need in order to drive you in the right direction. The algorithm I just showed you was for Rn, not for a subspace S. I'm very confident that what I just showed easily generalizes to R, to a subspace one. I suspect registers as well, but the devil's in the details in both of those statements. So now the question becomes, okay, how do we make a positive basis with maximal cosine measure? And I wish I had more time, but we're going to fast forward through this. It's kind of Going to fast forward through this. It's kind of fun. Okay, here are two optimal positive bases. So, we're going to call a positive basis optimal if it has the maximum possible cosine measure. It is as most evenly distributed throughout your space as possible. If the size of the positive basis is n plus 1, it's known that an optimal cosine measure is 1 over n, and it's known how to make. One over n, and it's known how to make it. That's how you make it, and that's easily generalizable to Rn. You take the coordinates of a hypertriangle. If your positive basis has size 2n, then your maximal cosine measure is 1 over the square root of n, and that's how you make it. You take the coordinate directions. Okay, nothing special about coordinates, of course. I can rotate these. Rotate these. You know, every other people had movies. I feel like I deserve one. Okay, so you can take any standard construction through it through a rotation matrix, and you're going to get another version. What is unknown is what if I am in between these two values? So I'm going to call that intermediate size. How do I build an optimal positive basis of intermediate size? Its size. And intuitively, we thought we had the answer. And then we spent a year working on it and proved that the answer we thought we had was wrong because we found a counterexample by finding something better. So then we did what any good black box optimizer would do. We wrote this as a black box optimization problem and hit go and saw what kept coming up, and that gave us some intuition. And then we dug deep down to a paper from 1987 by Romanowich, which is a tricky read, but the main theorem in the paper says this. If you have a positive basis in Rn and its size is intermediate, then you can break it up into minimal positive bases, where each of those minimal ones is for a subspace, not for Rn. Subspace, not for Rn. However, you can't quite break it up cleanly. You have to bring in these things called critical vectors. Okay, so each of these DMs is a minimal positive basis for Li. That's good because we understand minimal positive bases, but these CIs are critical vectors. I'm going to show you a picture in a minute. But what this critical vector is doing is warping the positive basis. Warping the positive basis to fit with L subscript I. Oops. There must have been a space in there. That should, of course, be L little little I. This is the positive basis in R3. I've got the three directions in the plane. I've got one direction going straight down, and I've got one direction that's going with a tilt. And you have to pick that tilt very carefully. Have to pick that tilt very carefully. It actually has to align with one of the three vectors. If you tilt in a different way, you end up being able to remove it to or remove something, not the one you're tilting, but one of the ones down there by building as a positive combination. No, sorry, you have to tilt in the opposite direction of one of the vectors, directly opposite of one of the vectors. So the critical vector, this is that same positive base. This is that same positive basis with the critical vector at it. And you see what it does is it just straightens that out. So that's what the critical vector is doing. It's pulling it back to make it pretty. And then finally, this would be that positive basis, except I've taken this guy and I've straightened him up and down. So looking at these three pictures, intuitively, if the critical vector is non-zero, it feels like It feels like pushing the critical vector back gives you something better. And we did lots of numerical experiments and convinced ourselves that was true. And intuitively, if this is not orthogonal to this, you can pull it up and you're going to get something better. Lots of numerical experiments, and we convinced ourselves that's true. So we're going to call our concept, we have a conjecture that an optimal positive. Conjecture that an optimal positive basis will have all the critical vectors be zero and all the LI's be pairwise orthogonal. So we're going to define that the set of positive bases that have those two properties, we're going to call omega plus. We're calling it omega plus because we defined omega to be the set of all positive bases. And what we can do is we can say if you're in omega plus, we can prove that this is the off. That this is the optimal breakdown of things. So, even though if you're an omega plus, there's lots of ways to build a positive basis. And what this is saying is you want all of the pos sub-bases to have the dimension as close to the average as possible. So you can't have them all be average because average will be the size of the average one will be n over s minus n, n minus s, which may or may not be an integer. So you round. Not be an integer, so you round up and you round down in the way that things are. So, unfortunately, I only have 30 minutes, and going through the details of that is complicated. But what we've seen now is the optimal plasma basis of a particular style. We can solve, we can tell you what it is. And in fact, we can prove that that is the optimal one in R3. In R3, there's only In R3, there's only one positive basis of intermediate size because remember, you have to be between n plus one and twon. So you have to be between four and six. So there's only one, it's size five. So in R3, you can just grind and grind and grind and show that these assumptions must be true in order for it to work. But even the proof in R3 ended up being multiple pages. Maybe we can shorten that a little bit. And we conjecture. And we conjecture that this particular style is going to be what gives you the optimal one in RN. On that note, I'll say thank you. If you want to read more, there are the two papers.