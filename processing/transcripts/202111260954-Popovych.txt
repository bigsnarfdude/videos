Is by Roman Popovich, method of moving frames and computing generalized Casimir operators. So, Roman, you are welcome to start. Thank you very much. And also, first of all, I should like to thank the organizer for invitation to participate in this excellent workshop. Maybe I will stop my video to make connection more stable because I'm not sure about my internet connection. So, yeah, in fact, I will talk about very elementary but still very efficient application of the moving frame method, maybe to a bit unusual field. But it is still a computation, uh usual invariance, but uh just um action maybe bit specific. Maybe a bit specific. So, this is why I will at first define which group action we will consider, and then we'll talk about connection invariant of this group action with Casimir operators, which will justify the name generalized Casimir operator and maybe will. And maybe later we'll talk about computational some facts and results. Okay, so consider finite dimensional Lie algebra and denoted dimension by n. So we consider the algebra over complex real field. And jointly with this algebra, we also consider the corresponding connected Lie group. So as usual. So, as usual, denotes a joint action of the group G on the algebra G. And of course, the image of this joint action is just the group of inner aptomorphism of the Lie algebra G. And we denote by G star the dual space of G as vector space. So then the Then the map at star from root G to the root general linear group over the space G star, which is defined, of course, in this way, is called the coadjoint representation of the Liekook G. So it is in fact a representation on the dual space to the Lie algebra G and the image of And the image of G under this mapping is subgroup of general linear group over dual space and we will denote it by this symbol at star G. The maximal dimension of orbit of this group in G star is called Zerenkovich. G star is called the Rank of co-joint representation of G and or also co-joint representation of the Lie group G and note in this way. And of course, this characteristic is the basis independent characteristic of the Lie algebra G and corresponding orbits, which in fact have the maximal dimension are called regular orbits. So So then of course functions which does not change under this action are called invariants of this conjoint representation. And we will denote the set of invariants of the section in this way. So of course this invariance may be This invariance may be locally defined, so we always can talk about some domain, but in fact we will not indicate on which domain these invariants are defined. And usually we'll consider some neighborhood of a point from the regular orbit in such ways that group g star acts regularly on this domain. Domain. Also, we can call the set of invariance of co-joint representation as the set of invariants of the algebra G and use this notation. So, in fact, this set will be the main object of the study of discussion in the talk. So, So in fact, the maximal number, which we will denote Ng, or functionally independent invariant in this set, concides with the codimension of regular co-joint action, which is expressed by this formula. And functionally independent invariance, of course, form functional basis of this set. Of the set of another fundamental invariance of the conjoint action. And we can express each element of the set as in a unique way as a function of this fundamental invariant. So to compute invariance explicitly, we should fix a basis of Lie algebra. Then we have corresponding dual basis on the Dual basis on the dual space. And then, after fixing basis, we can identify the group of inner photomorphism and coin joint representation with associated metric scope. Then, if bases fix it, so we can describe Blue algebra in the Trump commutator relation. And then, this is in fact like initial data for further computation. Further computation. And then, if we have fixed basis, so then we introduce in fact some coordinates on the dual space to the algebra g and these coordinates will be denoted by x usually. So it's just from elements of algebraic space fn. So, what is the relation of these objects to Casimir operators? So, in fact, it is well known that there is this bijection between elements of the center of universal enveloping algebra of the algebra G and these elements called the Casimir operators and polynomials invariance of G. So, this means polynomial invariance of injoint representation. Of the enjoint representation acting on the algebra G. And since this invariance polynomial, then of course we can assume them defined globally, so we have no problem with domain of this invariance. And this bijection is established by natural association that we associate basis elements of the algebra with subordinate functions. Algebra with coordinate function, corresponding coordinate function on the dual space and symmetrization operator which acts on each monomial consisting of products of basis elements in the following way. So we just symmetrizize this symmetrization can be correctly defined not only for polynomial invariance but also for rational. But if we have our basis, in fact, does not consist of rational invariance, then it is then question what means that symmetrization procedure. And in some cases, it's possible to interpret and do the symmetrization procedure, but still, it is already a question which algebra we consider. So it is specific for each algebra. But then not to have problems with procedure symmetrization, we can formally even call all elements of the corresponding set of invariants as generalized Casimir operators, which so if they are polynomial, so they are directly associated with usual Casimir operators, but if it's not, If it's not, then we will call them generalized casimir operators. So, of course, casimir operators have a lot of various applications. And in fact, in the beginning, they were started more even in physics than in mathematics. They related to some Hamiltonian conservation laws, but also rising a presentation theory. Of course, the more. Of course, the most famously known are Casimir operators of a simple algebra, which are second-order polynomials, but this is also often at the same time generalized casimiral operators also arise in the theory of Hamiltonian system. So this is why they also Also, quite useful. And this way, it is necessary to know how we can compute them if we have some for certain algebra. So what method of computation exists for Casimir operators and generalized Casimir operators? Of course, for certain class of Lie algebras, where exists a specific method. Specific methods, especially this for simple properly altopragis notable levit decomposition because universal methods work not so effectively for non-solvable algebra. And but additionally to this, there exists also two more universal approaches. So the first approach I will call First approach I will call infinitesimal, and the second, in fact, which is the main subject of this talk, is methods of moving frame. So, of course, so if we have some group action, so we have infinitesimal generator for this group action, in fact, they are given by this vector field X, which is here presented. And if we have this vector field, If we have this vector field, so then we have infinitesimal action non-invariant, and then it should be zero. And to find invariance, we should solve such system of equations where I run from one to n, up to dimension of our algebra. So, in fact, we should solve the system of partial first-order partial differential equations. Number of this equation and yeah, and then we will find this new variance, but but of course, integration even of course this system seems like simple because vector fields are linear with respect to x, but in fact it is not so simple problem if algebra is of Algebra is of high dimension. And the method of moving frame in contrast to this gives in fact almost purely algebraic method for finding invariance. So and then I will just very quickly recall some facts on Cartan moving method of moving frame in first over version. And I will simultaneously adapt this method at once in terms of conjoint representation of G with the group G on the dual space star. So yeah, we consider trivial left principal bundle over G anted with cojoint action. Anted with cojoint action. And then right regular rotation, which we will denote R hat or the quadjoint action of G on G star will be diagonal action of this quadrant action on this Cartesian product. And it is defined by this formula, as usual in theory of moving frames, equivalent moving frames. So then we have regular and free action and we will call this action the lifted co-joint action of G on the corresponding bundle. It projects back to co-joint action on G star. We add G star equivariant projection. Then Then lifted invariant of this cojoint action is a locally defined smooth function from this trivial bundle to some manifold, which is invariant with respect to lifted coadjoint action. And it is very easy here to find fundamental lifted invariant. It is just given by this formula. Given by this formula. This means that so I is lifted invariant, and any lifted invariant can be locally written as a function of I in a unique way. And if we have an arbitrary function, in fact, we can invariantize it somehow, and we first can produce a lifted invariant with this function. And by replacing x by expression for lifted invariant, so and ordinary invariant are particular cases of lifted invariant where we identify invariance forms composition with tantar projection. So, order invariant a particular function of combination of listed one would happen to be independent on hook parameters, okay? So. Okay, so to compute invariance using this method, we will use normalization procedure by Felson-Wolver. So the sense of this procedure gives the following statement that if we have fundamental lifted invariant and for some subset of fundamental lifted invariants and some constant, such system can be solved this. Uh, such a system can be solved with respect to a subset of parameters and substitution of these found parameters, found values of parameters into other lifted invariant gives precisely n minus rho and expressions which depends only on x. Then we know that rho is rand. know that rho is rank of the section, number of invariants them, and the corresponding expression form basis of the set of invariants. So what is specific algorithm in the case of generalized Casimir operator? So at first we fix basis and construct a general matrix of a joint action and Of a joint action and transpose matrix will be matrix of co-joint action. Then it's matrix with R parameters. So where, of course, is the dimension of our group, which is n dimension of algebra minus dimension of the center. Etc. Then we have representation of fundamental lifted invariant. So it is just given by this expression. So we multiply from on left by x, so coordinate tuple because it's co-joint action. And so in extended formula, so this is our fundamental invariant. Then we eliminate parameters by normalization. Parameters by normalization. So we just choose which equation we can solve and find tuple. And then, if we substitute into our components into other lifted invariants and they don't depend on parameter status, then we produce exactly necessary number of invariants. Invariance, which will form the basis of our set of generalized Kezimir operators. So, our experience on calculations of invariance for the wide range of Lie algebras and show that, in fact, this version of algebraic method, which is based on normalization project, is the most efficient because we also try. try sometimes for example it is possible to find nice invariants just combining lifted invariants but then the problem is to prove that obtain invariants are functionally independent first it is possible to check but then the that the number is sufficient to form the basis in the corresponding set and this may be quite non-trivial so but the advantage But the advantage of normalization procedure is that we, in fact, at once produce a functionally independent set of invariants and number of these invariants are sufficient to form base. Yeah, so this is advantage. And to apply algorithmic Apply algorithm even most efficiently. We can play with various kinds of coordinates in inner optimomorphismal groups. One can choose first canonical, second canonical, or mix them or even more special system of coordinates on the group. And we also can use various techniques of eliminating parameters. So instead of So, instead of solving the system of partial differential equation within the framework of conventional infinitesimal method, in fact, here we should construct the matrix B of inner aftomorphism, which is just reduced to solution of system of ordinary differential equations with constant coefficients, and then excluding parameters data from the fundamental. From the fundamental invariant constructed in this way. So we can exclude it, depends on our choice. To demonstrate this, I show very elementary example, but still very nice. And in some sense, it is illustrative for some claim. So we consider algebra G, which is G which is denoted in this way. So 48B. So this notation corresponds to, I use here notation corresponding to classification of low-dimensional algebra by Membarak Ziano. So it's four-dimensional algebra, which is have number eight in this classification. And additionally, we have parameter B. And these are all non-zero commutation relations. So and parameter B is normalized in such way that this algebra all Way that this algebra, all of this algebra are non-isomorphic. Within the infinitesimal approach, generalices mere operators of this algebra was computed in this paper. In fact, in this paper, were computed such operators for all three-dimensional algebra, four-dimensional algebra, and it seems to me five-dimensional and important algebra. So, how then do we work? We just take a joint representation for each element of the algebra. Then we construct matrix of inner optomorphisms in this basis, just to change sign data for convenience. So, it just so it is just illustration that we can play with parameters and. With parameters and coordinates for more convenient representations. So then we take this matrix B, multiply by X from the left, and then produce lifted invariants. And now we just should choose which invariance we will equate to some constant to solve the mix parameter O and which. Which this lifted invariance will be used to produce already final invariance. And here we have splitting into two cases. In this first case, so in fact we have no invariance because if you take this normalization constraint, so then we can solve with respect to hold up all the data, and in fact, we have no invariance. Second case more interesting, so it is specific case b equal to minus one. And in this case, this algebra, so g48 minus one, it's so-called diamond algebra. It's quite specific because it is metric algebra. In fact, over a complex field, where exist only two four-dimensional non-isomorphic metric algebra, so it is. Lie algebra. So it is this diamond algebra and non-solvable Lie algebra S2 plus Believe one dimensional abelian and over a real field we also have additionally two real forms. Okay and then it's this algebra have one-dimensional center so definitely X1 is an invariant. Is an invariant. So, and lifted invariant, of course, doesn't depend on parameters at all. And we can find another invariant by recombining, but then we should prove that it is really form basis with X1. And the advantage of this is that we at once produce invariant in nice form. But we also can construct one more invariant. One more invariance using normalization procedure. So, if we solve this equation with respect to theta2 and theta3 and substitute to theta4, so we will have such an invariant, and then we can combine it with x1 to produce again this 4. So, here I should like to remark that you can pay attention that, in fact, we constructed the polynomial basis of invariance. Of invariance. And it is well known that Nilpotent Lie algebras and the so-called perfect Lie algebras have basis of invariants which are fully polynomial. Perfect Lie algebra, this very simple definition is that this Lie algebra whose derivative coincides with Then type algebra. For example, of course, any simple or semi-simple algebra is perfect, but there exists also other perfect algebras. Okay, so, but at the same time, we see that this algebra is not important and it is not also definitely, but it has also both. It has also polynomial basis. And so the question is: so I don't know answer where it's a complete description of algebra, which admits polynomial basis of invariance. So this means that, in fact, all generalized Casimir operators are generated by usual Casimir operators. So it is interesting questions. And also, I remarked that. I remarked that it's also well known that algebraically algebras have rational basis. And again, it is not known for me where algebraically algebras exhaust the class of algebras which have rational basis for the invariance. Okay, so quickly review of results on Of results on which were obtained using moving-free method. So, in fact, we recomputed all results existing in the literature which were obtained with infinitesimal method. So, it's usually for various kinds of solvable, important, or low-dimensional algebra. So, a review of this results within the framework of infinitesimal method is contained in the monography by Schnobel and Winter. And winter. But here I will indicate here I indicated papers where the corresponding algebras were considered and their generalized Keismer operators were computed. So first of all, these are low-dimensional, complex and real algebras up to dimension things. So the first paper, of course, is a pioneer paper by Patera Sharp and Ternitis and House. They are computed, as I said already, generalized casimir operators for free, four, and important five-dimensional algebra, but then part of six-dimensional algebra was considered part of DOHM and then Kampo Moroshturk. Then also were computed invariants for real algebra with abelian near-radical of codimension one, it's so-called almost a billion algebras. Almost a billion algebras. Then, also for complex indecomposable, solvable algebra, whose nearly radical, in fact, is almost a billion. And so it corresponds to action of one element, in fact, generated by one single Jordan block on other basis elements. So then also. So then also was computed, were considered importantly algebras of arbitrary triangular, of upper, strictly upper triangular matrices of an arbitrary dimension n. Also solvable algebra, also triangular. Also, triangular, upper triangular metric, but already not strictly upper triangular, and special upper triangular matrices. And the last example is given by solvable algebra with Millier-Dical isomorphic to the algebra of stately up-triangular matrices. So, this algebra and so-called diagonal nil-independent. Non-null independent elements. So these are in fact algebra which can be considered as sub-algebra of solvable algebra of upper triangular matrices. So this first example were in fact computed completely in literature, but for this example were considered all only algebras with With follow values n, like one, two, maybe three maximum. And it was only conjecture about the form of invariance, for example, for this algebra. And using modeling frame methods, we easily computed this invariance. So I just present some results, but just for impression about invariance of this solvable Lie algebra. The solvable algebra. So let me consider this algebra. So, what is this algebra? So, it has nearly radical, which is morph, the algebra of strictly upper triangular matrices. And it has additionally S nilly independent elements, which acts on elements on radical, each of which acts on element of radical as diagonal matrix, where we have numbers gamma on diagonal. So, in fact, we have S ta P. As and tuples of such numbers, and so in fact, we have some matrix gamma consisting of this number. So, this is notation in the algebra. So, we should have some condition on linear independence of these elements for the linear radical coincide really with T0. And we can. And we can naturally embed this algebra into the algebra of upper triangular solvable algebra, up the triangular matrix. As basis in this algebra, we can take this as diogonal elements and canonical basis of this algebra TT0. Algebra tt0. Yeah, so triangular. So these are elements with one units in some place. Yeah. And we have commutation relation for this algebra and also we have corresponding group for this algebra. So this group where diagonal elements have the following form. Again, upper triangular matrix. So yeah, it was necessary to study this algebra. Was necessary to study this algebra because not all of this algebra say is non-esomorphic. So we have some equivalence relations on gammas, and this is why we can simplify this gamma, which are important for computation. And then we should construct lifted invariance. And but so here we will use this special triangular basis to present. Triangular basis to present this lifted invariance in nice matrix form. So yeah, we choose, of course, dual basis to dual which correspond to like an important part of the basis in the initial algebra. Yes, in corresponding coordinate xij, and these are. J and these are coordinates functions in the algebra itself. Okay, so we have this duality and here we permute index again for the nice representation. And we also have this elements, corresponding elements which correspond to diagonal elements. And then we additionally make tricks that we introduce such numbers and then. such numbers and then complete collection of xiji and yij to the matrices by zero. And thus we produce strictly lower triangular matrix capital X and non-strictly upper triangular matrix Y. And in this notation we have expression for lifted invariants. Parameters We have this parameter with B where matrix constituted by this parameter, an arbitrary matrix from the corresponding Lie group to this algebra. And then we permanently will use this playing with this specific metric, but it is necessary to emphasize that in this metrics Essential are only a part of entries. So, some entries always will be just for convenience to have complete matrix. So, to present main theorem, we introduce notation so that if we have some matrix A, so in this way we will denote submatrix where indices run from y one. Where indices run from I2 and from J1 to J2. Okay, and then if we have parameter matrix of algebra, so this matrix gamma in reduced form, then basis of the set of invariants of cojoint representation of corresponding group acting on this algebra is followed by. The algebra is followed by the following invariant. So it is just determinants of the corresponding submatrix in certain degrees. And this is also a combination of this invariance. And here I presented notation. So this is a numbers beta is some constants, but this constant I express in terms of matrix determinant of matrix alpha and this. Of matrix alpha, and this alpha expressed in terms of parameters our algebra T gamma N. So it is clearly visible from the form of invariance that not for all parameters gamma, our invariance will be polynomial or even rational. So And through having this theorem, in fact, we at once have invariance for the algebra of strictly upper triangular matrix. So it will give a polynomial basis because here we will have zero, so we will have only this part and now this element. We also have basis for the algebra of just Just solvable algebra of upper triangular matrix and also for all these like intermediate algebras. And so few words on the proof. So in fact, we choose normalization condition in such way that we equal all lifted invariants except invariants like singular, which correspond to diagonal elements, and lifted invariants which. And the lifted invariants, which like correspond to lay on secondary diagonal up under n-diagonal. But we still, so it is our like basic normalization condition, but in the course of the proof, we should add some normalization conditions for this invariance because the choice of normalization condition depends on gamma. So, but In the cost of proof, it is always possible to check that all imposed normalization conditions in all cases satisfy the condition required by the normalization procedure. Okay. And also important playing with form of sorry. Playing this form of matrices here, since lifted invariant presented in such form to solve it simpler for B, we rewrite this invariance, so this equality in this form. And once we in fact, for matrix B, we have a system of linear equations. System of Linux equations. So we impose here this normalization condition and then consider this equation. So in fact, yeah, they are of the following form. And then due to this, we in fact able to solve this system. Of course, it is still complicated, it's huge computation, but it is possible to solve. Okay, and one of the consequences of this theorem or the proof of this theorem is that, in fact, this Theorem is that, in fact, these determinants here, where this kappa is conjugate to k, so it is n minus k plus 1. So they are functionally independent relative invariance of this conjugate action for any admissible value of ka. Okay, so what is possible to consider more in this direction? So in fact, So, in fact, even for this near-radical of strictly upper triangular matrices, still not all algebras are considered because we consider only algebras with so-called diagonal differentiation, but where exist algebras with the same nil radical and which have different differentiation on nil radical. And as far as I know, this is the only, in fact, this algebra of upper triangular matrix, this is the only Borel algebra. So it is Borel algebra, sub-algebras for the first series of simple algebra. So for other series of simple algebras, their Borel algebras are not considered and corresponded. And corresponded mini-radicals will not consider it. So it is possible to try to compute generalize mirror operators for such algebras, but of course, this computation may be even much more complicated than for triangular numericals. So thank you very much for your attention. Thank you very much for your comment. Are there any questions from online or? From online audience or in person, I have another one question, Norman. Suppose you have you ever looked at the inverse problem? So suppose I give you the algebraic variance. How much of the structure of the underlying V algebra can you determine from that? It's like so given solution to find the A given solution to find equations, yeah. Yeah, that's why I call it an inverse problem. I don't know, yeah, because as I said, for example, that even if we know that, for example, invariants are polynomial, yeah, so we still don't know which algebra the whole we still don't have the complete description which algebras have. Right, so I understood that from Right, right. So I understood that from those remarks that it's it's not an easy question. Yeah, yeah. Yeah. So so interesting for me, for example, where all metrically algebras, I just not consider this problem, but maybe I will try to do this, where all metrically algebras have polynomial invariance, because of course, simple and semi-simple and reductively algebras are all metric, yeah. So they have polynomial invariance, but then we have different metric algebras, and I still don't know where they all have polynomial invariance. So I have just examples that they may have polynomial invariance. It's connected with another question I looked at briefly, but it's a different question. So when we do the algebra theory, of course, we have the structure process, Cijk. But those are But those are, of course, not invariant. They depend on the choice of basis. So one can construct kind of tensor invariance of the Cijks, but I've never seen work that says how do you do a structure theory of V algebras based on invariant structure theory of V algebras. Well, I'm sorry, Peter, maybe my connection is not good. I just listened to part of your question. William part of your question. Could you please? So, so you have with that CIK, all computations in Lie algebras are ultimately based on the Cijk. And of course, they have really. But the structure constants are not invariant. They depend on choice of basis. So, could one do an invariant theory of V algebras using the invariance? Using the invariance, the tensor invariance of the structure constants. You see, I don't know why it is relevant, but for example, I know that only description of three-dimensional algebras, yeah, like classification more precisely, were like more or less down in invariant terms, yeah, by one guy from gravity. guy from gravity uh but even usually even classification of Lie algebra is down in a non-invariant way so we work just with structural constants and try to simplify this tensor yeah this is why yeah I don't know how how much we can say in this invariant way even simple things how do you detect if it's semi-simple what is the level If it's semi-simple, what is the levy decomposition? Even very simple structure theory. I don't know how to do that in an invariant world. Peter Hayden has a question. Peter, could you please ask the question? Peter, it seems like you, Peter, it seems like you raised your hand. It seems like you raise your hand. Yeah. Sorry, I have my mute on. Beg your pardon. Yes, just to add to Peter Olva's comment there or question, even if we could do everything in terms of invariance, would it be a feasible computational method? It's theoretically nice, but actually. But actually, do we lose some of the simplicity of working in coordinates? Stunned silence. We just choose convenient coordinates here, and then this may simplify. For example, even in this computation on triangular algebra, so choose of a nice coordinate. Of a nice coordinate system was crucial to complete this computation. Yeah, exactly. It really simplifies the classification. Certainly the Schnobl and Winternitz monograph has gone a long way in choosing very elegant, simple coordinates to do the classification. Any more questions, comments? I just have a very quick question. So the last example was triangular matrices. This is one which wasn't computed before. You showed a lot that you recomputed a lot of examples which were previously computed with infinitesimal method. It with infinitesimal method. But the very last, yeah, this is a long list, but the very last example you showed is it one of those? So it's because I show it because in fact people which use infinitesimal methods were not able to in fact to complete this computation. Yes, so they only for small n, very small, like what is essential? Three, maybe three, four, because it is triangular, yes? Three four because it is triangular, yes. Three, four, uh, and that's all, yeah. And then due to this form, they did conjecture, which may be form of this invariance, but they in fact do this good conjecture only for, it seems to me, for T0 here. And then they were able to prove that all this invariant, they conjecture, in fact, satisfy this system, and then the addition. And then they additionally prove it, it was not a simple proof, that they are functionally independent and their number is sufficient for the basis. And in such a way, in fact, for this algebra, they were able to construct basis. But already, even for whole algebra of upper triangular matrices, so renewable results. This is why I presented this result to show that mo the method of moving frame can do much more. Of me and Frey can do much more in this moment. Thank you very much, Patsen Raman, again. And we will meet again at 11. So, how do we actually 