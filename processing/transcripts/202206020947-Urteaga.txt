For allowing us to present some of the work that we've done at Columbia in collaboration with Teresa Hardy at NYU and Dave, who is already there. This work was driven by Tristan, who was an exchange student at Columbia, and the goal was to try to come up with an end-to-end statistical learning approach to characterize the menstrual cycle based on noisy and missing hormone observations. So as you may have heard already from Erika, and she might also talk a little bit more about it later. Also, talk a little bit more about it later. The female reproductive endocrine system drives the hormonal cycle and its phases. So, as we observe an individual's hormonal cycle over time, the evolution of those hormones determines the menstrual cycle. The spike of the luteinizing hormone indicates ovulation, which separates the cycle into follicular and luteal phases that end with the period. So, if we are able to characterize the hormone levels over time, then we have a full understanding. Levels over time, then we have a full understanding of the menstrual cycle. However, the complete characterization of the cycle remains an open-ended research question, and in part, this is due to a lack of direct and reliable measurements over time and across individuals. In fact, the state of the art in hormone characterization work is based on daily measurements of hormones, but only for small-scale studies, such as 30 individuals. Such as 30 individuals. And interestingly, these data sets have been validated and fitted via mechanistic models that describe these small-scale data sets quite accurately. I'm sure Eika will talk, knows a lot about this and will talk more about it. So the challenges that characterize the hormones over time are due to some intrinsic factors, such as the variability. Intrinsic factors such as the variability from one individual to another, but also the variability within individuals, both in the hormone levels that over time different individuals have, which directly impact the timing of the phases and the length of the menstrual cycle. But there are also some extrinsic challenges, which basically relate to how costly it is, and I don't mean it monetarily, but in human burden, to actually have repeated sampling of daily. Have repeated sampling of daily hormones for so many people. And the goal to characterize the full cycle would be to have these daily measurements for many cycles and across many individuals. But as I say, that's very challenging and very costly in practice. So the goal of the work we're doing and that I will present briefly here is how can we infer the productive reproductive formal dynamics over time at the daily rate, but assuming a minimum. Rate, but assuming a minimal invasive pragmatic measurement setting. So basically, how can we model and predict the female reproductive hormonal parents at the daily level, but only based on few samples. And to do so, since we don't have access to large-scale data set, what we decided to do was to actually use the mechanistic models to simulate and create a diverse and realistic data set. And we do so by combining Data set. And we do so by combining the mechanistic models with data, real-life data that we have from women's self-tracking apps that allow us to ground the cycle length and the ovulation dates from real life individuals into a diverse set of outputs from the mechanistic models. That is, we run the mechanistic models with a different set of parameters and get candidate hormone level. Candidate hormone levels over time. Then we identify those that match actually the statistics from real-life users and we select a subset of those so that we can now have a very realistic but at the same time diverse data set where we do have interperson variability in the hormone levels. And the goal is based on these patterns, how can we reconstruct and predict these patterns based on only few samples? Based on only a few samples, and I think graphically that's best illustrated with these plots that I have here. So, the ground truth is the blue curves, which are given by the mechanistic model. And the goal is based on only the green crosses that are indicated for each of the hormones of interest, whether we can reconstruct in the green area, which would be Green area, which would be an inference, the actual pattern of the hormone that occurred, and also predict far down into the future. So that's basically the learning task at hand. And the approach that we decided to go with was to combine two separate techniques to combine probabilistic generative models with neural networks. And I will basically give a brief overview of why we chose each of those in the next few slides without getting too much into. Few slides without getting too much into details and leave some time, hopefully, for questions and open research ideas that we are dealing with right now. So, why individual multitask Gaussian processes? Well, the challenge is that we want to deal with non-uniformly sampled and very few measurements. So that means we need to deal with uncertainty. And Gaussian processes and their multi-output counterparts are pretty good at this. They are good because they can. They are good because they can incorporate both noisy and indirect measurements of the hormone levels, but more importantly, because they can deal with irregular and sparse sampling. So we decided to go along this route. And the way we incorporated prior knowledge was basically by deciding on a specific prior over the Gaussian process. And this is basically determined by the covariance function of the Gaussian. A covariance function of the Gaussian process. So the covariance function determines the characteristics of the prior over the function space that the Gaussian process will operate on. And in this particular case, we decided to separate the interdependency between the hormones into a kernel matrix and the dependency over time of each of the hormones with the covariance function over time. In particular, just by looking at the plots, we know. Looking at the plots, we know that over time the hormones are periodic, that they are relatively smooth. So we can incorporate all this knowledge into the Gaussian process. Once we have defined the prior, then we collect hormonal levels, sample that again, few and not regularly spaced intervals, and we use that data to learn the parameters of the Gaussian process at each. Of the Gaussian process at each individual that will maximize their data likelihood. Once we do that, then we now have an individual multi-task gaussian process posterior, but this posterior now is over any arbitrary time points within the time range that we're interested in. And again, a graphical illustration of that is shown here, where by only fitting a multitask Gaussian process to the green crosses of To the green crosses on the training data points. Now we have both the mean and uncertainty estimates over the full time range that we're interested in. Obviously, just by looking at this plot, the performance is not fantastic. We are missing some valleys in the estrogen hormone. The spikes in the luteinizing and follicule-stimulating hormones are not as sharp as they should be, but obviously, this performance gets. But obviously, this performance gets better as we get more and more data. But the whole point was that we wanted to reduce the amount of data that we have. So we thought, okay, given that a Gaussian process can fit more or less to these data points, now how can we improve this output by using neural networks? And the goal here was, okay, how can we basically use equally spaced time instance from the output of the Gaussian process to learn to correct these mispredictions? And the goal is that this And the goal is that this direct neural network is going to learn at the population level and at the equally spaced grid how to correct for the potential hormone evolutions that the Gaussian process thinks are more or less likely. So to get a little bit more specific, we can draw from these individual hormone levels that have been fitted to the Gaussian process and get S modern. And get S more than one hormone level functions over time. Now, if we learn a population-level mapping from these potential samples to the actual truth, then the goal is to back-propagate these laws to learn a neural network that at the population level will learn to correct these Gaussian process outputs. And again, to illustrate this graphically, what happens is that for the same Gaussian process that I was showing, Causion process that I was showing after we pass that output through a DCNN that has been trained at the population level, and that's that's important. So, the DCNN operates at the population level, but once we have trained it, now we can use it after the noisy output of the multi-Gaussian process. And what we see is that the expected output is much more accurate and also of interesting practice is that because we can draw from the Gaussian process, now we also have uncertainty estimates. Have uncertainty estimates of where the hormones might go or not go both in inference, as I said in this shady area, but also in prediction. Obviously, the longer we're trying to predict, the worse the predictions get. But we can see that the effect of learning at the population level, but combining only few time instant based Time instant based fitting of the Gaussian process can end to end provide pretty good reconstruction and prediction performance. So, to finally kind of wrap up, what we did was take this approach and compare the performance based on different sub-sampling budgets, meaning the maximum budget would be 70 days, which we know will probably accommodate at least two cycles for all individuals. And then we start. All individuals, and then we start sub-sampling again irregularly, not at equally spaced-time instants. And what we observe is that we can go all the way down to just 15 samples, which is more or less requiring that we only get one out of four or five days hormone levels instead of every single day hormone levels, yet achieve a pretty decent accuracy when compared to models that actually need to sample at every single day. So, to sum up, we're working on trying to improve this, but I think the takeaway message is that by combining two different approaches, we can accommodate realistic, non-uniform, and reduced sampling budgets to actually reconstruct the hormone levels over time in a somehow accurate way. And this is important clinically because now this would allow to reduce the burden for better understanding of the cycle in real life. Understanding of the cycle in real life. Now, obviously, there are some limitations with the work that we are doing and that I have just presented. Three of them that I am constantly thinking about. One of them is that the performance that I showed is very subject to the fact that we have an evidence of the phasing. And that means that we need to have at least some evidence of when the peak and the valleys of these hormones happen. If we miss a peak, then it's very unlikely that the model. Then it's very unlikely that the model will be able to basically spike again. Another issue is that the data set that we have is somehow limited in-person variability, meaning that we know from real-life data that the cycle length varies quite a lot from cycle to cycle, yet these mechanistic models provide cycle lengths that are pretty constant. So that's another limitation coming from the data set. And then overall, I would argue that. I would argue that this is somehow restrictive training setup because it is true that in deployment, we can just use 10 or 15 samples from a new user. But for the training of the neural net, we do need a population level training, which here we've done using mechanistic models. So, with this, I would like to conclude and leave some of the ideas or I think research avenues that we are thinking of and maybe. We are thinking of, and maybe people at the audience have been also thinking about. I know that you have already talked about hybridized modeling in the sense of how we can merge mechanistic models and prior knowledge with data-driven learning. Here, we only merge them as a way to generate data, but I have, and we've discussed ideas on how we can maybe even use the mechanistic models to better inform the Gaussian process kernels, et cetera, et cetera. Another interesting question. Another interesting question is how to further merge the other data modalities. As I said here, we only use the real-world cycle length and ovalation data to basically get realistic simulations, but it would be an interesting idea to combine self-tracking data with hormone levels because if the user can tell us when their period happens or when their ovulation is happening, that would allow us our models to sync even if we don't have a measurement of. Have a measurement of these spiking hormones. And obviously, the end goal and probably the ultimate goal is to run this in real hormone with real hormone measurements and understand and be able to connect how variations in the cycle length and the different phases and the different levels in hormone levels correlate or can predict different health outcomes. So, with that, I think I would like to conclude and maybe leave some time. Like to conclude and maybe leave some time for questions if you have any or any comments. Um, so yeah, that's that's all awesome. Thank you. And this wanted to see your face on part of my computer screen, but not on a please. Sorry, what Zoom's hard? Yeah, Zoom's weird. Um Weird. Nice job. Does anybody? I have some questions. Hi, Nigo. Thanks for the talk. Can you hear Erica? Yeah, I can. Yeah. I was just curious what level of complexity the mechanistic model we used was. So the mechanistic model that we used was actually basically the Basically, the Graham paper, the 13, if I remember correctly, it's the 13-dimensional state space mechanistic model that I think we've discussed many times. Yeah, so basically. I'm just genuinely curious. So basically, we took the state-of-the-art mechanistic model, and just by tweaking the parameters, we ended up with the like. Parameters, we ended up with an insane amount of different potential hormone levels. But we saw that many of those would not match the cycle length and the ovulation dates. So basically, what we do is we run tweak parameters of that mechanistic model, get a bunch of data, and then randomly select those that actually match diverse cyclings. I mean, the full procedure is explained in the paper, but yeah, that's the model that we used. And let's see. Did you vary all of the parameters in the model or just sort of a subset of them? A subset of them. It's way too much to edit. I mean, yeah, we can discuss on the specific ones we did. We tried to basically mostly vary the cycle length a little bit and also the levels, but we didn't do it too much. We can discuss the. It's too much. We can discuss the details if you want to. I know you know that model up and down. So, yeah, no. And I think it's only like my initial question was sort of based on the fact that, like, for example, like that model that you use has testosterone in it, right? But you probably don't have testosterone measurements. So, like, some of the earlier models might have been like sort of better in that sense, but then you run into having to. You run into having to use delays or deal with the delays of the model. But the model that you use doesn't have a hive, but I saw that on your on one of your slides. So like, yeah, I was just curious. So yeah, like we run all, like we run the full dimensional thing, but we only select some of those for the work that I have presented. So yes, we can simulate all of them, but we. So, yes, we can simulate all of them, but we only take some for these. We only focus on these five for this particular study. But, yeah, I mean, we could always argue or like try to narrow down whether we should even try to sample the five hormones or which ones would be best to sample. That's definitely an interesting question we could explore into. Okay, thanks. Thank you. So, I've got a question. When you look at the ovulation lengths and the measurements. The lengths and the measurements, they're noisy all surely. Do you, how do you deal with the uncertainty in those measurements? Do you treat those as real or do you have an ensemble that you use to feed into the neural network or what do you how do you deal with the uncertainty? Sorry, I didn't catch the full question, but there is there is definitely a distribution, if I understood correctly, there is definitely a distribution over. There's definitely a distribution over ovulation day and cycle length. So, what we do is we say, okay, let's draw just one sample and say, okay, we want a cycle that has ovulation day at day 13 and a cycle length of 27. And then we go to the data set and filter and get only, let's say, one sample that has that. Obviously, there's uncertainty. So there's going to be a little bit of a, you know, we don't always get the same curve. Always get the same curve because we are drawing. We're basically randomly selecting curves that match the ovulation day and cycle length that we want to incorporate into the data set. But that information about ovulation date and cycle length is not an input to the neural net. The input to the neural net is just the five hormone levels over time. Got it. Very good. Thank you. And there is a question, I think, in And there is a question, I think, in Zoom. I see Justin has a question. So if you want to. Oh, yeah, Justin. Oh, yes, thanks. That was a really great talk. I was just curious about, and you kind of hinted that you're already thinking about making it better with more mechanistic information, but how are you choosing the spatial and the temporal kernels? And I have another question attached to that. How much do you think the neural net kind of takes off? The neural net kind of takes off the need to choose a really good kernel. Great question. So, definitely kernel selection is a little bit of a trial and error. In this particular case, the specific one is an exponential periodic kernel, meaning we knew that the hormones had to be periodic. And based on looking at them, we knew that they were not going to be. On looking at them, we knew that there were not going to be abrupt changes. So we kind of hinted at those two. So it's over time, it's just we know it has to be periodic. We allow to have one, two, three periodicities, and then there's definitely some validation that we can do to improve the kernels. And over four months, we basically see that they are correlated. So we just decided on a metrics that correlates. So actually, the interesting part, I think, is to look at the trend. Look at the trained hyperparameters. And then there we see clearly that there is very high correlation between LH and FSH, and it separates kind of the matrix kind of distinguishes itself into two blocks. So I think another round of the same setup would be to actually not start with those kernels, but start with something already informed based on the simulations that we have. And then regarding the neural net question, that's a great question. And I think the answer is it is very important. Answer is: It is very important that the kernel that you use gives us at least the shape of the hormones, meaning if the caution process does not even try to give a peak, let's say of LH, just a little bit, like in the example that I show, we see that it's very nice, almost sinusoidal peak, which is not the correct peak. Then the neural net can learn to kind of squeeze that peak. But if the Gaussian process is not even able to But if the Gaussian process is not even able to give you a hint of that there is a kind of a peak over there, the neural net will not recover that. So we did see, that's why I said that it is very important to get a sense of where the peaks and the valleys are, because if the Gaussian process does not give any posterior over the fact that there might be a peak here, the neural net will not kind of come up with an idea of, all right, let's put a spike here. It's more, give me. Here. It's more give me a hint of where things might be going, and then the neural net is able to kind of squeeze and pull down those values. So it is important that the output of the GP is of some quality. And by quality, I mean, and that's why I said that it's important to get the peaks and the values, is if we miss those peaks and the valleys, then usually the prediction is pretty much flat. And if it's flat, then we don't know when evolution happens. Then we don't know when evolution happens, we don't know how the cycle operates. So, there is a lot of kind of tweaking into the covariance metrics so that the output of the GP is actually meaningful. Yeah, well, thank you so much. That's very interesting. Very good. Awesome. Nice job, Nigo. Thank you very much. Thank you. All right, so we're going to take a break until 11, this coffee, and we can talk and argue. And we can talk and argue and stuff. I do have one particular question for some of the more ODE-friendly votes. I was talking to some folks, some people the other day, and this is just something to think about. How would you cast or explain existence uniqueness, so the importance of existence uniqueness to solution for? Existence, uniqueness, the solutions for these and their impact on inference. Why is existence and uniqueness important for inference? So think about that. I'd love to hear what you think. I have some opinions, but it came up in discussing, in some discussions with some of the machine learning folks. And I think there's a disconnect in why that would be important. Of disconnecting why that would be important, that the ODE people would just know, and machine learning people would be surprised about. So, just a question for the thought. Go drink coffee. Will you be back in the room at 10:30 for discussion, or we should just come back at 11? Yeah, just come back at 11. I mean, I think people are just gonna break. So, if you want to discuss things with people, I guess people can. People, I guess, people can connect online with you. So, if you want to discuss it, Yapna or something, she could go online through it now or something. But we're going to have coffee and people will just be talking, and then we'll come back before the talk's 11, but people can talk that online. People okay, makes sense. Thank you. But you might want to do breakout rooms because we people might be a question for you. Do you hear me? Do you hear me? Yeah, it's better if you get closer to the laptop, but I can hear you, yeah. It's probably mistaken because you can go there, but six feet away. Can you hear me now? Will yeah, oh, yeah, yeah, yeah. So, my question is: basically, why what is the reason for getting rid of the mechanistic model? In other words, like what kind of building? Bottle, in other words, like you're kind of building a surrogate bottle, right?