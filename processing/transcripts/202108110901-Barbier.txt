Yes, results. So instead of doing a long and technical talk, I decided to present two pieces of works that are actually not unrelated. The first one, which is this first part on the so-called multi-overlaps, which is a class of rich order parameters that allows you to understand many things about what we call quenched Gibbs measures in space. Call quenched Gibbs measures in spin glasses and related inference models is what I'm going to discuss. So, this is on the scale of the field of inference and all that. That's a rather old work. It's a one-year-old work. Still, I think that I hope you will find it interesting. It's a joint work with Dmitry Panoshenko and Manuel Saiz. So, Manuel is a postdoc at ICTP. And on the second part of the talk, I will move towards something different, which is instead to study an inference problem, which is linear regression, so classical model, probably one of the most studied model in high-dimensional inference. And I will see it from the point of view of spin glasses. And I want to show essentially an alternative proof. This model has been studied to death by a number of people. studied to death by a number of people, including from the spin glass community under the name of the Chavinas-Tirozi model. And we essentially looked at this model from a new angle. And the nice part about it is some surprises that we discovered after mapping this model onto the inference setting. And additionally, we have a very straightforward proof with respect to the existing work. Existing work. All right, so let me start with what we call with Dimitri the phenomenon of strong concentration of measure. And the point of this part is to explain what does that mean. So we got these results in two different settings. One first paper with Dimitri only on optimal Bayesian inference and another paper with Manuel on the setting of log concave measures. I will first focus on Bayesian inference. I will first focus on Bayesian and France, and then essentially everything that I'm going to say now generalize to log concave measures. And why did we study these two settings? Because together with ferromagnetic models and high-temperature models, these are, I think, the only generic settings under which you can get such strong concentration results. So the setting of optimal VASI inference is known to everyone, I guess. Known to everyone, I guess. So you have some signal that I would denote X, some data Y that is related to it according to some model. I will consider the signal X to be drawn. So I will consider Bayesian settings. So there is an underlying prior for this signal, P0, which is factorizable. So this X, I will at the moment focus on the binary setting, but everything extends to boundary spins, at least, directly, but just for simplicity. Directly, but just for simplicity, I will take binary variables. So I have n of them, and then my data y is generated according to some model, some likelihood, okay, which is a conditional distribution on x. And the posterior distribution is given by Bayes' rule. And as I'm a physicist, I like to write that as a Gibbs distribution. So here is my exponential form. So here is my exponential form, and the Hamiltonian in this case is played by the log likelihood. And the normalization or the evidence in Bayesian terms is the partition function. So typical models that fall in these settings and that have been studied in great generality by many communities are, for example, the first model here, which is the so-called spiked Wigner model. Spiked Wigner model, where essentially you have a low-rank matrix XX transpose that is hidden by some noise. So you have some additive Gaussian noise, Z. You measure, you observe this data Y, and the task is to recover the X. Okay, and this lambda here controls the signal choice ratio. And this is a simple probabilistic model for TCA, for example. Second model is the generalized linear model. Generalized linear model where phi here is a press sensing, for example, that would be the measurement matrix. But if you are thinking of this model as a perceptron, sorry, I will stop the camera. If you are thinking about this model as a one-layer neural network, as a perceptron, this matrix phi would be the matrix of data. Each row would be a data point, and X would be the classifier. And X would be the classifier, this would be the non-linearity, this is the resulting data, and this forms a kind of teacher neural network. And the task is given pairs of inputs, which are rows of this matrix and associated outputs. Why? Can you recover the hidden classifier X? So this is the classical teacher-student scenario in the perceptron. All right, so the quantities that we like to compute in this type of settings are In this type of settings, are things like the free energy, which is the low partition function. And in the setting of inference, this is related to the Shannon entropy of the data, from which you can get the mutual information between data and signal. And from these type of objects, you can deduce the phase transitions in the model, in the thermodynamic limits, the fundamental reconstruction errors, and so on and so forth. So, these are the fundamental quantities you'd like to compute. All quantities you'd like to compute. But in this work, yes, so here is just an additional hypothesis that I will assume in this very generic setting. I will assume a kind of homogeneity property, a symmetry property, which says that this posterior distribution is equal, if I think of it as a random variable of the data, it is equal in distribution to the posterior where I permuted the variable x's. This means that essentially the disorder in the system. The disorder in the system why is data is homogeneous. You don't have special data points. Okay. And this is not a strong assumption. Essentially, any model that you can study through statistical mechanics techniques usually verify this assumption. Okay, but instead of focusing on this free energy and this type of objects in this work, we want to study something even more, I would say, fundamental about this type of system. About this type of system, so very generic Bayesian inference type of models, which are properties of what I call the quenched Gibbs measure. What is this measure? It's just the joint distribution of the data y. Okay, this data can be anything you want. It can be a scalar, it can be a vector, a matrix, a tensor, and this x1, x2, up to xn, which are what I will call replicas, which means simply IID. Simply IID samples from the posterior distribution. Okay, so conditional on the data, these replicas are independent, but they share, of course, the data. The data play the role of quenched disorder in spin glass language. So let me define the expectation with respect to this quenched Gibbs measure. So I use the usual bracket here to denote an expectation, a joint expectation over these replicas. Expectation over these replicas. So, this is my Gibbs expectation. So, concretely, these are my posterior expectations: x1 given y up to xn given y. And the outside expectation, this capital E, is the expectation with respect to the quenched randomness, in this case, the data. So, the objects we want to study in this work in order to characterize rather in a Rather in a rather detailed way, this type of measures are so-called multi-overlaps. So, multi-overlaps are generalizations of the usual overlaps in spin glasses or in statistical mechanics in general. So, R within this is 1 to n, so these are indexing replicas, is defined as 1 over the number of variables sum over i of xi1 times. Xi1 times Xi2 up to Xi N. Okay, so if you take R12, this is the usual Edwards-Anderson order parameter or the usual overlap of spin glasses. Okay, and I claim that looking understanding these generalizations is important because actually they are equivalent to all the joint moments of this quench-Gibbs measure. Okay, this Quench-Gibbs measure contains Okay, this quench-gibbs measure contains all the information about the system, okay, because it's a joint description of all over all replicas and the data. And these are equivalent to the moments of this distribution. And indeed, if you take any joint moment of this quench-Gibbs measure, which can be written generically as a product over some index set that I will call C here, which are indices for Which are indices for the spin indices and indices for the replicas. And I take a generic product of spins for different replicas, like this. So this is a generic moment. This can be re-expressed as a product of proper multi-overlaps. Okay, so for a given set here, C, for a given moment, if you want, there is an associated set of replica indices such that if you take the This is such that if you take the product of this multi-overlapse, then you have this equality. Okay. So essentially, the control of the multi-overlaps is equivalent to having a strong control of this quench-Gibbs measure. But they are, let's say, more natural from a physics perspective to study than directly the joint moments. But it is the same. Okay, so unfortunately or fortunately, Unfortunately or fortunately, let's say fortunately, these quantities are hard to study. They are difficult to access. Look at the overlap between just two samples from your Gibbs measure, from the posterior distribution in this case. This is a quantity which is standard to access and to control. And by control, I mean to show that, for example, in Bay. For example, in Bayesian inference, or for log concave measures, or for various settings, it is an object that concentrates. As long as you don't have what we call replica symmetry breaking, if instead you are replica symmetric, this is equivalent to that the overlap concentrates and we have techniques to control to control the variance of these overlaps. Instead, controlling multi-overlaps is a much more challenging task. More challenging task, and this is what we do here. And the way to access this quantity is through a let's say not standard technique but standard ID in the context of string glasses, which is using perturbations, which we translates in inference as what we call side information. So the idea is that you will define a kind of extended model, a generalized model with respect to what you had originally. The original model is that you have some data drawn. model is that you have some data drawn according to a generic posterior generic posterior distribution sorry a generic likelihood and I will consider having access to two additional types of data my first additional data or side information is this YG or for Y Gaussian which will be drawn from a Gaussian channel which means concretely what these are random variables whose mean is related to the signal x Is related to the signal x, but you see x appears like this. So the and the covariance is just the identity, which means that each entry of this vector contains information about the associated entry of x. It's a simple channel, it's a simple type of observations. And the signal-to-nose ratio will be controlled by this sequence epsilon that will go to zero, okay, such that this perturbation does not change the thermodynamic properties of the system, okay, in the proper limit. A proper limit. And this Gaussian channel is very well known. Since, let's say, I would say 15 years, it has been used to control this overlap in influence. And the novelty of the work here is this new perturbation, which are coming from what we call the exponential channel. So instead, here you have access to a matrix, okay, indexed by JK, the E refers to exponential, which are drawn from an exponent. Which are drawn from an exponential distribution with mean, okay, with a parameter, which is also connected to the signal here. And you can think of this lambda case as signal-to-nose ratios. They play a similar role as this lambda zero times epsilon n. You can think of this as something that essentially will be small in a proper way, in a proper sense. And this x ijk is a norable. Xi Jk is an horrible notation. Unfortunately, we didn't find simpler. This means the i just refers to an index, a spin index, and jk is a sub-index for this index, which means that for each entry of this matrix, you draw a random uniform index among the spins, and you obtain this exponentially distributed random variable. Okay, and these are your data. The j here, this in The J here, this index J will go from one up to a random variable, Poisson random variable with mean Sn, and Sn will control the strength of this perturbation, and K will go along all the integers. So here I'm just saying what weak means in this problem. So if this epsilon n that controls here this the strength in this Gaussian channel will go to zero when n goes to infinity. To zero when n goes to infinity, and the Sn here, which controls the number of observations from the exponential channel, goes, will go to infinite, but smaller than n, so that we have this sequence Sn over N going to zero. Then, in this case, these perturbations are weak, which really means that if you compute the free energy, for example, or the mutual information for this perturbed model, then Perturbed model, then you can show that these perturbations do not affect this object, do not affect free energies and thermodynamic properties like this, but they allow you to control these rather subtle objects, which are these mutual apps. And this Londa case here that you can think as perturbation parameters that will average over at some point in order to obtain results are drawn uniformly in some. In some sets that goes to zero when k is increasing. All that should make these perturbations big. So this first channel here, the Gaussian channel, this, let's say, classical one, allows you to control usual overlaps, which means essentially the second order statistics of your measure, your Gibbs measure. Okay. Instead, this new exponential channel, it's Exponential channel, it will allow us to control this much more difficult third-order and higher-order statistics. So, to summarize what we did, we started from a given statistical mechanics model or a given inference problem defined by an Hamiltonian or equivalently a log likelihood, and we purchased this model to obtain a new Hamiltonian, which is now conditioned on the original data. On the original data and these two additional types of data. And this new Hamiltonian is the old one plus a term which is simple, it's just a L2 norm. And here you have the Gaussian data. This is the log likelihood of our Gaussian channel. And here you have a more complicated term, which is the log likelihood of this exponential channel. And so we have now a new model, which is defined by this perturbed measure. Is defined by this perturbed measure, this p tilde, which is exponential h tilde over its associated partition function. It's what I will call the perturbed measure. So this kind of ideas of perturbing systems in order to access information about some of its statistics is not new, as I said, and it And it comes from the mathematical physics of spin glasses. And there are many names I could cite, but the more important ones I would say are Guera, Gueralonda, who thanks to these techniques obtained fundamental identities in spin glasses that have their name and that allowed, for example, Talagron together with and later Panchenko to obtain a rigorous proof of the formula of the Of the formula of the Paris formula for the free energy of the Schengen tongue-patrick model. And important names that are strongly connected and who are the original motivation of this work are Silvio Franz and the Sanctis, who published the first work on this multi-overlapse, from which we got inspiration, which is, let's say, part theoristic, and essentially, we made And essentially, we made these ideas fully rigorous and also extended them in this new context of Bayesian inference. While what they did was for spin glasses and it was, let's say, not completely rigorous. Of course, there is Eisenman and Contouchiev, who have also their own set of identities in spin glasses. In the context of in-front, these ideas are a bit more rest. These ideas are a bit more recent. To the best of my knowledge, the first person to really extend, to develop these techniques in this context was Nicola Macry, and soon followed by Andrea Montanari, who essentially did the same thing as him, but in a slightly different way. I worked myself quite a lot on these techniques. Amin did many things on that. Reeves, Jean-Christophe Morat. Reeves, Jean-Christophe Morat, and so on and so forth. All right, so here is the result that we called strong replica symmetry. So what is the setting? Let me recall what are the hypotheses. So the first one at the moment is that the spins are binary, the variables are binary. Binary, the variables are binary, so the original signal is binary. But as I said, what is stated here does generalize to bounded spins. And in the paper, if you are interested, you can look at this case. The prior is factorizable, okay, otherwise you cannot have generic results without some factorization properties. It's hard to get concentration generically. And this point two here essentially means that these perturbations that we introduced are weak, okay, in a proper sense. They do not affect the thermodynamic properties of the system. And the third hypothesis, which is the most richest one and I would say the most important, is saying that if you consider the log partition function of this Partition function of this perturbed system, okay, the one with this new additional side information, minus its expectation. So this Ztil is a random variable. It is random in the data, these three types of data, the original data, the Gaussian, the one coming from the Gaussian channel, and the one coming from the experimental channel. And this is telling you that this object has to concentrate. Subject has to concentrate. Okay. And so you have to be able to verify that in some way. So you may think that it's a strong hypothesis. I claim that it is not. Because essentially, free energies are usually the good objects. Okay. They are the ones that you know how to control because essentially depends in the randomness and the quench randomness in a rather simple way and using standard techniques. Using standard techniques using, I don't know, Gaussian Pointre inequalities, if you have some Gaussian randomness, and or if you can use pre-standard techniques to control that object. And also, I claim that if you cannot control that object, if you cannot show this inequality anyway, you are in a pretty bad shape in order to study the model. Because if it does not concentrate or if you cannot show it, it means that essentially your model is hard to. That essentially your model is hard to tackle from a statistical mechanics perspective. So, I would say that it's a rather weak hypothesis and it's something that you can concretely do. You can concretely compute and show that. Okay, and if that's the case, so our statement says that these multi-overlaps of any order, okay, do concentrate, okay, we do not control the rate. Do not control the rates, and here, what does it mean that they concentrate? It means that they concentrate with respect to the Gibbs measure of the purchase system. And again, this capital E here is the expectation with respect to the whole data in the model. So these three types of data, the same capital E as this one. Okay. And it tells you that the average variance of these multi-overlaps goes to zero. And what do I mean by average variance? And what do I mean by average variance? It means in average over these lambdas that, if you recall, were connected to these perturbations. This is what I call the perturbation parameters. You need some kind of external independent parameters to average over in order to get such concentration results. Okay. And this is generically true in Bayesian epitome inference. Let me just state that this addition, this, oops, this average here. Oops, this average here is not just a technicality. Okay, having such averages in this problem is necessary because the point is that in optimal Bayesian inference, you may have phase transitions. And this average is essentially telling you that even if you have a phase transition, if let's say you are unlucky and you are studying your model for parameters and you sit exactly at the phase transition point, concentrations do not occur. Okay, these are precisely the points. These are precisely the points where you don't have concentration. But if you average over a small window of models or equivalently of parameters around the phase transition point, you are smoothing out the effect of the phase transition. So this essentially is almost telling you that for almost every values of the perturbation parameters, which means essentially for almost every point in the phase diagram of the problem, you have concentration. Problem, you have concentration, but you have few isolated points where this is not the case, and this is dealt with by this average here. So, I see a question in the chat. Is Poisson Xi Poisson distributed? What if Xi is negative? That's a very good question. So, this, I should have said that, thank you. So, here, yes, these These things are Poisson distributed. So you need what is inside here to be positive. And that's the role of this plus one, because we are generically considering spins that are between minus one and one. So you don't have any issue anyway. All right. If that was the question, maybe I missed the point, but ah, sorry. So, here, maybe that was the question. This P0i here is a generic distribution. This is completely generic, but it's supported on minus 1, 1. Okay. Okay, so what are the consequences concretely of that result, of that variance of multi-overlaps going to zero, the thermodynamic limit? The first consequence. The first consequence is that using what we call the actually, say that if you take this huge matrix, this actually infinite matrix of spins for each, for all replicas, okay, so here the index I is again for the spin index, the index L is for the replicas. Again, what are replicas? They are IID samples from the measure at hand. In this case, that's the perturbed measure. Okay, sequence for converging sequence. Okay, we do not show that for any thermodynamic limit you have convergence, but there exists, if you have convergence along a subsequence along which this random array converges in distribution, then there exists an asymptotic representation of it, which is pretty simple. Just have a Pretty simple. Just have a function gx, which a priori is unknown, but that takes values in the alphabet of the original spins. So if these variables are plus one and minus one, this function will output plus one and minus one as well. And it takes as arguments IID random variables from a uniform distribution in 0, 1. Okay, so we can draw this huge array, which are all the possible. Array, which are all the possible samples from your complicated Gibbs measure, are is equal in distribution to this rather simple array of a function in which which takes as argument this IID uniform random variable. And here the important point is to see the dependency in the indices. The first variable depends only on I, which is the spin index, and the second one depends on I and L. On I and L. Okay. A second, maybe less abstract consequence is that if you take any set of continuous functions H J then again, in average over your perturbation parameters, or again, essentially almost everywhere in the phase diagram, essentially you have a strong decoupling. Okay, the expectation of the product. The expectation of the product of these functions applied to individual spins, whatever is the number of the spins, okay, all that average with respect to the perturbed measure minus the product of the expectations is going to zero. So you have a decoupling in the strongest possible sense, okay? And I think that you could not get something stronger. Maybe you could get a rate or something like this, but there is no stronger decoupling result, I would say. Alchemists. And let me just mention that I realized actually a few days ago, so an actual concrete consequence, even more concrete consequence of this first statement here. And because I'm working on something else, and I had to obtain independent samples of a posterior description of an optimal Bayesian inference. Of an optimal Bayesian inference porn. It was the Spike-Wigner model. And I thought about it a bit. And so, what you would do naturally to obtain samples is to run an MCMC algorithm, Monte Carlo Markov chain, Metropolis, and you would equilibrate. So this will probably take a lot of time. And then, hopefully, if you reach equilibrium, you're never sure you do, but let's say you wait enough, you're pretty convinced that you did, then you Pretty convinced that you did, then you take samples, you generate samples from this Markov chain, and hopefully they are equilibrated. Okay. But I claim that there is a much faster way thanks to this result. And what you can do is the following. So if you look at this representation here, it is telling you, for example, in the case of binary spins, so let's say these are plus and minus one, then One, then you see that the only thing, if you know this Vi, this Vi in the case of binary spins, they represent what? They just represent the magnetizations. So the average of the Bernoulli variables that you are drawing when you draw this W i else. Okay, so this Vi in the case of Bernoulli variable represents the means of the spins. And what is the mean of a spin? It means a sum over spin it means a sum over it's the same as the empirical sum over infinitely many replicas it's like if you're summing this over the over the columns okay so in a sense this representation tells you that if in some way you can access the magnetizations of your system in some way then how to obtain independent samples which are equivalently different replicas Equivalently, different replicas of your system, which means an array like this. You just draw at random coins, you flip coins with binary values minus one and one, Bernoulli variables, with a given mean, with a given parameter, Vi, which is this magnetization. Okay, so the point is, can you access in some way and fast these magnetizations, not using a Monte Carlo? Because Monte Carlo is slow. And yes, you can. Is slow, and yes, you can. So, in a large class of problems, what you can do is run an AMP algorithm, okay, which is a message passing algorithm, or this is for dense graphs, okay, this approximate message passing, or a belief propagation algorithm for sparse graphs, okay. And this will give you in a few seconds, okay, even for very large size, this sequence of magnetizations, okay, which are this. Which are these variables, which concretely is the same as these VI's. And once you have these magnetizations, what you can do is just draw at random strings of Bernoulli variables with these means, and you know that these are equilibrated samples from this posterior measure. Okay, so it's not that you remove the complexity, it's just that we have a faster Complexity is just that we have a faster algorithm to compute these means, which is AMP or BP, which is faster than MCMC. So if you do that, then you have your sample. Okay, so this was a side remark. So a question by Souvatra. So how do we calculate GX? So if you think about it for a moment, you will realize that you don't need to compute this GX because this GX, if these variables These variables are binary, plus or minus one. Okay, this gx, whatever it is, is completely parametrized by the only single parameter of the Bernie variable, which is the mean. So essentially, if you fix the mean, if you fix this Vi, this Gx is just that what it tells you is that you just need to draw, you won't get full n-dimensional samples this way. Dimensional samples this way. No, you will. You will. You will get full-dimensional and samples this way. So maybe we can discuss that a bit at the end. But let's say, okay, thanks. Thanks, Amin. All right. All right. So here I will just accelerate a bit and just say that in the context of log concave. Just say that in the context of log-concave measures, and this is really work that Manuel did. So now we are considering a generic log-concave measure, which may depend on a generic disorder J. Okay, and we have an Hamiltonian here. And the only thing we require is that its Hessian is negative definite. So we have a concave problem. Again, we assume this spin symmetry. Okay, so essentially the disorder is homogeneous. Order is homogeneous, bounded spins as before. And because now the spins are not just binary, but continuous, we need to consider a richer class of multi-overlaps where now you have multiple replicas put at any power. So you have to control this much richer class of multiple laps. And essentially, we do the same, and the statement at the end is pretty similar, but Pretty similar, but actually, it's not completely the same, it's actually stronger. Again, so bounded spins, negative Hessian, and concentration of free energy. Okay, but you see here, this is the low partition function of the original system. Now, there is no more perturbation, and this is actually a strong consequence of low concavity. Now, we obtain the result, the concentration of this multi-overlaps for any without any. Without any perturbation. This is really the Quenchkib's measure of the original system. Okay, so there is no perturbation. And the fact that here we can get, we can remove the perturbation. Actually, along the proof, we use it, but then we remove it, is a consequence of the fact that in this log-concave measure, you do not have phase transition. Okay, so there is no single point in the phase diagram where you do not have such concentration, and therefore, technically, you. And therefore, technically, you can, after you prove the result for the perturbed system, you can just remove completely the perturbation. And the consequence are the same. I won't describe again. It's just that here you see now that this concentration is again without any perturbation, but you get the very same decoupling and the same kind of asymptotic representations. All right, so I will now. I will now. So, how long do I have left? Let's say, I will try in 10 minutes. So, now I will discuss a different problem, which is a joint work with Dimitri and Manuel as well, but also with Waco Chen from Minnesota University. So, what we studied is the linear regression problem, but in a mismatch setting. So, what does it mean? So, we assume that the data We assume that the data y in this case is a vector of size m is generated as just linear combinations of some hidden signal x star. And the matrix G is what we call the design matrix, if you are thinking about regression, will be a random Gaussian matrix with IID Gaussian entries. And the signal is again factorized. So the entries of this X star are IID according. Of this X star are IID according to some round-through distribution, P star, and the noise is IID Gaussian. We are considering the high-dimensional setting, which means that the number of such data points is proportional to n, the dimensionality of the X star. And the ratio is alpha. So here there are two notions of mismatch that we're going to consider. So this is really the ground truth model that generated the data. But Generated the data. But let's say that you are a statistician that do not completely know the form of this model. And for example, you do not know the actual form of the p star. So you don't know what is the distribution that generated this classifier x star. Okay, so it's unknown to you. So a natural assumption you can make, something rather agnostic, is that you will assume a Gaussian distribution, a Gaussian higher. This is our first source of mismatch. Okay, and the second source of mismatch is that the statistician will not even know that the noise is Gaussian and additive. Okay, this means additive white Gaussian noise of variance delta star. This model is unknown to the statistician. Okay, so what the statistician will do is instead assume a factorized log concave likelihood of this form, okay, where u will be a generic function. The only thing we require is that. The only thing we require is that it's concave and we have this factorization over the data points, this index k that goes from one to n. So our statistician is Bayesian, so what this person do is to define a posterior distribution given this data, these pairs of y and the design matrix J, which is of the following forms. So it's an exponential form. form so it's an exponential form with this concave function u so the model is linear because you see that inside this u you have a difference between jx minus y and here is our Gaussian prior with a variance which is controlled by this parameter k and this is different so yes so I'm assuming that this function of course is negative and is concave okay and this is different than the base of And this is different than the base optimal setting, which would correspond to the posterior that I wrote there, where you would have a quadratic dependence because the noise is aditially Gaussian with a variance delta star, and the prior would be the correct one. It would be product of P star. But this is not what we assume. Instead, the statistic assumes this first form. This is really a mismatch problem. So the quantity I'm interested to understand is the... To understand is the Bayesian estimator for this problem, which is the mean of this posterior distribution given the data, and to quantify the performance and defining the mean square error, the large size limit of the mean square error, which is the expected square deviation between the ground truth x star and my estimator given by the mean of this post I. So you can think of this as a kind of So, you can think of this as a kind of finite temperature version of M-estimation, where M-estimation means that you are looking for the minimum of some convex function. So it's an optimization problem. Here instead, it's a sampling problem. We want to compute the mean of this measure instead of its mode. So, it's really a Bayesian point of view. So, some literature, I will be fast. This has been studied in the so this. Studied in the high-dimensional settings of random in our regression has been studied in the Bayesian optimal setting by a number of people, including my collaborators, in robust statistics and M-estimation. And here you have a full list of very smart people that did a great job on that. In particular, Suabatra, the last piece of work and closely related to the Of work and closely related to the present one, the one by Suha Vatrad that he discussed. And this is also connected to models in spin glasses. And actually, it has been noted by, if I remember well, by Andrea, I think, in one of his papers where he made the connection between this model and the Gardner model, that has been studied rigorously by Scarbinati Rossi. By Scarbinati Rossi and by Talagon. But they didn't use this mapping. While in this work, we fully exploit this mapping onto these spin glasses to study the model. And the point is really that our proof is, I would say, much simpler or let's say simpler than the one of Scarbina-Tirozi and Baitalag. So, like I said, this model is equivalent to a Scarbinatirozzi model, which I recall here. So, our inference model is equivalent to. Here, so our inference model is equivalent to the following spin glass. Here is the Hamiltonian. So, you see now there is no more a hidden ground truth signal. I use change notation for sigma, which I think now has spins. And so I have this sum of u functions of my linear projections of sigma. I have an external field, and I have this Gaussian contribution here. Okay. So, the quantities that we are interested to access, of course, are the other parameters of the problems, which are overlaps. So, there are, in this case, four natural overlaps to study. The first one is a kind of self-overlap between a replica and itself. Again, replicas are conditionally independent samples from the Gibbs measure at hand. This is the usual replica between two over. Replica between two overlaps. Okay, this bracket notation in this case does not mean the expectation with respect to the measure, but it means just the standard inner product between these two vectors. And this is the quantity that interests us most, because after you do this mapping on this Carbonati-Rossi model, or I would rather call it the Garner model, this overlap is precise. This overlap is precisely the mean square error. Okay, this is the quantity we want to access. And then there are kind of two conjugate overlaps that are sums over k of derivatives of u. And inside you have this kind of dual spins, which are projected versions of the replicas after you apply this random matrix. But so you have these four natural quantities that appear if you do computations. Okay, so what we prove is the following. So let me define the following equations. So there are some functions, psi, psi bar, phi, and phi bar that are easy to write on a piece of paper that are rather explicit. And what is important is the dependency here. So Q will be a function in the system of R and R bar, rho of R and R bar. And instead, Of R and R bar, and instead, R and R bar are functions of Q and Rho. Okay, you have a kind of you switch like this. So, the theorem is telling us that if this system of equation has a unique solution, then in L2 we have convergence of this various overlaps onto the solution of this system. Okay, and additionally, we can access the log partition function. The log partition function, the expected log partition function with an expectation over the data, will be given by a very simple replica symmetric potential, very simple function of this overlaps Q and Rho, where again Q really represents the mean square error in this problem. But here you see the difficult point is this unicity of solution. So we have worked hard to show that in general, and we didn't succeed. In general, and we didn't succeed. So, if anyone has ideas on that, it's be happy to hear about it, but it seems not so easy to prove in general. But that's life. So, this is what we got. But there is at least one case for which we could go a bit further, and which is the standard case, which is the quadratic function u. Essentially, we go back to the Gaussian setting where the status. Gaussian setting where the statistician assumes that the noise is Gaussian as it is. Let's say that the statistician still may not know the actual variance of the noise, so you have a mismatch here. But in this case, we can show that the system has a unique solution. And in this case, the MSC, which is this overlap Q, is given by a very simple function of all the parameters of the problem. So the assumed variance, the ground truth variance of the noise. Truth variance of the noise, the ratio between number of data and dimensionality of the problem, and the parameter that controls the Gaussian pile. Okay, so one, let's say, surprising thing for us was to realize that this formula is the same as in ridge regression. So this is the same formula as in the zero temperature case, which is non-trivial a priori. This a posteriori, you can argue that it follows from kind of That it follows from kind of rotational invariance of the problem because data is Gaussian, so it's homogeneous. There are no special directions in this high-dimensional landscape, but still we are in the high-dimensional setting. So it's not a priori trivial, if you think about it. So essentially, we show that in this case, the quality of inference you will do by sampling this posterior or by optimizing will be independent of temperature. Of a temperature, if you were to introduce a temperature in the model, this will not change. So essentially, in this setting, the mean and the mode of the posterior are basically the same. And I wanted to give few steps of the proof, but I will completely skip that. I think I'm already done for my time. Thank you very much. So if there are questions, please apply as well. Are there any more questions from the audience? Hi, I had a question. Great talk. Thanks. So in this first situation you talked about where you showed where you were studying multi-overlaps under perturbation, I was wondering if there are any like kind of interesting observables about the original system. Interesting observables about the original system that can be deduced, like some kind of higher-order generalization of a free energy or something. So, okay, so I should have mentioned that. Thanks for the question. So one case in which you would need to access this type of multi-overlaps, let's say, generic. Let's say generically, these are quantities that would be important in problems with an underlying sparse graphical structure. So, for example, in coding or in the stochastic block model in the sparse regime and things like this, it is important to access multi-overlaps and you can think of them as parametrizing the distributional order parameter in this problem. Okay, if you are considering dense problems, I believe that most of all thermodynamic information, so the one you need to get the usual object for inasmuch and all that, is in the usual overlaps. If you want to access more subtle statistics, probably would need in some way to compute this materials. But if you have in mind a kind of generalization of a free energy that the Of a free energy, that's a good point. I have nothing in mind at the moment. But I think it's important for sparse graphs. And this was the original motivation, actually. Any more questions from the audience? Joe, you keep saying when you put up the slide of your main result again. So, you keep saying that this perturbation is necessary, exactly. This lambda perturbation is necessary because otherwise you might kind of hit the bullseye and be spot on at a phase transition. Yes. I agree with that intuition, but do we actually rigorously know any examples where strong replica symmetry breaks in a halfway natural model? Yeah, halfway natural model of a base optimal inference problem. Yeah, I mean, take like even don't think about strong replica symmetry. If you just take the linear regression, compressed sensing or the spike being normal, whatever you want. If you are at the transition, at the information theoretic transition, you completely lose concentration of the overlap, right? Is that proved? That is that proof? So, yeah, that was precisely a question. Is that rigorous? Physics says so. It's not true. I agree with the intuition. I don't know. I'm just thinking it might be an awfully difficult thing to prove because you would have to prove very, very fine control of your system, right? Yeah, no, that's true. But, like, even in the in the curry vice-sizing model, you can show that the evaluate sizing model you can show that the the what's the the name of that quantity the yeah yeah so spontaneous magnetization you you get that yeah yeah yeah but there is another quantity that diverges at the transition i don't know i i have a blank uh but you have you have actual signs of of uh long-range correlations you have other parameters to show that so but but that's a good point actually But but that's a good point actually.