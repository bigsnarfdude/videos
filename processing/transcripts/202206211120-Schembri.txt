Well, thank you to the organisers for giving me the opportunity to speak. Thank you to Vincent, who figured out all the technical support. I tried loading Znoom, had to update it. He got me an HDMI cable. Thanks also to Drew because my port connection to the LMFDB timed out and he showed me how to do it in the last break. A different talk otherwise. Otherwise, um, so yeah, the general opinion I'm going to be talking about today is something many of you have come across, I'm sure, maybe all of you, which is that we love equations, we need equations, but sometimes they're just too big. Too big in the sense that we might need them to fit on a slide in a talk, we might want a computation to run by the time we leave the office and go home, or we might just want there to be no set problems. We really need equations to be formed. But we really need equations to be small to use. So, specifically, I'm going to be talking about these branch covers of the project design. Specifically, we've been working on fairly maps, whether it's what I say should hold for general branch covers. So, many players are equipped with a branch coverage project, as we saw in this talk. So, the context for this work is that my collaborators. My collaborators, Sam, Chavon, and John Poit, they have this database so they can compute belly maps, along with Mike Musty and Mr. Billy Incisely, who's in the audience. And they work really hard to compute these belly maps, but they come out very large. And so they've been stuck in an MFW beta for a while. I'm not actually sure how long, but longer than I've been a postdoc, I think. A little while. So, yeah, this is really necessary to get out of LMFDB beta and share it with the world. We need to have the equation. Okay, so by the end of the talk, hopefully you'll understand how we get from these models. This one's not too bad on the whole, actually. But we have some nice smooth models and some belly map. And what we're going to do is essentially treat the belly map as Essentially, treat the belly map as one of the variables, and then we're going to produce a plane model. So, we'll get to something like this, and we may properly. So, a belly map is morphism of curves ramified outside of three different points. It's up to linear transformations of V1, so it picks them to be zero. So here's an example. Take this polynomial as a map from P1 to P1. And if we factor it, we get, well, we're going to put that an x squared and that it's going to be ramified above zero, something with modifistic two and something with modifistic one. So on the right-hand side, here we can factor it and Here we can factor it, and you see it's ramified above zero. Then, when we take minus one, it's ramplied above one, which is easy to see as well. The fact is like linear time times a linear time squared. And you can see this from the diagram on the left. So, everywhere except these three control points, it's going to be like if we were to remove them, it really is like a covering space. So, we have these sheets. We have these sheets except where it's ramped where they get glued together. So this is the famous theorem about Edmath, which is that a smooth predictive curve defined over the complex numbers can actually be defined over the accurate closure for rationals if known if it makes that. So there's many reasons to So, there's many reasons to care about belly maps from their connection to decimes, modular curves, the adverse Galileo problem. I won't go into those today because I'll spend most of the time talking about how to reduce them, but I'm sure you'll have your own reasons for caring about building maps. So, there's like I said, there's a database. There's a screenshot of the homepage because I wasn't sure whether this is actually going to work, but I'll Actually, going to work, but I'll show you what this looks like. As you can see from the URL, I'm not actually on the beta version of the LMFDB. I'm running a Samsung Omnis branch of the LMFDB from my laptop, which is the thing that disconnected. Is the thing that disconnected. So the only difference is that on this branch we have all of the reduced maps. If you go to the MF2B beta, you'll see this database there, exactly the same of the reduced maps. So here you can see you can search by all of these different parameters. And I'm going to click random belly map and hope that there's a smaller one. Hope that there's a smaller one than the one that was confusing. Okay, great. So here you can see under curve, that's going to be a plane model of our curve. And the value map is one of the variables. So you're thinking, okay, that looks terrible. But hopefully, when I switch to the standard smooth model, Model, it will be a lot worse. Okay, then I'm not disappointed. It's all lined up. In terms of lines, it's pretty small, that's true. So, here's a couple of other So, here's a couple of other ones that you can see more clearly with your eyes. Here is a better one. So, here's the model over the rationals. And it comes from this map. So, you can see that it's a lot neater, coefficients are much smaller. Smaller. Just in terms of literally, you know how big they are. I guess they're kind of similar, but the plain model will be okay. So, hopefully, you're convinced that there really is a need to reduce the effect maps. So, how do we actually go about doing this? And it's essentially two And it's essentially two steps or two and a half steps. So, one is we actually create this plane model. So, the idea is that we're going to use the ramification points, which is the special property of the belly maps, and we're going to look at other small functions by using these ramification points. So, we look at elements in the function field of the curve and Occur and we're going to produce them using divisors and small points that were given to us by the bell. And then we're going to eliminate variables to actually get some plain model for the original code in terms of the belly map and this new small function. And then once we have this, we can actually do some relatively basic things, which is just Basic things, which is just scale by elements of the field. So, what we can call gm reduce, because we can, so we have two variables and we can just scale each variable and then essentially clear denominators at the end. And this turns out to be pretty effective both looking parameter prime and then oh, and the Oh, and the sort of half step is that the way we choose these small functions is we need some sort of sorting algorithm because if we run over lots of different points and elements of the function field, even for a fixed degree, we might get a lot. And that really slows down the algorithm. So we have this way of sorting, which we know that ahead of time, we've got something small in the end, right? And the way, so the soundbroad proposed, the way it works is to just reduce the belly map of some large kind of production and see how many homials it has. If it has unnomias, it's just easy. And we've run some kind of tests. Would it be someone that really folks a lot wouldn't be sweet? We haven't done some proper diagnostics on it, but it seems like this way of sorting for few monomials actually really is the best thing most of the time. It's normally like in the top five. And when I say best, we're not using a very technical measure. We're just measuring by the string size of the thing that we're doing. And we want things to be small, so So, to create the plane model, well, we can look at any element, any element of the function field. We can look at its divisor. And here we have, so we take the valuations and then we can just look at the support, which is everything with valuation theorem. For the value map, For the value map, we want to look at the ramification points. So we're going to look at the support of pi and pi minus one. And we want to create by getting bigger and bigger degrees. So we're going to create essentially a pool of points, which is the set T and we're going to create more and more until we're satisfied. Create more and more until we're satisfied. So we have this set which these sums of points. So we can consider the Riemann-Rock space in which we ask for the poles to be constrained by the ramification points that we started with. And we'll consider those of dimension, the vector space of dimension one, so that we can. So that we can scale by this gm action. And then to actually create a plane model, well, we take our small functions and we're going to relabel phi as u so that it looks like a variable. And then we're going to compute minimal polynomials. Then we can compute the resultant to get some irreducible polynomial for both u and v satisfy. U and V satisfy. So this is because the resultant is going to live in the elimination ideal, and then you can just project away from the other variables. It's possible to do directly with Groban bases, and I think Sam tried pretty hard to do that, but it turned out to be kind of slow. The resultants also kind of slow, but the Grodner bases was worse. So this is the best way we can. Way we go, and then we have a possibly singular plane model, and the value map is just going to be projection onto you. So the fact that it's singular, be highly singular, is a feature, not a product of our methods, as I'll show in the next example. You can do things to retain. You can do things to retain the smooth model and try and make the value map nicer, but it's not going to do that much to the map because using these functions like minimal model, of course, they're amazing on the curve, but we're interested in both reducing the curve and the belly map at the same time. We think about it as reducing the pair. So using these functions doesn't really help with the belly. So here you can see I've taken Here you can see I've taken the minimal model and just pushed forward the value map, and it does it does look a bit nicer. But then if we run this algorithm where we try and find the best small function under our sorting algorithm, create a plane model from this. And now we can act on it by the number field, just rescaling the variables. You can see that You can see that it's done two things. Yeah, one, the p-adic reduction, and then one normally the units, or so the units have zero seconds here, which is reassuring. And now it looks a lot smaller. Okay, so the second step, how do we actually rescale these polynomials? And this is just a question. These polynomials, and this is just a question about polynomials now. How do we rescale them in an optimal way? So we're going to consider prime ideals and units separately, partly because we want things to be integral with the prime ideal case, although it's possible to consider both using similar techniques. Are using similar techniques, we'll have to do slightly different things because we've been cowski embedding and things over the reals in the units. So starting with the prime ideals, well, we understand how to substitute in variables pretty well. And hopefully, we can choose them to be optimal. So we sub in A, B, and C. These are elements of K. And we want two things. I guess the first being the most important, we want the valuation of the coefficients to be small when we work for each prime. And actually, in our codes, there's an optional parameter which is for G to be integral as well. Although everything that we've run has been integral because they look kind of similar, although it'll be worth testing out, but they're actually like. Out, but they're actually like similar in all cases. Just looking at examples, they look similar when you work into what so we want the sum of these valuations to be minimal, and this is fixing a prime, and we want each valuation here to be bigger than or equal to zero. So that's the integral condition. And I'm just going to assume that chaos class number one for a moment. This is actually how I code started. But then to actually consider the general case, we need to essentially stitch all of the crimes together in a way that makes sense. So just for the illustrative purposes, Proposed as class number one for a moment, we're going to work prime by prime. We're going to work prime by prime so that A, B, and C are going to be powers of a prime ideal. And we want to minimize this function. So this comes from the top condition where we want that to be minimal. You to just expand that out. What we're solving for is the valuation of A and the valuation of B and the valuation of C, which is the X1, X2, and the X group. And if you see the Group and if you see if you expand that out, then you're minimizing this linear function. And then the constraints to be integral is, yeah, if you expand that out, you can quite easily see that this is just these constraints for each code. So, these are just minimizing linear functions over convex regions. So, one of the nicest things you can do. So, this is an interdollinear program, and you put it into Magma, and it has the infrastructure to do this. There's some former Magma that has all of these things that I've never seen before. And yeah, you just ask for the solution in integers, and it returns to. Integers and it returns the solution. So it's quite script. Of course, if it doesn't have class handle one, it's a bit more tricky. I'll get to that in a second. So let's look at an example. If you have some function, or this is actually one of the plane models that pops out in our algorithm, but unreduce. So you can see that. That so the planes are defined by these constraints, and so you can see the first one, you have two and zero, so two in the x coordinate, there in the y coordinate, and then there's always a one z coordinate, and then the valuations give you where that thing essentially sits, and because you're minimizing a linear function on a common A linear function on a convex region. So you can imagine this is a function that's some constant k. So it's going to intersect this region. And we make k smaller and smaller. And it's quite easy to see that minimizing this on a convex region has to be at the vertex. It could occur. It could occur that it's not just a vertex, but it's like two vertices, and the plane happens to be the same slope. It's the joining the two vertices, and you get convex region, we get some compact region, we get some polytope, and you can take one of these and see which gives you the best one. We did actually try doing this, I think that's how the code started out, but for some reason, makes. It out, but for some reason, making polyhedron polytopes in microwave is really slow, whereas solving stellarian approaches is pretty quick. With this method, you just get one solution to a solution. And I guess it was possible and something that you can try is to see if that region the best solution is actually not just point and then maybe try the points. Try the base. So, this minimal solution actually occurs at 4, 2 and minus 12. So, probably wouldn't it probably take you a while to figure out by hand, like, what should I do to that polynomial F to make it as nice as possible? You just substitute this in, and it gets rid of the twos and all of the coefficients, except Okay, so now if we consider any class number, so the primes don't have to have a principle, we're going to do a very similar thing, except we have to consider them all at once and then just answer these powers to ensure the thing we end up with is actually right. It's actually nice. So we let A just be some product of primes, same with B and C. And we're essentially going to be solving for the A, P, B, P, and C P. And so we want to minimize this linear function. And we're going to scale each piece by the log of the norm of the ideal. And this is because you would, you know, you'd Is because you would, you know, you'd rather have two it than three, so we just give it a weight. So, hopefully, this like because we're going to have some condition of being trivial in the class group, we can move primes around and we just want to get smaller primes. And then we have the same integral condition for each prime, and then we have this. This principal condition, which is that the A, B, and C need to be principal. And you just need to introduce a new variable for each generator of the class group. So we end up with this big interdelinear program. So there's however many primes you have, and the primes are coming from possible whichever primes is divided because. Whichever primes just divide the coefficients of the polynomial. And then you times that by three, plus whatever you're getting from the class group, you have this very big interface linear program, but it still works. You can still put it into magma and it still gives you something good. Maybe I should say it actually, despite it being so large. Actually, despite it being so large, it generally takes about one second. So it's really quick. Okay, so now the units, we have to consider them in Cowski embedding. And now the condition we want, there's no integral condition, but we're really just going to minimize something. And the only thing we can minimize is that we want this to be small in the cluster. In the quadruped cluster, or each coefficient. So let's take an example. So you can see this with this diagram. So we're going to take q root 2, m is going to be 2, and we're going to have one fundamental unit. And if we look at the Minkowski embedding of the units, well, it is in this trace zero microplay coming from the fact. Coming from the fact that it's a normal one. Now, if we take one of the coefficients, so we take the first coefficient here. In R2, it's actually going to live up there. And when we multiply, when we try and scale by units, all we can do is. All we can do is move along this trace zero hyperplane, or really the discrete subset in this trace zero hyperplane. And so the question is, how do we actually scale this coefficient, try and make it small? Well, the reasonable thing to do is just ask for each coordinate to be scored. So So we want to be close to this average vector, which is going to be the same in each coordinate. So it's close to zero. And then we're going to try and multiply by elements that are going to make this close to this arbitrary. So that's the average vector there on the right. And when we substitute in our units, we're minimizing this function over all coefficients. So this is asking that all coefficients be small or as small as possible. And the units have given us a certain amount of Certain amount of trouble. So, one way of thinking about this is we have some linear maps and we have basically a sum of applying the linear map. And we're trying to make this small to a bunch of different vectors. So, if you were to imagine the linear map was the identity from some real vector space to itself. To itself, then we'll be trying to minimize. Um we're trying to find some point which minimizes the sums of distances to that point. And I actually thought this was just the average of all of the points. It turns out this is a relatively difficult problem in applied maths. I don't know if it's difficult, but it's not. It's non-trivial. And so, this is purely for illustrative purposes. Being close to summing the distances so that the distance is minimal is this thing called the geometric median. I guess we're doing like a skewed version of the geometric median. So, the way we work around this at the moment is that we use the L1 norm on Rn. And the reason is that you can. And the reason is that you can actually turn when you just have a sum of absolute values, turn this into a linear function. So absolute values, they're not a linear function to begin with, but you can turn this into an equivalent problem, which is having a linear function and trying to solve it on some convex region. And we get some humongous linear program. So we have to introduce variables. Program. So we have to introduce variables for almost every term. So every coefficient and every embedding needs a new variable. So you get this humongous new program, which is maybe quite hard to debug. It's hard to see where things go wrong. But nonetheless, in almost all of the cases, it gives you something which is reducing the size of the equations. The remaining cases, it is the Cases, and it's stake-haulted on this affair few times, maybe 5% of the cases. So we actually try something else, which is to just go out in more and more units. I think it's smaller each time. And then we maybe try some random things to see if it gets smaller when you leap around. Doing lots of evaluating the function to see if the string size gets smaller. And when the belly map is really humongous and we get a segbole, then that's it. Of the nuts. Okay, so here's an example. I would normally apologize to you binoculars to see that map, but this is half the point of the talk. So that's our map by 36 belly map. And And I'll show you how each function works. So we create a planning model. This is sorting out of these functional field elements. And again, the smallest one, according to our sorting algorithm. And then we can try and do this GM reduction. And I'll first go with the p-adix. So p-adics don't do a great deal there, but then you put it for units. Um, you put it for the units, and the unit three does well. So, this is um No Melkis' example of looking at the inverse Galois problem, finding Galois groups for isomorphic 2023 and On the slides, it doesn't look like it's improved a great deal. And I guess that's a testament to GNOME's map. It's already very nice. You can see that the coefficients are somewhat smaller. And especially if you look at the coefficient C, here's the fair bit bigger than Alice. And I will risk demoing it in Magma. Okay, so this is Gnome's. Okay, so this is GNOME's file. And the function is supposed to create some plain model and reduce it all at the same time is this best model. And it should take 10 seconds, but it's the same as earlier. And you can see that it's going to get a bit smaller, like smaller than you had to tell from the slides. So you're trying to do something computational with it. So it did tell me. Uh, so I don't have a proper analysis of the runtime. So, instead, I'm going to talk about what some of the issues we've come across were still. Come across where still are how we've solved them, what we'd like to do about them. So, the first thing is that when you have maps like notes, for example, you have so many coefficients and you want to figure out what are the prime ideals I should look at. And you run through all of these coefficients, you try and figure out what are these prime ideals. This takes too long. It's just takes too long. We were using trial division, trial division function in magma, but it really just sometimes it just doesn't seem to terminate. And the way we solve this is to just fix a bound. So if it's really large, fix a bound, say like 10,000 and just try and reduce the price. Let's move to 10,000. That's what trial division does. But that's what trial division does, right? Um, well, we create a big set of all like integer primes that appear in the numerator and denominator. And then I think it tries to figure out out of that huge thing, which ones are actually primes. Yeah. There's one extra thing. If it pulls, if you have a number which is smooth and then a large prime at the end. And then a large prime at the end, it will want to certify that that last number is prime. And we don't care. So we have, and I'm not sure that that's the only thing that it's doing. That's why there should be like a proof equals false or something to say. Yeah, so that really helped a lot. And then results and factorization, I mentioned it earlier. I mentioned it earlier. So we create the result, and this can take a while, but I believe it's actually still a very efficient function. It's just that the things we're feeding into it are very big. So there's nothing we can really do about it. And then we have to factorize to create something irreducible. That also takes a while. The way we work around this at the moment is that we have some sorting algorithm where we look at these small functions and we just look at not that much. And we just look at not that many of them. Actually, we look at 10 by default, and then there's like a linear map where if it's really big, you just look up once, so you don't have to do this many, many times. It's just a function depending on the string size of the input. So, like I said, there has been quite a few issues with the L1 not and it's been And it's been somewhat difficult to debug because it often just segfaults and exits the session. So you can't really tell what's going on. But it looks like the reason it does this is because you're solving something over the reals in many, many variables. And we actually want an integer solution. You have to set a lower bound on these variables. So we set like a really, a really low bound. Like a really low bound so that we see everything, and it's just trying to find better and better approximations to the best solution in integers. And so it ends up going to all of the lower bounds, it would seem, and it just takes it so long that at some point it step false. So, one way around this is to perhaps just bound the region somehow so that it doesn't do this. The other thing we're looking The other thing we're looking into is working with BR2 norm. Because, yeah, what I described earlier is we have the sums of norms of things that are close to another vector. And there's no reason to use the L1 norm necessarily. The L2 norm might work just as well or better. We'll see, but yeah, we'll try and prevent that. The number of deals too large. The number feels too large. This just slows us down everywhere. So, yeah, when Sam first let the computations go, most of the maps finished very quickly. It was great. But then there are a lot of maps that have just been running for two or three weeks. And it's not really great. And I think part of the problem is just that if the number feels too large, like you can't compute the units, for example. The units for itself because it's not confusing. So we just, yeah, we just try, like the naive thing I said with the units, we're just trying to scale back some of the computations. So potential applications, this is still a work in progress. You can see from the fact that I had to run the LMFDB on my laptop. It's not in the full LMFDB yet. So we'd like to hear from you if you have any ideas. So, we'd like to hear from you if you have any ideas about applications. If you have any maps that you want to send us, we'd love to run our algorithms on them. If you've had any experience with some of these, like L1 norms or something, we'd love to hear about that. But yeah, things we'd like to look at is, well, more belly maps. So you can send us our belly maps, we can try and incorporate them together in FPD. Modular curves, and more generally, any maps, not just ramp on round power. Maths, not just unramplified away from three points, but unrampled away from any number of points. And then we're not sure if this would work or not, but it'd be interesting to try if we just have some small points. We don't have a prescribed map, but we can create some and then do the same thing with a plain model. As long as you don't mind that it's singular, then there might produce some features. Okay, so the summary is we've made an algorithm. We've made an algorithm where we want to reduce both the curve and the map that it comes with, and we have a reduced pair, reduced meaning small in string size, literally, yeah, also plain model, possibly singular, with this action of scaling by the field. So we've run this on all of the maps in the LMFDB now. LMFDB now. A lot can look a lot better, as you can see from the demo I gave earlier. There's still some work to be done. Thanks for listening.