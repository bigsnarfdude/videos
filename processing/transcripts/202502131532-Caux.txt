Okay, well thanks a lot. So thanks for being here. You know, I guess when you're old enough and you work long enough, you get invited to conferences whose titles you don't necessarily resonate with completely anyway. Because for me, exact solutions certainly resonates. Quantum certainly resonates. Information, well, I guess I information as well. Entanglements, I must admit, it's not my way of thinking. I do other things. Topology, I don't do. Topology, I don't do either. Quantum circuits, circuits, the only circuits that I really talk about are the ones that I use on my laptop here to produce the data that I'm going to show you. But nonetheless, I hope that I'll give you a bit of an entertaining talk about some speculative things that I've been thinking about for a long time and which I hope to finally be able to make progress on in the coming weeks. So it's going to be very, very different from the other talks where you've heard beautiful results from people who have been proven and peer-reviewed and that sort of thing. Been proven and peer-reviewed and that sort of thing, this would absolutely not pass peer review at SciPost. Okay, so it's the way it is. But nonetheless, I mean, I hope I can convey some interesting things. I'm going to focus on one model, which is the Beep Lininger model. However, what I say really is not associated with Diebleninger. It's certainly applicable to any integral model. I hope also that the ideas behind that are going to be a further utility for other places. Further utility for other places where integral models can be used. But just as a matter of introduction, before I get to the meat of the stuff, let me just remind you of a couple of things. So like Frank, I believe, no, it was Nathan who said, yeah, like everybody knows, beta equations, blah, blah, blah. So that was the right attitude. And essentially, like everybody knows, the Lieb-Lineger model is a model for interacting bosons in 1D with a point-like interaction. With a point-like interaction. Considering the repulsive case, although the attractive case is also solvable, the eigenstates of that can be written formally as a beta-Ansatz wave function, so a combination of plane waves with judiciously chosen amplitudes between the different waves in order to solve the Schrodinger equation. Okay, so again, I presume that you live and die on these equations. Not necessarily, but you just know that the Schninger. But you just know that the Schrodinger equation can be solved exactly, at least in principle. And the question is, what you can do with this. Okay, so what you can do with this, well, of course, you can construct a basis of eigenstates. And each eigenstate is characterized by one particular solution of these beta equations. If you know how to solve these nonlinear coupled equations and find all the solutions, you have a full basis in principle. Although the use of the beta beta answers is very impractical, using the wave function itself. Using the wave function itself is very unfriendly. There are other technologies available that make a lot of physical results accessible to, you know, if you're willing to do a bit of computation on this. Okay, so most trivially, these systems are characterized by the existence of many, many charges. The eigenvalues of these charges, of these eigenstates, are trivially computed. In this case, there's just power-sum polynomials. We have one product sign too much, right? I'm sorry? One product sign too much? Yeah, yeah, there was. Uh yeah, yeah, there was uh uh indeed yeah so I I had ten minutes. I have to say I wrote this talk from 3.15 a.m. until 7.30 a.m. this morning. So there will be typos. Yeah, I volunteer to spot it. That's good. Okay, so what was I meaning when I said technology was available to do quite advanced stuff? The most important technology Advanced stuff. The most important technology that goes beyond the wave functions themselves is associated with the algebraic beta ansatz, which is this really beautiful framework with which you can actually construct wave functions by acting on some reference state with some creation operators that you don't necessarily need to know its constituent form or constituent representation in terms of the original type side dagger operators. You just know its commutation relations, and essentially it's enough. Essentially, it's enough. These ABCD operators there, they form some kind of quadratic algebra. And you can control that and use these things to then translate all questions that you have about eigenstates, operators acting on these states, matrix elements for these operators, between states. You translate all of that into algebraic questions in these ABCD things that you're able to compute. Okay, so for example, you know how to compute matrix elements of Compute matrix elements of physical observables in these models. So, for example, for the Lieb-Vinegar gas, you could compute the expectation value of the local density operator. You can compute the matrix element of the local density operator between Bra and Ket states. And yeah, you need a few things for that. First of all, you need the norm for the beta eigenstates. This was given to us by Michel Gaudin and also others, like Barry McCoy for the spin chain, but not. I for the spin chain, but not. These things are just known neuromorphic functions of the rapidities associated to an eigenstate. You can just readily compute these things. Determinants are the instrument of pleasure in these things. Everything is expressed in terms of determinants, which are readily computed if you know the rapidities of your state. So that's for the norm. Then for the matrix elements, you also need to know how to represent the physical operators in terms of these ABC. Operators in terms of these ABCDs. This is known as the inverse problem. There's a solution to this, namely a mapping between these operators that you use in second quantized form and the ABCDs that you have. And then finally, there's the crux is Slavnov's formula, which allows you to compute the overlap between a state which is on-shell and a state which is generic. It doesn't have to be on-shell. So, armed with this thing, this is again expressed as. This thing, this is again expressed as a determinant of neuromorphic functions of the rapidities involved in both states. You write this as a determinant, and equipped with this. You can then ask me what you want to know about this model, and I will get the machines working to compute the rapidities of the states involved, feed those rapidities into those formulas, give you values for the matrix elements, and then you use correlation to. Okay, so that's the kind of broad outline of the approach that's been used for now a couple of decades. Used for now a couple of decades to compute interesting things in there. So, dynamical correlations, they're the most important things that you can get out of these considerations. So, in general, you'll be interested in two-point functions of operators, two-point functions, because most experiments are still in the linear response theory regime. You use the Pubo formula, therefore you just need some retarded correlations between observables. So, our challenge is to compute indeed such correlators. Such correlators that are insertions of operators at a certain space-time point, coupled with another insertion at a different space-time point, and then you just do the expectation value of this. So, this is extremely important because really, literally, I don't think that there is a single experimental observable that you have ever come across which is not described by linear response. So, I think it's very difficult. Of course, you've got experiments on quantum dots and things like that that are. On quantum dots and things like that that are beyond the linear regime, the vast majority of everything you see is linear response. Okay, so these two-point functions are really particularly important. Now, those correlation functions, if you happen to have an eigenbasis for your system, are readily expressed in a so-called Lehmann representation, which is a summation of contributions from the possible intermediate states. So you imagine that you have a first insertion of the operator here, and you insert a resolution. Here, and you insert a resolution of the identity in here to sum over all the possible intermediate states that jump from the original state alpha to the intermediate state and then back to alpha. Okay, so the correlation here is just given by rates here. Think of Fermi's golden rule. Fermi's golden rule gives you the rate, so the kind of probability that goes into a state from another as a function of time. Function of time. So these Fermi-Golden rules rates here are just summed up over all the available intermediate states. And the response function that you see in this thing is just a series of delta functions in momentum and frequency at the excited state, energy and momentum above the state you're considering, modulated by this rate here. That's given by the mod square of the matrix element. Okay, so this is really what you learn. Okay, so this is really what you learn when you do basic quantum mechanics. In the good old days, you used to see that in your second core in quantum mechanics. These days, I'm not quite sure which number you need, but hopefully you still see stuff like that when you start talking about at least experimental correspondence for these things. So our general challenge is to try to compute such things. It's a real object of interest. And that's when, you know, so I knew how to use computers, I knew big onsats, and somehow, you know. And somehow, you know, that's what I did back then. I wrote lots of codes to do that, and the code was called abacus. And that's from one of my posts that actually was moving in London at some point, and we saw that track, and he thought that that was cool. So how does that work? That code is essentially simply taking the state that you're computing the correlations on, constructing all the intermediate states that it thinks are contributing significantly to the correlation, and then To the correlation, and then solving the beta equations for those states, feeding the results into Svablov's formula to computer determinants, and just summing everything up to obtain correlations. And that little movie here was a movie of the density-density correlation in Liebline as a function of momentum and frequency. And the color coding is just giving you the amplitude of the scattering. So you can do that for Liebline. A lot of work on spin chains as well. So that's kind of the main use for these things. Main use for these things. This was for 256 particles by then. It's an old movie, and you just tune the gamma. I think I tuned it from like 1/4 to 16. So you're not completely in the Tongado regime at this stage, you still see ununiformities there. But these days, for ground state, correlation functions can go to about five hundred variables. Can go to about 500 paraguits. And depending on the value of the interaction, the calculation is more or less difficult. If you're at high C, which for me means much more than probably four, four is the most complicated. If you're at high C, it converges very, very quickly. If you're at low C, it's ironically a little bit more difficult because the solution of the beta equations is more difficult. But the number of matrix elements contributing is also greatly reduced. Greatly reduced. So the most complicated is about C is equal to 4, 4.3, which is also the point where the energy, the kinetic energy density, and the interaction energy density start coinciding. So the system doesn't know if it's weakly or strongly coupled. So that's it. Okay, so that's. Okay, then so that first plot was about zero temperature. And zero temperature is easy because you start from the ground state. Because you start from the ground state, everything's like a Fermi sea of available particles, and you just compute excitations moving away from the Fermi edge step. If you're trying to do things at finite temperature, however, there's a huge increased complexity to the problem. Why? Because suddenly, what was your Fermi C has become dressed with finite filling borders around the edges. And that means that the number of possible excitations, if you imagine. Possible excitations, if you imagine moving your quantum numbers around, they become much, the number of those becomes much greater. So it's created entropically many of these excitations can be created. So this is much more difficult. So back then, when Milwaukee Panfield, who was my PhD at the time, and I were looking at that, what we did is that we used the logic of the representative state. So we used the thermodynamic beta ansatz to find. That's to find a representative eigenstate sitting at the saddle point of the Gidds distribution for this. So that means that we compute a finite temperature correlation function on a single state that happens to sit at that saddle point. And the properties of that single state are thermal. And you can show that if you move around the thermal saddle point and you compute the correlation, you get the same result. Modulos and little details, dark and bright lines. Dark and bright lines on the edge here that depend on a microscopic placement of the quantum numbers. But that doesn't matter because you should sum over the different positions there a little bit to smoothen those out. And so these computations are immensely more complicated. You can only go to much smaller system sizes. And then when you go to about 60, you are talking like a billion intermediate state contributions before you get anywhere. Contributions before you get anywhere near some rural saturation, which is significant. But you can parallel for the intermediate state. Okay, so now the intermediate states, of course, here Hilbert space is infinite dimensional. The states that contribute significantly to the correlation are a measure zero subset of the states. So there is no true Monte Carlo thing that's going to do that. The way the code works. The way the code works is that you kind of know which states you create from the operator you have. And so, my code has a recursive logic using a kind of principle of continuity of the intermediate state contributions based on small deformations. So, it's going to follow the state families that have the large contributions. And it's coded in such a way that these directions are descending, always decoding. Descending, always decreasing. So it's like you're getting off a mountain and you're always taking the path of least descent. So it's the unsteepest descent method on this. But once again, in terms of all the states available for intermediate states, the totality of them, except for the tiniest fraction, are utterly irrelevant, even at finite temperature. Which doesn't mean that what's left is not very large as we started from infinity. Because we started from infinitely n. So, if you try to do Monte Carlo, if you try to do machine learning on this random stuff, you're just going to get stranded. It's going to find nothing. I've heard a couple of experimental master students on this, and they just prove that point. So, I still think that this is what works best. But you look doubtful. Okay, so just Okay, so just once again, I mentioned non-linear, sorry, linear response theory experiments and things. So we have the pleasure to collaborate with a couple of experimental groups that use those calculations to try to fit temperatures and gases of, in this case, rubidium, in this case, cesium, nitrogen, and then in positive. Okay, so it's got contact with the lab, but that's not what I want to talk about. Okay, so we can discuss that privately. What I want to talk about is Achilles. So if you're a So, if you're of a certain age and you play impact sports, then be careful. Because what happened to me at the beginning of March last year is that I was in the final of the badminton tournament, my institute, and at some point I needed to go forward very quickly, and I went forward very quickly, except I didn't go. And I felt like my badminton partner fell on my leg. I heard a kind of snap, though, it's bizarre, big snap, and partner fell on my leg. Then I turned around and my partner. Partner fell on my leg. Then I turned around and my partner wasn't there. I realized that my foot was completely loose. So, interestingly, yeah, it doesn't hurt, but the foot is completely loose. Why am I saying this? It's because I had two months of work at home without any administrative bother, any teaching, whatever. So finally, I sat down and I rewrote my codes in modern C. And you know, Jehome had been complaining to me, why do you have all these codes and they're not accessible and whatnot? And they're not accessible and whatnot, so they will come out finally. The new version is going to be publishable this year. So that's finally happened. So I started experimenting with these things, and I'm looking for optimizations. I'm looking to make the thing much better than before. So machines faster, that's one thing. But I'm really looking for much more powerful conceptual advances to accelerate the logic. So just to illustrate a bit what kind of work we're talking about for the things, I compiled a few numbers, simple numbers. Compiled a few numbers, simple numbers, to give you an idea. If I'm doing computations at zero temperature, I only need the order of thousands of states to saturate one of the available summaries to 99%. It's arbitrarily chosen. Let's say 99% experimentalists are happy. Then these are the number of intermediate states I need to include in order to reach that figure of merit. As I increase the temperature, of course, this number just. Temperature, of course, this number just completely explodes, okay, and it quickly becomes in the billions here. So I wasn't at 99, I was at 98 here, and I was at bordering a billion cents. Okay, so it's really a bit prohibitive. The first thing that you can say is that you will not be able to go much beyond that. So if I manage to do 128, I will not manage to do a thousand particles. It's not going to happen. Okay? Unless, you know, Okay? Unless, you know, I'm like Altman and I've got connections to all these people, and then they build nuclear power stations for me to power like this. So that's reserved for the AI bullshit, not for integrability. Okay, so then the question is, which states contribute to these correlations? And in the case of density correlations in the closed gas, a density operator is essentially freation multiplied by an inhalation operator. So mostly we're So, mostly what it does is it takes one of these quantum numbers and displaces it somewhere else. And then there are interaction effects that mean that other particle hole excitations also show up. But the main contribution comes from these single particle hole states. So you can see that that's indeed the case. So this is for zero temperature. Essentially, the quasi-totality of the correlation is carried by these single-particle hole states. Now, there's a funny feature of these determinants. funny feature of these determinants and beta onsats is that if you look at their expression as a function of n, you know that the thermodynamic limit is that they are all zero in the thermodynamic limit. So if you were able to extrapolate those numbers much further out, the first row would go to zero, monotonically going to zero. The second row would grow for a little while, reach a maximum, and then go to zero. The third as well, the fourth as well, et cetera, et cetera, et cetera. As well, the fourth as well, etc., etc., etc. So, this is to show that even if you've got something like 500 atoms in your league-like gas, in terms of the distribution of weight in particle whole numbers, you are not even beginning to get anywhere inside of something that remotely looks like the thermodynamic limit. On the other hand, the physical correlator itself looks very much like the one of the thermodynamic limit, even at finite size. Limit, even at finite size. So, why is that? It's because when system size increases, there's a kind of leakage of weight from one particle hole to two particle holes. There's a leakage of two to three, three to four, etc. And this leakage is compensatory. What the single particle loses and the two-particle hole picks up looks the same, more or less. Okay, so there's something that we can think of as the thermodynamic system. Thermodynamic system correlation that is hidden somewhere in this data, if you know how to extract it. Okay? And that's part of the spirit. So these are similar numbers, but now for finite temperature. So for finite temperature, of course, the distribution between different families is radically different. Higher particle whole numbers are thermally excited anyway. So then it becomes much easier for them to carry weight. But again, all these families contribute zero. All these families contribute zero in the thermodynamic limit, but at any finite size, you can saturate your sum hole quite easily with these truncations up to here. Okay, so somehow, if I have information about two, three particle hole excitations at a system size like 60, somehow the thermodynamic form fine the thermodynamic correlation is extremely closely approximated by information hidden in this data. Hidden in this data. Okay, and that's what I'm trying to get to. Okay, so again, just higher temperature, even more dramatic, like that. Okay, so conclusions for this part. So on the one hand, this abacus machinery, it works beautifully. It works super well. I take a measure zero subset of states and I get saturation of my sum rules to essentially whatever percentage you want. Okay? So even at final. So, even at finite t, it works reasonably well. It's difficult, but it works. So, on the other hand, if you do want to push the system size higher up, you can't really do that. The more you push, the harder it gets, and you can't go much further than what I've shown you. I mean, without burning the planet, but let's not do the paperclip problem with Bethanzats. We can do better. So, this is what I want to talk about. What I want to talk about. And 15 minutes left, right? Something like that. 25, 25. 25, okay, very good. Very good. Okay, so what are we going to do? We're going to go boxing. We're going to go on a fight. I will try to give you a kind of scenario whereby such information can be extracted from the finite system that can then be used to understand more of the thermodynamic limit of. Understand more the thermodynamic limit of it. Okay, and of course, the thinking behind this is really associated with the work that lots of people have done, so Robert Miwash, many of you people have done things. I think this is just a slightly different, more engineering style way of thinking about it. For me, it's like a practical recipe, but I just want to explain what I'm trying to do. So finite temperature data in the four-particle. The four particle mole was contributed four percent for t equals eight? Yeah, uh so uh hang on, let me let me just get to the incorrectly. Um probably you spent most of your calculational time screwing around in the four-part sector. Yeah. But out of the billion states, four-part multiple states you looked at, how many actually matter? Yeah, okay, so you're and and then the follow-up question would be: why don't you think you can use some something? Don't you think you can use something like machine learning to tell you which states actually matter in the four particles? So I believe that there is a way to use machine learning to work like that. However, in that specific setting of integrable models with known matrix elements and whatnot, I think it's clear where you want to go. It's clear with information that you've already gathered on matrix elements, it's clear where the next most promising ones are. Next, most promising ones are. Okay, so, and if you see, if you do a recursive logic that keeps track of where it is at all times and keeps digging in the place where it's found the latest gold, then you will be very, very close to optimal. So you could do a machine learning on it, but I don't think it would overwhelm the recursive algorithm of that. I think that would work for a thing where you did not have. A thing where you did not have such regularity of the matrix elements. But I mean, the challenge is open, right? So I'm not going to be the one. So, don't you also have a prediction from Lettinger Liquid Theory for this matrix element? Completely beyond Lettinger. We're completely beyond the reach of Lettinger theory. No, we are. I mean, you're in the ground state with two particle bullets attention to. You're very far from the you're you're high uh energy. This is a final. Energy. This is a finite temperature. This is fine. No, no, this is zero temperature. This is this is T is a this is temperature is about Fermi energy. But even at zero temperature, even at zero temperature, I look at correlations at high energy, where I've got curvature and interaction effects that are completely beyond the reach of to have some estimate of the how big they are. No, it's not going to give me the pre factor of those things. Going to give me the pre-factor of those things. The only way to get the pre-factors is actually to look at the matrix elements from data answers. The pre-factor is the same for all conformed file, for the whole conform file. So you just need to label it once. You need to read my papers with Adilatin Beikov and the papers. But this is really only valid asymptotically close to the edge of the dispersion in the case of non-linear Lettinger theory, where you put one impurity in your theory. Put one impurity in your theory, then you have the edge mode on it. Or if you're at very, very low energies. But for example, this is for Kf over 2. Forget about Lettinger Theory. It just doesn't work. It's not quantitative. And I think at very high temperatures, nobody has any ideas how the metrics element contribute, right? That's right. Sorry, that's those two words. And to go back to your question, so indeed, I've spent So, indeed, I spend, well, my code spends much more time doing the higher particle hole number states just because there are so many more of them. However, in, say, the two particle hole states, those that contribute are those that are exactly like the single particle whole state with a dispersing particle and hole, but with a tiny extra little dressing by the soft node. The four particle hole states that contribute are the ones where one particle flies out, one hole. Particle flies out, one hole digs in, and the other three they kind of go. So the vast majority of these things are irrelevant. So four particle hole states, I have eight parameters. I fix momentum, I've got seven integrals left, okay? And really, two of those integrals give me something significant, and the rest, not. So it's extremely undemocratic. So the ones that contribute here are the That contribute here are the small deformations of the single particle. And the code just gives you that. Okay? Okay, so yeah, that's the thing. So now the question is, can we do better? Now, I just want to explain how I tried to massage this finite size information that I argued for you contained more information than just information about its own size. So an eigenstate is given by quantum numbers. Quantum numbers. Given by quantum numbers. Quantum numbers are either filled or empty. And the choice of fillings of these quantum numbers, on it, one-to-one mapping to an eigenstate. So now, usually, of course, when I sum over intermediate states, I have to sum over every available set of choices of quantum numbers for all the available quantum numbers. Okay, one after the other. However, of course. However, of course, if I move one quantum number, one tiny little unit from one place to another, what am I doing? I'm taking one of the constituent particles and I'm changing its momentum by something like 2 pi over L. So in order for me observationally to see this, I would need to coherently follow the wave function over system size length to see the difference. Because the difference between the two wave functions is that. Because the difference between the two wave functions is that in one of the wave functions, that coordinate is flat, and the other one, it just goes through a change over a system length. But in a finite length, I'll never see it. So then physically, what you say is, well, let me put a little boundary around the group of quantum numbers here. And let me tell you, I don't give a damn what pattern you choose within that little segment. I'm just going to call that the same. Just going to call that the same thing. So let's take these four here. Just going to say we're going to box those four. You're going to tell me you've got two quantum numbers in there, but I don't care where they are. Just know that there are two in there, you're half filled, and that's it. So I take the whole segment of quantum numbers and I divide it into boxes. And then, so we choose the box size, and the filling inside these boxes is necessarily between zero and the length of the box. You know, the length of the box. So the density is always between 0 and 1. Okay, so that's the kind of box regularization. Now, what do I do with that? So let me refine the labeling of my states. Instead of giving you all the quantum numbers, I'm going to give this information in two ways. First of all, you're going to give me the sets of box fillings. You're going to tell me how many particles there are in each box. And then, if I'm finicky, I'm going to. And then, if I'm finicky, I'm going to ask you: what's the shuffling of the column numbers you have in each box? What is it exactly? Okay? So, this I expect to survive in the thermodynamic limit, and this is information I would like to integrate out. Okay? So, that's going to be the strategy. So, the strategy is going to be that I'm going to view states as having boxes. As having boxes with specific filaments. And now, what I'm going to call a particle hole excitation is if I take one particle from a box and I put it in another box. But if I move within a box, I don't care. This is the shufflings. So now when I characterize the state, we will agree on a certain background density. I will ask you to tell me how many excitations you have done on that. Excitations, you have done on that, excitations being defined as one of numbers you've shifted from one box to the other. And then there are going to be the shufflings that you'll need to tell me in order to fix the state microscopically. Okay, so this is just labeling of the original full basis of eigenstates. It's very much in the spirit of Yeng and Yeng, so. It's exactly how you. Yeah, indeed, indeed. Except that now, so we'll get back to that because when you're computing things like energies and things. Things like energies and things, this goes very smoothly. But the problem with the matrix elements is that they do not have this characteristic. So you have to find a way to reintroduce smoothness somehow. You must be choosing the density of the boxes to get it adaptively. Yeah, good question. Okay, so what's the prescription? The prescription is that I need the length of the box to go to infinity as I take the scaling limit, but I need the length of the box divided by system size to go. Box divided by system size to go to zero. Okay, so because then I'll show you equations for the dynamical structure factor later. If I take the limit this way, then if you tell me from which box one particle went to which box, then that gives me the physical momentum limit. And that's what I want. So my resolution of the identity summing over all states is now rewritten in terms of this. In terms of this new way of thinking, so first of all, there's a background density. Here I write it like rho zero. So this is like the background state. On this background state, I put excitations. One, two, three, four. I don't put thermodynamically many. I can if I want, but I'm going to be interested in situations where you can view this here as giving a certain number of excitations whose number is zero as compared to the number of. As compared to the number of rapidities in the field. And then there are these shufflings in there. So very many. So that's the thing I'll try to play with. Okay, so now there's this question of the Yang Yang entropy. Indeed, what's the entropy of such a state now? Well, if you only look at box fillings, then the number of states that are compatible with that box filling is just given by the products overall boxes of this bike. Overall boxes of this binomial coefficient. So the entropy that you associate to that is just something like this. And this goes into the Yang-Yang entropy when you do TBA. But of course, here, the way you do that while you're doing the box regularization, you still need to give me the box lengths. So this is only this knows about the box regularization gamma. Okay? So I'll just use that a bit. So I'll just use that a bit. So, what do I do? I define box average states. So, here you don't see the statistics of the model. In none of what I do here, do you need the S matrix or anything like that? You have to view these as classical billiard balls that you place, and you're just counting configurations. So, the mapping of the quantum numbers into the states and then their physical properties takes care. And then their physical properties take care of whatever effective scattering matrix you can think of and whatnot. You don't need any of that here. If you're taking quantum numbers of firmware, yeah, yeah, yeah, yeah, yeah. Sorry, yeah, so yeah, always, always, yeah, so it's like quantum numbers are either on or off. There's this effective Pauli exclusion principle at work there. You can choose the meaning of these things to be such that quantum numbers are occupied or empty. Okay, so. Empty. Okay, so that's what you have. Okay, so now what I'm saying is that I am going to perform a many-to-one projection of my microscopic states onto box average states. So I'm going to define a shuffling operator that's going to, if you apply the shuffling operator on any state, what it does is that it takes every single box and it goes, haha, it gives you all the combinations of quantum numbers in there, and it does that for every single box. That for every single box. Okay, so what you have is that if you apply that shuffling operator on a state, you get essentially the sum over all the shufflings compatible with that filling. And then we say, I will take a single representative of that, which is the flat linear superposition of all those states, with a norm-preserving entry here of this box entropy. Okay? So here. Okay, so here it's a many-to-one mapping. I'm losing lots of information. But all the information that I'm losing is the information that I don't care because it's the high, you know, I need very high energy to see the difference between the two. So these states are going to have a proper thermal ion equilibrium. Okay? So now, since we become blind indeed to this resolution of momentum within a box, if I'm looking at my In a box, if I'm looking at my correlation functions, I might as well average them a little bit in this window of momentum that defines the width of my box. Okay, so then if I look at that, I just do a summation over all the momenta inside this box. So k check is this external momentum queue, the one that you measure with an accuracy of box size over system size. Okay, and you do the same with the energy, you do a little averaging over the energy over. Over the energy over a width corresponding to essentially the resolution of the box. And that just gives you some heavy-side conditions. Okay, and now the point is that if I look at a state with its excitations, then I can compute the momentum and the energy of that state, because I just substitute that in the beta ants. And what I find is that modulo, tiny things that disappear extremely rapidly in the thermodynamic limit, these eigenvalues. These eigenvalues do not depend on the shuffling. They only depend on the filling, the background, and on the excitations. Okay, so I can just do this replacement here. And now, so yeah, I just have effectively these entries that, yeah, let me just get to the solution. So now what I do is that I consider a smoothened resolution where I've averaged over these things. I've averaged the microscopic correlator over these things. Okay, so this is what you'd see in a lab. In a lab. Okay, and this is what you can compute with beta on sites. But the smoothening of it simplifies its calculation. Okay, so now when I sum over my intermediate states, I can now separate this summation process into two parts. One summation over the shufflings and one over the excitations. So I want to perform the summation over the shufflings. Okay, so my correlation then becomes, if I just Then becomes, if I just substitute the equations I've shown you, it looks something like our Lehmann representation, but now there's an object here, which is a rate, which is given by the summation of the microscopic rates over the states participating inside this box average state. Okay, so each single term here is a Slavonov determinant, but now I am summing over all these shufflings here. Over all these shufflings here. So I'm summing over entropically many contributions to compute this rate here. And this rate here has a direct physical meaning. It's just the rate of transition from this packet of states to that packet of states from that operator. Okay? So now some comments here. So of course there's a shuffling in the brat, there's a shuffling in the ket. But actually there's a covariance between these shufflings. Covariance between these shufflings because I'm summing over all shufflings anyway. So, kind of all the other shufflings are on the other side. So, this thing does not depend on the shufflings here. This thing here is shuffling independent in a first approximation. It only depends on the background density and on the dispersing excitations that you find. If the system is large enough and my box size is large enough, then Large enough, then this will hold true. This is an object that has a proper thermodynamic limit. It doesn't blow up. So that's the idea. So now what that gives you is a way to rewrite your correlation. So now what I'm going to do, I've done my summation over shufflings. I need to do my summation over the dispersing excitation. And the summation over the dispersing excitations, I just need to specify from which box. I just need to specify from which box to which box these particles fly out. So I can just use that and rewrite my correlation as an integral over these momenta of the dispersing excitations. Okay? Just a way to do it. And finally, I end up with an expression for the correlation that looks exactly like an expression you would get from, say, you're doing integrable field theory or anything. You're integrating. For anything, you're integrating over denumerable sets of excitations weighed by some functions that are smooth functions of those parameters. Okay, so that's the story. The claim is that this rate here, given by this expression here, is what you need to know. It's the one that exists in the thermodynamic limit, but I believe that you can compute that at small signs and get. At small size, and get very close to the thermodynamic one. Okay, and that's the challenge. So now we'll just discuss doing that. I guess I'm doing badly on time. Okay, so let's see. Yeah, so just another rewriting. If you prefer, you can factor out some densities of particles and holes, because then that's more like thermal field theory things. But it's equivalent. It doesn't matter. Okay, so properties of these rates, these box average rates. So, first of all, Rates. So, first of all, if you think of the axioms of integrable field theory, let's go through them one after the other. So, there's the scattering one where I take different order of the particles in there. Here, I don't care, because the beta wave functions are symmetric functions of these arguments anyway. So these sets of particle and whole momenta, they're just symmetric. I mean, the rate is symmetric in all of these. The second one is this annihilation property. What happens if I Innalation property. What happens if I take a particle and I push it towards a hole? Well, here it just disappears. You have to be careful. And indeed, one of the reasons why I can keep things simple is because I'm not looking at effective matrix elements. I'm looking at effective rates. Because I'm summing the mod squares. It's not like I'm summing for the operators and then doing the square of the sum. Summing the squares. Summing the squares, but I think the eigenstates is completely worse. For all beta states, you can choose them to be purely symmetric functions of their abilities. There's no need to put any scattering phase in there. You you can choose them to be anti-symmetric if you want. It doesn't matter what you choose. It's unphysical. Sure, but it's we're not acting choosing one way. And they come from the original field theoretical definition of the scattering matrix, which is that you've got an in-state, you've got an out-state, and the S-matrix is what connects the two. So then you get phases. But here that's not the same, because in the beta state, you've got simultaneous presence of all in and all out states. Okay? So you've symmetrized all of that. Okay, so this is the thing. Then the crossing property is also trivial because what Property is also trivial because what I can do is think about a certain rate here with certain particles and holes in a certain background state. But then I can say, well, actually, let me change the background by having some more particles here and some holes there to swallow up these things. So they just flip sides. Okay, and that's the same. Actually, here, if this was not a Hermitian operator, it would just be O-dagger. But there's this symmetry property here. So it's a very high. Symmetry properties here. So these are, it's a very highly symmetric object. The crossing allows you to kick some of the. Yeah, this is not exactly like the crossing thing, where you change the rapidity with an I pi or something. It's not quite the same thing. But in spirit, it's the same. What's a particle on one side becomes a hole on the other. Because you're flipping them all. You're flipping them all. But I could flip one or two of them also if I wanted to. Right? I wanted to. Right? Because I can just choose, you know, what I'm doing really is that I'm choosing the row to be slightly different on both sides, but slightly different in the sense identical thermodynamically as far as the densities are concerned, but just with isolated particles and holes here and there. But that doesn't change the matrix elements. Okay? So that's the thing. Okay, so then that's one thing. Now let me give you the next step of the idea, which is what I call the Next step of the idea, which is what I call these operator pseudo-Hamiltonians. All right, what do I mean by that? I'm going to declare that this rate here is going to be given by another function that depends on the excitations and that has the following properties. So if the rate is very high, the value of this function is very low. Of course, these rates are all real positive numbers, right? So then this is a real number. The operator Hamilton. The operator Hamiltonian is bounded from below because I've got some rules limiting for me the possible values of that thing, to some finite values. The thing here, it also asymptotes to infinity because I know that if I put a crazy amount of excitations here, my matrix elements, my rates are going to go down. So this thing's going to blow up. So it asymptotes to infinity. Okay, so that's what this thing is. Now, let me make another. Now, let me make another statement. It's that this pseudo-Hamiltonian, it looks just like a many-body Hamiltonian that you use in your quantum many-body textbooks. Namely, you can write it as a one-body term plus two-body term plus three-body term plus four-body term. Let's say the other. Okay? Again, you choose to write it like that. And in the n-body term, you also have different types of contributions here. You've got, you know, particle, hole, hole. Particle, whole, whole, whole, particle, particle, particle, whole, etc. These are all different functions appearing in there. And the point is that I can compute those elements because they're tabulated from the knowledge of the rates that I can just compute with my circuits. Okay? So, for example, the simple one-body terms are given by this. Terms are given by this, the single particle whole term also is easy to compute. And these are, in the end, ratios of great big summations over determinants. If they were ratios of single determinants, then I'd be very happy because I could just take limit, red hold, see what I get, and I compute that thing. But the difficulty is that, inherently, I need to perform a summation over soft modes in these things. In these things. But if you do that, then these should be finite numbers. Okay? So that's the story. And now, so what do we stand to gain from this? The conjecture is that this pseudo-Hamiltonian, it's composed of things with very rapidly becoming irrelevant terms. If I truncate it to the first few terms, I'm able to get a very accurate prediction for all the rates that you're going to ask me. For all the rates you're going to ask me. Because even if I've got 12 excitations in there, it's going to be mostly the single-body terms and the two-body terms, perhaps some three-body terms, but the four-body terms, don't forget that. I'm going to give you a numerically accurate evaluation for the matrix element you're looking for, for the rate that I've defined like that. Okay? And that's a conjecture. I was hoping to show you hard data on this, but the nights are not long enough. But I'm almost there. Okay? So that's the idea. You can truncate the pseudo. So that's the idea. You can truncate the pseudo-Hamiltonian, and then you get essentially everything you need. So when you look at the correlation then, you see I've replaced the rate itself by this thing here. So if I just go back to a Fourier transformation to get the real space correlator in this limit, what do I get? I just get contributions from like free-like excitations, you know, e to the i kx minus omega t, where k and omega. Omega t, where k and omega are functions of these particle and whole momenta that I've got in there. And then I've got this pseudo-Hamiltonian there, tempering terms one with respect to the other. But if I happen to know this as a function of these elements here, I'm laughing because I can start doing contour integrals and I can look at holes in a complex plane. I can do Feynman diagrams if I portray my things. If I perture my thing, you have to have analytic expressions for that. You need numerical results. Indeed, indeed. But you see, so what I'm telling you here is that for the moment, of course, the only thing I have are numerical handles on computing those rates. However, if you ask me if there's any analytical hope of computing these things, then perhaps, because what do I need to do? I need to do summations over soft modes. So I do little moves of things. I know my patch. Little moves of things. I know my back flows. I know that one rapidity is going to move over the one over L, the others are going to move one over L squared. I can substitute that in the slab nodes. I can try to expand, and then I can sum over these things. So it's not impossible. And I think it's doable. But for now, I'm doing it numerically. Because that's the best I can do. I want to see first that it works and then try to confine these things. So if I compute these parameters numerically and then fit them, then I can use that as phenomenology in these states. Phenomenology in these things. But then I'm just as good as anybody doing high energy theory. Right? So, okay, go down. Okay, so that's the idea, right? So you've got energy, momentum, you've got these pseudo-energies for the rates, and suddenly, indeed, the calculations become much easier to do that. And you can even do like perturbed models if you know these things. If you perturb by this operator, say, at a certain, you know, Operators, say, at a certain non-spatially uniform place, then you could just put that in there and literally do Feynman diagrams with this. You could do diagram atoms. Yeah, I know. Okay, so just as an example, so these are the one-body terms for Lieb-Lineger. This is the first interaction term for Lieb-Lineger. I'm touching that very quickly because these results are from last night and I will confirm them before publishing. Okay, so that's. Okay, so that's the idea. The idea is that I view a quantum mechanical integrable model as a field theory that is completely regularized to all orders. You can ask me any question. There's no infinity in there. And if you go from the finite model to the field theory, we're all used to using integral models to compute things like magnetic susceptibility, perhaps input for CFT, dimensions, pre-factors for both nice. Dimensions have pre-factors for Bosonization. But the message is that from the integrability, you can compute a lot more parameters that feed into the field theory. You just need to find the right path to evaluate those things. And when you've got these things, you can forget about where these things come from and just do simple diagrammatics, and it's going to work well. Yeah, so indeed, in those correlations. And so the way I like to talk about this is beta-liquid theory. You've got a simple field theory. You've got a simple field theory in the background, you're using a microscopically chosen model, you feed the parameters into that more generic theory, and you try to do something with it. And with that, thank you very much for your attention. Right, we already had some questions, maybe another one more question, maybe I hope you could do that, for example, on a relativistic model, maybe most of the models. Because in the normal you really have to renormalize because the theory doesn't have a problem. Yeah, so here so here I don't have that problem, right? I mean exactly. You see, but my starting points here are always a mechanical model that doesn't have this signals. I understand, but its interoperability doesn't cure this problem that you need to introduce a cut. To introduce a cutoff frame to do reorganization. But here I don't. Here you don't. For the spin chains, I don't. For the Hubbard model, I'm not going to do it. Because I'm already in the field theory. Here I start from quantum mechanics. Right? So I cannot start from the field theory because I don't have the microscopic shufflings available anymore. Because I don't have the resolution, I need the one over L. So in order to do this, I need to know. To do this, I need to know my stuff at the UV limits. But you could do use lettuce signboard or let us make that. Yeah, then that would work, right? That would be at the same word. Well, then you would have to do the scaling limit at some point. But it would resemble what I'm doing here. Yeah? Yeah, I think we should move on. We are running well over time. Sorry, yeah. The next speaker wants two final lines.