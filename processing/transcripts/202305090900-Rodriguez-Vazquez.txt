It's not working, so I'm gonna sorry moving here. Yeah, okay, starting with the evolutionary algorithms. I think everybody knows. I think everybody knows what a GA is or an evolutionary algorithm. So just a brief description. For evolutionary algorithms, we have to create an initial population, random initial population to solve a given problem. This population is evaluated. We have an objective function, we have the problem. So okay, so just a moment. I think there is a problem with the Zoom. Yep. No, I think it's back here. Okay, so I'm back. So we have the random initial population, so we have to evaluate. And then if the termination criterion is not met, so we have to select best individuals and then create a new Create a new population by applying some genetic operators like crossover and mutation. We have a new population, and we are going to be in this cycle until that termination criterion is met. So we have different types of representation like binary, the simple GA, integer with can be for combinatorial problems. For combinatorial problems, real coding for parametric optimization. Also, we can have a variable coding, and also that's quite interesting. Let me we have here, can we because I don't want to Okay, and the last coding is hierarchical trees for genetic programming. Genetic programming has the same structure as a GA, but in this case, we have this kind of representation trees. I didn't define values for the nodes because we have here internal nodes. We have here internal nodes that are functions and the leaf node. Depending on the problem, we can define arithmetic, trigonometric, boolean function, etc. So I didn't change percentage. Simple problem: GA binary. Problem GA binary GA. We have here a binary coding that for us is going to be our genotype, it's not the genotype that's in bio. We are borrowed some terms. Our genotype, decoding that genotype, we get the phenotype. And with this phenotype, we can evaluate our objective function that in this case is a parametric. And we have here the values. We have here the values. With these values, we can apply selections to get the to select the best individuals and then apply genetic operators to create new solution by means of crossovers of two individuals and mutation. So, a best individual has a higher probability to be selected. Genetic programming has the same cycle, same step, population of random trees that represent a model, a function. So evaluate that population, get the best individual by means of selection, create new individuals, new programs, and then until the termination criterion is not met. Genetic programming. Genetic programming, the idea of genetic programming is trying to get a model that gives a relation of input output data. We have this representation of trees that can be prefix notation or can be a program, small program, or can be a function. All these are equivalents. So, crossover. Here, individuals are different. They don't have the same structure. So, we have to select a note in one individual, parent individual, another note in the other parent individual, and exchange the branches. Mutation is quite similar, one individual select a node. One individual, select a node, random, a random node of the individual of the tree, and then take all the branches and change for another one created from the function and terminal. I'm going back. Yeah, I forgot something. Individuals are created depending on a function set or terminal set and terminal set. Terminal set are Terminal set are arguments of the functions internal node. So, in this case, for mutation, we are going to create this new branch from that set of primitive applications. Yesterday there was some talks that were talking about regression. We can use genetic programming for regression problems. For regression problems. So, the idea is to find a program to model the relationship of giving input top data. So, genetic programming is supervised machine learning. And we have terminal set, independent variables, we can have constants, we can have the random generator function, function set, we can have originetic, trigonometric, any kind of function. Any kind of function and all the parameters for the evolutionary algorithm. We can have a difference domain like Boolean functions. We only change function set and terminal set to create individual for a different domain. Modeling and prediction, this is not a bio. Not a bio example, but some modeling. We have a time series attractor. This graph is the attractor of the original data. This one is the attractor of the model generated by means of genetic programming. And And also, yesterday, I think it was at breakfast, they were talking about GPT and art and something like that. Genetic programming can generate art. So this graph was produced by this function. Here we have unsupervised learning. It's not supervised machine learning because we don't have. Back machine learning because we don't have a reference to a dust. Another kind of bias part, PSO, particle turn optimization, quite easy. The algorithm is only these two equations. Each particle is a possible solution. Suppose that this particle is the leader. Particle is the leader, is the G-best. And we have also the particle and the best historical position of that particle. We have a cognitive component, a social component, and here we have the velocity. Each particle has associate a velocity, and this term gives the exploration of the search space. Exploration of the search space. Because we have this parameter R. If this parameter is close to zero, so we are going to move with short steps. If this parameter is near upper bound, so we are going to have long steps. And at the end, we update the positions. So here we You have here a list that is not updated of a lot of various part proposals. But at the end, it's the balance of exploration and exploitation. All of these can be combined with multi-objective optimization. I think there is a talk about multi-objective later this morning. Morning. And what multi-objective optimization is, is just to optimize two or more functions simultaneously, and this has to be in conflict. So here, this is the objective space and the decision space, decision variable space. So we have here this solution are not dominated. There is any solution that is. There is any solution that is better than the other in both objectives. This one is better than this one in F2, but in F1, this one is better than the other one. If we have a solution here, so that solution is going to be dominated by this one in terms of F2, and it's going to be dominated by this one in terms of F1. So that's One. So that is multi-objective optimization. So this is easy because we have only two functions. We can have three, but if we have four or more functions, we can visualize them by means of parallel coordinates. Here, horizontal, we have function. Tau, we have functions, and in the other axis, we have the value of the fun of the normalized functions. Each line represents a solution. So if we take this one, so what happened that comparing F2 and F1, this one is better than the green one. Than the green one in F1, but not in F2. There is conflict between F1 and F2. But I'm going to the last two, F3 and F4. So we have again the green and the purple. So if one F3 is improved, also for function four improves. So there is any conflict. There is any conflict between F3 and F4, so we can reduce the problem to 3. And here is also parallel coordinates, seven functions. The last two are constraints. So this has to be met in order to get all the other solutions. So we can visualize more than two, three functions. Functions. Applications. Sequence alignment. We have here a sequence alignment problem. It's only two sequence, very short. And we have four alignments. The first one minimize substitution. This one minimize indels. This other one is a balance between substitution and indels. And the last one minimize evil. Last one, minimize events. Event, index, or substitution. So it's an optimization problem. And our regulative function is just a global criterion for sequence alignment, similarity in columns, block of gaps, and also this is the penalization. Is a penalization for increment in column. The weight for each term. This is also a multi-objective problem because we want to optimize similarity in column and also the block of gaps. And this is a penalization that has a negative value. And similarity in Similarity in column is only just the identity, the similarity, and we can measure by means of entropy inverse of entropy. We have also here a minimized gaps block of gaps. These three alignments have the same value of similarity in columns, but the first one has three blocks. The first one has three blocks, the second has two, and the last one has one block. So we prefer the third alignment. And finally, incrementing column is just to compare the initial matrix with the aligned matrix. Causing with GA. So the only thing we can change are gaps. Are gaps. So we are going to code gaps. This is a matrix of four sequences. So the first one has a gap of size four after second position. The sequence one has two gaps: one of size two after position two, and the other one of size one after third position. The sequence two has one gap of size two at the beginning, and the last one has a block of three gap after position three. Final gaps are not count. So crossover operator, we can choose randomly a row and exchange food matrix. Mutation operators, we can have insertion. We can have insertion, increment, recrement, shift, or delete a gap. We have this matrix, and at the end, we get that one. Also, this is for DNA sequence alignment. We can move to protein alignment, amino acid, changing from four to twenty. From 4 to 20, and there was another work of metabolic pathways alignment. We have the metabolic map, take the graph to get the trees, and then the trees map to gene sequences. And from the gene sequences, move to EC numbers and semantic commission numbers from EC4, for commission numbers. EC4, four commission numbers, two, three commission numbers, because the last one is not relevant, doesn't give any information. So we have these sequences, so we can apply the same ideas of alignment, but in this case, because this matrix is very small, we can use a binary coding. One is it represents an easy number, zero. An easy number, zero gaps. Objective function: we can have the same entropy inverse of entropy to maximize the similarity in column and also optimize gaps and penalize for increment in columns. Genetic programming. We have here this problem of rule prediction. We can use We can use rules for classification if then rules. So these rules can be expressed as trees. So we have a rule that attributes bigger than value one or the other one condition. So we can get the class. Can be binary, can be multi-class. Multi-class objective function: we can get the confusion matrix and get the true, positive, negative, false, positive, and negative, and then obtain sensitivity, selectivity, precision, accuracy, F score, etc. So, we can have here also a multi-objective. Objective function. So rule prediction for medical diagnosis in Luensa. We have here our primitive functions and terminals. So we can evolve this kind of trees: logic operator and first condition. And first condition, fever difference to zero, or the other condition, sorry, and the other condition, so then gives a class, has or no influence. With this implementation, we apply the genetic programming for data from the General Hospital of Mexico City, 26 attributes. Six attributes, one of them categorical corresponding to age from zero to six, from seven to fifteen, sixteen to sixty, and 16 and over, the old people, and get some rules like this. So it's quite interesting because we can get a kind of biclustering attribute and set of patients that have the same symptoms and combine with. And combined with the other groups. So, conclusions: this kind of algorithms are robust, has a diversity of coding, diverse application. Yesterday, we all in the old session, clustering, classification, regression were mentioned. uh were mentioned uh pattern recognition alignment so also if no information if information of the problem are available can be integrated in the initial population and also can be combined with multi-criteria optimization our team we have a lot of people working on that topics uh Algorithms and also application to healthcare or bioinformatics. And finally, just to finish, I want to leave finish this talk with this phrase by Andrew Steele. Andrew Steele was a postdoc, I think, from 2000. I think from 2014 until 2018 at Francis Craig Institute and he said that. So now I was checking yesterday internet and I found that he is now an independence writer, scientific writer, based on London. So no, based on Berlin. Based on Berlin. So machine learning. So machine learning approaches often find things which are inexperienced researchers might miss simply because they are unspect. Yeah, I think there was also a talk yesterday with Lasha. Is not here? No, no, I can't see her. Yeah. About that there are new things that merge on machine learning because they had they are They produce novelty solutions that we have to take into account and evaluate. Okay. And we have the control, not like GPT. Thank you so much. Thank you. Thank you, Katya. So, do we have questions? There are also about 13 participants online. If you have any questions, just put your hand up. You have any questions? Just put your hand up. Are there any questions here in the room? Okay, hold on. Of these things you showed us, a lot of the optimization and the hardware for machine learning. Do you find cases where some of these algorithms, you can put them on GPUs or some of these new TPUs? Does some of the bio-inspired algorithms also benefit? Algorithms also benefit from the things in machine learning on the hardware and optimization side? Yeah, like thousands of epic algorithms on a single deal. Right. So you can't give up on two machines and multiple uh sets of cases. So you can evaluate one set one case. For the same individual, evaluated in the different situations and in each I don't know which word. So what's the focus on the original? Well, if you're trying to articulate the plan, we'll talk to the question that we have. Okay, thank you. Any other questions? No? Well, thank you so much, Katya. Our next