Anyway, I'll just introduce myself. I'm Jatao. I'm currently a research research scientist at Apple. And before that, I was at Meta. Today, I would like to introduce some of our recent works towards efficient diffusion models for 3D generation. So I will just be quick. So all of my research focus or hope is trying to build something. hope is trying to build something called a world model, which you can have if in the future we want to build some AGI, you can use this world model to interact with it. And you can learn all the experience. It can be a replicate or rereal world and the model can interact with it and learning from that. So in my view, this kind of world at least have to be multimodal, which you have to be physical world. It can also be interaction with human or latilanguage models, but also it has to be. Language models, but also it has to be generative because you want to handle uncertainties. So, in terms of generative models, recently people are more like shifting the trend to diffusion models, where you have already seen a lot of progress in image generation, text-to-image, and also text-to-video generations. But our real world is 3D, so people have already moving, trying to using this kind of 2D diffusion model for 3D. So, you have already seen Ben was already presenting. Already seeing Ben was already presented dream vision, and my co-author Linjie also presented NerfDeep, which was appearing in SAML two weeks later. So the whole idea is like we can be you can using a pre-trained diffusion model so that help to generate 3D contents. And recently this field is progressing super fast. You can see right now there's some papers can already generate very high quality 3D given the printing distribution models. But the main focus today I want But the main focus today I want to discuss with us people is here: diffusion model itself is very slow because it's inevitably an iterative process where you do this kind of step-by-step denominating process to generate high-colored images. And also, diffusion model, applying diffusion model for 3D is even slower because, for example, here, the example of doing dream fusion, you typically take over 30 minutes or 60 minutes to generate one single scene. Generate one single scene, and even for our case, is the Nerve Deep paper we presented yesterday. It also takes at least 10 to 15 minutes to fine-tune one single sleep nesting, which is really not impractical. So why we care about this? Because in the world model diagram, I think another important factor is you need to make the model efficient. It's not only efficient learning, but also efficient inference, because you want this model can be. This model can be real-time intact with agent so that you can learn from this. So, in this talk, we are mainly focusing on how can we making this kind of diffusion model to 3D more efficient. But the first part of this talk is maybe you can change the target. The target means that is the dream fusion type of 3D diffusion or really 3D diffusion. So, for the left part, is we train a first between the 2D diffusion. Train a first train a 2D distribution model and we have our 3D scene on the left. We render the views and distribution model we're training to denoise it and provide information. But in the case, the 3D scene is mainly still optimized by this kind of gradients. And this process is very slow. And because you have multiple views trying to do the diffusion models. But for the 3D diffusion, we're directly learning a diffusion model on the 3D things so that you can directly add in noise and the noise context. So the speed of this kind of 3D diffusion. So, the speed of this kind of 3D diffusion model will be the same as the standard 2D diffusion. But the high market has some challenges. So, the first paper we will present is called SST Nerve, which will be presenting SECV this year. But the whole idea is we come back to the original type diagram. So, why the, okay, if 3D diffusion is very, I mean, it's more efficient than the. I mean, it's more efficient than this kind of using 2D to help the 3D generation. Why people are not really heavily using that? But the one potential issue is you typically need to have this two-stage 3D diffusion phase. So the real reason is because the 3D, our diffusion model has its artifact is that you need to always have a target to adding noise on top of that. But in 3D, there's not such thing. So you have to making this kind of your first build. Making this kind of you first build a latent space or some kind of suite representation, and then you learn this diffusion mode on top of this. Here, we're showing this one simple one typical people was using is using auto-decoded fashion. So you given a data set for each scene, you optimize the latent space for each scene, and you have given multiple images, and then you learn the diffusion mode out for this. But that comes to some limitations. First thing is, it has to be working on a lot of views if each thing you On a lot of views, if each thing you only have sparse views, the latent space what you optimize will be very bad because you only have three view information to get the latent space. And more importantly, even if people have trying to add in some regular additions to make the space as small as possible, but this kind of regular addition tends to lose a lot of details, which will in turn causing bad performance. So in this first paper, we want to say, yes, okay, maybe we actually, we don't need to separate these two stages. We don't need to separate these two stages. We can have one single model which you have both the learning the diffusion prior, the latent space diffusion, and not simultaneously you have a decoder which is a normal 3D rendering and learning from this data set. We jointly optimize between the diffusion loss and the rendering part. But in practice, the diffusion prior can just is equivalent to optimize the diffusion objective. So the thing is, you So, the thing is, you can train this. But why this one would be more preferred? Because first, if you have this single-stage model, you can train it, all the training loss will become end-to-end. You don't need to have the first training, first model and second trainer, second model. And more importantly, there's no, because the diffusion mode itself will act as a localization. So, remember, previously, I mentioned people have been trying to do using something like localization, but here diffusion would. Like the localization, but here the push would act as the localization. So you need to have additional activity loss to make the space as smooth as possible. And finally, because you have this diffusion prior information there, you don't need to have, this model can be actually training on sparse views because the diffusion prior helps you to complete the missing information from other parts. So it's basically having, how can we have other information during the building the latent space times? The building and latent space times. So the algorithm detail is here is very complicated, but the simple thing is you're just alternating between the diffusion mode and the rendering mode. You have that, you go one-step diffusion model, you frozen everything, you just learn the diffusion model, and then you froze the diffusion modifier and you do the rendering. And we developed some cached gradient check to making the optimization as quickly as possible. After the model was trained, you can also using this one. First, you can just using the pre-trained 3D diffusion model to do unconditional generation, which is running as the same diffusion model. But you can also do single image reconstruction. For example, you can using an image-guided sampling. So in that case, our trained unconditional model will be guided by the image constraint loss during the diffusion process. So everything will be the same as the standard in 2D diffusion. Be the same as the standard in 2D diffusion, but we're just replacing the thing we are denoised as a latent space. We can also use this combined loss function as a fine-tuning objective when we want to really achieve high quality when doing a single synthesis. For example, when we balance between the diffusion loss and the rendering loss, we are essentially trying to maximize our, it's an MAP estimation, we maximize the view we see and also we want the See, and also, we want everything else looks consistent as it's from its prior. So, here we're showing some results. This is the unconditional generation performance. So, we can compare it on several data sets and compare with some existing method where we don't have this unconditional evaluation. This is also showing our results on single-view construction. We give it a single view, and you can just render. Can just render. And here we're also showing some results on the KT data set, which was the model was trained on SN shaping at and finally, this page of showing that the original motivation of doing this is because this model when we actually train it on three views. So here the method is only each thing unit gives three views, random three views. But if you just apply the same thing for the standard Apply the same thing for the standard auto-decoder, we won't be able to get a reasonable code. But in using when you train them jointly, you can still have a reasonable result when you're doing this. So that's it, the one part. But if we go to the, if we push ourselves further, if we only have single view images, what will happen? So in that case, the auto-decoder method basically breaks because it's very difficult to using this auto-defer. Difficult to using this auto-defined equal method, except you have very strong prior localization over the latent space. But we also want our latent space as much rich as possible. It's better not be a vector, we want to be some triplane or something like representations. So we are just shifting our ideas that maybe we can use different ways to get this latent space, not only using the auto-decoder, but here we can actually learn from. Can actually learn from literature from the 3D GANs where we can learn directly on this latent space of 3D GANs. So, this is another paper we're trying to introduce, which is called control 3D. Unfortunately, it's not accepted by ICD, but yeah, the whole idea is maybe we're just doing the 3D diffusion on the 3D GAN space. So, the thing will be very simple. So, you can first train a 3D GAN, just what we typically doing, we give What we typically doing, we given we have a 3D, it's a triplane-based 3D gun. You can train it with adversarials given the real images. In the case, we don't need to have multi-view image at all. We just given a set of single-view images, and then we just train a model to get these kind of images. And then once you train the 3D guns, you can take this latent space out and you add a noise on top of that, and you're doing trainer deposition models directly. So during the depression time, you can just So, during the division time, you can just train with the unconditional model or conditional model. And after you train this model, similar to the first part work we presented, we can doing a different ways to inference that. We can have a conditional generation because we can, you can during training time, you can just plug in any kind of condition, like the image or any kind of signals. Or you can also do in a classified guidance. You train first, you train unconditional diffusion model under latent space, and then you. Diffusion model in the latent space, and then you do the guidance in the latent space. But overall, why we want to do this? Because 3D GAN itself can be another very efficient generative model, right? So, why you still want to have additional diffusion models on top of that? But my thinking is first, 3D GANS itself GANS is an inclusive model. It can help you to, it can generate very good samples, it helps you to explore the space, but it didn't have the density evaluation on the space. So, you don't have the proper product. Of the space, so you don't have the proper prior over the latent space you have. So, usually, you can only rely on a low-dimensional vector of the gains. But in the case, it's very hard to control the full details by very, very small low-dimensional vectors. But in diffusion models, we can just learn the score function over the 3D, so which was already explored by 3D Gans, and you can add additional controls into the 3D diffusion space to achieve better control abilities. So, in this framework, we consider on six different. framework we consider on six different controlling tasks. Here we so we compare them with image inversion. Here we can compare image inversion. We compare where the standard optimizing the latent space. I mean GAN's latent space is very small with raptor. You can see it lose a lot of information. You can also optimize directly in the true latent space, which we call it the triplane latent space. But if you're doing this, it will get very bad artifacts because you don't have the prior over. Fast because you don't have the prior over this. While if you first modeling the diffusion model in the latest one, then you just do this inversion on the latent space. You can mostly capture all the missing details and still you can get a good inverting results. We also show other experiments on different kinds of control tasks, for example, segmentation to catch. Here we compare with which it's called pixel-pix3D paper. It's called pix to pix 3D paper. And also we tracked on different phases and H23D tasks. And also the, we can, we can, with the model condition, when doing training, we do our model condition on the clipping banding, we can eventually in the testing time, we can replace the clip embedding with the text, the clipping banding of the text. The clipping budding of the text, then we can just do text-to-3D generation without any kind of pair data sets. And finally, we're also showing some similar to the previous work. We are showing those on the shape net, but to note that it is different from the previous case, we don't have the multiple data for this data set. We always assume it's only single view. So the quality-wise, it will be a bit different because everything was. The bit is different because everything will be bounded by the final quality will be bounded by the 3D guns itself, but still you can have more controllability of these kind of models. So the first previous two papers, I was just trying to mention that we see if we shifting our focus from 2D diffusion to improving the 3D, we can actually achieve a bit more efficiency on this generating 3D models. But maybe Models, but maybe on the other hand, maybe our focus is kind of also be on can we make the diffusion model itself more efficient, which you can combine with the 3D things. So the first, the thing is in standard diffusion models, usually it's restricted by the same ambient space through the diffusion models. So for example, even if you're generating a very high resolution images, you're adding a lot of noise on the same resolution and you just keep going until you're getting the clean images. Getting the clean images, but of course, there are some reasons. I mean, people are more focusing on LDM or like stable diffusion type of things. We can kind of work in the latent space. But if we even though the whole process is the same, if you're just running a diffusion model, that doing each time step is already in the same space. But this would be a bit inefficient because usually in the low dimension, in the very beginning, if it's followed. In the very beginning, if it's full of noise, then you can't tell what's the details there. So it's maybe not meaningful to have model everything in the highly dimensional space. So this paper we called FDM, which is while trying to do this kind of multi-stage distribution model, where signal processing, signal transformation. So the whole basic idea is just this slides where you is replacing the standard distribution model X with a transformed FX here. So the F can be any kind of So the F can be any kind of discrete or continuous transformations and to transform the signal to something else. It has to be some function and you can have approximated invert of this function, but it does not have to be invertible. So here, the most related to our topic is you can actually make as f as a bound sampling operator. But in the original paper, we can make it more general to do any kind of transformations. So the major challenge of this type of model. The major challenge of this type of model is two things. First, compared to standard diffusion models, this kind of additional transformation will have additional information loss. If you have a way to reverse that, just like adding noise to the space, if you have additional transformation, you still also consider that information loss. And second thing is it's also unclear at that moment how to adjust the noise schedule when this dimension changes for distribution models. So the first thing is. Models. So the first thing is solved by if you can incorporate the interpolations trick in the diffusion models so that you can make sure the diffusion when you're changing the space, you want to reverse this change as smooth as possible. And the second thing is we found that whenever your your resolution changes, you always need to increase the noise level. For the become larger images, you want to make it more noisier. Images, you want to make it more noisy in the noise part. Otherwise, it will become too simple to predict. Something that is not discovered by some waiver later, but it's really making a difference in the final results. So for the sampling part, after related to models, the sampling can be done very easily by just adding additional term into to reverse the transformation and doing the reverse division models. Here I'll be showing an example how this. Here I'll show you an example how this model can be just handled during one diffusion for the iterations, and you can get this kind of high this kind of high quality images. And during the whole diffusion process, it's starting from low resolution images and then till you get high resolution images. So, by formally doing this, we can actually gain some speed up. I mean, not great, but still you. Great, but still, you because depending on how you allocate your transformation and how much resource you compute for each solution, for example, if you have 10,000 steps, 1,000 steps, you can make several steps in the lower resolution space, and then you move into the high resolution space. One interesting thing we found that at that time is when you're modeling this kind of multiple resolution generation, you can actually control the multiple level details when you're generating to the images, which you can see there. To the images, which you can see there's similar things in GANs, but not really common in the diffusion model area. The last paper I want to introduce is something, a new paper called Boot. Yeah, I want to base very quickly. So the whole idea is we can reduce the whole everything into one single step. So previously, we just make it model to become hierarchical things. We can speed up things. But if we have a model for one step, it will be very more efficient. One step will be very more efficient. So, I skip all those slides. But the main idea is we can have a model directly to predict the ODE derived from the original distribution models. And the simple thing, actually the thing can be optimized directly by a bootstrapping loss by given the two time steps over images and the UD loss between them. And you predict the difference. So that by doing this, we can. So, that by doing this, we can have a single stage model, a single-step model directly to distill any kind of favorite diffusion model model, something like the stable diffusions. And we here some two videos. I'll skip this because I don't have time. So, the final slide is why these two things might be important. Because, in my view, compared to 2D, 3D is more insensitive to resolutions, right? Because it's quadratic and not quadratic, but the cubic. Cubic. So maybe you can have also combining this kind of multi-resolution things in the 3D diffusion space. And the mean importantly, we can combine it with sparsity informations. Like when we're doing high resolutions, can we change the considering the sparsity in the FDM thing? The last slide is also, as I mentioned earlier, the 3D fusion is hard because it's unable to get the target data. But our Mercer method of using the boot, you need the targeted to get this. Can you need targeted to get this kind of student model to train? So maybe you can treat the 3D generation itself as a distillation problem for this kind of 2D distribution models. Okay. Yeah, thank you so much. I'm happy to have questions. We have a more harder to find. We had a bombardment of questions within the week, but I have one. So, in the very first project, you used an auto-decoder. Could you train an outline coder in the same way? I guess the auto-decoder restricts the number of images or scenes you can deconstruct. Yes. What are the trade-offs? Yes, I think the main difference is the auto-decoder, each thing you have a code. Auto-decode each thing, you have a code. So the problem is it's hard to scale. If you have like the object Excel scale things, if you have 10 million or 100 million things, it's very difficult to optimize them jointly. But the good thing about auto decoder is you can, it's not restricted by, but if you put the auto encoder, it has to be, if you want to really make has to be if you want to really making this latent space uh capture the all the 3d information it has to be uh multi-view auto encoders so a encoder right so if you only have single view or few view it may be give you better latents similar to the auto decoder but the auto decoder it can optimize by the gradients so in the quality wise it's going to be better than the auto encoder and also making making this auto encoder Auto encoder with multi-view auto-encoder is also very challenging. Amazing. Yeah, then. Yeah, that was super cool to see how you're able to use the diffusion model to add control to the YAN model. And I was wondering, do you think there are ways of getting, or taking advantage of this flexibility in the conditioning, you get a free diffusion, but not having to learn that model from multi-data? So if you pull that into the train. Data. So people that could train some of these systems on like three images. But if I only have single images, do I have to do this two-stage thing of learning CAN and then do three because you shouldn't? I was curious if you had any thoughts on how you can directly get there in a single stage versus this multi-stage approach. Yeah, sorry, I didn't hear the, it's hard to hear the question. So any questions? Maybe I'll try to repeat it a little louder. I guess I'm just wondering if you had any ideas for how to get a three. Have any ideas for how to get a 3D diffusion model from only 2D data? Do we need to go through this DM training for single images, or do you think there's another way? I see, yeah. So that this is just one potential idea we are exploring. I think the gun, the control 3D idea, I think it's also one. I think it also won't be the ultimate goal because it's after all it's based on the 3D gangs, right? So you give it, I mean, even you, so if you can train on