Get my screen the way I want it. And the kind of things that we're discussing today have to do with our increasing ability to record simultaneously from a large number of neurons, which gets us into this notion of being able to measure population activity. And these measurements typically come in the form of raster plots, like the one I'm showing you here. This is about 30 seconds of neural activity. Every row represents a different neuron that is being recorded. A different neuron that is being recorded from, in this case, 110 neurons, and every tick represents an action potential emitted by that neuron at that particular time. So, this is a particular recording of in primary motor cortex of a human subject who was waiting for epileptic surgery and doing, in the meantime, performing a task that I will tell you about in a couple of minutes. So, what do we do as theorists with this kind of As theories with this kind of activities. So we consider the population of n neurons, the spiking activities observed during a time interval capital T, and we just slice that interval in bins of size delta so that we have k beans where k is t over delta. And we label with an index. And in each bin, we observe the number of spikes of k per mean k emitted by neuron i for all the neurons that we are observing. So we can display this kind of So, we can display this kind of data in the form of what we call a data matrix that has the same structure as the raster I showed you earlier. Every row corresponds to a different neuron, and every column corresponds to one of these discrete times. The index time now just is a counting of the bins. So that capital T is the duration of the experiment or the duration of the measurement in units of the V-size delta, and N is the number of neurons. And ideally, we would be able to measure all the neurons that are modulated. All the neurons that are modulated by a given activity, which is what I call n-infinity here. It's not that it's an infinite number of neurons, but it's all the neurons that commodulate and participate in the given activity. For instance, if I do again a rich movement towards my cup of tea, that's the order of a million neurons in my primary motor cortex that are involved in that task. So let me illustrate a very simple motor task that I will use as an example to guide you through what I want to tell you about. This is a What I want to tell you about. This is a simple center out task with eight targets arranged around a circle. It's a delay response task, and it goes as follows. You fixate at the center, one of the eight targets comes on, you wait for a go signal, and when the go signal comes in, you move your arm or a lever of a keen arm, as in this case, towards that target, and you get a reward. So the go-kue, the time between the target. Go queue, the time between the target onset and the go-cue is variable between half and one second. And then there is also the order of 300 milliseconds between the go-cue and the onset of movement. And then it takes, you know, the remaining of a second to get to the target and get the reward. So this is a very commonly used task, and we know about it. The first things we know about it come from the early 80s, from the lab of Georgiopoulos. At that time, these were single neuron recordings. So it's again this age-center. So it's again this eight center out task, center out task with eight targets. And now this every row is a different trial. It's always the same neuron. And what you can see here is, first of all, a lot of variability. So even for moving in a given direction for a fixed target, there is a lot of variability from trial to trial to trial. But within this variability, there is a specificity. This neuron is more likely to be triggered by movements to the left, while it's actually its activity suppressed. While it's actually its activity suppressed when the movements are towards the right of the subject, so now instead of measuring a single neuron, we can measure of the order of 80 or 100 neurons again for 30 seconds. During these 30 seconds, there are many trials to different targets. And the question that we ask is like playing a computer game. This was the yellow target that came up. I wait until the Goku is going to come up. And I measure the number of spikes in an interval of 300 milliseconds. Spikes during an interval of 300 milliseconds, which is centered around the GoQ, 150 milliseconds before the GoQ and 150 milliseconds after the GoQ once the movement about to start. And as you see, there are neurons that fire a lot, like this neuron 46, neurons that fire very little, like this neuron seventy, and neurons that fire sort of in an irregular pattern, sometimes more, sometimes less, than neuron number seventy eight. Again, every row corresponds to a different neuron. Every row corresponds to a different neuron. So, the question is: if I just showed you the number of spikes emitted by every one of these neurons during this 300-millisecond interval, would you be able to tell me to which target this motion was going to take place? And the answer is yes. And we know this now for many years. This is one of the examples of decoding this kind of data. This comes from the lab of Krishna Shanoya at Stanford. And in this case, the space is a latent space spanned by three different neural modes. Space spanned by three different neural modes. Every trial is only one point because we measure in a cumulative manner, we measure all the spikes during this 380 second window. And what we see is a very nice clustering. And moreover, the organization of the clusters corresponds to the organization of the targets in actual space. So, you know, we go from red to orange to yellow to green. We go from red to orange to yellow to red to orange to yellow to green. And so the organization. To green. And so the organization of this neural code in this very abstract space of neural modes captures the organization of the targets in real space. If we had picked three arbitrary directions in neural space, like these three neurons I showed you before, neuron 46, neuron 70, neuron 78, and again put one point there for each one of the trials and color-coded these points according to the target to which that trial was going. That was the target for that particular. That was the target for that particular trial. Then, what we see is no clustering and no clear organization that would allow us to tell us from the location of the point, which is the target that was the target for that particular reach. So, we need to do dimensionality reduction in a smart manner, in a manner that captures this underlying three-dimensional structure. And this gives us to the idea of neural manifolds. And now what we have is not the full neural space. Is not the full neural space that I told you about before, but a sub-sampling, a very strong sub-sampling of the neural space. If we use a multi-electrode array of the type that I showed you earlier, we can record of the order of 100 neurons. If we use neural pixels, we can record thousands of neurons. But in any case, we again have a data matrix with the same structure as before. Every row is a neuron. Every column is a time bin. So this is discrete times. But now the number of neurons is what we call. But now the number of neural systems we call d, where d is what we call the ambient dimension. It is essentially the dimensionality of an empirical neural space, which is the number of neurons that we can record simultaneously. And this is what is called the ambient dimension in the trait. So this is a scheme for what's going on. We have a very large interconnected network of neurons that are co-participating in the task, but we come in with relatively few electrodes, much more than one new. Few electrodes, much more than one neuron at a time, but much less than the millions of neurons that commodulate and participate in the task. So we have these three electrodes, and each one of these electrodes is recording spikes for that particular neuron, and after you do spike sorting. And so we have an ambient dimension, which is the number of recorded neurons. And what we know now for many years is that at every time step for every location of this beam, we can put a point in this empirical neural space. Empirical neural space spanned by the axis n1, n2, and n3, which are the spike the spikes emitted by each of those neurons within that time beam. And therefore, at every time beam, we have a point in this empirical neural space. And what we know empirically, again, is that these points tend to lie in a low-dimensional manifold. I don't want to call it a subspace because it doesn't really go through the origin. At best, if it's flat, it's a hyperplane, and if it's curved, it's not really a hyperplane. That if it's curved, it's not really a hyperplane, it's a manifold. So we have to make this distinction: whether we are talking about manifolds that are flat or manifolds that are curved. Manifolds that are flat are easier to deal with, because if I give you these two vectors, U1 and U2, these two vectors, which are orthogonal to each other and have unit length, provide a basis for every point in this hyperplane. So if the dynamics of the network is confined to this hyperplane, to this platform. Confined to this hyperplane, to this flat manifold, every point has coordinates that can be specified by saying what is the projection along U1 and U2, instead of thinking of it as a point in the high-dimensional N1, N2, N3 empirical neural space. And this basis is universal in the sense that it describes the whole manifold. If we have a curved manifold like this one, we still have a base locally, but this base has to But this base has to reorient itself and change as we move along the manifold. And one of the big discussions that we need to confront is whether we are going to focus on linear manifolds or non-linear manifolds, how we establish their dimensionality, and how do we construct good descriptions of the non-linear manifolds when needed. So, in the linear case, as I said, you have a basis U1 and U2, which is what we call the neural modes. And now I can describe the dynamics. And now I can describe the dynamics in that space in the hyperplane spanned by U1 and U2. So I can project the dynamics along the direction of U1 and the direction of U2, and I can get what is called the latent dynamics. And the interesting thing about this latent dynamics is that they are actually a generative model for the actual spikes. So if I know the generative dynamics, if I know the latent dynamics in this case, by using the matrix U, I can recover the spike. I can recover the spikes. And what is the matrix U? It's a matrix that has the neural nodes, the basis for that latent space, organized. In this case, I have two vectors, the green vector and the red vector, a blue vector that gives me the number of columns, and then as many rows as dimensionalities I had in the original ambient space, the number of neurons that I was recording from. So, this is a very interesting idea: the idea that you can actually use. That you can actually use these latent variables as a generative model, and that they are, in some sense, the building blocks of neural activity. And the existence of these low-dimensional manifolds has been established not only for motor cortices, which is the example that I'm going to give you in the rest of my talk, but for many other cortices, prefrontal, parietal, sensory cortices like visual, auditory, tactile, and also in hypogenetic. Tactile and also in hippocampus, not only in cortical areas. So, the existence of these low-dimensional manifolds to which the dynamics of the system is confined for a given perceptual task or motor task or behavioral task is a well-established empirical fact. And of course, what we want to do is understand better, like in the earlier two talks that we heard this morning, how does this dynamics arise? What is it about the network? Is it about the network, in particularly the trained network, the network that has learned how to perform a given task that allows results in dynamics that are confined to these low-dimensional structures, these low-dimensional manifolds. And what I'm going to do for the rest of my talk is I'm going to stay linear. And I want to convince you that staying linear, not staying linear, but starting linear is a very, very good approach because these linear methods can be in some cases. Methods can be, in some cases, unreasonably effective. And if, of course, they are not a complete description, they will eventually break down. But by monitoring how and when they break down, that gives you a lot of very good guidance for going further and going into the world of non-linear analysis and non-linear models, which is a very difficult world to explore just because there are so many possibilities. You cannot possibly explore all non-linear models. Cannot possibly explore all non-linear approaches to a given problem. So, which one to choose is an intuition that you can develop by looking by starting out with a linear method. So, let me go back to the simple sentenel task that I told you about before. This is a task that for monkeys in the lab of Leigh Miller, who is my collaborator at Northwestern, where the data is taken, is a task that is what we considered a very practice skill. A practice skill. The monkeys learn this task, and after a learning period, you know, they come back to the lab and they do other things, but every now and then they do this task again and again. And what we find is over many days, look at this, it's day one, day 462, day 732. So this is a span of more than two years. The monkey comes back to the lab and the trajectories that it allows, that it uses to reach each one of the eight targets in this simple certain task. Targets in this simple certain task are very conserved. They are very well conserved trajectories. So, for instance, if we look at the trajectory to the target, which is up and to the right, and we look at the X and Y profiles of the velocity in going from the center to that target, what we see is very well conserved velocity profiles, both for the X and Y components of velocity over this very, very long period of time. We can look at the correlations between X. Between VX and Vy, and the times the plots are where we have data, where the monkey was actually performing that task. And you see that these correlations remain very close to one. This is an aggregate plot of these correlations. So we have a behavioral observation of very stereotyped, very reproducible, very stable behavior. So the question is that we ask ourselves: is there a neural correlate of this? Of this stereotypical behavior? Is there a stereotypical neural activity that guides this behavior? And so, what we need to do then is as we measure behavior on day one and day n, measure neural activity in day one and day n. The problem that we have is that we implant these electrode arrays and then the electrode arrays move a little bit down and some scar tissue forms around some of the electrodes or suddenly some electrode captures a neuron that was not captured before. Neural that was not captured before. So the actual population that we're recording from on day n is not identical to the population that we're recording from on day one. So in order to establish stability of neural behavior, of neural activity, we have somehow to compensate for the fact that if there is a true latent dynamics in the full neural space, it's being projected onto different empirical neural space as time passes. So this is the picture, and this is. So, this is the picture, and this is essentially the take-home message of my talk. In principle, there is a very high neural space, which is the true neural space, in which we believe there is a true manifold that is associated with having acquired, having learned this task. And in this true manifold, there is some true latent dynamics that corresponds to the task. But we look at day one, we are measuring an empirical neural space, and when we look at day n, we are measuring a different empirical neural space, and then in that empirical neural net, Space, and then in that empirical neural space, we find a hyperplane using PCA or some other linear method, and we do the same thing on day n. And the question that we are contemplating, the hypothesis, was that these are just simply two different views of the same object, and that therefore it should be possible to align them, that it should be possible to align the trajectories on a given day with the trajectories of another day. And we were very lucky that we thought. That we thought of using a linear method. The reason why we thought of using a linear method was because we thought this dimensionality reduction is based on two linear operations. The first one is a projection from the true neural space into the empirical neural space, which is a linear projection. And the second is the finding of this flat hyperplane using PCA or some other linear method within the empirical neural space to find a flat linear manifold. Linear manifold. And because this is the composition or the concatenation of two linear operations, we thought a linear method for aligning trajectories should suffice to undo the effect of these two different projections, this two-step projection, and therefore allow us to align this data. And that's what we did. We essentially used canonical correlation analysis, which is a linear method. And I just very quickly walk you through it. Quickly walk you through it. So, we start with these matrices that have, this is the T discrete time steps, one per column. And we have a matrix of day n and a matrix of day M. And the number of neurons that we use is actually the number of neurons that results for doing a union between the neurons recording at AN and the neurons recording at AM. So, both sets of neurons are represented in this neural space. And when a neuron was not measured in a given day, we just Was not measured in a given day, we just simply put the activity of that neuron as equal to zero. So the dimensionality is the empiric, the union of these two empirical neural spaces. And for each one of these matrices, we do a singular value decomposition. So now the vectors, the column vectors of U give us a basis for describing neural activity on day N and the column vectors of Um give us a basis for describing the activity on day M. The activity on day M. So, what we do, of course, to do dimensionality reduction is simply to just skip the leading decolumns of these matrices UN and UM. We call that UN tilde and UM tilde. So now we have lowercase D vectors. This is going to be the dimensionality of our linear manifold. Each one embedded in the capital D dimensional space, or capital D is the ambient dimension. And we have a hyperplane which is. Have a hyperplane which is spanned by the columns of U on day N, and a different hyperplane spanned by the columns of U on day M. So we project the data into each one of these hyperplanes. We project the data on day N into a latent space by using the corresponding matrix. And we do the same thing on day N. So we are taking our data matrix, which is D by T, and we use the U transpose. So now we put this data. Transpose. So we now put these vectors as row vectors, our basis vectors, our neural nodes, lowercase d by capital D, and we get the dynamics of the latent variables. So now we have a low-dimensional number of latent variables that have a dynamics as a function of time. So the next thing that we do is EQR decomposition of these dynamics so that instead of looking at the true latent dynamics, we look at the version Q, which is a version in which these dynamics has been orthogonal. Which this dynamics has been orthonormalized by the matrices R, which are upper triangular matrices. And now the matrices Q are the ones that are used again by doing Q transpose Q. We get a similar value decomposition. The matrix U gives us a basis for the space for the dynamics of day N. The matrix V gives us a basis for the dynamics on day N. And S is the diagonal matrix. Is the diagonal matrix whose elements are the canonical correlations and they are sorted from larger to smaller. And we go from latent dynamics, which is not very well aligned, Ln and L M are not as aligned, into new and new descriptions. And how do we get these new descriptions? We construct these matrices M, which are the inverse of R times U and the inverse of R times V for days N and M, and we And we transform the latent dynamics using these linear transformations. Everything is linear. And now, if instead of doing looking at the correlation between the latent dynamics, we look at the correlations between these transformed later dynamics, where they have been transformed by multiplying from the left with the matrix M transpose. What we get is S, where S, I remind you, was the diagonal matrix that had as That had as elements in the diagonal the canonical correlations. So, this is a lot of math, but it's just linear algebra, you know, so it's not very conceptually complicated. And the beautiful thing is what kind of results we get from it. So, here I'm showing you the same central task. And what we have done here is we have averaged all trajectories corresponding to a given target. So, I'm only showing you eight trajectories, one for each one of the. Eight trajectories, one for each one of the targets in this centered out task. And I'm showing you again the trajectories in a three-dimensional neural space spanned by neurons mode one, two, and three, which are principal components. And we have one set of trajectories on day one and a different set of trajectories on day 32. And they don't look at all like each other. But when we align the dynamics, so we transform using the matrix M for day one and the matrix. Matrix M for day one and the matrix M for day 32. So we can form these latent spaces into these aligned latent spaces. Now we see the beautiful alignment. Now here I'm not showing you just one cluster per target because I'm showing you the evolution over time in order to reach that target and come back. So each one of these trajectories looks like a petal or a flower. You go out and you come back to center. And again, if you look at the color code, you again see this. Code, you again see this yellow to orange to green to blue, yellow to orange to green to aqua, where again, this representation is capturing the organization of the targets in real space. But now we have been able to align the dynamics, not just to cluster one point per trial, but to align the dynamics, the neural dynamics in the trial. And so this is consistent with the idea that these dynamics look different. Were look different because they were just simply linear transformations that involve the line-dimensionality reduction of a true neural space in which there is a true stable manifold that remains invariant across so many days. And so this is a plot of the correlations that we get between neural activity within the same day is in gray. If we don't align the dynamics, is this yellow color? And when we align the dynamics, Color, and when we align the dynamics, you recover essentially the same high degree of correlation across days as you had within days. So, this is the picture I want to leave you with again, that what we observe, first of all, are by our limited measurement abilities, are low-dimensional empirical spaces that are a sub-sampling of the true neural spaces. But within those low-dimensional But within those low-dimensional empirical spaces, we can identify even lower-dimensional dynamics that suffice to describe the phenomenon on the neural activity that is controlling a given motor behavior. Now, you could ask yourself what comes next. And let me tell you what comes next before I thank my collaborators. What comes next, of course, is not to stay within this very simple stereotype. Simple stereotype behaviors like the central task, but allow for more natural behaviors like moving within a larger environment, grasping, grabbing. And then we have to ask ourselves, what is the nature of the neural code? Do we still get such low-dimensional representations? And what is the information contained in these neural spaces? And my colleague Ege Altan will tell you more about that this afternoon. So I just want to thank the people who work with me on this project, Limb. People who work with me on this project. Lee Miller runs the Bunky Lab, where the data was taken. Juan Galliego was a postdoc with us at the time the work was done. He's now an assistant professor at University College London. Matthew Perrich was a graduate student and Rahid Chadadi was also a graduate student. Matthew completed his PhD, did a postdoc at EPFL, then is now with Kalaka Region at Mount. Calaca region at Mount Sinai in New York and is starting as an assistant professor at the University of Montreal later this year. And Rahid Chudhuri is also completed his PhD with us and is now a postdoc in the lab of Adon Batista in Pittsburgh. So I just want to leave you with this plot, which is in some sense a little bit against. In some sense, a little bit against the idea of using this variance-based criteria for deciding on the dimensionality. So, this is the variance accounted for. And if you were looking at that, you would decide, okay, if I want to achieve, I don't know, 90% variance accounted for, I need to keep of the order of 40 or 30 to 40 latent variables. But now let's say that I want to use these latent variables to extract information out of them. Extract information out of them. For instance, predict the activity of the various muscles that these neurons in motor cortex relate to. And when you do that and you look at the prediction ability as you increase the number of related variables that you use as input for your decoder, you see that you very quickly saturate and you reach the performance indicated here by circles, which is the performance that these very simple linear decoders would achieve. Would achieve if you had used all the recorded neurons. So now you see that actually, when you ask a question about information content, not about variance, but what information these neurons have about, for instance, muscle activity, now it suffices to consider 10 to 15 dimensions. And what is interesting is whether these very simple load numbers that we observe across all laboratory activities are or not. Activities are or not still there when we look at more natural behaviors. And again, I refer you to the talk of Ege Alta this afternoon. And that's all. Thank you. All right. Thank you so much, Sarah, for a lovely talk. And the first question I see here is from Alana. Do you want to just unmute? Do you want to just unmute and ask? Sure, yeah, I can do that. Thank you, Sarah. That was lovely. Great talk. I'm wondering what the limits are to this, like how many neurons do you have to have shared across the days? And also if it could extend to aligning across animals. That's great. We had a big session this morning within the group about aligning across animals. And the answer is yes. For these very simple tasks, like the central task, it's so stereotypical. It's so stereotypical. The dynamics are so stereotypical. All the monkeys essentially, no monkey does something totally weird like that. All monkeys move in a similar way. And so there is an abstract representation, which of course is different in every monkey. So there is no shared neurons at all. But if you do this procedure in monkey N and monkey M instead of day N and day M and you use the same simple canonical correlation analysis, you get very good overlaps between the trajectory. Very good overlaps between the trajectories, which implicitly answers your question about whether you need to have the same neurons or not. The answer is you don't. But this is a very abstract representation of the neural space. It's not what individual neurons are doing, but it's what are the patterns of collective. Essentially, they're covariance-based methods, right? Which, like the RSA methods that I learned about from you, you're asking whether the structure of the covariance. The structure of the covariance matrix here is similar to the structure of the covariance matrix in a different animal or in the same animal many days later. Yeah, good. Awesome. Thanks. Sorry, trouble unmuting. Yeah, do you want to ask a question, Tara? I think you're next. Sure. I really enjoyed that. So, my question was more about. So, my question was more about: in this case, you're assuming this comparison using these like linear hyperplanes, right? So, can you, because this is empirical, right, you're required, you're recording from MEA, and so you get these different sub-samplings over time. Can that also tell you something about if there is this non-linearity? And can you extract something? Can you talk a little bit more about if it was non-linear? Yeah, so most of what I know about the comparison between linear. know about the comparison between linear and non-linear comes really from very simple these very simple stereotype laboratory tasks so we took the same data which is the data for the central task and instead of use of doing this flat analysis projecting into a hyperplane we use isomap for instance and what we found using isomap is again that the two or three dimensional projection was sufficient and the projections look very similar but what is very interesting about using isomap Is very interesting about using ISO map is when you look at the eigenvalue distribution for principal component analysis, you see a very gradual decrease of the eigenvalues. So essentially, stopping at three has more to do with our knowledge about the task than something if it was a very complex task by looking at the eigenvalue distribution, you would not decide on truncating at three. However, when you look at the isomap eigenvalues and ISOMAP, I get values. And one of the reasons why I like a submap is because you can, by increasing the neighborhood in which you make a linear assumption, you can smoothly go from isomap to PCA. So you kind of anchor isomap onto PCA at what end. When you look at the isomapigen values, then you get one, two, three leading eigenvalues, and then you get to number four, and then all the eigenvalues are almost the same and decrease very, very slowly as you increase the number of the eigenvalues. Slowly, as you increase the number of dimensions. So that tells me that there is essentially a three-dimensional object in that space, which is non-linear, but when I need to, and of course the eigenvalues in the other dimensions are not zero because they are fluctuations out of the manifold, but they are all of the same thickness. It's like it's the amplitude of the noise, essentially. And the amplitude of the noise is almost the same in all these dimensions that involve the null space. Involve the null space, the space complementary to the manifold. And so by looking at things like that, you can begin to understand the effects of curvature, which are essentially the effects of the first order way in which to get account of non-linearities is to say, okay, the surface is not flat, it has a curvature. Can I begin to quantify that curvature? And again, I sum up gives you a way of doing that because you can. Map gives you a way of doing that because you compute geodesic distances. And by looking at the geodesic distance along the manifold as opposed to the geometric Euclidean distance between two points, you get a sense of the degree of curvature in the surface. Thank you. All right. Thanks. And there's another question in the chat from Danny Wen. So for each day, the PCF components Day the PCF components are weighted measurements on a different subset of neurons. So, Danny asks, Is there a way to combine all daily measurements to an aggregate weighted sum of all neurons? Well, in a sense, I am, because in order to relate, for instance, one of the things we have asked ourselves is how different are these hyperplanes on different days, or how different are the hyperplanes for different tasks for a given monkey? Tasks that are Given monkey. Tasks that are more similar, do they have more similar orientations in the hyperplanes? While tasks that are more different have hyperplanes that tend to be more orthogonal to each other. But in order to do that, I have to have all the hyperplanes embedded in the same neural space. This is why I construct this union of all the recorded neurons. I combine all the recorded neurons in a single empirical neural space in which I can place all the hyperplanes. Now, the hyperplane would have no The hyperplate would have no components in directions that involve neurons that were not measured for that particular task or for that particular day. But the idea of the union of the neurons allows me to place everybody in the same empirical neural space so that these comparisons are meaningful. All right. Okay, thanks. Any other questions? Thanks. Any other questions for Sarah? Just one from me, Zach, at this time. Yeah. Is there an issue to just putting in zeros for the neurons on the day they're not observed? Because you could imagine they were actually there and just had zero response. And that's kind of what you're saying when you put that in your data, but that's not quite what happens. Yeah, go ahead. No, putting zero means I just didn't record. Zero means I just didn't record them. It's not that they were not, and you're absolutely right. It's not that they were not alive and well and doing something. They are just not part of my empirical data for that particular well, for that particular day. So one possibility to do better than that is to use the neurons that you record from to do inference on the neurons that you were recording from early on, but you're not recording now. And that's a good way of dealing with that. That's a good way of dealing with that problem. What I'm doing by putting zeros is essentially I'm preventing the hyperplane from acquiring components in the directions of those axes when those axes, those neurons were not measured. So in some sense, I'm doing something similar to the original projection from the true neural space into an empirical neural space, which you could argue is the same as having put zeros everywhere where the neurons were not recorded. Yeah. Zeros were not recorded. Yeah, and I guess that makes sense for these linear methods where the zeros, like you said, sort of prevent the function from going on that axis. If you had a non-linear method, you might need to use the like imputing approach you just described. Exactly. Exactly. I completely agree with you, Joel. Yeah. Cool. Thank you. Okay. Well, thank you, Zach, and thanks, everybody. Yeah. Thank you so much, Sarah. Yeah, very interesting talk. Yeah, thanks again. Talk, yeah. Thanks again. All right, so our last talk for the morning, the mountain time morning, I guess, is Chung Chung Huang from University of Pittsburgh.