And the next speaker is Anna Siegel from Harvard. And yeah, we're happy that even though she's not here, she agreed to give a talk and inspire new discussions. And so she will talk about linear causal disentanglement. So Anna, please go ahead. Thanks a lot, Carlos. Thanks for the Hello, Carlos. Thanks for the introduction and for the invitation. I'm very sad not to be in Oaxaca in person. I was very much looking forward to it. But USCIS had other ideas. Yes, so I'm happy that I can at least give a talk online. So, as Carlos said, this is a talk on linear. Talk on linear causal disentanglement. And this is joint work with Chandler Squires, Salil Bharti, and Caroline Ulla. Please feel free to interrupt. Stop me at any time if anybody has any questions about anything. Okay, so I'm going to start with a sort of broad overview of three types of causality problem that we might encounter. We might encounter. So, the first type of problem has to do with parameter estimation. So, let's say we have two variables, Z1 and Z2. And assume we know that Z2 has some causal effect on Z1. So we can represent that pictorially with this directed graph here with two nodes, one for Z2, one for Z1, and an edge, an arrow from Z2. An arrow from Z2 to Z1, which encodes that causal dependency. So the parameter estimation problem is: how can we quantify this causal effect? Is it a strong effect? Is it weak? Does it have some positive effect or some negative effect? So we want to assign some weight to that edge. All right, the second type of causality problem we can consider is. Can consider is called causal structure learning. So, in this case, again, let's say we have two variables, Z1 and Z2. Now, we want to find what are the causal dependencies between Z1 and Z2. So, in this case, with just two variables, we have these three options. Option A is that there's no causal dependency between them, so we represent that by Represent that by a graph with two nodes and no arrow connecting them. Option B is that Z1 has a causal effect on Z2. And option C is that Z2 has a causal effect on Z1. So we need to decide between these three options. All right. And then the third type of causality problem is called causal disease. Is called causal disentanglement. And in this case, we measure some variables. Let's call the variables that we measure x1, x2, dot dot dot xp. And let's say that these variables are all caused by two unobserved or latent variables, z1 and z2. So in In both of the previous types of causality problem, we had direct measurements of the variables Z1 and Z2, but now we don't. Now we just measure some other variables, some X variables, and the X variables are caused by the Z variables, but we haven't directly measured the Z variables. And then we can still ask the question: okay, what are the causal dependencies between Dependencies between Z1 and Z2. Okay, so this is a more vague statement of what causal disentanglement is, and I'll say something more specific a bit later on. And before that, I'll talk in a bit more detail about these other two types of causality problem: parameter estimation and causal structured learning. Okay, so our parameter estimation. So, as I said before, we have these two variables, z1 and z2, and we assume that z2 has a causal effect on z1. And let's say that that causal effect is this variable a12, and we want to find the value of this unknown. That's our causal effect. So, okay, how can we do this? How can we do this? Well, for example, we could assume that the noise in this system comes from some multivariate Gaussian. So then the variable Z2 is entirely noise. And the variable Z1 is equal to some scalar multiple of Z2, where this coefficient is our causal effect. And then we also add. Is our causal effect, and then we also add on its own noise contribution. So we can assume that the epsilon i's, the noise terms, are normally distributed with, say, mean zero and some unknown covariance, omega i. All right, and then we're looking to find the unknown parameters in this system, omega 1, omega 2, and a12. So this is a regression type problem. How can we? Problem. How can we solve it? Well, we can gather some data. So our data will be an n by 2 matrix with some columns y1 and y2. And then what are our estimates for these unknown parameters? Well, our maximum likelihood estimate for omega 2, the variance of the variable Z2, is just the norm squared of all the data that we get. Of all the data that we gathered of variable two. So this column y2. And then next we're going to find the cause of effect, so the a12 parameter. And to do that, we project the first vector y1 onto the linear span of the second vector y2. And our best estimate for a12 is the coefficient alpha such. Is the coefficient alpha such that this projection is alpha times the y2 column? Okay, and then the last parameter we get by subtracting this projection from the column y1 and taking its norm squared. Okay, so that's regressing one column of this matrix Y on the other. In the case where we just In the case where we just have two variables. And then, more generally, we can package these causal dependencies into a linear structural equation model. And we can relate the different variables by some directed acyclic graph. So a linear structural equation model is governed by an equation of this form. By an equation of this form, so z is now a vector of variables, d variables, z1 up to zd. And we have that the entries of z relate via z equals a times z plus some noise term epsilon. And we can impose some. Impose some distributional assumptions on the epsilons, like we had previously, some normally distributed noise variables. But here I'm actually not going to do that. I'm just going to say that the covariance of epsilon is given by some matrix omega. Okay. And then how does the matrix A relate to the DAG? Well, the A is a D by D matrix. The A is a D by D matrix and its entry Aij is zero unless there's an edge from J to I in the graph. Okay, and I'm going to assume that the noise terms are uncorrelated. So that is to say the matrix omega is diagonal. Okay, so what does this look like in an example? Well, let's say we In an example, well, let's say we have four variables: Z1, Z2, Z3, and Z4, and they relate to one another via this DAG. So each edge is directed and there's no cycles. Then the matrix A has a lot of zeros, but non-zero entries corresponding to these four edges. So the A12, A13, and so on. And omega is now a diagonal. And omega is now a diagonal matrix with diagonal entries omega i. All right, so these linear structural equation models have been useful in applications such as protein signaling and in gene regulatory networks. And yeah, I'll come back to some gene regulatory networks again towards the end. Towards the end. Okay, so how in the case of more variables and all these different linear dependencies between them, how can we estimate parameters? So we can approach this in a few different ways. So one would be to project To project different data columns onto other data columns, as I did in the example where we had just two variables. And another possibility is what I'll outline here. So we're looking to find these unknown parameters given by the unknown entries of this matrix A. Unknown entries of this matrix A, so these four Aij entries for the four edges, and also the unknown diagonal entries of our diagonal matrix omega. Okay, so here's what we can do. So we can compute the precision or inverse covariance matrix of our Vectors of variable z, and this can be described in terms of the matrices A and omega. So I'll call it theta, and it has this form. It's the identity minus A transpose times omega inverse times the identity minus A. Okay, and we can notice that this precision matrix is in finite. In fact, equal to B transpose B, where the matrix B is very closely related to A. It's I minus A with the rows rescaled by omega to the minus one half. Okay, and one way in that B is similar to A is it has a very similar sparsity. So the entry Bij is zero. j is zero unless there's an edge from j to i in our graph. So how can we then find these parameters? Well we're going to find the matrix B by reordering the nodes of the graph to make sure B is upper triangular. So this is the reordering that ensures that whenever we have an edge from J to I, the index of J is greater than the index of I. Is greater than the index of i. So that's the ordering that we have on these variables. And once we have that ordering, the matrix B is the Cholesky factor of the precision matrix theta. So we can just compute this Cholesky factor. That gives us the matrix B. And then from B, we can find omega from looking at its diagonal. Omega from looking at its diagonal, and then we can also find the weights A by looking at its off-diagonal entries. Okay, so maybe I'll just pause in case there's any questions. Okay, great. It looks like everything is clear. Everything is clear. Yeah. Thanks. Okay, so soon we'll see how to extend this type of argument to the setting of causal disentanglement. But first, let me go to the second type of causal problem we could have, which is this causal structure learning problem. So as I said before, we have these three different options and we want to distinguish them. So what can we do? So, what can we do in this case? Well, for example, we could seek the causal dependency between the amount of rain. So, let's say that's our variable Z1, and the amount of umbrellas, let's say that's our variable Z2. So, let's gather some data of umbrellas and rainfall and observe that they're correlated. This rules out the first option, but the correlation doesn't allow us to distinguish between. Doesn't allow us to distinguish between options B and C. In order to do that, we need to observe data in some other context or intervention. So we can remove all umbrellas in some region and see if the rain still falls. And this would help us to rule out the causal dependency in C. So that would give that the causal structure that remains consistent with Remains consistent with the data observed to be this option B. So that's the type of data that we can gather to make this decision, whether we're in case A, B or C. And this type of argument can also extend to a larger number of variables as well. So So, how does this work? Well, now we observe some Z, and we're looking to find some unknown DAG on nodes Z1 up to ZD. So, to do this, we could hope to compute this DAG from the precision matrix theta, like we did when we were. Theta, like we did when we were recovering the parameters for some already known DAG. But the following theorem of work by Meek and Spertis and Tickering in the turn of the century says that the precision matrix theta gives the DAG G, but only up to Markov equivalence class. So this So, this Markov equivalence class captures the skeleton, so the underlying undirected graph, and the V structures. So, that's two nodes that point to a third node with no direct edge between them. All right, so here's a picture of the Markov equivalence classes on three variables. Three variables. And in order to go beyond the Markov equivalence class, as we saw in our example, we need to consider some intervention. So to actually find the DAG rather than just the Markov equivalence class it belongs to, we perform some intervention and an intervention at node i changes the weights a i j. The weights aij of all edges from j to i and it changes the variance omega i. So, for example, if we take this graph and intervene on node Z1, then this changes the two weights A12 and A13. It changes this top row of the matrix, and it also changes this first entry, omega1. Omega one. And under this intervention, we get new observations. So we can compute a new precision matrix. And the new precision matrix will be of the same form as the old one, but with this new updated weights in the ith row of A and in the ith row of omega. And a theorem of Eberhardt et al. from 2005 says that d minus one interventions are sufficient and in the worst case, necessary to recover an unknown DAG G from its precision matrices. So, for example, for this graph on So, for example, for this graph on four vertices, we would need, in the worst case, three interventions in order to recover the structure of this graph. Okay, so we're going to now see how all of this extends to this causal disentanglement setting. So, I have D laser. So I have d latent variables, so these are the variables z1 up to zd and p observed variables, so these are variables x1 up to xp, and they relate via this graph here. So I have some unknown DAG on my latent variables, and then I have a full bipartite graph of connections between the z and the x variables. Okay, and our goal is the following. And our goal is the following. So, based on some observations of the variables x, we want to recover the latent DAG. So the DAG on the DZ nodes. And moreover, we want to recover the map that takes our Z variables to our X variables. So that is, we want to recover. So, that is, we want to recover all of the unknown weights on all of these edges from Zi to Xj. So, there's various approaches that have been taken to causal disentanglement over time. So, probably the oldest imposes certain restrictions on what the latent DAG can. On what the latent DAG can be. So, the strictest assumptions that we could impose is that there are actually no edges between any of the latent variables. And this is the setting of independent component analysis. So this has been studied for the linear case. So that is a linear map from the Z variables to the X variables in this work of Comon from the 90s and has also been. Common from the 90s and has also been more recently extended to non-linear maps for the latent to observed variables. So then another line of work that fits into this setting of causal disentanglement imposes restrictions on what this mixing function can look like. So the mixing function is the function that takes the latent variables to the observed variables. To the observed variables. And there's work in the area of latent variable models that looks at how we can recover a latent DAG with various assumptions like this. And the assumptions say things like there exists a child node that only has one latent parent in the graph. Okay, and I gave a few examples. Okay, and I gave a few examples here. And in this work, we don't impose structure on what the latent DAG can be. So it could be any DAG. And we also don't impose assumptions on what the mixing function can be. Well, we impose that it should be linear, but we don't impose assumptions on which latent variables map to which observed variables. To which observed variables. But the price that we pay for this is that we require some interventions on our latent variables in order to recover our graph. So we're going to be incorporating interventions that are assumed to take place on the latent variables themselves. So on these unobserved variables. Variables. All right, so this is the linear causal disentanglement setting that I'm talking about. So we have some linear structure, a linear structural equation model on the node Z1 up to Zd. And we assume that the observed variables are some unknown linear transformation of. Linear transformation of the latent variables. And I'll call that linear transformation capital G. So now how are we going to find our unknown latent graph and also our unknown map G? Well, the precision matrix on our X variables. X variables is well, the precision matrix is usually the inverse of the covariance matrix, but for us here, the covariance matrix will not be full rank because all of the structure comes from just d latent variables, and we assume that d is less than p or less than or equal to p. But we can define the precision matrix to be the pseudo-inverse of the covariance matrix. So then what does the So, then what does the precision matrix look like? Well, it has this form. So, we have in the middle this usual precision matrix coming from a linear structural equation model. So, we have the identity minus A transpose omega inverse identity minus A. But then it gets multiplied on the left and on the right by some matrix H. So, H is the pseudo-inverse. matrix H. So H is the pseudo inverse of our mixing map G All right, so we can we have observations of our X variables, we can compute their precision matrix and that will give us this map, this matrix theta. Okay, so then our goal can be restated as given this matrix theta, Given this matrix theta, we want to recover the latent graph G and these unknown matrices of parameters A, the weights on the latent edges, omega, the unknown variances, and H, the mixing map, or its pseudo-inverse. Okay, so we saw that in the We saw that in the observed case, in the usual causal structure learning case, where our variables z are observed, we saw that the precision matrix theta tells us the graph, the DAG G, up to its Markov equivalence class. So, what about in this case? What does the precision matrix theta tell us about the Theta tells us about the latent graph. And remember, the latent graph is the sparsity pattern of the matrix A. So it tells us which edges are, the zeros in A tell us which edges are present or absent in the matrix A. So Hannah, quick question. Hi, yes. Hi. Hi, yes. Hi. So when you estimate your matrix data and then you try to factor it obtaining B, does the numerical noise change the combinatorial structure of B? Or how does numerical noise play into this process of recovery? Yeah, that's a great question. Thanks a lot. I was hoping you were maybe going to give an answer to this question, but you also, yeah, you asked a good question. Yeah, you asked a good question. From a numerical perspective, the idea of using the precision matrix is not such a good one. The very first thing you do is gather your observations of your variables x, you compute their covariance matrix, and then immediately you invert it, and everything you do, we do just depends on this inverse. Do just depends on this inverse. So, mathematically, algebraically, that turns out to be a nice thing to do. And we'll see a bit more about that later. But from a numerical point of view, it's really not the best idea because if you have some ill-conditioned yet rank D matrix, then you're going to be introducing some numerical errors. Some numerical errors. Yeah, thanks. Okay, so yes, I posed this question. What does the precision matrix given by this product tell us about the latent graph, i.e., the sparsity of the matrix A. Day. And the answer is: it tells us nothing. So there's nothing that we can infer about the adjacency structure of our latent graph from just this one precision matrix. And the reason is that we could just as well factor theta into a product of this form with the matrix A diagonal as we could. Diagonal, as we could factor theta into a product like this, where the matrix A has full upper triangular support. So there's nothing we can say about the structure of our latent graph just from this one precision matrix. So the situation is even more extreme than in the fully observed case. So Anna, a great So, a quick question. I think you maybe already answered it, but so could the sparsity pattern of A change because of numerical noise when you do the computation? Oh, yes, certainly. I mean, the yeah, we'll get to the details of how you can recover the matrix A and the matrix and the other parameters H and omega. But yes, in the presence of noise, enough noise, these estimates for the parameters and consequently for the latent DAG itself will change. I'll show some plots later where we can see how well we do at recovering the latent DAG under the presence of noise. Thanks for the question. Okay, so then, as we saw in the observed case, the way we're going to recover this latent graph is to incorporate some interventions. And these interventions are on the latent variables. And we assume that each intervention doesn't change the mixing map, so the map from the latent to the observed variables, but it does change. But it does change the matrix A and the matrix omega. So each intervention gives us a new precision matrix that has this form. So it's the same matrix H, our matrix A has been changed along its kth row, omega has changed at its kth diagonal entry, and now our main result is the following. So we So, we show that D perfect interventions are sufficient and, in the worst case, necessary to recover the graph G from the tuple of precision matrices theta k. So this is just one more intervention than we saw in the result of Eberhardt in the fully observed case. So, there we needed D minus one interventions. needed D minus one interventions and here we need D. And how do we prove this result? Well, we studied this as a decomposition problem on a tuple of matrices. So our tuple of matrices are the tuple of precision matrices, one in each intervention. And we're looking to decompose each matrix in the tuple into this form. Into this form, where h is the same for every matrix on the tuple, and a and omega are also closely related, so they only differ along one row. Okay, so I didn't say yet what a perfect intervention is. So, what's a perfect intervention? Well, I said before an intervention at some node i changes the weight of all edges from j to i, and it changes the variance. i and it changes the variance omega i well a parfreight intervention zeroes out the weight of all edges j to i and again it still changes this variance omega i. So a perfect intervention at this node z1 would zero out these two parameters a12 and a13 and it would replace omega 1 by some other Replace omega 1 by some other value. Okay, so this is the setting of our result, the perfect intervention. Okay, so firstly, what are the sources of non-identifiability that are present in this problem? So we're looking to recover the DAG G and matrices AK, omega K and H from these matrices theta K, which have this form. matrices theta k which have this form so as i did earlier i'm going to introduce this matrix b k which is this combination of omega and a and these are the matrices i'm decomposing these theta k matrices so let's assume that we have one solution given by a tuple of b k's and one matrix h well then we can observe Well, then we can observe that multiplying BK on the right by some lambda and multiplying H on the left by lambda inverse for some matrix lambda that's diagonal with positive entries, this will also give us a valid solution to the problem because we only ever observe BK and H multiplied together like this. So this is one source of non-identifiability and to get around it we can without loss of generality set Without loss of generality, set the largest absolute value in each row of the matrix H to one. Another source of non-identifiability is the following. So let's assume the latent graph G has no edges. Then our BK matrix is diagonal. So A is just the zero matrix, BK is diagonal. And then any permutation of the rows and columns of BK is also going to be a maximum. Columns of BK is also going to be diagonal. And that's because we can multiply out this product and see that P B K P transpose and P H is also going to be a valid solution to this decomposition. So these give us two sources of non-identifiability of our problem, and these are the same sources. And these are the same sources of non-identifiability that arise in the usual classical setting of linear ICA. And what we'll show is that these are the only sources of non-identifiability of this problem. So we show that assuming we have one perfect intervention on each latent node, then the graph G and the matrices BK and H are Matrices BK and H are identifiable up to a set which I'll call S of G. That is, given a solution which is a tuple of matrices B and then a single matrix H, the set of solutions is given by P B K P transpose and P H, where P lies in this set S of G. S of G. Okay, so what is S of G? Well, it's the set of permutations on D letters such that sigma of J is greater than sigma of I for all edges from J to I. So Sg is some subset of the permutation matrices, and it's a subset that's determined by the structure of the graph G. So for example. G. So, for example, if G has no edges, then S of G is all permutations. If G looks like this, we have two edges, two to one and three to one, that S consists of the identity and the permutation, the transposition that swaps two and three. And just to see one a bit more complicated example, if G is this graph, then S of G is the set consisting of the identity matrix. Consisting of the identity matrix, the transposition 2, 3, and the cycle 2, 4, 3. Okay, so that's our identifiability result, that this is the exact set of solutions to this problem. And I'll say something about how we prove it. So, our main proof ingredient is what we call the partial order RQ. We call the partial order RQ decomposition. So, given some partial order, the partial order RQ decomposition writes a matrix as the product H equals R times Q, where R is a square matrix with non-negative diagonal, and with R ij equals zero, unless I zero unless i is less than or equal to j in the partial order and the matrix q has rows that are norm one and they have a certain orthogonality property so qi should be orthogonal to qj whenever i is strictly less than j in the partial order so uh it turns out that um Out that a full row rank matrix has a unique partial order RQ decomposition for any partial order. So here are some examples. So if the partial order is actually the full order, one is less than two is less than dot dot dot is less than p, then r i j should be zero whenever i is greater than j and q i should be orthogonal to q j whenever i is To qj whenever i is less than j. So that's the usual RQ decomposition. If the partial order imposes no relations among any of the values 1 through p, then the condition on the matrix R is just that it is zero whenever I is not equal to J, so it's diagonal. Diagonal and the condition on q is that the rows of q should have norm one and another example so if i is less than if one is less than two and one is less than three but there's no other relations in this partial order then the matrix r has this support at the orthogonality imposed in the matrix q is that q1 is orthogonal to q2 and q3 okay so that's Okay, so that's our partial order RQ decomposition. And then the proof looks something like this. So our input is these precision matrices. We Anna, may I ask something? Yes. In the previous slide, so this partial order is on P. On P or D or at the same time? So, like the columns, the rows, or should I think of it's on the D. It's on D. So one to D. Yes, you're right. So this should be a D. Yes. Okay. Yes. Thanks, Carlos. Okay. Thank you. And thanks also for interrupting. So I think, in the interest of time, I'll actually skip the The discussion of the steps of the proof, although I'm happy to talk about that. And that tells us the sufficiency of our D interventions, but we can also prove the worst case necessity of requiring these D interventions to recover the latent DAG. And we can do that as follows. As follows. So we can assuming we have fewer interventions than the number of latent nodes, we can show that there exist matrices H and BK, and also that there exist a set of distinct intervention targets such that our solutions are not identifiable up to this set of permutations S of G. So that gives the worst case necessity in our The interventions are sufficient and, in the worst case, necessary to recover the latent DAG. And again, maybe I won't go through this linear algebra and I'll share some computational results since there was a question about this. So, as part of our identifiability study, we also give an algorithm for recovering. Algorithm for recovering the mixing matrix H, the matrices BK, and also the intervention targets. We don't assume that the intervention targets are known, so we want to recover them. And this is the data we get. This is the result we get from some noisy data. So at each sample size, we generate 500 random models and Random models, and we assume that there are five latent variables and 10 observed variables. And we sample our graph G from some Erdos-Rainy random graph model with edge density 0.75. So as our number of samples increases, we can see that our error in the mixing matrix A. Mixing matrix H goes down. Our error also in our matrix B0 is also decreasing. And the fraction of models where all of our intervention targets are correctly estimated is increasing. And let's see. So the question from earlier was about if, in the presence of noise, we can recover a latent graph that is different from the true latent graph. And this figure C captures that, because if we're not assigning our intervention targets correctly, in this, let's see, 10 to the final. Let's see, 10 to the five samples. Maybe this point is like 0.92. I don't know. In this 0.08, in this 8% of cases, we have not recovered the true latent graph. Okay. So we also tried this out on some biological data to see if it gave some interesting findings. So the data that we considered. So, the data that we considered here was a single-cell RNA sequencing data set from a lung cancer cell line. And the interventions or the different contexts that we observed are different mutations of the K-RAS oncoj. So, then what the method will do is take each of these different observed contexts. Observed contexts and assign each of them to be an intervention on some node in our latent graph. So this, okay, this figure is maybe a bit small to be able to see, but this is our latent DAG of our Z variables. And each of our different contexts has been assigned as a perturbation. Assigned as a perturbation at that latent node. So here are two zoomed-in pictures, which may be slightly easier to see. And they show the latent variables that various different mutations are assigned to. So we have these G12 and G13 perturbations. Are perturbing highly connected nodes with several descendants. So they've been assigned to nodes in the graph that have a big effect on the other variables. And this matches up with known understanding that these G12 and G13 positions in the KRAS protein are known to be causal drivers of cancer. So, in this sense, the Of cancer. So, in this sense, the output of the method roughly aligns with what we know to be true about the relative importance of different mutations. Okay, I was asked to say a little bit about some open problems stemming from this work for the workshop. So, here's some of Of my thoughts on open problems. So, the upshot of this project is that we can learn a fully latent graph by observing different interventions. So, this was our setup. Our graph was fully latent, but nonetheless, we were able to recover the graph and the relation between the latent variables and the observed variables. And it's an open problem then to extend this to other latent structures, so other fully latent graphical models. So for example, we could consider an undirected latent graph. So that is a latent graph that's capturing the correlation or uncorrelatedness of different latent variables. We could extend it to a directed graph that is allowed to have cycles. And this would be a latent analog to some recent results. And we can also consider some latent graphical model that has a coloring on its vertices and edges, where the coloring encodes that certain weights of the latent graph should be. Latent graph should be the same. Okay, so those are some other latent structures that we could perhaps study with some similar approaches. And another direction that's also open is to find the equations that define this model. So, for example, we could look for equations or even inequalities in the In the entries of this tuple of matrices, theta k, this topple of precision matrices. And yeah, that's all I wanted to say. So thanks a lot. And for more, you can check out our preprint. Thanks for listening. Wonderful. Thank you, Anna, for this great talk. And yeah, are there questions from the audience? Can we hear from the audience? Hi, Anna. Can you hear me okay? Yes, hi. Thanks for the really nice talk. It was really great. Super interesting. I seem to be in a place that's hard for them to corner me. So, but yeah. Anyways. Luckily, I recognize your voice. Fantastic. Yes. This is. Oh, yeah. Yes. This is oh, yeah. There I am. Oh, yeah, yeah, there you go. So, okay, cool. So, if I understood your results correctly, the idea is that you imagine that there's these latent variables governing your causal system that you have identified, right? So, these are like latents that you know, but you just can't measure. And we don't know what the latent variables are. Oh, you don't know that. Okay, that's great because that gave me. Don't know that, okay. That's great because that gave me a moment of pause. Um, but that's right, yeah, we don't know what the latent variables are, but we assume that our different observed contexts are interventions on those unknown latent variables. Okay, great. So that's why, so in a sense, you're imagining that you are targeting exactly the D variables that you have, right? So you can apply your theorem. Exactly. You're matching up the different contexts that you've observed to these D variables, but To these D variables, but they're not known. By the end of the algorithm, you can find them. For example, in the case where we were looking at gene expression data for the observed variables, we then also have this map that sends us to the latent variables. And that tells us, you know, this latent variable is this linear combination of genes. But we don't know what those are from the outset. Okay, cool. Fantastic. Then my thinking is right in line with what it should be. Is right in line with what it should be, I think. So, but then the underlying assumption, though, is that those variables that you do target, there exists this DAG structure on them, right? Yes. And you were mentioning these very nice generalizations, which I totally support. But it seems like somehow the most natural one to me would be like assuming that there is an underlying DAG structure on the latent space, like this is maybe. The latent space, like this is maybe the easiest assumption. Like, but then you don't actually know what these latents are, and you're only going to be able to actually say anything about the ones that you've targeted, right? So in some sense, the remaining latents in the DAG get marginalized out, which should put you in the realm of like, you know, like a maximal ancestral graph or something. So I was curious if you saw how like maybe. How, like, maybe it's easier to generalize that than, like, say, things with cycles. Do you have the sense of how that generalization might play out? Yes. So let me see if I understand your question correctly. You're saying if I don't have an intervention on each of the latent variables, then I know that I'm not going to be able to recover the full latent graph. Latent graph? Yeah, I guess I'm saying that, like, I imagine that there's all of my latents, but, and they do form a DAG structure, but then I do my interventions, but it doesn't target all of them. And maybe it like misses a latent that has an arrow pointing into two of the things that I do target. So now I sort of need to think of that latent as marginalized out. As marginalized out from what I am capable of observing using your methods, which would then sort of put a bi-directed edge between them. Do you have a sense of... I see. You're saying. Yeah, that makes sense. I see. You're saying that the latent structure could itself, it could be a graph that has both directed and bi-directed edges. Yeah, exactly. Okay, yeah, that's nice. And I suppose another way to encode that would be. Encode that would be in the same setting as this one. But if I go back to the place where I assumed that the matrix omega was diagonal. Exactly. I don't know. I don't know what it is. It's a decomposition that looks like this, where in this setting, we're assuming that this matrix omega is diagonal, but you're saying we can assume that omega. Diagonal, but you're saying we could assume that omega has some sparsity, some block structure, and it's encoding a collection of bidirected edges. Exactly. Yeah. Do you have a sense of how hard the problem gets from there? I do not. But I think that's a nice suggestion. I think it's very much at home on this list of three things. Cool. Yeah. Thanks. Fantastic. Yeah, thanks. Fantastic. Yeah, thanks again for the nice talk. Thanks. Nice. Yeah, thank you. Yeah, I think I was going to ask the same, if it made sense to consider a mixed graph on the sets, right? So basically you have bidirected edges and the omega is not diagonal anymore, right? So I guess that's what the conclusion, right? Yeah, nice. So more questions? Nice. So, more questions. Maybe from online. Is there anything, Elena? So, do you have a sense of how hard it would be if the interventions weren't perfect, but somehow gave you some like linear independence or something? So, in the case where the interventions are not perfect, so we can still recover the Cover the latent graph itself, but only up to transitive closure. So the transitive closure, meaning whenever we have an edge from i to j and from j to k, we also have the edge from i to k. So we can't get the true graph, but we can get its transitive closure. But we can't really, we haven't been able to do much about recovering the parameters themselves in the case of non-perfect. Case of non-perfect interventions. And the challenge is that for perfect interventions, the parameters you're looking for are all present in the original matrix, except these new variances. So for each new intervention, you're only adding one extra parameter. Whereas in the case of a non-perfect or soft intervention, you'd be adding the in-degree many new variables in your new. Variables in your new intervened context for all these unknown weights on these new edges. It's not necessarily impossible, but more difficult. Another question here. Well, I have one. Well, I have one more question. In the data that you showed us with the cancer, so how big are we talking here? Like the number of nodes, like the number of data points? And how computationally intensive was this? Yeah, so the data set we were looking at has 78 different mutations. So we're looking at a graph on 78 nodes. On 78 nodes. And in terms of the computational expense, it is. Let's see. I remember we worked out the number of operations in terms of the parameters p and d, but it's not too much worse than inverting these. Inverting these p by p matrices. And then the way the algorithm works is by looking at various different pairs of contexts and subtracting the precision matrices and then computing the rank of this matrix and various things like that. So I don't remember what it is, but it was something like, I don't know, D to the. Um, I don't know, D to the something, P to the something, where the somethings I think were both less than or equal to five. It wasn't too bad. Okay, well, you can do it on your laptop. Oh, yeah, and in practice, yeah, it wasn't, it wasn't too, uh, it wasn't too expensive. Okay, that's good. So, anything else? Any last question? Yeah? Hey, it's me again. I was curious if you knew of any work. Curious if you knew of any work, sort of so everything that you're doing here is very linear. Um, so but do you know if there's any work about like the same sort of identifiability problem and following your perspectives, but from like a non-linear case, maybe like you know, additive non-Gaussian noise models or something like this? Yes, I think there is not yet, but this is something. Yet, but this is something that I'm quite enthusiastic to work on. I support that enthusiasm. I think that'd be super cool. And I mean, certainly in the case where the latent variables are independent, there's a lot of work in non-linear ICA. Yeah, for sure. Cool. Thank you. Okay, so if there are no further questions, then I would say we thank Anna again for the very nice talk. Thanks. Nice talk, thanks. Yes, yeah, and especially thanks for giving us ideas in the open problems. That's nice. So, yeah, okay, maybe we can have like a five-minute break, and then we're gonna.