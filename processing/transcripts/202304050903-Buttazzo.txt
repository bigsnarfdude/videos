These two quantities are related to each other. If we take a measure constraint to the measure of omega fixed equal to small m, it is very well known that the biggest torsional rigidity is provided by the ball. This is called the San Vena inequality. So T of omega is always less than or equal. Ways less than or equal than the torsion of the ball of the same volume. And similarly, if you take the eigenvalue, again, it is very well known that the minimal eigenvalue is provided by the ball. And this is called the Faber-Cran inequality. So it seems that the So it seems that the two quantities behave in an opposite way. I mean, if you like a big torsion, you should be ready to have a small eigenvalue and vice versa. The question is, is this intuition correct? In the sense, is it true that lambda behaves like one over t or more generally? Or more generally, like one over T to some power Q. Is it true or not? Behaves like, I mean that the ratio is between two constants. When I say A behaves like B, means that A over B, this ratio, is between two positive constants. So is it true this intuition that That lambda behaves like one over t or one over t to the q. This is the goal of this lecture. Even more generally, the goal is to study what people call the Blasque-Santalot diagram for these two quantities. And this means that we need to We need to identify a particular subset of R2 or the plane. And this set E is made by all points XY, where X is a torsion and Y is the eigenvalue. So this is some particular subset of R two, and this is very important. And this is very important. This is very important because if you have a shape optimization problem, which is in general a very difficult problem, a certain function and eigenvalue, you want to optimize, minimize a certain function of torsion and eigenvalue, which is a quite difficult problem. Then you can reformulate this problem into a very simple one because this difficult problem is reduced to an optimization problem just in R2, finite dimensional. In the sense that you minimize this function f of xy, where xy, the point xy, varies into e, where Into E, where E is the Blasque-Santalot diagram. So a difficult problem becomes a very simple one. Where is the trick? The trick is hidden in the fact that in general the characterization of the Blanske-Santalo set is very hard. And actually, we do not know how this set. How is this set Ed? We can only give bounds by studying this product lambda of omega times T to the Q of omega when the measure of omega is fixed equal to M. So this gives some bounds and may give some hints on how they set the blast. How the set, the Blasque-Santalo set E is made. The two quantities, torsion and eigenvalue, they scale very well. Indeed, the torsion of T omega, this is very simple to get, is equal to the T power dimension plus two times T of omega. times T of omega and eigenvalue of T of omega scales like T to the minus two eigenvalue of omega and thanks to the scaling properties above we may reduce to study the this this functional this shape functional when omega equal to one Omega equals one, or if we want to remove the measure constraint, we should scale this product, dividing by measure of omega to a suitable power in such a way this ratio is scaling free. So we may choose either we put the constraint measure equal to one. measure equal to one and then the functional is simply lambda times t to the q or we may remove the constraint by considering this this scaling free functional on some classes of admissible domains. So I think for the for the presentation the first option is simpler so I simpler so I always fix the measure of omega equal to one so we may consider also different various classes of domains and I start by considering the class of all possible domains no geometrical restriction apart to the measure which is equal to one so this is a research I did with Michael I did with Michael Vandenberg, with Aldo Pratelli, Francesca Prinari, and Luca Briani. But the problem is not new. I mean, you can find inequalities like the one I want to discuss in the past by some very famous names, George Paul. George Polya, Gabor Zego, Andre Mackay, Joseph Hirsch, and Hans Weinberger. They studied inequalities depending on a domain omega. But in my opinion, the best result was obtained by a young young at that time. I mean, Marie-Thérèse. Marie-Thérèse College-Jobin. This is the deepest result in this field. She obtained this result in 1978 in the case of Laplacian. And Lorenzo Brasco generalized this to the P-Laplacian, to the non-linear case. We wrote some papers about the relation between torsion and eigenvalue together with. And eigenvalue together with Vandenberg and Velichkov. And there is another interesting paper by Vandenberg, Ferron, and Nietzsche and Trombetti. I will recall later on. And a more recent paper by Ilaria Lucardesi and David Zucco. Of course, the Blasque-Saltalo theory is much more general. You may consider More general, you may consider other quantities, not only the torsion and the eigenvalue, but every pair of quantities. And indeed, in the literature, you find several studies. For instance, the two quantities are lambda one and lambda two, the first two eigenvalues of the Laplace. And I did this with Doreen Bucou. I did this with Doreen Bucour and Isabelle Figuero, 1999. And more recently, Lambda 1 and Perimeter. This is also an interesting study by Elias Stef Tui and Jimi Lamboulet. Then torsion and capacity, there are interesting relations between torsion and capacity, and I did with Michael van der Berg. And torsion and perimeter, but there are a lot of inequalities you can study. All of them depend on the domain omega, and the goal is to study optimal bounds for these two quantities. But let me go back to the torsion and eigenvalue. There are two crucial There are two crucial thresholds in the power. So the first one is what I say, the Courlen-Jobin result. Well, actually, let me say something which is very, very natural. And that so the functional I want to study is this one, lambda times t to the q. Well, you can Well, you can guess very easily that when Q is very small, very close to zero, the effect of the torsion almost disappears and you remain with only the eigenvalue. So it is very reasonable to think that when Q is very small at this This optimization problem, the optimal domains tend to the optimal domain of lambda only. And this is known to be the ball. So when q tends to zero, you expect that optimal domains become round and round, and at the limit when q tends to zero, you get the ball. On the contrary, when q On the contrary, when Q becomes very large, the effect of the eigenvalue disappears. And then only the torsion remains. And again, you may expect that when Q becomes very large, tends to plus infinity, the optimal domains optimal this time in the sense of maximization. Maximization, thanks to the Sanvena inequality, they tend to the maximum of the torsion, and again, this is given by the board. This is the intuition. But the deep result by Marie-Thérèse College-Jovain, she proved that when Q is below this threshold, two over-dimensional Over dimension plus two, which means one half in dimension two. Not only the optimal domain is close to a ball, but the optimal domain is actually the ball. Not only close to a ball, but is the ball. So below this collerge-band threshold, the optimal domain, optimal. Optimal in the sense of minimum. The optimal domain is a ball. And so this number is the first important threshold. And another important threshold is Q equal one. Q equals one gives a much simpler in the proof. I should say that the Color-Jobain proof is rather deep, rather delicate. On the contrary, On the contrary, which is called polia inequality, in spite of the importance of the name, this inequality is very simple. Just use in a smart way the older inequality. And you can prove that the product lambda times t is always between zero and one. This is a very simple proof. Proof. Which is a little bit more delicate is to show that this product, the supremum of this product, is exactly equal to one, because Pauli inequality simply says that the product is between zero and one. But indeed, you can prove that the supremum of the product is exactly equal to one. And to do this, you need to consider. To do this, you need to construct a particular maximizing sequence for the product. And this is the way you should do. You take a disc or a ball and remove very small holes. If you call epsilon the distance between two holes, you should tune very carefully the Tune very carefully the radius. And in dimension two, the correct tuning of the radius is e to the minus one over epsilon squared. So extremely small. And also in the other dimensional case, d bigger than two is a power epsilon to d over d minus two. So if you tune the radius of the holes in such a Of the holes in such a way, you can show that the supremum of lambda times torsion is one. So let me summarize what we have obtained. So below the Colonel Jobin threshold, the minimum is given by the ball, and the supremum is plus infinity. It's very easy to construct. Infinity is very easy to construct a sequence of domains for which this shape functional becomes larger and larger. And so, if you take the functional is not between two constants, so the answer in this case is no. Our intuition was wrong. If you are between Coler Joubert and one, in this case, the infume is. In this case, the infimum is zero and the supremum is plus infinity. So it is rather easy to construct sequences of domains for which the infimum is zero and the supremum is plus infinity. Q equals one, the infimum is zero, and as I said before, the supremum is equal to one. And so again, if you take If you take the infim equal to zero, so our intuition is wrong also in this case. And finally, if Q is strictly larger than one, the infimum is zero, but the supremum is finite. This is a very interesting case which needs some deeper investigation. So, for the moment, let's skip this interesting case. And you see, this table shows very well that our initial intuition that lambda behaves like one over torsion to the Q is always wrong, because either the infinite is zero or the supremum is plus infinity. And putting into a plot what is written in the table above, you get this colored region, which is not the Blasque-Santalo diagram, but it is larger than Blasque-Santalo diagram. So it is an upper bound. Blasque Santalo is called. Blasque Santalo is contained in this colored region. So the lower line is the colour Jobin line and the upper line is the polio line. So all the other cases should be contained between these two lines. But we do not know the precise form of the Blasque-Santalot diagram. Diagram. So this is an upper bound and this is a lower bound. So the Blasque Santalo diagram contains this colored region. This is very simple. This colored region is the region that we obtained. We obtained it by taking omega. Taking omega made by union of disjoint disks. For these joint disks, we can compute explicitly eigenvalue and torsion, and putting everything together, we get this picture. So, the mysterious Blasque-Santalot diagram is somehow between these two plots. You see, there is a lot of room. You see, there is a lot of room here that should be investigated. So it is smaller than this and bigger than this. The only case in which we can characterize completely the Blasque-Santalot diagram is the one-dimensional case, and this is precisely the Blasque Santelot diagram. Well, the reason is that in dimension one, In dimension one, every open set is a union of disjoint intervals. So this is why we can plot the full Blaski-Santalot diagram. So let us add some geometrical constraint, and the simplest one is convexity. Now we consider only convex domains omega. Convex domains omega. And in this case, there is an old conjecture, even in dimension two. The conjecture says that the product lambda times t divided by measure of omega by, but you can take measure of omega equal to one, for instance, then this product should be, this is a conjecture, should This is a conjecture, should be always between these two numbers, pi squared over 24 and pi squared over 12. Pi squared over 12 is the number that you get by taking omega a long and thin rectangle. Then you can compute very easily for a long and thin rectangle, lambda times t. Lambda times t gives you pi squared over 12. On the left, pi squared over 24 is what you get by taking omega a long and thin triangle. And again, the computation is simple and asymptotically, you get pi squared over 24. The conjecture says that all the other cases All the other cases are between these two ones. All the other cases are between the long and thin triangle and the long and thin rectangle. But the conjecture is still open. So if the conjecture is true, of course, the previous picture becomes slightly smaller. You see here. So what is So, what is known up to now? Well, up to now, the best which is known is that lambda times t is between pi square over 48, the double number than the conjecture. So it's about zero two. This is proved, but the conjecture says zero four. So you see there is a lot of room to be investigated. To be investigated. And from the right, the conjecture would say 0.8. The best available proof up to now is 0.9999. So also in this right bound, there is a lot of work to be done. And in dimension three, there is a similar control. There is a similar conjecture: pi square over 12 always from the right, and pi square over this number, 2d plus 1, d plus 2 from the left. Pi squared over 12 is given by a thin slab. You can compute easily. A thin slab gives pi squared over 12, while the left bound. The left bound is given by a rather flat cone, a cone which has a very small height. Also, in this case, you can compute easily this bound, and the conjecture is that all the other cases are between the flat cone and the slab. This is the table which summarizes what we get for convex domain. For Q less than one, the minimum is rich. You can prove rather easily. Thanks to the convexity, which is a rather strong constraint, the minimum of Fq is reached on a convex optimal domain omega. Optimal domain omega, while the supremum is plus infinity. Q equals one, Q equals one, our initial intuition is right this time, but restricted to convex domains in the sense that the infimum is strictly positive and the supremum is strictly less than one and cube. And q bigger than one, again the intuition is not true because the infimum is zero. The maximum exists and is finite, but the infimum is zero. The only case in which the conjecture has been proved is the case of thin domains. Thin domains are domains for which one of the For which one of the coordinates, one of the coordinates is small. So omega epsilon is like that. If you restrict the analysis to thin domains, then you can prove the conjecture. So this is a good sign, but this only says that the conjecture is true for thin domains. Actually, the true conjecture is for all. Conjecture is for all convex domains, not only for the thin ones. And so putting all together, you have these two plots. You see on the left, I asked to a student to take crazy domains, and for every crazy domain, Domain plot torsion and eigenvalue. So in the horizontal absisa is one of eigenvalue, one over in order to get the interval 0, 1. And on the vertical, the torsion. And so this is the plot. You see the small dots are the numerical components. Are the numerical computation. You see that this numerical computation concentrates near the Koleshoben line. The red one is the Koler-Joben line. On the contrary, they seem to be rather far from the yellow line. The yellow line is the poleon line. But this is just made. By taking crazy domains, strange domains. So we want to know more, at least numerically. Well, the only thing we were able to prove is with Aldo Pratelli, we proved that the true Glasgow-Santalo diagram is the region between. Is the region between two graphs. There are two lines, and the full Blasque-Santalot diagram is the region between these two lines. The line below is the colour Jobin one, and we see rather well in the picture. The upper one is not the polio, is not the polio line, but we can prove that there. But we can prove that there exists a mysterious function h of x such that the Blasque-Santalot diagram is the region between Colange of n, which is a kind of parabola and this mysterious function h of x. We cannot say anything more on this function h of x except H of x, except that it is increasing and some bounds. But the picture of the function h of x still remains mysterious. And so the idea is to try to obtain at least numerically. But here is a surprise. So if you ask to If you ask any people, you say, well, what do you suggest to plot numerically the function h of x or if you want the Blasque-Santalo diagram? Well, the answer is almost unanimous. People say use Monte Carlo, use Monte Carlo method. You plot, you consider some random. Consider some random domain or random polygon, if you like, because polygons are dense in the class of ball domain. So take a certain number of random polygons and then plot if the number is large enough, you can get Plaski Santalo. Well, this is absolutely wrong. Monte Carlo is Carlo is very bad. This is what I want to show in these last minutes: the failure of Monte Carlo method in order to draw carefully the image of a function. So, here is another attempt made by Lias Tuy and Jimmy Lambolet in their case. In their case, they had the perimeter and eigenvalue. They tried many samples, I think 1000, something like so. But you see, you cannot guess. This is Monte Carlo. You cannot guess the Blasque Santelo diagram. This is rather accurate and near the left corner, but I mean, you cannot get. I mean, you cannot guess at all what happens on the right. So, in general, we want to draw carefully the image of a function. So, this is the problem. We have a continuous map, capital F, from X. X is a compact set, even finite dimensional, if you like. In the previous In the previous case, X is infinite dimension because we had the eigenvalue and torsion. But even for finite dimension, we have a continuous map, capital F, from X, which is a sub-compact subset of RK, into R D. And the problem is that the dimension, the starting dimension. Dimension, the starting dimension is much larger than the final dimension. So, k much larger than d. And you can prove very easily this theorem. You have two compact metric space, a continuous map between X and Y, and you want to draw Y, the image. You know very The image you know very well x, but you want to draw y, which is the image f of x. And the result is that for every probability measure nu on the image, there exists a probability measure mu on the initial space x such that the push forward of mu. The push forward of mu is equal to μ. So, what is Monte Carlo method? Monte Carlo method is equivalent to take mu equal leb. But then if you take mu equal leb, which means distributing the samples in the space initial space x uniformly. Uniformly, this does not give a very careful description because you get a measure new, which is a strange measure. On the contrary, you would like to obtain new equal bag. You want a very well-distributed samples in the image, and the student says that you should take a careful A careful measure on the origin domain X. So, how to do? And so, up to now we did not consider torsion eigenvalue, but we took much simpler algebraic manipulation. So, I will show you some pictures: what happens if What happens if we take Xer, the set of n by n symmetric matrices with the entries in minus one, one and the function is simply trace the terminal. So the image is in R2. You see the starting set. Set has a higher dimension, and the final set, the image, is just enough to trace the terminal, where the matrix A is symmetric and varies in this class. So I will show you how wrong is Monte Carlo. Look, on the left, this is the plot of the pure Monte Carlo with 1,000... With 1000 points. Well, you can start to guess what happens. In dimension two, dimension two, the degrees of freedom is three because symmetric matrices two by two has degree of freedom three. So is a map from R three into R two and you see Monte Carlo works rather well. Rather well. The left is 1,000. In the middle, you have 10,000 points, and on the right, the full image. You see, Monte Carlo works rather well, but you need 10,000 points. The right picture has been obtained by our method, the method we developed together with Edouard. Together with Edouard Roudet and Benny Bogozel. And to get our picture, we need much, much less samples, some few hundred, if I remember well, 300, only 300 points, but well distributed, well chosen. But now look what happens in dimension 3. This is what happens in dimension 3. In dimension three. In dimension three, the degrees of freedom is six. And you see, you cannot guess anything. You see 1,000 points, the first picture, you don't guess anything. But even 10,000 points, you don't guess anything. And the true picture is on the right. And in dimension four is even worse. Dimension four, the degrees. Dimension 4, the degrees of freedom is 10. So the function is from R10 to R2. And you see 1,000 points. On the left, 10,000 points. In the middle, you don't guess anything. I'm able to guess. And the true picture is on the right. You may say, well, but Monte Carlo maybe requires more points. More points. And so we try the 1 million, 10 to the 6 points. Look what happens. Dimension 2 with 1 million gives a very good result on the left. Dimension 2, 1 million of random points. Very good result. But still in dimension 3, you don't get anything. 1 million points is not precise. Points is not precise. And on the right, in dimension four, one million points is very bad what you get by Monte Carlo. And this is, of course, what people call the course of dimensionality. In fact, in dimension four, the degree of freedom is ten. So our method, on the contrary, is extremely accurate. Extremely accurate, gives the true picture, and I try to describe what is the method. So you start by taking n random point, like in Monte Carlo, but n very small, few hundreds in the previous picture. Then you get the image y1, yn in the image. In the image. At this point, you do the Voronoi installation in the image. And there are computer programs which do this very, very quickly. So this is not a problem. It is very fast. So you construct the Voronoi escalation by taking this Yi, which are the image of random samples Xi. Xi. At this point, this Yi and the Voronoi cells, the Yi in general are not the barycenters of the Voronoi cells Vi. And now instead of Yi, you take the barycenters of the Voronoid escalation. And this is also very fast, very, very fast. So up Very, very fast. So, up to now, we don't waste time, computer time. The only difficulty is now that these battery centers could be out of the image. There is no reason why the battery, the Voronoi sense, yes, but the barycent, when you compute barycenters, you may in principle go out. In principle, go out of the image, and then you have to project these barycenters on the image. And this costs a little bit of computer time. And this is how we project the barycenters into the image. This is the part which costs a little bit of time. And finally, you iterate. And finally, you iterate a procedure. Procedure is Voronoi, barycenters, projection. Then you redo Voronoi, you redo barycenters, you redo projection. After four or five times, what you get is a very well distributed points on the image. The image YR. The image. The image why I are now very well. Can you wrap up in two, three minutes? Yes, yeah, I almost finished. So this is what our procedure, and I should say that to obtain the very nice picture that I have shown, the computer time on a small portable computer. Portable computer is of the order of two minutes. Two minutes, and we get extremely careful pictures by repeating a few times, four or five times, this iteration, which is Boronoi, very centers, and projection, four or five times. And then at the end, we get these points YI now very. Yi now very well distributed. So the image measure is the level measure. So I can stop here. Thanks a lot for your attention. Okay, so you have thanks for the question. Can you hear me? Yeah, go ahead. So I just wanted to. Okay. So I just wanted to. So this is the night's algorithm, but I wanted to mention there are also so-called metropolis-Hastings algorithms for creating Markov chains whose equilibrium distribution is to be found. These sorts of, it's not Monte Carlo, it's Markov chain. There's a version of these algorithms that has been designed for funny shapes. Funny shaped distributions that have corners or are skewed in strange ways. These are called affine invariant methods. It's not the same as your method. Anyways, I just wanted to mention there's an alternative to Monte Carlo. Sure, sure. Yeah, it would be interesting to compare the computer time you need by using the method you say. By using the method you said and the one we use. As I said, we use only a few hundred of points, and the heaviest step is this step number three. And this requires about a couple of minutes of computer time. It would be interesting to compare. According to what Edouard and Beni told me, Told me this method by Voronoid escalation should be faster. This is, I can repeat what they told me that this method by the center is faster. Thank you, Giuseppe. I think you have to wrap up here. We have another talk. The non-play next. Thank you so much. Thank you. Thank you. Thank you so much. 