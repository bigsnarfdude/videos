And so as a brief overview, I'm going to start by giving a very short introduction to compact binary formation and essentially try to establish the problem that we're trying to solve with our approach. Then also talk about the existing approaches that try to achieve the same goal. And I'll give specific examples on how to. And I'll give specific examples and highlight some limitations. And then I'll talk about how we can significantly progress in this issue with simulation-based inference and the promise of simulation-based inference in sort of extracting astrophysics from compact binary populations. Then I'll show you some preliminary results and then conclude with how all of these shares in the context of current and next. Context of current and next detections. Okay, so as many of us know, one of the most fundamental questions of modern astrophysics is that of the formation of compact object binaries, or rather merging compact object binaries, and specifically binary black holes. And formations of binary black holes have remained largely uncertain. So there have been several proposed formation mechanisms or formation scenarios for binary black holes. Formation scenarios for binary black holes that can merge within a Hubble time due to that additional wave emission. Now, I've highlighted two of the major proposed formation channels. So on the right you can see the isolated formation channel where an isolated stellar binary in galactic fields can undergo orbital hardening by means of multiple stages of mass transfer or common envelope or chemical mixing and then leave behind binary black holes. Leave behind binary black holes that are close enough to merge by gravitational wave emission alone within the age of the universe. And then on the other side, I have the dynamical formation channel where in dense environments such as stellar clusters, dynamical interactions can lead to binary level formation and merger within a level type. And these two channels actually are associated with several sub-channels or Sub-channels or specific evolutionary pathways, each of which are characterized by many uncertain parameters. And even more, there are more channels that I've not listed here, for example, binary blacklist formation in AGN disks. So in short, there are many, many proposed formation channels, each with several uncertainties associated with them, which makes modeling the observational signatures of these uncertain formation mechanisms very hard. But then again, what R. But then again, what are the observational signatures of these binary black hole formation? So the main observational signature of various kinds of formation mechanisms is the population level distributions of binary black hole parameters. So the distribution of binary black hole masses, spins, and redshifts are highly sensitive to these formation mechanisms and the astrophysical initial conditions of these formation channels. These protein channels, as well as their branching fractions relative to the overall margin rate. So, as an example of what I mean by population properties, so I've put up this less appealing version of the scalar gravier where we're showing the measured chirp masses of all the compact binaries that we have observed through GWGC3. So, each of these individual gray lines represents the posterior distribution of the chirp mass for a single binary. Mass for a single binary. And then, even by eye, you can see that there are several mass ranges that are more densely populated than the others, which means that these regions of over and under densities can sort of trace out a spectrum of compact binary chair passes, which is the measurement of which is shown in the form of this band over here. And I won't go into the details of how this measurement is performed because we have already heard a lot about it from Ben Stock. So, why do we care about measuring these kinds of Measuring these kinds of distributions. So, this is a distribution of detectable chirp masses, and simulations of binary black hole formation can predict exactly what this distribution looks like. So, if we focus on the top left panel here, that is showing the predicted Cherkmass distribution for five known binary black hole formation channels. So, we have three sub-channels of isolated formation, which are labeled by C. Which are labeled by CE, CHE, and SMT. And then the GC and NSC are dynamical formation subchannels. And so each of these predicts a unique distribution of chirp masses for black holes, which means in principle, by comparing the measurement we saw on the previous slide with this panel, we can, in principle, extract astrophysical measurements on the initial conditions of each of these simulated formation pathways. Formation pathways. And in addition to demonstrating that principle, the other two main takeaways from this plot is that we need to worry about not just chart masses, but many more binary black parameters. So we need to care about mass ratio, spin, redshift, and even more parameters that are not listed on this plot. And so we need to account for correlations between them. And the other takeaway is that each of these shapes are fairly complicated enough. Are fairly complicated enough by themselves. And imagine that the observed binary blater population is a weighted combination of all of these channels. And so the resulting shape can be expected to be very complicated, and hence we have to be careful while modeling them. So there are several approaches towards extracting astrophysical measurements by utilizing the observable population properties of binary black holes. Properties in binary black holes. So, broadly, they can be classified into sort of three methodologies. So, the first one is sort of we construct an astrophysically motivated phenomenological model for these distributions, and then we directly infer it from gravitational wave data, and then the inferred hyperparameters of that model can tell us about, tell us something about some unknown astrophysical quantity that motivated the construction of the model. And then, secondly, we also have And then, secondly, we also have more flexible population inference approaches that reconstruct the shape of the distribution by relying on more on the data than non-astrophysical assumptions. So, these methods are very effective in searching for new physics beyond the scope of the more strongly parameterized models. And then, lastly, we have a direct comparison of the distributions that I showed on the previous slide with the dripulation weight. Previous slide with the reputation weight data. So, using the predicted distributions of population synthesis simulations to construct the population likelihood, and then from that, measuring the astrophysical initial conditions of each simulation. So, I'll give examples of each of these approaches and while highlighting some benefits and limitations of them. So, the first case is the parallel. So, the first case is the parametric population inference, and the example I'll give has to do with the isolated formation channels. So, we know that for binary black holes formed in isolation, we expect there to be an upper mass gap, which due to the apparent stability supernovae leaving behind no remnant above a certain mass range of black holes. And studies have shown that this sort of maximum mass of black holes correlates very strongly with. Correlates very strongly with an astrophysical parameter, which is the carbon-oxygen reaction rate in mass of stars. And so the plot on the left here shows the theoretical relationship between the maximum black hole mass and this carbon oxygen reaction rate. And then on the right represents a phenomenological population model that is designed to capture this trend. So we have a sharp cutoff in the merger rate density above a certain Margarita ray density above a certain mass, which is let which is allowed to vary. So we're essentially modeling this observational feature in the vinyl aqueous population. And then when we infer that from gravitational data, we get measurements of the astrophysical parameter that we wanted to know in the beginning. Okay. So this is all well and good, but well this approach has the advantage of measuring quantities that can directly give us astrophysical measurements. Directly give us astrophysical measurements, it also has a limitation, which becomes apparent when this measurement is compared with existing theoretical measurements, theoretical anastrophysics, sorry, nuclear physics and astrophysical measurements of the same carbon-oxygen reaction, right? And we can see that this measurement is actually in tension with existing measurements. So, what is going on here? One possible explanation is that the population model that we used in the previous slide was not. We used in the previous slide was not flexible enough to capture the full complexity of bio-black formation. There are possibly several other formation channels and other physical effects that we have missed and hence obtain a biased measurement. So parametric population inference can, in principle, steadily give us astrophysical measurements, but they can also be biased. Now, in the second case, we have more flexible models, and I'm going to talk about one such model in the example of a particular physical scenario. Of a particular physical scenario. So, dynamical formation in globular clusters can give us an excess of binary black holes in the 30 to 40 solar mass range. And dynamical formation also predicts a more isotropic spin alignment of the binaries that are formed. So, which means that this excess of 30-40 solar mass can be, in principle, associated with isotropic spin alignment or rather asymmetric pipe-effective. Or rather, asymmetric chi-effective distribution. And by symmetric, I mean symmetric about zero. And so, in order to capture this trend, what we can do is we can model the joint distribution of binary black hole masses and spins using a data-driven model. The model I've chosen here is a very simple data-driven model that is essentially a regular isohistogram. So, we have pinned up the binary black hole parameter space into several bins. Several bins, and then we need further margin rate density regularized by a gaussian process, which essentially reconstructs this distribution. So, here we can see on this plot that binary black holes in the 3 to 40 solar mass bin have a chi-effective distribution that is more symmetric about zero than the others. So, this can be interpreted as a hint of the dynamical origin of the 35 solar. Dynamical origin of the 35 solar mass feature in the black mass spectrum, but as you can see, there are several uncertainties here. And even if there weren't any uncertainties, flexible population models such as these suffered from another problem is that while they're very good for validating the findings of, validating or disvalidating the findings of existing studies, due to the lack of strong model assumptions, it's hard to make quantitative astrophysical interpretations. Astrophysical interpretations from these data-driven constraints. For example, if we use the inference showed on the previous slide to try and measure the branching fraction of dynamically formed black holes as compared to isolated, we get overestimates compared to theoretical predictions. And that is to be expected because these data-driven models are very flexible. They do not have strong restrictions built into them that can separate out. Built into them that can separate out contributions from different formation channels as compared to, say, a mixture model, which would have been able to give us this prediction better. So flexible models are model agnostic, so their results will be much more unbiased than phenomenological models or strongly parameterized models, but they are hard to quantify or interpret astrophysically. And then we have this third approach where these Where these predictions of population synthesis simulations are directly compared to graduation rate data. So these distributions corresponding to each simulation hyperparameter is used to construct a population level likelihood. And then these parameters are varied or sampled across to maximize that likelihood, which directly gives us astrophysically relevant simulation parameters. Simulation parameters and their measurements. So, this is more flexible than parameterized approaches because they directly can incorporate a large number of, in principle, incorporate a large number of formation channels. And also, the distributions that are obtained are coming directly from theory, so we're not using any phenomenological assumptions that might bias the conclusions. So, an example of when this study was conducted, several measurements were obtained. Several measurements were obtained. So, on the left are shown the measurements of the branching fractions of the five formation channels that were simulated. And on the right, we have a table of base factors for two other astrophysical initial conditions. So alpha is the government level of ejection efficiency, and then chi b is the black hole birth spin, which were characteristic to the simulations performed. And then the table gives base factors for each of those choices relative to DWDC2 data. To GWTC2 data, so that is essentially a maximum likelihood measurement of these parameters. So these studies are more flexible than parametric models, and they give direct measurements of astrophysical parameters, but their current implementations suffer from this limitation that we are unable to interpolate across these simulations. And because the simulations are very costly to implement, we will always be left with a coarse grid of initial conditions and then Of initial conditions, and then so we need efficient ways to interpolate across them and while also not while also avoiding any spurious features coming up from the interpolation. So the question that we're trying to answer using the method that we're developing is can we do better? Because the current limitations of the third approach that we just talked about is only going to get worse because as more and more advanced theoretical tools are being developed for Tools are being developed for binary black hole formation simulations. Those tools are even possible to simulate, and we, in principle, want to vary even more astrophysical parameters, such as those characterizing the star formation rate. So, in the end, we'll be left with a posterior simulation and even coarser grids, which will severely limit the scope of the N-program. And a novel way to get around this problem is simulation-based inference. Simulation-based inference. So, the basic idea of simulation-based inference is that we have a bunch of astrophysical parameters which can be used to conduct population synthesis simulations. And the simulations will give us essentially marginal rate densities across various variable level parameters. And those marginal rate densities can be binned or coarse-grade if we need them to. So, the merger rate densities are the little r with a vector, and then essentially they have a one-to-one mapping. Essentially, they have a one-to-one mapping with the astrophysical hybrid parameters. Now, this is from the theoretical side, and then on the data side, we have gravitational rate data, which is essentially parameter estimation of various events, and then sensitivity estimates, and then we can do population inference using some data-driven model to get a measurement or an uncertain measurement of the bin-grade densities. Wind rate density. So, what simulation-based inference essentially wants to do is wants to, it wants to sort of backward model this gravitational wave data to measurements of this astrophysical hydroparameter. And the way we implement this is we sort of construct a neural network regressor that essentially learns the inverse function of population synthesis. So, if the win rates are obtainable as a function. If the wind rates are obtainable as a function of the hyperparameters or the astrophysical parameters, then the regressor can learn the inverse mapping given a set of population synthesis simulations. And then the main advantage is that the placement of these lambdas can be either on a grid or stochastic or a mixture of both, and the network will perform regardless, provided it's complicated enough to capture all the features. To capture all the features. And so, what we want to do is we want to train a regressor on this existing population synthesis simulations. And then, once our regressor is trained, we will take measurements of bin large array densities that are coming from gravitational wave data, and then we'll feed it to the regressor to obtain the posterior distribution of the astrophysical parameters given gravitational wave data. So, that is essentially. So, that is essentially the workflow that we use. And an additional feature of this workflow is that the trained regressor can have some randomness associated with it. So, if we train the same regressor on the same data set multiple times, it will not essentially necessarily give us the exact same answer every time, no matter how much we regularize it. So, in addition to measurement uncertainty, there is some uncertainty associated with the regressor, with the regressor's network. With the regressors network architecture. And we need to quantify that. So the way we quantify it is we sort of use a, instead of just a training and validation split, we use a training, validation, and testing split of our data set. And then once the regression is trained and validated, we sort of estimate the deviations of its predictions from the true value on the test data set, and then density estimate that using analysis flow to estimate. To estimate the uncertainty of the regressor. And as long as that uncertainty is smaller than our measurement uncertainties, this workflow will give us unbiased measurements. So to do a preliminary test of whether this approach works, we took the data set of the five formation channels that I'm showing a couple of slides ago, and then And then we took out one particular simulation from that data set that we did not put in either of the training validation or testing. And from the excluded simulation, we generated a mock catalog of gravitational wave events in O3 sensitivity that consists of 276 events with realistic parameter estimation. And then once the catalog was filled, Once the catalog was constructed, we inferred the bin rate densities in the M1, M2, and Pi factor space for this catalog. And then we fit that into our trained regressor to obtain these posterior distributions of the seven astrophysical parameters that characterize our entire simulated data set. So, here you can see that there is good recovery of all of these parameters and so. And so the method in principle works. We're currently working on tuning the architecture even more to see to further stabilize these results and then to see if they can be even improved even more or if we're already at the optimal architecture. And then once we have established that, we will shift to results from real reputation weight data. To summarize, so we are constructing a backward model for binary-black hole population synthesis that can interpolate across costly simulations of binary black hole formation. There is very high flexibility in choosing our simulation parameters. They can be grids or random values of both. And even if the grids are sparse, we can get very good interpolation at points that we're not. At points that were not included in the training set. And then this is very easily scalable to infer more astrophysical parameters and for including more accurate simulations and more accurate possible simulations. So and the reason it is scalable because all of this is trained on GPUs and we can get like training the model that we used for obtaining the previous plot. Obtaining the previous plot is order of 10 to 5 minutes on a single GPL. So the next steps for this is to, as I mentioned quite a few times already, is to test on, is to train on much more realistic and accurate simulation sets. So I haven't put up any references, but there are much more accurate descriptions of isolated binary stellar evolution that are available now as compared to. That are available now as compared to when the previous slides data sets were released. So, we're going to include that. We're going to vary more astrophysical parameters such as star formation rate. We're going to include additional formation channels into our data set. We're going to include, for example, binary level formation in AGN disks. And then once we have constructed a, once we have augmented our simulation set with a more advanced prescription, Set with more advanced prescriptions of binary blackle formation, we will train our regressor and obtain new and precise astrophysical measurements from the growing gravitational weight catalog. And the nice thing about this investigation is that the gravitational weight catalog is expected to grow in size. So here we can see the growing size of the transient gravitational catalog. Trans-index catalog. We're currently in the O4B era, which is the red line there. So everything on this plot after O3 is predictions from a previous study. So none of this is actual O4 data. But so we're currently here, and you can see that we can expect several orders of magnitude, or at least a few orders of magnitude, increase in the size of these catalogs. And then we will get the, and for hierarchical inference. And for hierarchical inference, the precision of our measurements will continue to increase roughly with a 1 over root n scale. So once we have a realistic data set, realistic simulation set, and a trained regressor, we can expect very precise measurements on astrophysical parameters that have remained unconstrained for a long time. So that's all I have to say. So that's all I have to say. Thank you for your time. Do you have any questions? Hi, thank you so much for the very nice talk. Can you go back to this fine where you were talking about a train holiday text?