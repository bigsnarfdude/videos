They define uses linearization or linear approximation from a billiard functor calculus. The problem is that from the perspective of Cartesian differential categories, the Bjork construction is actually backwards. And what I mean by that is because in any Cartesian differential category, it's always possible to define the notion of a linear map and to linearize a map using the differential combinator. Differential combinator. However, Vior uses, constructs their differential combinator using an already established notion of linear map and linearization. So this, for a long time, this made Robin and me very confused. But thanks to Christine, Brenda, and Sarah's help at the CMS summer meeting back in 2018, we set out to understand what was going on. So essentially, the story today is that we're reverse engineering Bjorth's construction by abstracting the notion of By abstracting the notion of linear approximation from functor calculus. So, to that end, we introduce linearizing combinators on a Cartesian left data category. And then I'll show that every Cartesian differential category comes equipped with a canonical linearizing combinator, which is obtained by differentiating at zero. And conversely, that every differential combinator is actually constructed a la Biort when one has a system of partial linearizing combinators. Linearizing combinators. And then, for those of you that were there for Rick's talk and maybe know a bit more about the monoidal side of the story, well, this correspondence between linearizing combinators and differential combinators is captured on the monoidal side via the bijective correspondence between deriving transformations and coderelctions. And so, the reference for this talk is this paper by Robin and me, just called Linearizing Combinators. So, I'll just quickly go over this definition. I won't go into too much detail. If you want more detail, you can check out the recording of my tutorial talk from Monday. So, the base structure that we're working with are Cartesian left-additive categories. So, left additive category is essentially just the category where every hump sets a cumulative monitor. So, I can add maps together and I have zero maps. So, it's that composite preserves the additive structure in the following sense here. Here. However, it's important to note that it doesn't preserve it in the other sense like it would in normal additive enrichment. But maps that do preserve the additive structure are called additive maps. A Cartesian left additive category, which I'll just call a clack, is a left additive category with finite products just so the projection maps are additive. A Cartesian differential category, which I'll just call a CDC, is a clack. Call a CDC is a clack equipped with a differential combinator D, which takes in a map from A to B and then spits out a map across A to B. And so to help with the axioms and throughout the talk, I'm going to use the term logic for differential for C D C's as follows. So I'm going to use sort of element-wise notation. And this, and then a differential combinator needs to satisfy seven axioms. So I'm not going to go over them, but I write them out here for those. Go over them, but I write them out here for those of you that want to look at the slides later on. The main example of a CDC and the main example that I'll use throughout my talk is the category of smooth functions. So this is the category whose objects are the Euclidean vector spaces Rn and whose max are smooth functions Rn between the Euclidean spaces. Then smooth is a C V C where the differential combinator is just defined as the Combinator is just defined as the directional derivative from calculus. So if you have a smooth function from rn to rn, which is in fact a tuple, then the differential combinator is given by the sum of the partial derivatives of each of the little fi's. In any CDC, there's a natural notion of linear map. So a map from A to B is said to be delinear. So I'm going to use D. I'm going to use d to help us differentiate between the various notions of linearity that we'll see in this talk. So, a map is said to be delinear if essentially its derivative is just the map F. So, what that means is that D of F is equal to F precomposed with the projection. So I've written in the term logic here. In the category of smooth functions, it turns out that something is delinear if and only if it is R linear in the classical sense. Linear in the classical sense. So now we like to have the ability to linearize maps in a CVC. So for a simple example, just take any smooth functions from r to r. Then its linearization is the best r linear function which is closest to f. And this is given by the first degree term of its Maclaurin series expansion, which is, or equivalently, the Taylor series expansion at zero. In terms of the differential components, In terms of the differential combinator, what this ends up being is precisely the directional derivative of f evaluated at zero in its first argument. So what you get is precisely this, f prime at zero times x. And this is indeed an R linear map. And the neat thing is that this construction can be done in any Cartesian differential category, and we can use this to abstract the notion of a linearizing combinator. And the axioms are going to parallel those of a differential combinator. Parallel those of a differential combinator. So, a linearizing combinator on a clack is a combinator which takes in any map from A to B and spits out a map of the same type A to B. And again, I'm going to use the following term logic to write out my axioms because it'll be easier to understand them this way. So, the first axiom says that the linearized So the first axiom says that the linearization of the sum of maps is equal to the sum of the linearizations, and that the linearization of zero is equal to zero. The second axiom is that the linearization of any map is additive. The third axiom tells me what the linearization of the identities and the projections are. And essentially, they're themselves. So the linearization of the identity map is the identity map. And the linearization of the projection. And the linearization of the projection map is that projection map. The fourth axiom says that the linearization of a pairing of maps is the pair of the linearizations. And the fifth axiom doesn't look very nice, but it's essentially the chain rule for linearization. And the first thing I want to point out is that there's this extra F0 part in the linearization of G. So essentially what's happening there is that it's a value. what's happening there is that it's evaluating the failure of f to be zero, of f0 to be equal to zero. And if you go through and read that Bjork paper on the abelian functor calculus model, this is actually a crucial part. And for those of you that may be wondering, well, why can't you apply the chain rule onto this right-hand side again? But it turns out that you'll just end up, you'll just end up where you started. So there's no really simplification of this. It turns out, however, that if f and g if f However, that if f and g, if f or g is additive or simply reduced, then the linear, the chain rule can be simplified even further. But in general, this f0 here is stuck. And then the sixth axiom says that linearization is idempotent. So if you linearize something twice, it's the same thing as linearizing just once. And that's it. A linearizing combinator on a clack is just a combinator L, which satisfies L1 to L6. Satisfies L1 to L6. Now, for those of you that are familiar with CDCs, you'll point to the fact that a CDC actually has seven axioms, while a linearizing combinator has six. And that's a really good observation. And specifically, the last axiom, CD7, is not present. But that'll come up when we talk about partial linearization. So just like the differential combinator, there's also a notion of being linear with respect to this linearizing combination. linear with respect to this linearizing combinator. So a map is said to be linear if when I linearize it it does nothing. So by L6 every map for every map F, its linearization is L linear. So our first result is that every Cartesian differential category admits a linearizing combinator where I just evaluated the differential of f at zero. So I've written it here term logic. Not only that, Not only that, a map is delinear if and only if it's L linear. And furthermore, for every map F, so what that means is that for every map F, its linearization is linear in the differential sense. So for smooth, what this ends up, you can compute it that for any smooth function, the linearization of f is just equal to the sum of the partial derivatives evaluated at zero. evaluated at zero. So here's an example. If f xy, if f is equal to the function e to the x times cos y, then you can work out its derivative to be this. Then evaluating in the first two arguments, so replace x and y by zero, you see that the linearization of f is just equal, ends up just being equal to x. So now for the other direction, we'd like to construct different Direction, we'd like to construct differential combinators from linearizing combinators. So, for this, let's consider the classic limit definition. So, here's the very famous limit definition of the derivative. If we evaluate x at zero above, we obtain the limit definition for our linearization. Now, for a fixed x, define g of x to be the smooth function where if I evaluate g of x at y, it's just equal to f. equal to f at x plus y. Then if you work this out, you'll see that it turns out that d of f is just equal to the linearization of g of x evaluated at y. So what this says is that the function is that the derivative of f is actually just the linearization of f in the variable, oops, that's a typo, that should be in the variable x. And this is And this is precisely how Bjort defines the differential combinator. And in fact, every differential combinator in a CDC can be obtained in this fashion. So to do this, we have to require the notion of partial linearization. And here's where things get a bit tricky. While it's always possible to define partial differentiation from total differentiation, in general, it's not. In general, it's not necessarily possible to define partial linearization from total linearization. As such, we need to define a separate notion of partial linearization, or in category theory terms, the notion of linearization in context. So, first, we need to talk about what it means to be in context. From the categorical perspective, a map is in context if it's a map in the simple sex category. Category. So very, just to review the simplest category. So if I start with a category of finite products, then for each object C, the simple slice over C is a category whose has the same objects as my base category, but where a map from A to B in the simple slice is actually a map from C cross A to B. And we say that F is in contact C. And then the identities are the projections, and then the composition is given us. The composition is given us here. More importantly, for every map in the base category, there is a substitution functor, which is defined as just modifying the context. And as such, the simplest lice categories sort of together form a vibration. And these are the fibers of said that vibration. So, a system of linearizing combinators on a clack is a family of linearizing combinators. is a family of linearizing combinators. So, which takes in, so for every object C, I have a linearization combinator L of C, which takes in a map C cross A to B and spits out another, a new map C cross A to B. And what this L of C of F should, you should think of it as, is the partial linearization of F where I've only linearized in this second in this term A and kept C sort of in a constant. And so I write it out in the term logic here. We see that C. Here, we see that C is an unbounded variable and X is the linearized variable as bounded. And L of C has to be a linearizing combinator over each of these simple slice. So I've written out the axioms here if you want to check out my slides later. And such that two extra axioms hold. So the first axiom is the symmetry rule, which essentially says that if you linearize something in terms of x, and then you linearize something in terms of x. And then you linearize something in terms of y, then it's the same thing as linearizing in terms of y first, and then linearizing in terms of x. And then the second axiom is the context substitution, which essentially says that any modification you do to the context or any modification you do to the variable that isn't being linearized, you can do it before or after the linearization. And this also tells you that essentially all the linearizing combinators across all the simple slices are the same. Across all the simple slices are the same. So, yeah, so a system of linearizing combinators is a linearizing combinator on all the simple slices plus those two extra axioms, L7 and L8. And the cool thing is, is because the simple slices over the terminal object is isomorphic to the base category, having a system of linearizing combinators implies that your base category has a linearizing combinator as well. So here's an example of the differentiate between total linearization and partial linearization. So take this polynomial. If you do the total linearization of f, so linearizing jointly in x and y, what you have to obtain is a linear map linear in x and y at the same time. If you linearize, if you do take the partial linearization only in terms of x, what you get is Of x, what you get is all terms of f where x was of degree y, and so this is linear in x. And you can do the same thing, but only take the partial linearization in terms of y. And then if you do the partial linearization of, say, the y, if you take the partial linearization of the partial in x and then do it in y, it's the same thing as doing. It in y is the same thing as doing in y first, then x. So, this is this last step here is an example of that axiom L7. And this time, the remaining thing here is bilinear in x and y. So, hopefully, that illustrates the difference between partial linearization and total linearization. So, how do I obtain a system of linearizing combinator from the differential combinator? Well, the cool thing is. Well, the cool thing is, as I've mentioned before, it's always possible to define partial linearization from total differentiation. So, how do you do that? Well, essentially, the partial derivative is just defined as evaluating something at zero. And not only that, this partial derivative is actually a differential combinator on the simple slice. So, because it's a differential combinator on the simple slice, we automatically obtain a linearizing combinator on Linearizing combinator on the simple slice as well. And it follows from the remaining differential axioms that L7 and L8 are also satisfied. But I wanted to do the converse. So how do I obtain a differential combinator from a system of linearizing combinators? So remember, it's obtained by linearizing f, but where I first precompose by summing x plus y. By summing x plus y. Luckily, in any clack, I can define the sum map as just the sum of the projection maps pi 0 and pi 1. So then, so here's the proposition, every clack with a system of linearizing combinators is a CDC where the differential combinator is simply defined as linearizing the map F, but where I've where I precompose by the sum. And I think there's a type of this X. And I think there's a typo this x should be on this side. It should be x plus a, not, but it doesn't really matter since addition is commutative. But the point is that I'm linearizing one of the arguments in the sum. And the main result is that for a Cartesian left out of category, there's a bijective correspondence between differential combinators and a system of linearizing combinators. So what that means is that a C D C can be is precisely. Can be is precisely a clack equipped with a system of linearizing combinators. Well, there is an example that something that has a total linearization but doesn't have partial linearization. And essentially, those are the category, it's the category of functions whose functions are only C1 differentiable. So this has a total linearization combinator. Total linearization combinator, which is defined the same when it's smooth as it is in smooth, but it have partial linearization because if it did, then it would also then have a derivative, a differential combinator. But if we look at the function, say fx e f x is equal to x to the three halves, well, this is a d1 function. But if partial linearization was possible, partial linearization was possible then you would end up with you'd end up with the map three over two x plus y uh square the square root of x plus y but this linearization is not defined when x plus y is less than zero so this is not a map in c1 and therefore c1 doesn't have a system of linearizing combinators since i'm running out of time i'll just say this it is possible to define partial linearization Partial linearization from total linearization, but this is only possible in the Cartesian close case. And what you do is that from the total linearization, the way you obtain partial linearization is essentially by currying and uncurrying and then linearizing the curry of a map F. And then we also obtain this equivalence for Cartesian closed differential categories. So, just to wrap up, again, the main purpose of this project was to establish an alternative axiomatization for CDCs using a system of linearizing combinators. However, the weakness of this axiomatization, of course, can't be overlooked because you have to assume partial linearization, and that is a pretty significant requirement. However, what this points to is that since linearization can exist for functions which aren't infinitely differentiable, this suggests that maybe. Differentiable, this suggests that maybe there is a place for linearization in the non-in categorical approaches to non-smooth analysis. One thing that we haven't checked yet is if the Bjort model is Cartesian closed, and so can it be defined using this exponential linearization. And then because this correspondence is captured on the Cartesian side, maybe and in the monoidal side, maybe there's an equivalent notion for say tangent categories or differential restriction categories. Or differential restriction categories, et cetera. And that's it. Hope you enjoyed. Thanks for listening. And merci. Thank you, JS. All right, so.