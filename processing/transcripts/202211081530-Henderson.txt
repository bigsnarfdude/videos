All right, well thank you everyone for being here for the second talk of the afternoon session. We're happy to have Chris Anderson from the University Farm Center who will talk about two results on the local opposingness of kinetic equations. All right. Thanks Nestor. So it's the second talk. It's also the last talk. So I'll take this responsibility very seriously. Okay, everyone has to eat dinner and go for hikes. So I'm kind of in a kind of limbo in between Stan and Andre here. So I'm giving a talk that kind of fits in with both of their talks. But what I'm going to try to do is kind of excise sort of two interesting pieces of them and focus on that so you can kind of see the kind of acute clue for each. So that's my hope at least. So it's joint with Wen On Wong, who's a postdoc at Arizona. He's on the job market this year. So, you know, advertising for him. For him. And so, what I want to talk about is, I want to talk about Landau and Boltzmann, which is why I have this vague kinetic equations up here. But so, I'll kind of give one result for each. That's why it's the two results. One existence and one uniqueness. Now, if I want to totally, if I want to be general and then kind of lie to you, then what I'm going to tell you is that the equations I'm looking at, okay, they satisfy this. This. And okay, we'll do this in, say, X in T3 and V in R3. Okay, and now I think what I want to do is write one more line and box this and then never touch the top left corner again. And when I say I'm going to lie to you, what I'm going to lie to you is I'm going to tell you that Q takes this form, okay, where I'm going to think of it as being a convex. I'm going to think of it as being a convolution of G with some kernel, campaign plus 2s, dw. And then a fractional applaution. And I know I'm committing a sin by not putting two minus signs, but come on. We all know what I mean, right? Okay. I think Andre is probably in the back, terrified when I'm learning math. Sorry, Andre. Okay, so I'm going to. Okay, so I'm going to say that it kind of looks like this. Here, s is going to be between 0 and 1. Maybe I'll do my box now. When s is 1, I'm thinking about this as being len now. When s is less than 1, I'm thinking about this as being Boltzmann. And I'm wanting in many ways, kind of. In many ways, kind of the most obvious is I'm completely suppressing all of the anisotropy of the operator. Okay, so but of course if we want to get into the crazy details, then the talk will be like three hours long and no one will be happy, least of all me. So let's pretend that this is true. And I'm also kind of suppressing how complicated it is to kind of read Boltzmann and think about it in this form. But for the sake of argument today, let's just sort of pretend that this is the truth. Just sort of pretend that this is the truth. So, s is going to be my parameter for how high of order of my operator. And then gamma is going to be some other physical parameter. And of course, okay, it can take all sorts of values. Today I'm just going to focus on gamma being negative. Of course, you can take gamma positive, but that's sort of another talk. And then if gamma is down to negative 3, this should really just be g, but again, let's not worry about the technical. Let's not worry about the technical details. So I think a lot of you are pretty familiar with this, but let's kind of just remind ourselves of what's so complicated here. Okay? So the first thing is: okay, you want to worry about blow-up. And why should you worry about blow-up? Well, if I kind of squint and ignore a bunch of the terms. Or a bunch of the terms. I see a time derivative, I see some stuff, I see some stuff, and then I see something that kind of looks like f squared. And of course, if I ignore all this stuff, I get f dot equals f squared, and that blows up in finite time. So if I'm going to save myself in some way, I better use kind of the regularizing effect that comes from these things. And then the second thing is: okay, now we know that the regularizing effect We know that the regularizing effect is important. But now we have ellipticity problems. Okay, so the first one is: okay, there's no ellipticity in X. This isn't really as much of a problem. Okay, you have to use kind of some hypoellipticity to say that somehow you get smoothing in V, and that smoothing in V by the transport operator gets tossed over to be smoothing in X. But certainly it comes. thing in X. But certainly it complicates things. But the other thing is that you could have vacuum regions. Okay? So this is physical locations X, where for every velocity v, f is 0. So if that's the case, then okay, if this is f, this coefficient is 0, and I just don't even see my fractional employee. So if you pick So, if you pay very close attention to Sam's talk, vacuum regions, they fill in. But the problem is that if you fill in, then at some small time t, you have a very small coefficient here. And so any estimates you get are going to depend very poorly on that small time. And this is going to be important in a second. Second. So now, okay, with this blow-up, this causes horrible problems. You might at first dream big and say, okay, what do I want to do? I'd really like to show global welfare in this, right? But okay, there's no way this is going to work, right? This is way too hard. Is way too hard. So, one way you can see that it's too hard, which people have talked about a few times at the conference so far, is that there are these hydrodynamic limits that connect these kinetic equations with fluid equations. The fluid equations are either known not to be globally well posed, or it's an open problem if they're globally well-posed. And for Navi-Stokes, you win a million dollars if you get it. So, if you want to do it for Boltzmann, it's like you get all the difficulty of Navi-Stokes, but you don't get the million dollars at the end, anyways. Million at the end, anyways. So, what's the point, right? So, okay, so this isn't going to happen. But a kind of more manageable goal or more tractable problem is, okay, one is local well-posedness. And then understanding kind of minimal conditions. Never mind wall okay now the uh to kind of these things are really tied closely together because what do you want to say? You want to say, okay, I have some preliminary local pose in this result that lets me get a solution for Result that lets me get a solution for some amount of time. And then, based on whatever conditions this is, if I can get enough regularity to say, actually, at some time, say, order one, I'm actually still in the class that my local opposedness applies to, then I can reapply my local opposeness and keep taking me forward, right? So, somehow getting a local opposedness result that is in the right class to match with the sort of regularity estimates lets you do this kind of You do this, kind of find nice conditions that let you go forward in time. Okay? So, again, Stan really talked about this a lot. So, what I want to do is I want to show you the local pose in this piece that is the stepping here. So, the first part of the talk will be existence for Boltzmann. And And importantly, because of this issue of getting a local business in the right class to match with the regularity estimates, we want existence for just polynomially decaying initial data. Okay? Because for some technical reasons, okay, so this is also for Boltzmann. For some technical reasons, you know, sort of, Maxwellians decay like a Gaussian, but for some technical reasons we can't kind of match up the nice regularity estimates that give us this kind of, let us propagate forward this kind of Gaussian decay. We can do it for polynomially decaying data. So having a local opposence in a polynomially decaying class is important. Class is important. Of course, the regularity estimates are all the Clono Moho serial on there, and we'll use Sylvestria stuff. So that's going to be the first half of my talk. And then the second half is going to be a little bit of a discussion on uniqueness. And there I'll talk about Landau and some kind of shader estimates. That are general for kinetic fulker plot. Okay. So let's talk about existence for multiple. Um so So thinking about existence for Boltzmann goes back at least to the early 2000s, and there were a lot of papers written by this group that people call Amuxi. I don't know, I was pronouncing Amuksi, but then during Bob's talk, he said A-M-U-X-Y, and I wondered if I made up my own name for this group. Well, I don't know, I just made that up. Okay. I just read the letters out loud. Okay. Okay. So this is in the 2000s, there's a bunch of papers. Sorry, does someone have an expert opinion on this? It's a proper, so that's like an amazing name. Yeah, yeah, it's great. They really arrange themselves nicely in the process. They call themselves a mootie? Okay. Absolutely potentially a mootie. So it's Alex Okram, Morimoto, Yukai, Shu, and Yang. And so they did some very nice local opposing results in the early. Posnist results in the early 2000s. The issue is that all of their results start off by assuming that you have Gaussian decay in some kind of HK space. So I'm going to be really vague on what the space is, but okay, if they do it on all of R3, they have this uniformly local space, but let's not get into the details of it. And why does Gaussian work so well? So well. So Bob and Stan both in their talks showed the trick. So basically what you can do is you can make a change of function where you take f and instead you look at f times a Gaussian, where the Gaussian is becoming sort of narrower in time, or wider in time. It changes in time, and that gives you kind of a moment gain term that lets you offset all the moment loss issues. Because if you look over here, let's say gamma plus 2s. Here, let's say gamma plus 2s is positive, then this convolution gives you something that looks like v to the gamma plus 2s. And you see that kind of there's some issue with weights. Okay? And so I just want to stress this because this doesn't work at all for this doesn't work for polynomial null decay. Okay? their trick, I guess. Okay? Their trick, I guess. Their trick doesn't work. Okay? And let me also point out that they always make this assumption that s is between 0 and 1 half, and gamma plus 2s is greater than negative 3 halves. And I'll talk about these two conditions in just a second. So then some time went by, and in 2015, And in 2015, there's a paper by Morimoto and Yang. So the M and the Y from this group. And what they showed is that actually you can do it with polynomial initial data in some kind of space. So if you don't know what the Japanese bracket is, just pretend it's an absolute value. It's just like a regularized absolute value. A regularized absolute value. So there's some condition on L, there's some condition on K, I guess K has to be bigger than 6. Here they have a complicated condition on K. So if they have this, then they get existence. But they have this same kind of condition. So they have S, which means 0 and 1 half. And then they have gamma greater than negative 3 halves. So you see, they're quite. You see, they're quite far from getting all the way down to gamma equals negative 3. And they're also quite far from getting all the way up to s equals 1. Is that like a short reason why you get stuff they can go up? Okay. Yeah, so for the gamma condition, there's a very short reason. I'll give it in a second. For S, let me convince you of something, right? So Y is bigger S part. So why is bigger S harder? Okay. I think that there's certainly a feeling that Boltzmann is harder than Landau. And for some things, it's certainly true because it's a way more complicated collision operator. But somehow bigger S causes some issues for the following reason. We have these vacuum regions, right? And they fill in, but they don't. And they fill in, but they don't the and they fill in sort of instantaneously, but like the lower bound for a small t is very small. So that gives you uh horrible estimates in terms of their dependence on t. So because of the vacuum region, you kind of you can't use smoothing at all. Okay, because any estimate you try to get will have some coefficient that will look like 1 over t to some huge power or something like that. So if you can't use smoothing, then the only way you can get estimates is you have to propagate estimates hyperbolically. And what I mean by hyperbolically, I mean if you start with L2, you can push it forward to L2, but you can't start with L2. Forward to L2, but you can't start with L2 and push it forward to H1. Something like that. Okay? And why does that cause issues with S? Well, if you're working in L2, if I take a quarter derivative of something, it's not as bad as if I try to take a full two derivatives of something. Okay? The AC was on and turned off, and it was. And turned it off. I know it's. You're really quiet up here, also. It psyches me out. Okay, so that's why it's sort of getting a bigger S is harder. And I'll show you in a few minutes precisely a case where the S between 0 and 1 half proof is like completely trivial. Okay? Okay, you know, it's these things, if you look at it the right way, it's trivial. And if you have the the the benefit of seeing, you know, five extra years of papers uh in between Years of papers in between. So there's that. And then, okay, now why gamma plus 2s to be bigger than negative 3 halves? Okay, so I don't think I'm saying anything controversial when I say, okay, it's nice to work in L2. All right. In particular, the Fourier transform is really nice in L2, right? It's an isometry. You can do everything. In Fourier space, it has nice duality properties, blah, blah, blah. So now let's try to look at what happens if you're working in L2. If you work in L2 and you want to bound on this term here by Cauchy Schwarz. By Cauchy Schwarz, right, what do you get? You say that the integral of f of v minus w, w to the gamma plus 2s dw, that this thing will be bounded above, okay, by the L2 norm of f. But then we have the integral of gamma to the Of gamma to the power 2 times gamma plus 2s dw 1/2. Now, don't worry about infinity. Just pretend everything's okay at infinity, because what do you do? You like sneak in weights here to take care of infinity, so it's really not an issue. And you say, okay, well, this thing is a horrible singularity at the origin, right? And when is this singularity integrable? At the origin, precisely if... Precisely if 2 times gamma plus 2s is bigger than negative your dimension. And now we see gamma plus 2s has to be bigger than negative 3s. So that's why they have this condition. So you're saying that L2 norm of f controls the diffusivity constant from above. If you have this condition. Right. Yeah. Here I'm also kind of ignoring x, but this is just a kind of proof of constant. With an implied constant equal to infinity, but yeah. To equal to infinity, but yeah, original less. Okay, yeah, yeah, yeah. Okay. So when do we use the human rights? Here's S. Let me draw gamma. We have negative 3. This is, let's see. This is, let's see. Negative 2, negative 1, 1 half, 1. So here's our parameter space. And the Morimoto Yong estimate gets you this top left corner. Okay. The next result is Is myself, Stan, and Andre, which was published, I think, in 2020, if I had to guess. And here we do a polynomial, initial data, okay, k is slightly larger, slightly smaller, sorry, but still large. And we can take any s but we still have this in the But we still have this annoying condition on gamma plus 2s. So this result gets us, I'm sorry, my square is wrong here, should go all the way down here. This gets us to here. And then the sort of insight is that L2 is not a great space. Well, L2 is nice for some things, but there are some advantages to working in L infinity. So, okay, work in L2 intersect L infinity with weights. Really, this should be H. Really, this should be hk, I guess. Intersect L infinity. And why is L infinity nice? Well, L infinity bounds are easy. Why? Because of this barrier argument that Stan gave in his talk. You can just say any polynomial, sorry, one over any polynomial is actually a super solution. So if you start with some sort of L infinity. Start with some sort of L infinity polynomial decay, then that automatically propagates forward. And L infinity bounds are useful for bounding the coefficients. Okay? And then taking it a little bit further, myself and Waynon, I think this was published this year, proved two things. Prove two things. The first is the same as before. So HST, but no condition on gamma plus 2s. So that gets us all the way down to the full parameter space. So in particular, we fill in this bottom left corner. And, okay. I'm not excited about this. I'm not excited about this result, but it makes a terrible talk because it's like 30 pages of sitting and crunching through the collision operator and getting estimates and improving previous estimates and tweaking things and blah, blah, blah. And like, you guys will hate every moment of it. I'll hate every moment of it. So let's just agree to stop talking about it, okay? What I want to talk about instead is a very cute second result from this. A very cute second result from this paper, which is that, okay, if you're willing to accept that I just work with s between 0 and 1/2, then you have existence and uniqueness, but let's just talk about existence for the moment. If your initial data is, say, in just w1 infinity, okay? And why do I say that the q result? A cute result because the entire proof is two pages long. It's completely, completely trivial by just using barriers instead of L2 estimates. So let me tell you the entire proof. Okay? I'm not going to fit it here, but I'm pretty sure I can get it before we get down to here. So, okay, so you want to prove local opposedness. At PDE, we basically have a rule. In PDE, we basically have a religious belief that if you get a priori estimates, then you get local welcomes in this, right? So let's stick to that and we'll just do our a priori estimates. So, okay, we're in W1 infinity, and actually you can do W2, W3, whatever. Take as many derivatives as you want, as long as there's at least one. Okay, so you need an L infinity bound, right? That's the first thing you need. But Stan already gave this argument. Okay, and what is it? It's just the offset. What is it? It's just the observation that v to the minus l is a barrier. So if you start below it, you stay below it forever, and then you're done. Okay, you have to put like an exponentially growing thing, but it doesn't really matter. So you have an L infinity bound, okay? So now we need an L infinity bound on the derivative of F. So I'm going to be vague about what D is. If you like, pretend it's all of the It's all of the total derivative in both x and d. But we're going to do is we're going to write down an equation for it and pretend it's a scalar quantity. And Bob was trying to trick me, place that there two days ago. So it's a vector value quantity. I promise you there's absolutely no issue with adapting the proof I'm going to tell you the fact that. The proof I'm going to tell you, the fact that it's vector value. So let's just pretend it's scalar value for a second. So let's call G to be DF so we can forget that it's a vector. And then we say, okay, well, what does G satisfy? Well, I want this one to be G F. No, I want it the other way. Okay, it doesn't matter. And then, okay, technically, And then, okay, technically there's also potentially a kind of another g term here because this doesn't commute with the v dot grad, because if the v part of this derivative hits the v, but then it just swaps for a grad x, so it really doesn't do anything. But we'll put a plus g there so we can be kind of honest. And now we say, okay, go to a maximum. Go to a maximum of g. Well, at a maximum, you get, okay. Well, at a maximum, you get, okay, m dot. That'll be m dot. At a maximum, that's zero. Okay, I'm going to put a less than or equal to, because technically you have to do a less than or equal to whatever. And then here, okay, this might be annoying, so let's leave it. Here, if we forget about the lower order terms, which just give us an m squared term, we say if you're at a maximum, this thing has a good sign, right? So if you're at a maximum, this thing has a good sign. thing has a good sign, which means I can just replace that term with a zero for the highest order term and then an m squared for the lowest order term. And then I guess I have this plus g, so I should be honest and put another m up to some constants. And now I say, well, I'm really worried about this term. But let's look at the highest order part of this. The highest order part is going to be an integral of g v dot w, w to the gamma. w to the gamma plus 2s dw. But this thing is going to be bounded by m up to weights. Chris, can you explain again why it is zero, this qgf? I cannot get that. Oh, it's negative or no, qfg, qfg is negative. qfg. Right, so so uh you're at a maximum of g and at a maximum of g the Laplacian or the fractional Laplacian gives you a sign. Uh yeah, okay, okay, yeah. So uh there's that part, and then there's a Laplacian s of f. Well, s is between 0 and 1 half, right? So this is less than a derivative of f. So I can certainly bound it above by m up to, you know, also putting in the l infinity. I can interpolate between the l infinity and the w1 infinity lines. So in all of this, I end So, in all of this, I end up with m dot is less than, say, m times 1 plus m, which tells me that m is bounded, at least for a short time. And then I have, and then I'm done. So if you if you look at this, I really did like zero work here. I mean, you know, this is all just observe, observe, observe, observe, and then you get an OD, and then you're finished. Okay. And then you're finished. Okay. But you use also the fact that sharp is below one-half because saying that W1 infinity estimates are enough. Exactly. So there's no way to push this estimate past s equals one half. Maybe you could get the edge case, I'm doubtful, but maybe you could. But you certainly can't go over. And this is why I say smaller s is actually easier in some ways. Because it's kind of less brutal to the hyperbolic propagation of estimates. Propagation of estimates. What about the fact that g is not a vector? Look at the maximum or the norm of the g? Yeah, so what do you do? You say, okay, I'm going to take all the partial derivatives, and I'll look at the maximum of all of them, and one of them gets realized first. So you go to at time t, one of them gets realized, that's the equation I analyze. And then you say, okay, well, something like this grade, you know, maybe I worry about, instead of doing d, I just take the one partial. D, I just take the one partial that realizes the maximum. That does give me some problems because this is in all of the v variables as opposed to the particular partial that I take. But it doesn't matter because I know that the one that I take is realizing the maximum of all of them. So, you know. It's just the same kind of trick, right? You push something forward until it's violated, and then you get. Another question. So why couldn't you take your initial data at W to infinity and then take S greater than one half? So why couldn't you then triple H? Ah, because so you take two derivatives of the equation, and unfortunately you get in a situation where you get one derivative on here and one derivative on here. Right? And then you have one derivative on here, but then you apply also a Laplacian to it. So now you're over two derivatives. And that one doesn't have a good sign. The only thing that saves you here is that the bad term, the term that's too high of order, has a good sign. But when you go to higher, when you try to do higher order derivative, when you try to do larger s and then higher order spaces, you end up with these cross terms that cause problems. So the only don't cause problems when s is small. Okie dokie, so so this is existence. So now let's talk about uh uniqueness. And we'll talk about the the Lambda equation. Okay, so let me say that there's a kind of a very close relationship between uniqueness and regularity estimates. And in particular, what I'm going to talk about are shower estimates. Are shatter estimates. Shatter estimates say that if your coefficients have some holder regularity, then your solution has kind of an appropriate number of derivatives more holder regularity. So for lambda, where s is equal to 1, that tells you that if you have c alpha holder regularity of your coefficients, then you should be in C2 alpha, at least in V, and then, okay, there's some C. At least in V, and then, okay, there's some scaling to worry about what you get in T and X. And if you don't believe me in this, that's okay, because luckily Andre will discuss this in more detail. So I'm going to, as with the last part of the talk, I'm going to try to overlap as little as possible. But what I want to point out is that if you want stronger uniqueness, right, so when I say stronger uniqueness, what I mean is like, When I say stronger uniqueness, what I mean is like uniqueness amongst solutions in a weaker class, amongst a larger class of solutions. Then, what you need are regularity estimates with fewer conditions. Okay. So, kind of the less, the less my less conditions in my shadow estimates, the wider class I can do. The wider class, I get some kind of uniqueness. And so let's look at our equation. And what do we notice right away? We notice that our coefficients get v regularity for free. For free. Okay? If I look over here, this is a convolution. So this is F and F, I have a convolution of F with some kernel. So if I want a little bit of holder regularity, I just get it by putting that on the kernel, right? It's like if you want a derivative, you just put it on the thing in the convolution that's smoother. Well, this is always kind of smooth enough. So if I get this regularity for free, then what I might do is that's the same thing. For free, then what I might dream of is I might say maybe I can prove shower estimates which require only V regularity. Okay? So instead of asking for regularity, holder regularity in all variables, I say maybe I can just get it, or hold irregularity in just the velocity. In just the velocity variables. And this isn't just some crazy dream. There's this very nice paper that goes back to the 60s by Brand, who has like five papers on math scientists. And I think because he wasn't a famous guy, this paper is kind of somewhat overlooked. I think it's gotten 20 or 30 citations or something. But over 60 years, there's not very much. But over 60 years, that's not very much. And what he proves is: let's say you take, I think I'm trying to over-optimize, so actually, let me just put the conditions here. Let's say you take a parabolic equation. Okay, in non-divergence form, And you say, okay, I want my A to not be holder in time, but to be holder only in V. Okay? Similarly with G, but let's forget about G. A is really where we want to focus. Then if you have this, then what you get is that F in some appropriate C2 alpha space will be bounded above in terms of its L infinity. In terms of its L infinity norm and the regularity of G. Okay? So basically, if you want to prove, if you want shadow estimates for fairball equations, you actually don't care about polar regularity and time at all. You need measurability or something like that, but you don't need a measurability and boundedness. And I should say there's a lower lepticity condition here as well that I'm just kind of hiding under the row. Left-hand side is in V, right? Left-hand side is in V, right? The left-hand side? You're saying this thing? Yeah. I'm being deliberately vague. Okay, it's C2 plus alpha in V. It's C, it's L infinity in T, and then there was a follow-up paper 10 years later that showed that actually you get regularity in T as well, if I remember correctly. Despite the proefficience, despite any regularity. Any regularity, I guess, or hold it. I'm sorry? But like Lipschitz stops, sort of, right? What do you mean Lipschitz? F? The D squared V of F will be like truly C2 alpha, the parabolic C2 alpha space will be truly that. The time derivative will be C alpha in V and just bounded in V. But this isn't. But this isn't so much what I want to focus on. Okay. So this set, okay, so you see this, and you say, okay, I don't need to have regularity in everything. And then kind of my hope is like, okay, maybe I think that my transport term for the kinetic case is kind of analogous to my time derivative in the parabolic case. In the parabolic case. And if such an analogy holds, then I can kind of get these shatter estimates with only the regularity of the coefficients. So I picked this Hageromo chalk because I was tempted by it. I mean, it's a little too small to slum it with a regular piece of chalk. So this is the hope you have. So, this is the hope you have. And unfortunately, well, we couldn't prove it. But we got something. So, this was posted a few months ago, so it's, I don't know, I think people sometimes put a plus there if it's a weird thing. So, what we can do is we can do, we can get shower if the coefficients, so let's say we do d2 plus v dot grad f. d2 plus v dot grad x f is trace of a of t x v d squared v f plus g. So if a is l infinity t, but then we weren't able to reduce the holder index. Okay. And then the upshot of that Is that, okay, under some technical condition on vacuum regions, so let's say no vacuum, then what we can say is we get a weak, strong uniqueness for Lando. Where you just need, so f, you want to be in a polynomial weighted L infinity space. So let's say sort of vaguely v to the l f naught. We want it to be in L infinity, and then we want it to be, okay, I've run out of space, so we'll put it up here. So we need whole irregularity in x, but we need almost no regularity in v. C alpha over, I think I want to put alpha over 3 because I think that's the right scaling. And then I'm going to write this in a very silly way: log of 1 over cv to the minus theta. So what do I mean by this? So when you write cx, I mean like if you take a shift x minus y, then of f of x minus f of y, then you want absolute value x minus y to the power alpha over 3. So you just put the shift where the c is. So here I'm saying the modulus of continuity is like log of 1 over v. Like log of 1 over v minus w to the power of negative theta. So it's like a little bit stronger than the Dini continuity that Hongji talked about, but not much. It's basically on the border of it. And then you have some requirement that your theta is not too small. So the actual reason why shower The actual reason why Shouter and the thing that ties Shouder and uniqueness together, Andre will talk about tomorrow. So I'm going to skip that. Let me make just a few comments. So the first is that at the same time we were doing this, there were some similar papers that were looking at chatter estimates. So we all, all of our papers came out like, I don't know, a few weeks from each other. From each other. So one is by this Italian group, Bromanti, Viaggi and Bromanti, but it's supposed to be the opposite order. That's how the alphabet works. And they were kind of building on some other estimates. So there's Bramanti, Polidoro, and then there's a paper by Roussertini. Paj Pajmi Rani and Mascuchi. And their approach is to get very sophisticated fundamental solution estimates. So they're estimating all of the regularity, the fundamental solution for the full problem. And then there's another paper which is by Hongji and And Yostra Jemskai. I'm sorry about the pronunciation. I think that's the right spelling. And they do something which is the complete opposite, which is that they use the kernel, the fundamental solution, not at all. So our proof, which I think I don't really have time to get into, unfortunately. Unfortunately, and maybe fortunately for you, I don't know, depends how you feel about the talk so far. It is basically: okay, how do you do shatter estimates? So you usually do them in two steps. You first define what you mean by the homogeneous equation. You get the estimates for the homogeneous equation. And then you perturb off of the homogeneous equation using the regularity of your coefficients. Right? And that second step is basically the same in every single proof. So let's just not talk about it anymore. Okay? You just do the same thing. Talk about it anymore, okay? You just do the same thing that everyone else has ever done since the dawn of time. But what it tells you is that if you don't want to assume T regularity, then when you do that perturbation off, you don't have any T regularity to kind of use in your perturbation to get smallness. So it means that your homogeneous equation has to be the equation where A isn't constant, but only depends on T. So it's constant in X and V, but it depends in T. And what's kind of amazing, Sorry? Two minutes? I will do it in one. What's kind of amazing, okay, is like Wayron was talking about before, right? Sometimes you get lucky and you have something you can just write down a solution to. So you can just do a Fourier computation. You can just write down what the fundamental solution is. When A is constant, this is a computation going back to Kolmogorov from like the 1930s. And it's kind of clear, you get something that looks like a Gaussian. It's kind of clear, you get something that looks like a Gaussian, and it definitely is integrable, and you can multiply it by as many polynomials as you want, it's still integrable. When A is not constant, you get some horrible mess that's like a bunch of products of integrals of matrices, and it's like even the sort of Gaussian decay of it is completely not obvious. I'm not saying it's like the deepest math in the world, but it's kind of a fun puzzle to sort of figure out why this thing decays. And once you do that, okay, then you have to kind of do this careful estimate to figure out In that, to figure out why you get the kind of regularity you hope. Okay, let's stop there. Thanks for your time. Thank you, Chris, for keeping the art of Blackboard talks alive. Uh questions or comments? I was interested in the the boundary case, gamma equals minus three exit. Gamma equals minus 3. Is it important in any of the results, or is it? Okay, so for Boltzmann, it's undefined, I think, 10 gamma is negative 3. But for Landau, that's the physically relevant one, right? And our uniqueness results at play, it applies equally well to gamma equals negative 3. I was really worried about this, actually, all the way through the writing of the paper, and then we got to the last page, and it never came up. It was kind of nice. It causes some. Things. It causes some annoyance halfway through because in the uniqueness arguments, okay, when you apply the shouter estimates, what's playing the role of g? Well, in the game equals negative 3 case, that's going to be an F squared term. You get a little nervous about the quadratic, but then it's a lower order term. So you can play these games where you kind of interpolate back and forth, and then it just goes out in a wash. So yeah, I assumed the whole time that it wouldn't apply to gamm equals negative 3, and then it was. I meant to say something about this. So I think this is like an observation that maybe is new to me, but is not new to anyone else in this room, which is that V regularity and so what do you want to do? You kind of want to So, what do you want to do? You kind of want to separate into kind of like V regularity and sort of transport regularity, whatever that means. Okay, what does it mean to talk about V regularity? It means like how much is your function changed as you shift in V, right? And what is transport regularity? It's like if you follow kind of the ballistic path based on your velocity, how much does the function change? But the issue is that these things don't commute, right? So, if you shift in V and then you shift in transport, it gets you somewhere different than if you shift in transport, then shift in V. And And that's not the case for something like the heat equation. You shift in time to your heart's content, you shift in space to their heart's content, these things don't interact at all, so you just pull them apart. So it seems to, you know, you see it even very clearly in the fundamental solution. Anytime you try to break apart the X and the V, you can't do it. And so that's why you kind of are forced to just assume regularity in X, assume regularity in V, and go for it. I was, you know, in this. I I was you know in this paper we we like formulated a grand conjecture that maybe you can prove it. And I would really like someone to do it, but I spent uh enough of my life on it so far. I think it's time for me to give up. I was hoping that HomeG would be able to tell me how to do it. But I think he found the same issue. Right. I've got two questions. First, concerning S, you've shown an instance where large S is worse than small S. S is worse than small s when there is a problem, but some of one of things s larger means more diffusion or whatever, so better coercivity. So, can you please comment on what is better or what is worse in relation to larger or not? Yeah I think it should be technical. Technical, right? Because somehow, if you're in total vacuum, this thing doesn't show up at all, and then there's no problem sort of propagating something for it hydrographically. If you're not in total vacuum and this thing, you have like a great coercivity bound, then you can use all the smoothing estimates to your heart's content. The issue is that we don't know how to patch together those two arguments as the coercivity bound here kind of gets smaller. And since you're working, since we're starting from Since you're working, since we're starting from, we're not starting from near equilibrium, we're not starting from near Maxwellian. So we're not assuming any kind of, in the existence results, we're not assuming any kind of vacuum, no vacuum condition. So you have this issue where you might have great coercivity in some regions in X and terrible in other regions in X. And so you kind of, there's no way to kind of break these apart and use smoothing sometimes and not use it other times. It also might not be true, I don't know. I mean, somehow it's like, if this were. Like, if this were an ODE, it was just f dot equals something about f, then it would, getting estimates is utterly trivial. But then when you put in a derivative of s, well, why is the Laplacian a good derivative? It's a good derivative because it's a positive operator. But like, I don't know, what if you took negative the Laplacian and you had time derivative of f equals negative Laplacian? I mean, if you don't have a good way of coming up with some kind of smoothing estimate, then that can cause horrible problems. Estimate, then that can cause horrible problems. I mean, it's just the backwards heat equation you pull up. No, no, I perfectly understand. It's very much related, maybe, to the second question. So if you could wish for a result on degenerate equations, like what would help you? So, I mean, there's this diffusion coefficient, right? And it depends on the function, so that's why it's complicated. But now maybe. Maybe if we make it simpler, if we take a degenerate operator with a fraction diffusion driving operator, so which kind of degeneracy is like so particular that you would love to have results for this specific operator or that would help you? So the thing that has been bothering us for a long time is we have this vacuum filler. We have this vacuum filling result that basically tells you that, as long as you wait sort of a boundary layer in time, you get as good of coercivity as you could want. Everything's smooth, you get all the estimates that you would like, and you're happy. But we always have to assume a kind of strong condition, which is that your function, your initial data has below it kind of delta times the indicator function of some ball. But there's no reason that I think that that should be true. You should be able to say, let's say you start on a torus. You start on a torus, you should just be able to say that for some reason, as long as your kind of mass isn't spread out too far in velocity space, that, okay, it's not sort of nicely arranged in a ball, but it's cut up in all these crazy, only measurable, not continuous way. But after a moment in time, kind of between shearing and diffusion, everything should kind of spread around and we should get to a point where you have no reaction. Where you're going to know it actually. So, what we would really like to do is be able to come up with some kind of way of getting rid of the strong kind of L infinity condition on the initial data. You mean the lower bound? The lower bound, yeah. But somehow it's hard to get your hands on anything. You say, okay, I've got some mass sprinkled along some line in V. Okay, and maybe I want to say, great, then my thing will be nicely coerced, at least for that V, and so it'll spread uniformly. The problem is that. It'll spread uniformly. The problem is that you try to go forward in time, and everything just gets sheared with a transport. And now it's not along the same line in V, and so you could still have, you know, a total mass of one along the skew line, but along any particular x, the line in v, has kind of no mass, or at least you have no estimates on how much mass there's that. Can I ask a question on your question, Mauritz? Were you curious if you just think of a kind of generic hypoelliptic equation? Generic hypoelliptic equation. What would the degenerate elliptic part, that is the part that has the arrow on it, the fractional Laplacian, what assumptions on that would you like? Could you imagine a list of assumptions that if those were satisfied, you would get the regularity that everyone's looking for? Is that very much? Yeah, so I wanna I wanna go home wi with with a task. Yeah, yeah, you wanna you wanna go solve a uh write a new theorem. Write a new theorem. Yeah, yeah, yeah, yeah, yeah. So if you just take, think, think, so you want the v dot grad x on the left side or just ignore it? Oh, I mean, that's an additional problem for me. Yeah, I know. So the thing is that, like, there's a huge lie in writing it this way, because the collision operator is much nastier than that. And what you might think is maybe, like, actually, what I'm saying. Is maybe like actually what I'm saying is that this thing looks like you know the fractional Aplacian, but I put some kernel in there, right? But in fact, this isn't true. It's kind of some messy