So, thanks for the invitation to speak and to the organizers for organizing. I've been learning a lot. So, I'm going to present about something that's a little bit off the maybe main track of what we've been hearing about. But hopefully, I'll be able to convince you that some of the kinds of ideas that people like. Ideas that people like to think about in this community can actually be useful for kind of a quite different, more sort of classical math number theory kind of question. And some similar ideas kind of transfer quite directly. So let me tell you first about what this Paley graph is and why its queak number is interesting in number theory. So this is a deterministic graph. It's defined on a finite field. So let's say z of So let's say Z of prime order, so Z mod P for P a prime number. We'll see in a second that it's important that P is one mod 4. There are infinitely many primes like that, so it's some sequence of growing graphs. And the way that we build this graph is that for any two elements of the field, we look at their difference, right? And we say they're going to be adjacent if their difference is a square. If there's some non-zero number, that number squared equals the difference. Squared equals the difference mod p. So it's a kind of classic number theory fact that if p, so if and only if p is one mod four, negative one is a square mod p. And so i minus j is a square if and only if j minus i is a square. So that assumption is to make this kind of adjacency relation symmetric. So here's an example. 5 is a square. It's 1 mod 4. If you look at what are the perfect If you look at what are the perfect squares mod 5, there's two of them: 1 and 4. 4 is the same as negative 1. So, like I promised, negative 1 is a square mod 4. So when we build this graph, we write down the elements of Z mod P of the field, and we say, okay, we connect any two if their difference is either one or minus one mod five, right? So that just means we connect kind of going forward or backward equivalently along this cycle. Equivalently along this cycle, and we end up with G5 being this pentagon graph. Okay, so let's talk about some kind of basic properties of this graph. So the simplest thing we can say is that the degree of any vertex, so it can be connected to anything with whom its difference is a square. So its degree is the number of squares mod p. It's another kind of classic number theory fact that the number of squares of non-zero squares is exactly half of the non-zero elements. Of the non-zero elements mod p, so p minus one over two. So asymptotically, each vertex is connected to half, up to this small correction, half the other vertices, or actually exactly half of the other vertices. There is a much more kind of broad heuristic about how this graph behaves that comes up in number theory, comes up sometimes in computer science, which is that it basically behaves like an Erdős-Reni graph with edge probability. With edge probability one-half, which gives us at least kind of the same typical degree. So here is kind of a standard manifestation of this sort of pseudo-randomness. So imagine, let me think of GP as being a labeled graph. Imagine I have some other labeled graph. Yeah. Back to the example. Yeah, so whether there's an edge between I and J only depends on the difference. Edge between i and j only depends on the difference between j and i. That's right. So, is this graph like rotationally invariant? Um, it has a big automorphism group, which is a subgroup of kind of affine maps. So, okay, it's translation invariant, for example, under right. So, that's, I guess, right, written this way under rotations, yes. These like chords, like some people have graphs, you know, and then they connect any two vertices that are distance three, five, or six, you know, long. or six along right right it's a it's a yeah it's a similar it's a similar kind of idea right so if i write the elements of fp along a cycle it is invariant under uh rotations um another way to say that is that its adjacency matrix is a circulant matrix right so it has uh it's kind of easy to understand its eigenvalues and stuff yep that makes it great and actually there there are even more right yeah so More right, yeah. So, so right, we'll see that eventually. There are even more um automorphisms, which so that those are kind of uh additive ones where it's invariant under adding anything to each i under that map. It's also invariant under multiplying each i by anything that's a square mod p, because a product of two squares is again a square. So, here there's not any, you know, okay, so it's invariant under multiplication by negative one or multiplication. Multiplication by negative one, or multiplication by four, mod p. So it's really a very big automorphism group, and this is useful for some of the stuff we'll talk about. So the degree is the number of perfect squares. Yes, mod p. And that's about half. That's right. That's exactly half of the other vertices. So why are half the numbers perfect square? Well, it's not hard. I mean, you know, there's some counting arguments about you can show that the product. About you can show that the product of any two squares is a square, a square and a non-square is a non-square, two non-squares is a so there's partitioning of the multiplicative group. Yeah, it's not hard. Okay, other questions about the definition? Yeah, interrupt me as much as you want, and I'll just stop whenever it's time to stop. So, okay, so one manifestation of this pseudo-randomness is right, I'm going to think of Is right, I'm going to think of GP as labeled. Let's say I have another graph H that's also labeled, and I'm going to count how many injections are there from H into GP so that the induced subgraph on the image is exactly the graph that H kind of asks it to be. So, for example, triangles or something, or cycles of some finite length. If I count this number, then This number, then asymptotically to leading order in the random graph, it's easy to check that this is what I get. So, this is the number of injections. There's no denominator because of the I'm thinking of things as being labeled. And this is just asking that every, this is the probability that each edge or non-edge is kind of the way it should be according to h. And it turns out that using some kind of number theory machinery, you can show that the number of these injections into Of these injections into GP, also, this should be a p, sorry about that, is asymptotically the same. So, this is kind of a pseudo-randomness of counts of finite kind of fixed subgraphs. So, this is this is the type of sense in which in which GP is pseudo-random. Using some kind of delta theorems, it's using uh so we'll talk more about it, it's using kind of character sum estimates about uh so the the kind of adjacency in GP is Adjacency in GP is kind of evaluations of the Legendre symbol, this kind of indicator function of squares and non-squares. And so it's about cancellations and certain sums of that. But I'll give kind of some theorems that describe that, and I'll mention it later. Okay. Okay. So let's kind of try to push this a little bit further. One way to try to do that is to say, okay, what if H is not just a fixed graph, but A fixed graph, but what if I want to understand the number of occurrences of graphs that are slowly growing with p? So, one example of such a question is to ask about the size of the largest clique. So, if I look at a complete graph whose size is scaling somehow with p, what is kind of the threshold where that graph stops appearing in GP? That's the clique number asymptotically. So, okay, what do we think should happen? Should happen. We think that GP behaves like a random graph. For a random graph, it's kind of a probability exercise, first, second, moment methods to calculate the typical clique number. It's this two log log p, if p is also the number of vertices in there. So maybe we would expect that the same thing should hold for the paleograph. So the reality is that we don't really know what's going on. It's definitely not quite the same. Definitely not quite the same. So there's an infinite sequence of primes for which the clique number is kind of a hair larger. This log log log, right? What does a drowning number theorist say? A hair larger than log P. There's in here? No, I don't think so. And also, so this is just, I mean, this is a weaker statement than it would be. Weaker statement than it would be with a factor of two. I think, oh, maybe I should write a greater than with a squiggle or something. Maybe there should be some constant in there. From this? Well, this is off by even more, right? Because it's off by this, it's off, it's larger by this, by this very slowly kind of diverging. Right. Yes, yes. Good. So there are some. So, there are some, so this is known. Then, actually, so conditional on like some generalized Riemann hypothesis, you can drop one of the logs and make it just log log, but whatever. There are some random heuristics for kind of how the set of perfect squares mod p looks, so that it's not just totally a random subset, but it has some multiplicative structure. That might lead you to predict that this should grow like log p squared. This should grow like log p squared, and generally speaking, I mean, loosening all of this a little bit, most people, I would say, believe that this should be like O of poly log of p. So, log p to some power is definitely a very kind of plausible upper bound. On the other hand, the best upper bounds that we can actually prove scale like root p. So, there's a so g p is a regular graph, right? Like we mentioned, there are, there's this Hoffmann or kind of spectral bound on the clique number for root. Bound on the clique number for regular graphs. And so, as Bruce was kind of suggesting, the spectrum of GP is easy to evaluate. So, if you calculate this Hoffmann van, you get square root of P. And actually, there's an easy combinatorial argument to prove that also. This was the best thing known for a long time until quite recently, there was a breakthrough of sorts that improved on this by just this constant one over root two factor. And this is kind of where. Factor. And this is kind of where we're stuck right now. So there's this huge gap. And this is kind of a big open problem in number theory that's related to a bunch of other questions about the kind of distribution of squares and non-squares moda prime. And the thing that we would really like and where we seem to be stuck is to find some proof techniques that let us prove upper bounds that scale strictly better. That scale strictly better in a polynomially than square root of p. Okay, so that's the kind of question that we're inspired by, and I'll describe sort of two lines of work that try to address this. So the first one is about sum of squares relaxations, which probably people have heard of at least, but I'll explain what those are. And this is joint work with Shifan Yu, who's a wonderful grad student at Yale. So, sum of squares is kind of a framework for writing down semi-definite programs for bounding exactly this kind of quantity. So, how do we do this? Here's kind of the quick and dirty version. So, we want to write down the clique number as a polynomial optimization problem. So, here I've done it mostly, except for this Boolean constraint I'm being kind of lazy about. But this vector y that I'm introducing is the indicator vector of a clique in my graph. This constraint over here says that two vertices can't both be in the clique if they're not adjacent in my graph. So subject to that constraint and being Boolean, y has to be the indicator of a clique. And the objective function is just that I'm maximizing the size of the clique. So this is just another way to write down the clique number. Quick number. And now to write down some of squares relaxations, again, here's kind of a rather non-conceptual way to think about it. So, what do we do? We take a vector y like this. We take monomials in y up to some degree d. We stick them together into a big vector. And we think about the matrix, which is the rank one matrix formed from that big vector. So, if we could optimize over Could optimize over the set or the convex hall even of these x's, then we could solve the we could calculate the clique number exactly. We can't do that because, I mean, this kind of the fact that x has to be ranked one sort of makes that makes that difficult, but we can write down a relaxation by just looking at this matrix, finding some constraints that it has to satisfy, and optimizing over just those instead, right? Standard kind of stuff. Standard kind of stuff. So the ones that we, the constraints that we choose to keep are kind of what give sum of squares its particular flavor. The main thing is that we keep the constraint that X has to be a PSD matrix. Then we keep some observations about its entries. So notice that if we look at a particular entry of X, that's going to be the Entry of x, that's going to be the product. So entry ij, that's going to be the product of a bunch of, let's say, y k's over all the k's appearing in either i or j. But y k squared equals yi, equals yk again. So this will just be some function of the set of indices that occur any number of times without multiplicity in these i and j taken together. So I'll show an example on just the next slide. On just the next slide. But for this set function, also the kind of empty set occurs in the top left corner of x, right, when both of the indices are coming from this one in the vector of monomials. So that value has to be one. And coming from this constraint, right, about adjacency in the graph, if we have a set of indices that doesn't form a clique where not every pair is adjacent, then that value has to be zero. That value has to be zero. So, this is one PSD constraint and a bunch of linear constraints on this matrix x, right? So, now we can optimize kind of the proxy for our objective function over this set of x's. This is a semi-definite program, so we can just do it. And this is a kind of relaxed, a larger feasible set than the one defining the clique number. So, you can just. Right, uh an an upgrade because we are including this thing. Right. I think for d equals two, this is exactly the linear program that we have involved. For d equals two, so there's still a PSD constraint. Um, specifically for the paleograph and kind of very symmetric graphs, you can, um, you know, kind of you're you're able to assume. Of you're able to assume some extra invariance of this feasible set by kind of averaging over, let's say, automorphisms of the graph. And in this case, and in like for strongly regular graphs and stuff, that makes it a linear program, right? In which case, it's right, it's very close to these, I would say, like exactly that, Del Sartre. Right, Del Sartre from coding theory, yeah, exactly. Right, right. So you'd probably just have to find a rule, right? Right, so I mean, it's kind of like a semi- Right. So, I mean, it's kind of like a semi-definite program where all of the matrices involved commute, and so they're simultaneously diagonalizable. Yeah, exactly. Okay. Other questions? Can you say one more time the us? Yeah, so let me try using the other, the low-tech version. So if I'm looking, can you see if I write like this? That was a disturbed look. So if I, you can, you can see. So, if I you can you can see like this, sort of. Okay, so let me. So, let's say I look at x one, two, three and one three four. I'm saying that this is just some function of the set one, two, three, four. Because this, you know, in the if x is rank one like that, this is okay, y one squared, y two. Squared y2 y3 squared y4, right? But this is just y1, and this is just y3 because the y's are Boolean, right? So I'm just saying that the entry depends only on the, so this is S, right? Only on the set of indices occurring at least once in either the row or column index. That's it. It's not any kind of big observation. Does that make sense? Okay. Other questions? Uh well, right. So right. So the values of s that I need in order to kind of specify x, right, are the values of s up to size 2d. But each s will occur many times in this matrix. So the row and column indices are, you can view them. So here I'm writing them as kind of tuples, but you can view them as sets of size less than or equal. You can view them as sets of size less than or equal to D, right? And so then the unions are sets of size less than or equal to 2D. More questions? We can just talk about this and then everyone will understand SOS and can go home. Okay, so let me give you an example. So when D equals one, right, this is so the SOS terminology is that the degree is two times this little d, right, which as Jia Ming was saying. Right, which, as Jia Ming was saying, is the maximum size of any set that you need to kind of worry about. So, right, so here I'm writing down exactly what we were just saying. Think of this big matrix X as indexed by the empty set here, and then by all the singletons here. And in a given position, I'm writing this kind of function of the set that is the union of the two indexing sets. So, in particular, on the off-diagonal. So, in particular, on the off-diagonal right here, in position one, two, I have the set one, two. In position one, one, I just have the set one because I'm taking the union. Um, so, right, the constraints are that this thing needs to be PSD and the kind of clique constraint, well, here only sets of size two up to two appear. And so, all we need to say is that whenever two indices are not adjacent in the graph, then the value is zero. And we, okay, we optimize right the sum of all this. Right, the sum of all this over here, the kind of proxy for our objective function. Now, you have maybe heard of this accent, it should be going the other way, the Lovas theta function. This is a bound on the independence set or the independence number of a graph. And so taking the complement, you can use it to give an upper bound on the clique number. So it turns out that this degree two relaxation is actually equivalent to that. And this is kind of a typical situation where. And this is kind of a typical situation where the, you know, often there's people have developed these kind of ad hoc SDPs in the literature, and usually SOS kind of reproduces the first level of SOS reproduces these kind of quote unquote natural SDPs. But then when we take this parameter D to be bigger, we get bigger SDPs. They give us tighter, or at least no worse, bounds on the clique number, but they're bigger. On the clique number, but they're bigger SDPs, so they're more costly to solve. In particular, they take time, you know, p to the some constant times d. So the question that Shifon and I asked is, okay, can we use these things to prove better bounds? Let's like evaluate how this works for this number theory question of trying to bound the clique number of the paleograph. So since we think the paleograph behaves at least sort of like a random graph, we should look at what's known about random graphs. Should look at what's known about random graphs, and it turns out that a lot is known about random graphs. So, there was this long line of work in the 2010s that at the end of the day showed that, so we've already seen that the true clique number of an Erdos-Reni graph scales like log p. And they showed that, okay, if you stop at some particular level of this sequence of SDPs, then the value that that achieves up to polylog factor. Up to polylog factors is like square root p, right? So there's this big gap, which is quite similar to the gap we saw in kind of the state of the art for bounds on paleographs for random graphs. So maybe we expect the same kind of thing to hold for paleographs. Let me go on a little digression about the kind of why this is interesting from the perspective of SOS relaxation. Of SOS relaxations. So, the reason why this whole big line of work and theorem was interesting is that we can view this as a demonstration that bounding the clique number in the average case is hard because there's other evidence that SOS is for many problems kind of an optimal algorithm or something like that. So, from that point of view, it's kind of natural to ask, well, here we're talking about this very symmetric. We're talking about this very symmetric, very kind of maximally random and uniform distribution over random graphs. So we can ask, you know, can we kind of strip away some of this niceness and still get similar results? And kind of what properties of a graph or a distribution over graphs are enough to ensure this kind of thing. So, and this is a kind of question that people have started looking at for some other kind of old sum of squares lower bounds. They've started asking, well, can we find They've started asking: well, can we find when we have lower bounds on average case instances, can we kind of derandomize and find explicit, just usually kind of combinatorial constructions of some objects that reproduce the same gaps between the value of some optimization problem and the value of SOS relaxations. So in this case, we can ask: can we find explicit deterministic graphs whose, we can be precise to varying degrees, but let's say whose clique number Varying degrees, but let's say whose clique number is like a polylog of the number of vertices and where the values of the SOS relaxations look like they do for random graphs. So, okay, hopefully, obviously, I'm kind of indicating that we might hope that the paleograph would achieve this, and that would be kind of a positive result. It would be a negative result about bounding the clique number of the paleograph, but a positive result about, you know, this kind of line of work to do with SOS. And it turns out that this happens, but we think only sort of. The situation is a little bit kind of subtler than we expected. So we can kind of partially derandomize these lower bounds, or we can find some indications that actually SOS might improve on this square root p upper bound, but we can kind of restrict how much it can improve. Restrict how much it can improve. So, the first result, which is easy to show, this is coming from this Lovas data function where you can reduce things to an LP. The value of that first relaxation is actually, this is true without an omega. It's exactly square root P. It's exactly the same as that spectral band that I showed before. Yeah, just to ask the question. So, there is an Skyrim function and the Lawrence data function, right? So, once you draw the positivity constraints. Drop the positivity constraint of x than the entry-wise positivity constraint. Yes, right? We usually think about Scriber because we also impose positivity constraints. Yes, yes. But usually it's like it doesn't give any, you know. Yes, I'm pretty sure that's the same in this case. And I'll mention later a paper where they look at that and a bunch of other kind of variations on like approaches that involve using this relaxation, but in slightly smarter ways. But in slightly smarter ways, and I'll say how well that does. Just to sort of in coding theory, people spend enormous effort to apply those hemming graphs. Yes, right. And there was also a similar finding that actually SOS beyond degree two doesn't actually help. At least, you know, there's no proofs, but it was completely a failed program to improve the balance. Right, right. So, this is, I mean, this is kind of what we expected also, but yeah, so, so. So, but yeah, so I'll show you. I mean, there is actually, it seems like for this problem, it might improve, but only sort of somewhat. So, we'll see. Other questions? Can I ask you something? Yes. Thanks. Yeah. Just a quick question. So, you said there is a question that like people ask also about the constant in front of the square root p, right? Uh, yes. So, is this a simple thing you're describing giving square root p or square? Been giving square root p or square root p over two, or I guess no square root p. This value is just square root p. Okay, I wrote it in this kind of soft way to compare with things I'm about to show, but this is exactly square root p. There is, so later I'll show a kind of adjustment of this that achieves, at least empirically, people noticed a slight improvement of this kind of achieves this state-of-the-art square root p over 2 bound. If you do this to basically this. If you do this to basically this relaxation to a slightly different graph and use some of the symmetries, some of the automorphisms. I'll mention it when we get there. Okay. Okay, so our main theorem is about one step higher in this sequence. So remember, these are indexed by even numbers, so degree four sum of squares. And we can show a lower bound that is not like root p, but like p to the one-third for this. For this. And I'll point out that: okay, this doesn't derandomize the big random graphs theorem that I showed before, but it does derandomize a result. This is by Despande and Montanari that was kind of proved along the way to that result. So a kind of intermediate thing, and they showed basically exactly the same thing that we show in the random graph case. This was suboptimal, it turned out, but we sort of follow their argument and adjust as needed. And adjust as needed to make it deterministic. So, okay, this is the main result. There are a bunch of questions to ask here. So, first of all, like, you know, why is it not as good as I was suggesting it might be? Can we improve this lower bound to scale like square root p? And here, surprisingly, so we can run some experiments. Some other people have run some bigger experiments on weaker. Bigger experiments on weaker SDPs than degree 4 SOS and found something similar. But when we run experiments and try to identify this kind of polynomial scaling, actually it looks fairly convincingly like this is something smaller than one half. So smaller than square root p scaling. And so from the point of view of kind of the number theory question, this is an exciting observation. It seems like this degree four relaxation can actually improve. Relaxation can actually improve on the square root p bound. And so I'll come back to that in a second. But at least from the point of view of improving our theorem, this is suggesting that probably we can't do it all the way to this square root p scaling. Okay, so if there's some magic number here, maybe we should be able to improve our lower bound up to that polynomial scaling. So here we can also give kind of So, here we can also give kind of a reason why our, at least our specific proof technique is not able to do this, which is that so we use this. So, to prove a lower bound, you have to construct this matrix X, right, that has a large objective function, and you prove that it's PSD and satisfies all the constraints and so on. So, there is a simple kind of class of construction that people used in that random graphs line of work, which we also use, where we say, okay, this function is supposed to be zero when s is not. Is supposed to be zero when s is not a clique. So, okay, we'll stick in this indicator here. It's zero when s is not a clique. And then, otherwise, when s is a clique, we'll just make it some function of the size of the clique. So it has some value on singletons, some value on edges, some value on triangles, but no further structure of the graph kind of plays any role. For random graphs in that long line of work, there was at some point this argument due to Kellner, who said that, so while the optimal lower bound we expect, while the optimal lower bound we expect here is like p to the one-half, actually if we restrict ourselves to this class of constructions, then we can only, we kind of, as the degree goes up, we can only do worse and worse. In particular, when d equals two, so for degree four, this scales like p to the one-third. So this class of constructions can prove a lower bound for random graphs no better than p to the one-third. And Schiffon and I can kind of derandomize Kellner's argument. Argument and say that, okay, if we stay within this nice, easy to work with class of lower bound proofs, we can't improve on the scaling that we get. And so as we'll see, using fancier versions of these X's really starts to kind of introduce some more technicalities. So this would probably be hard to improve. So, okay, coming back to the previous To the previous remark. So, you know, it's kind of surprising, I mean, maybe based on these other problems that Yuri was mentioning and such, that degree four SOS would actually make kind of substantial progress on this problem. We can give kind of a small shred of theoretical evidence for that, which is by looking. So, this is the paper, this is the quote-unquote pseudocalibration paper. That proves this optimal lower bound for random graphs. So we can look at their proof techniques and ask: do the kind of technical tools in there carry over to this deterministic paleograph case? So this is kind of quite a technical proof. It depends mostly on bounds, on norms of certain matrices that you build from a graph. These are called graph matrices. They're built. They're built, I guess, so from two graphs. Here's an example. So there's this kind of template graph H, and there's your input graph G, let's say the Paley graph or a random graph. And you kind of combine these two pieces of information to build a big matrix. So it's a little complicated. This isn't even quite exactly the definition, but this is kind of in the spirit. So roughly what's going on is that you have this template graph H. It has a distinguished subset. It has a distinguished subset of vertices on the left and on the right, and some other stuff going on in the middle. The matrix is indexed by ways to label the vertices on the left and ways to label the vertices on the right in H with your sort of vertex set, let's say numbers from one to P. Then to calculate the value of a particular entry, use sum over always to assign labels to these middle vertices. To these middle vertices, and you sum a big polynomial of entries in the adjacency matrix, plus minus one adjacency matrix, of your graph, where the factors that are included in here correspond to all of the edges in this template graph. So, if people have seen things like tensor networks, this is a very similar kind of idea to a tensor network where A, network where a this matrix corresponding to g is kind of playing the role of a tensor and h is playing the role of a network or you can think of this as some you know kind of weirdly deformed like large tensor power of of a but so this is some very big class of matrices you can build from a small matrix and you need to control the norms of of all of these things so what we can show is that there are some matrices in this big class where the norm for Where the norm for the paleograph is very different from the typical norm for a random graph, is much bigger, and we want norm upper bounds. So the takeaway, I mean, all this technical stuff doesn't really matter, but the takeaway is that there's this very important kind of technical tool inside of that proof of the optimal lower bound for random graphs. And this does not derandomize. It doesn't apply to the paleograph. So that proof from the pseudo. From the pseudocalibration paper, if people have heard that word floating around, won't transfer at least kind of totally indirectly to this deterministic setup. But this does work for kind of sufficiently small ones of those template shapes H, and that's why we can prove our kind of more humble lower bound. And the way that we can build these examples is to take advantage of kind of a simple sense in which the Sense in which the Pelegraph is different from an Erdős-Rény random graph. It's kind of a spectral difference, which is that if we look at this plus-minus one adjacency matrix and we square it, in both cases, we get two terms. We get this p times the identity because we're taking kind of norms of rows of the matrix. And then we have some other term. In the paleograph case, it's this rank one, one, one transpose term. And in the Erdős-Rény case, it's really kind of a much more It's really kind of a much more uniform random matrix. So we can take this and kind of fiddle with it a little bit more to prove this result. So, at a very high level, right, our intuition coming from this stuff is that if SOS can do degree four, let's say can do better than this square root p scaling, then the reason is that though the Paley graph is kind of pseudo-random and all of those nice ways of subgraph counts and stuff that I was saying before, it's very And stuff that I was saying before, it's very much not pseudo-random in terms of its spectrum. So, like I was saying to Yuri, right, its spectrum is supported on, so there's a tiny little bump at zero, but mostly it's supported, and this is again the plus minus one adjacency matrix, it's supported on just two atoms, right? Two numbers, plus minus root p, I think. And this is, of course, very different from the semicircle kind of law that we get for random graphs. And somehow it's For random graphs. And somehow it's taking advantage of this kind of non-pseudo-randomness that seems to let SOS beat this square root barrier. Okay. Let me maybe just quite quickly mention the proof ideas going on in this lower bound. So like I said, we still want to bound these graph matrix norms. So the way that people did this in the random graph case, People did this in the random graph case is kind of a standard random matrix technique. So they take expectations of traces of powers. This reduces things to some kind of quite complicated combinatorial calculation, and that gives you norm bounds. So sometimes we do something similar. Sometimes we work directly with matrices, but in any case, we replace those combinatorial kind of counting arguments with what I was saying earlier, these character sum estimates. So these come up because this plus minus one adjacency. This plus minus one adjacency matrix of the polygraph is an evaluation of this Legendre symbol chi, which is just the function that's plus one on squares and minus one on non-squares. So this is a character, a multiplicative character, meaning it's a multiplicative function. This is what I was saying to Bruce before, right, that the product of two non-squares is a square and so forth from the field to two complex numbers. Complex numbers. And so when we write out these graph matrices, we end up with polynomials of this thing kind of appearing inside there. And there are not very kind of general tools for bounding the kinds of things that we would get if we tried to do this trace power method. But we can use, you know, we have maybe like 20 of these or something. You know, we have maybe like 20 of these or something to deal with, and we can use different tricks and kind of make our way through and get the estimates that we need. So, here's the idea of the kinds of estimates that we're looking for. This is kind of an older theorem, and it says, let's say you have some polynomial over your finite field. Let's say it's not a perfect square, or it's not some number times a perfect square. And you want to take a sum. And you want to take a sum of this character, this function chi, kind of precomposed on the image of that polynomial over your whole field. Then this says, so this is a sum of p numbers, each of which is plus minus one, and maybe a few zeros. This says that this thing is of order square root p. So the intuition is that this says there are square root cancellations in this kind of sum, right? It behaves as In this kind of sum, right? It behaves as though these were just random plus minus one signs, or maybe very weakly correlated, and that's where this degree term comes from. Now, what we would actually need for our proof, if we didn't kind of try to take any shortcuts, is to do this for multivariate polynomials, right? So, here there would be p to the k terms. So, imagine f now takes k inputs. Now takes k inputs. There are p to the k terms in here, and we would want this to scale like square root p to the k. So this vial bound and all of its kind of progeny are old number theory results. These multivariate character sums are really kind of an area of current research. So here's an old book from the 80s about this. And if you try to look at like what would it take to prove new To prove new estimates like this, you know, there are things about sheaves and cohomology along the fibers and et cetera. So we made, I think, a good faith effort to try to understand, to try to appreciate that we are not qualified to prove new things like this, and instead kind of maneuvered around using some existing estimates that other people proved and applied them in some, you know, slightly Applied them in some slightly creative ways to the matrices that come up for us. But we got kind of quite lucky that we could play those tricks and reuse existing results rather than having to prove really new things in this direction. Okay, so that's all I want to say about some of squares relaxation. So any questions about that? Okay, so the other kind of direction. Kind of direction that I want to tell you about is asking, okay, so I was suggesting that the reason that it seems like some of squares can improve on this square root p bound is that there is this kind of failure of pseudo-randomness to do with the spectrum of the paleograph. And the idea now that I want to tell you about is to ask, well, you know, is there some maybe more subtle kind of pseudo-randomness involving this? Of pseudo-randomness involving the spectrum that actually can help us prove bounds on the click number. So here's the kind of proof strategy where this comes up. This is a paper by Magsino, Mixon, and Partial. I think this is kind of a natural idea, so maybe it's been around before. So this is the, well, okay, so if we want to calculate the clique number of the paleograph, this is how they. graph. This is how they start out arguing. We can observe that it's vertex transitive. So for any two vertices, there's an automorphism that maps one to the other. So there's always going to be a clique of maximum size that contains any particular vertex, let's say the vertex zero, because I can apply an automorphism to it that maps some vertex in there to zero. I'll get another clique of the same size that contains zero. So I can kind of restrict the calculation of the clique number. The calculation of the clique number to this graph I'll denote GP0, which is the subgraph induced on all the vertices that are neighbors of zero, right? If there's a maximum clique that has to contain zero, I can say, okay, well, I'll count zero in my clique, and then I'll look at the largest clique among all the mutual neighbors of zero. So there must be a maximum clique that is zero plus a clique among its neighbors. So this is this is just. Its neighbors. So, this is just equal to the click number. This is the kind of thing they used. I'll say more about it in a second. But they didn't, I think, kind of notice this, but you can improve on this idea a little bit more. You can observe that the graph is also edge transitive. So for any two edges, there's actually an automorphism taking one to the other. And you can do the same thing with mutual neighbors of 0 and 1. These are two adjacent vertices in the graph because 1 minus 0 equals. graph because one minus zero equals one is a square mod p and so there will have to be some maximum clique that contains those two and a large clique among their mutual neighbors and you don't even need to use all of this transitivity stuff if you don't want to i mean this is nice because these are just two specific graphs but you can just fix some number k and say okay the clique number is uh Is that number? It's some, a maximum clique must consist of some k clique and then some large clique among the mutual neighbors of that. So here's a picture. For example, we can say there is some maximum clique that is some triangle. And then in this graph GPC, which is the induced subgraph on the simultaneous neighbors of everything in that triangle, some large clique in the Some large clique in there. So it's just this is a simple observation. That's where these formulas come from. Now, what this paper did is they said, okay, we have these formulas. Now, if we want to bound the clique number, instead of just applying, let's say, Lovas data function or whatever our favorite bound is to the paleograph itself, we can do this kind of localization trick first and then apply the bound to the remaining term on this. To the remaining term on this side, which is a smaller, a smaller graph, right? It's an induced subgraph on some small set. So, this, they did this to the first level of to kind of one, to the mutual neighbors of zero, the kind of large of these small, empirically seems to reduce the state P bound. So, this is what I was what I was saying to you. Um, right, this is you. Right, this is using the cessation, but it slightly kind of modified away, and it's achieved the best that we know how to. I want to talk about a kind of related strategy, so I want to use more general form and think about doing the graphs and do on mutual neighbors of larger cliques. But I want to use bounds, so I don't want to compute with SDPs, I want something that's easier to work with. Easier to work with. I'll think about spectral bounds. So, might have seen this Hoffman bound. Regular graphs, there's also a variant of it for irregular graphs due to Amers. It looks like this. Doesn't really matter. It depends on the spectrum of the graphs involved. So at the end of the day, to try to implement a strategy, we want to understand the spectrum of these induced subgraphs on simultaneous neighbors of cliques. So let's do. So let's do some experiments. Here's the spectrum. This is some large prime. Here's the spectrum of the paleograph, right, that we saw before. This is of the plus-minus one adjacency matrix. Now I restrict to the neighbors of zero, right? This is a subgraph. The spectrum changes. The support, you might be surprised, doesn't change. There's a reason to do with what the eigenspaces are like. There's an explanation for that. Like there's an explanation for that, but the shape changes a lot, right? These two atoms kind of come down, they just become sort of peaks of this distribution and it becomes continuous in between. Let's restrict further to the simultaneous neighbors of 0 and 1. Now the support changes, right? It was around some, I don't know, 80, 90, right? Now it's around 75. So the support, the width of the spectrum is kind of shrinking, and the shape of this spectrum also seems to change. So let's get out of this kind of So let's get out of this kind of vertex transitive edge transitive case. Let's pick some x that's a simultaneous neighbor of 0 and 1. So now we have some particular triangle. I think 2 works for this prime I was dealing with. We look at the subgraph induced on the simultaneous neighbors of all three of those, right? It has some particular shape. So if you've done some kind of random graphs stuff, maybe you're getting a sort of nostalgic tingle. Of nostalgic tingle looking at these plots. And it turns out that these look like the Keston-McKay distribution. So if people are not familiar with this, this is a probability distribution. It's supported on an interval. It has a lot to do, I'll mention this in a second, with random regular graphs. This support of it might look familiar as something to do with Ramanujan graphs, all that good stuff. stuff. Now we saw, so this distribution, this parameter d has to be at least two. This is kind of a you know absolutely continuous distribution on this interval. We saw that the first of our examples was just these two atoms, so that doesn't fall under this category. But it turns out you can take this parameter to be smaller, and then there's kind of some missing mass from this distribution. And it turns out that the natural thing to do is to add these two kind of atoms. Of atoms to it to kind of complete this to be a real probability distribution. And what we can observe, at least empirically, is that if we rescale and kind of shift as needed, then those empirical spectral distributions of these induced subgraphs I was putting up before look like, so for a clique of a particular size, they look like the Keston-McKay law whose parameter d is 2 to the power of the Is two to the power of the size of the clique. So, yeah. My graph is dense. The graph I start with is dense. I know what you're worried about. It's weird. There's a reason. There's a reason. Yeah. So let me just convince you that I'm saying something that is plausible first. So, right, here's the graph without any localization, right? So this is like a clique of size zero. So the parameter of kest. zero so the parameter of kest and mckay here is two to the zero equals one right this is so rescaling by this root root p uh looks like everything works here is a clique of size one so the parameter i'm claiming keston mckay should have is two to the one equals two right you have to rescale in a slightly different way looks good right a clique of size two two to the two equals four also seems to fit clique of size three clique of size three. And here, so there are many different cliques of size three you can choose. I'm not showing this, but you can plot, you know, you can try this for any triangle that you want, and you'll have kind of roughly similar results. So there seems to be this kind of uniform convergence even overall cliques of a particular size. So here are the parameters, two to the three equals eight. So, okay, like you asked, right? What's going on? Right, what's going on? So, the first guess of kind of random graphs people would be to say, well, the place where I know Keston McKay shows up is it's the spectrum of a large, sparse, random regular graph, right? Where the degree is D, the same parameter in the distribution. So as far as I know, this is not the kind of manifestation of Keston-McKay that is relevant in here. There's another one that I didn't know about until I started looking into this stuff, which is that Kestin. This stuff, which is that Keston McKay appears in a different place in free probability. So there is a so Vojkulescu is kind of the developer of all this free probability technology, and he just mentions this as an example in some papers and books of like, look at this nifty or random matrix model I can do with all my cool new stuff. So here's the model that is maybe more relevant. I'm going to build a random matrix. How am I going to do this? I'm going to build a random matrix. How am I going to do it? I'm going to take a diagonal matrix. So, I'm going to build this U D U star. This is the spectral decomposition of this matrix. The eigenvalues, which are on this diagonal matrix D, are uniformly plus minus one with probability one half. And the eigenvectors are just basically a uniform orthonormal basis of complex numbers. You can take them to be, so you can take these to be. So, you can take these to be real also if you want. So, this is Haar measure on the unitary group. You can take it to be on the orthogonal group if you prefer. So, that's some random matrix. And then I'm going to kind of further deform it a little bit by taking some number alpha and taking a principal submatrix where I include every row and column with that probability. It's kind of a Bernoulli principal submatrix of this nice kind of rotationally invariant random matrix. Invariant random matrix. So, this is something, you know, if you've seen some free probability stuff, this is exactly the kind of model that is, you know, bread and butter for those things. And the thing that Vojkuluscu said is that if we rescale and shift and so forth, then the empirical spectral distribution of this random matrix converges to Keston-McKay. And the parameter is one over this probability. That's a number that's at least one, the same as our parameter range for Keston. The same as our parameter range for Keston-McKay. These, so if you know a little bit about free probability, this is saying that Keston-McKay is a certain free multiplicative convolution. This appearance of Keston-McKay, if I'm not mistaken, can be viewed as saying that it is a free additive convolution of certain measure, of a certain, of basically this uniform plus minus one measure with itself D times. So these are different, and I think that this should be seen as. And I think that this should be seen as kind of a coincidence, but I'm not sure. Maybe there's some special explanation. Anyway, so what's the relevance for this for us? So these are the same authors as before, Maxino, Mixon, and Parsh, but a different paper. They proved that this same convergence holds if you take this UDU star, this very nice, you know, unitarily invariant matrix, and you replace it with a totally deterministic matrix. Deterministic matrix. So here it should be rescaled a little bit. But this is some matrix whose spectrum is just a big atom on plus one and on minus one. And the plus minus one adjacency matrix of GP is also like that, right? It's so, except it's plus root p and minus root p. And so what these guys showed is that the same convergence holds for basically random induced subgraphs of the Paleograph. Subgraphs of the Paley graph, right? Or equivalently in random matrix language, we can kind of totally derandomize this part. So we still have to keep the randomness of the submatrix, but we can replace u d u star with this deterministic matrix that looks like that. Yeah. Oh, yeah, yeah, yeah, sure. Yeah. Yeah, GP makes a circular tree, so that's why like UU star basis. Yes, you use star as the Fourier basis, yeah. U is the Fourier matrix. Yeah, um, okay, so. Okay, so what I can do is to go one step further and say, okay, actually, we can even remove the rest of the randomness from this claim and say that if we have a particular fixed clique size, then these specific deterministic induced subgraphs have spectrum converging to the corresponding Kestin-McKay. So, and there are some characters, some estimates. I can only prove kind of a small number. If I can only prove kind of a small number of them, it's hard. Even the number theory people tell me not to worry about it. So I'm not going to worry about it. But they're very natural. And the intuition here is that these deterministic induced subgraphs at the level of the spectrum behave like induced subgraphs on random sets of vertices, right? Where the probability of inclusion is 1 over 2 to the size of the clique, which makes sense because it's probability 1 half to be adjacent to anything in the clique. To anything in the clique. And okay, so like I was saying, right, it seems like these estimates will be hard to prove, but they're just scalar character sums. They just encode these square root cancellations that I was talking about before. So it's just that kind of we don't have the technology to deal with lots of these things. So a friendly reminder that this is very hard and it's not my fault. Okay, so quickly, right, I want to actually apply this to bounding. You know, bounding the clique number, right? But I've only proved this kind of weak convergence statement. So it's very natural in this kind of situation to conjecture that the edges of the spectrum, the smallest and largest eigenvalue, should also converge to the edges of Kestin-McKay. That's what I'm saying here. And the exciting thing is that if hypothetically we could prove this kind of thing, then we would get, you know, depending on the clique size for which we can do this, we would get a sequence of. Do this, we would get a sequence of bounds on the clique number that scale like root p, but with a better and better constant decaying exponentially with the clique size you can deal with. And in fact, even doing this for triangles would beat this state-of-the-art root p over 2 bound. And if we could do this for any clique size, then, well, we would be able to say that we can make this constant arbitrarily small. So this wouldn't quite break the square root p barrier like I was, you know, hoping, but it would make. Hoping, but it would make a bunch of progress, right? It would show this soft kind of little O of root P bound. And so, how can we prove these edge limit theorems? It turns out to be hard, and it turns out to not even have been known, as far as I know, even for this old free probability model. So, I can prove kind of a partial result like this, for this is the same exact thing that was on the slide just before. That was on the slide just before. And I can do this for the largest eigenvalue. There are technical reasons why the smallest eigenvalue is different, but in fact, the largest eigenvalue would be enough even for the applications. And already, this requires some kind of recent fancy stuff. This is a paper by Collins and Matsumoto. So there's some fancy representation theory that gives you formulas for the moments of a uniformly random unitary matrix. And basically, there were no an asymptotics for this. There were known asymptotics for this, and this paper proves non-asymptotic bounds on this so-called Weingarten function, in terms of which you can express the moments, any particular moment of entries of a random unitary matrix. So it's only thanks to this that this is possible. And so probably it's not going to be easy to kind of de-randomize this and so on, but it's an interesting, kind of promising, I think, high-level direction for trying to improve on these bounds. So let me wrap up. So, let me wrap up very quickly with some open questions. So, first, of course, okay, we want to know: like, are those numerical experiments for SOS actually valid asymptotically, right? What's this exponent with which the value scales? And if it's strictly smaller than one half, like my small experiments were suggesting, then, you know, can we either look at these numerical dual certificates in there, or can we look at that graph matrix kind of heuristic and turn that into a formal? And turn that into a formal proof of an improved bound on the quick number. So we don't know how to do this, but maybe it should be possible. We only looked at degree four, also, it might be nice to look at higher degree, although probably quite technical. And as far as the second part goes, so there are interesting just even purely random matrix theory questions about how do we do this, these edge limit theorems for matrix models where we start losing these tools from representation theory for this nice, you know. From representation theory for this nice, you know, har measure of the unitary group. If we try to, let's say, replace that matrix U with the Fourier matrix or some other kind of nice flat-looking deterministic unitary matrix, how can we kind of reproduce those proofs? And finally, you know, the number theory question that would be, that we would need to answer to make all of this go through is how can we bound character sums that come from these kinds of powers of traces of some complicated matrices with character evaluations. Complicated matrices with character evaluations in there. Okay, thanks for your attention. Happy to answer questions. Thanks for a great talk. Bye. Yeah, just a quick comment about the bias. So in boot bound, which I'm assuming, like the two to the minus scale here, this reminds me of this function, which can also be this more vibration in some of that. Oh, it's a constant. So that like the spectral method. So that, like, the spectral method you will get a score of p, but then I want to include these constants. Ah, right, right, right. Yeah, yeah. And I guess it's exactly what P divides. Yeah, so I did, right, I didn't know if that had appeared somewhere, but yeah, right, this that, yeah, it makes sense. Yeah. But but is that does it make sense to consider like a field version of these when you say that? I mean I mean, I don't know. So it's just one graph, right? So, I mean, in some sense, right, we're asking for like certificates, deterministic certificates, that there's no large clique in there. I guess you could think about a random model where you put a large clique into a deterministic graph, right? Yeah, maybe, yeah, that would be interesting. I don't know. Maybe there's some kind of tricks that let you solve that easily. Yeah, I haven't thought about it. So, I think one question I have is: So, do you guys want to study the chromatic number for this? Um, no, I bet that somewhere in combinatorics lands, there are some pseudo-random graphs, maybe pseudo-random like sparse graphs, for which the chromatic number isn't known, but there's some prediction coming from pseudo-randomness. But, yeah, for that, for kind of But yeah, for that, for a kind of independent set for all of these kinds of quantities, I'm sure there are examples where the situation would be similar. Oh, I see. So that's perfect transitive drugs. Interesting. Yeah. No, we haven't thought about that, but yeah, could be. Is okay, okay. But this idea of localization and encoding theory is called Elizabeth-Solid method. Yeah, so it's also a common track. Okay, I don't know if you know about this, but uh, if the best upper bounds are called first linear programming and second linear programming, right, where there's like Del Sard style right, so then the first one is when you apply it to the whole graph, the second is when you apply it to chosen to the smart to chosen subgraph. Ah, I see, okay. So, uh, so that's uh, yeah, and okay, so what you proved was actually. Okay, so what you prove is actually it's a very so you prove a lower bound on an upper bound, right? In the first part, yes, right. So it sounds a little strange, right, to do this, but according to actually the guy who did it was Samaridninsky, and what he showed was a dramatic result because there's a lower bound, Gilbert Markshanov, right? And then for a long time, I mean, people hoped to close it, right? And then what he proved, he proved the lower bound, which showed that the first linear programming bound, right, even if. Right, even if you could evaluate it exactly, it would never give you anything better than Gilbert or Shah. So it was kind of a big blow to the program of it because we can't SOS exactly until you show that it's actually essentially validated correctly. And I think the same story, I mean, in other kind of packing types of problems, like in sphere packing, there's this old cone LK is paper giving LP bounds, and then much more recently using the kind of fancy red modular form stuff. Of fancy red modular form stuff. There's this red. It's the same. Yeah. And I think it's an interesting thing to tell you that some proof technique is kind of limited and to tell you that this one can only do so well. Okay, okay. Great. Thanks. Okay, let's thank Tim again for really nice copy break until 11. 