It's always a nice experience. Wonderful place. Alright, so let me start with a short outline of my talk. I want to tell you about shape analysis. I'll start with some motivation, describing some examples of where shape analysis can be relevant, and then I will get closer. I will get closer and closer to the work that we have been doing and try to devise numerical methods or techniques for shape analysis that are also inspired by deep learning, even though it's more like approximating neural network-like functions rather than using deep learning in its full glory. And then I will discuss also some work that we have been doing for shapes on Lee groups. One of the applications is in this context. And then if I have time, I also tell you a little bit about the signature, which is another approach that we have been considering. But it's mostly because I have exam numerical examples that use the signature, so we'll have we've tried to to tell you what that means. To tell you what that means, okay. So, let's see if I understand how this works. Okay, um, all right, so the first observation or the first viewpoint I want to make is maybe Euclid metrics are not always the most appropriate choice when you want to do data classification for classifying things. So, it's an example. Hopefully, it looks like. My goodness. So my example, let's see. Please. Click on the slide. My example comes here from this simple. comes here from this simple problem in neural networks, classification with neural networks. So you can look at this spiral data on the plane and make a neural network to classify the blue points, separate the blue points from the red points. And if you look at what the neural network does, I mean this is a simple thing, it's a red network. I mean, this is a simple thing, it's a ResNet, but what does it do? I mean, what it does is transforms the data into a new configuration where you can easily separate it. So it's not, if you look at Euclidean distances in between the data in the original configuration, then nearby points should be classified differently. For example, there are red points which are very close to the blue points. Are very close to the blue points in Euclidean distance, and with the Euclidean distance, you wouldn't do a good job. On the other hand, if you first transform it into the new configuration, which is the job that the neural network is doing, and then you use the Euclidean distance, that's a different story. Then you get it quite right. So this is something similar to what is going to happen in my applications as well, in my Applications as well, my approach. Oops, well, it was too fast. Shape analysis. What is shape analysis? You could say this is a framework for treating complex data and obtain metrics on data spaces, spaces of data, appropriate ones. Examples can be curves, spaces of curves, of time signals or Or surfaces, images, probability distributions, depends what you are dealing with, your application. I mean, it's not that this is a framework that works everywhere, but it's a framework that works in many different settings, as long as you can look at your data as unparametrized, so independent on the parametrization. So, like, for example, in this case, you want to recognize an object. You want to recognize an object or to identify an object like this leaf here, and you look at the contour of the object, and that's a curve. But this curve, it doesn't matter which parameterization you use for the curve, it will always represent the same shape. And so that's why for recognizing the object, you should sort of look at the curve independently on the parameterization. But this can be used for surfaces as well. And here it's an example. It's not my work, but it's kind of interesting. You can take a surface and make a deformation of surfaces into a different horse with a longer neck here, or you could make this animal here to make movements, to move around, to form the shape in the sense that you make Form the shape in the sense that you make it move. But this is the application involving curves on Lie groups and this is for analyzing human activity or for computer animation, if you like. So the data here are time curves that describe Curves that describe the movement of a skeleton. And for each point here marked on the skeleton, there is a rotation matrix, a tree by trick rotation matrix. So what you have is to describe the pose, to describe the position of the character or of the human body, you have a bunch of rotations that describe how How these bones are posed with respect to a reference point. So you have a curve with the values in SO3 to the n. Here is another example which is something quite interesting, but I've not worked with it, but I think it's still interesting. It's interesting to me because it's It's interesting to me because the lead group involved is SC3, it's not SO3, it's Roto-Translations. And we have been trying to do tools for SC3, but we have never done applications on SC3, so it was interesting to look. But this is also quite different. So what you want to do here, you're looking at paleobiological reconstruction problems. So you have a bunch of bones from extinct. in extinct animals or dinosaurs or whatever, and you want to know what are the possible movements that the bones can have or how they can work together. And it turns out that in this kind of applications they've been trying both to use SO three and SE three and SE three has some advantages, gives uh better results. So it's a better modeling tool A better modeling tool for this, it makes a difference. Okay, so let me. So, I hope this convinces you that there are, in fact, diverse applications for this sort of framework, where the reparameterization invariance is crucial, or it can be an important tool that you have to look at. So, let me go back to. So let me go back to the case of curves, because the curves are the easiest way to handle it's less technical than the rest, so I will keep to that. And I will consider shapes as amperemetrized curves or surfaces, taking values either in a vector space or on a manifold. And you should use shapes when it's natural to use them. When it's natural to use them. So, when reparametrization variants is really important. So, there are many ways to define shapes, but the way we do it is by starting considering the space of immersions. Here, to begin with, we look at curves which take values in Rn, and then I will discuss Lie groups and manifolds later on. Later on. So we take look at curves which are defined on an interval with values in Rn, C infinity, and with non-vanishing derivatives. These are inversions. And this is an infinite-dimensional manifold. And on this space, we define an equivalence relation, so we say that two curves are equivalent if they differ only by the If they differ only by the parameterization. And with this, we can make a quotient space, so the quotient of the immersions with the diffeomorphism group. Orientation-preserving diffeomorphisms are the reparameterization, the way we think of the reparameterization. And this gives us our shape space, so the space where we, the shape space. And in most applications, all Applications, okay. One slide. This is dedicated to Takaharu, who asked me this question last time we met, in person, at least. And you asked me about the topology of this, and here is some information about top the topology. Uh so these infinite dimensional manifolds are a little bit uh s s somewhat strange objects. Somewhat strange objects. They don't have the usual intuition that you have in final dimensions. But we are lucky enough to have an expert on this in our department and I've been collaborating with him. And so if you want to know about if I want to know about this, I go to him and he starts a story, a long story, tells me a lot about this. Okay, but what about this topology? What about this topology? The topology in C infinity is the compact open topology, which means in fact you start with just the continuous curves and the topology is generated by these sets here, which are defined by other two sets, K and U. K is a compact in the starting set here, and U is an open inner n. And you're looking at all. And you're looking at all the functions, continuous functions, that sends the compact into this open set. And these sets here, they generate the topology in the continuous functions, and they give to these continuous functions the structure of locally convex space. So you can use calculus in this way. In this way. Turns out that the immersions are an open set in C infinity with respect to this topology, so they are a set mark. Okay, so what you really need in order to do anything in with these shapes is to have a way of defining a metrics, defining distances. Defining distances and have them also have a way to compute them in the end. So, what do you do? Well, one way of defining a distance is to start with the distance on the pre-shaped space and then to take the infimum over the diffeomorphism group of all the distances in between, say, two curves, Czech. Say two curves, C0 and C1, reparametrized. And this turns out to be a good definition of distance on the shape space, provided that your DP metric, so the metric on the pre-shaped space, has this property here. So it's invariant under the diagonal action of the disteomorphism group. So if you reparameterize both the curves, Reparameterize both the curves that you're trying to compute the distance of with the same reparameterization, you shouldn't change the distance. If that's happened, then what happens here is that this definition here is independent on the choice of representative of the curve, of the equivalence class. So it gives you a well-defined. What's the problem here is that you have a nasty, I would call it nasty. A nasty, I would call it nasty optimization problem to solve in to to in order to compute this. Okay, so but but then you need you need a way, let's see, a way of defining this dp on the pre-shaped space, and you typically do it by defining or searching for a Riemannian method on the pre-shaped shape space. And then, by means of the Riemannian metric, you can define geodesics, you can perhaps compute them also. And the length of the geodesic is the distance in between two objects. So, if you want to find the distance between C1 and C2, you compute the geodesic between C one and C two with respect to your Manner metric and compute the dist the length of the geodesic. That we see distance. That we see this. Here is an example. I don't tell you what this metric here is, I will discuss it later. But so here is your curve C1 and this is C2. And computing the geodesics means, or the geodesics is a deformation of the one curve to the other. And here the dots mark the parameterization. Parameterization. So these two are equidistant dots here, so they are parameterized in the same way. If we change both the parameterizations of the curve in the same way, so we apply a diffeomorphism, phi, here, and change a parameterization, then the deformation doesn't change. But if I change only one of them and I keep the other one the same, then also the deformation changes. Change. So the reparameterization invariance is this property here. I'm not allowed to change the reparameterization one way for the first curve and the other way for the other curve. Okay, so in order to get reparametrization invariant distance on the space of immersions, we have a favorite way of proceeding, which is also Proceeding, which is also hinting us what to do for an algorithm. And this is the following: What we're going to do is first we do an appropriate transformation of the curve and then we use a well-known or familiar way of computing the distance, so the L2 method. And what is the transformation of the curve? Here is peculiar error, it's given in the case of vector spaces. Is given in the case of vector spaces. So we have two choices here. The first one is the so-called square root velocity transform, and is simply take the derivative of the curve and scale it by means of the square root of the norm of the derivative. And the second one is the so-called Q-transform, or it's known as Q-transform. What you do is just you scale the curve by mean of the square root of the L. By mean of the square root of the pyramid. And then, as I said, the dp distance which we define is the following. You take the curves C0 and C1, you transform them by Q, and then you take delta distance between the two obtain transform curves. It turns out that this is a good definition because it leads to a DP which is reparametrization of value. And so it leads to a shape distance which is well defined, as I said before. The crucial thing that you have in these two transformations that I've shown you. That I've shown you is that they have this property here: this equivariance with respect to reparameterization. So, by means of this property, so this is a relatively easy thing to prove, and by means of this property, what you can show is that indeed DP is a reparametrization invariant. It's crucial for that. Um so some uh interesting things here is that um the SRVT it leads to indeed a Riemannian metric, it's the distance due to a Riemannian metric and it's a sole type Riemannian metric on the pre-shaped space. In that case, you can prove that you I don't think there are similar results for the Q-transform, but we For the Q-transform. But we like the Q-transform because it's easier to use with the surfaces. We don't like too much the well, there are some pitfalls with the generalization of the sRVT to surfaces. Okay. Okay, so now let me get back to this optimization problem. Problem, which I called nasty optimization problem a while ago. And okay, so we have our distance on the shape space between the equivalent spaces, and it's given by the infimum over the diffeomorphism group of this distance here. And now we have a definition of the distance, which we go through this transformation and take that to distance. So we now use the equivariance property. I call q0 here, q1 the transform curves, and I can rewrite my objective function here in this format here, more common format. I've just rewritten my problem into a different problem equivalent. Okay, now I have to solve it. Okay, now I have to solve it. It's an infinite, it's an optimization problem of the diffeomorphism group, and now I have to find a way of solving it. So first of all, I'm going to make use of the Lie algebra of the diffeomorphism group, tangent space at the identity. This is going to be a tool. Possibilities. One way to do this. One way to do this is by using dynamic programming. And this works well, but it's a bit expensive. Another way is to try to make a gradient descent algorithm, and this is what we have seen. We have taken as a starting point for the case of surfaces. We were trying to implement this in the case of surfaces. There was a a gradient descent algorithm we wanted to do. Descent algorithm, we wanted to do understand that. But our favorite way of doing this is to is inspired by neural networks, is given by composing elementary differential diffeomorphisms. And these diffeomorphisms are just the identity plus the rectangle field, which is parameterized by these parameters beta here. V i's here are the basis of the Lie algebra. Of the Lie algebra, and of course, this is an infinite-dimensional space, so we have to truncate that. But so we have diffeomorphisms like this, and we compose L of them, like L layers of the neural network. Okay, let me say some just a few words more about uh the the various type of approaches here. Type of approaches here. The DP is essentially the following. You, at least the way I understand this, you make a grid on the box on 0, 1, 0, 1. These are curves now, so hithiomorphism from 0, 1 to 0, 1. So you make a grid, a regular grid, and then you construct all possible piecewise linear strictly increasing functions. functions, okay, one-to-one functions. And it turns out that you have a finite number of them, but this finite number increases very fast when you increase the number of nodes in your region. And what you do, you have a clever way of solving the optimization problem on this set. So you just find a minimum there, and you have a very clever algorithm for doing that. Algorithm for doing that in a good way. But as the number of nodes increases, this goes very fast. The complexity increases very fast. Gradient descent will look like this. We are doing gradient descent on a Lie group, infinite-dimensional Lie group. So you'll have to use the composition. Those who are familiar with the Lie group gradient descent will recognize, will understand. Set will recognize, will understand this quickly. But so you have a previous diffeomorphism, synchronous approximation, and you compose it with the perturbation, with an update. And the update is the identity plus a vector field, which is the negative gradient. You cannot tune the negative gradient, you have to truncate it and sort of make a projection of the gradient on the. On a finite-dimensional subspace of the Leader. Our method instead is as follows. So it's similar to the gradient descent, you can say, but we compose several of these. We have parameters beta J here, beta J1 to beta J L, depending on how many of these different morphisms you compose. And we optimize overall. Optimize over all these parameters. So, this is our model for phylodimensional diffeomorphism group. And it's nice because you have a group to start with and then you make compositions when you construct your finite dimensional model, you can say. Okay, let me show you just as how this works. Just tell us how this works. In a few examples, so here we have the circle, and we have two different parameterizations of the circle. And we want to, so here it's equidistant points, and this parameterization, we know what it is to begin with, and we want to recover it with our algorithm. And what you see here is the convergence of our method compared to the convergence of the gradient. compared to the convergence of the gradient descent. Gradient descent doesn't do a very good job. It it gives you the rough idea of how this diffeomorphism is, but not much more. While our method is giving you a pretty good match with the black line, which is the original reponentization. Similar results we get for surfaces. And yeah, so same similar case here, we just this is the reparameterization and this is the found one, the one that the algorithm computes. Convergence, numerical convergence, no, we don't have uh results for the moment. Results for the moment, uh working on it, thinking about it. Um, so here what we we show you is uh the error as the number of basis elements for each diffeomorphism increases and as the number of layers increases. So, both of them need to increase simultaneously in order to have a trick error. So, in one case, you increase So in one case you increase just L and these different curves are the values for different values of M and in the other case you increase M and the curves are the different ones for the different values of N. So both the first line here is just for the curves and this is for the surfaces. And everything is implemented if by torch so it's 10 to the 10 to the minus 6 is single precision, it seems to be. But you need to increase both L and M simultaneously. N here is sort of a function of N. Here is data generation first attempt example. So we So we have two digits in the MNIST database, and we are mapping Digit 7 to Digit 2 by using two different methods. The first one is just linear combination, and as you see, this doesn't work extremely well because it makes it so just that the 7 is slightly disappearing little by little while the 2 is appearing. So it's not doing. Appearing, so it's not doing a deformation really, while the second one is not going like that, it's really trying to deform. And the second one is, so they're both quite simple. The first is a convex linear combination, but the second one is exploiting the optimal reparametrization. And here is the optimal reparametrization, which is not trivial. I mean, it's it's not easy to compute obviously. Easy to compute our second. But with this composition of bifeomorphisms, you can do okay. Some expressivity results. I think I will not talk too much about this. So, what we can show is that finite composition of diffeomorphism of this type here, so identity. Type here, so identity plus F L, where F L is a one Leap Sheets vector field, can be used to describe the whole group of diffeomorphisms, provided you're looking at diffeomorphisms that fix the boundary, which is a bit of a restriction, or diffeomorphism of the Q, which is not such a restriction. I mean, it's an interesting case. So these are our So, these are our. So, what you need is a finite number of compositions. This is not so surprising because you have similar results, for example, in control theory and Agra Chev as similar type of results. Okay, the clue or the main thing here is that for describing the diffeomorphism group. The diffeomorphism group, you need just one chart. And that this map here, whose inverse is the Euler map, the Euler method, identity plus F, is describing, is this chart, is the chart map. So you can use things like this to describe the whole diffeomorphism group, provided you find the right vector field. The right vector field here. You get use the right vector field. Now it turns out that these vector fields are not so easy to characterize. Don't really know how to characterize them. So it's convenient to instead restrict to one Leap Sheet vector field because we can do, we can construct one Leap Sheet vector fields, make sure that they are one Leap Sheet, and this will guarantee that this mark here, identity plus. This mark here, identity plus F, is going to be invertible. That's the crucial thing. At the same time, you have results that tell you that if you restrict to one League shift vector field, then you get a set of functions which is a zero open neighborhood of or in the Lie algebra. And this is very important for concluding that that you can cover the whole diplomacy. So you restrict, you don't look at all the vector fields that you would like to look at, you restrict to the one leap sheets, and the trade-off is that you have to compose several of them instead of doing it just with one Euler mouth. Okay, um last five minutes. I will Five minutes, I will discuss a little bit this problem of manifold value curves. So I've always been quite fascinated by this square root velocity transform approach because it's very algorithmic and it has very nice properties. So I liked it. And so we tried to, when we had to deal with When we had to deal with curves on Lie groups, SO3, we try to generalize these two Lie groups. And then what you can do is to try to think about it and desiccate the process. You have to take the derivative of your curve, differentiation, and then you have a scaling. You want to generalize the Lie group, you need the third part here, or two manifolds, which is uh transport. Transport. You have to bring the curve into common space. This is pretty simple in Lie groups, if you have curves on Lie groups. We have done this also on homogeneous manifolds, but it's a bit more technical. I don't want to discuss that. But on Lie groups, if you're looking at C infinity curves on Lie groups, then it turns out that this is an infinite dimensional Lie group. Infinite dimensional Q and the corresponding Lie algebra is the C infinity curves on the Lie algebra. So this is nice. And the map, the exponential map from the Lie algebra to the Lie group is the so-called evolution operator. And the evolution operator is simply the operator that sends a curve on the Lie algebra to the solution of this differential equa linear differential equation. Linear dissemination equation. You have an infinite-dimensional Lie group, you have to, your exponential will look a little bit different, a bit more powerful if you like, and you have to solve the differential equations for computing the exponential. This evolution operator map has a as an inverse which is the right logarithmic derivative. In this case, right, because I'm taking right multiplication here. Right multiplication here. So I could also use left multiplication, but let's stick to right multiplication. So this is what we use for the transport. So as set, differentiation, transport through the right-logarithmic derivative and scaling. That's the three parts of our generalization of the square root. Generalization of the square root velocity transform to equals, which is here. And there's this nice expression by means of the electricality therapy. Okay, you have nice properties for the corresponding. If you do the same game as before, you transform by SRBT and then take the L2 metric or the algebra, you get very nice properties for the magnitude. So it's indeed a Metric. So it's indeed a metric on the Lie algebra and it's globally defined. We did that some years ago. Okay, I want to show you. You can use this for motion blending. Use the sRVD, transform the curves, make a convex linear combination, and then transform back. And this gives you your motion blending. For numerical methods, you have to discretize. And when you discretize, if you look at, for example, at piecewise exponential curves, so like the analog of piecewise linear approximations of your data. If you look at piecewise exponential curves, then all these operators here that look scary or computationally demanding to begin with. The mind to begin with, they simplify. So, in the end, you have just to compute a bunch of simple SO3 exponentials, which is not necessarily very cheap, but it's much better than computing the solution of the differential equation. Okay, here is an example. We designed a gradient algorithm for making closure of curves. So, if you want to So if you have an animation and you want to repeat the same animation again and again, there are discontinuities at the beginning of the curve, in between, joining the curve like this. So if you want to smooth out this discontinuity, you can use a gradient descent algorithm which is based on this. Okay, I'm not defining the signature. The signature is a different way of dealing with Way of dealing with this data. We have played around a little bit with that as well. And I'll show you now some classification result. So here is a method that we propose or that we use where the curves are taking into account the geometry because these are animation data. These are animation data. So we want to classify walking, jumping, and running animation. And the way we do it is by using distances that they are either computed by the signature, comparing the signatures of the curves, or comparing the SRBT, using SRBT and the And the shape distance in that way. So, here is the result that we have with the signature, and you have clusters which are quite good, but we don't understand very well the convergence of this. So, we don't know how to make these results better and better. We don't understand it, really. That's the point. So, we're still working on this. For the SRV teams, this is what you get, and we then That this is what you get, and with the dynamic programming. So, we didn't implement here yet this other approach with epithomorphisms. But you see, here, if I want to, it's just to increase the resolution. My problem will take a long time to be solved because I have this dynamic programming to to do, but but I know how to improve my result if I want to. Okay, so that was uh what I wanted to say. What I wanted to say. Thank you very much. Question for Elaine? Thank you for a very nice talk. So you saw you found the Morphic transformation. Even though I saw change one shape to the other Morphic set is interesting. The Morphic Sense was interesting. But you be able to do other things like some shape analysis of objects or finding different, for example, discrete representations of objects in some way or does it give you like the metric that you obtain can be used for something like this? I don't know exactly, but I think you Well I think you can use this for predictions, for example. So these results that you have seen in the classification case here, you know, you look at the distances of all the data, pairwise distances, and then you have a good way to plot them. But you could make some sort of approximation of the whole surface here and try to Surface here and try to make predictions, like using, for example, radio disease functions or things like that. We have been thinking about it, but not done anything. This will compete with neural networks sort of. But otherwise, I think the deformation would be nice, so blending would be nice, it would be more in the regime of generating data. But I think it's important. But I think it's important the geometry of the metrics. I mean, to use good metrics is important, and also that they are geometric. You cannot guarantee just you have a metric, this will work no matter what, but I think it's still a good hit. For example, the question is: how is it to use this for 3DOP? I mean, I don't have an army of people who can do this. The rest of the army are doing other things. But it would be nice. Yeah. So I couldn't resist, you know, asking the following question related to the the the data s uh the MNIST data set. So what happened if you try to merge or to morph uh say seven to four? Uh, say seven to four. What else is so here? I know, I know, I I don't know. We didn't do extensive uh testing, but that would be nice. That would be nice to make it even seven to nine, it would be just what happened to the future. I think to do experiments. Experiments, you need to work hard normally, and it takes a lot of time, time which I don't have. Clever people, clever people as well, very clever people. So that's not really a point that I would like to put as the ideas. But I'm interested. Yeah, this is the outside of your forte, given. But this is very interesting. And this section might relate to the applications. Have you thought about doing anything with this related to dynamics? Because dynamics is littered with, you know, this is typiomorphic to that. And like Topkin's embedded Ethereum tells us chaotic characters are diffiomorphic. It would be really cool if you could do some classification of chaotic characters. So, you're thinking of solution of differential equations? Yeah, yeah. So, for example, Hawkins' embedding theorem tells you that if you take the partial activations of a chaotic system, if you delay embedded in a high enough dimension, you can diffeomorphically reconstruct, say, the Lorentz butterfly. But it won't look like the regular Lorentz butterfly that you and I know because it's diffeomorphic to it. And so the reason I thought about this is. So, the reason I thought about this is there's a guy at UT Austin who just did some experimental results where he's trying to classify these things. But I think you might be able to give your opinion on it. Sounds interesting. Yeah, I mean, it's something to think about. It might be a little bit more. I mean, then you're dealing with fractal dimensions and stuff like that. You'll have to understand what what how to think about the data, right? But the manifold is from many dimensions to fractal. Are you saying it's not going to work or it's going to work? I'm saying it's it's okay. It's it's not a thing. There's quite a lot of issues and this is unlikely. Yeah, I mean I I don't know if it's gonna work. I said have you thought about it? Maybe I'll Discussion, maybe after the coffee. Let's take a speaker here.