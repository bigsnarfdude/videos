So, first off, I apologize to anyone who looked at the schedule a week ago and saw a totally different title. I had planned to talk about a combination of non-parametric learning with deep learning. However, that was something of a speculative absurdity. That was something of a speculative abstract, and we're still working on the final experiments. So, Peter very generously let me change my abstract at the last minute. So, sorry if you really wanted to hear about normalizing flows. Hopefully, we'll have the results for that wrapped up soon. And so, I've actually not been doing much Bayesian non-parametrics over the last 12 months. So, I'm actually going to talk about some work that. Some work that we got published last year. So, apologies for it not being quite as new and exciting as some of the other work. However, because it was during COVID, this is the first time I've actually written slides for it. So at least something there is relatively new. So this is going to be joint work with Avinava Dubai, who is at Google, Michael Minya Zhang, who's at the University of Hong Kong, and Eric Zing at CMU. Jing at CMU. And I'm going to be talking about a way of doing MCMC for a very general family of Bayesian non-parametric models where we want to make use of distributed computing. So we want to partition our algorithm across either multiple threads on one computer or multiple computers in a Computers in a network setting. And I'm going to kind of motivate this with like a toy example of the sort of problems that we might see in Bayesian non-parametrics. You have a bunch of data and maybe you want to do a clustering model, but it's not obvious how many clusters you should have. So you're like, all right, we got to do it the hardest way. Like, all right, we got to do it the hard way. We're going to bring out a Bayesian non-parametric model and use infinitely many clusters of query. However, depending on how you look at it, you can either think of this as being a very hard problem, right? I don't know how many clusters there are, or as a fairly easy problem with a little bit that I don't know how to do. So if I look at this cluster, I can see there's like clearly one big cluster there. Clearly, one big cluster there, one cluster here, etc. And then I've got a whole bunch of maybe small clusters, maybe noise, that it's less obvious what dimensionality I should be using. So I can kind of think of my problem as being, well, there's a fairly easy clustering problem that I maybe don't need the big guns to solve. And then something harder with a lot of smaller, low. Harder with a lot of smaller, low-occupancy clusters that maybe is a bit more challenging. As a slightly more realistic example, here's a bunch of hand-drawn digits. You can imagine modeling this by clustering them into types of two. And there's probably two obvious clusters, right? There's the people who write their twos without a loop, and there's the people who use a loop, right? And I probably don't need to do anything. I probably don't need to do anything particularly fancy to find those two clusters of twos. But then you've got people like this who have their own idiosyncratic ways of doing twos, or like this guy over here, that it's maybe harder to know where to cluster those. So the intuition behind this and behind several other inferences. Several other inference directions they've been looking at is to say, okay, let's split our problem into two. For the big clusters that are fairly obvious, that are going to be found with any clustering method, I can use a cheap parametric method. I maybe don't even need to update it that often because it's fairly easy to find these clusters. These clusters, and then for all of these smaller, lower occupancy clusters, I can use something a little more specialized, maybe a little more computationally intense. So the scope that I'm going to be looking at is a very general class or cohort, I don't know, of Bayesian parametric models. Know of Bayesian non-parametric models for exchangeable data that are based on completely random measures. So, either latent feature models based on directly on completely random measures like the beta-Bernoui process, normalized random measures within reason. I'm mostly going to focus on the Drichlet process there, but also some more complicated variants such as Pitmanure process or hierarchical Pim and your process or hierarchical Dirichlet processes. Now, at their core, these are all some sort of infinite dimensional random measure, discrete random measure, that we're going to use to parameterize some sort of likelihood. And with that underlying similarity, we see the same families of MCMC crop up for each of these. MCMC crop up for each of these types of model. So we can either take a collapsed approach where we take our infinite measure, whether that's a beta process or a D Richelieu process, and integrate it out to get an exchangeable sequence. And then we directly work by updating those observations, those either latent features. Those either latent feature allocations or the cluster allocations. Or we can instantiate the latent measure in what's called an uncollapse sampler and iterate between updating the latent measure and then updating the cluster or feature assignments. Obviously, we're not going to integrate, sorry, obviously we're not going to represent the entire infinite dimensional object because that's impossible, but we can either use Possible, but we can either use an approximation or do something like slice sampling to kind of marginalize out some of that latent measure. And both of these approaches have their merits, right? In some scenarios, one will work better, in others, the other will work better. But both of them kind of struggle if we think of a distributed setting. Setting, so multiple machines where we don't want much communication between the machines, and high-dimensional data, right? So, our observations are living in a very high-dimensional space. Now, the collapse samplers aren't great in that setting because of the parallelization. Once you've integrated out the latent measure, everything depends on everything else. So, to get the conditional probabilities for your sampler, you need to know. Your sampler, you need to know what's going on with all of your other latent representations. Each data point depends on every other data point, which is a problem if you need to spend some expensive communication to hear from another data point. The uncollapse methods, on the other hand, are relatively good at being paralyzed because conditioned on a latent measure. Because conditioned on a latent measure, the allocation probability, sorry, the feature allocations are IID. But in general, the uncollapsed methods don't work very well in high dimensions because they kind of generally rely on sampling new features from the prior or for something like the prior. So they generally work pretty well once you've Work pretty well once you've got a feature going in your sampler and then you're deciding exactly what the parameters of that feature are. But they tend to operate by proposing new features from the prior, which in high dimensions are likely to be arbitrarily far from the data. So we've got two approaches and they both fail in And they both fail in one setting. So the uncollapse method is pretty good for updating the existing features, but it struggles a little when we're incorporating new features. On the other hand, a collapse model tends to be better at finding those new features, those new parameters in the sort of small case, but because it But because it has the inherent dependency, then we tend to get poor mixing. If we have one large cluster, it's hard to break it into two. And also, it's hard to distribute because we have this dependency on the entire set of sufficient statistics. Okay, so we're going to take each of these approaches and use. These approaches and use them for what they're good at. So I've got my big already instantiated features that in my current state of my MCMC sampler are already occupied by a lot of data points. I'm going to use that uncollapsed method in that case. I'm going to instantiate the latent measure. But for the low occupancy features, Low occupancy features, the ones that maybe pop in and out of existence in our sampler, I'm going to use a collapse method for those. And I can do this because we can split completely random measures into two or more than two, but I'm going to stick with two because I've only got two types of method here that you can use. So I'm going to take my underlying random measure and I'm going to split it into a finite dimensional measure. Finite dimensional measure, which is going to contain those blue features, the ones that I've previously seen. And I'm going to have an infinite tail that I'm not going to instantiate. And what I'm going to do is for the first part, the previously seen features, I'm going to take a distributed approach. I'm going to make use of parallelization and I'm going to keep instantiation. I'm going to keep instantiating those features. I'm going to use an uncollapse sampler. For the low occupancy tail, I'm going to use a collapse sampler. And ultimately, I'm not going to distribute computation there. I know that the collapse methods don't distribute very well, so I'm just not going to do that. So, kind of to set the stage of what To set the stage of where this lies amongst other distributed MCMC methods for Bayesian non-parametrics. Here I've got like a small table comparing a subset of the methods that exist. So at the top here, the parallel Deutsche Leigh process mixture model and Pitmanure mixture model papers. These are collapsed inference algorithms that partition the clusters. That partition the clusters onto multiple threads. So each thread in our processor deals with the entirety of a subset of the clusters. And this allows us to distribute computing because we know the relevant sufficient statistics. But because we're distributing the clusters, we need to periodically. We need to periodically reassign clusters to processors, which involves moving around data. So, this isn't going to really work unless we've got a shared memory storage. So, we can do this in a multi-threaded way on a single machine where every thread accesses the same memory and we don't have an additional cost to reallocating data to threads or much additional cost. But if we have like a cloud computing scenario, But if we have a cloud computing scenario, we're going to have to either reload the data from memory or physically, well, not physically, you know what I mean, move around the data around the network. They're also highly specialized for the Drichlet process or Pitman Yule process here. So, for example, there's not an obvious way to use them for beta-Bernui processes. Next up, we've got a couple of approximate methods. So, the first one I've got here, and this is an example of several of this type, is an asynchronous collapsed algorithm that, when communicating sufficient statistics between processors, uses approximate sufficient statistics. So, you're a single processor is updating. Processor is updating based on approximation of the other processors' sufficient statistics. So, this can be done in a multiprocessor setting, but it's not going to be asymptotically exact. Similarly, there's a parallel inference algorithm for the IBP that uses an approximation when adding in new features. It overestimates. It overestimates the probability of adding in new features into the representation, which again isn't going to give the convergence results that we would want from like a standard MCMC algorithm that wasn't distributed. The final one here is a distributed slice sampler that uses a really neat trick where what they do is What they do is they share random seeds between the processors. So when two processors both sample a new location from the prior, they're going to sample the same location. So it's kind of aligned the randomness between the different processes so we don't need to communicate. As we'll see, that doesn't. As we'll see, that doesn't, like many uncollapsed methods, tend to work that well in a high dimension because we're still sampling from the prior. And it's also not obvious how this would extend to different distributions. All right, so here at the bottom with all of the nice ticks, I have our method. Let me actually tell you what our method is. It's kind of more a framework than a specific method. Method, and I'm going to start by describing it using a completely random measure example. So, for example, let's consider a beta-Bernouille process. So, I've got a beta process, completely random measure with atoms between 0 and 1. These are often used for latent feature models, where each atom location corresponds to a feature, the atom sizes. The atom sizes correspond to the relative prevalence of that feature. And then we can combine that with a Bernoulli process, which basically goes through all of these infinite features and includes them or doesn't include them into our random measure Z with probability mu k. So this gives us something that up to the normalized, up to kind of a normalized. Up to the normal, up to kind of a normalizing rearranging constant is the Indian Fay process. It's a way of building a non-parametric latent feature model. You can do similar things with a gamma-Poisson pair or a beta-negative binomial. So if I have a beta process prior and then multiple samples from a Bernoulli process, then the posterior. Process, then the posterior over my latent random measure is again going to be a beta process with updated parameters. And this is going to be true for any conjugate pair, gamma Poisson, etc. There'll be a similar relationship. Now, because the beta process is a completely random measure, that means we can partition it however we want. Concretely, we can partition it if Concretely, we can partition it into a finite random measure that just has support at the fixed locations, right, the points where we've seen the data. Sorry, this should be, theta here should be Z. I apologize. So B1 is just going to have locations, have atoms where we've seen the data, and then an infinite tail. And then an infinite tail that is going to have the random location atoms, which is going to be basically just an appropriately scaled version of our prior beta process. And then we add them together, we get our full posterior. So, what we're going to do is say, okay, this finite bit, that's easy to work with. We don't need to do Easy to work with. We don't need to do any non-parametrics, it's just a bunch of betas. The infinite tail is going to be infinite, so a little harder to work with. We're going to have to use a specialized inference algorithm. So we could imagine coming up with a hybrid sampler. So I'm going to split all of my infinitely many. Features in my posteria into the main features and the tail features. The main are going to be either everything I've seen at this point in the sampler, but they could also be everything that I've seen at least j times in the sampler. So I can then iterate through each data point and then iterate. Data point and then iterate through each feature to sample its allocation. I'm first going to sample actual, let me backtrack, let me backtrack our ecstasy line. So I've got my main finite, I've got my tail infinite. For my main, the posterior probability for each feature is just a beta random variable. So I can sample that here. m sub k is the number of times I've seen the kth feature. Seen the kth feature and obviously is my total number of observations. Then, as I go through sampling each data point and each feature, I can either use that uncollapsed representation, if it's one of those main features. So pick an entry for Z. Okay, clearly this is wrong. Clearly, that should be 1 minus mu k. Mu k but with probability proportional to mu times the likelihood or zero with probability proportional to one minus mu times the likelihood. And then once I've got through all of the main features, I can then use a collapse sampler to sample the tail features. So sampling those remaining features based on the raw counts. Counts and sampling uninstantiated features using a Poisson random variable. So on the left, we have a standard uncollapsed sampler for the beta-Banui process. On the right, we have a standard collapse sampler. We're just going to pick and choose based on whether we're in the main and the tail. And then once you've gone through this, either once or a few times, we can re-decide which features belong to the main part. Which features belong to the main part and which features belong to the tail part. Now, I'm not going to use this hybrid algorithm as it is. This is kind of a stepping stone to get to a parallel algorithm. So what might that look like? Well, naively, I could say, okay, my main section at this stage in the sampler is going to be all of the features I've seen so far. So far, and my tail features is going to be everything I haven't seen so far. Again, I'll sample probabilities for my main features, I'll split my data across my processors, and I'll send the probabilities for the main features to each processor. Then I could do exactly what I did before, right? Sample the main features using the unclapsed, the tail features. The uncollapsed, the tail features using the collapsed, and then periodically redefine what is the main and the tail, resample the beta random variables, boom, boom, boom. The only problem here is for the tail part, my probabilities depend on the sufficient statistics. And that's not ideal because the sufficient statistics depend on what's going on on all of those other. Depend on what's going on on all of those other processes. I don't know how many times the other processors have seen a given feature. This is kind of why collapsed inference is hard to paralyze. So that's not going to work. But I can fix this by saying I'm only going to let one of my processors add new features. So I'm going to pick one special processor that Special processor that can sample both the main features, the ones I've seen before, and also features that have not appeared before this iteration. Now I can do that because I know the number of times I've seen all of these tail features outside of the current processor is zero because no one else is allowed to add them. So that's now valid. I'm not approximating these processes. Approximating these probabilities. I don't have alignment issues anymore with what if two processors happen to sample a new feature at the same time. And then when I periodically regroup and sample the betas for the main features, I can also pick a new processor to be the one that adds tail features. So that can cycle through all the processes. The processes, so every data point has a chance to start off its own new feature. The only downside of this is I'm kind of wasting a bit of time, right? So if only one of my processors is allowed to add new features, if I'm starting from far too few features, it's going to take a while to build up to a reasonable number of features. To a reasonable number of features. So, what we can do instead is kind of a pragmatic approach, which is to start with the naive approach and then switch to the correct approach later. And so what we can do to kind of speed up the burn-in part of the algorithm is start by doing the sort of free-for-all every feature. Free-for-all, every feature can add new features. Sorry, every process can add new features, and then gradually anneal that down so we finally end up with the asymptotically exact sampler where only one processor is allowed to add new features. So, interestingly, the naïve approach here is almost exactly what that. Is almost exactly what that parallel IBP paper that I talked about a few slides ago does, right? That paper doesn't take into account that it's approximating the sufficient statistics for the tail. So, just kind of like to explore that pragmatic warm start initialization. Here I've got Here I've got this is on synthetic data. I'm looking at how the number of features and the test set log likelihood varies with time with between one and 128 processors. And we can see that if we have here on the right, we're letting all the processors add in new features. Processes add in new features, we end up kind of overshooting the appropriate number of features, right? Especially on this 128, we've got 128 processors all anxious to add in new features, and we end up overestimating the number of features. When we compare to, for example, the cold start, which is doing it. Doing it exactly, right? Only allowing one process to add a feature. What we find is that the warm start kind of allows us to ramp up adding some features near the beginning, kind of getting out of that initialization point. But by ramping down, we can still get similar distributions to what we had if we started correctly from the beginning. Now, practically, even though the incorrect one is overestimating the number of features, it doesn't actually have that big an impact on the overall log likelihood that we get. So, you know, if you want to do the not quite correct thing and use that naive method, Naive method, and you only care about log likelihood, maybe it will work okay. There's just no guarantees there. And if we're taking the time to do MCMC, we probably want some sort of guarantee. All right, so when we move on to normalized random measures, it gets a tiny bit more complicated, but really not that much more complicated. So if I have a So, if I have a Dirichlet processed pisteria, I can write this as a beta mixture of two Dirichlet processes. Any Dirichlet processed, I can write as the beta mixture of two Dirichlet processes. But when I'm looking at the posteria, I can partition this posteria into a Dirichlet process with base measure at the empirical measure of the empirical measure, and one with the base measure being. The base measure being the continuous prior base measure. So again, I'm partitioning my posteria into something finite. I've written this as a Dirichlet process, but because I have finite support, it's just a Dirichlet distribution. And something infinite, the infinite tail where I'm going to have to do something fancier than just using a Dirichlet distribution. Using a Dirichlet distribution. The only difference is rather than just summing them together, I have a mixture of these two, so I'm going to have to introduce a mixture probability, which is going to be a beta random variable. All right, so again, before we get to parallelization, we can construct a hybrid sampler that uses an uncollapse method. That uses an uncollapse method for that finite body of the distribution and a collapse method for the infinite tail. So we're going to sample the weights for the finite bit from a Dirichlet. We're going to sample the probability of belonging to that finite bit using a beta. And then we're going to combine that beta and my weights to get a. My weights to get a conditional distribution that, with probability b will use the instantiated weights, and with probability 1 minus b will use the sufficient statistics and use the sort of Chinese restaurant process-based sampler. Now, as with the previous example, I'm not necessarily saying that the I'm not necessarily saying that the hybrid samplet is something that you should use if you just have one machine. However, I'm going to take a little pause to look at what it would look like if I did use this hybrid sampler just on one machine. So this is on some synthetic data where I just used a mixture of, I can't remember, some large number of Gaussians in varying dimensions. And you can see that if you have low. And you can see that if you have low dimension, it doesn't really matter what sample you look. That's this dark blue line. They all look great, they all converge pretty quickly. So this is evaluated on F1 score on the posterior co-clusterings. However, if you have high dimensions, the collapse sampler is a lot quicker to get somewhere half decent. Somewhere half decent than the uncollapsed sampler. And that's because the uncollapsed sampler, whenever it wants to add a new cluster, it first samples the location from the prior. And as the number of dimensions grow, that's going to be further and further from our data. So here I've got this hybrid approach that uses the uncollapse sampler for instantiated features and the collapse. Features and the collapsed sampler for uninstantiated features. It's still not converging anywhere near as well as the collapsed sampler, but it's doing better quicker than the hybrid sampler. It's able to find those good locations for new features a lot quicker. Now, we know that it's going to be challenging to Going to be challenging to parallelize a collapsed sampler, but this maybe suggests that if we have a choice between parallelizing a hybrid sampler or an uncollapsed sampler, then maybe the hybrid sampler will do better in these high dimensions. So we're going to take this hybrid sampler, we're going to do exactly the same thing, right? We're going to partition our data into the instant. data into the instantiated features and the tail features. We're going to send our data, sorry, partition our features. We're going to send our data out to our various processors. One processor will see both the probabilities from the instantiated features and will be able to add new features. So one processor will do exactly what the hybrid sampler did on the last but one slide. Last but one slide. And the other samplers will only be able to see the features in D1. They'll only be able to sample from these first K features in D1. They are not yet able to see the new features. Then periodically, we'll have a big communication step. This special processor will say, hey, here are my new features. Hey, here are my new features. We'll get rid of any features that are no longer occupied and redefine our groups D1, the instantiated features, and D2, the tail features. I put a star next to one. Again, we can do the warm start thing in the experiments we're going to get to in a couple of minutes. A couple of minutes. We didn't do the cold warm start thing, we did the exact thing, but the same idea applies. You can totally use that to kickstart your burn-in. I've accidentally copied the same slide twice. So let's take a look at that. So here on the left, On the left, I have, sorry, on the right, let's look at first. I have the test set log likelihood. This is on an image data set, the CIFAR 100 image data set, that I reduce the dimensionality using PCA to one-dimensional through that's a lie. That was a different experiment. I reduced the dimension, I think, here. I reduced the dimension, I think. Here, I forget what it was to. So, this is a reduced dimensionality image data set. I apologize for trying to explain, not this slide. Here we're looking at how the performance varies with the number of processes. We're getting a roughly linear speed up as we double the number of processors. And on the left, we're comparing with Comparing with appropriate comparison methods. So in the red is our method. In the green is that slice something base method. So that's a completely uncollapsed. And then the blue is the approximate method, right? The approximate collapsed inference method. So again, we are doing better, quicker than these methods. Quicker than these methods, which we would argue is because A, we're doing something exact, and B, we're making use of that benefit over a fully uncollapsed method, sorry, a fully uncollapsed method in the high-dimensional space. Now, I've used the two popular examples: a beta-Benoui and Dirichlet process. Process, but if you've even taken a passing glance at a completely random measure, you'll be able to see that there's a whole bunch of models that we can partition in exactly this way. And if we can partition it, we can take this approach of using an uncollapsed sampler on one part and a collapsed on the other. So, for example, we can partition the pit manure in a way very similar to how we partitioned the Dirichlet process. Dirichlet process. If we have a hierarchical model, like a hierarchical Dirichlet process, beta process, whatever, we can partition each layer separately. So while I focused on two examples, the principle here extends to a large number of things. So I know we're while I'm under time, the session is at time, so I'll Time, so I'll take that as a win. I kind of want to summarize with some thoughts. When we're doing inference, the infinite tail is what makes Bayesian non-parametrics hard. We know that. We know that if we knew K, we would not be spending anywhere near as much time waiting for our samples to converge. So, why not use an easy method for An easy method for the large, obvious majority of your model, and then save the big guns, the complicated samplers, for the low occupancy features. So here I've done this in a very sort of asymptotically exact setting using MCMC, but there's no reason you couldn't apply this general idea to other inference methods. There's no reason you couldn't do variational inference. Reason you couldn't do variational inference on the finite part and the more expensive MC on the tail. There's no reason you couldn't have a schedule that updates the hard part more frequently than the relatively easy part. So, this is kind of one example of how we can take our hard models and break them down. And break them down into a familiar, easy-looking model that we know how to distribute, that we know how to do inferencing quickly, and then just using an infinite-dimensional sampler or model or what have you for that tricky tail where we really need to spend the time. So, thank you for your time. We have time for questions in Zoom or in real life? Zoom is real life. It's a virtual reality. I have one quick question. How do you define? How do you define your main features and your tail features? How do you differentiate them? Yeah, so in principle, you could define them anyhow, right? So it doesn't make sense to have a given step in your sample. What I've done in practice for most of this is to say, well, my main features are going to be the features that were there when I last had a communication step. Right, so when I, every time I sample my instantiated features, I'm just going to see: hey, what features are there? You're now my main. Everything that I haven't currently got instantiated, you're my tail. You totally can change that. You absolutely can say in the hybrid sampler, I'm just going to have features that appear more than 10 times. Appear more than 10 times. That might be appropriate if you're trying to do something much more approximate with those. Like you want a really big cluster, and then you'll count the low occupancy features, but not currently zero occupancy features as the tail. It would be harder to distribute in that setting, but I can totally imagine a hybrid approach that would make use of that. And we've got some experiments looking at how. Some experiments looking at how that works in a non-distributed setting in the paper. Well, thanks, Jeanit. There's another question in the chat. You can read it, please. Yeah, so Trevor asked, how general is the decomposition that you rely on? Yeah, so it's it is okay, so it is inherent that any completely random Inherent that any completely random measure we can decompose into two parts or as many parts as we like, right? Because completely random measures are infinitely divisible. If we have something that the posterior gives us a representation that is a completely random measure with fixed locations and random locations. Fixed locations and random locations, then we can always split that into a completely random measure with fixed locations and one with random locations. In theory, if we're normalizing and we've still got that property, then the mixture still holds. In practice, if this is something like a generalized normalized gamma process, then you're going to have a hard time actually calculating how to do that. How to do that mixture proportion. So, in order for this to be done easily, you need a marginal representation like you get out of the beta process, the Dirichlet process, that if you have a finite partition, then you have some known parametric distribution. There's another question in the chat. Sweet. The chat? Sweet. To what extent does the approach in the tail part depend on the process used? Oh, so I think here you're asking how does the choice of sampler in the tail part depend? So, sweet. So, again, in principle, you can do what Principle, you can do whatever you like in these, right? There's no, like, mathematically, it all holds. What we were trying to do, though, was avoid using anything uncollapsed in that setting because we found that in high dimensions, it takes forever to get something that will be accepted. So, for everything that we've looked at, we've done a A conditional sampler for that tail. There's, I'm sure there's something else smart that you could do. I just can't think what else you'd use there. But yeah, if you have something other than that.