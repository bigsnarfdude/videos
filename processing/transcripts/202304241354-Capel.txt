It's really a pleasure. I feel like I really don't know many of you in the room, so I'm going to take the liberty to just briefly introduce myself and my background. So I am working in astrophysics, unsurprisingly, more on Bayesian approaches. In particular, I'm working in cosmic ray and neutrino astrophysics. And in those fields, I've been working on the development of detectors and also data analysis. So very much the experimental side of things. Side of things. In this field, the main question right now is trying to figure out where these energetic particles are coming from. So we're able to detect them on Earth. Trying to use amazing large-scale detectors, but it's actually pretty tricky to figure out where they're coming from. Something else to give some context in this field, on the more astro-particle side of things, I think there's actually quite a lot of influence from particle physics. Influence from particle physics, and so I would say it's still definitely more frequent tests than Bayesian in terms of thematics. And yeah, I won't comment on that. So more generally, I would say that Bayesian approaches are becoming extremely popular and probably even dominant in other areas of astrophysics and cosmology. So I took this nice idea of putting Of putting crude identification of Bayesian articles into the ADS from this older paper by Laredo. And we can see that as time is going on, it's getting more and more Bayesian on the astro side of things. I think this makes sense. If you would ask me, the driving factors would be that in these fields, we're working on the observational side, and so having the idea of any kind of controlled, repeated experiment in a laboratory is quite difficult. Experiment in a laboratory is quite difficult. Borderline impossible, I would argue. I think it's also very much spurred by the computational developments that have been taking place during this period that have made Bayesian computations possible. Probably also the increasing complexity on both the model and data side, and in particular, as I would say, it's probably the same in particle physics: the fact that we now have a lot of data, and at some point And at some point, the systematics are becoming increasingly important. So that's what I'll be focusing on in this talk. So, as you can see here as well, there's like some hundreds of papers per year. So, that's my disclaimer that I'll be having kind of a random selection of examples that I talk about that are very much biased by the kind of topics I'm working on and maybe might hope to know something about by now. So, with that, the outline for the rest of this talk will be to just have another look at the Bayesian view on systematics. And then, I really want to focus on general strategies for how people are dealing with systematics in astrophysics and cosmology. And then I'll try and wrap up in time of being a discussion so that I don't get any angry face emojis from one of the second. Okay, so we've already heard quite a bit this morning about the standard Bayesian picture of systematics. We would argue that nuisance parameters are just like any other parameter, and as you can have uncertainties on parameters and distributions of parameters, it's quite a clear rationale how to deal with these uncertainties in that we can just marginalize or integrate over them. So you have formed a joint posterior of whichever parameters are in your model, whether Of whichever parameters are in your model, whether they're interesting or annoying. And then this will be proportional to the likelihood, plus, of course, the prior of those parameters. And then integrating out is pretty straightforward to get the marginal posterior distribution. And you can use this to create whatever summary of your parameter of interest that you might want to use, such as a credible region or a limit, and so on. So we already heard in the talks this morning about the more frequentist perspective. A more frequentist perspective. I would say maybe what's nice about this is the natural treatment of uncertainties and the fact that these parameters are just like any other, so you don't really need to classify them in a different way. But on the other hand, you do need to think about how to define priors for these parameters. And if you're interested in coverage, there's no real guarantee on this side. So I could say, okay, that's great, that's how Bayesian's doing systematics. It's that simple. And just finish the talk here. Probably the organizers would be. Talk here. Probably the organizers would be glad, but I'm sure. But I think there's a few more things to go into. So, again, as we talked about earlier, there are many different types of systematics and the way that this word is used is quite vague in a sense in astrophysics. So, I would say the categories I went for are pretty similar to this good, bad, and ugly picture that we had earlier. First up, you have systematics that are really just statistical. Really, just statistical uncertainties, masquerading, systematics. And if you add more data or if you have some ancillary measurement, you could hope to constrain these. And so you have some power to constrain the posterior distribution of this systematic parameter with some conditioning on data of some kind. On the other hand, we have difficulties in model specifications and certain assumptions on both the measurement or analysis. The measurement or analysis sides, and then even worse uncertainties in the theoretical framework that we might be working on. And in this case, you end up with some kind of bias on the parameter, and it's not clear how we can get more information or constrain these systematics. And of course, unfortunately, a lot of the time in astrophysics, these are really the ones that you're dealing with. There's a lot of The ones that you're dealing with. There's a lot of this kind of uncertainty. So, what to do about it? I want to go through a few different examples of the kind of systematics just to explain a little bit more in detail. I picked this example of the on-off signal measurement, which actually already came up this morning as well, because I thought it's a very well-studied example for both particle physics and astrophysics. In astrophysics, it's very much used in the analysis. It's very much used in the analysis of gamma-ray data where you might have on-source and off-source measurements or the low-calibration of the high energies. And the way we would deal with this is again this simple marginalization picture that I've already explained. And then we can use this to really focus on the source signal, including our measurement of background and any uncertainty about that. And then depending on what these measurements are, And depending on what these measurements are, we'll get some different constraints that will be summarized by the marginal posterior for the signal grammar. Then maybe a more realistic picture of the kind of systematics we're dealing with would be this not so well constrained by observations and assumptions and simplifications necessary case where we're, for example, trying to understand how to model magnetic fields in the context. Model magnetic fields in the context of searching for the sources of ultra-high energy cosmic rays. So these ultra-high energy particles are charged, so they're deflected by the magnetic fields as they propagate towards us from what we think are extragalactic sources. And in particular, the galactic magnetic field has a very tricky to model effect in that the way these particles are deflected will depend on the way that they come towards us and so it will have a sort of lensing effect on the distribution of the particles arising. Of the particles arising. And of course, this is very much model-dependent, depending on which model for the galactic magnetic field you might prefer. And what this picture is trying to show is that depending on the two different choices, you have really different results for what kind of direction the particles might go in as they pass through the Earth's magnetic field. So, the way we currently deal with this is to have a set of discrete models. Of discrete models such as these two here, and probably report the results of analysis in both cases. So, similar to a discussion that came up earlier, we would not really try to average between the two cases because it's not clear whether the magnetic field model that would imply would be useful. However, there is interesting work being done by this IMAGIN group, which aims to turn this into a kind of ancillary measurement. Ancillary measurement calibration problem, more of the first kind of systematic, which would be great, by doing a joint fit of all different kinds of magnetic field constraints. That's pretty interesting. But I think this is still an early stage of what this work could be doing. So I won't go into that much more now. Finally, I wanted to give an example of what an uncertain theoretical framework could look like. So what this typically could be in astrophysics, and we might use some just In astrophysics, that we might use some just empirical functions to model the shape of things that we maybe have some idea about what they should look like physically, but we're not sure, or even just the choice of priors in general for parameters of a model. So the example, one recent work I thought was quite interesting was this approach in modeling the binary black hole population properties for gravitational wave data. So at the moment there's not So, at the moment, there's not huge amounts of observations of binary black holes, but this will probably be changing in the future. And the way we might normally model the mass distribution would just be a simple power law with a little bump in it. And so, this is an empirical function that's been used to fit the data, as shown by these dashed lines here. And this mass distribution and its possible evolution with redshift is quite interesting. Evolution with redshift is quite interesting because it will have some connection to the physics of black hole formation that we could use to have a deeper understanding about these things. And so this group here, Ray et al., have been working on a non-parametric way to reconstruct the shape of this mass distribution with a binned Gaussian process. And there's a bunch of work in different kinds of non-parametric approaches as well. And I think what you can see immediately is that you have. And what you can see immediately is that you have, if you don't have this assumed empirical shape constraining the distribution in certain regions of the mass parameter space, it's way less constrained. And the amount of constraint you have seems to be quite dependent on the redshift, as it will be coupled to the number of objects in that redshift bit. So I think this could be an interesting way. In general, it's probably going to depend on the type of problems that you have. Type of problems that you have, whether or not the examples I've discussed will be a successful approach. And probably I would even argue that it's in this field very difficult to define a general recipe for how to identify and deal with systematics. But then having said that, I'm going to try and do it anyway and talk about some general strategies that we might use to deal with these kind of problems in national. Deal with these kinds of problems in astrophysics. So, one approach I think can be quite interesting, or at least as a starting point when building models, is to really focus on like forward or generative modeling. So, this is really just simulating the kind of data that you might think you're going to observe. And going through this stage should allow you to identify, you know, what are the high-level parameters or your simulation inputs, the latent. Simulation inputs, the latent parameters that are necessary to get from those in various steps to the observations that you have. And doing going through this process, you should be able to identify what kind of known systematics you might have or expected ones and of what class they might fall into. Generally, map out the complexity of your model, and in principle, having a full simulation of the data that you're. Of the data that you're observing should give you all the information that you need to derive a likelihood function. Having this framework available, you can also do things like prior predictive checks, where you can put assumptions for your prior parameters through your simulation and view them more in the data space, so to speak. And in that way, you can really visualize and quantify what kind of constraints you're putting. Constraints you're putting with these priors on your expectation. And yeah, this also gives you a test whether you want to evaluate possible simplifications of your model and what the impact of the different assumptions might be and which parameters and which systematics might be the most important. So, just to give a little bit of an example, recently I've used forward modelling actually to investigate a possible blazer or neutrino connection. Blazer or neutrino connection that's been reported. So, in this paper in 2018, the IceCube collaboration found a single high-energy neutrino event coincident with a flaring laser on the sky. However, it's quite difficult to reconstruct neutrinos, so you have quite a large one-square degree region as a confidence interval for the uncertainty. And then there are quite a lot of blazars on the skies. Are quite a lot of lasers on the skies, so it's turned out in the end that they reported something like a three-sigma uncertainty. But seeing as we at that stage had yet to detect any sources in high energy neutrinos, that was still pretty noteworthy. And what we wanted to do was use forward modeling to investigate the assumptions that were behind this three-single result from a population perspective. So, if we would consider that all blazars were behaving in a similar way, Behaving in a similar way, but also if this plays out with its known properties is capable of producing neutrinos, what can we extrapolate by thinking about the population as a whole with a single forward model? And what we found is actually that the connection between gamma rays and neutrino fluxes that was crucial to identify this significance, although I would say probably not considered a discovery in particle physics, is that you Is that you'd actually, if this was true, in order not to overproduce the rest of the neutrinos that we've seen, this connection would have to be very small. So that the neutrino flux would have to be two orders or so of magnitude at least smaller than the gamma ray fluxes, which might lead us to maybe construct the different models in the first place. So yeah, that kind of allows to put some observations like this in more of a context here. More of a context here. Then, once you have a forward model, it's quite easy to then take your simulation and think about, okay, if I want to infer these parameters from my data, how can I do that? So by using hierarchical modeling to organize parameters into this hierarchy that describes your data generating process or the forward model. If you're able to If you're able to specify the various connections between the different layers of the model, you basically have your likelihood. In general, it might be difficult to really capture all the complexity, but it's somehow much easier to simulate forward than it is to do backwards. And so, generally, when doing this, my impression of the literature is the goal was really to have a good enough go at the modelling. Good enough go at the modelling and being aware that the data is not being aware of the level of information that's in the data and how important the assumptions and the things are in that context. Again, the forward modelling can help with that. So hierarchical modeling is really popular in astrophysics and cosmology. There's really a wide range of different applications I could talk about just to capture the breadth. Just to capture the breadth of this, I put a few different examples that I'm familiar with here. So, fitting lines in the case where you have errors in both directions, so for correlations. And then supermosar cosmology and the Hubble constant estimation. This is also, in my view, a very complicated regression problem. And I put a nice hierarchical model there. Nice hierarchical model there. Hope soft feeding it well. So it starts to look when I see this. I don't know, we had some kind of complicated DAG earlier this morning from the LHC, I think. So to me, that looks kind of like a hierarchical model, but I don't know if that's really the case. And yeah, also in gravitational wave astronomy, these approaches have become very popular, things like estimating redshifts from very limited Estimating redshifts from very limited spectral information from large numbers of sources. And also, a case that I'm very interested in: so, associating particles and sources, or photo documents and sources, or cross-matching sources across different catalogues. And in that case, the hierarchical nature can allow you to really add in the properties of the sources and the particles or photons into the inference problem and so that the Inference problem, and so that the possible connections between the two can be weighted by physically motivated models. So, the kind of models that I've specified with this hierarchical picture in complex simulations generally will have large numbers of parameters. So, how to do inference in this highly dimensional setting. I don't want to go into too much detail, just to say that MCMC methods. Just to say that MCMC methods are extremely popular in astrophysics and cosmology. And this way you're essentially approximating high-dimensional integrals, which could be expectation values or variances of your parameters. And the nice property of these methods is that you have exact convergence in the limit of infinite samples, and there are at least nice diagnostic techniques available to help you assess whether you're in a reasonable regime. One of my favorites. One of my favorite for working with models, whether perhaps hundreds of parameters or more, is Hamiltonian Monte Carlo. And this is just a type of Markov chain Monte Carlo, which uses Hamiltonian dynamics to move effectively in the parameter space. And the trade-off there is that you need information about the gradients of your distributions in this parameter space. But in general, there are many different There are many different implementations, and probably this could be a whole talk, so I won't go on about it too much. So some other strategies that are quite common are various ways of checking your model once that you've constructed it. And one very straightforward kind of sanity check is to simulate data from your forward model, fit that data, and verify that you can recover the parameters. That you can recover the parameters that you've put in. So, as one example, going back to an ultra-high energy cosmic ray source association problem, you could use this method to either assess what, so if we want to analyze the fraction of alternative cosmic rays that one could associate with a set of sources, and our data is energies and directions of these particles, we could say, okay. Particles, we could say, okay, what if I, so first of all, I'm able to recover the true fraction in the case where I include all the information, and what happens if we remove some information from the fit and we can start to evaluate what can be important there. And we see in this case, if we leave out the energies, then we're going to bias our fit in general to lower associated fractions. On the other side, you can also use this approach to This approach to test out different ways of expanding your model or simplifying it. So, in this case, we wanted to check that the assumptions we made in our inference model actually weren't capable of capturing the important physics of a more complicated Monte Carlo model that was too slow to try and fit. And you can use this approach to evaluate how well you're able to recover at least the parameters of interest for your problem. Another interesting strategy that I think can be useful in this model checking and validation context is that of posterior predictive checks. So here I'm actually going to talk about a particle physics problem that I've worked on recently thanks to this data science center that's been set up in Munich, which is an interdisciplinary idea. And that's where I was working in my postdoc. And that's where I was working in my postdoc before my current position. And now Lucas is leading this group, so you can hear more about that if you speak with him. Then, yeah, here what we wanted to do was take a Bayesian forward modelling hierarchical approach of the kind that I've just described and apply it to the problem of parton density extraction. And in particular, we were focusing on this high-ex regime and looking at archival Zeus. Regime and looking at archival Zeus data. So, here we wanted to use the posterior predictive tracks to assess the goodness of fit of the models that we, the resulting models. So, what that looks like basically is just taking samples from your posterior distribution for your parameters of fit, running them through your simulation, and generating simulated data sets under these. Simulated data sets under these parameter assumptions. And then here we visualize this with these two kinematic variables, wooden for the part on density extraction, so the x and q squared, x on the x-axis and q squared with the color scale. And what we basically had as data were these solid points, which were just count data in different xq squared bins. And then these faint points underneath represent the prediction. Represent the predictions from simulated data sets under our best fit model. And what we would look for in this case is that we have a good overlap between the data that we see and the regions covered by these expectations. And any mismatch could then be used to visualise and identify possible ways that we could even find new systematics that we had not modelled before. That we had not modelled before, or suggest ways to improve Spartanal model to have a better representation of the data. Okay, so I think I will skip model averaging in the interest of time. What I said earlier and I think came up was that in general it's difficult to really understand what it means to average over different models, but there can be contexts where it is useful. A few practical aspects of all this that I wanted to highlight is that while you can just include any kind of systematic uncertainty by just adding parameters to your model, it does get increasingly computationally expensive. And the implementation of these strategies can be very challenging in different scenarios like how to visualize large-dimensional data sets, what to do if you can't write down your likelihood, and I think you'll hear more about that later. About that later. And then a few other points here that have already come up in discussions before. So I won't go into them again. With that, I'll wrap up. So I think the Bayesian methods are often talked about as simpler in a way to deal with systematics, but I think they can still be really challenging to implement and it's more to think about than just priors. But I think it can be worth it to really try and understand and make the most of the data that we can. Make the most of the data that we can. And then here are some thoughts for the discussion. I think the availability of reusable data is really important. And in general, I don't see any issue with trying both frequentist and Bayesian approaches and interpreting them in context. So I'm interested to hear what you all think about that. Thank you. Questions? So back on slide 15 when you mentioned, I think this is what in particle physics we call closure. Yeah. Is this because the method assumes that both pieces of information are available and then you take one out and you see how biased it gets or how does it get biased by removing this piece of information? So in this particular context we were interested in comparing with We were interested in comparing with other faster methods that ignored the energy, and so it was more of an argument of why make the extra effort to include it. But it can also be that actually including the energy in this model was quite complicated and required a lot more computational power, but also a lot more, I guess, kind of you couldn't just use textbook-type methods anymore. So I guess it helps to also. I guess it helps to also maybe assess whether it's worth doing or not to a broader community. But the design of the method used to extract the parameters, I don't know what kind of information it's supposed to get. Agnostic, the method. I'm not sure what you mean. I mean, sorry, so the reason I have so many caveats in my question is that when I hear patient, I hear you. When I hear behavior, I hear you need to put in ingredients that you choose. So what I'm wondering is if if the method was supposed to work with energies and directions, and then you one of them put an ingredient, but then the recipe doesn't work. Right, no, but this, yeah, so I think I understand what you're getting at. I think what's really changing here is not the method so much as the model. And so it's. And so here I have a likelihood that depends on energies and directions, and it's maybe aware that lower energy cosmic rays could be more deflected from sources. Whereas here, the model is really saying energy isn't important. Let's say that on average cosmic rays are equally deflected from sources, regardless of energy. Sorry, explain. Yeah, I had a question actually on the same slide. So the one on the right, so this is kind of also. So, the one on the right, so this is kind of also an important question for us. So, we have very elaborate simulations that are very expensive, and even if we were able to run like MCMC, I mean, we've tried, it's very expensive, and so on. And so, can you explain again how you validate that you are more simplified, so let's say you have like a very complex graphical model and make a very much more simple one. So, how do you verify that it's like sufficient? Yeah, no, I think it's an interesting. Yeah, no, I think it's an interesting point. So, here, this CR proper model that I've highlighted is a full Monte Carlo simulation. You go from injecting some cosmic rays of the source, then they have these kind of stochastic interactions with radiation and matter fields as they propagate, they lose energy, they get deflected. Whereas the approach I used in a model which I implemented in STAN was basically a semi analytic approximation to all these different interactions that were going on. Interactions that were going on. So, what we did here was simulate a data set where the cosmic rays had followed the full CR proper interactions and then fit that simulated data set where we know the true parameters with our simplified model. And so, if, I mean, this here in the end, I would say this is probably you want to repeat this procedure many times with different simulated data sets and see how you're And see how your marginal posteriors maybe reconstruct the true value on average. It's kind of interesting because you're kind of comparing them in the inference space, whereas typically, I mean, you could basically run like prior predictive on both. And so you basically say, okay, do they generate the same type of distribution? This is what we do if we want to do like a machine learning fast simulation or something. But you're kind of comparing that in the inference space, which is Comparing that in the inference space, which is typically much smaller, maybe so that's interesting. No, it's a good point, but I think it's a very convincing way to explain the method to people. Although, in the end, I would argue that, yeah, it's not perfect, and what we treated was the best simplified case, anyway. So, I think in the future, also these simulation-based and likelihood-free approaches, I'm very interested in. How about this truth analysis? I didn't quite see in the plot where the prediction was shown. This is the points are the data with the. The solid points are the data. And the kind of, I don't know. It's just not visibly, right? Maybe it doesn't come up well. The blurry points are the spread of possible predictions from the data. Is this supposed to be good or bad? This is supposed to be good, yeah. The observed data is in the range of the prediction. Yes. So, like, if this is my prediction, let's take one where it's clearer, you know, and there's the data. And the points are the data. Points on the data. And so, if I would have a lot of data points where the prediction and the data mismatched, I could then use that to understand in which regions of the parameter or the data space this was happening. And hopefully, that would help me decide how to improve. Decide how to improve things. Do any of them mismatch? No, I think this is maybe one of the worst ones. But you would expect some voice, you expect some to be answered. Yeah, so we actually quantified this as well with some Bayesian cell. You know, you can do a Bayesian p-value or just tail probability. And yeah, so we had also a more numerical weight, but I find personally the visual is the most convincing for me. And I should have included. For me, and I should have included a back hit, then probably it would have been more instructive on the topic. So, question to be accomplished. Take it off. I just had a question about the hierarchical nation modeling. So, is there a need when you're building these hierarchical models? Are you building posteriors over all of your latents? If so, how do you scale that up? Yeah, it's difficult. And it depends what kind of relationships your parameters have, whether this is going to be feasible or not. So that's the kind of place where you might want to make some centrifications, or what if these distributions exist, but they're not easy to define, and they're not normal distributions, or some other statistical distribution that's easily implemented. So that's definitely difficult. Yeah, because I'm sort of thinking of a collider perspective, we were because we can build a Either perspective, we were to look because we can build a very similar sort of hierarchical model, but the data set size is so low. I couldn't imagine computing a posterior for every event at the LHC. But wait, what do you mean by every event? Well, because you could sort of imagine the latency could be all the particles. It's like the data would be your observations. Right, okay. Yeah. Particle content of the event could be the latency. There's a lot in your model. And then there's eventually the standard model templates and stuff like that. Yeah. So I don't. Yeah. Yeah, I don't know if it would be possible, but I I just feel like maybe with a more simulation-based approach where you don't have to do that and it's kind of built in. Is it ever done where you just sort of skip the latent estimation and just go directly from observations to theta? Or is that not successful? Sometimes you can integrate the manual. Yeah, that's also true. So like it's random exponent standard. That's sort of a special case, I guess. Special case, I guess. Well, they're all special cases. Yeah, I think it's hard to talk generally about this. These are just some common approaches/slash things that I like. So, it's very much a random sample. Yeah, just to clarify, I'll just slide the idea you're proposing is that the latent parameters, those are system types. Not necessarily, no, sorry. Not not necessarily, no, sorry. Um systematics could be high-level parameters, they could be part of your latent space as well. I yeah, I wouldn't say um yeah sorry, that's probably a notation thing that I used by the systematics at one point, and then yeah, that was not my intent.