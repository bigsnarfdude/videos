Even without a title, that's a good try. Well, and well, thanks so much for the invitation. And yeah, I hope I can see all of you in person soon. So since this was about random graphs and statistical inference, I thought I hadn't given a talk about this topic before, but the This topic before, but I'm interested in the question of sort of recovering the locations of points of a random geometric graph. So here's the setup. So you have a box in RD, and let's say side length n. And I pass on points in the box, let's say just with rate one. And for the nearby points, For the nearby points, I tell you how far apart they are. So I'll give you a noisy estimate of the distance. So if I'm in Princeton, maybe I tell you how far it is to New York or Philadelphia or Atlantic City, but I don't tell you how far it is to Banff or Berkeley. And you get this roadmap of all the local distances. And now the question is: I want to know. is you want to know I want to know how far it would have been to go to to actually go to Banth, say as and how accurately can you recover those distances, the long range distances given the short range distances and what sort of accuracy is possible. So just to start off with an easier question, imagine that there was no noise. Imagine that there was no noise. So any pair of points that are within distance L, I just tell you exactly how far apart they are. Then, well, if you have, let's say in two dimensions, if you have points and they form a clique, or like say a triangle, then you can work out sort of exactly the sort of relative locations of them up to sort of a rigid body transformation. Transformation. And you could imagine that if then you could sort of piece together using sort of all the different local neighborhoods and work out the sort of entire sort of location of all the points up to some rigid body transformation. Of course, if you just do a reflection or rotation of all the points, then that won't change the distances. When there's no noise. And if you think about it for a few more minutes, you can sort of convince yourself that if you're a little bit above the connectivity threshold, so if this was, say, a torus, then that would correspond to if you take a point, the distance threshold L should be such that if I take a ball of radius L, then the Then that has volume greater than or equal to d log n approximately. Then that's the connectivity threshold. And if you're a little bit above the connectivity threshold, then you can exactly recover the locations of all the points. It's not enough to be just connected because you could have sort of a point like this one that only has one edge and then it could lie. And then it could lie, there'd be some sort of arc that it could lie on, and you wouldn't be able to distinguish between the two. Okay. And another question I'll be interested in is, sort of, I give you even less information. I just tell you the graph structure. So you just have an underlying random geometric graph. I tell you the vertices and which and the edge set, but I don't tell you where any of the points are. But I don't tell you where any of the points are. Can you reconstruct the underlying locations of the points? Okay, and this came up in an AIM meeting in 2017. Okay, so that's the sort of problem I want to address. And so, how would you go about trying to work out where points are? Well, the first thing you might Are well, the first thing you might think of is something like first passage percolation. So if I have a point U and a point V, and I want to say how far apart they are, I might sort of look at paths of sort of intermediate points, add up the distance along the path, and then take the minimum path from u to v and make that my estimator. Make that my estimator. So we know some things about this from sort of standard methods and first passage percolation. The estimated distance divided by the real distance will convert to some constant. So, with some appropriate normalization, this would give you a good estimate up to like a one plus the lower one multiplicative. Below one multiplicative error. And the variance will be: you can show that it's bounded by order n in any dimension. But that would still mean that the errors you have are sort of square root of n. And I'd like to do better than that. Now, in dimension one, you can't do any better than that. In dimension one, you can't do any better than that. In dimension two, people think that you should be able to do better for first passage percolation, that the variance should be n to the two-thirds. But this is not known in any model of first-passage percolation. It's known in a couple of exactly solvable models of last passage percolation, and people sort of think by on the principle of universality that it should be true for first passage percolation as well. So, if we knew this in the context of random geometric graphs, then you'd get a sort of better distance estimate, maybe n to the one-third, but still not that great. For larger dimensions, I'm not quite sure how large D would D would need to be here, but the prediction, but thinking like 20 or bigger or something, then the variance is predicted to be just order one. And so if that were true, then one could sort of really accurately recover the locations of the points. You'd be able to estimate the distance from Princeton to Bath with just a With just a constant error. Okay, but this is very far from what we know how to do. Okay, but we can at least get a sort of approximate range of distances with first passage percolation. Another thing you might sort of say is, well, if I have a collection of points, I can sort of locally work out how they, you know, their geometry with respect to each other if I know. Geometry with respect to each other, if I know, sort of, if they form a clique and I know all the distances. And so maybe I can do it locally in this neighborhood and locally in some other neighborhood. But if they're sort of far away, can I, and sort of, let's say this is north in this neighborhood, can I work out what's the corresponding north direction in a faraway neighborhood just from this local intervention? Neighborhood just from this local information. So, sort of, what's the relative rotation of these two neighborhoods? And so, the sort of simpler version of this problem is a problem called synchronization. And here, the question is: you have a group G. And let's think of the, this could be anything. Un this could be anything, but in this case, I want to think of the group as being like the orthogonal group, say relating say a rotation. And so you have a rotation for each vertex in some lattice. So these are sort of a series of directions. So this is a sort of vector x. So and let's assume that the origin is the identity. the origin is the identity. And we're not told what they are, we're just told sort of the relationship between neighboring ones. So if we have two neighboring locations here, there's some edges between these two, and so we can work out the approximately the relative rotations of them. So for each neighboring pair of vertices u and v, we can get x u inverse x v, sort of the inverse xv sort of the relative rotation between neighboring pairs here but with some noise and i'll assume z is some some iid noise and uh and the question the sort of synchronization is can we can we estimate the the value of far away vertices from the origin sort of independent of the distance. the distance okay um does that does that question and any questions about okay well so some sort of obvious negative results one is if the noise is too um noisy uh if it's sort of something that was fairly close to the uniform distribution just with a little bit of information then then you won't be Then you won't be able to recover anything about faraway vertices in any dimension. There's essentially coupling with percolation and you can show that far away vertices you don't have any information about. And this is sort of similar to saying that if you have a spin system like the easing model in high temperatures, at high temperatures, you have At high temperatures, you have exponential decay correlation. In dimension one, it's sort of a similar story to a spin system or the easing model in dimension one. The information you have just looks like a Markov chain, and so you'll get, again, exponential decay of correlation. So we can't hope to do it in dimension one if there's any noise. one if there's any noise. What about in dimension two? Well in dimension two is you also can't do it if the group is continuous. So something like these like this the orthogonal group. And essentially it's because in two dimensions you don't have sort of breaking of continuous symmetries. Continuous symmetries. So an example would be: let's say the group was the unitary group U1. So it's basically just like a unit complex number. And so, and then we know sort of a noisy estimate of the relative, the multiplicative. Multiplicative sort of ratio of neighboring vertices, and we want to work out the value of faraway vertices. What I want to do is start off with a vector x and then perturb it to get some other vector x prime such that at v, I'm going to shift the value from x of v to x of v times some rotation of e to the. Rotation of e to the is. But I want to do this in a sort of gradual way, so I'm not rotating, the amount that I'm rotating neighboring vertices is similar. So I'll rotate V by S and the neighboring vertices by something slightly less, and their neighboring vertices by something slightly less. Vertices by something slightly less, the amount. But because, well, if I take a ball around V, the bigger the radius, the more things on the sort of ball of radius L around it. So I'm going to have sort of a sharper rotation sort of around the middle and sort of more granular. The middle and sort of more gradual changes around the edges. And in particular, according to the following formula. And so x prime u will be x of u times e to the i theta s h of u. And then y prime would be the corresponding measurements of neighboring vertices we get from this. X prime field. Then it turns out that if you look at the, say, the total variation distance or the L2 distance between y and y prime, it's something that goes to zero as n tends to infinity. So we've sort of shifted things in a soft, gradual enough way such that on the one hand, we've shifted On the one hand, we've shifted v by a constant rotation, but the total change in what we observe is something that's sort of tending to zero and total variation distance as sort of the distance from v to the origin is going to infinity. And so you can show that essentially this means that you can't You can't accurately recover what the angle at v is. And so, this is not really a surprise. This is analogous to the Merrim-Wagner theorem from statistical mechanics, which says that in two dimensions, if you have a continuous spin system, something like the XY model. something like the XY model, then you don't have phase trend. Well, you don't have, I mean, you always have decay of correlation in dimensions, in dimension two. Now that's not true if you have a discrete spin system instead. We know, of course, for the easing model, at low temperatures, you have a plus phase and a minus phase. And if you have a box with a plus boundary condition. Plus a plus boundary condition, then the center will be much more likely to be plus than minus. But when you have continuous models, you always have decay of correlation in two dimensions. And the proof is basically exactly the same as the one here, just understanding what happens when you sort of do some slight perturbation sort of gradually over a Sort of gradually over a series of scales. And the important thing is that in two dimensions, the size of the boundary of a ball is growing like, if the ball is radius r, it's growing like r. In high dimensions, it's growing like r to the d minus one, and that's too big for this construction to work. Okay, so. Okay, so we can't hope to do synchronization in two dimensions. What about in high dimensions? Well, fortunately, I do have some positive things to say. We could imagine rather than taking something like a complicated group like the orthogonal group, just take the group to be the real numbers. Real numbers under the operation of addition. And then this synchronization problem just means you're given y, which is m, which is just the matrix that gives you the difference of neighboring vertices, mx plus z, and this is now just a linear problem. So we know how to, I mean, know how to do statistical inference for linear problems. I mean, we can take like the least square estimator. So in this case, our So in this case, our estimate would be the take the pseudo-inverse of m times y. And if you do that, then the mean square error, sort of the average L2 error for each vertex, you can write in terms of the trace of the Laplacian of the graph. And suppose our graph is the, let me just make it a bit more symmetric and say the graph is the Make it a bit more symmetric and say the graph is the torus. Then you can explicitly calculate all of the eigenvalues of the Laplacian and calculate exact, you know, at least asymptotically what the mean square error is. And it's of order n in dimension one. This is what we'd expect from sort of the variance of a markup chain. Chain. Order log n in dimension two, so just something slightly worse than constant. And then in dimensions three and higher, it's a constant error. So we might hope that so that this should give us hope for being able to do this in dimensions three or more. But of course, we can't do this same linear. Do this same linear algebra approach when we have a group rather, or you know, a general group rather than a nice Arbelian group like the real numbers. Well, actually, no, I mean, you know, not just being Arbelian, just the fact that we can do the linear algebra on the real numbers. Okay, so let me give you a So let me give you one solution to the synchronization problem. So here the group will be the orthogonal group. By symmetry, it's sort of reasonable to expect that the noise, the expected value of the noise is lambda times the identity. And if the noise is small, then lambda will be close to one. And now let's try and reconstruct the value at v. And just to begin with, let's say v is say on the diagonal, so it's like n and n. And I'll just take some path from the origin to v and an oriented path, so only going in the positive direction. If there was no noise, I could just multiply the Just multiply the y's along the path, and these would all cancel out, and we just get xv and we'd be done. And if okay, but now if there's noise, if we take the expected value of this product, we'll get xv times lambda to the value. times lambda to the length of the path, lambda to the length of gamma. So if I want to correct for this lambda factor, I can, I'll just sort of take this product and also multiply by lambda to the minus gamma. And then this estimator will, its expected value will be xv. But it will be very noisy. So we'll sort of get multiplicative noise in each step. And so In each step, and so you know, when n is big, this would be a very noisy estimator. But this was just for one path, and the idea is you can sort of average over paths. So just take all oriented paths from zero to n and take some weighted average of them. Then, when the dimension is at least four, Least four, and if I take the uniform measure, then as long as the variance of the noise on each edge is small, then the total variance you can show is bounded by epsilon. Oh, I should say, you know, if we take this sum and average, actually we'll get something that might not be in the orthogonal group anymore, so we have to sort of project it to something that's in the orthogonal group. Projected to something that's in the group. But we can calculate the variance and the second moment of this estimate. And essentially, when you work out the second moment, so for the second moment, you're going to get sums over pairs of paths. And what will matter is sort of the number of intersection points of this path. So how many times these paths How many times these paths have are in the same a random pair of paths are in the same spot and we're essentially going to get a term that's like e to the size of the intersection or yeah basically the expected value of e to a constant times their intersection size. So we want them to meet not very often. Now if we take two uniformly chosen paths and as we're sort of And as we sort of move along and we could look at their sort of relative difference to each other, that's essentially something doing a random walk in dimension d minus one. And so when d is greater than or equal to four, it's a three or high dimensional random walk, which is transient and so therefore doesn't return to the origin very much. The number of return times has exponential scales, so we can. So, we can control this term. Doesn't work in dimension three. In dimension three, the random walk would be recurrent, and then so the number of intersection points would be about log n, and this term would blow up. So, that's not so good, but it turns out that there's a simple way of fixing this, which is that This, which is that rather than taking the uniform distribution over paths, we could take some other distribution over paths. We could take a distribution mu such that if I take two pairs of paths, they're likely to have fewer intersections. Have fewer intersections. And to do this, you want mu to be sort of less predictable than just a standard random walk. And there are, and it turns out there are constructions of this. It goes back to work of Benjamin P. Mantle and Perez in more than 20 years ago now. And so, you know, if you have two random walks after n steps, the probability that they're in the same spot is about That they're in the same spot is about one over root n. You can construct mu such that the probability of them being in the same spot is like one over n to the 0.9 or any number less than one. And just to tell you one construction of how you could do this, I think, just because it's cute, you could take the sort of infinite binary. sort of infinite binary take an infinite binary tree put the easing model at low temperature on it so this and then just read off the the spins on the leaves and and these uh so these spins will be correlated and if you think of them as telling you the increments of a random walk it will be more spread out and less predictable than just a standard random walk. Just a standard random block. And so it turns out that if you kind of build this measure based on these unpredictable paths, then you can get this construction to work in dimension three as well. Okay, and so this reconstructs, you know, let's say this point NNNNN, and by sort of chaining together a few of these estimates, you can do synchronization. And do synchronization for the origin and any pair of points with an error that is independent of the distance. Maybe I should pause and ask if there are any questions about this. Okay. Let me say a little one about the robustness of this algorithm, which is that extremely Which is that extremely not robust. You can, by making very small changes in the distribution, it will completely not work at all. And so imagine that Z was not, we said the errors Z were identically IID. Imagine that they're still independent, but not identically distributed. And let me make it even better. Let me reduce the amount of. Let me reduce the amount of error for, let's say, the vertices in some thin strip close to the diagonal. So rather than the expected value of z being lambda times the identity, here it's lambda plus epsilon times the identity. Now if I sum over paths, the average is going to be dominated by paths that spend most of their time in this region with the high. Of their time in this region with the higher mean. There's a sort of pinning phenomenon that goes on. And basically, the second moment analysis will fail. And you'll get very bad estimates if you try and do it with this. So, some more recent work, which we're hoping. more recent work which we're hopefully going to finish by or hopefully by the end of the month with Emmanuel and our student Xuan Ping Li is to give a more sort of robust algorithm for this which based on a sort of multi-scale approach which is essentially to do the synchronization problem in some box on some scale and then On some scale, and then kind of connect up our estimates in different boxes. So it's multi-scale. And the way of sort of aligning our estimates in different regions is quite similar, although somewhat more complicated, than the sort of toy problem we had before of doing the synchronization problem over the real numbers. Problem over the real numbers. And so, yeah, and but that will work for sort of general Z that are where the noise is small, but the noise need not be identically distributed. Okay, so now let me get back to the question of localization. Localization. So, our original plan for tackling this was sort of take all the points in the grid and we'll sort of break it up into some coarse series of blocks, which we could construct approximately using different methods. One might be to use first passage percolation to sort of approximately say what the distance between different regions are. What the distance between different regions are. And then, in each of these coarse blocks of size about log n, we could have some set of points. We can sort of locally reconstruct their distances and then also understand how those are aligned and what's the displacement and rotation between these points in one block and the points in a neighboring block. Points in a neighboring block. And then to do long-range estimation, so between say this block and this block, we want to do something similar to synchronization, summing over part paths of blocks. And we convinced ourselves that we could do this, and it was going to be very messy. Was going to be very messy. Unfortunately, before we started writing it up, we realized that there's a much simpler way to do it. So that's always fortunate. And so the simpler approach is really just counting paths in the random geometric graph. So your vertices u and v, and I'll let s uvl. Let SUVL be the number of self-avoiding paths of length L from U to V. And this will be in the regime where big L, the sort of distance cutoff for the random geometric graph is so a constant times log to the 1 over d. constant times log to the 1 over dn. And so if this is the right sort of regime for connectivity, but rather than taking sort of exactly the connectivity constant here, I'm going to take sort of a large c to be a large constant to make the proof work. Okay, so we're counting numbers of parts between u and v. Let's let xi be a random walk where taking continuous values where the increments xi plus 1 minus xi are uniform on a ball of radius L. So this is, and you should think of this as you're starting off with u. What's the next, if I take just a randomly chosen neighbor of u, this will be a point that's sort of uniform in a ball. sort of uniform in a ball of radius L around U. So that's why we would consider such a random walk. And I'll let G be its density and let rho be the radius, sorry, the volume of a ball of radius L. Then expected value of SUVL, we can just write as rho to the L minus one times the density of this walk. Of this walk. It's a simple calculation to do. And then our estimator will basically just be to use the method of moments. So our estimated distance from u to v will choose so that the expected number of paths of that distance is equal to. Is equal to the observed number of parts. So essentially, just sort of inverting this equation. So it's the inverse of the density of the number of paths divided by rho to the l minus 1. And in dimension d greater than or equal to 4, if this constant is big enough, Big enough, and we're looking at points that are far enough apart. And our choice of L is something a bit bigger than dn over L would be about the total diameter of the graph. So taking something a bit, you know, a constant factor larger than that. So with this choice of parameters and Of parameters, and this is mostly chosen for the convenience of the proof, then the estimator or the error tends to zero. So actually, for really long distances, we're actually getting better and better estimates as n tends to infinity. So it's not even just constant, it's going to zero sort of error. Sort of error, but independent of n. The reason why we can get something that's actually going to zero is, well, the radius of the balls that we're looking at are going to infinity. That means that the degree is tending of each vertex is tending to infinity. So essentially, you have more and more information for each vertex as nth is going to infinity. So it's maybe not surprising that. maybe not surprising that you know you can do a bit better as n grows but but the point is that um a bound is really sort of independent of the distance um okay and uh so in order to analyze this we we just need to sort of analyze the the number of paths between two vertices at distance l. Distance L. Okay, so one observation is that this density g is log concave. That's because after one, like the indicator of a disk is log concave and the sum of like the convolution of log concave functions is also log concave, and by symmetry, it's also radially symmetric. So you can estimate sort of the weight. Estimate sort of the way in which its density is decaying. Because we're considering points that are far apart, we're sort of looking at the random walk, the density of the random walk in its large deviation regime. So this is the, so the so the log of the density will be decaying pretty quickly as we move away from the origin because it's a large deviation. Because it's a large deviation rate. Oh, actually, before I go on that, one other thing I wanted to say was, you know, if we can accurately estimate the distance between pairs of points, then we can pretty well recover the locations of all the points. We could just sort of fix a certain number of kind of anchor points, look at their relative distances. Their relative distances work out sort of how they're placed with respect to each other. And then for any other point, look at its distance from each of the anchor points and kind of triangulate where its location must be as a result. So if we can estimate pairwise distance as well, we can sort of estimate the location of all the points. Okay, so So g, this function g has is well behaved, the density. So, really, we just want to show that the number of paths is concentrated around its expected value. So, the natural thing to do, I guess always the first thing I would try and do is, does the second moment method work? It doesn't. It doesn't. So, unfortunately, the answer is no. The second moment is much bigger than the first moment squared. And the reason is essentially you can have rare things going on in the graph, which cause the second moment to blow up. So, imagine I have a unit box with k points for some very big value of k. Big value of k. Well, the number of points should be Poisson, so probability of having k points is one over e times k factorial. You could ask, well, how does having this sort of dense region of points affect the number of paths? Well, a path can sort of start at U, enter this dense region, then choose one of the k-factorial permutations of the way. Permutations of the ways of visiting all the points there, and then exit and continue on to V. So that's sort of giving you k factorial choices. But there's also a sort of, I mean, if you're making k factorial moves in this clique, it means that the rest of the path has to be a bit shorter. So you'll sort of lose some power of L as a result of that. As a result of that, but if k is big enough, this click will sort of have a cause a big increase in the number of paths. But of course, that's very rare. But it's a small enough increase that it doesn't affect the first moment. But it does affect the second moment because essentially, if you take this factor and square, Take this factor and square it, then well, you get a k factorial squared. The probability only has a one over k factorial, so the total contribution to the second moment will have a k factorial that will sort of blow up for large enough k. So, okay, so you can't do the simplest version of the second moment method, but so we sort of identify. And we've sort of identified what's going wrong with that. So we kind of can think of a way of fixing that. And one way of doing this is modifying it by restricting to essentially paths that don't spend too much time hanging around the one location. So I'll call a good path one that's sort of constantly making progress from U to B. Progress from u to v in the sense that there'll be constants delta and m such that if I look at ui and ui plus m log n, so you move m log n steps along the path, then it's made at least delta times n progress in the direction from u to v. And you can check that a typical path will have this property. So if S tilde is the number of these good paths, then the expected value of S tilde will be only slightly less than the expected value of S. So we'll only lose a little over one factor in the expected value. And And this, and when we're counting good mile paths, the second moment method does work nicely. We have to sort of count some over pairs of paths and look at how many times they're using the same vertices. But, you know, so there's some calculation there, summing over the There, summing over the number of different ways they can intersect and the number of ways you can join the intersection points. But if you do the calculation, you see that actually most of the contribution comes from paths that don't intersect at all, apart from the starting and ending point. Okay, and so this combined with the shape or the derivative of the density function is Density function is enough to tell you that this distance estimator is concentrated. Okay, one drawback is that counting self-avoiding paths might not be computationally efficient. It's much easier to count just paths that can return to the same vertex because you don't have to keep track of where you've been earlier in the path. In the path. So, actually, I'd prefer to do this estimation where we're counting just the total number of paths of length L from U to V, where the path is sort of allowed to return to vertices. So it can sort of have loops, maybe a more complicated sort of figure of H shape here. Sort of figure of H shape here and some over these paths. So we sort of think of this as having nice paths, which are just sort of self-avoiding strings, and then sort of sausages in between, joining these sausages, these more sort of complicated regions. And so now the expected number is not just sort of going to come from a density of a random. From a density of a random walk, it's going to be a density that comes over summing over a more complicated set of objects, essentially, involving these sort of sausage-like shapes that it can do. And unfortunately, in this case, the total number of The total number of paths t is much smaller, sorry, the typical number of paths is actually much smaller than its expected value. These counting sort of just the total number of paths is affected even more badly by having small dense cliques. Because now, if you have the path and it enters some clique of size K, it can stay there for a long time. K, it can stay there for a long time. And if it stays there for time K, then you'll get K to the J possible ways of traversing through that clique. And if you let J be big enough and K be big enough, then you get something that blows up the expected value. So you also can't just compare with the sort of expected value, but you might hope that the typical Might hope that the typical value of t is concentrated. And this turns out to be the case. And essentially, what you want to do is condition that there's no sort of dense regions and regions where there's sort of a much higher density of points than you should typically expect. And on that event, conditional on that event, the typical number of paths is close to the conditional expected number of paths. The conditional expected number of paths. To analyze that, we have to count. So we sort of count paths and we look at the number that have where none of these sausages are bigger than a constant times log n. And we also, again, restrict the paths to good paths. The density becomes a The density becomes a sort of analysis of a more complicated large deviation problem, but it turns out that that's one that you can analyze as well. Okay, so let me just end by mentioning some of the things that we don't know how to do. In d equals 3, I don't know any algorithm that gets sort of good estimates on the distance between pairs of points. Between pairs of points. I've been saying all of these estimates in the case where the graphs are not completely sparse. So the radius parameter for the random geometric graph was big enough that the graph is connected. But you can think about L being constant. So if you do that, the graph's not connected. So some of the vertices would be isolated. And of course, you won't be able to say where they are. Course, you won't be able to say where they live. But for the if L is a big constant, there'll be a giant component. And you can ask, can you estimate the locations of the points in the giant component well? So just counting paths the way I said, you can check that won't work. But you could try But you could try, but one possibility that might work is looking at sort of self-repelling walks, so ones that are sort of not allowed to spend much time close to their past. And then I think the enumeration of those might require something like the lace expansion. And possibly this is something that could be carried out at least in maybe not in dimensional four, but maybe. Maybe not in dimension four, but maybe in sort of for larger values of D. And finally, the algorithms for the localization depended like very crucially on it being sort of a rate one Poisson process for the underlying points. If you start perturbing that a little bit, they'll completely fail. They'll completely fail. So, can we come up with algorithms where you know it will work even in a let's say if the underlying Poisson process had a non-constant intensity? Or more generally, if we're in the setting where we're told noisy distances between pairs of vertices, can one estimate the locations even when Even for sort of deterministic sets of points, as long as they're sort of well enough distributed in the plane. And I think I'll finish off there. Thanks for listening. All right. Thank you, Alan.