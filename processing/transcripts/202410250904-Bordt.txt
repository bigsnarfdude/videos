What is the answer to the undergrad machine? No! Never, never, ever, ever. I'm applied. Okay, so now we're going to say graduate version. That's how those actions, okay? Yes, thanks. So I'm Sebastian. And I want to talk about some of the topics that have recently started to interest me a lot in the evaluation of large language models. In the evaluation of large language models, right? And so, as many of you probably know, there are like a lot of debates about the evaluation of large language models. And one of the key aspects is that we don't have access to a test set anymore, like we always used to have in traditional machine learning, to sort of do like the default evaluation. And at a very high level, what I want to argue for in this talk, that there are well-defined settings, but it actually doesn't matter. So, I want to argue that there are not all, but some settings, relevant settings in language. Settings, relevant settings in language modeling, where the model can have seen the test data during training and you still get a valid estimate in the sense that you can still reasonably well estimate the accuracy, the average accuracy from the test data. Okay. So, like at least since last year when the GPT-4 report was released, right? And like the model had all these good scores on all the different benchmarks, and nobody knew. The different benchmarks, and nobody knew what exactly was the training data that went into this model. Like, people really started to wonder: what is the reason for the good performance? And sort of, you know, immediately, people started to proc the model with data that was after the release, and sometimes found that it maybe performed worse than what was reported in the GPT-4 technical report. And this has really given since rise to an entire literature, which runs under terms like data contamination or memorization and generalization. Or memorization and generalization, where people sort of try to develop strategies to estimate how good these models actually are, right? And I would say, like, at the very end, I'm like highlighting here one of my papers, which was for me also in the background, the motivation now to do this work, and one nice paper that was recently released by Apple. And sort of like at a high level, it's very clear that there are settings where the language model only performs very well because it has seen certain. Very well because it has seen certain data during trend. Like, for example, GPT-4, it will be able to predict past stock prices very well, much better than future stock prices. And this shouldn't really be a surprise to us. On the other hand, for example, in this Apple paper, what they did, they took a standard benchmark and they sort of changed all the numbers and the nouns and everything in this benchmark to sort of be able to redraw new benchmark question instances. And they found that, like, And they found that, like, for this most powerful model of, or one of the powerful models of OpenAI, this UT4O, it actually only made a tiny difference, only about one percentage point, right? If we sort of like change the evaluation in this way, sort of suggesting that the model is really actually able to perform this kind of task, and also that like the original data set already give like a reasonably good estimate. Okay, so this just for background. And now I want to take another step back and quickly talk about the independent test set. Because if you think about it, it's like kind of crazy. We now have all this murkiness in the evaluation of large language models. But historically, machine learning really became big with this idea of an independent test set, right? And for example, I guess people have different views on this, but like David Donahoe in this review paper, 50 years of data science. Paper 50 years of data science. He argued that sort of like this approach of having the train, validation, and test split and allowing everybody to train on the training set and everybody was evaluated on the test set, that this is like really what set machine learning apart from statistic and also made machine learning big easily applied. Because it allowed us to develop all these different models and have a fair evaluation criteria. And there's actually some interesting research on this. So I just want to highlight this one. On this, so I just want to highlight this one paper. So, this paradigm is like surprisingly valid. There's a literature that started in 2019 with this paper where people sort of regenerated the ImageNet and CIFAR 10 test sets. And perhaps surprisingly, they found, although there was a drop in the accuracy, the ranking of the performance evaluations of the classifiers was actually the same as on the public available test set, which had been out there for many years. Right, this is so expensive. So, okay, like from a traditional statistical perspective, right, you can only test so many hypotheses on a given amount of data, right? And so, obviously, this makes you worry if you have like a fixed test set for 10 years, that maybe like after 10 years you've tested so many hypotheses, you're overfit, right? And so, like, they actually sort of tried to sort of resample, recreate new test sets for image net densify. For image net and so forth. And they found that there's like an accuracy drop if you evaluate the models, but like the ranking of the models is still the same. By the create, you mean they use the same procedure, but generally you exactly, Lara, like they tried to do this, and of course, I guess this is much more difficult than you would imagine to actually do this, like in an IID way, right? So probably there's like a bit of a distribution shift going on here. Yeah. Okay. Okay. And also, I want to say it's very clear. Like, if we start to think about it a bit, also, maybe theoretically, which maybe I want to pitch a bit for this audience, because I think these questions are actually interesting theoretically. So it's very clear that there are well-defined settings where we absolutely need an independent test set. And one setting that we understand theoretically well, for example, is benign overfitting. If you are in a setting where you sort of interpolate noisy training data, it's like, Noisy training data. It's like directly clear you can't use this data to estimate the generalization performance of the classifier, right? It's like not possible. And also, from a more practical perspective, you know, there's this nice review paper by Sasha Kapoor, Leakage and the Reproducibility Crisis, where they basically go over all the different ways that sort of like test data can leak into the training data and practical applications of machine learning. And they basically find that there are like many applied results that just Many applied results that just don't hold because, like, they didn't properly separate the train and test data and make no applications of machine learning. Okay. I think the more further problem is not testing, but it's the selection of what you publish. I mean, you have your algorithm, you test it on different sets. You only publish the one that performs best. Yes. And that's a different problem. It's also a problem. It's also a collection of what you've got. Right. Yeah. I guess, although in language modeling, there are like kind of these standard benchmarks on which you are supposed to test your model. I mean, like if you see the GPT-4 report, and also one of the motivations for this work was they always showed you this big table, and there are the numbers for all the different benchmarks. And then they showed for the new model, all the numbers are five percentage points higher than for the last model in the model. Points higher than for the last model and the competitors, right? And sort of like, I was, I'm like trying to understand what does it actually mean, right? Is there like some validity to this table or not? So is your goal to say that like the performance on the training set is a good estimate of the performance in test? Like, I mean, there are a couple different things you could hope for, right? So what I'm like, very good question. So the specific settings that I'm interested in is like the leakage. That I'm interested in is like the leakage question. So, if you know, like, some of the test questions maybe leaked into the training data. So, I have a big training data set. And, you know, let's suppose all the test questions leaked bombs into some points of the training data. Sort of, I want to ask how bad is this really for evaluating the model in the end, like on these setting points. On the test set. On the test set. Yeah. And I'm going to show you experiments where it will show that there's one setting, like where it does matter, but if you scale the data enough, it starts. The data enough it starts to give you a valid performance in this estimate. And when you say skill, what do you mean that the test set becomes a shrinking fraction? Good question. So like I keep sort of the number of times that I insert the benchmark questions into the training data constant and I increase the overall size of the training data. And sort of like um you know, you see, so this is what we're interested in. Are there also well-defined regimes like where does it matter? Regimes like where it doesn't matter. Okay, and now sort of on the slide before we sort of like asked about the settings where it does matter. So are there like, you know, like arguments why it actually might be okay? And for like me, sort of like the main argument why it might be okay, like is because we know that there are like connections between stability and generalization. And there are like different notions of stability of a learning algorithm. But at a high level, it means if I sort of add But at a high level, it means if I sort of add or remove individual points from the training data, like the output of the learning algorithm, like measured in some norm, it can't change too much. And sort of here the setting, if we think about like we're inserting a point into the training data and we're asking how much does the function change on this specific point, right? Sort of therefore sort of we need this notion of uniform stability, sort of like the supremum difference between the two functions over all points. You don't get this stability once you insert or delete as a constant function of the whole train data. It will be a shrinking fraction. Well, it has to be a shrinking function in order to. Yes. I mean, that's a good point. I mean, and maybe this is also what we will see in the experiments, right? Because kind of the point is that in this, like, what is in the background of it, like, the training data is really large, right? Sort of, yeah. Yeah. And I guess it'll be crucial in this case that we're like not getting, this is like LLMs and not vision models where we generally train to zero training error. This is another important difference, right? Yeah. So also like a bit at the back of this question is sort of how does like the structure of the LLM training data give rise to the behavior of the final model? Or even the training procedure where you only touch each data point once. Exactly. Right. Yeah. It was. It was yeah um the thing about stability I guess you don't know whether stability holds for these algorithms right for the elements for instance. Yes. And if you knew they were they're generalized you could maybe infer that stability holds but you want to check if they're generalized so it's kind of natural. Yes. So I guess maybe what we will see in the experiments is maybe just an empirical demonstration that they are actually stable. Yeah. This is like maybe how you could frame the results. Yeah. Okay. And sort of like I guess like sort of the Okay, and sort of like, I guess, like sort of the empirical analogy to sort of this is that you know, like, I mean, these models are trained like from 100,000, so even millions of gradient steps, right? And like one gradient step, it makes like a tiny modification to the weights of the model. So, how bad can it really be? Okay. And now, before we go to the experiments, like a disclaimer, but we've already discussed this, so there are many other relevant ways in which the performance evaluation. Ways in which like the performance evaluation of language models could be like gamed, or you know, could there could be many other problems, right? And like, we've also seen this, for example, like I could generate synthetic training data that is like generated around the same topics of my benchmark questions, right, and rephrase this often and have this as a constant fraction of my training data. And then, like, of course, I'm going to sort of, in a sense, artificially increase the accuracy on the benchmarkers. There are sort of like many of these aspects. There are sort of like many of these aspects. And this talk will only be about this: you know, what happens if the benchmark question occurs like once in training? Yeah. Okay. And now what are we going to do in the experiments? So we are going to train like relatively small language models from scratch. Okay. We have test data. That is, we need a lot of test data points. So we take all the standard language modeling benchmarks and sort of we concatenate them into a big universe of benchmark questions. Into a big universe of benchmark questions. And then we sort of like split it into random parts. And some of the parts we insert into the training data n times, and like an other part, I keep as holdout. And then in the experiments, I want to measure the accuracy difference between the holdout and sort of the performance on the benchmark questions. What other training data are you using, except for these benchmark questions? A good question. I use a data set that's called the FineWeb AdU data set. So there are now on Hugh. So there are now on hugging phase like a lot of language modeling data sets. So you could think about like it's like not like the raw internet, but it's like a filtered version of common crawl. So what people have like recently done is you take like some training data and like then you apply a classifier to the data point from the internet and you ask it, is this actually good training data? Is this like informative content? And then sort of all the sort of sets where the classifier says this is like educational content in this. Says this is like educational content in this case, this is put into the training data. But how big is that? You can have trillions. I'm training here on billions of tokens. Okay, a serious subset like this. No, no, like I have like a stream. Like for the compute that I have, I always have enough training data. So this data set is one trillion totals, I think, the full data set. Yeah. I guess I feel like. I guess I feel like it matters what is the proportion. I understand that you go until the end, you take gradient steps and you just run out of gradient steps until you convert. You don't go through all 1 trillion tokens. Yes. But how many tokens do you get? Around 40 billion tokens, I would say. So in practice, I ran out of compute much before that. Yeah. How large is the 9 minutes? Say that again. How large is the 9 minute 100? Say that again? How large did I be on the line? Up to 1.6 billion parameters. Yeah. And then I will have some other experiments where I try to scale this up even further. Yeah. Okay. So as also many of you guys probably know, in language modeling, everything is about scale. And sort of like here for this problem, there are like three dimensions of scale that I think are sort of like most relevant. Like one is sort of the number of parameters of the model. Because there's sort of a lot of research that shows. Sort of a lot of research that shows, and also like a lot of intuition, if I have like this very high-dimensional parameter space of the model, you know, maybe like the model can somehow encode all the data points in all these ways because it has so many weights, right? And sort of like the second dimension of scale is the number of repetitions that I have in the training data. And the third dimension of scale is the overall scale of the training data, keeping the number of repetitions constant. So maybe that I move away from the constant fraction and go to stability, right? Okay. Stability, right? Okay. And here, sort of in this plot, we see like two dimensions of scale. Sort of on the x-axis, we see the number of model parameters. And sort of I always normalize, sort of like in these plots, I sort of depict the accuracy gap, right? Sort of actually like these models get increasingly good, but I always depict like the performance difference between sort of on the holdout, depicted in blue, and sort of in colors, the number of times repetition. Okay? So the the measures the accuracy improvement when you entered the test set into the training set? Exactly. So like if you wanted you could say we are seeing here the causal effect of inserting certain benchmark questions into the training data versus not doing this, right? Yeah. And sort of I'm doing four times repetition: 12, 32, and 144. They're kind of arbitrary numbers. Yeah. But sort of But sort of, I mean, you should think about sort of the baseline accuracy is around 50%, 40-50% of these models. So here we are sort of in a regime of above 90% of about 95% absolute accuracy. So this is like the regime of like epsilon cobalt. Yeah. Okay. And I choose this sort of so that here the differences are like statistically significant even for like the smallest. Okay. Number of repetitions as a fraction of the total number of tokens somehow. Like, just to make a sense of if I wanted to extrapolate these results to larger models, how would this like a good question? It's like a crucial assumption, right? So, would you want to assume that if you scale the training data, and if you have leakage of the test questions, also like the number of times that you leak also scales with the size of the training data? Or do you assume that you can like scale the training data with sort of clean data? And if so, it sort of suggests you could just use that clean training data. Just use that training data if you care about this, right? Exactly right. Sort of like one of the things here we see is like it could also be the case, like it matters at what point you sweet during training, right? Sort of things that you see early, they are not as important as things that you see late. So if you can, you should like continue training on keen data as long as possible. But it's like very crucial, sort of, yeah, so it's very important, I think, sort of like to exactly think about like the assumptions and like what is your setting, right? And like I'm not claiming here that like these results are I'm not claiming here that these results are applied to all settings. But I want to say that there are settings that I think are realistic, and maybe to specify this, there are now all these 7 billion parameter models that are trained on trillions of tokens. And I think for these models, these results really apply. Because there, the scale of the data is so big. But this actually looks like when you have more parameters, you get more memorization. Exactly. This is what we see here in the plot, right? And this is also what we would expect from the difference. And this is also like what we would expect from the distribution. Sort of, you know, like this plot is completely expected. Sort of in increasing the number of parameters, we get more memorization. And also increasing the number of times of repetitions, we also get more memorization. But sort of the key thing is, oh, there's also the other effect. Sort of here, we sort of keep the number of multiplier niters constant, and we sort of scale the amount of tokens that we train the model on. And for the language model conversation, And for the language model connoisseurs, I depict it in Chinchilla tokens, which is some measure of the training data of the model. So, what does it measure? So, there's like this Chinchilla paper where they sort of if you have like an n billion token parameter model, an n billion parameter model, you should train it on 20 times x, 20 times n, the amount of tokens, right? It's sort of like The amount of tokens, right? It's sort of like some rule that comes out of this one famous paper where they say, How many tokens should I train the model on? And this is compute optimal in a certain sense, because you can think about like you can make the model better by increasing the number of model parameters or by increasing the amount of training data. And the compute that you need to spend, it scales linearly in the amount of training tokens and also roughly linear in the amount of model parameters. And what they said in this famous paper is: how should you train? Famous paper is: How should you train like the training data? If you double the number of model parameters, you should also approximately double the number of training tools, right? So, again, what the x-axis, what does it measure? It measures, is it just like the size of the training data set? In multiples of the size that they recommend. Exactly, like in multiples of a quantity that is in a certain sense appropriate for this model. So, this is like a clear drive. And sort of here, we see some. And sort of here we see sort of like if this quantity sort of goes up to 15, which is the regime that some of the models that we now have are actually in, right? Then, like, few times contamination here in this experiment, they are like roughly within the confidence interval of the whole model. Okay. And now, of course, like the key question is, we have sort of like these two scaling dimensions that go against each other, right? Like scaling the number of model parameters and scaling the number of tokens. Yes. So just to clarify, so you said before that you actually run out of compute. Before that, you actually run out of compute before you run out of your training set. Yes, so in what sense does it matter the size of your training setting? We're not going to go over it. Right, like, oh, okay, okay. Like, this is, um, good question, sorry, this is like the effective training set, right? Like, that is like, you know, on Hugging Face, they released this super big data set, and I sub-sample this to create like a data set for my model. And this is like the size of the data set of my model. And you do go over. And you do go over all of it. And I go over all of it like bonds. Yeah, like promise. Good question. Okay. So, yeah, just make sure you understand: you're wanting the model size to be the same and scaling by the number of what happens when you keep the ratio the same as the rough. Yes. Okay, this is like the next optimal, like the next obvious question. Sort of what happens if we sort of follow this chinchilla scaling and we sort of, if we double the number of model parameters, we also double the number of tokens, right? And like for me, the ideal And for me, the ideal result would be if it would be roughly constant, right? Because this would also mean that we could maybe extrapolate from the much smaller scale that I have here, the larger scales, which is what we are actually interested in. And so it's not really completely constant here. This is some development. But there's no super clear trend here. It roughly seems to be like a constant. Can you move back a slide? So for each data size, So for each data size, do you go over all the data set once? Yes. So here I train four different models here. And I also do like, yeah, I train four different models. Accordingly, like for each model, I do like a cosine learning rate schedule over the appropriate data set size. But for a smaller data set, that that means you get just fewer gradients. Exactly. And also the ratio of the benchmark questions in relation to the data set size is just high. To the data set size is just high. So, what happens if for a data set that's half the size, you just train it, just iterate it over like twice instead of once? Good question. I didn't try this. I mean, I think generally, there's like this interesting difference. Also, if you train the language model, I think multiple times on the same data set, you go a bit more into the regime that we maybe have in computer. More into the regime that we maybe have in computer vision, right? Where you like actually maybe start to interpolate the training data like overall, and then maybe you still generalize, maybe not. I haven't tried this, and I don't know if people, yeah, I don't know anything about this. I think people have generally found that it's better to have like more fresh data. Like, I don't think people have found like giant benefits to like have the data, you might as well. Yeah, yeah, if you if you have additional data, better to use the additional data. Yeah, obviously, but if you don't have the additional data. But if we don't have the additional data, it makes sense to be able to do that. Right. I don't think people even really operate in that space for LLMs because there's so much trading data. Yeah, I was just wondering with this experiment, whether that was a problem. So I have some more empirical results which are like, okay, which I will go over quickly. So one additional experiment that I did is, you know, like there's like, many of you have maybe also probably heard this, there's like this term of forgetting. Heard this, there's like this term of forgetting in deep learning research, right? And sort of like you can show that, like, the reason for sort of why if you scale the training data, like the amount of overfitting goes down, it's actually like forgetting, right? So the experiment that I now do is we sort of can insert all the training data at a very specific point in training. And then we sort of just continue on clean data. And we sort of like look how sort of the accuracy overfitting develops. Sort of this is like the accuracies directly after I inserted the benchmark data. After I inserted the benchmark data. So it's like hugely overfit. Okay? And then I continued training, and fairly immediately it goes like really strongly down. But it's like still reasonably, I mean, we are still here at 10% of the points. So can you tell that experiment again? Yes, so here I'm training also a model, one model now, for 15 epochs. And I insert all the benchmark data between the first and second epoch. Why would I do like one big cosine scheduler? Right. Right. And then I sort of measure after every epoch what is like the accuracy on the different bits that I've inserted. Yeah. And then we sort of can see how it goes down, down. And if I've like continue training for far enough, it becomes staticity incident. Right, basically similar to what we've seen in the previous video. And then you can do more stuff, you know, like for a bigger model that I can't entirely train. Or, like, a bigger model that I can't entirely train from scratch, I can take a checkpoint and insert the benchmark data at the checkpoint and then continue training. And then you can also see the forgetting and so on. I'm currently trying to scale this up as much as I can. It's maybe not that interesting to do that. And now, sort of, what is like a bit maybe like sort of the conclusion from all of this? Sort of one thing that I think that's interesting is that we've seen that sort of Seen that sort of the regime where it doesn't matter if certain data points were included in the training data or not. It's like a regime that is in a certain sense inefficient, right? Like where the amount of training data is very large in comparison to the number of parameters of the model. And there's like this interesting recent paper from Dan Roy and co-authors, where they study a very different setup, but like related to the same overall question. Question. And they sort of conclude that, like, any sample-efficient learner needs to memorize a constact fraction of the training data. Does that mean it cannot be differentiable? This is what they mean in this paper. This is the definition in their paper. Yeah, exactly. Yeah. And sort of like this sort of led me a bit to this personal hypothesis that I had, you know, sort of maybe like if the learner is in a certain sense data inefficient, right? This is what sort of like allows us to see the test data during trend. To see the test data during tracking, right? But like for an efficient learner, and we've also seen this in the chinchilla optimal plot, right? I mean, you can also interpret the results the other way. If you scale sort of chinchilla optimally, then sort of contamination leads to overfitting, right? Even like if you insert it only once, you get like a couple of percentage points overfitting due to the insertion. Like in this lim if you if you were to take the limit of this graph as you scale the model like all the way, right? Scale of model all the way, right? At some point, increasing model size, some of this becomes essentially not parametric, right? Like increasing model size that much, and then, but the contamination traction is broken down. Like, wouldn't we expect all the way at the end of this thing, all those lines to go down to zero? Even in the chinchilla optimized scaling. But at some point, like parameters isn't the right. Like at some point, you're just learning some kind, essentially, like something not there. Some kind of essentially like something not that. Oh yeah, I I maybe not experimentally. But like in scale, if you were to like really explain this line, would we expect It's very interesting that you say that because I think most, I would say the most obvious criticism to this work that I would expect to get is to say the number of parameters is just too small, right? If I scale to trillions of parameters, the model is going to remember so much better. And you will have this overfitting and memorization even with few samples, which is in a sense the direct opposite of what you have just been saying. At what you have just been saying. So I guess this is the intuition that many of the, I think, many people would have. But I mean, I would, I've never thought about this so far. Yeah. Actually, that doesn't totally like, why wouldn't it just memorize vocal training? If you train it forever and it doesn't connect to best, if you want to memorize everything it's ever seen. Maybe, but on the other hand, the fraction of contaminated data is a limited help. No, but this is the one where I thought this was the experiment where you scaling like the data set side. I thought you're scaling the model set of the data set. Oh, yes. Keeping the contamination confidential. So keeping the number of contamination is constant, right? So the fraction of contamination is still... Sure, well, the fraction of the contamination is still because he's not scaling the training set of size, right? He's only scaling the model set. So basically, you have a bit of a. So basically you have a text I don't have it in the in the TV model. I could I could see like either way happening. Like we we the intuition for why why you might not go to Tao is like what you say like a big model one minute might be enough to one. It's gonna be a matter of whether which one is scanning faster, you know? Like if they're both winter at what rates are they going? Because if the model size is going faster, then you can rather exactly. And if the data size is going faster, And if the data size is going faster, then maybe it drowns out.