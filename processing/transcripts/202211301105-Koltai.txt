So we thank you very much, Rupert. And obviously, especially to the organizers for having me here, it's a pleasure. It's going to be on collective variables. And I'm going to talk about multi-scale or multiple time-scale systems as well. Maybe a little bit off the mainstream, what we have seen in the previous days. I'm also going to talk about manifolds. They are definitely going to be off the mainstream. It's not going to be your usual. Be your usual stable, unstable, critical, and so on manifolds. So please try not to put them into those drawers because they live in completely different spaces. But we will see as we get there. I will have to start with an epsilon of data science. So here's my take on dimension reduction of high-dimensional data sets. What you see here are thirty-six images. Are 36 images in a 6x6 array, like no particular arrangement. These all have approximately 10,000 pixels, so they are basically 10,000 dimensional vectors. There are 36 of them, and the question is, I usually pose in my audience, how to reduce this or how to describe this set of images in a very simple sentence. And the answers, which I usually And the answers which I usually get are revolving around something like: well, it's a head of a person taken, the image taken from different angles. And, you know, that nails it. There is a one-dimensional variation which we pick up by basically human perception. There might be some other sources of variation, like lighting change, camera tilt, change in the mimic. You might see it, or you might not see it. You might not see it because you are sitting way away. But it's insignificant. So we try to pick up the largest variation and that is sufficient to describe the data set. Something similar I would like to do for complex dynamical systems in this talk. And the message which I will try to convey is going to be that such kind of effects pop up in Of effects pop up in all kinds of different applications. The way to formalize this mathematically, this dimensional reduction, is to have a high-dimensional ambient space X. There is the data set D, and we would like to have some kind of a mapping which maps it in a low-dimensional set. In this case, this would be two-dimensional, and the structure itself is what we. And the structure itself is one-dimensional, coming from 10,000 to one or two dimensions. This is a parametrization, basically, what one would like to learn. And there are methods to do this. And those are called manifold learning. And for no special particular reasons, what I'm going to use in everything that you're going to see is diffusion mass. But the details are not important. There is a way to get And there is a way to get information on this side of the picture and obtain the mapping from to here. That's the important part for now. Alright. What you will see here is a movie of a molecular dynamical simulation. I have to find the cursor. And what you see here on this allen dipeptide, very simple molecule that it evolves on multiple time scales, there is this fast fluctuation on the atomic level and on the coarse geometric or conformational level it changes its rough geometric shape and that's color-coded on the left as well on a comparably lower or slower or larger time scale. Time scale. And exactly this slow time scale dynamics is what is important for the biomolecular or biochemical properties of this molecule. And that is what applications or people from the applications are interested in. So especially, what are these so-called matter-stable conformations? Where are they in the high-dimensional state space, right? Each atom has three or six degrees. Has three or six degrees of freedom, and what are the transition rates between them? So, what are the time scales of switching between these different colors? This is what we would like to pick up. And for starters, we have this high-dimensional description. 14 atoms, again, each have at least three degrees of freedom. The question is: how do we see where the conformations are, let alone what the time scale is? Are, let alone what the time scales are between them. If we know a little bit more, if we have an oracle, things get more simple, right? Look at the backbone of this molecule here as a cartoon and this so-called torsion angle or dihedral angle. That's basically the twistedness of the outer limbs around this central piece. This is a one-dimensional observable, and if I plot the molecular dynamical simulation. The molecular dynamical simulation in this one-dimensional observable, I get this time series, which makes everything clear. If I look at, for instance, a clustering of this time series on this one-dimensional interval, 0, 2π, I have everything that I want to know about my conformations, and I can just count the time scales. So, in this sense, this scales. So in this sense, this theta parameter, this variable, is a collective variable, it describes the collective motion of my 14 atoms here. And this is something which we would like to find and be able to compute. So it's mapping from here to here is what we are after. And the question is how to obtain it in general situations. Well we need definitely. Well, we need definition for what properties do we like to inherit or keep under this mapping. And our answer to this, there are obviously multiple different ways of approaching it, is that the slow time scales of the time series viewed through this so-called collective variable should approximate the slow time scales of the original time series. Original time series. And what I'm going to show in this cartoon is how do we characterize or how do we actually get here and that there will be a theorem telling us that this construction actually gives us an answer to this desire. But it's not obvious why it is the case. So let us look at this diffusion process. We have a potential energy potential landscape and there is some small diffusion to And there is some small diffusion to it. For the sake of the cartoon, this two-dimensional picture, A and B indicate small values of the potentials of potential wells. And I'm now kind of simulating for you what this trajectory in this potential landscape would do. You have seen something similar in Near Bergen's talk, right? The diffusion in a potential landscape. It will hop back and forth, but mostly it will oscillate around. It will oscillate around potential minimum. Now, what I'm going to do is take any initial condition in this two-dimensional space here and take a time scale, which I have to choose appropriately, tau, and plot the so-called transition density function associated to this SDE in to the right-hand side, which is L1. side which is L1 for the space of probability density functions. So this p tau x dot, if you can read it, that is just the probability density of process governed by this SDE if it starts at x after time 12. So if I start in one of these white points, then after some intermediate well-chosen time scale, it's more likely that I Chosen time scale, it's more likely that I'm going to be close to the potential minimum B than to the potential minimum A. So my transition density functions are going to look like something like this. There is a bigger bump on the right and a smaller bump on the left. And it shouldn't really matter from which white starting point do I start here. Meanwhile, from the gray starting points here on the left, I'm closer to the set A, so I will see afterwards a transition density function. Transition density function, which looks like this. And again, the vertical coordinate doesn't really matter too much in the rough shape of this transition density function. I see variations in this transition density really only if I push my starting point horizontally. That's the main idea. So if I start in whichever point here and I map all possible start I map all possible start sorry transition densities for all possible starting points here into the L1 space, I get this gray sausage kind of structure, which is, in this case, approximately one-dimensional, so I can find this one-dimensional core, hopefully I can find it, parametrize it, it's one-dimensional, pull back the parametrization to this space, to the space of starting points, and what I Points, and what I should see is something which varies from left to right that is going to be my collective variable, or that is going to be my candidate collective variable. So I map all starting points by transition densities into this infinite-dimensional space. Here I have something which should be a one-dimensional manifold in this case. And if I find a parametrization of this approximate manifold, Of this approximate manifold, I go back and I hopefully have a good collective variable. That's the idea. And the theorem is that it really works like this. For these kind of systems, if I'm looking at this set in the space of densities, if it's epsilon close to an R-dimensional manifold, and R should be some small number, then there is an R-dimensional collective variable. Collective variable, right? So this ψ maps into an R-dimensional space, reproducing the slope time scales up to O epsilon. This is why the cartoon on the previous image was describing good collective variables. And note that this is a constructive approach. You can implement what I have shown you on the previous slide. What I have shown you on the previous slide, and there is a quantitative goodness measure because this epsilon translates to zoo epsilon. Alright. So everything started with an if I am close to a low-dimensional manifold, then things work out. And there are systems for which this if is actually satisfied, systems of multiple time scale. X is a slow variable, y is a fast one. Is a fast one. In this case, x will be itself, so xy mapped to x will be a good collective variable. Metastable systems, where the spectrum of the generator has a spectral gap after the kth eigenvalue, that will have a k or k plus 1 dimensional collective variable. And there is something in between where you could think of, well, this is again a cartoon, something which is metastable in this direction, but there are lots of. This direction, but there are lots of small metastabilities, and something non-spectacular is happening orthogonal to that, so that it's not interesting. It's a metastable system because each little well will be somehow keeping the process in it for a longer time. But there are dozens of it, and we don't want perhaps a dozen-dimensional collective variable. But if we look at something like an effective slow-definition, Like an effective slow diffusion along this structure, we see that basically a one-dimensional collective variable should do the job. So this is somewhere in between these two at the top. And I think one should be able to formalize this somehow, but I haven't so far. Please don't read this slide. The important part is down here. This is just an example. Down here. This is just an example. Here we have this lemon-slice potential. So this is two-dimensional space, x1, x2. Seven potential wells, these are the dark blue regions. The process, again, evolves mostly in them and switches to one of its, usually the neighbors. So this white dashed line should be something like a transition path between them. So we expect a one-dimensional collective variable, and we lift this system into ten dimensions by doing. Dimensions by doing something confining, but again, non-spectacular in dimensions 3 to 10, such that it's a little bit more interesting. And we learn the collective variable by mapping this dynamical situation into the space of densities, learning a one-dimensional parametrization of this so-called transition manifold. So, this is an image of that. Each point here represents a transition. Point here represents a transition density from a different starting point in this picture. And once we have a parameterization of this set, we pull it back to initial states. And you see, we really parametrized the angle as we expected to do it. One can, since that was the original motivation, use it in molecular dynamics, and my colleagues have done this. This is some protein-folding example. The color in the image represents the... The color in the image represents the value of the corrective variable, and the insets show that we are really parametrizing the folding process of this NTL9 protein. And there is a little bit more to it. So now to switch gears, this is going to be about mixing. Going to be about mixing in fluids. So think of your coffee, pour milk in it, stir it, it mixes, right? How do we describe it? There are many ways to describe mixing in a flow. The way we are going to look at it is very much inspired by this coffee picture. If there weren't any molecular diffusion and you would have an infinitely good eyesight, then if you stir, Then, if you stir, then milk and coffee gets kind of filamented and intertwined into one another, but you will still see black and white separately. It's molecular diffusion which smears the picture, which makes things mix. And we are mimicking this by our description here. We are looking at a non-autonomous fluid flow, right? This is this ODE here, and we are perturbing it by a And we are perturbing it by a little bit of Brownian diffusion, so epsilon is small. And we are interested in the spread of trajectories of this SDE, sorry, the spread of this SDE on trajectories of this deterministic flow. So if you like it more like this, we are looking at the solutions of this SDE in the Lagrangian framework. So we are pulling back everything to trajectories. To trajectories of this. This is the process we are looking at. And if we ask for collective variables for this process, what we get is the following. Here on the right, I'm showing you trajectories of this system, and on the left, I'm showing you learned or computed collective variables from this set of trajectory data. Here's a flow going on on a silly. Going on on a cylinder. There are no colors at the beginning. The colors arise as going here, learning collective variables. So, this is a three-dimensional picture here on the left. I'm clustering in this left-hand side picture. This is how the colors arise. And then I impose the colors. Then I impose the colors to the right-hand side video and low-hand. And lo and behold, similar colors stay together. They don't mix with one another. So, collective variables for this process here show me points which have, in a mixing sense, a very similar fate. They do not mix with one another, and this fishbone kind of picture unfolds a mixing-related geometry of whatever you have seen here on the earth. Seen here on the road. And this can be connected to something other people have been working on, especially Gary Freueland coined something called the dynamic Laplacian, and we regained the dynamic Laplacian in this collective variable learning process. Now, what am I doing in time? Use that 20 minutes. Use that 20 minutes. So we can do it for real data, and I'll skip this since I want to get to a point. We can talk about the ocean picture later. This is still fluid dynamics, but not anymore in the physical space, but in phase space. And I'm going to skip this for temporal reasons as well, since uh I seem to be slower than I thought I will be. The point is that we can learn collective variables. Can learn collective variables also in something which seems to be the solution, actually, the experimental solution of Nebier-Stokes equations and gaining some additional information on this process with respect to what colleagues from physics have found. And we are doing it in different geometries. But what I would like to talk about in the remaining couple of minutes is yet another kind of dynamical model, namely opinion, dynamics, opinion spreading, or if you wish, epidemic spreading. On a mathematical level, they can be very similar. Consider a fixed network of nodes. Of like nodes, agents, whatever. There is each node, each agent, has an opinion. There are finitely many, let's say two, and they switch somehow stochastically depending on what kind of opinion concentration do they see in the neighborhood. So, a simple stochastic process with random switching, and we would Switching, and we would like to forecast whatever is happening in this system. The motivation is that all what we can measure are actually macroscopic observables, like poles, if you think of political opinion, for instance. And also what we are interested in is to be able to say how macroscopic observables of these opinions are going to look like in the future, not really whether, you know, still. Really, whether Steve or John are really going for this party or the other. So, not the microscopic, it's the macroscopic information what we are interested in. And this somehow alludes to collective variables. And in this particular setting, opinion shares in the whole population seem to suggest themselves, and they are going to be good collective variables. So, we are asking for what is the amount of Asking for what is the amount of opinion m at time t. There are continuous time versions available which we are actually working with. And what we see is a kind of concentration effect or a convergence to a deterministic mean field limit or mean field equation. So, what you see here on the left are different random simulations, many of Simulations, many of them, of this very same system, on a randomly drawn graph of two different sizes. The blue, the shaded regions are always the variance. The dashed lines, if you can see them, are the mean. And there are, I think, hundreds of simulations represented in this one picture. Blue is thousand agents. Orange is ten thousand agents. You see that everything seems to converge. Everything seems to converge. The more agents we take, the more narrow the band is in which we evolve. So there is almost sure convergence as n goes to infinity, and we were able to show this for random networks. So drawing random networks, once at the beginning, freezing the network, running the process, convergence of the stochastic process from the previous slide to the deterministic. To the deterministic ODE here. Both for Edde-Schelleny random graphs, which means that each edge between two possible neighboring nodes are drawn in an IID fashion with a fixed probability. This is the probability which we need to make this work. So you see the expected degree is going to be log n, which means it's a pretty sparse graph. A pretty sparse graph, and we also have the same kind of results for random regular graphs, which means they are random, but every node has a fixed degree, let's say d, and this degree has to converge to infinity. So this omega is 1 over little o. Yeah, this is the notation. So edge probability here has to grow faster than log n over n as n goes to infinity here. The degree To infinity, here the degree has to grow faster than one as m goes to infinity. And then we have other kinds of models like stochastic block model for something like a clustered population and also heterogeneous populations where you have different kinds of people showing different kinds of resilience to opinion influentiation. And this I would like to view as a zeroth order model. Order model for this convergence to this equation, the zeroth order model for collective variables. I'm still not quite sure how to connect it to the previous theory, what I was showing you, but I think we made a first step here and we are working on kind of trying to bridge together these two. Together these two realms. So, since my time is up, what I tried to convey through lots of pictures is that there's a framework for collective variables, which was inspired by statistical physics or molecular dynamics, which we came up with, but it delivers very sensible answers in different kinds of fluidic situations and also agent-based systems. systems and I'm wondering how universal framework is. Is there something else which can tell us how collective variables arise and when do they arise in different dynamic situations, even if we alleviate perhaps the reversibility condition which we had in our theory and so on and so on. I'm grateful to my colleagues with whom I was able to work With whom I was able to work on these things and obviously funding, and I'm very grateful for your kind attention. So, thank you. Thanks for giving the time. Our floor is open for discussion. Yeah, we should meet. Yeah, I think that at the start of the talk, you talked about collective variables where the synapse. It kind of comes from a slow time scale in the original system. Did I get it right? But I think that in the networks a lot of the time there might be collective but you can't find explicit slow time scales in the original system. Is that what about? Um yes, this is this is what makes uh this part I suppose quite hard to nail. Um actually this is if you wish something slow compared to whatever is happening transversal to it because there agents in a large network very quickly change their respective opinions, but this kind of fluctuation is happening such that the Such that the, let's say, the bulk, the mean, or the expectation still sees a very deterministic drift. So maybe there is a hope to pin this down the way I was introducing collective variables, but maybe one needs a completely new theory and maybe it doesn't fit. So, yeah, I've thought about how to make this. Thought about how to make this scale separation happen here. It's just unclear to me how the shape of the transition density functions looks like in this situation. Because it's so high-dimensional, it's hard to develop a geometric intuition for it, at least for me. I was wondering, uh, this plot that you have here is in some way related to the order parameter that one has for like a networks of oscillators or things like Of oscillators or things like that. So, this is like the stochastic version of it in some sense? If you wish, yes. Order parameters is one field, one scientific field talking about collective variables. Reaction coordinates is another field talking about collective variables. It's just the same side of the different, sorry, the different side of the same coin. All kinds of different fields have come up with different names very well. Order parameters. Very well. Order parameters is just one example of collective variables, if you ask me. Thanks. I have two more contributions. The two of you you negotiate who go first. Okay, so when you do the fluid dynamics examples, so there are different versions what you can do as a collective variable. So which one have you used? Can you elaborate that? Because you can use Lagrangian descriptors, you can use main. Descriptors you can use, maybe, so to say, for identifying these kinds of vautic structures, you can take auticity or you can take gary valence approaches. So, what did you really use? I used Weinstein. Sorry, to be scientifically more serious, Gary Freueland's coherence business is a Lagrangian descriptor. Business is a Lagrangian descriptor in physical space. That was the previous slide, which I'm not going to go back because otherwise, somehow movies don't work well. So you recall the mixing case. That was based on coherence, and coherence is something which Robin is also going to talk about a little bit in the next talk. This picture is not about coherence anymore, it's about state space, if you wish, of the Nebi-Stoke situation. Of the Nebiestov situation. At least this is how I would formalize this experimental situation here. And here I didn't use any descriptor. I had measurements, measured time series. I really learned collective variables and I obtained a two-dimensional collective variable, which this figure is comparing to whatever these two things which Weiss and others, two physicists who Two physicists who built this couple of meter large container, actually, somewhere in Göttingen, obtained. And those are the colors. And the only message here is that the position of the points here, which is our collective variable, and the colors, somehow seem pretty well correlated. So we find the collective variables which they find or which were they interested in without knowing anything about the physical setup of this system and without. Of this system and without knowing how to write, maybe a stove section. Does it answer your question? Yeah, we can discuss it in detail later. I'll be glad to. One more quick question and quick answer. So we don't have the collateral variable, but we do have the small system only with collateral only with two C Lis. So the how lately can you recover the big high-dimensional system something? Whether I can recover the Whether I can recover the large-dimensional system from the collective variable, I don't want to. I just want to be able to forecast my collective variables because they are going to contain all the information about the slow time scales of the process, which I'm interested in. If I were to somehow lift it, I would have to add some additional tool to the whole construction. So this lifting from low-dimensional So this lifting from low dimensional to high dimensional is not included here, but I also don't want to do it. Was that faster? Thank you. Sorry, does that answer the question? Okay, now after Batman we have Robin. I'm sorry. Robin Kebnitz is speaking next.