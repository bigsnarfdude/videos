My conference have two parts. In the first part, I summary the use of physical informatic neural network for kinetic energy density functions. Is a work that we in Ecuador published last year. And the second part of my conference is the use of the new Kohlmog of Arnold network for kinetic energy density functions. In my last slide, I present I present some of our last projects. Of course, we are open here to any collaboration in this line. So, first, I need to mention our team. I work very close with three three three three co-workers. One is Javier Torres here in Ecuador. He's the head of the computational and theoretical chemistry lab in the Universidad San Francisco de Quito. The second is my old students, now Professor Students, now professor in Universidad de Rosario in Bogot√°, Colombia, Luis Ajas. And the third one is in my home university, my Lazun University, the Universidad de Los Santes in Venezuela, is Rafael Almeida. Probably you know this Ecuador, Colombia, and Venezuela. Ecuador, Colombia, and Venezuela share are very close. In the 19th century, for 10 years, we are the same country. Then we, well, they separate and are now each different country, but in some moments we are the same. some moment we are the same the same country ecuador colombia and venezuela we are in the north of south america so well i i start with this work that we published in 2023 years ago and in this work we use a physics informed neural network to To not only obtain accurate kinetic energy density, but most important to obtain the functional derivative of the kinetic energy function. So in fact, the physics informed part is to obtain the functional derivative. The functional derivative. So, okay, the philosophy behind the physics informat network can be summarized in this slide. In the naive way to see the The machine learning, all the information are in the data. What you need is a lot of data. You don't need to incorporate any kind of physics. Of course, this is not absolutely true, but with data, you can do a lot of things. A lot of things. But the reality is this situation in many fields, in particular in engineering and science, is not true. You don't have a lot of data, so you need some intermediate compromise with Compromise between the amount of data that you have and the physics information that you include in your network. So how you include information with the bias. We can include information usually in three ways by Include a bias in the data. So, you only include the data that you are interested in that. You can include a bias in the network, in the architecture of the network, or you can include a bias in the learning, in the In the learning, in the law, in the function that you need to optimize. This last kind of bias that we are interested in. So in our previous work, what we do, we calculate the positive Positive kinetic energy density. From this positive kinetic energy density in some database of atoms and molecules, we obtain the enhanced factor. The enhanced factor we approximate using the second order perturbation approximation. Perturbation approximation plus correction. And our goal is to obtain the correction using a network, a neural network. So the target of the network is the correction to the second order perturbation approximation. Approximation. And in fact, we assume that this correction is a function of the reduced gradient and the reduced Laplacian of the density. Also, in addition to the reduced Reduce gradients and reduce laplacian. We also incorporate a non-local correction as we show in the next slide. So we have two kinds of network. One is a semi-local neural network. The input are the input. The input of the network are the reduced gradients and the reduced Laplacian. This goes to a normalization to put this number between one and zero, and then goes to the network, to a classical neural network, which files. Network with five layers with this number of neurons. Finally, this collapsing single neuron, that we have the normalization, normalized correction to the enhanced factor. After a normalization back, we obtain the correction. With the correction. So, our network usually has five hinder layers with this number of neurons. The activation function in the semi-local neural network is RELU, which the physical informed network is silo, the sigmoid. MoIPS version of RELOP. We use as optimizer the ADAM algorithm. The loss is the mean average percentile error of the network. We have a second network, it's a non-local neural network. network this neural network have two independent neural networks one is like the semi-local one with the reduced gradient and the reduced Laplacian. The second one is independent of the first one and contain the non-local information. Contain the non-local information. For atoms is the atomic number of the atoms and the distance of the point of the grid to the atoms. But for molecules, we use the Beller-Parinello functions that say what is the the the chemical behavior around along around one point of the space. At the end, we have a semi-local and non-local independent network and this collapse the last The last number of neurons is the same in both networks, and they collapse by pointwise multiplication in a single one single single one layer, and this goes to And this goes to a third network that collapses in the reduced in the correction to the enhanced factor. So in some way, the non-local parts work as a filter of the semi-local part. Semi-local part, and they collapse in the last network. A very important part of our work is the normalization of the gradient and the Laplacian of the density, and also the normalization of the correction to the enhanced factor. We use the hyper We use the hyperbolic tangent to normalize these three the output and the input of the network. What this remarkable from this normalization is that the three, both the gradients, the Laplacian and the correction have the shell structure of the atoms or the Of the atoms or the electron localization behavior of the molecules. So, for example, in the krypton atom, we can recover the shell structure both for the input of the network and the output of the network. So this So these are the comparisons of the performance of the semi-local and the non-local and some well-known functionals, kinetic energy functionals. We compare the mean absolute The main absolute percent error because it's almost constant with all the atoms. The database is for 18 atoms. For these 18 atoms, we have the hatrifock density and the hatrifock orbitals, and we calculate the Orbitals, and we calculate the kinetic energy and the Laplacian and the gradient and the Laplacian of the density. And we take the full QMNI molecular data base that have 113 K of molecules in a given. In a gilt of one, we calculate the quantities. Well, these are the error. The error for second-order approximation in atoms and molecules are 0.73% and 0.41 Or one. As you can see, our network work produces a very, very small error in comparison with the standard functionals for the enhancement, enhancement, enhancement factor. So after this, our This our work tries to obtain the kinetic energy potential using a physically informed network. We have a neural network as like we show before a physical informant network that produces the derivative of the derivative. The derivative of the correction of the enhancement factor through the reduced glient and the reduced Laplacian. With this derivative, we have a correction of the potential and with a feedback mechanism, we try to improve our Our enhancement factor that at the same time we obtain a good potential. These are the potential. The black line is the true potential for the krypton atoms. The blue line is the second order. Is the second-order approximation to the potential. You can see that it is very bad near of the nuclei. With the semi-local and non-local physical informat network, we can improve our potential. With this potential, we can do orbital-free DFT and estimate the density. And estimate the density. And these are the comparisons of the radial density of NEOM and argon for the semi-local and non-local neural physically format neural network. In the left part, Left part, which of the error. So, in this slide, we make a comparison of two PV enhancement factor and the semi-local, non-local neural network and the physical informatic neural network. And these are the And these are the errors for the 18 atoms that we consider in milli-har3. So you can see that the error for the first and second row are of the physical informant network are less than is only a few millihits. are only few millihartring well for for heavier heavier atoms are much larger more much larger here perhaps at least two order of magnitude lower than than class than standard enhancement factor so okay this these are our first contributions Our first contribution. I will go very fast with the Kolmodocor of Arnold network. The main idea in the Kolmodocor of Arnold network is replace the edge by functions, by to enable To enable functions. These functions are obtained in the original work, and we use the same philosophy using basic splain approximation. The idea is that in contrast with a standard multi-layer perception. Multi-layer perception in each node can be replaced by unknown functions that this unknown function are obtaining via a spline fitting. In one of our implementations, we use Implementation: we use spline. In the second implementation, we don't use spline, we use Gaussian Rayot functions. So, one important thing of Colmogor of Arnold network is that it's the regularization. Usually, regularization is to avoid overfitting. Overfitting. In this case, the objective of regularization is simplification of the network. The regularization is using the L1 norm and additional entropy regularization. We can prune the results. Results and at the end of the day, we can obtain a symbolic form of the enhancement factor. Kolmogorov Arnold network are better in, are similar in accuracy, are more interpretable because it's smaller, but actual implication. Actual implementation of this network are not as good as the standard and neural network. Well, in one pass, we use PyCam, which is the official implementation of the Colmograph of Arnold Network, the original. We use two CAM, two very small CAM, with two nodes in the input, the The gradient and the Laplacian, a node in the hinden layers, and one note in the output, and a little bit later. The spline is with 10 points, 10 knots, 20, 15, 120. This is the optimizer with a residual function. Function, and we obtain even with a very, very small Kolmogorov Arnold network, almost the same error than in the very big, very large standard neural network. Usually, when we do a Kolmogorov Do a Kolmogorov network, we obtain something that this is the grain, this is the Laplacian, five notes in the Hinda layer, the estimation of the enhanced factor, and the network say what are the important nodes, and we can simplificate in a big manner this. This tree. We also have our own implementation of CAN using TensorFlow with this small network with this grid. The optimizer was for 2000 epos. Particle swam optimization and then 200 epochs of ALAM optimization. No use residual activation function and can you obtain almost the same error than the original PyCam implementation? PyCam implementation. In conclusion, I think that my timing is over, right? No. Polmogorov, Arnold Metwork achieved almost the same. Almost the same error than semi-local, also non-local network with far, far, far fewer nodes. The Kolmogorov network only needs three or four or two hinder layers. One or two hinder layers. Of course, the number of parameters is at least an order of magnitude lower than a standard neural network. But the main complaint of CAM, of Kolmogorov Arnold Network, is that the Is that even if the error, if the network is very small, the training is much, much, much slower. So you need to employ many times in the optimization of these plans or radial basis functions or whatever. Or, whatever you use to obtain the functions in the next work. So, actual implementation are very slow. What we doing at this moment is almost complete also. We use the information of the Kornogorov Arnold Network to. To in symbolic regression to obtain an analytical form of the enhanced vector. Our next project is the use of Mogorov-Arnol network to obtain the electron density directly from the From the geometry of the molecules. So I stop now. Thank you again.