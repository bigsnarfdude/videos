Thank you for the introduction, and thank you, Bruce, Jia Ming, and Yi Hong for inviting me. It was really a pleasure to come here. Before I start, I want to say I'm relatively new to this problem. I started this when I was looking for new problem after joining UPC. And it was really great to see many of the major players here for this graph. This graph alignment problem. And so, in the talk, if there is anything I get wrong, please help me fix it. So, in this talk, we're going to introduce a model called attributed graph alignment or graph matching. So, this model will help us understand how some of the additional side information can help in terms of graph alignment. Of graph alignment. And moreover, this model also tries to unify several different models of different versions of the graph alignment problem in a unified manner. And we will see how this can help us understand some of the existing graph alignment problems. So this is joint work with my PhD student, Zhao Wang, at UBC, my former student, Ning Zhao, and my colleague Wei Na. And my colleague Weina is sitting here. Okay. So, as you all know, this is motivated by trying to find vertex correspondence when you have two correlated graphs, for example, coming from social network. Usually, one graph is anonymized, but you're asking whether just randomizing or anonymizing the user identity in one graph is. Identity in one graph is safe enough because potentially you could have another graph coming from a social network for the same set of users. And these two graphs are either the same or highly correlated. And you might be able to de-anonymize the user identity from finding the vertex correspondence to the other known graph with user identity publicly available. Okay, so there are this. Okay, so this kind of problem arises naturally in many applications. So I'm not going to go into the details of it, but let me just start the problem right away. So one of the simplest version of this is the two graphs are identical to each other, right? So you have the first graph coming from the early Freine model, and the second graph is just isomorphic to the first one. So this is what Julia mentioned yesterday. Julia mentioned yesterday. And our goal is to recover the vertex correspondence with high probability. So, here, please help me if I'm wrong. So, if all I want is just one algorithm, I can get to the connectivity threshold, right? So, and if what I want is a polynomial time algorithm, do I still get it? So, okay, so the open problem yesterday was about having a local algorithm. Algorithm and okay, great. All right, so I was not fully sure when I was making this. Okay, so great. So, in this simplest version, when there is no noise, the threshold is exactly at the connectivity threshold. When your MP is larger than log M plus anything that is going to infinity, there exists an algorithm that recovers this with high probability. And if you are only probability. And if you are on the opposite side, then no algorithm can recover this. And this is for P is less than half. And for P larger than half, it's a mirror image of, so it's symmetric with respect to half. And in the denser case, if your P is larger than N minus log N, then you have no L within. Okay. All right. But now when you put noise between the two graphs, Between the two graphs G and H, things become much more interesting and much more open. So in the literature, there has been a lot of studies starting from this so-called correlated Erdős Reigny model. So each graph, the two graphs are still Erdős Reigny, but they are not isomorphic, they are highly correlated. And so that's one version, the most classical version of this problem. The most classical version of this problem. And then there is this seeded alignment where you start from the Erdős Reigny graph pair and the correlated Erdogani graph pair, but then you assume some of the vertices are already pre-aligned as seeds. And in that case, what can you say in terms of fundamental limit and polynomial time algorithms? And moreover, And moreover, I also want to mention another variant of this, what I call bipartite alignment. So here, the two graphs are both bipartite graphs. And for bipartite graph, you have two set of vertices. We are imagining one set as already, one set of vertices are already pre-aligned, and then the other sets are our users that we want to find the vertex correspondence. Okay. Okay, and so this is a model for this. You can think of this as the application, I mean, the model of the when people de-anonymize this famous Netflix challenge data set. So there is correspondence between user and movies they have watched. And initially, the user was anonymized and think that was safe. And later, there is an attempt to. There is an attempt to de-anonymize that based on some other correlated bipartite graph. Okay, so, and of course, there are many more. Right. So, can I make a comment about this bipartograph alignment? So, if I understand correctly, you're thinking about two bipartisan graphs, but then say the right nodes already match. Yes, yes. Exactly. Yes, yes. I think another way to think about this problem is that. Way to think about this problem is that you just have one bipartite graph. You want to match the left to the left nodes, the right nodes. Each of the nodes will have this attribute vector. For example, like you could have users, like in this movie, then their user attribute vector is their ratings across different movies. Right, right, yes. I think these two are equivalent if I understand correctly. These two are equivalent. Two are equivalent. So, in the single graph, you want to find a perfect matching between the left and right? Yeah, so I only have one biparticle. I want to match the left nodes to the right nodes. But in each node, you will have this attribute of vector. Right. So, so your attribute is your right node, no? No, I have attributed back to for every node. Oh, for every node. Okay. Okay. Oh, yes, yes, that is indeed equivalent. Yeah. is indeed equivalent yeah yeah yes so then i think if this is equivalent then there are actually many more work uh this uh particular problem like uh this uh this so this is what is called database alignment yeah database alignment feature matching and i think yeah like you and i will also have some work and then check on yeah we'll work on that basically you compute a similarity score based on their opinions about movies right and then just give you a five-file graph index by you I see, I see. Yeah, in fact, I mean, this work was formulated in the way you described. And maybe, yes, I didn't do exhaustive literature. Yes, yes, indeed. Thank you. Okay. So, what we ask is: what if just by looking at the graph structure, it is not enough to align the verses? So, for example, in this case, so this Alex and David are in equal position, and in this graph, one, three, four are all in equal position. So, because of those automorphism, there is you cannot figure out a unique way to do this that that happened to be the That happened to be the correct one. Okay. So we argue that sometimes we do know a bit more about the users, for example, their affiliations, their educational background, and these are less sensitive information that are more publicly available. So, for example, Alex works at Google, Bob works with Linking, and David works at both Facebook and Linking. And together with those additional And together with those additional attribute information for the users, now you are able to match Alex with one, Charlie with three, David with four, and Bob with two. So we are going to model the attributes as vertices and then use an edge between the user and attribute if this user has this attribute. And this creates such a two-part graph. Such a two-part graph. And we want to understand how much benefit can this vertex attribute bring to the graph alignment problem. Okay, so more formally, we introduce this attributed early training pair model, and it has three pairs of parameters. So the first pair is N and M. So you have N users and M attributes. And the second pair is P and Q. P is the probability. Q, P is the probability for having an edge between user-user versus, and Q is the edge probability for having user-attribute versus. So this is how I generate the base graph. And after that, I'm going to do two sub-samplings. So in the sub-sampling of user user edge, there is the sub-sampling rate SU, and for the sub-sampling of the user attribute edge, Of the user attribute edge, subsampling rate is the SA. So that's the third pair of parameters. I do another independent subsampling. And after that, I will anonymize only the user part. So I apply a permutation on the user vertices. And that gives me the G2 prime graph. And G1 and G2 prime are what we observe. And from there, we want to come up with an estimator that figure out the vertex. That figure out the vertex correspondence for the uniform underlying permutation. And we want this probability of success to go to one. And achievability, we're trying to find the set of parameters for which this exact alignment is achievable with high probability. And we'll consider both information theoretic limit and efficient algorithms. And converse is the opposite of that, the set of parameters for which no algorithms are. For which no algorithms achieve exact alignment with high probability. Okay, so that's the setup. Yeah, can I also make a comment? Yeah, so I think going back to my earlier comment, another way to think about this is that each node, like these crossing edges, can be each node is associated with a binary attribute of people. Like one, zero. Right, right. Right, right. Yeah, indeed. So you could. Yeah, indeed. So you could just write whether it's connected or not to that. That's your M vector, right? Yeah, so that's the actually the vector is a binary. And at least binary vectors are correlated in G1, G25. So this self-samping. Right, right. Yeah, I just want to say, and then that's where, like, the other work that we are doing, like, you can consider not the binary, like attributable vector, you can consider, for example, Gaussian. And then you have it twice. So I think that's the. So, I think that's the study by team and who wants that all the one. So, for Gaussian, is it equivalent as adding weight to the edges? No? Yeah, you can think about it's the weight. It's just fully connected. Right, fully connected, and every edge has a weight, a Gaussian weight. Yeah, Gaussian weights, and then the other graph just added some additive Gaussian noise to make up two correlated Gaussian attribute vectors. Right, okay. Sure. I just wanted to give a sense in your in the setup you're gonna study, what is the size of M? Should I think about it as comparable to M or something much smaller? And is Q quite sparse or is it also comparable to Q? Yeah, so what's the relation about, yeah, that's what we will see next. So we consider all possible M and N regimes, but what I'm gonna show you is a simplified virtual A simplified version so that we can view it in a two-dimensional plot. So, we are only considering for the illustration purpose, we are considering this regime where P and Q are small of one and the subsampling rate are constant. But we have more general resulting paper. Okay, so oh, is it is it even visible? So, there is a green area here. Maybe, okay. Yeah, let me see. Let me see. Okay, it's not visible, but there is a green area here, which is the visible region that is not visible. Right, sorry. So, this contract is not okay. Anyway, so okay, so in this simplified setting, we try to plot our durability and converse region in a two-dimensional plane. And converse region in a two-dimensional plot. So, this one, this axis, we're plotting npsu square. So, what is this quantity? So, p times su square, that is the probability of having an edge in the intersection graph. And we think this quantity is an analog for the signal-to-noise ratio for the user-user part. And similarly, this MQSA square, we think this is perhaps a good quantity of. Uh, perhaps a good quantity to look at for the signal-to-noise ratio for the user-to-attribute part. And then, um, as you can see, the converse, the relation between them is roughly the sum of them is roughly linear at up to log n or smaller. So in this part, it is infeasible. And unfortunately, we couldn't prove exactly matching achievability result. Here, there is a small gap. Here there is a small gap, and this part is the feasible region. So that roughly tells how large M and N can be. So for example, M can be similar order to N, then you are hitting something maybe around here. And then if M is very, for example, when M is zero, then this goes back to the early Freini alignment problem. So you're just looking at this axis. And then when And then, when you could also look at this axis, that would correspond to the bipartite line. Okay, great. So, that is in the picture how our durability and converse look like. And going back to, oh, there is a small comment when M is reasonably large, then this gap. Reasonably large, then this gap actually doesn't exist. So it's actually, we're actually exaggerating this gap to just say this is not actually tight, but for large enough attribute versus you actually have tight attribute and converse in the simplified regime. All right. And how does this picture tell us the benefit of attributes? attributes so as i said if you specialize to m equals zero then that corresponds to so this line corresponds to a dishraney alignment and uh there we know exactly what is feasible and not feasible so the benefit from attribute now it's more visible is this part uh blue part okay so any questions on this result yeah one comment i think is uh i think it's Things that I think is it should be possible to close the gap like this. Yeah, I actually wanted to like post it as open, but yeah, maybe we can actually work out like we still work out some calculation. We never really somewhere, but yeah. So I think oh that would be great too. Yeah, so we look at this one access, I guess, and then it's it's not a hard to get like the shop special. Oh, that's great. So, yeah, yeah, that's awesome. So, we actually also tried and couldn't really get there. So, I will briefly get to that a bit later. So, which area? Is it supposed to belong to one of the two or is it supposed to be divided somewhere? No, I think okay, that I know probably this was a y-axis. That I know for sure. Like, you can get a Like you can get a sharp threshold. So I don't remember exactly the threshold, but I think probably it's still like that was a lot of the one. So you can push the like a 2G button towards the in. Right, I would also bet on this one. I will have some more discussion on that later when we get to the achievability for bipartite. Okay, so and now let me get to the And now let me get to some brief ideas about achievability. So, first of all, for Erich Reini alignment, yesterday Mickey already gave us a very nice descriptions of the major techniques there. So, the map estimator there is just also yesterday we stated in terms of maximizing the overlap between edges. It's equivalent as stating the minimizing the edge misalignment between the two graphs. Alignment between the two graphs, the G1 and the permuted G2 graph. And now for the attributed ordering model, any guess on how the map estimator should look like? So you could also do minimizing edge misalignment, but the edge probabilities are different now. So what should we do to fix this? What should we do to fix this? Exactly. Likelihood ratio. I guess you is there a way to phrase misalignment in terms of likelihood ratio? But the weighted part is correct. So it turns out the optimal estimator here is the weighted minimum distance, where this part is your misalignment from user-user edges. Alignment from user-user edges. And then this part is your misalignment from user attribute edges. And we figure out this optimal weight between them. And these are all, so this P11, P00, these are all parameters coming from the graph parameter S and P. All right, so yeah, this looks like likelihood ratio. So yeah, so just to Yeah, so just to give an example, P11 is our P times S U square. So these are all computable using the S and U and Q and S A. All right. And the error bounding technique is really just the standard technique where yesterday we have a very good overview from Mickey's proof. We're also using the generating functions and exploring the orbit decompositions and so on. So I'm Compositions and so on. So, I'm not going to the details here, and that is also not the major new part of this proof. Now, getting into the converse, now here there is a small new idea. So, first of all, the map estimator, the probability of success for map estimator can be upper bounded by one over the number of automorphisms in the intersection graph. In the intersection graph. So, this is a small exercise you can do that in case there is a non-trivial automorphism in the intersection graph, like here you have one and four, and their connection to any other vertices are either simultaneously connected or simultaneously not connected. So, this one and four will give you a, if you flip one and four, that gives you a non-trivial automorphism in the intersection graph. Automorphism in the intersection graph. So you can convince yourself once for any permutation in this graph, it only can give you equal weighted minimum distance or even smaller weighted difference between G1 and G2. Okay, so that's how we have this. And now it boils down to bounding, making sure there's no non-trivial automorphism in the intersection. Automorphism in the intersection graph, right? So, in the standard technique for the early training model, all we do is just looking for automorphism induced by isolated verses. But it turns out, just using exactly the same argument, it doesn't directly work in our model. So, instead, we look at a larger set of automorphisms induced by so-called indistinguishable verses. Okay, so when to When two user edges i and j are indistinguishable if their connection to any other verses is always the same. So for example, this one and four are indistinguishable pair. So their connection to any other vertices are always either both connected or both not connected. By analyzing the number of indistinctional pairs, we are able to establish the converse result. The commerce result. Okay, any questions here? Yeah, I just want to comment with this swapping idea, like a sorry, like IMJ and then pi, pij. So swipe. Exactly the same thing. I think you are talking about your commerce proof, which we actually look into and We actually look into, and it seems the relation is they are they have overlap. There are cases where your technique is strictly better than ours. There are also cases where just by looking at this part, our technique can be, is not covered by your technique. We will look into that. So, very good question. So, the MLE objective is a sum of the quadratic assignment plus linear assignment. Of the quadratic assignment plus linear assignment, like the seminar? Yes, yes, weighted sum. Okay, so we have the durability and converse for this new model. And you might say, okay, so what? What do we learn about the how does it help us learn about existing models? So it turns out there is a very nice connection to three existing models. Three existing models by just specializing our model. Okay, so first of all, of course, when there is no attribute or the edge between user and attribute is non-existent, then this reduces to the standard early training graph alignment. And second, if you set p equals q and the subsampling rate to be equal, and then you can view the attributes. Then you can view the attributes as this pre-aligned seed. And this goes back to the seeded graph alignment. So you might, there's a small detail here. You might argue, oh, in the seeded alignment, there are also edges between the attributes. And here you don't. Does it help or does it not help? How are these equivalent? And it turns out we managed to show that since those verses are already pre-aligned when you are looking for permutations, those are. For permutations, those edges don't help. So just looking at this user-user edge and user attribute edge are sufficient statistics. Right. So if assuming what I say, because there is no edges in the edge. Right, so that is exactly what I was trying to explain. So if you look at the map estimator, when you permute the When you permute the verses and look for which one has minimum weighted difference, those edge doesn't contribute because the permutations are only on this side. So the map estimator for the two problems are equivalent. Yeah, I guess I'm a little confused about so CD graph alignment mean by CD graph alignment. Yeah, just you start from origin and then you make some of the verses as seeds. Is as seeds like they are pre-matched correctly, right? They are pre-matched correctly. Like here, our attributes are pre-matched. So that's right, but I guess I'm a little confused about the edges between them? No, not really. I guess what are the C when you say they reduce the C. So those are the C. Yeah, this attributes are the C's. But then in the original C D clock alignment, then the C are the like. Alignment that just see the other, like these black dots, right? So I was sum of all together. Yeah, so I'm going to treat this whole thing as though. So that's normal. Yeah, so yeah, you have m plus n total versus regarding this bipartite alignment. So that is when you specialize PS to be zero, so there is no edge between user and user. Is no edge between user and user versus, and there is only edge across the two parts. All right, so now we can get some results for free by specializing our results. So as I already said, when you look at this access, it goes back to the early training and we indeed recovers the best known durability and converse, of course, under our specialized assumption. So as an QR. As an QOR small of one submarine rate are constant. And this is actually the interesting part. So when we specialize to the seeded graph alignment problem, so that corresponds to looking at this line with slope m over n. Okay. And what can we say in this case? So let me mention that in order to But in order to make a comparison, so for fair comparison, we're going to assume there are M unmatched verses, and sorry, N unmatched verses, there are M seats. So the total number of verses is M plus N. And the reason for such a choice is we want to keep the dimension of the problem to be the same. So there are both M factorial permutations to start with. And what is the best known information theoretic? Information theoretic limits in this case. So we try to search the literature and seems the best achievability we can find is just unseated achievability. So there are many polynomial time algorithms for seeded algorithms, but none of them can do better than the unseeded achievability. So we just put this one here. And then for converse, there is a very sharp converse by Jamin and I come for this reason. For this region, when your seed is at most the same order as n. And for m larger than this order, we didn't find anything. So this looks like matching, but it's actually not when m can go beyond this regime. So by specialize our result, what we get is this. So in fact, the problem The problem dimension is n factorial. So actually, the threshold actually goes at log A instead of log M plus N. And this is a strict improvement to this. And converse, in this regime, Jia Ming's result is actually tighter, but we can freely extend it to a regime where M is going larger than N. And this tool gives. And this tool gives tides. Yep. Yeah, so the reason why it's a log n versus log n plus n is just because here we have n unmatched vertices, so that's the only thing we need to match. So that's why you pay log n versus log n plus n. Right, right, yeah. Yeah, this is, I mean, not it's it's a more like putting things in the right perspective. It's not like really a huge technique improvement. Unique improvements. Okay, so that's about the seeded graph alignment. And finally, coming back to this bipartite graph alignment, just by looking at this figure, there is a gap here. And let me walk you through what is known in the literature, at least, I mean, to the best of my knowledge. There are a lot of other work I was not aware of. So we analyze this database alignment. So we analyzed this database alignment paper and their achievability and converse result were initially written in to me a little more complicated form. So let me walk you through. So here they write the bound in terms of the so-called cycle mutual information, where the cycle mutual information is defined here. So you have a matrix A, and then this is log of the trace of some other. The trace of some other matrix where this Z matrix is just entrywise square root of the A matrix, and Z has the same dimension of A. And you take ZZ transpose square and take trace, take minus log, that gives you the cycle mutual information. And for our problem, you need to look at the tensor product M's, tensor product of this probability Q matrix. And that's how they write. And that's how they write their Geovitian converse, which initially for me was a bit hard to parse. How does it compare to us? So we look into the analysis and their derivation and refined this result. So we're able to, so these are equivalent. We're not making any relaxation. So that circle, two mutual information, it can be written in this form. Written in this form. So here you have minus m over 2 log of 1 minus 2 times this phi a quantity, which is written in this form. So when phi a is very, very small, this log 1 minus 2 phi can be approximated by 2 phi, and this 2 and 2 cancel. So it's roughly just m times phi a when phi a is very small. So that's a little So that's a little closer to what we saw before. So before we have m times q s a square here they have roughly m times phi a. So yeah, so that's their attributing converse. So here they are converse. They cannot prove the minus omega 1 is a constant times log n. Okay. So how does our results specialize in this case? So after a small So after some more manipulation, we could also recover this, the same achievability, whereas for converse, we can only get to MQ as A square. So this goes back to the discussion of can we close the gap, which do you believe is the correct threshold? If you want me to bet, I would bet this quantity. I felt like this is not tight, although I cannot show, so maybe it's. Title, though I cannot show, so maybe Xiaomi has approved for this, or yeah, we can maybe discuss. I guess I'm a little bit confused about because you start discussing how it's already already have the covers, but you're saying that you want to get a log n minus omega one instead of one minus five. Yes, yes, yeah. So, yeah, of course, I mean, this depends on what you mean by tightness. So if right, so if you're happy with constant times log n. With constant times log n gap, then I guess this is tight. If you want to get plus minus omega one, then there is a difference between these two. So, okay. So, yeah, maybe we can discuss more about this. So, so so this go, this is the last slide for the information theoretic part. Any questions here? All right, so now let's move on to efficient algorithms. We're going to propose two efficient algorithms. So they are divided by whether the attribute information, this MQSA square, this quantity is large enough or it is very, very small. So in this case, we call attribute rich for rich attribute information, and this we call attrib sparse for sparse attribute information. Attribute sparse for sparse attribute information. And let me first tell you what we can achieve in this case. We can achieve this amount. So it is once again constant gap versus omega 1 gap. So if you're happy with constant gap, then this is almost tight. And then in the sparse regime, we can get to so this part we can really get to the threshold, although there is a small. Although there is a small part, we cannot. So, you really have to start with some attribute. You cannot start with zero attribute on this. But this quantity can be, once again, I'm exaggerating this quantity can be as small as a constant. So, this is really a vanishing gap here. So, let me go one by one. So, the first algorithm, it's a two-step algorithm. The first step we are going to The first step, we are going to use attribute neighbors, and we only exploit the edges between user and attributes. And this step, we are going to, you will see we will align most of the users. And we're going to call the successfully aligned users as anchors. And in the second step, we're going to forget about the attribute part. We're just focusing on the user. Just focusing on the user-user part, and we are aligning using the anchors. So, here it is really what you have in mind, the seeded problem. So, our anchors will be the seeds. And we're going to explore their one-hot neighborhoods. So, this is in Julia's language, a local algorithm. All right. So, more specifically, the first step, the way we align through attribute neighbors is for attribute neighbors is for every pair of users i and j, i in g1 and j in g2 prime, we are going to count the number of common attribute neighbors. Okay, and if the count is large enough, we say they are a match. So here is example. So here our threshold is, let's say, two, and you can see this vertex. If you can see this vertex 2 and 2, they both have attribute neighbor A and B. So there are two common attribute neighbors, which exceed the threshold. So we put two and two in the seed set, anchor set. And similarly for four, they both have C D as common neighbors. So we put four and one in the anchor set. And in case there is a conflict, we just declare. A conflict, we just declare an error. So, what we can show is okay, if you choose the threshold properly. Okay, so how do we choose the proper threshold? So this is the case when the attribute information is rich. So, this number is large. And for the correct pair, you can show that the expected number of common attributes is given by this number. And then if ij. This number. And then, if ij are the round pair, there will be an additional q factor here. And we're assuming q is small of one. So these two are really, I mean, separated. So we just need to choose an easy threshold. So choose something in the middle. And yeah, this is the conflict case. We declare failure if there is a conflict. So what we can show is in this case, the anchor set will only with high Will only with high probability only contain the correct paths. And moreover, this anchor set has size that is almost all the versus is n minus some n to the c where c is determined by the graph parameter. C is a console. Here what's important is just c is a constant between strictly smaller than one. So you will be able to align most of the users in this case. And And okay, now going into the second step, we are going to forget about the attribute versus we're just going to use the anchor set. So we're going to, for all of the unmatched pairs, ij, we're going to count their common anchor neighbor. So this is very similar to what we did in the previous step. And if it exceeds some threshold, we pair them. So examples here. So let's say the anchor we find. So let's say the anchor we find are 2, 2, and 4, 1 in the first step. And now for these two unmatched verses, so 1, 3 both have common anchors, and this has another common anchors, and this is for threshold 1. So we just pair them. Choice of threshold, there is a similar argument. So for the correct pair, it concentrates around this quantity. Although, I mean, this is a random number that comes. Random number that comes from the first step. So we'll discuss how to bound this. And that is actually why we try to analyze the size of it to give some bound for this. And in the round case, there is additional p factor here. So we choose also a threshold in the middle. And in the end, if it is not a bijection, we just declare failure. So there are some key observations in analyzing. There are some key observations in analyzing this. So, first of all, this two-step we are using disjoint edges. The first step, we are just using user attribute edges. The second step, we're only using user-user edges. So the two edges, set of edges are independent of each other. And this makes the analysis possible because if you both use some same edges in two steps, there is the dependency problem where a condition or what happens. Dependency problem where a condition of what happens in the first step, the distribution of the second step can be different. Here it's completely independent. So that's one nice thing. And there is another important, this is a small idea in the proofs, but it turns out to be crucial for our analysis to work. So normally when you analyze the step two error, you would just condition on step one. On step one, you only have correct pairs. That's what you would normally start with. But it turns out there, the analysis is just too loose. The union bound doesn't give us the right answer. And it turns out the key is to condition on another auxiliary event, which is this size of the anchor set. So once your anchor set contains Your anchor sets contain most of the verses, then in the union bound, you could just take union over the unmatched vertices, which has a much smaller size. So that is what makes our proof works in this case. All right. And once again, this is what it can achieve. But for this algorithm to work, we really need a very rich Really, you need a very rich set of attribute information for the concentration to work. And the natural question now is: what if you don't have that much of attribute information? So that goes to the second algorithm. So in this, this is once again a two-step algorithm, very similar to the first one, but we will choose a different threshold so that only a small fraction of users are aligned. Are aligned. And that is kind of a choice we have to make. We also wish to align more in the first step, but because the attribute information is very sparse in this case, that is what you have to sacrifice in the first step. So more precisely, once again, we're looking for the number of common attribute neighbors. But because the attribute informations are very, very limited, this case we have a very flat. case we have a very flat two um uh distributions or very two distributions very close to each other so if you choose some threshold in the middle that wouldn't give you good enough error probability so we have to choose something here to make sure the error probability is really small and as a sacrifice for this uh is we can only match a small version of a small fraction of users A small fraction of users in the first step. And here, instead of having concentration, we actually have the anti-concentration. You want to have a lower bound on how many you can match in this part. So as I said, in those sparse regime, what will happen is once again, you only have correct pairs, but you sacrifice here. So you can only match very small fraction of you. Very small fraction of users. So, now going to the second step, starting from a very small number of anchors, we further divide the alignment in the second step into two cases. In the dense case, when the user-user connection are dense enough, you just do the same thing as the previous one. You just explore one hop neighborhood. But in case your user-user connection is very sparse, Your user-user connection is very sparse. We called one of the interesting algorithms by Jaming, and there they explore the multi-hop neighborhoods in the seeded problem to make sure you can still obtain enough information to tell the difference between users. Okay, so yeah, that is how much we can achieve. So, I want to point Achieve. So I want to point out this point is perhaps, I mean, to me, the most interesting part. It gets really close to this. And here, what we want to say is as soon as you have a tiny bit of attribute information, how tiny is this? So this quantity can be, depending on the order of Q, if Q is one over n or any polynomial in one over n, this can be really a constant. So you really just need tiny bits of Tiny bits of attribute information, and then you get to the information theoretic limit for the Russian Reini alignment part. So, and my guess is this is also where we could improve some of the existing seeded alignment algorithms. Okay, so once again, we play this game. We specialize to p equals q, s equals, s u equals s a. equals S u equals S A. Here the region expressions are really complicated. So I don't want to confuse you. So I don't have an expression to compare. But we did a very refined comparison in all different regimes. It turns out, let me say this in the fair way. So there are regimes where existing algorithms can do strictly better than us. Do strictly better than us, the specialized algorithm. But there are also regimes where we can achieve exact alignment and existing algorithms couldn't. So we could in general strictly improve the union of the best known. All right. Can I ask like the last reference by Maul Lucison and Chipping Maul? Yeah, this is actually unseeded. Yeah, this is an unseeded problem. So then I don't understand what you mean by stripping people. Yeah, since they are unseeded. Yeah, since they are unseated, so that's including just right. So, first of all, any unseated problem is a valid seeded problem. So, what we do is just we analyze the region and put it in the union of the seeded polynomial time achievable region. Because any polynomial time algorithm for unseated is also a polynomial time for seeded, right? Yeah, on this axel. So in order to attach it, yeah. So that algorithm is like there's no attribute information, right? Just look at it. Right. Almost like that. So I don't understand. Oh, where the improvement comes from? Okay, good question. Okay, good question. So the improvement actually comes from when there are There are the seed is in the order of omega n, so where you will have significantly more seeds than unmatched vertices. And there, the information theoretic limit for the seated problem is what happened. So there the information theoretic limit for the seated problem is strictly larger than the Larger than the unseated one. So, this always wouldn't achieve anything beyond the information static limit for unseeded problem. But actually, for seeded, you have this is actually an easy observation when significant, like the significant number of them are already aligned, then of course you can do the remaining. Basically, in your case, you have this actual C information or attribute information, which is better comparing this. Comparing this model and then right, right, exactly. Yes, and there it's really just because we're putting things in the perspective of unseeded, where there like there is a huge gain from attribute information theoretically. Yeah, but I feel like it's not that fair. Sure, we can remove this one and not. Sh we can remove this one and not compare to them. Okay, all right. Is there a natural way that Margaret using here could use um the CD itself? Yeah, okay. Well, so that was the useful almost running colour. Sorry, some popular colours. Some CD itself, right? Yeah, actually. That's true. But I guess the whole point now is they're so they're really so they're not easily seen right right yeah yeah I mean yeah like I agree this is not a fair comparison their goal is to design for unceded problem um I'm just putting it in the perspective because it does enlarge the region for Enlarge the region for polynomial time visible region. So, can I clarify that a bit further? So, basically, you're saying that in this attribute, like the sparse region, like once you have a little bit of attribute information, then in your case, it's really easy, for example, to tolerate like the S is a constant and then Q is some like login. Right. Right, right. Well, in Maul and also even in our recent work. Even our research work is actually really difficult to do that because we don't have this attribute information. Yeah, in some sense, maybe. I recall like in mouse work, there is also another part. So I think their S has to be really close to one. And all the other S they don't consider in their, although maybe the close to one S are the hard part, but yeah. But yeah, so just looking at their regime, there is also the part when S is not close to one and we can work and they don't consider that region. Yeah, even in the in the regular like seed regime. Okay, I have one last technical slide, which is specializing to the bipartite graph alignment case. Here we Here, we couldn't really improve the best known existing achievability because I personally think the Hungarian algorithm is already optimal in terms of achievability, but we could slightly improve its complexity when m is in the order of small of n by just specializing our algorithm. All right, that's essentially all. So Essentially, all. So, to summarize, we propose that this attributed ordering graph pair model helps us understand the benefit of attributes and unifies many of the three of the existing models. And we have some information theoretically limited and algorithms, which in some cases improves some of the best known results for the existing model. And this wish list seems Jamie already has it. So, that's it. That's it. Thank you. Thank you, Lynn. This is likely the toughest audience you're ever going to see. Are there any more questions for me? I have a question coming. So you were thinking about just the one that's online for the left on the y-axis, see where you're going to look. Yeah, the one that's upset. Yeah, I was thinking about. One plus epsilon. Yeah, I was thinking about like one plus epsilon and one minus epsilon, but I think another little yeah, yeah, but I think that's probably still doable. Yeah, but maybe more kind of refine analysis. Yeah, it would be great if it is doable. I also feel really bothered this part is not closed. Good. Sure. I guess the flavor of the two algorithms you mentioned was sort of you take this these attributes and first do a step to sort of convert it to some kind of seated information and then operate with that information. Do you think that there are regimes with a problem where you kind of need to go back and forth? Or some regimes where this kind of one-directional approach might not be sufficient for us. Yeah, I think I mean intuition. Yeah, I think, I mean, intuitively, definitely exploring in the second step, you we're completely throwing away the attribute information. And if you also include those intuitively, it would help. And I think empirically, it will also help. It's just that now you are using edges repeatedly, and then it's hard to decouple the analyze the event. So then, condition on previous event. Condition on previous events, the distribution changes, and then you don't know how to just analyze it. But certainly, I think back and forth, or you just incorporate all of the information would be more beneficial. I have a related question. Have you considered this far stretching? So, where you know, those quantities are not log n but say content, and maybe there you might need to go back. Go back. So, you mean the in the seated problem or in the exactly your setting, but these the x and the y axis, they're not order log n, but they're order functions. They're large enough. Then it's uh so you would so you won't be able to get the exact um alignment, but you could oh, partial alignment. Oh, yeah, so so presumably side information still helps absolutely. I believe we are. Absolutely, I believe we're actually currently looking at the partial alignment. Yeah, I think I mean we don't have a concrete answer for that, but I would definitely, I mean, believe side information will be helpful in that case too. Right. So you want to really look at this this part, right? So when it is very, very small. Very, very small that is not organic concept, and oh, you want to look at here, yeah, yeah, yeah, whether it is. Oh, okay, you just want some weak recovery of the alignment, right? Yeah, yeah, right. In fact, I mean, we have some like very vague conjecture that the curve would look something like this. It would no longer be a linear curve, it would be something like that. Something like that. But we don't have concrete answers at this point. Okay. Okay, that's thank you, Negative. Thank you. I think we break for half an hour and go back to 11.