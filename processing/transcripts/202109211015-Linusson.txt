I'm very happy to join you virtually, even though I can't come to Canada. And I will speak about some random process on the while groups BN, CN, and DN, which are assigned permutations to oscomatoralists. And everything I say is joint work with Eric Os, Arvind Ayer, and Samuel Pott. Arvind Ayer and Samuel Potka. And at least two of them are listening, so they can feel free to add things if I miss something. Yes, so I will start with talking about a setting for regular permutations and tell a story of things that were done like a decade ago, and then to explain that process, and then I will go to what. That process, and then I will go to what we've done for the signed permutations. Actually, it goes for everyone. If you have any questions, just please ask them. I'd be happy to answer any questions that come up. So I'm not sure how it seems to be a mixture of probabilistic and comatorialists, but I'll go very slow with some probability here in the beginning. It might be too slow for a lot of you, but anyway. You, but anyway, so for the process on permutations, we think of a cyclic permutation. And the rule is that small numbers can jump left, that is, or clockwise. So here, for this example, you can choose number three, and number three can jump left because four is greater than three, so they can trade places. And so that's what. And so that's one step in this Markov chain. And the idea here is here that at each time step you choose one position uniformly at random. And if that particle can jump, it will jump. Otherwise, nothing happens. That's sort of the process you study. And yeah, so this is the, that's just what I just said. And this is often called a tricep, a totally asymmetric. Call a TISEP, a totally asymmetric, simple exclusion process. And there's a lot of this work has been done on these things, both in theoretical physics and people in probability and combinatorics. So some of the organizers of this conference, Angel, and was studying these. We have three different kinds of species, and a little bit later. A little bit later, Martin, together with Ferrari, found a fantastic tool called Multi Line Q's to study these and to look at these probabilities at stationarity. But I was going to first just do the smallest possible example, but it's remotely interesting to see that everyone is with me. So here I've written all the six permutations, even though you Permutations, even though you clearly can say that you have a you can quotient out the rotation, but I find it often easier not to do it. So for instance, if you have, you look at this state, if you choose the one, it can jump and trade place with a two and you end up here. If you choose the two, it can trade place with a three and you end up here. Or you choose the three and it cannot trade place with the one because the one is smaller, so then you stay. Smaller, so then you stay. And if you want to analyze the stationary probability of this, let us do that. Then you clearly see that you have two different possibilities by rotational symmetry. And if you look at the balance equation around one possible position, so for instance, around three, two, one, we get that there are the two ways of jumping out, and there's one way of jumping in. And there's one jump way of jumping in, and it comes from one, two, three. So just solving this, and we get that the probability is one-ninth and two-ninth, respectively, for these possible states. Yeah, so like this, these are the possible stationary probability. And these probabilities will play a crucial role in this story. So, yes, and then I, in the early when this, when I first saw this, was a paper by Lam and Williams, where they asked about these probabilities. And they've even added variables instead of having the jumping with uniform probability, you had an Xi if the particle I jumped, and maybe even a Yj if it jumped over J. J and found very interesting, intriguing examples. But if we stick to this, the uniform case, it was observed by different people that if you take the reverse experimentation, you got this very nice looking sequence of probabilities. And Ferrari and Martin, they proved that this was actually the product of rows of Of rows of Pascal's triangle. And later was proved by Eric Ors that if you took the identity instead here, you would get the previous nine we had with new 96. You get product of one row of Pascal triangle divided by product of this next row of Pascal's triangle. And as I said, this has been generalized. As I said, this has been generalized in many different ways. It turned out that these multi-line Q's, you can use them to study Schubert polynomials and many other things. And you will actually hear some other talks about these things in this conference. This is very dangerous to list all these names. I probably have forgotten someone totally obvious. So anyway, I'll continue to the next slide. So the way I got interested in this was a paper by Thomas Lam, where he Where he looked at random reduced walks. So this comes from comatorics, where you study affine while groups. So you have the, if this is A2, this means it's the group of permutations S3. So we have the dividing lines like this, that Xi equals XJ, that in this case, it chops up. That in this case, it chops up the space in six parts. So, this is a really projection to two-dimensional a three-dimensional picture. And then, if you do the affine version, you add another line, and suddenly, when you start reflecting in that line too, you get this infinite pattern of hyperplanes that subdivide the space into alcoves. And you can study. Space into alcoves, and you can study walks on these alcoves. And they correspond to infinite or has semi-infinite reduced words in the affine groups. So for instance, this if you start with S0, then you cross the hyperplane label zero, and then S1 and S2. And okay, I should explain these S's. So S1 and S2 are the standard transpositions in Coxider theory. So S1 means positions in Coxury theories. S1 means that you just means that whatever is in position one and two swap places. And S2 just means that whatever is in place, position two and position three swap places. And so on. And the S0, that's a special for the affine group then. The S0 is that what's in the first and the last position. What's in the first and the last position swap places? This is caused by the highest root in the root system, and this gives the affine arrangement. So if I want to, so and so the condition here is that the this word should always one must not add add something here like s1 again because then it would not Like S1 again, because then it would not be a reduced word anymore. So, this one must always be a reduced word in Coxeter theory, and this means this corresponds to that the path must never cross a hyperplane it has crossed before at some point. So, when we're standing at this position, for instance, we cannot go back here because we have already crossed this hyperplane previously. And we cannot go back here either. So, actually, in this case, the only possible way is forward. Case the only possible way is forward like this. And to see the connection with the marker chain I just drew previously, let us look at, so it's really if we continue to mark these with zeros, we can think of if at these states we can go like this. And this corresponds to all the downward arrows I had in the diagram previously. And then the, let me go back to that. The upward arrows, which corresponds to changing the last in the first element, they then actually means that you're leaving this little hexagon. So you go out of this. But so the plane is tile with these. The plainest tile with these small hexagons. And so, what Lamb proved was that, first of all, he proved that the probability of getting stuck in a chamber is actually equal to the stationary probability of that Markov chain we just saw. I think I have this. Yeah, so here, these are since you cannot cross a hyperplane that you already crossed, at some point pretty soon you will be stuck. That pretty soon you will be stuck in one of these six large chambers, and they're given by the fat lines. And the probability of getting stuck is given by the stationary probability of that Markov chain. That was sort of one of his main theorems in his very nice paper. And then it sort of follows by general considerations that this walk, this walk here, it will keep going and it will tend to. Keep going, and it will tend to a certain direction. And then he conjectured this direction should always be a certain vector, and that actually was the sum of all the positive roots. And this was the first thing that I studied together with Arend Ayr some years ago. He, Twaslam, he gave a formal. Tomas Lam gave a formula for this limit indirection. And without going into too much details in this formula, so the theorem is for a general while group, not just the symmetric group or the an so the point here is that we're summing over all permutations such that when you reflect in the highest root you get something larger. So this amounts to it, you have to compare what's the Amounts to you, you have to compare what's at the first and the last position. So that if you flip them, you also get something that's higher in the Coxeter order. So therefore, to prove this theorem, so maybe I should explain. Yeah. Yes, and this here, well, the check mark here is not very important. Here, well, the check mark here is not very important. It just means there's a co-root instead of the root, but that's not the important thing. But the point is that since we have to look at permutations where the first and the last position are such that when you flip them, you increase in cox order, what becomes important to solve this conjecture by Tomas Laham was to study correlations of two adjacent, since the rotational. Adjacent since the rotational symmetry I have here written as the probability that i and j are at the beginning of a permutation. But going back, it's really important what's in the first and last position. And when you compute these, you see several nice patterns here that we let me show up the proof. So up in this region here, for instance, we see that we have just one over Just 1 over 4 squared in this case. And that's a general fact that up in that triangle, we get 1 over n squared. There's some sort of strange independence because we have n possible positions. And here you also have a very nice pattern. They're increasing nicely, and we can prove that they follow this formula. And then you see this last. And then you see this last one diagonal was one over n squared plus some little bit extra. And this was exactly what was needed to prove this conjecture by Lam that Arvin I and I did some years ago. And indeed, it was the sum of all positive roots, as Lam had conjectured. So that was the So that was the 15-minute introduction of the story of unsigned permutations. And now I'll switch to looking at signed permutations, unless there's questions about this. So now we want to study the same problem for other while groups. So in comatorics, there's the standard way of doing this is starting sound practically to coxider groups. So we have permutations with signs and then. Permutations with signs, and then you should draw them like if I have a permutation of length, say five, and then you put some negative signs on some of the elements. So you have both a permutation and a sequence of plus and minuses of length n. And there's two ways of doing this, Bn and C, and they have a slightly different algebra to them. And then you have Dn, which is where the condition is that you have to have an even number of negative. Even number of negative designs or even number of negative, I should maybe add. So and to so we have so now we have to do the opposite. We have to first look at this reduced random walk in the F1 Weil group, and from there determine what should be the appropriate title. TISEP on sign permutation to study. And I will, as an example, I will use BN all through because, well, mostly because that's where we have the nicest results. Okay, so BN, we have first we have the same simple roots as we had in AN. That means these correspond to a reflection in the Reflection in the hyperplane when x1 equals x2 or x2 equals x3, etc. And on a permutation level, it means correspond to swapping the elements in position one and two and two and three. But here we also have E1 as a simple root, which corresponds to swapping the sign of the first element, very first element of the permutation. And then to make this then affine, we have Make this then affine, we also have to add the highest root, and the highest root is this en minus one plus en. So, this corresponds to flipping, changing place with the last two elements and changing their signs at the same time. I'll get back to these interpretations. Yes, and again, we get these hyperplanes that will chop up the space in alcoves, and we have we start with the fundamental alcove. Fundamental alcove here, and then we study walks out of this alcove. So let me have a picture here. We do the same thing again. Well, Lamb defined these things for a general while group. So we start and then we keep walking. We can, with the same rule as before, that you must. And with the same rule as before, that you must not cross a hyperplane that's already been crossed. And then again, we at some point we're going to be stuck in the chambers defined by these ups, the eight chambers. And so in this case, we get stuck in the chamber where the fundamental alcohol is. And we will go in some direction. Direction. And what we want to prove is what is this direction? And we can now be using, so yes, and I have here I said at random. And note when it was for A and it was uniformly at random. But Lamb suggested in his paper that for these other wild groups, it would be better to use Would be better to use some weights to increase the probability of crossing certain of these hyperplanes, weights that are called Kach labels. So don't ask me to explain why there is in some deeper level why this should be interesting, but these are the weights that it works. It becomes, you get a nice Markov chain for these with these labels. So AN was the case with Was the case we did was for preparing in general. Today, I will talk about BN. I will, in our paper, we also do C, check, and D. So there's the cache labels and then the dual cache labels. So there's two possibilities. For A and D, they're the same, but for B and C, they're different. And actually, so in principle, we could do B, check, and C with our with our. Our with our method two, we believe, but our paper was long enough already. But there's some interesting conjectures to study there. In particular, we think that these two have the same limit direction. Modulo, I think one of them is going down one dimension or something. So these two have the seem to have the same limit direction, but we never. We sort of never really go through and prove that in our paper, but it's more of a conjecture of ours. Okay, so what this table means is that if we look for the B and I will talk about is that in the bulk, you will have weight two for at the rate of swapping two places and for changing the sign of. And for changing the sign of the leftmost element, whereas changing the position of the last two elements or changing the position of the last two elements and changing their signs, that will have half the rate of doing that. So, yes, here's to summarize again the transition. So, I'm thinking by having this. having this x1 x2 up to xn minus 1 xn and this is what stands here is an element in bn it's a signed permutation and as before in the bulk what happens is that if we choose to do a move in the bulk the if the l and here l and m are supposed to be they could Are supposed to be they can be both positive or negative numbers. So if the L is smaller than the M, it can jump. They will trade places. The small numbers go left. And if the small numbers, if they're negative, which are the smallest numbers, if they get all the way to the left, what happens at the very first site, they can lose their negative sign and become positive again and sort of then start to travel back. And what happens at the last two sites? And what happens at the last two sites is one thing is this, which is the same thing that happened in the bulk: that ji becomes ij if i is smaller than j. There's four of these have that type, and the other four of the type where i is smaller than j and j i becomes i bar j bar. They get they swap places and get negative signs. Negative signs. So these are the rules for this B multi-ticep. And if we can prove enough about the certain correlations here, we will be able to limiting direction. Yes, so this is the corresponding smallest example for B2, where we have eight elements. And again, we have the red arrows are just when the leftmost thing, when it's The leftmost thing when it's just swap signs, and the blue elements are when they arrows when they swap places without swapping, changing signs. And the black arrows, they go back sort of in this order, in this partial order, where they change position of the last. Position of the last two elements and change to the signs. They both become negative for these two errors, this two error, for example. And so here's what we prove. So this is our theorem by, so I should have written that, I guess. This is OS IR. Ayer, Potka, and myself. We prove that the Limitan reaction is this particular vector. And it's interesting because again, it is the sum of all positive roots in this root system. And this is what's sort of more not a conjecture, more like a question by Lamb, that he's, it seems like we could get positive roots again. Uh, positive roots again is this the case, and so the answer is that it is true for this um type B, but it turns out not to be true actually for the other types, it's somewhat close, but it's not true in general. But for this type B, it is true. And so, how do we prove this now? Actually, maybe we're studying certain correlations, and just like the AN case, we looked at A and case, we looked at the correlation that with the last and the first element was, because that's the highest root would swap those. Here, the highest root swapped the last two positions and gave them new signs. So, here what's interesting is to study correlation of the two particles at the last two positions. So, here's an example of what it looks like for n equal four. looks like for n equals 4 and b 4 and so then we what we really want what we need to prove this limiting direction is certain sums of these correlations and we sort of barely are able to prove the sums but you immediately also see a bunch of patterns in this table and but we haven't been able to prove these patterns so this we have a bunch of conjectures here that i would love if someone would be interested I would love if someone would be interested in helping us prove. So, first, I should say that the probability it never matters if the very last position has which sign it has. So, I've chopped up half the table here. This is the same table just reflected, right? So I've ignored that. That we know. But maybe the most striking to me is this here. This is just This is just 1 over 2n squared. These are all conjectures. This reminds a lot about they had 1 over n squared in the type A. And if you look at this part also in bold face here, well, at least for the upper half, you can easily write this, let's see. you can easily write this, let's see, as I minus I minus J and the denominator is 2n choose 2 times 2n. And actually, what's on this diagonal, we also have a conjecture for that. This should be 1 over 2n squared plus. squared plus something extra namely 4n 2n choose 2 n squared minus j squared and we also have conjectures for these other that triangle and this this diagonal but there's they're slightly less nice but what's most striking to me is that we conjecture these correlations for the last two positions and they look so much like what we had in So much like what we had in n. So here we have 2n, here we have 2n squared in denominator. We have 2n times 2n choose 2. And here again, it's 4n times 2n choose 2. So if we go back to the where I had it, I had it here. Remember, so here was here I wrote it was 1 over n squared. Was one over n squared here up here. And here was n times n choose two and i minus j. And here on the diagonal, it was one over n squared plus something extra n squared and n times n minus one. It's very similar somehow, but in this situation, our methods do not suffice to prove the exact conjecture. And we don't really need it, and we don't need it either. We don't need it either. So, with what I described soon, we will determine the sum of a row, the column, the sum of a row, the sum of certain hooks like this. And this turns out to be enough. So we don't have to go in and prove this conjecture. But I find this very tantalizing, and I would very much like to see someone prove this for us. Let's see, I have another 50 minutes. Okay, so this is sort of basically the results in type B. Well, the most interesting things I find, this limit interaction and these correlations that we need for proving these limit directions. So let me say something about the proofs. So we have, so you saw these. So, you saw these Kach labels. That means we have a total of five different types of multi-ties that we want to study. We have the C check and the C, depending on we have a dual or not, and the same for the B. And then we have the D multitasep. And we lump all of this down to a new chain that we define that we call D star time sep. And we do it in two steps. The first step is what we call The first step is what we call a decay coloring. So, for instance, if we have n equals 4, then the possible numbers in the system is these eight numbers. So then a two coloring means that we take all these that are between two and minus two and we replace those with zeros. And we replace those that are larger than two with the ones and those that are smaller than minus two with. And those that are smaller than minus two with minus ones. And then we get down to somewhat easier system that only have three different species: minus, one, zero, and plus one. And then things become easier to handle. But of course, then we to get back to know the probability of, say, two at the last position, we have to compare two different K colorings of the B tricep. Rings of the B tricep and compute things higher up. And then this D star tricep we find seem to be the right definition here because it's also again a tricep on minus 101. And there's one extra object we call a star that can be occupying only the last and first positions. And we have Positions, and we have all rates are one except in the bulk, except at the end, the rates can be alpha and alpha star, depending on what they are. And then we set these alpha, alpha star, beta, and beta star. So alphas are in the beginning and betas are at the end. And they correspond to these five special cases we get for these things. So let me. So let me then just walk you through what these become with it. Was it the B ticep now? It becomes zeros, ones, and minus ones. We have in the bulk, we have small, the minus one goes left, the one goes always right, and the zero goes sometimes left and sometimes right, depending on me, this meets a minus one or a one. And when the minus one comes to the first, it One comes to the first, it can become a one, and on the right-hand side, we see that we get these six possible transitions happening. And just like before, I said, it doesn't matter if it's J or J bar at the end of the multi-ticep. Here, you can see that if it's a one or a minus one, it's the same thing afterwards. It just swap places. And here, a one or a minus one in both cases. One or a minus one in both cases, it becomes minus one, and here one becomes a minus one or one. So, what we do is we replace this minus one, one at the last position with a star, and that makes the tie-sep easier. And it follows that we have this lower transition probability at the last position. Yeah, so let me. Yeah, so let me then describe this D-star Ticep where we actually do our combinatorial analysis of. And we, this is, as I said, this is a Ticep with four possible objects. When it's subject to the following things, first, the number of zeros is fixed. That you could see also for the B that the number of zeros never change here. It's the one and minus ones that. It's the one and minus ones that can change. So, the number of steers is fixed, and the first and last position can only be zero or star, whereas in between those, there can be one, zero, and minus one. And here are the transitions for this d star. In the bulk, we have as before the same things. Same things that can happen: minus one going left, and the one is going right. Now you know we've changed to n minus one instead. That's because we've added, made the chain slightly longer. So if you look at the B ticep, it's correspond to adding a star in the beginning that will never change. Because in the B ticep, the specialization we gave was that the alpha star had to. Gave was that the alpha star had to be zero, so this would never happen. And that means that we stay with the star at the end. And the only thing that happens is that the minus one can become one in the second position. So then we think of this star as sort of a zeroth position. But some of the other, the C and the D, they have things happening with two involving two parameters on the left as well. So therefore, we have to include this object. And on the right, we have we define some of these. And on the right, we have we define some of these probabilities to be b and b star, whereas this one last transition is still always one. And then the way we attack this two-room model is fairly somewhat intricate and I will not describe that. I would just say that it's very much thanks to Duce and Scheffer model they had earlier this century. Earlier this century, we modify the two-row model they have to study such Ticeps. And let me just give two examples. If this is all the configuration, two-rule configuration where we have n columns and n zero zeros, then so here are the possible states when we have of length three with one zero. zero so this becomes the only possibilities you either have to have a zero or a star in the um at the left and the zeros always come in a column and the stars do as well and here's a case where we have no zeros and of length four so then we have to have stars in beginning and the end um and then now um the um The bijection we use, combinatorial bijection, is that if we have a one, we map to Motskin paths. So a one and a minus one, then it maps to a horizontal step. And this horizontal step are colored in two different ways. There's one color if the minus one on top, and another color if the minus one is at the bottom. And then this corresponds to an upstep, and this corresponds to a down step. And the rules for these configurations are just such. Rules for these configurations are just such that this corresponds to what's called Motzkin paths with two possible colorings of the horizontal steps and then with labels or weights that are given by the alphas and betas. The transitions become sort of tedious to describe, but it's a useful tool that we are able to use this. So with the risk of finishing a bit early, this is going to be my last slide. Going to be my last slide. I'm just, this slide is actually just repeating what I just said: that we have, if we define this notation, that this denotes the probability of the configurations ending in i and j. Then these two Kouro configurations without zeros will are in bijection with bicolored Mutskin paths, which in turn is in bijection with dick paths. So we just count these paths with certain weights, alphas and betas. Path with certain weights, alphas and betas. And this is enough for us to compute certain sums of these correlations j, i, probabilities at the end of the tie-sep. So which is just enough for us to actually compute the limit direction in B and tilde. Even though we cannot compute these i. Compute this ij explicitly, unfortunately. That was the challenge I wanted to give to you. Was this my last slide? It seems I have. Oh, I also, so I want to mention an example of another result for Cn. We do this compute. It's still again, it has a very nice formula, this limit, but this time it's not just not exactly the sum of positive roots. It's very close, seemingly close, but it's no cigar. And that was my last line. Thank you for your attention. Any questions? Swante, can I ask about a detail? Yes, please. When you were describing the rant. When you were describing the random walk on Bn, you were allowed to swap i to the left of j if i is less than j right and you were also had a move where you could change i j to minus j minus i so we're looking at these right huh um yes yeah okay you look in this in this you're thinking of this last column Yeah, this one here, JI goes to minus i minus j. Was there a condition on i and j for that one, or is that one available? Yeah, the conditions are here that the i is always smaller than j. But then on the next slide, you had a black arrow from a case which didn't look like that. Could you go to the next slide? So So, oh, well, maybe it's because you get to go around the corner. That's, I guess, that's the only problem, right? You had minus one, minus two, one, but I forgot you could go around the corner. Yeah, that's the problem. Here you don't go around the corner, but it's like it's so this one, two maps to minus two, minus one. This would correspond to let's see, this one. And then when 2, 1 goes to minus 1, minus 2, this would correspond to this one. Oh, I see. So, right. So, I is always less than J in these pictures, but you have these various different options. Yes. Gotcha. Okay, thank you. Yeah. So, this, let's see. So, these four are the ones where they swap signs, and the other four is when they keep the signs. four is when they keep their signs and these they correspond to the they they are the other when they keep their signs it's uh really the same as the bulk move that the smaller one goes to left and the larger go to the right okay very good fascinating stuff thank you peter you go back a few slides i think there's 12 or 13 um so third was it uh so uh before you uh so uh before you do the uh the last things you were saying about the a i'm sorry i can't really hear can you go back uh another couple of slides yes so here in this uh chart that you have for the i and j is that you have so do you have an can you extend this to more than two positions to to ijk to three positions yes Oh, to three positions, yes. Yes. Yes, so for we can, yeah, so yeah, so that's a good question. So, we for three positions we did in the same paper, we can get any formulas, and that's actually very much similar to the things you do in your paper with the TISEP speed process that you have. It's sort of very much sort of the similar calculations. But we also discovered that this. this um this one over n squared is actually true if you if you uh if sigma is um if here's i and then it's like two steps in between and then come to j if now the um if i is less than let's see how it should be than j minus three i think let's see I think, let's see. Yeah. Then it's again the probability is one over n squared of this happening. So this thing that they get one over n squared, if they're adjacent, it happens, but it's also if they're far apart, as long as they're further apart in value than they are in position, you get one over n squared. This was a conjecture of ours that was proven by Eric Oss later. This we have not checked in the BN case. This could be a pattern in the BN case. A pattern, the BNKs too. I don't remember that we ever checked that. Is there some soft reason to expect why those probabilities would even be constant in that range of I and J, let alone that particular constant value? I mean, that seems surprising. No, no, no, I have no intuitive reason why they should be constant or this. Constant or the same. I say it's independence, but it's like really, so it's, of course, the probability of the last position being a four, could you think of that being one over four, but then it's also being a one next to it. Well, because you think of that being one over four, but really you already use the four, so it's not really independent in that way you want, and it's sort of. Way you want, and it sort of and you sort of double up this ability of the one coming just before the two. That's that they have twice the probability, and that's the same thing for B and then. And I guess my other question sort of about these correlations, you said that your proof lets you compute certain sums of the correlations that I get, but not the actual correlations themselves. Actual correlations themselves. So, yes. So, how much more information would you need just to obtain the correlations from just sheer linear algebra? But I guess we have to complete it. I guess the thing is that we were able to prove it for every column, what the sum is, and for every row, and a certain number of hooks, like this, we can prove it. And that's enough for us. But it's a linear number of sum. Linear number of sum. And of course, you have the number of positions is growing by square of n, right? And we have a linear number of sums we were able to compute. I think. So I should basically regard most of the work as not yet done. Well, sort of. Yeah, regardless of correlations. You might need just sort of one more information bit in every position or somehow, but there's a lot of information lacking. Well, and we check, so the sums we can. And we check so the sums we can compute are consistent with our conjectures here. So it's a and it's true, we check it for five four, five, and maybe six. Interesting. Okay, thank you. Thank you very much, Manta. That's fascinating stuff. Thank you, James. If anyone has any more questions.