Today, I'm going to talk about one of my recent works on text analysis. And this is joint work with Professor Tony Kai at UPenn Watton and my former postdoc, Paxton Turner. The online customer reviews is a proxy to the so-called. Is a proxy to the so-called word of mouth. And in the marketing research, there is a lot of interest. There are a lot of interest studying the patterns in online customer reviews as they can use these findings to redesign the sales strategies. For example, in this paper, they claimed that younger travelers, women, and travelers with less review expertise tend to give more positive reviews to hotels. To ask statisticians, this is kind of Statisticians, this is kind of, this is an hypothesis to test. If you want, you may use the numerical ratings to test against this hypothesis. But for hedonic products, such as movies, books, and hotels, people find that the text reviews provide more information. And the question is: how to test this kind of hypothesis using the unstructured text data? Text data. Nowadays, when I present this problem, people always ask: Can ChatGPT do this? Can GPT-4 do this? So in my perspective, I think the success of these pre-trained large language model is a little bit overclaimed in the media coverage, as they never mentioned the huge amount of unpublic data set used to train these models, as well as the These models, as well as the huge computing power that a high-tech company owns. Then, as a researcher, typically we just have a small amount of text documents in our domain. And we want to build an interpretable model. And we also want the model to have reliable performance in the sense that if something goes wrong, we know how to diagnose this and how to fix the issue. How to fix the issue. For this purpose, the white box approach based on classical statistical modeling actually works very well, especially for unsupervised learning tasks such as hypothesis testing. Today, I'm going to use the simplest multinomial model for text. And in the first talk today, Professor Jing also used this multinomial modeling. I'm going to just slightly reiterate it. Reiterated. We think that for each document, there is an underlying distribution over the dictionary. If the dictionary has p different words, and then this is a discrete distribution, and it's represented by a p-dimensional probability mass function. And then, if a document has capital N words, we think that these words are drawn with replacement from the dictionary. And every time the probability. Time, the probabilities we use to draw this word is from this distribute PMF omega. Okay, so for this model, the sufficient statistics are the word counts, and the word counts follow a multinomial distribution, where capital N is the number of trials, and omega contains the event probabilities. So, we use this multinomial distribution to model each individual document. Now, let's come back to the Now let's come back to the problem I posted on the previous slides. So we want to know if, for example, travelers with younger and women travelers have more positive reviews on hotels. Let's say that's our problem. And then we think that for each document, there is this multinomial distribution, and omega i contains the population word frequencies for this document itself. This document itself. Okay, then we can divide all the documents or reviews into groups. For example, if we want to know if younger travelers give more positive reviews, we can divide the documents according to the age group of the reviewer. So these groups are known. Given this known partition of these documents, then we can define the Then we can define the within group mean. This is a weighted average of the omega i's of the documents in the case group. So here I give just a slightly higher weights for those longer documents. So this vector mu k is the average population word frequencies for documents in group k. Then we can use this mu k, we can view this mu k as representing View this new k as representing the average opinion of the reviews from the age group K, for example. And then the null hypothesis is that these average vectors are the same. And we want to test against the null hypothesis. And I want to remind you that this is not a standard K sample test. So I'm not assuming that the reviews for age group K are IID from the same. Are IID from the same multinomial distribution? Even when the null hypothesis holds, they can still have their individual omega i's. I'm not assuming that these are the same. I'm just assuming that if you look at the within group mean, the k different mean vectors are the same. So this is a weaker hypothesis than the standard k-semple, the weaker now hypothesis than the standard k-semple testing, and it is more realistic. So, in order to test against this now, we must recognize that this is a very challenging problem. So, first, even under the null hypothesis, it's still highly complicated. As I mentioned, even the null holds, those omega i's for different documents or reviews can still be different from each other. So, we still have a huge number of unknown parameters. And the second, this is a high-dimensional problem. High-dimensional problem. P is the vocabulary, dictionary size. Nbar is the average document length. For most real data sets, P is much larger than Nbar. So this means we will have a lot of zero counts in many documents. A lot of zero entries in word counts for many documents. And also the number of group K is not assumed as finite. It can be any number between 2 and N. In fact, 2 and N are. In fact, the two and other two cases that are of most interest, and you will see that later. And last thing is that we have very severe word frequency heterogeneities. So given these challenges, we want to design a test statistic so that enjoys both parameter-free limiting null and also optimal power. So, how do we do that? As the first step, I'm going to reformulate it as testing against the null that rho square. Against the null that row square is equal to zero. So, what is a row square? I will define mu as the overall average among all the documents. Okay, and then if the mu case are the same with each other, and then you can look at this sum of squares. Okay, so the null, when the null hypothesis holds, all the new k's are the same with each other, so we get zeros here, then row square will become zero. Then row square will become zero. So now hypothesis holds if and only if the row square is equal to zero. Then my approach is as follows. First, I want to define a good estimator of the row square. Then I want to use this estimator to get a test statistics. I particularly require that this test always has a standard normal distribution, even under the highly competent null hypothesis. This is my goal. Goal. Okay, and we can consider a naive estimator of row square. So, this mu case and mu are the average population word frequencies within group and also overall over all the documents. And we can estimate them using the population word frequencies. So, this is very easy. And then we just ungumble these estimators to get a naive estimator. To get a naive estimator of row square, and fortunately, this is a biased estimate, as you can see very easily. Okay, so how to do that? As a statistician, we know this standard routine. If we have a biased estimator, then we de-bias it. So, let's first compute the bias of this naive estimator. So, this calculation shows us that the bias takes this form. So, here, nk is the number of NK is the number of documents in the group K. Unbar K is the average document length in group K. So N is the total number of documents. Unbar is the overall document length. All these are known. The only thing unknown in this bios are these population frequencies, omega ij. I will never try to estimate them because we can consider the high-dimensional situations. It's just impossible to estimate these omega ij. To estimate these omega ijs accurately. However, we can still construct an unbiased estimator. We just want to get a data-driven quantity. We want to construct a data-driven quantity so that the expectation is just equals to this part in red color. Then I'm going to replace this part by this data-driven quantities here. Then I get the debaus term. So this is the idea in the The idea in the next slides. So, here I'm going to subtract this debiasing term. And this debiasing term follows the formulas in the previous slides. We'll just replace that part in red by this data quantity. And we know that it's obvious that this is an unbiased estimator of rho square. And this is exactly unbiased, not even asymptotically unbiased. In our analysis, In our analysis, we also found that this debiasing is quite interesting. It not only kills the bios, it also reduces the variance. The reason is that this debiasing term itself, bias correction term itself, has some noise. It turns out this noise term also cancels with some noise term in this T0. So that's why it helps reduce the variance. Now we have got an unbiased estimator of rho square, and we want to develop it into tests. And we want to develop it into test statistics, and we needed to estimate its variance so that we can standardize it. The formula of the variance estimator is on the next slide in the theorem statement. But before doing that, I want to show you some regularity conditions. So the first line shows the key conditions. As you can see, it's very mild. So for example, the L infinity norm of each omega i, I mean, on a null heterogeneous on a null heterogeneity degree frequency heterogeneity should it should be at the order of one over p. So here we just require it to be below a constant, which is very mild. Unbar K is the total number of words for documents in group K. We don't even need these to be at the same order across different groups. We just require that no group can be too dominating. Additionally, we have some regularity conditions. We have some regularity conditions. In the paper, we carefully justify that these are mild conditions. For example, if the p goes to infinity in the high-dimensional situation, these are all very easy to satisfy. Okay, and then this is a formula for the asymmetric variance of the estimator of row square. This just comes from first calculating the variance and then estimate. Variance, and then estimate each term in this variance expression using some data-driven quantity. So the details are in the paper. And then we show that if we use this to properly standardize the test, the estimate of row square, then we get this test statistics, and it always converges to a standard normal. And this is parameter-free. It means regardless of what those omega j's are under the null hypothesis, we always. are under the null hypothesis, we always have this asymptotic distribution. Another thing I want to mention you is that in this asymptotics, we only require the total number of words to go to infinity. We don't necessarily require the number of documents and the average document length to both go to infinity. So we just need their product to go to infinity. So it can be the case that we have a few very long documents, or it can be the case that we have many, many short documents. Be the case, and we have many, many short documents. Okay, and then these simulations justifies the asymptotic normality results where the blue histograms are from the null and it fits the standard normal density very well. Because of this parameter-free limiting null, you can use the normal quantiles to set to control the type 1 error relatively precisely. And we also have this. We also have this histogram under the alternative. As you can see, the two histograms are separated from each other, suggesting that this test does have power under the alternative hypothesis. And this theorem is very compact. It combines several theorems in the paper. So the main message here is that the test has minimax optimal power. So we have defined this signal-to-noise ratio. This signal-to-noise ratio in this way, and it's easier to look at a simpler form when the new case are all at the same order. Okay, and you can see it's related to the number of documents, average document lengths, the number of groups, and the vocabulary size. So what is omega n? Omega n is a rescaled version of row scale. Row scale itself is not scale-free, so the scaling changes with the dictionary size p. But this But this rescaled version is always bounded by a constant. So if this is in a constant, that's kind of the strong signal situation, but we also allow for weak signals where this omega n can go to zero in the asymptotics. And we show that if this goes to infinity, and then the test has an asymptotic full power. And we also have a matching lower bound to show that if this goes to zero, then it's just impossible to separate the two hypotheses. Impossible to separate the two hypotheses. So the test is minimax optimal. And then the reviewer asked us to compare with other testing ideas. One idea is the likelihood ratio test. We have to point out that the likelihood ratio test is not directly applicable because our now is highly compensated. It's not the standard K-sample testing. But we can consider the special case of the standard K-sample testing. So in this K-sample testing. So, in this special case, we can still compare the likelihood ratio, and you will see that we can still outperform the LR test. Another thing is that, what if, another thing that review asked is what if we don't care about the normality, we can just use bootstrapping or resampling to get the p-value, but we only care about the power. So, in this case, is it still necessary to debiasing? Yes, it is. As I mentioned, the debiasing always. Mentioned the debiastin also reduces the variances. And we have examples showing that without this debio-sting, you will get you will not get power. Okay, so these are the power curves. As you can see, in both cases, a power of our tests is actually either similar or better than the likelihood ratio test. The difference between these two settings is the vocabulary, is the dictionary. Is the vocabulary dictionary size? So, this is the low-dimensional situation where the LR test is already optimal, but our test also has similar power. But in the high-dimensional setting, the LR test performs actually pretty poorly. And I have to mention that this is just a special case where we have this requirement. So, LR is applicable, but for the more general settings in our paper, the LR is actually not applicable. Applicable. Okay, I hope that I have convinced you that this testing idea is useful. And then after we finish these results, we realize that's very interesting. There are several well-defined problems in the literature of text analysis and discrete distribution inference that can be viewed as special cases of our problems. Okay, so therefore, in those examples, In those existing studies, actually, there is no work showing that we can get the parameter-free limiting now for these problems. So it turns out that we can view them as the special cases of our problem, and then our results apply directly to all these problems. And we get tests for free that have the parameter-free limit now. And the flexibility comes from the fact that the user has the flexibility. Has the flexibility of determining how you group the documents. So, if you group the documents in different ways, then you basically get different kinds of information. I'm going to show you two examples. One is the global testing for topic model, and the other is the authorship attribution. Professor Jing already talked about the topic model earlier. In the first talks, I don't need to further talk about it. In the topic model, In the topic model, actually the omega i's have some special forms. It's a convex combination of a few topic vectors. So here I use r to represent the number of topics, not the k. And then the global testing problem means we want to test if there exists multiple topics or like all the documents are just about one topic. Only if this global test, global now is rejected in a Global now is rejected. It makes sense to do the topic modeling. So it turns out that this problem is a special case of our problem with capital K equals to N, not capital K equals to R, capital K equals to N. So let's see why. Under the global now, there is only one topic. So therefore, all the omega i's will be equal to that single topic vector. So in this case, the omega i's of all So, in this case, the omega i's of all the documents are actually the same. So, if we assume that each document itself is a group, then this null hypothesis is the same as our null hypothesis. So this is a special case of our problem with k equals to n. Then we can just apply our results directly. For example, if we let k equals to n, then we know that our test has a symptomically full power under. Symptotically full power under this regime. Okay. But we can generalize the upper bound, but we cannot directly generalize the minimax lower bound because the topic model has more special structures. The lower bound may change. So fortunately, we studied the lower bound separately for this problem and found that our test is still optimal. Now you can see that we have figured out this phase transition. Okay, so interestingly, in my previous work, I also Interestingly, in my previous work, I also studied at what a situation we can even not only detect the topics, but we can also estimate the topics. So as you can see, if we combine these results together, actually we know that for this topic modeling, the global testing is indeed significantly easier than the estimation problem. Okay, the second example is the authorship attribution. So it was one famous example. So, it was one famous example is in this Mostella and Wallace paper, where they considered a collection of federalist papers co-authored by two authors. But for some of the papers, actually, we don't know who wrote them. And the idea is to allocate the authorship. And then in this paper, the author showed that we can do this by just looking at the empirical word frequencies. Okay, so how to form. So, how to formulate it in our problem? Let's do this. So, let's suppose we have two groups of documents. The first group comes from this collection of documents for which we know the authors. And then the second group is the collection of documents from an unknown author. Like, for example, if m equals to one, that means we have. M equals to one, that means we have one federalist paper with unknown author. And then this one, for example, is like Hamilton. So for example, this is the collection of the Hamilton's federalist paper for which we know the authors. And we want to test if they have the same probability of population word frequencies. So if we get a p-value, We get a p-value for this test. And we can do this for Hamilton's collection, or we can replace it by the collection of the other author. Then, for each author, we get a p-value. We just compare these two p-values. So the larger p-value that we can select this author is more likely to be the author of the collection of the unknown author federalist papers. That's the idea. Okay. And in this, Donaho and And in this Donahoe and Kipney's paper, they actually think that this problem has the rare and weak signals, which means for the majority among all the words, only a small fraction of them actually contribute to differentiating the two authors. Then under this framework, they extended the famous Donnehall and Jeans higher criticism-based test. Then we found that this problem is actually a special case of problem with only two groups. And then we also study. Two groups. And then we also studied the power. We found that our test actually provides a kind of counterpart of the chi-square test for this authorship attribution problem. Okay, how many times do I have? I think I start like 10-15 minutes. I have some time. Okay, good. Next, I'm going to show you two applications, real applications. So the first is on the Embdo moving reviews, and the second is Moving reviews, and the second is on the MATSTAD data set, the project that Professor Jin led, and it produces several data sets. I feel very lucky to be part of this project. And today I'm going to only use this phase one of the data set, which consists only the papers from four journals. And you can download it from Professor Jean's website. Okay, so for the MNO movie review. So for the MMO movie review data, we have reviews, hundreds of reviews for each movie. Okay, then we do two experiments. In the first experiment, we just treat this as a corpus, and then we view each separate review as a group. So k is equals to n, then we apply del. Then we get a z-score or p-value. So this z-score or p-value measures the divergence. Measures the diversity of reviews for this movie itself. So, the larger this score, the more diversity of the reviews of this book. Then, in the second experiment, we use some other features of the reviews. For example, for the reviews, we also have the associated star ratings. Then we can group the documents into five groups based on the star rating. And then we will do a pairwise testing. So, we will. Pairwise testing. So we will apply k equals 2 for each pair of the groups. So these are these scores for the movies. As you can see, if you look at the total number of reviews, we selected these movies that have like more than 800 reviews. So all of them have a lot of reviews. But interestingly, some of them have really large discords. Like the one is the largest is the movie Probuses, which is a high. Probuses, which is a horror movie. And it means that the reviews of this movie are very diversified. People have a very split view of this movie. Okay, so in real data now, it's not a, it's very common to get a really large D scores or small p-value, but it's less common to get really small these scores and large p-values. And one example is this product by the comedy show called by. It's a comedy show called by Jeff Dunham. Although it has more than 800 reviews, the z-scores are surprisingly small, showing that people have very homogeneous review opinion on these shows. And also we can do the second experiment where we do tailwise comparison according to rating. So here I select the two movie reviews. I want to show if you look at the distribution of the five stars. They are more or less the same. They are more or less the same. But if you look at the pilewise z-score plots, you can see very different patterns. For this movie, Night of the Living Dead, the pattern is that the three to five stars reviews are more or less similar, and the one or two star reviews are more or less similar. But for this movie of Harry Porter and the Death Hallows part one, the situation is that the two to four star reviews are more or less similar. star reviews are more or less similar and the one star and the five star reviews are very different from are very different okay so maybe this is um yeah then there are a lot of studies like saying which are more like healthier patterns and how to you may use these patterns to detect the like fake reviews okay then the second example is on the metastatic data set and here we treat the app Here we treat the abstracts of each author as a corpus, and then we get a double z-score for each author using k equals to an. So these z-scores shows the diversity of these authors' abstracts. I have to make a disclaimer here that this diversity is only the semantic diversity. I'm not saying that higher these scores means this author has more diversified research interests. It may be true, but it also may be the case that this author has a more Be the case that this author has a more diversified styles of writing the abstracts. Okay, so this is just what the data tell us. And these are the representative authors. They have both a large number of papers. These are log of the papers and also higher diversity. Even if you compare the authors with similar number of the papers, some of them, like Jian Ting Fen actually have a very high z-score compared with others. And we can also do the pairwise z-score plot. Do the pairwise z-score plot. We can divide an author's papers according to publication years. Then we do the pairwise comparison. So this is the plot for the Peter Hall. You can see that, for example, 2009 and 2010, the abstract semantic similarity across the between the abstracts of Peter Hall's papers are quite large. Okay, very quite similar. And another noteworthy discovery is that the abstracts from two Is that the abstracts from 2004 seem to be very different from the other years? Then we looked at the papers for Peter Hall. So this is the data set with only four journals. So these are all the papers Peter Hall wrote in 2004. We found that in this year, actually, he had a very special interest in the so-called many of these papers are related to the bandwidth selection in the non-parametric settings. In the non-parametric settings. Okay, I want to wrap up that we consider a between-group variability testing problem by doing the multinomial modeling of text data. And we propose a test statistic as both parameter-free limiting now and optimal detection boundary. And we showed a couple of applications of this work. I think the main message I want to show you is that the classical statistical modeling of text actually is very powerful. Of text actually is very powerful for a lot of tasks. So, I do think that the value of the statistical modeling text is quite undervalued, especially given the median coverage of large language models. And the results can be found here. Thank you.