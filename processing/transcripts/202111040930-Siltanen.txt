Today, I wanted to share with you quite a nice inverse problem I worked with some years ago, quite intensively. And I think it's still a little bit understudied, and it holds some nice secrets for our community to unlock. Let me just quick mention that I represent the Finnish Center of Excellence in Inverse Modeling and Imaging. In in-resex modeling and imaging. And as you can see, we try to cover the map of Finland with in-rest problems research groups. So we're kind of maybe halfway or so. And also, hey, let me quick draw your attention to a couple of YouTube channels for fun science. The one is my own. It's mostly in Finnish, but with English subtitles. And the new one is inverse problems channel. We have some fun stuff there. And we want more. So if you have. And we want more. So, if you have something you would like to put up there, please let me know. Okay, and somehow the introduction to this topic comes from the way Stephen Hawking used to talk. So, let me show you a quick video. Why should we go into space? What is the justification for spending all day? Justification for spending all that effort and money on getting a few lumps of mood right. So that's the speech synthesis device of Stephen Hawking. And let me also give you a little idea about what mathematics can do for the quality of synthetic speech. So I will play to you four sound samples. None of them is spoken by a person. They are all. Is spoken by a person. They are all mathematically constructed signals. So, first sample one and with low and high quality. No one tried the challenge. No one tried the challenge. So that's the first one. And then there is a second one: first low and then high quality. Now they face a right-wing revision. Now they face a right-wing revision. And let me also not overstate things. I mean, part of this quality change is about the inverse problem of glottal inverse filtering, but not all. There is also other differences between this low and high quality. There's a lot of stuff there, including hidden Markov models, but good glottal inverse filtering is a very important part of making synthetic speech. Of making synthetic speech better. And why should we be interested in this one? Well, apart from the mathematical structure of the problem, many, many people have a medical reason to use synthetic speech. There can be some surgery taking away people's speech capability or also aphasia. And for all of them, synthetic speech is a really, really important part of life. And of course, I mean, also like robo calls, which we all probably love very much, but also public announcements in railway stations and stuff. All of their, I mean, we need good quality synthetic speech, preferably with some like emotional content that really sounds to us like meaningful and having to write, right. Right information content. So, I will be describing some advances in plotal inverse filtering. It was done with this group of people, mostly from Finland, but also Brad Story from Arizona. It was some years ago we did these works. But first of all, as always in applied inverse problems and applied math, we need to go a little bit deeper to the application area. To the application area, what are we dealing with and what do we need to model? So, let me introduce human speech apparatus a little bit to you. So, first of all, lungs are very important because with lungs, we push the air up the trachea to the voice box or the larynx, in other words. So, let me show you a detailed image of the larynx. So, this is the front view of the larynx. So, this is the front view of the larynx. We don't see, I'll show you another view in a minute, but there are a couple of things we can recognize. So, this is the Adam's apple. We can feel there are many cartilages. This is the windpipe coming up from here. If we look a little bit from side and behind, we see more structures. So, there are vocal folds, sometimes called vocal cords as well. Vocal cords as well, but I think I was told by the experts that vocal folds is a more precise term for them. There is epiglottis, this thing which closes the trachea when we eat, so the food doesn't go to the lungs. There are many structures like that. And for speech, vocal folds are important. And also these arytenoid cartilages here. We will soon get a closer look at how they function. At how they function and the vocal folds they provide. Sorry, on the previous slide. Yep, yep. What is the on the left bottom in the middle, it says posterior, but that's not Bayesian, no? Oh, yes, indeed. So, but there is no prior either. So, yeah, it's important to keep the term straight. Yeah, indeed. Indeed, so yeah, this is the anatomical posterior, meaning behind backside, front side, and backside. I was just being silly, but yeah, I know, but anyway, it's good to yeah. So vocal folds, when we breathe, they are just open so the air can flow. But when we speak, well, also, they are closed if we are, for example, listening. Are closed if we are, for example, lifting something heavy. We kind of we make a kind of a sturdy mechanics to the upper part of the body by taking some pressure into the lungs and closing up the vocal folds. But when we speak, they move. They move really fast. They open and close 100 times a second for a typical male voice, 200, 300 maybe for a female voice, and for opera singers, it goes. Voice and for opera singers, it goes up, I think, even like something 700 hertz or something. So these are actually the fastest moving parts of the human body. And another thing that's important for speech sounds and also for glottal inverse filtering is this yellow part here, well, including the pink cavity as well, but the yellow part is the vocal tract. Is the vocal tract, and the shape of the vocal tract provides important filtering to the sound, which I will describe in a minute also mathematically. But that's really important for making vowel sounds. They are made by changing this yellow shape according to which vowel we want. And we face our first simple inverse problem first. If we want to take a look at the vocal folds, we can't because if you choose. We can't because if you just look into the mouth, we see here this part. So, to see here, I offer the first solution of an inverse problem here. Just placing a mirror here, we can actually see the vocal folds. That's what I'm gonna show next with the help of my friend Ahmed Genet, who is a medical doctor specialized in speech issues. Speech issues. And here, let me state a content warning. If you feel uneasy with kind of medical views of the human body, you can close your eyes if you want. But on the other hand, I went to great lengths to provide you this illustration of how to see the vocal folds. So, as I said, we can place a mirror to the back of the mouth. I can assure you, this was not. This was not extremely pleasant, but it's something that's possible to do. And then, like I promised, we can take a look at the arytenoid cardages. There they move, there they move. But this was not yet really a vowel sound. So, let me show you another view. This is with a stroboscopic light. Stroposcopic light that's flashing very quickly and synchronized almost perfectly to the movement of the vocal folds, but almost so it shows a slowdown of the movement of the vocal folds. So, this is the kind of view that the medical doctors take a look at every time when someone has a Take a look every time when someone has a speech problem. That's a way to see if there are some kind of whatever problems with the vocal folds. Okay, and then let's go a bit more to the signal side of things. So what we just saw, the movement of the vocal folds, it creates a kind of a buzz here, like this. Something like that. And when that bus travels through this yellow cavity, the volume Yellow cavity, the vocal tract, it gets filtered. So it sounds like this. Of course, this is synthetic, but let me also offer you a real, real world view. I have this kind of a device, which is made for people who have lost their voice. So, what this does is it vibrates. There's a membrane that vibrates quite fast. Quite fast, so we can hear such sound. But if I put it here, it becomes a vowel sound. So that's how it works, the buzz and the filter. So there are two parts. So losing your voice is what you mean is that they're losing the vibrating cavities. Yes, yes. If, for example, if the walker Yes, yes. For example, if the vocal folds are surgically removed, for example, or they are paralyzed, or there's some problem with, so you don't get this buzz anymore. That's not there. But well, the cavity is still there. So if I'm not using my vocal phones, but I'm just using this one, I can even kind of speak. Oh, my, oh, my, oh, oh, my, oh, my God. I'm training for reality TV, still, still. TV, still, still some work to do. But I've seen, I mean, the people who sell this product, the salesman really can actually speak with this. He has such a problem himself. He can really speak very well with this device. So it's not really a filter, it's a resonance effect, right? Indeed, it is a resonance effect, yes. But engineers call it a filter because it modifies the incoming signal. The incoming signal. So, yeah, Plumin, I agree. It's not completely a mathematical term to use. But that's, yeah, that's how it is working close to the application. I mean, it's a lot about the language they use in the application field, which needs to be translated to our language, of course. So now, taking steps towards mathematics, let's look at the airflow function. So the vocal folds at some point. Folds at some time they are closed, then they start to open. For a while, they are completely open and then they quickly close. So they're going to open and close and open and close. So this red function here is like liters per second, how much air is going through. And it happens roughly speaking periodically, like this: the airflow. And what we measure with the microphone. We measure with the microphone is actually pressure, not flow. So if we differentiate this one, we get this function. So this is just the derivative of the function above. This is the air pressure at the vocal folds, the thing we cannot measure, but it's roughly what this is doing. And then after the filter or these resonances, it's changed. It's changed to this one, and this is what we actually measure by the microphone. And we would like to solve the inverse problem back to the flow function and also invert the filter in the process. And also, let's talk about the filtering or this resonance effect in more detail. So, what is this yellow thing doing? How can we get a mathematical grab on it? Of course, there are many ways of doing it. Be there are many ways of doing it. You can do a really 3D analysis or simulation of all the resonances and what's happening, but we can also do a kind of engineering type simplified model. So here, the red line goes through the vocal tract, and I kind of straighten up this red line into the x-axis in this graph. It's roughly like 18 centimeters maybe in length. And here, In length. And here you can see the cross-section of the vocal tract at various locations along transversal locations along the line. So it's kind of a fantastically shaped cavity, but we can record the area of each cross-section like this. So we get a function like this. So this is like an area function along the vocal tract. And this one we can. And this one we can approximate by a piecewise constant function. Of course, we make a little mistake there, or some approximation error happens. And then we can also take another step to go to rotationally symmetric models. Here I have 3D printed Japanese vowels. Let me quickly demonstrate to you. So again, using this one, and I have This one, and I have the 3D printed. So, this is the vowel R. And for example, if I take E, it looks like this and so, even with this rotational symmetric piecewise constant. Symmetric piecewise constant radius model, we do get a rather nice approximation to vowel signals. It turns out that each vowel is mostly described by just two numbers called formants. So, in this resonance process, the engineers call filter. If we look at the Fourier side, we see that for each vowel, there is a Vowel, there is a certain pair of frequencies that are enhanced. So for R, it's this one, 594 and 977. And for A, we have 617 and 2000 roughly. And here are the numbers for the Japanese vowels. And if you're interested, you can go to Wikipedia. They list a lot of different vowels. They list a lot of different vowels from languages all over the world, and they give these two formats that are kind of the major property of each vowel. Okay, so then I think I'm ready to go a little bit more mathematical. So we use convolution. So plumbing, we really are now using a convolution model. Convolution model, so that is a filter for modeling this resonance effect. So, S is our signal, and P is now the convolution kernel. And we use the usual discrete convolution model. And of course, we could do this in a continuous setting as well. We could be working with functions, for example, in Banach spaces or something like that. But here, I will. That, but here I will work with discrete signals. So now we have the direct problem and inverse problem. So if we knew the excitation signal, the buzz, if we knew that one, and if we knew the filter, then, well, the direct problem is just calculating the convolution. And in discrete world, we can follow the engineer tradition and We can follow the engineer tradition and use the so-called Z-transform for it. And then, in the inverse problem, of course, well, we record a noisy vowel sound, and then our task is to recover both the excitation signal and the filter. So, this is blind deconvolution in the language of our math field. So, it's a blind deconvolution problem. And let me just quickly give you a little illustration. Just quickly give you a little illustration still. So, this is now my excitation signal S, something like this. And then I will convolve it with this convolution kernel you see here. So it's the vowel E. Now I have this one the same, but I use a different convolution kernel, and then we get. Convolution kernel, and then we get the vowel R. So we can here see the difference between the convolution kernels. So this is a not perfect model. So actually, there are some non-linear connections also between the filter and the excitation. But if we ignore those, we arrive at this linear model I just showed you. But the inverse form is non-linear. You, but the inverse problem is non-linear because we are inverting for two functions instead of just one. I took a look at the history of this problem in our field of inverse problems, and there's, I would say, surprisingly little here. This is a problem that has been studied a lot in the engineering community, really a lot. And in recent years, machine learning approaches have made big advances, of course, in this field as well. In this field as well. But somehow, looking at this kind of small list, I think I gave it a quite good effort actually to find all the papers about this stuff. And this is all I found. Please let me know if you find more. I would really love to see what's out there, if there is more. But it may be that this is pretty much all from our inverse problems point of view that there is. Okay, and the main Okay, and the main problem, main motivation here, how to or what are we after here? This was carefully explained to me by the engineers who had been working with this problem for a long time. This is pretty much also like an equality issue, because for disabled people who need synthetic speech and good quality synthetic speech. And good quality synthetic speech that has emotional aspects and contents, like I played to you, these better synthetic speeches in the beginning. There is a curious problem that if we take a look at the formants we saw a bit before, like here. So, oh, sorry. So, here. So, many formants are already in the like from 200 to 400. From 200 to 400 hertz, roughly. And now, when we want to separate the filter and the excitation signal from each other, it's easier for a male voice because for male voices, the fundamental frequency F0 is something between 100 and 200, which is below the formants. So it's easier to tell them apart. But for women and children, the formant is in the The formant is in the very range of the lower formants, which makes the inverse problem much more difficult because these close by frequencies are harder to tell apart. So, this really is an equality issue. The synthetic speech prostheses are much better for men than they are for women and children. So, this is like one big motivation behind this study. And I don't think it still solved this problem, even with machine learning. Although, I must say, Machine learning. Although I must say, I'm not aware of the very latest developments there, but still, I'm doubtful that it would be completely solved. Okay, so finally, after this lengthy introduction to the application area, let me describe a couple of attempts we took at this problem with Vegas and MCMC first, and then we also did some optimization-based thing. Based thing, and I would be curious to see if that has Bayesian extent. I mean, of course, it has, but how interesting they would be. Okay, well, for this audience, this slide is pretty much self-evident, but maybe as a point that now I am working in finite dimensional spaces, although for sure this problem could be studied in function spaces, and maybe it should be studied in function spaces. But this is more like the engineering. But this is more like the engineering approach to go to finite dimensions. And let me describe what kind of modeling we use. So we took something called the CLOT model, which is quite traditional in the speech engineering side. It has one parameter. So here it's between zero and one, this K number, and when it's closer to zero, then the airflow is there only for a short time. Only for a short time and then the vocal folds close. If we take a bigger number, then there's a longer relative time when the air is flowing. And with k closer to one, then it's even longer period here. And how to think about this number is that if we think of this case, it's like it's called pressed speech where. Crisp speech where it's a very short time that the vocal folds are open. This would maybe be like a normal speech, the red one. And the blue one would be breathy sound, where air is flowing almost all the time. So even with this one number, we do capture some of the properties, important properties of the excitation signal. And then we put And then we put once we fix this one number k, then we just put several of these bumps in a row, we differentiate and we sample. So this is like from just one number between zero and one, we get this kind of discrete excitation signal for our model. And then for the filter part, so So I learned from the engineers that the Z transform is their favorite tool for this one. It's kind of a Laplace transform for discrete signals. So how it works is we take this power series expansion going from minus infinity to infinity. Z is a complex variable, and the coefficients in this And the coefficients in this series are the signal elements. So they are put into this infinite series. And then there's an inverse Z transform, which looks kind of like the Fourier series formula here. We have this e to the i frequency times angle. And then it turns out that convolution. That convolution on the signal side is pointwise multiplication on the transform side, like for the Fourier transform. And then I also learned from the engineers that there's a natural parameterization now for the filter effect of the vocal tract. And it's described actually by two complex numbers. The red one. The red one and the blue one. So, and well, well, they also have their complex conjugates involved in this development of the filter. So, it's called so-called all-pole filters. So, it's like a Meromorphic function. So, curiously, also like complex analysis appears in this approach. But now, this theta angle describes what is the frequency of the first formant. Of the first formant. And R, the radius of this number or the length of this complex number, gives kind of the strength of that enhancement of that formant. And the blue angle theta is for the second formant, and the R is for the strength of that emphasis on that frequency. And these other black dots, they are also poles in the filter, and they are important, but And they are important, but not so important that they would need to be taken as degrees of freedom in this model. So we have four numbers now on the filter part. We have these two angles and two radii. So here we can, yeah, what I just said, yeah, the red, red number is describing the first form and then the blue number is the second form. So now we now arrive. We now arrive at a five-dimensional model for a vowel sound. It's a simple model for sure, but apparently it's rather powerful anyway. So we have the cut parameter between zero and one, and we have the argument and modulus of the two complex numbers in the all-pole filter corresponding to the two formants. So then we built up a base. We built up a Bayesian inverse problem based on studying the probability, the posterior probability of these five numbers given a noisy recording of a vowel sound with a microphone. And we, of course, could put in some prior, for example, we could we could We could have some bounds where the formants can be. And so we can bound both theta and R for both of these. Of course, it takes some considerations to think how much do we want to put a prior onto them, but definitely they cannot be whatever. So it's a good idea to have a prior on them anyway. And then for the likelihood distribution, again, we learned from the engineers that it's usually not enough to just take a look at the usual like L2 penalty for the synthetic signal and the measured signal. They said it's also a good idea to take the FFT of both of those signals and absolute value of the FFT and subtract them. The FFD and subtract them. So that was something like a working knowledge from the engineering side, which we then took into our model. And well, with MCMC, that's something easy to do. With like maximum opposite oriented, maybe not so easy. Oh, Richard, you have a question maybe? So I'm just wondering: is this a proper log-likelihood function that comes from a statistical model, or are they just adding some kind of fit term like ad hoc? I mean, because the log-likelihood function. I mean, because the log likelihood function typically comes from a specification of a statistical distribution, given a parameter, and or are you just adding this on because you want to maximize or sample from something different? Right. That's excellent question. I don't know, to be honest. So, this is definitely not derived from any like noise statistics or something like that. I don't think so. I think this is more like I think this is more like as if we would be doing some like Tihanov regularization and adding some nice penalty that works well. Okay, so they're just adding like something to the objective function, but that does not, I mean, it's not a proper posterior distribution in the statistical sense. Good point. Thanks. Yeah. In the course of that project, that's something I really didn't think much about. We just wanted to get it done. Much about it, we just wanted to get it done and also follow the folklore of the engineers. But that's, yeah, thanks, Richard. That's a really good point to think if this has a mathematical interpretation or not. But I mean, you can kind of revert the question and ask the engineer or maybe not, maybe rather ask yourself what the statistical model would be that you would have to suppose on the data generating process for you to get that log likelihood function because it might give you. Lock likelihood function because it might give you some idea of what you're actually modeling by it, no? But it's just a remark, so never mind. Thank you. I mean, yeah, it's a really, really good point. Yeah, I really didn't think of that, but oh, good, good. So there's already one point of further research for this one. So here we have an MC-MC chain, or because it's five-dimensional, what I'm showing you is this like marginal distributions of. Like marginal distributions of our point cloud in the 5D space. So you can kind of compare how each of our five variables relate to each other. And first of all, we can see this is not a Gaussian situation. We see this. Well, of course, well, R is should be non-negative. So that's one thing that makes a difference. But otherwise, maybe these ones are pretty close. Maybe these ones are pretty close to Gaussian, anyway, but I think these bottom ones are not. But anyway, what we could do, what we'll show in our 2014 paper was really nice. We could compare to several traditional methods where, as you see, everything becomes more difficult when we go up in fundamental frequency. Like this is like going from male voices to female and child voices. And child voices, especially the traditional methods, start to fail. This was really the starting point for us. The engineers told us that this is the traditional problem in the field, that even if they do pretty well with low fundamental frequencies, when they go up, the quality really disappears. So with our MCMC, we could, it's the red one, so we could really reduce the errors. I'll make a note of these weird error measures, H18. Weird error measures, H1, H2, and knock in a minute. Here's a picture of what we can do. So, this is one of the traditional methods for a bit higher fundamental frequency, and this is the airflow. They insisted that they want to see the inverse map applied to the noisy measurement. We didn't quite, we thought it's much nicer to show the The CM estimate, but they really wanted to see the inverse filter applied to noisy data. So that's what we showed in the paper therefore. And then for the next approach, this was still seven years ago. Probably the MCMC would be faster these days already. But still, we kind of felt we would like to take another look at another inverse problems methodology for this. So we need to. This. So we did some dehonotype regularization with Rodrigo Blair, who was my postdoc for a few years in Helsinki. He did his PhD with Ronnie Ramlau about, so really, it was really a nice fit because he was working with blind deconvolution and alternating minimization in his thesis. So he was really, really well prepared for this problem. Really, like, well, prepared for this problem. So, then we wrote up our problem in the form that they developed with Ronnie. So, the theoretical framework required this kind of variational functional to be minimized. So, here we have the usual data discrepancy, but then for the regularization, it was essential to have some estimate for the signal part and then these. Part and then these penalties on top of that, which is kind of having three parameters is not so nice. But anyway, that's what we went for. And then the alternating minimization works like because we have now two unknowns, the signal and filter. So we first fix the signal and solve for the filter, and then we fix the filter and then we solve for the signal. And then we solve for the signal, and then we alternate between that. And Rodrigo and Ronnie had developed a nice theory for this already before we started. So this was quite nice. So then we could actually further improve in some cases the results. So here our new blue one is our alternating minimization. So this is for not so big fundamental frequency, but when we take Frequency, but when we take, did we take a bigger one? Then we start to get more benefits from our alternating minimization. It's performing better. And let me also show you some numbers. So these graphs are, of course, nice, but the numbers maybe are more. So another thing I learned from the engineers is that it's really the measuring the quality of Really, the measuring the quality of the result shouldn't be done like we usually do with like some L2 error comparison or something like that. They have their own ways of measuring. So the first one looks at the so-called spectral tilt of the glottal flow. So, well, I'm not sure if I can give a very deep explanation of what is going on with this one. Going on with this one, and the second one is looking at how well the reconstruction captures the closing, the duration of the time when the glottis is closed or the vocal folds are together compared to the ratio of the peak flow and the negative peak amplitude of the flow derivative. So, this is something they've developed in their speech engineering field. Speech engineering field. So we took these two measures for our quality measure. And this is where we got with this project. So we see that when we look at the higher fundamental frequencies, it seems that our alternating minimization, well, in this case, it's the best one in both error types. Types and here it's better in this NOC sense, but actually, actually, the MCMC seems to be better in this H1, H2 sense. But nevertheless, in some cases, our alternating minimization could provide the smallest error anyway compared to the other methods. Okay, and somehow what I wanted to tell you is. What I wanted to tell you is that this is maybe a bit overlooked inverse problem. It's very important in practice for many people. It's really, really important quality of life thing. And it's nicely non-linear, but not very non-linear. So maybe it's a nice problem to study if you don't want to go like full super non-linear first, but have something intermediate. And like I showed you, this list of papers, there are very few papers. I think there would be a lot. Very few papers, I think there would be a lot of work to be done on this beautiful problem. And I think Bayesian approach could maybe offer something extra compared to machine learning because we could do like this credibility analysis and uncertainty quantification, which the black boxes will not provide anyway. Okay, so that's pretty much all I wanted to say. So thank you so much for your attention.