Absolute pleasure meeting everybody. Special thank you to the organizers and everybody here, except for maybe Andrew. But I'd like to give a special thanks, special thanks to Andrew for talking about a lot of things that I'm going to be talking about, so I don't have to spend as much time on that. So, just the very briefest of outlines on what I want to talk about today. The underlying motivation for what I'm going to talk about. Motivation for what I'm going to talk about has to do with research I did in my master's and my early PhD when we were all many years ago, when we were all entering the primes of our research careers, and we were talking topo2-induced strand passages were the hot topic at the time. And it kind of opened some questions that led into my research. And in particular, I'm trying to develop these lattice models that are. Develop these lattice models that are more DNA-like, and I use experimental DNA-nodding probabilities to fit and optimize my model. And it kind of is not an easy thing to do. So I'm going to talk about a really cool algorithm that uses a process called stochastic approximation. And this can be used to optimize pretty much any type of model in a very efficient way. And then I'm going to show the results of this stochastic operation. Of this stochastic optimization of my model to this DNA naughty experiment. And I'm actually going to compare it to kind of a discrete worm-like chain model with some small tweaks that I'll get into. So, that being said, my master's research combined two different projects into one. The first was something that my predecessor Michael Saffron did. My predecessor, Michael Safron, did, and he wanted to study strand passage in lattice polygons. And in particular, he wanted the result of the strand passage to also be in the lattice. He didn't want to do these so-called virtual strand passages. So what he did is he made his lattice polygons contain this thick structure called theta, which was these two separate parts, the red part at the bottom and the blue part at the top. And as long as And as long as certain vertices in the middle are unoccupied, you can pull the blue part down and pass the red part over top. So you get something where the strands have crossed and you're still on the lattice. And the other part that I combined was something that was done in 1994 by some very awesome people, Carla Tessi, Stu Whittington, DeWitt, Enzo. I think I'm forgetting one person, but. I think I'm forgetting one person, but my bad. I don't think they're here. And so, what they did is they attached an energetic term to a lattice model. And what they did is they had this short-range contact potential, where whenever you have two vertices in the lattice that are unit distance apart and not bonded, like those two are an example right there, you add this parameter V to it, and it's representative of. And it's representative of solvent quality, and it has this really fancy phase transition at something called the beta point. And so that's their short-range potential. And then they also include this long-range potential where they look at every pair of unbonded vertices. And they compute this kind of Yukawa electrostatic screened Coulomb potential. It has many different names. And it takes into account via this parameter zeta. Parameter zeta. It's called the inverse debiling, and it's related to salt concentration. So it models the effect, hopefully, between the vertices of the polygon, your DNA or polymer, and the counter ions in solution. And so, again, this salt concentration is kind of absorbed in this zeta, and it has this additional parameter A, which kind of Parameter A, which kind of scales up the effect of this potential. So I said in my master's thesis, well, why not both? And so I applied these energetics to this strand passage model, and I only studied it from a very surface level. I took a fixed value of V and a fixed value of A that was used in this testy at all paper. And then I just wanted to see how strand passages change with. Strand passages change with salt concentration. So, without getting into too many of the details, I sampled a bunch of different unknots using the BFACF algorithm, or a special version of it that never messes with the fixed structure. And I generated samples of unknots of a bunch of different lengths for a bunch of different salt concentrations. And what we found was that, well, starting at the bottom is the lowest salt concentration, the lowest zeta. Salt concentration, the lowest zeta. And as the salt concentration increased, so too did the probability of transitioning to a knot when you do a strand passage. And these are plotted for various different polygon links. And one of the nice things with the lattice is you have these asymptotic properties of scaling forms that, you know, as the polygon link goes to infinity, this should approach a fixed value that we call a limiting strand transition property. A limiting strand transition probability. But that's not going to be the focus of my talk. It's merely kind of what motivated what I'm about to show you today. Again, I used a fixed value of A and B. But the big question, at least the first one is, well, what's the best value of A and B to choose if we want to best model DNA? So, what is the most DNA-like values of these parameters? And I guess. And I guess the next question is: what does it mean to be like DNA? So finally, okay, if I can answer these questions, how the heck do I find these parameter values? Because as Andrew said, again, special thanks, running these algorithms takes a really long time. So I would love to do a grid search of a bunch of different parameter values and hopefully find some measure that I can. Hopefully, find some measure that I can answer that's the most DNA-like. But that takes a long time. And so I need to be more clever and try and find something more efficient than just a very naive grid search of my parameters. So to answer kind of those first two questions, what does it mean to be DNA-like, I turned to DNA-notting experiments and two particular experiments where they were Sean Wong. were Sean Wong and Ribbenkopf et al. and they randomly cyclized this phage DNA where they had these lovely sticky ends. I guess that's what Javier was talking a lot about is you have these complementary ends and these are probably not the base pairs. I'm not a very good biologist. I literally just put base pairs that are complementary to kind of illustrate this thing floats around and eventually the two ends meet and it forms into And it forms into a knit, randomly cyclized DNA. So, when this process happens, you get a distribution of knots. It can float around and tie itself into a knot. And what these two experiments did for two different lengths of DNAs is they both found that as you increase the salt concentration, the probability of, I guess in Sean Wang, the probability of nodding, or in Ribbenkopf, the probability of nothing. Or in ribbon cuff, the probability of just specifically a trefoil increases very significantly. And the kind of justification for this is the more salt you have, the more sodium counter ions you have in the solution. It screens the negative charge of the DNA, which allows it to get closer to each other, which means it's probably more likely to form a knot. So, in particular, I'm just going to look at this data set, the Shawn Wang data set. The Shawn Wang data set. And what I want to do is try and model this experiment and find parameter values from my model that give me nodding probabilities as close as possible. So that's going to be my metric of what does it mean for my model to be DNA-like. Again, I'll just kind of skip this page. There are different ways to model DNA. There's off-latts, these freely jointed chains. Chains. Discrete worm-like chain models are the most in vogue type of model right now. Biologists really like them. But there is some benefits. I'm not by any stretch of the imagination saying a lattice polygon is better than this because it's not. But there's a relatively unexplored is like just how well can a lattice polygon model do? And what I'm going to show you is that if you get a To show you, is that it gets a lot better than one might inherently expect. Because when we look at something like this, that doesn't really look like DNA. And you get something with a lot of edges, like Andrew had 10,024 edges, and the polygons are all like twisting around like that. And that's not something that DNA does. And that's probably why biologists don't typically aren't a huge fan because, well, that's not what DNA looks like. What TNA looks like. So we're going to model it with these lattice polygons, and I assure you, we'll get something that may be a surprising result. We're going to use this Yukawa, this screen Coulomb potential with this parameter A. And just to kind of describe it in more detail, we look at every single pair of unbonded vertices, we compute the distance between the vertices. Distance between the vertices, and then we can compute this term for every single pair in Admola. And just to give you an idea, when you have a really low salt concentration, you get something that's nice and open. Again, you still have all these lattice-y things that are kind of not DNA-like. And when you have a really high salt concentration, it can get nice and close to each other. So you can pretty easily see why something with a high salt concentration might be. With a high salt concentration might be more likely to be nautile. So, the only other thing to add is that because we're modeling this Shaw and Wang experiment, they have six data points at six specific salt concentrations. So, for the context of what I'm doing, the salt concentrations are predetermined. For what I'm doing, it's essentially a fixed parameter. Fixed parameter. So I'm only interested in adjusting and switching the dial on A. And so, second, that same model that Tessie et al. did, they had this short-range contact potential where whenever you have two vertices that are distance apart, you add this term v. And if you have a really low value of v, it prefers these contacts a lot, gets a lot closer. And if you have a really high value, well, then maybe. Well, then maybe it prefers them less, and you get them much more spread out, and you get these little kind of zigzaggy sorts of patterns. So, as it turns out, this model isn't the best at modeling DNA, and so we'll explain why a bit later on. So, I ended up looking at a second short-range model, and I replaced the contact potential with something based on bending rigidity. So, basically, it's a penalty. So basically, it's a penalty for bending. So, whenever you make a right angle in the lattice, you add this parameter gamma, which I just say it's related to bending rigidity. It's a nice thing in the lattice because you're only ever going straight or in a right angle. And so if you have no bending rigidity whatsoever, you just have a typical lattice polygon conformation. But when you crank the dial up, it really doesn't like to bend. Don't like to bend, and so you get these long sequences of straight edges because that bending penalty is so very high. So I looked at these two separate models. Both of these models had this long-range Yukawa potential to take into account salt. And then one had the contact potential and the other had the vending potential. And under statistical mechanics, we are sampling from a Gibbs-Boltzmann. Are sampling from a Gibbs-Boltzmann distribution where the probability of a polygon is proportional to e to the minus potential energy over Boltzmann constant times t. Okay, so we've defined this lovely model where we're going to model this DNA from this experiment, this 8600 base pair phage DNA with a lattice polygon with a fixed number of edges. So, how do So, how can I sample from this model? Well, there's the pivot algorithm. Except the pivot algorithm, in its typical sense, doesn't take into account any sort of energetics. Again, like Andrew was saying, you can take a segment, randomly chosen segment of your polygon, and you can do this lovely inversion move that he is such a big fan of. And that's kind of step one. Your program picks a segment, tries to pivot it, and says, hey, Tries to pivot it and says, hey, this is a new polygon. Can I keep it? And it puts it on a pedestal and it kind of proposes it. Well, you need to compute, well, what do the energetics have to say? So you compute what's the change in energy with the new one and the old one. And the general rule is, if you get a lower potential energy, I'm going to keep it. It's going to be great. But if it's a higher potential energy, you don't want to immediately discard it. You just only accept it with a certain probability. With a certain probability of e to the minus change in energy. And it just so happens, the Markov chain that you get, when you combine the pivot algorithm with this metropolis sampling, the stationary distribution is exactly the same as the equilibrium distribution from my model. So what's going to happen is I need to find some values of my parameters that fit this DNA experiment as closely as possible. This DNA experiment as closely as possible. And so, what does it mean to be close? Well, the nodding probability should be close, but let's again try and be more specific. I basically compute a weighted sum of square errors. So, these nice black square data points, that's the experimental data. Let's say I ran some simulations and I got these six nodding probabilities. Well, I look at how far am I away from the actual. How far am I away from the actual experimental data point? Is x, but I also want to weight it by the size of the error box. Like, I should penalize myself a lot more for missing this point than I should for missing this point, because it's got a much wider range. So I essentially look at x divided by y, or roughly how many error bars am I away from the actual experimental point, and then I square it, and then I do this for all six of these points, and I add them all up to get one single. And I add them all up to get one single number that kind of quantifies how close am I to this experiment. And so I'm going to claim that the optimal parameter, the best fitting DNA parameter, is whatever is going to minimize this sum of weighted square errors, which in these problems we often call it a loss function. So I just call it L of P, where P is my parameters. So that's the dream. Find the best possible. Find the best possible one. But like I said, these simulations take a long time to run. Like calculating the change in energy, granted, I have a very naive implementation of this algorithm. It's an n squared operation. So simulations can take days to run, just at one salt concentration, one A and one B. And now we need to run it at six of them. And if we want to do a grid search, like Neem, if we have infinite. If we have infinite computational resources with Alliance Canada, then we can do these things. But so, in a perfect world, you could just take a mesh of all your parameter values, and if this star is whatever the true best value is, hopefully we can pick the one that ends up being hopefully the one that's close to the true value gives you the lowest loss function. But that takes a lot of time, so But that takes a lot of time, so we need to be more efficient. And this is where this stochastic approximation comes in. And so it's thanks to JC Spall. It's been around for about 30 years now. And what it does, and again, this can work for a model with any number of parameters. And so I kind of refer to it as build your own gradient descent. So what So, what we do is because we don't actually know what the true loss function is at any parameter value. We can only estimate it with simulations. And so, let alone try and compute a derivative, which we've used for gradient descent. So, literally, what you do is you pick a starting guess for your parameter. And then you perturb that parameter a bit in two different directions. Basically, you generate, for each parameter, you generate a plus one. For each parameter, you generate a plus one or a minus one, and you kind of travel out a certain distance in the plus and minus directions. So you get these two dark blue points that are close to your parameter guess. You run simulations at these values. You figure out, well, how close am I to my experiment? So I run simulations at the six-salt concentrations, figure out how close am I to the experiment. How close am I to the experiment? I can estimate this loss function, and then I can basically make a really terrible estimate of the gradient. So literally, I'm only looking at one particular projection direction, and I can estimate the gradient to be the slope at that particular point. And if all goes well, hopefully the one that's closer to the true optimal parameters will give you a lower value, and you can track. Lower value, and you can travel along that direction a certain distance. So, in the interest of time, I won't get too much into the details, but by repeating this very simple, very naive process, estimating the gradient in only one projection direction, you are guaranteed to find the global optimal solution. So, in a perfect world, you know, you do this process over and over, and hopefully, you get close to this value. This value in far fewer iterations than just doing a grid search. And again, this same thing works. You could have 100 parameters. You only need to run simulations at two particular parameters. So again, this could be a hundred dimensional graph. You only have to look in two different vectors and estimate the gradient along that direction. So it's actually quite a powerful way of doing things from an efficiency standpoint. Way of doing things from an efficiency standpoint. So, this is what I did. I did some previous research. I tried to find a good starting guess for my parameters A and V. And then I ran this SPSA, or simultaneous perturbation stochastic approximation algorithm. And as soon as my loss function kind of stopped decreasing, I said, Okay, I'm as close as I'm going. I said, okay, I'm as close as I'm going to get, and what are the results? So this was for this contact ukawa, the short-range contact potential with the salt energy term. And I ended up running 50 iterations of this, polygons with 400 edges. And, you know, after a while, it didn't, it pretty much flattened out, especially on the right is the value of V, and this one's A. Value of V, and this one's A. It kind of looks like it's going up and down still a little bit, but I forgot to include this graph, so I'm going to look at one from our previous talk. Basically, the loss function just started kind of fluctuating around. Even after 10 iterations, there was not significant improvement. So I called it a day. Said, okay, these are the best values I think I'm going to get with what I did. Think I'm going to get with what I did. And then with these kind of optimized values, I ran some really long simulations. And the black data points are the Shawn Wang data, and the red data points are from my optimized parameters, and they're kind of close. Kind of. The big, most annoying data point in the world that I hate with a flaming passion is this one right here. It seems to be unusually low. So anytime I plot something, I plot something, the success inherently comes down to how close am I to this third data point? Because we're taking a weighted square error, like we're about one and a half to two error bars away, so that contribution to this is about four, whereas all these other ones, if they're one error bar away, the contribution is only one. So inherently, it's kind of juxtaposed based on this data point here. So the next question we asked. The next question we asked was: well, what if we just increase the number of edges in my polygon? Like, maybe we're just not big enough yet. So, if we get more edges, maybe we'll get a higher resolution model. And so, I did the same process again with 800 edges, and it was maybe a little bit better. I tried running my simulations for a lot longer as well to try and minimize the error. Minimize the error, so did everything I could. And this process took me over a month and a half of computation. So each one of these iterations in this algorithm takes like two or three days, and I run 20 of them, that's two months. And so it ends up getting a little bit closer to this middle data point, but everywhere else is pretty comparable. And so I pretty quickly, with my naive implementation of the pip. Implementation of the pivot algorithm ran into a computational wall. Increase the edges again with my implementation would just take too much time. So I think Chris got a recommendation for someone. He said, why don't you use a bending energy term? And so that's how this sort of additional short-range term came into play. So we said, okay, so we swapped the contact energy with the bending energy. With the bending energy. Still kept the salt term because we're modeling something with salt. And we ran the same process again and optimized those parameters. And we got something that ended up being actually a lot closer to this point, but a bit further away at the end points. So at the end of the day, I'd say it's, if anything, it's a marginally better fit, marginal. So the question. So the question is: well, which one's better? If their fits are pretty close in quality, which one's better? And well, we can look at other properties of DNA. So that's just all of them on the same graph. You can see they're all, there's not a ton to choose between all of them. The key difference is this green one is from the bending model and it's a little bit closer. But it sacrifices it at other points. So let's look. So let's look at a different topological property, very important to biologists, persistence length. And if you try and compute the persistence length from this contact model, again, you've got all these little hinges, but DNA is not that flexible. We can actually kind of estimate persistence length in the lattice and convert it to base pairs. And the persistence length is 22 base pairs, which is not even close to what you'd expect to see in DNA. You'd expect to see in DNA. But with this bending rigidity, and believe it or not, this is a lattice polygon with 600 edges. You just have these long, straight segments. And because of this, you have much higher correlation, which ends up inflating the persistence length. And it's actually even a little bit higher than that of DNA. It's 172 base pairs, but in the grand scheme of things, it's a lot closer than it is. Closer than it is to the other model. I don't really think this particular type of model has really been explored that much. So, like, just from an objective point of view, yeah, there's a lot of still lattice artifacts in this, but it's way, way closer to an actual picture of like an electron micrograph or DNA than something like this. So, what I would claim is that I would strongly prefer the bending model over the contact model. Now, what the thing that you might find interesting is, is I compared this to a discrete worm-like chain type experiment. So, Florian Rieger and Peter Vernal basically used a discrete worm-like chain model, but instead of just plugging in the persistence length of DNA into their parameters, And A into their parameters, they ended up fitting those parameters to the same knotting probabilities. And so they want to see: is fitting to knotting probabilities a good idea? Because persistence length can change with salt concentration. And if you're picking just a fixed persistence length for everything, then maybe there's some small issues there. So they ran this experiment, they optimized their parameters, and then with that, they increased the DNA length. They increased the DNA length. And so I did the same thing. I increased my DNA length. I compared it to their data, and it pretty much agrees spot on. So just to run through the conclusions in 10 seconds, my algorithm produced parameters that fit this DNA experiment pretty well. And I guess the big thing is the bending Yukawa model has better persistence length, and it can actually compare. And it can actually compare reasonably well with this discrete worm-like chain type model, which is, I'd say, unexpected. So, that being said, thank you very much. Questions?