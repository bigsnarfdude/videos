Free to interrupt me at any time if you have questions. And the setup here, I'm going to just introduce some language and notations, but I will use the Gaussian mixture model as a running example. And if you have a class of parametric class of densities, let's say P sub theta, if you randomize theta by putting a distribution on for the parameter, you can think about it as a prior or mixing distribution, and this induces a mixture density, right? and this induces a mixture density right which is an average of of of over theta and we're going to just going to overload the notation denoted by p sub pi pi being the the mixing distribution p pi being the mixture density and the goal is and the setup is that we have data drawn from p pi for some true pi and the goal is to learn this pi okay and the running example as i said is the just the vanilla gaussian mixture model so here i'm So, here I'm looking at the simplest possible mixture model. Basically, it's a mixture of normals with different centers, right? The center is theta. And in this case, if you think about the representation of the mixture density, it's nothing more than a Gaussian convolution, being the convolution between pi and the standard normal density. So, this is the data generating distribution, and the special case of interest to. And the special case of interest to statisticians is in particular the finite Gaussian mixture where you have, let's say, k components. So, in other words, the true distribution is the convex combination of k different, I mean, potentially different Gaussians, but they could overlap with centers at theta i and weights being Wi for the ith component. So, in this very special case, the mixing distribution just a discrete distribution with k atoms. Discrete distribution with k atoms. And the major difficulty that everybody knows that underlies any mixture models is the non-convexity of the mixture likelihood, not in the ways, but in the locations. And this is basically the driving force for the invention of many different heuristics, most prominently the expectation maximization, the EM algorithm. But people have realized that it can suffer from spiritual. It can suffer from spurious local maximas, and this gradient descent-like algorithm can get stuck at points which is far from the ground truth. Okay, so there's, you know, as opposed to this more parametric type of approach, there's the non-parametric approach. And I'm going to mention three of them. The first one is metal moments. So think about as opposed to explicit. out as opposed to explicitly parametrizing theta by the weights and the locations. I'm going to do it by learning its behavior of a group of test functions and try to reconstruct it on what I learned on these test functions. And metal moment refers to choosing the test functions being polynomials. And this is a very old method that goes back to Pearson. And the second one is minimum distance estimator when you set up some meaningful distance. When you set up some meaningful distance that measures the empirical and the true density, and then you can minimize this distance over the mixing distribution. And this is proposed by Wolfowitz in the 50s, but probably not directly applicable if you have a Gaussian mixture model. And the one that I'm going to discuss today, which is non-parametric maximum likelihood, was also introduced in the Was also introduced in the 50s by Kiefer and Wochowitz. And it just means I'm going to treat the problem as if I know nothing about the mixture distribution, thinking it as an arbitrary probability measure and just maximizing it. So, this is where the name numper metrics comes from, even if I know the true model has a finite number of atoms. So, that's the So that's the focus of this talk, of this work. So, as I said, this optimization problem refers to maximizing in this case the total log likelihood. I put one over n here just to make it average over all measures over the parameter space, all probability measures. So, you can think about this as a convex relaxation to the previous finite mixture model by just, you know, which is a non-convex set, right? Is a non-convex set, right? A little bit like you know, from L0 to L1, if you will. And this does make the problem convex. So the objective function now, as a function of the measure, is concave. And the price is this becomes an infinite-dimensional optimization problem. But you will see that it's actually quite cheap to solve, at least in low dimensions. Okay, so perhaps in more Perhaps in more timely terms, you can think about this as a way of over-parametrizing the model. I'm going to infinitely over-parameterize it. So in one dimension, basically, the statistical property and basic structural properties of this solution are very well understood, starting from the work of Seymour in the 70s and the Zhuao in the 80s, and then finally culminating in Lindsay's. Uh, culminating Lindsay's sequence of papers and his monograph on the mixture models, so which confirms the existence, uniqueness, and also the discreteness of this solution. So, this solution is going to be some discrete measure. So, these are all well understood in one dimension. There's many different style of computational efficient solvers have been proposed over the years for this infinite-dimensional concave maximization problem. Some of them are of iterative nature. Are of iterative nature, and some of them, and more recently, people just start to just grid the domain and then run CVX. Okay, so advantage of this method, as in all, you know, relaxed approach, is that it's very flexible in the sense of there's no tuning parameter, right? I'm not penalizing anything, I'm not encouraging the solution to output a certain size. And I do not know what is, I do not. And I do not know what is, I do not need to know what is the upper bound on the size of the mixture model. Computationally, it doesn't suffer from these non-convexity issues that were present, for instance, for EM algorithms. And it actually is very accurate, right? It achieves nearly permetric rate for learning the density, close to what central limit theorem tells you with some extra log factors. So that's the good part. Part and more advantages is widely used in empirical Bayes for learning the prior and executing the corresponding Bayes strategy. So I don't have time to go over this part in these 30 minutes of the talk. So let me just skip this. But there are a few potential issues, right, in addition to these advantages. So as I already hinted, this is an extreme version of Is an extreme version of the over-parameterization. Even if I know the true component has at most six components, I'm going to over-parameterize it by probability distribution. Therefore, it perhaps runs the risk of overfitting. It might return to you a model which is super complex. Okay, so, and this is basically the main question, the main motivation underlies this work: is that does numper metric? Is that does numperometric MLE does genuinely overfit? If data is drawn from a three-component mixture of Gaussians, does it return to you something much more complicated than that? It's a very simple question. And what is the typical model size with high probability? What is the model returned by this completely unregularized maximum likelihood? Sorry, I have a question. Yeah, please. What distributions are you searching over? You're searching over literally every possible distribution? Yeah, over r or let's say over plus minus y intervals and so on. Okay. Is it obvious why it shouldn't just be like the empirical distribution on the observed points? No, it doesn't maximize the likelihood. Yeah. Okay. I mean, i that's what people usually do uh as a initialization for starting p for iterative algorithm. For iterative algorithm, but then you're going to improve. The solution is not. Yeah. Sure. Yeah. So, yeah, maybe it's worth clarifying this. I'm maximizing the density of a mixture of Gaussian. So this is a convolution with pi with standard normal. If you plug in empiricals, it's not clear. It's a solution. Yeah. I see, I see. Okay. Thank you. And these two questions, the typical behavior of this optimization problem is not answered by class. Problem is not answered by classical theory. So I'll start with the classical theory, which tells you how this solution behaves. So the objective function is the total likelihood, and then the pi hat is proportionally the maximizer. So we can write down very simple first-order optimality conditions. If you add a new component to the solution by taking By taking a convex combination of the maximizer and the extra new component, it will not increase the likelihood. So, this gives you a collection of inequalities, which if you can take derivative, right, this perturbation along these directions as zero, it tells you an inequality. So, you can think about this as a KKD condition, if you will. And this function is a function of theta, and this is true whenever you. And this is true whenever you inject a new component, right? So, this inequality is true for every theta. In other words, if I call this function d of theta, and this first order optimality condition says this d function is globally upper bounded by one. And thanks to convexity of this optimization problem, this is also sufficient to guarantee optimality. So, in other words, this infinite number of infinite Right, this infinite number of inequalities certifies the optimality of i hat. So, what is this thing? It is the ratio of the just normal centered at theta evaluated i sample versus your fitted likelihood. So, this is the gradient of the right, the log likelihood. Okay, so from here, you can already extract what a classical structure property on the solution is, the existence of uniqueness, and so on and so forth. The existence of uniquely, and so on and so forth. This condition might look a bit unwieldy because it's a tail-binding condition, right? So, pi-hat also enters here, which I have no idea. However, you can already say quite a bit about the solution just based on a very simple observation. So, this function is globally had a ceiling at one. And then the key observation is that if you average theta according to If you average theta according to the pi hat itself, you get exactly one because the numerator cancels the denominator after you average. So you have a strange situation of a function which is globally upper bounded by one. And then when you average it over a special distribution, you hit exactly one. There is only one possibility for this, is that this special distribution is supported at these global maximizers. And the global maximum exactly equal to one. Maximum exactly equal to one. So you can think about a curve that oscillates, attaches one at five different places, and this pi-hat must be supported at that five different points. So that's the starting point of the classical theory and also of this work as well, is that the support of this solution to this infinite dimensional optimization problem is contained in this global maximizer of this d function, this gradient, and the gradient is further included. Gradient is further included in the critical points. So then it boils down to counting the critical points. But again, this is the thing, right? I cannot write down this function because it involves the optimal solution. But the form of this function is already powerful enough. So what is the form? Well, if you write down, I mean, we cannot say anything for this number because this is the optimal fitted density at the IC. Fitted density at the eyes observation, but it's some number. And the numerator is just a translated Gaussian, right? Centered at, if you think about this as a function of theta, it's centered at your i-theta point. So the form of this gradient is exactly a mixture of Gaussian 4 of n components, n being the sample size, centered at each data point. So I call this NGM, just means n component mixture of Gaussian. Component, which of Gaussian. And there is a lot of theory that tells you functions that behave of this form, analytic functions, this is very close to exponential polynomial, can have how many modes, modes meaning local maximums. You can prove this by checking how many roots the derivative can have, which is further proved by taking more derivatives and so on and so forth. So there is a classical. And so and forth. So there is a classical, you know, well-known theory that underlies this type of results. And using such results, Lindsay proved for the Gaussian model, this solution exists and unique and discrete with at most n atoms, n being the sample size. So let me repeat. So what Lindsay proved is that if you have 1,000 data points, you will not fit a model with more than 1,000 components. Okay, so this is probably Okay, so this is probably not powerful enough to answer the question I asked in the beginning. If these 1,000 data points are formed from a three-component mixture of Gaussian, I truly don't want them to fit the 10,000 component, right, 1,000 components. I mean, it's useful result, but not very useful for the questions I raised in the beginning. And but on the flip side, these results being completely deterministic is actually. Completely deterministic is actually tight. And I'll explain this in a second through experiment. But people observed in practice is that this methodology actually typically fits a very small model. And this is the reason people like this model in practice, is that you don't need any intervention to encourage to fit smaller models, penalize complex models, kind of automatically resolve itself. Resolve itself. So the question is: we know these results being worst case cannot be improved. Can we improve on the average case? If the samples are drawn independently at random, can we improve it? So this is why we turn to the typical guarantees. But let me show you some examples. So here example where it will generally fit a huge model. Here, the sample consists of Here the sample consists of 11 points separated by a lot, a hugely separated point. So in this case, three times the standard deviation. So in these red dots here. Suppose this is truly your data and someone insists you to fit a mixture of Gaussians with one standard deviation. The best fit is just 11 components, each component corresponding to one data point and equally likely. And this is what a typical solver returns. Returns. So, I mean, there's, I mean, although there is no proof that this is happening, but it's not surprising that this happens, right? But of course, the hope is that this is not a typical configuration of the data. If you draw things from a three-component Gaussian mixture, things tend to cluster as opposed to very separated in this pathological way. So So, the bad news is you cannot improve this. In the worst case, the good news is this is not typical. So, typically, it will look like this. So, this is drawn from a two-component mixture of Gaussian, and the data point, these red dots, tends to cluster as opposed to this hugely separated situation. So, in this case, the maximum likelihood non-parametric one does fit a two-component. So, we try to explain this. So, we try to explain this using some theory. And this is a further experiment where I run the numperometric maximum likelihood 500 times on 10,000 data points drawn from just a single standard normal, one component. This is a histogram of the fitted model size. So, typically 9, 8, or 10, definitely not 10,000. In fact, there was a conjecture that based on this type of experiment maybe five years ago that suggests maybe typical is Roupen, but what we showed is actually logarithmic. So here is a result. And I'll see if I can hint at maybe some of the proofs. Here's one result which has this dimension-free flavor. It controls, by the way, It controls, by the way, a deterministic guarantee, the size of the solution by the range of the data squared, regardless of the number of data points. So it says within some absolute constant, the solution to this optimization problem has a support size at most maximum data point subtract, minimum data point squared. And this squared is best possible. So therefore, if you apply So, therefore, if you apply it to a random situation where data is drawn from some Gaussian mixture, yes, there was a question? Yeah, something is real. I mean, on the left-hand side, you have an integer, on the right-hand side, you have distances. How does this scale I don't understand? Yeah, good question. So, this is the integer. This quantity has units, if that's what you mean. If that's what you mean. Here we assume that the standard deviation is equal to one. If it's not, you would divide by sigma squared, which will be, you know, which will. Yeah, okay. Thanks for the question. This is great. Yeah, so therefore, if things are random, this range is, you know, under sub-Gaussian conditions, it's at most the root root log n. So you get log n number of components with high probability. And that is the main result. And that is the main result in this case. And the result extends beyond Gaussian mixtures. But I am only going to discuss mixture of Gaussians. And so this provides some answer to the question I raised early on. If you have a three-component Gaussian mixture that generates n data points and you run this documentation, it will fit typically log n. We don't have a lower bound. I cannot tell you that it will genuinely overfit by. You that it will genuinely overfit by a log factor, but maybe that's the price to pay being so flexible. So there is maybe a little bit of overfitting. And this universality, there is some universality, this log n. So it's not totally attributed to Gaussian tail. You might think if the tail is fatter, then you will get maybe log n to some power, but it's somehow it's always log n. And this log n is also optimal. Within these confines of this theorem, it's impossible to replace it by something smaller. And I'm going to skip this. So maybe let me just end this part with explaining the name of the talk. We call this type of property self-regularization because there is no explicit model selection that forces it to choose its spark. Choose a sparse type of solution, right? So automatically, the solution is sparse, and actually, automatically, the model size is at the right level. So there's similar behavior of this in previous infinite dimensional maximization problem in maximum likelihood for shape constraint models, in particular the Grander estimator. And then if you're interested. Estimator. And then, if you're interested, you can read the paper to discuss some ramifications along this line. So, let me end with the last five minutes, maybe just to give you a hint of how this proved. The proof starts with the same premise of looking at the first order optimality condition, which counts the number of modes of a Gaussian a different way. The key results is here. So, I'm still going to rely on the fact that the Rely on the fact that this gradient is Gaussian convolution with some measure, right? This measure is re-weighted empirical distribution, but supported on the data set, which is contained in this range. So if I can control a measure supported a bounded interval with a Gaussian and count how many moles it have, and then I can control how many critical points it have. Points it have, and therefore, how many atoms in the maximizer. So this is the puzzle measure supported between plus minus A. Think about A being large. Convolved with standard normal, how many moles can it have, right? So clearly you can have A, right? If I have uniformly over a grid of size separation, let's say 100, then you can create A also. Then you can create A observations, right? So that's what this picture says. So this is everybody knows. But perhaps surprisingly, you can create A square mode. So you can have very little separation, but somehow have a lot of oscillations. So this is what we showed at most a square, and this A square is tight. And this can And this can be achieved by some sinusoids construction. And there is a more simpler construction by a different paper around that time. And I should also say this is also this type of results counting the modes of mixture of Gaussians also showed up in some other information theoretic problems. Okay, so the main tool for doing this is some complex analysis techniques, which allow Techniques, which allows you to count zeros of an analytic function in a given range. So, here we are going to count the derivative of this Gaussian convolution, which is analytic. We're going to count its roots within some complex disk, which in particular includes the real interval. If you can show all the roots are in some predetermined interval, which is easy, then you are done. So, there is the main. Done. So there is the main tool is there are many different versions of this. One of them is called Jensen's formula, which says if you have an analytic function g and you are interested how many zeros it have within a small disk R, you only need two pieces of information. One is the maximum of this G on a bigger disk at the radius big R and some lower bounds on the function at the center. At the center. And the logarithm of this ratio or the maximum and the center value basically tells you how many zeros there is in the entirety of this disk. So that's what we're going to apply. Yeah, so there is some work involved choosing the right radius, choosing the right function, and a square, right? And a square, right? It might look a bit mysterious. It's coming from the fact that a Gaussian on a disk of radius r can be at least e to the r square. And on the on some point you carefully chosen is at least e to the minus r square. So you have logarithm of e to the r square. r will be chosen to be on par with a. So you get a square in the end. So, it's a very short proof. You can read the paper if you're interested, but I think the main technology I already mentioned is based on Jensen's formula. And the proof for other exponential families is more involved. So, maybe let me use the last two minutes just to highlight some open problems. Yeah, so there are things that are out of the reach of the current techniques. For instance, the second paper on this topic. Second paper on this topic about mixture of exponentials, we have no idea whether the same results hold or not, right? So, and this Jensen's formula type of techniques doesn't work well, give you useless result. Yeah, perhaps more importantly, and let me just end on this point, is that there's very little that people know what happens even in two dimensions. So, there is the theoretical part and there is the The theoretical part, and there is the algorithmic part. On the theoretical part, even basic properties like whether the solution is unique is not known because I didn't tell you how to prove the uniqueness. It goes hand in hand of counting how many solutions there is. The difficulty is that in two dimensions, if you have n Gaussians, you can shift them and re-weight them any way you like. You can actually create. Way you like, you can actually create these square modes as opposed to d. And this was crucial to prove the results for one dimensions, which fails in two. And second thing is that there is a positive probability, right? At least for some models, this solution is not even unique. But it doesn't prevent you from studying its typical behavior. And the last thing I want to mention. The last thing I want to mention is that all the algorithms right now don't scale at all with dimension. It will depend one way or the other exponentially on the dimension. So there's just basically no eligible solvers for this, even in three or four dimensions. It's going to be painfully slow. And there's a lot of needs and Needs and recent interest in computing these types of things, mainly coming from empirical base and empirical-based PCA to compute these type of solutions for in multiple dimensions. But we don't have a good algorithm. And also the optimization property, as I said. Yeah, let me just end the talk here with the references. Thank you very much. Questions? I have a question. So your main lemma in dimension two, do you have, have you thought about any construction? Like, you know, if I have a square A by A, you know, how many balls can you create? We don't have such a result. Yeah, so that will be the that that if that result is true. If that result is true, there is actually a lower bound. So, the paper I mentioned that gave a non-sinusoid construction did give a lower bound for any dimension, which is the 2D. A to the 2D. A to the 2D. Yeah. I've yeah, this one. Yeah. So because there. So, because their construction for one dimensions naturally lends itself to d dimensions. And if that is true, that is tight, then it will show you that in two dimensions, you will have at most the log square component, which is compatible with what we know about this statistical property. So, I would wager that is probably true, but it seems very hard to show. You can then choose a complex analysis and one of the couple analysis, it doesn't work very well. About coupler analysis doesn't work very well in two dimensions, whatever that means. It's more about just setting up the right, yeah. It's like step zero is not doing very well, I would say. But I think it's one of the most exciting questions in this field. Just what is step zero to you? Just to count all these to count like zeros of the derivative? Is that step zero to you? Yes, but then Yes, but then these extensions to the complex domain, let's say two dimensions, I don't even know what it means. And then you are looking at a system of equations, right? So the gradient give you two equations in two dimensions. I'm not entirely sure how to proceed from there. But there are certain parts which is true. For example, you can localize the roots in some, the real roots, the relevant roots in some. Puts the relevant rules in some box. So that's fine. Yeah. So, and in fact, a simpler problem is for the Poisson model, where it's a polynomial system. But of two variables, even that is very hard. I don't think anyone knows how to analyze it. Thanks again. And if you can unshare your screen, our next speaker is Eric. Can you hear us? I think you're muted. Yeah, I can hear you. Can you hear me, guys? Can you hear me? Okay, can you see the screen now? Can you see the screen now? Yes. Okay. So I'm going to talk about estimation of distances and application.