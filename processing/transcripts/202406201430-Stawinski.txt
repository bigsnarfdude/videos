So right. So continuing the afternoon session is Sven Stalinski, who is going to tell us about ultra-elliptic polylogarithms for fine-wheeled integrals. Thank you very much. So also let me start by thanking the organizers for putting together this very nice workshop in this amazing place and giving In this amazing place, and giving me the opportunity to speak. And yeah, if I'm not talking loud enough, please stop me. So, indeed, yeah, I will be talking about some work in progress with Claude Durr and Francisca, who is also here, where we're trying to generalize elliptic poly logarithms to hyper elliptic curves in the context of finite integrals. So just as a brief motivation, where if we want to study some quantum field theory observable, say a scattering amplitude or so, it is a natural question to ask what type of functions do we need to express the answer? Do we need to express the answer? And just by looking at the basic ingredients of quantum field theory, we can sort of already get a hint that we need some intricated branch construction. The answer cannot be that simple. And we expect some logarithmic singularities. So it's sort of natural to expect some type of generalized logarithms. We now take some pragmatic view at this and say, okay, perturbation theory of perturbative point of field theory, well, the basic ingredients are Feynman integrals. And in Feynman parameterization, these are nothing but integrals over rational functions. integrals over rational functions. So by studying integration of rational functions, we should sort of naturally be led to this function space that we are looking for. So let us do this. Well, okay, everybody knows how to integrate a rational function. We just partial fraction and then everything is elementary, except possibly for one integral, which we would give a new name, which is the logarithm. So we already see that we need to introduce a new translator function to do this. If we now take multiple integrals, then we just iterate this procedure and we are naturally led to some definition of iteration. Naturally, it led to some definition of iterated integrals with this non-elementary integral appearing as an integration kernel. And this is the definition of multiple polyorgons that you've probably all seen before. Of course, this story is not as simple as I just put it. Namely, there is a very obvious obstruction to this. If we take some simple example, this is clearly a rational integral, then, well, the integral or the answer might involve some algebraic square root, for example. So here, for example, we find a logarithm and then some square root involves. And then some square root involving a fourth-order polynomial. So if you want now to take another integral in x, well, we are obstructed by the square root. And so we could, well, call it a day, or we could take a different viewpoint, which has turned out to be very fruitful in the past. Namely, we introduce a new variable y, which is just sort of defined to be the square root. And then we have a new function, which again looks rational or looks like it derives from the integration of rational functions, now in two variables related by this constraint. So this constraint defines. So, this constraint defines some algebraic variety, in this case, an elliptic curve. And so, we could say we take a different viewpoint and still consider integration of rational functions. However, now on non-trivial geometries, in this case, an elliptic curve. And indeed, this viewpoint has been taken in the past. And not only the elliptic curve has shown to play a role, but also higher-dimensional varieties, as we now saw by Saura. And also, up to debate, as I now learned, higher genus Riemann surfaces. For these, there is actually already an existing polygonal construction by Olly and collaborators that we heard about on Monday. Also, on, I think also on Monday on the archive, there appeared a paper by the Zurich group, which are also taking a different approach to this. And this talk, I again, yet again, want to take a different view on this, not coming sort of from the string theory side as these two papers were, but sort of from the final integral perspective. So just to flash. So, just to flash the definition of hyper elliptic curves that we will be using in this talk, they're just defined by this simple polynomial equation, which is clearly an obvious generalization of elliptic curves, where g just would be one and we would find a fourth-order polynomial on the right-hand side. If we look at this geometrically, well, for almost every value of x, we have two values of y, so we should think of two sheets, which are then glued at the branch point, branch cuts between the branch points, which are here called lambda i. If we then glue this together, we are. If we then glue this together, we are naturally led to something like a torus, but not with one, but with G holes. This geometry has a non-trivial homology, so we can find non-contractable cycles, actually two G linear independent ones. We will sort of classify them into two cases, the A cycles and the B cycles. The A cycles just encircle one of the branch cuts, while the B cycles start on one branch cut, go to a different one, change the sheet, and then come back. Okay, let us come back. So, okay, let us come back to our original objective of integrating rational functions, but now refined to live on hyper elliptic curves. So, we will start with some rational function of two variables, x and y, which are connected by this hyper elliptic constraint. And we can actually use this constraint to fully fix the y dependence, just to be this 1 over y or the y just being absent. Well, if the y is absent, then this is precisely the story that I told at the beginning, so we can just ignore this part. And so we can focus on the one where there's some rational function of x times 1 over y. Rational function of x times 1 over y. And so we now play the same game as before, we just partial fraction. However, now none of the integrals are elementary, of course, because of the appearance of the square root. However, also not all of them are independent. We can take IBP identities to reduce them to a minimal set of what in physics language we would call master integrals. And so we can choose the spaces to involve these 2g plus 1 integrals of the form x to the k over y and this integral with the pole at x equal to c where c would be some space. At x equals c, where c would be some special point on the hyper elliptic curve, but not a branch point. If it's a branch point, we can reduce to the first family of integrals. So, again, these are sort of new transcendental functions that we need to express some one-fold integral over a rational function on a hyperliptic curve. Actually, these integrals, or rather, the integrands, the differentials, are not something random, but something that mathematicians have known for some time and classified it to three different kinds. They are, first of all, holomorphic differentials. Well, the name says it. Differentials, well, the name says it. They don't have poles on any point on the Riemann surface. And in the case of the bases that we chose before, they're precisely the first g of the first kind. So the forms dx over y until x to the g minus 1 over. There are also differentials of the third kind. These are not holomorphic, but rather meromorphic, so they have poles. In particular, they have non-zero residues. The examples of the type above are from the left side, we take k equal to Or from the left side, we take k equal to g, then we find a simple pole at infinity with one zero residue. Or this clearly has a residue at x equal to c. The remaining forms have higher order poles, so they're almost second-kind differentials, which are meromorphic but do not have a residue, so in particular need a higher order pole. And so to get true second-kind differentials, we just subtract the residue. So we define some polynomials phi i, which are just this x to the k minus the residue. So we find true second-kind forms. Second kind forms. So we could now take these master integrals or reveal in differentials and play the same game as before. We put them into iterated integral kernels and then define a function space of iterated integrals that we could call hyperelliptic multiple polyrodic. So in principle, we could call it a day here. In principle, we are done. We have some function space with which we can integrate rational functions on hyper elliptic curves. We could now start trying to compute hyper elliptic Pynon integrals. However, already at genus one, we know. However, already at genus one, we know that the story is not so nice. So, if we specialize to genus one, we would actually be led to the definition of before elliptical logarithms defined by Claude and collaborators around seven years ago. And there are some problems with this function space that we know about. First of all, at the start of the talk, I said that we would like logarithmic singularities just by basic principles. And indeed, these E4 functions would have logarithmic singularities in this x variable, which arises from integration. But it also depends on some parameters. But it also depends on some parameters, which would, for example, be special points in the poles of these functions or in the points of the integration kernels. And in terms of these parameters, actually, we do not have logarithmic singularities, but can have simple poles. In this case, for example, we have a simple pole at infinity. However, the kinematics is encoded both in this x and in the c variable. And so this is something that we would not like to express our answer. Another problem is if we take some degeneration limit of the hybrid or the elliptic curve. limit of the elliptic curve. So we take some roots to coincide, then, well, we expect our elliptic polylogarithms to reduce to some nice combination of multiple polylogarithms. Indeed, this happens. We find polylogarithms as we expect, but the coefficients are not something nice or rational, but some algebraic functions of the kinematics. So in this sense, we would call this, this is not a pure combination, and this is something that we do not like. So these two problems are something that we do not like about this function. Are something that we do not like about this function space, so we should look for a different basis for this function space. At genus 1, we know how to do this. Namely, it's very helpful or fruitful to change the description of the elliptic curve. So far, we've been talking about the elliptic curve as two variables connected by this algebraic constraint. However, there is also, as you probably know, this description in terms of the torus. So a complex plane modeled out by a two-dimensional integer lattice described by some complex number called the module parameter. The map from the algebraic side to the geometric side is very simple. We just take the unique holomorphic differential, integrated from some base point to the point on the elliptic curve we're interested in, and the sign decides the branch of the original point. So this integral is a very natural map to this torus, also called Jacobian in the higher dimensional context. And we can use it to put coordinates on the elliptic curve. The natural functions that live on this torus are so-called theta functions. Are so-called theta functions. They're defined by this very simple series representation over an exponential, and they depend on the geometry of the elliptic curve through this parameter tau. They depend on some variable z, and also on some parameters which are called characteristics, and sort of decide if the theta functions odd or even in this argument z. So we can take these theta functions and use them to define integration kernels by writing down a generating series. Explicitly, this just gives us these integration kernels. Explicitly, this just gives us these integration kernels as rational combinations of derivatives of theta functions. And we can then put these integration kernels into a definition of iterative integrals and find a new definition of elliptic polygons. From what I've said, it is, of course, not obvious that this is the same function space as before, but one can actually show that this is the case and that actually this basis for the function space is a lot nicer, in particular solves the problems that I showed before. So this is what we would like to generalize to hygienists, not the E4 elliptic foil log. Hyogenous, not the E4 elliptic foil origins. So, how could this generalize to hierogenes? Well, first of all, there is a generalization of this torus picture. It is called the Jacobian, as I already said. And well, it's nothing else but a higher dimensional torus. So we take G copies of the complex plane and mod it out by a G-dimensional version of this integer lattice, now described by a complex matrix, a G by G matrix. It is symmetric and has positive definite imaginary part, and you could call it period. Part, and we could call it period matrix. And well, it is clear that this is now a higher-dimensional variety of dimension G, while the hyper elliptic curve still has complex dimension one. And so this is the maybe the biggest novelty at higher gene is that the Jacobian is something different than the hyper elliptic curve itself. However, this Abels map that we already saw before still exists and is now an embedding of the hyper elliptic curve into the Jacobian. Namely, we take now the G holomorphic differentials that we have and integrate. Morphic differentials that we have and integrate them from some base point to some point on the hyper elliptic curve. And we naturally are led to something that lives in this higher-dimensional torus or Jacobi. Again, the natural functions on this space are theta functions with the very obvious generalization of the one-dimensional theta functions. Now everything will just be lifted to g vectors or g by g matrices. In particular, also these characteristics determining the parity of the theta functions are Parity of the theta functions are lifted to g vectors in the value zero and one half. So I just want to stress again that really the main novelty here at higher genus is that the Jacobian is something different from the hyper elliptic curve itself. And so while we can still use it as, for example, the space that the theta functions live in, we cannot use it to give coordinates to the hyper elliptic curve. And we need a different way to do this. In practice, we do this through Schottky uniformization, which is, I guess, a very stable. Which is, I guess, a very standard method in that business. But I don't have time to talk about it. So if you have questions about that, please ask. So now that we have started with the sort of algebraic curve description of things and defined some abelian differentials sort of as master integrals for integrating rational functions, it is now, of course, the question, how can I move all of that, what I have seen, to the geometric side? And instead of boring you with some technical details, I just want to illustrate this on a very simple. I just want to illustrate this on a very simple example of this third kind differential, which just has a pole at some point C. Clearly, this has poles at the points x equal to c and the two branches of the elliptic curve, or hyper elliptic curve, with residues plus and minus one. This is ensured by this numerator factor. And so the first step is to just mimic this analytic structure by some function expressed in terms of beta functions. And actually, we don't have to do anything here. And actually, we don't have to do anything here because the mathematicians have already defined these things and called them normalized abelian differentials. So, what we are looking for is just the normalized abelian differential of the third kind, which is defined by this d log of a rational of a fraction of theta function. By normalization here is meant that it has poles at w1 and w2 with residues plus and minus 1, and it will integrate to zero on all of the a cycles. So, if we take this and integrate it around any of the branch cuts, we will find zero. Any of the branch cuts, we will find zero. Okay, so this clearly has the same analytic structure as what we saw on the algebraic curve. So we can just equate the two things. Well, almost. The singularities already match. So the only difference can be something holomorphic. And so we can express it in our basis of G holomorphic differentials. We can actually fix these coefficients now using the normalization of this third kind differential by just integrating both sides over the A periods, and we can find Over the A periods, and we can find a complete expression for these coefficients. We can play the same game for all other abelian differentials that I showed you in a very similar way. There are only minor technical details that one might need to consider. But in practice, this means that we can express all of the differentials that we saw before in terms of theta functions on this geometric side. Okay, so now we would like to start building some kernels as we saw in genus 1. However, here we don't really have a top-down view on this as we already had a Have a top-down view on this as we already had at Genus 1, where we had this generating function, but rather we will take some pragmatic view because, in concrete computations, we never need this whole tower of integration kernels, but mostly just maybe G1 and G2, maybe even G3, but not much more than that. And so we will just take a very pragmatic view and just define sort of the first couple of weights of kernels that we need and see what is a good definition for them. So the first thing is, well, maybe obvious, the holomorphic differentials, we certainly need those. The holomorphic differentials, we certainly need those. So let us put these into our integration kernels. At weight one, we know that at genus one, the weight one kernel is the one with the pole. So it's a generalization of the d log and explicitly looks like this. It's just the d log of a theta function. And now the interesting thing is that there are sort of two ways of generalizing this to higher genus. We could interpret this derivative as a derivative with respect to z or with respect to w. Or with respect to w. This is now different because we do not have translation invariants anymore. So the theta functions don't depend on z minus w, but on z and w separately. And so this would lead us to two sort of natural ways of generalizing this genus one kernel. And a priori, it seems like we would need both. So the question now, of course, is how what about higher weights? And again, we will take a pragmatic view at this. And so how higher weight kernels appear in practice is through products of lower weight kernels. Products of lower weight kernels and then using the Fay identity. Again, we don't have a top-down view on this, so we don't have a top-down view on Fay identities. But we will just use the fact that we want our higher weight kernels to be free of poles. So what we will do is we will take some weight one kernels, for example, these two different kernels that we found here. We will match all of the singularities using the weight one kernels. So the rest has no poles anymore and should somehow be expressible in some higher weight kernels. Now, of course, there's a large freedom in how to do this. There is a large freedom in how to do this, and the question bears the question: what is the correct way to do it? And well, the only way to decide this is to find some examples, to generate some theoretical data, and see what is better than what else. And so our guiding principle will be, well, the non-planar double box, which is already studied in these examples here, which is, however, as a full integral, still a bit too complicated for what we're trying to do. And so we will take the maximum cut of this, looking somewhat like this. Looking somewhat like this. Actually, we can generalize this a bit to what is called Laurie-Chella function, which is a hybrid geometric function, which has an integral representation of which this maximal cut is a special case. And so clearly these Lauricella functions, we can expand in epsilon and at any order in epsilon, we can express them through any basis of hyperliptic polylogarithms. And then we can try to decide from what comes out what is a good basis and what is not. Good basis and what is not. So, what is our guiding principle to decide this? Well, to this end, let me take one step back and quickly review the concept of purity. So, at genus zero, if we compute some final integral in terms of polylogons, then the general form looks something like this. We have some algebraic prefactor and some rational combination of polylogens, some pure combination of polylogens. And this leading singularity, as this prefactor is called, can be accessed by just taking a Is called can be accessed by just taking a maximal codimension residue of the original integral. So by changing its integration contour just to a torus. And so clearly, dividing by this leading singularity does something nice to the integral, namely it makes it pure. So the question is, how does this generalize to hydrogenous? Well, at genus one, the problem is, of course, we cannot fully localize the integral. So if we take enough residues, then at some point we will be left with some differential form on the elliptic curve, which might not have more. Elliptic curve, which might not have multiple poles. And so, well, the question is, what to do now? Well, we can take an agnostic view at this and say, okay, there are two independent contours on the elliptic curve, namely just the A and the B cycle that we saw before. So let us just take both. And if you look at it, for example, from the perspective of differential equations, as, for example, Sarah just also mentioned, one shouldn't think of an elliptic integral as a single entity, but rather as a two by two block or two iterate, two elliptic integral integral. Two elliptic integrals at a time. And so, actually, we should also always look at two integrands. And so, it is a very natural thing to just look at this two by two matrix called period matrix. And so, this seems like a very natural generalization of the leading singularity to higher genus, or in this case, genus one. However, if we now do a concrete computation, again, turning to some hypergeometric toy model, in this case, the Gauss hypergeometric function, then what we will see is that if we take some basis of integrands and take some Basis of integrands and take some contour and compute these integrals, then we will see the rotation that brings us into some pure basis, so into some rational combination of elliptic polylogarithms, looks like this. And this is not the period matrix of the elliptic curve, but almost. So if we would take the period matrix of the elliptic curve, we could split it in a way, as Sarah already mentioned, into a semi-simple and unipotent part, where the unipotent part is supposed to satisfy a unipotent differential equation. equation so the matrix defining the differential equation is supposed to be nilpotent so go to zero if we take its power often enough and the rest is supposed to be the semi-simple part and this semi-simple part of the period matrix seems to be the correct generalization of leading singularity because if we sort of divide this two vector by this two by two semi-simple part of the period matrix we find a pure basis. Not only that, we actually find that the master integrals in this pure basis are related very simply. So one can Related very simply. So one can a bit simplify it, actually read off the second master integral from the first one. And so this is very nice, but this is a toy example. So the question bears, does this generalize? And at genus one, there is a lot of evidence by the Munich group. They have done this for a single elliptic curve, for multiple elliptic curves for K3 surfaces, as Sarah mentioned in more detail. And we would like to understand if this could also generalize to higher genes. But here we do not have a set. But here we do not have a set of kernels yet, so we will try to fix the kernels by using this as a guiding principle. So we will study this hypergeometric pole model that I introduced, this Laurie-Teller function, and try to see this vectorization or define the kernels in such a way that this vectorization occurs. And we see these simple relations between the then outcoming pure master integrals. And so this is still work in progress. So this naturally leads to me to my conclusions. My conclusions. So I have sort of shown you or tried to motivate that we should study integration of rational functions on non-trivial geometries to find function spaces interesting for finite integrals. And just from a very simple IBPP analysis, we can get one set of integration kernels defining the function space we are looking for. However, this function space is not very nice. It has various problems that I described. And so the solution seems to be to change to a geometric description of this function space. In this case, using the Jacobian as a In this case, using the Jacobian as a generalization of the torus and the Schottky uniformization. And now on this geometric side, what we are trying to do at the moment and what is work in progress is we're trying to build a set of kernels that are a nice basis for this function space in the sense that we see some factorization of the period matrix, which should be a generalization of what we see at genus one and should lead us to sort of a procedure of finding epsilon factorized differential equations, for example, or just Differential equations, for example, or just bringing fine integrals into a pure base. So, yeah, with that, let me thank you for your attention. Okay, thank you. That's a great talk. So, questions? Can you relate the G1 and G1 Twiddle kernels to components of the Enriquez connection? With the Enriquez connection, we have not. With the Enrique's connection, we have not tried to match, partially because I find the Enrique's paper very difficult to read. And yeah, so that we have not tried. So this is a very naive question. But my understanding from discussing with other people is that the shot key uniformization maybe got more difficult beyond genus two. Is that Is that I don't I've been told this, I don't know why, but do you know anything about that? At least theoretically, I don't see any reason why it should be more complicated. It might be numerically more demanding, sure, but I don't see a problem with it. I mean, what is maybe a problem about the hyper elliptic curves? That at higher genus, there are so genus higher than one, genus higher than two, there are Riemann surfaces which are not hyperliptic curves. So, hyper elliptic curves are actually a Elliptic curves are actually a restriction, but the Schottkin neutralization itself, I don't see your problem. Others? So you say that basically studying this non-planner double box, essentially studying the Lower Chella version. Yeah. I mean, the maximum cut. Yeah. Yeah. Yeah, yeah. Then you know that the Laurie Shell and the Laurie Shell function, we have hyper functions, right? Yes, so if we expect if we twice well, what's the debate about that? Well, I guess the debate about if one is allowed, I mean, I haven't been able to have the suspension. But then you're saying that you're studying this example, which is a five-minute zero value, the maximal gut. Valid firm in the maximal gut, and then you find the hyperlink functions. We so we can certainly express this in terms of hyperlink functions. I guess the debate is if we can also, if we need to. I mean, I haven't had the chance to discuss this yet. I'm very happy to discuss this if we have time, but this is my understanding, right? Okay. But but okay, but in a in a way, in a physical problem, we already have something that we need. To me, it seems like yes. Okay, other questions? But I just want to understand your eventual goal for this non-fetter integral when you said uniform basis, they all studied only on the cut, right? They all studied so far this only on the maximum cut. We also do. But eventually, your goal would be to. Eventually, your goal would be that would be nice, but that isn't the further future, of course. Yes. But yeah, sure. Yeah, and for the first kernels I saw, you gave fully explicit theta function representations. And then I saw a the type identity which defines what your G2 is going to be. Yeah. Can you iterate that to retain contact with theta functions to all steps of that? All steps of that? I mean, it certainly gets more difficult at higher weights because we don't have, because we would also like the, I don't know, G3 kernel not to have poles, so we don't have this matching of singularities anymore. But the question is if we even need a G3 kernel for what we're trying to do. So of course, in the end, a top-down view on this would be nice. But for just doing some examples, the first two ways might suffice. And okay, to the extent that you have data function formula, is it okay? Formula, is it okay to go beyond the hyper-elliptic divisor in that case? No, so we're really focusing on hyper elliptic curves here due to your priority list or due to some problem in going beyond hyperlipped. I mean, I guess just from the outset, this discussion we've had I mean, yeah, I could get I could say it due to our priorities, yeah. Okay, so yeah. We've we've decided to focus on this case, yeah. Decided to focus on this case, yeah. Okay, thank you. Um, more questions, okay. So, from mathematicians, can I ask, so class library, the accuracy is your guiding principle. Yes. Do you explain why you want this? Well, it seems to me, so my understanding of this is that sort of pure expressions of Feynman integrals, or if we have a fine integral and we can express this in a pure way, that is sort of the nicest. We can express this in a pure way that is sort of the nicest expression of the final integral. So, by purity, in the polygorithmic case, I just mean rational combinations of multiple polygorithms. And at higher genus, we would like to have a function space so that purity means rational combinations of these functions. And for example, purity is very closely connected to this epsilon factorization that we saw before. So, an epsilon factorized basis should produce pure combinations of whatever the function space is that comes out. But I thought that the reason I thought the project is background, so factorization you want to show the equations of distinct right combinations, right? Yeah. Okay, so but I'm just saying because you're saying rational combinations of these functions, and I understand that, but I mean, the example you gave, you only get sort of a, you could only get something algebraic about some particular, I don't know why. I don't know. Why is that more complicated than just rational? Well, I mean, the expression will also shrink in general. So it's not only that, I don't know, the expression has the same size, but then one has some algebraic stuff and the other doesn't. Usually the expression, if it's written in a pure way, it's a lot short. And that's good. Okay. If there are no other questions, thanks. 