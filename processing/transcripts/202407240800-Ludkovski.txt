We should get started. Today is a great pleasure to have Mike Bukowski from UC Santa Barbara. He'll continue the Minfield GAN series from the more application side. And for the lecture one, Mike is going to talk about the stochastic gain for energy investment. Thank you, Whitney. Hello, everybody. I'm Mike from From UC Santa Barbara, so down the coast, close to LA. So, this is actually my second time in Kelowna this summer. Me and Yang were here exactly a month ago. There was a workshop on Minfield Games. So, that's a nice coincidence. And I'm just coming here from the Power Engineering Society general meeting, which was in Seattle. Which was in Seattle, actually, still going today. Um, so I was hanging out with engineers for a couple of days. Um, so, um, Roxana was also at the workshop here in June. Um, and so my plan is to kind of complement what she was talking about the first two days. I looked at her slides, um, have a sense of what she talked about. Um, so I will try not to go too technical. Um, I'll try to focus more on applications. Um, so this is a So, this is the plan for today. I want to kind of review and reinforce some of the basic MFG ideas into games. And I'll talk about, I'll actually take a step back and talk about not mean fields, but just two players or M-player games. I think it's helpful to have some intuition. And then towards the end of this morning, I'll talk about a model about carbon taxes, which fits nicely, I think, with the theme of this. Which fits nicely, I think, with the theme of this summer school using mean field games for pollution and energy markets. And then afternoon, I will do, I'll do something similar to what Roxana was doing, which is basically go through a paper. And I'll go through one of my papers from a few years back on competitive. So it's not going to be mean fields, you know, only two players, capacity expansion. I wasn't exactly sure how the afternoons work, so I had. Afternoon's work, so I have I don't have a full plan for tomorrow, but I think I'll try to actually do something more hands-on computational that we can discuss during the break. Um, all right, so I'll use mostly my slides, I'll scribble a little bit on the slide, and I use my board, the board as well, occasionally. All right, so here's the here's a big picture where games, how games look like. Um, so here's a little matrix. Um, so So, the two things we can distinguish because essentially it's three axes, but the two axes are: are we talking about one period or dynamic versions? And are we talking about one agent or competition? Okay, so if I have one period, one agent, this optimization, if I have one agent and many periods, that's called optimal control. If I have one period and several agents, that's called. Period, then several agents, that's called a static game, and finally, the net games. So, I worked, um, I didn't work too much on optimization, but I worked on all the other things quite a bit over the last 20 years or so. And so I think, you know, that's kind of the starting point. And so the mean field game fits in here. I go from having several agents to having infinitely many agents. So at the third axis, how many agents I have? And generally speaking, we can talk about two. Generally speaking, we can talk about two. So, duopoly or competition of two agents, this is kind of nice and tractable because it's always just you versus me. So, I can, you know, I can directly think about who is doing what. With mplayers, it gets quite messy. And so, this is kind of the ugly child nobody wants to work on is mplayers. And then the mean field, essentially, we take a mathematical limit that simplifies. That simplifies a lot of the ideas. And so that's why people like this version as well. Okay. So just to go really basic, you know, if I do have several players and one period, okay, right, so the basic idea is that it has an action. Okay, since we have one period, we don't really have to state. We do, you know, this is kind of directly sort of agent identified with a state. Agent identified with a state. And then the ideas of a game is that some interaction. And so with n players, this means there's some kind of collective profile. So the vector means this is a vector of length n. So this goes a1 up to a n. Okay, and my cost is impacted by what the collection is doing together. Together. Okay, and so it's not clear. I mean, there's no natural way to explain what a clearing should mean. Okay, so this is like, I want to be clear, this is not a solved problem. This is a convenient mathematical formulation that works as a nice tool. But this is not a natural or justifiable thing in practice that this is what should be happening. But the one we, you know. But the one we use a lot for mathematical reasons is the concept of Nash equilibrium, which means that unilateral deviations are suboptimal. So once you know what everybody else is doing, so minus I means everybody except I, then nobody else, then player I does not want to change their actions. They like AI given A minus. Okay, so. Okay, so basic trouble you have is that, you know, if you define this constant, you say, is what can I say about the distance uniqueness? And the basic issue is that it's already with two players, things get somewhat complicated, but with the M players, it gets really messy. There's really no good theory. So if you think about in this setup, existence is generally ruled out. And so the only fix is to have mixed strategies. Is is to have mixed strategy, which means that somehow I'm going to toss a coin, and rather than picking one action, I'm going to look at my coin to decide what to do. Okay, so randomization is very good because that's a way to convexify the whole action space. You can make things sort of continuous, and then you can rely on implicit results from abstract math to show that there's going to be an existence. Okay, so under weak weak assumptions, once you learn. Are really weak assumptions once you allow mixed strategies, then you're going to have an existence of a Nash equilibrium. Okay, so essentially, that's this fact is why people like Nash equilibrium, because back in the 50s, Nash, when he came with this idea, he said, okay, how do I handle mplayers? And he basically was able to invoke, you know, again, very abstract fixed-point theorem to show the ultimate existence. Okay, now. Okay, now, you know, mixed strategies, especially both in one periods and in many periods, are very questionable if that makes sense practically. Okay, so this kind of construction is kind of weird in practice. First of all, second of all, the existence is abstract, so there's no good construction to actually get it. And so this is kind of So, this is kind of not very practically relevant, okay? But having a general existence result is very nice. So, people like Nash Club. Okay, so this is one period. If you want to go to multiple periods, it's the same idea exactly. Okay, the only difference is that I have a second thing to worry about. I have to have how periods change from one to another, so I have some kind of dynamics. Okay, so now I have two things I can interact with: I have a state. interact with I have a state and I have actions so I can interact either through my states or through the actions or both okay so that's kind of the biggest difference compared to one period is they have this um and obviously I have now more sophisticated you know ways to to be affected by the collective because you know it's not just about this period but maybe the future About this period, but maybe the future, so I have to kind of anticipate what might happen later. But the biggest effect is that you know, either it's through this state, which is kind of the memory of where we came from, or we would write it. Okay, so the mean field version is that if I think about two players and you versus me, so I have to really think very carefully what you're going to do to know how to act myself. If there is Myself, if there is many players, right, like us in this room, that becomes very, very tricky because if I have to try to think about everything that you each of you might do, I'll get a headache. So that's essentially intractable. Okay, and so the idea is that let's make ARVD irrelevant, right? So let's make the collection really big. That way, but to which player. Actually, really big, therefore, the impact of each player is negligible. Okay, so that's one advantage of taking the limit. The second advantage is that any randomization disappears, right? So, you know, if it's me and I'd have to lose a coin to decide to do A or B, then my coin toss makes a difference to what's going to happen. But if there is thousands like me, and well, each of them tosses a coin, the budget of large numbers, everything gets eroded out. Everything gets eroded out. And so you can directly say what the fraction of people are going to do A and the fraction of people are going to B. So this whole randomization disappears. We just take integrals and averages. Okay, so that simplifies this idea of randomization and the distinction mix and pure strategies goes away. Okay, so how does the mean field work? Two main ideas, homogeneity, average is the same, and anonymity, which means any one person doesn't make any difference. No, any one person doesn't make any difference, right? We're anonymous, we don't affect the collecting collective individually just by averaging, okay, and therefore we can ignore individuals that think about just representative ages, right? So one out of the crowd. Okay, and the one person has no influence. And so essentially, you fix the crowd, you think about how one person acts, and then you tie it back together. Go okay. Now, these two points are not something you want to do in applications, pretty much ever. There's very, very few applications where this actually makes sense. Okay, so there's two fixes for this issue. So the one fix is: if you don't want homogeneity, then you can just make everything a vector. Instead, instead of saying everything looks the same, you say there's two types. The same, you say there's two types: type A and type B. There's five types: types A, B, C, D, and E, and each type has a lot of people inside it. Um, but you know, I can go from everything the same and everything one of K types. Okay, so it just replaces everything by a background. Okay, makes the crowd bigger. Um, and so there's like a bigger crowd which has heterogeneous sub-crowds. Okay, so that's one the things for the first. The fix for the first issue, and then the fix for the anonymity. We can have what's called the major player, which is one special person that has actual inputs, and we actually keep talking to them. Okay, now the major player has to be essentially one because that means we're going to really distinguish them from everybody else. And because of that, you know, this is going back to having n finite, and so the only thing we're like, well, we can handle is maybe. What we can handle is major versus a crowd. So, two every else and the major player. So, we can go from the major players here. Okay, so the mean field game works by a fixed point. Okay, so mathematically, the whole theory of games generally and mean field games in particular is also. In particular, it is all kind of from fixed-point theorems. Hey, this is kind of the thing to keep in mind. So, you know, in a way, if you really want to study mutual games or again, games generally, then you should read up about fixed point theorems, which is a long topic in mathematics. Okay, so why a fixed point? Well, because I go back to this logic here. First, I study the behavioral currency of the player. Behavioral credit player. So that sounds like a one-agent problem because this is the person optimizing something. Okay. And then you say what they do is what the crowd does. So you connect, you say, given the crowd behavior, figure out what presentative agent does. And then you say, the crowd should behave. The crowd should behave like the representative agent. That's what representative means. So you connect it, their action, to what the population is doing or their status or population state. Okay, that's a fixed point that given the crowd behavior, the best response of the relevant agent matches what the crowd is doing. Okay, so in applications, and we'll see this thing pretty much exclusively. I think pretty much exclusively today and tomorrow and everything. And all the examples I'll have is the interaction is through the average behavior of the crowd. Okay, this is a natural interpretation what mean field means. It doesn't have to be, right? So essentially, with some little bit of extra complexity mathematically, you can have much more complicated interactions, not through Zimming. Okay, but for all examples, Okay, but for all examples, you know, the average behavior is what is the easy thing to think about, to justify, and to implement. Okay, so think me, yeah, so I'm going to get to an example in a second. Okay, so we can use it, I'll use the board so we can have a little illustration of what all the things I just mentioned are about. So it'll be about criminal model. This will be about Crunel model. Just before doing that, so I think this is going to be a preview for tomorrow. Right? So, why do you want to do mean fields? Because I told you, right, homogeneity and anonymity are not very practical. Okay, we don't really have the situation happening. Okay, the answer is because I actually want to know the end player problem, but I don't know how to handle it. But I don't know how to handle it. So I'll make an approximation. Okay, so this is a weird logical way to go about things: is to say, instead of thinking about n players, n players, 20 people here in this room, I'm going to think about infinitely many and use that answer to tell you what to do for the finitely many. Okay, so the mean field solution is in some sense optimal. In some sense, optimal, epsilon optimal, epsilon is roughly one over n for the m players. Okay, so it's kind of like a low-large numbers type type type comparison. Okay. Okay, so I think I'll talk more about this tomorrow. I'm not going to get time to get to this, but this is, you know, really helps for us for conceptual understanding what mean field practically means. Caption. How do you interpret the statement? interpret uh the statement epsilon uh optimal like one over n optimal so epsilon optimal means so epsilon equilibrium so this means that any deviation is going to be you know epsilon suboptimal right so epsilon nash equilibrium means that any deviation will give me most epsilon more so i'm not really i don't really care to do it Okay, so let me go in. So, this is a super old example, or at least it dates back to way, way back. So, Cournot was a French economist in the 19th century. So, this thing is like 150 years old. It's about as old as things get in games. So, here's the problem. So, this is it, and this is a nice setting for talking about energy production and accruity. Okay, so it's about non-cooperative competition. Non-corporate competition. FAF players or producers, and each producer has a cost. Okay, so think about I produce electricity from different sources. Okay, and so you know, I can produce electricity from natural gas, I can produce electricity from oil, I can produce through something else. So each will have different costs. Else. So each will have different costs. Okay, so first thing in this setting for now is these things are obviously heterogeneous, not the same. Everybody has their own, everybody's unique. Okay, so that's the kind of the distinction between the players. And then the question is very simple. We just decide how much to produce. Do I produce a lot or a little? Okay. Okay, now the way the way, why do I care about, how do I, why is it a game? Well, because the price depends on the collective production. Okay, so a good reference point, okay, not directly linked to electricity, but a good reference point is OPEC. OPEC is a cartel for oil. Okay, so you can think about players being countries. Players being countries. Okay, so Saudi Arabia, US, Mexico, Venezuela, so forth. Russia, each of them decide how much to produce. But the price is a function of their collective production of the sum. So, if a country produces more, then price goes down for everybody else. So, this is an externality. If I produce more, then everything else. Produce more, then everything else gets hurt. Right? So that's kind of a standard setting where, therefore, competition hurts our profits because we each try to think for ourselves, and then we don't think about the others who are affected by our actions. Okay, now, how does it look like? Well, it's very, the profit function is super simple. Okay, so this will be the point of this model is to have. The point of this model is to have things in closed form and very, very simple so we can see what's going on. Okay, so profit is quantity times price. Okay, sorry, quantity times profit. So here's quantity. Okay, now if we produce QI, how much do I, you know, collect, take home, right? I have a price, P, that's a price. Price, P, that's the price, P and I have a cost. It's super simple. Price minus cost times quantity. Okay, yes. If two pairs produce simultaneously, if the P will change by QI, Q1 and Q. Yes, I'll do this on the board in one minute. Once I get to the end of the slide, I'll do this on the board so it makes sense to you. Yes, but yes, this is very. This is a very simple model. So, right, this is a function of three things, ostensibly, costs, which are fixed. So this is a parameter, not a variable. Capital Q, this is the sum of all the QIs. And little QI, which is what I actually get to choose, right? Remember, what the players choose? QI. Okay, now QI shows up in two places. It shows up linearly here outside quantity, but it also shows inside here because I want to emphasize that the inside Q, there's a little QI. So I pull it out just so we can see it. So it's here and here. So I'll have to, you know, when I try to maximize, I have to think about the two impacts. Okay, so match limit are going to mean that I want to do the Want to do the best for me, okay, holding everything else the same, holding area the same, such that when I solve for Q star, then fixed point that's going to match the capital Q star. Right, so first I fix Q star and I do this, and then I check that the Q star, the Q little Q star. Star and the Q little Q star add up to the capital Q star. It's a fixed point. Okay, this is a little mean. Okay. So let me do this on the board right now for two players. And I'll talk about the mean field version in a second after that. Just aside, this is called the Cournot market. And the Cournot market, players pick quantity and price is given. So this is called inverse demand curve. More production, lower prices. Prices. Okay, there's a different model called Bertrand competition, which is the opposite where players choose prices and the quantity, the demand is a demand term, not inverse demand, but demand. So essentially, there's two things to think about, quantity and price, and you can decide which one you want to control. Okay, so Courna decides to control quantity. Okay, so let me try to do this two players. Do there's two players two-player Crunal. Okay, so I have player one, player two costs C1 and C2. Capital Q now is Q1 plus Q2. Okay, and I'm going to take the easiest possible inverse demand curve, one minus Q. Okay, now the U1 doesn't mean anything. Okay, what matters is that. Okay, what matters is that it's linear in capital Q. This is inverse demand, high quantity, lower price. So this is decreasing. That's good. Okay, so I want to have something decreasing and generally convex. So that's, you know, the linear thing will do it. Okay, so let's go back. Okay, so PQ is one minus Q. Okay, so what I try to do, so my profit is, right, is Qi times, okay, P is one minus, okay, capital Q. Capital Q is Q1 plus Q2. So this is Q. Let me write one here. Okay. Okay, this is P minus first player C1. Okay, so this is player one. Player two has the same thing, same problem, except everything is with twos. Okay. So, what am I supposed to do? To think about. Do to think about player one, I'm gonna first fix q2. Right, I don't get to choose what q2 is. I fix q2, I solve for q1 star. I do the same thing for the second player, and then I'm going to do a fixed point. I'm going to match it up so that I get the stars lining up to get capital Q star. Okay, so I want to maximize this fixing Q2. Maximize this fixing q2. So q2 is fixed, q1 is fixed. Find q1 star. Okay, try to do this. That's a quadratic function, right? That's the whole setup. Okay, so one theme for all the examples today and probably tomorrow, maybe everything is linear quadratic. So we can more or less do things by hand. Okay, here we really can't do it by hand. Okay, so it's a quadratic. So it's a quadratic in Q1 decreasing. So this is a maximum. That's good. Okay, so I can do it by hand. First order condition. Okay, so first order condition for Q1 star as a function of Q2. That's a key point. I'm going to think of this as a function of Q2. Okay, so if I differentiate this, right, for circumstances. If I differentiate this right for circulation, I'm going to have right, so if I think about this as a product, so I have one minus q1 minus plus q2 minus c1. So let me write this out so it's easier. So I have one minus q1 star minus q2 fixed minus c1. Okay, that's kind of from the taking graph q1, and then vegetation like this. Can you have q1 and then if it shaves back to this guy, I'm going to have minus q1 star. That whole thing equals to zero. Okay, so q1 star, so not equal, this is dot, so q1 star is I have one minus q2 minus c1 over 2. These are true examples. Okay, this is my optimized. This is my optimizer as a function of Q2. Okay, same thing here. Q2 star as a function of Q1 is the same thing, except symmetrically. Okay. Okay, and now I do the fixed point argument. I say that I want. say that I want this to be q2 star such that this thing matches up right so want you know plug in Q one star of Q two star equals to well, how do I want to write this? Well, so I'm going to basically substitute this thing in here and solve for Q1 star. Right, so I'm going to have. Right, so I'm going to have one minus q2. Q2 is here, so that's one minus q1 star minus C2 over two. That whole thing goes in there, minus C one over two. That should be Q one star. And I solve for Q one star. And I solve for Q1 star. That's a fixed point. I make sure that there's both on the side spy simultaneously with the same Q stars. Okay, so I solved for Q1 from here. Okay, so what am I going to get? So I have two Q1 star is there's a half here. There is is plus q one star over two plus c two over two minus uh did i this is fours right oh yeah sorry it's fine uh minus c one okay so i'll move this over this is three halves Halves that goes away. Okay, and so finally I can see the answer: Q1 star is one third, right? So multiply everything by two, divide by everything by three, one third. And what I have left, I have one minus two, and multiply it by two, two, C1 plus C2. C2 that's the fixed point, that's the Nash equilibrium, and therefore Q2 star is also right, everything is symmetric or anti-symmetric, so this can be one minus two C two plus C one. Is it okay? Okay, this is high school level math, hopefully. Okay. So you see again, right? So you see the two steps, first order conditions, then fixed point. Okay. So, okay, if you think. Okay, if you think about this whole thing, it makes sense, practically speaking. It says that if my costs, I'm player one, if my costs are higher, I'm going to produce less. Is this too low? You cannot see anything, or is it okay? Am I blocking everything from the laptop? Well, it's okay. So, if my costs are higher, then I'll produce less. Make sense. Makes sense. If your costs are higher, I'm going to use more. That makes sense. It also makes sense that I'm more sensitive to my costs than to your costs. There's a two here and there's a one there. So I'm twice as sensitive to my own cost and to your costs. Because I care when my costs directly affect my revenue, your costs indirectly affect me. So the externality is less important to me than my own costs. My own cost. Okay. Now, even in this simple model, there's already a scournau model has a nice little twist. Okay, everything did here, starting already from there. This is not okay because here, what kind of cues am I allowed to have? What do you think? What kind of cues should I have? User to have real value or something else. What kind of view ones should I have? This has to make sense economically. I'm producing something, I cannot produce anything else. That would be ridiculous. So it has to be positive. Okay, also okay, prices should not be negative because that would be weird. Okay, so it has to be at most one because of this one here. Okay, so p cannot be negative. Okay, why is negative p ridiculous? Well, because if p is negative, you're going to have negative revenue, right? Because you're going to have, this will be negative, this is negative, the whole thing is negative. And in which case, you can just say, well, I do nothing, right? You can just say, well, I do nothing. If I pick one to be zero, I get zero back. So, you know, I have an up, you know, the worst thing I can do is just do nothing. I cannot never end up with negative something. So this is up to one. So you can see that that can be binding. That can bind here because when I wrote this down, in principle, again, Q2 is given to me, so C1 is given to me. So, C1 is given to me, then this can easily be, you know, it's, it's, obviously, it doesn't have, right, because these are positives, but it can be negative. So this is only a candidate guess, assuming this is between, this is bigger than zero. So if it's not bigger than zero, then the answer should be q1 star is zero. Right, so essentially saying I also have the option to quit and do nothing. I also have the option to quit and do nothing if things look really bad. Right? So that's essentially like saying that if I have really high costs and you're producing a lot, then I have negative profit from doing anything. So I should just do nothing. Okay, now that creates an interesting effect. So again, at this point, Q2 is given to me. So, you know, it is what it is. But in the fixed point, right, I get to this, and that only works. And that only works if these two terms are both positive. Now you can see that they are sort of complementary of each other. So you can see that capital Q star, the equilibrium total production is Q1 star plus Q2 star. What's going to happen when I do this? What's going to happen when I do this? I'm going to have one-third, add these two together. I'm going to have two minus C1 minus C2. So that symmetries. Okay, so you can see now here, right, it's clear that because of this one minus Q there, so C1, C2 has to be 0 and 1 as well. Otherwise, it'll be negative. Well, otherwise, it will become negative immediately. So, you can see that this always looks okay because this is always C1 plus C2 is less than two. Each of C1 and C2 is less than one. So from this side, it looks okay, but it's certainly possible that one of those things will be negative. For example, if C2 is like 0.7 and this is 0.1, that's negative, which means this is. Which means this is this guess is a bad guess. Okay, now what happens in this situation? Well, if this doesn't work out, okay, so that's what I'm saying. You have to go into cases to see if the fixed point you try to find makes any sense, right? Because there's these extra constraints which are not checked when I did this fixed point construction. So if this thing is negative, that means that this is not the right answer. And the only other option I have. Answer and the only other option I have is that the second player should do nothing, quit, then q2 is zero. And if q2 is zero, I click I go back here and I put zero here and end up with q1 star is one minus c1 over two. Okay, which is the situation where there's no competition, the q2 is not there. And then if I have no q2, then this is exactly what we call the single-agent monopoly. I have one player, they have an inverse demand. They have an inverse demand, they do the best for themselves, don't care about competition because there's no competition. So, this is called a blockading effect. Blockading means that due to competitive disadvantage, C2 way bigger than C1, the second player in equilibrium is out and gives the whole market to the first player who becomes an up. Okay, so this is something like saying, you know, the example. Saying, you know, the example was a few years back in the oil market was that the Canadian oil sands were too expensive compared to, let's say, cheap oil from the Middle East. And therefore, there was no production of oil sands in Canada because of this effect. So essentially, if the Middle East is producing enough. Is producing enough, then the q1 star becomes zero. Okay, so this is the two-player version. Okay, and so now you can see that, right, why am I doing this example? Not because I want to talk about two-player, I want to illustrate the fixed point logic and the basic idea of how this construction works, but you can see it's very easy to generalize to n players, right? Right, replace this thing by a sum. I mean, everything essentially will carry through, so I can make this to be n players. In this linear construction, it looks exactly the same. I just have more cases about what can happen in the clearing. So the further conditions really don't care about how many players I have here. So it didn't matter if this was a It didn't matter if this was a right, so I could say now there's three players, and I could stick up Q3. Okay, what do I do with Q3? Nothing, nothing, right? I just keep fixing it. How does it affect this? Nothing. Nothing happens here. What will happen is when I get to the fixed point argument, I'll have to check that. Argument, I'll have to check that, you know, I'll have to plug in Q2 star and I'll have to plug in Q3 star, which will have the equation for Q3 down here. And then I have more cases because I have a case where, well, it's layer one, two, and three all producing positive, or maybe some of them are high, some of them aren't. So I have many more sub-cases to think through. Okay, so. Okay, so I can take this to infinitely many players. Okay, so then I'm going to have a big sum over J naught equals to I of Q J. Okay, now if you try to think about how this might look like, right, so if there is infinitely many players and they're all doing something, then this will be an infinite sum. Right, so it doesn't make, you know, that creates a mathematical inconvenience, let's say. Okay, so the logic is essentially try to put some kind of a rescaling here so that this doesn't blow up. Okay, so here's an example. So, this is a nice survey kind of paper in science review by Shannon Serker from about 10 years ago. So they had a trick. Okay, so it's, let me say two things. So it's a trick. So it's not completely natural. Okay. But it gives you nice answers. But it gives you nice answers, and there's some justification economically why this trick can be kind of accepted. Okay, so the trick is you have what they call infinity plus one player. The plus one is that I'm going to put what I do, but what they do here, they put a capital Q. Okay, and they have this model where this is. Where this is finite as before, and this is finite. Okay, and this Q is interpreted as the average, the average production every day else, average. Okay, so it's not the sum anymore, but the average. Okay, one way to explain why average makes sense is to think about some kind of differentiation. So my stuff is not exactly the same as every other stuff. Stock is not exactly the same as everybody else's stuff. So I don't care about their total sum, I care about their average, how similar it is to money. Okay, so that's what they do. Okay, and now they have one more, right? So you can see the same linear inverse demand. Okay, now they have infinitely many players. Now they do not want to make them completely homogeneous. Completely homogeneous. They make are denigable, but not homogeneous. They all have different costs. Remember, I had C1, C2. Now I have cost CX. X is the state, the type of the player. Now, X is in a continuum or in some state space which has a density of M representing how many players of height X are there. Okay, so M, so this. Okay, so this looks kind of like this. So this is X, and here's a density of M of X. So this is M of right, so this is so maybe this is like, you know, these are all the producers in the Middle East, and then this is US somewhere here in the middle, and then other. US somewhere here in the middle, and then to Canada somewhere there. Okay. Question? Is that similar to probability screen? Yeah, yeah. Okay, so the player, each player solves the same problem. Okay, sorry, so this is one small twist. The one small twist is this one more player. There's one more player. Let me see where this thing goes. Yes, there's one more player called the green player. I guess it's a special player. This is the other reason why there's a plus one. The special player has no X. It has essentially like sort of separate cost C0. This is the supposed to be the. Supposed to be the renewable player. Okay, maybe I think I think the slide actually doesn't show up too much word, it makes a difference. Um, okay, so the main point is the problem representative player solves the same as we had before, which is this, right? You see the same idea. Okay, now, okay, so there's um now the sum has three terms, okay? So the slight difference is that I have to have like a three-player version. Is that I have to have like a three-player version, so I have a player called X, okay, maximizing their Q production quantity. So here's a Q. There is everybody else, all the other fossil producers, and then there is, it should not have a star here. This is the green player. You have, should not have a star. Okay, this name is still quadratic and q as before, and the cost depends on x. So this is depend on x, and the class depends on x. The green player. Did I lose something, or it's okay? I have to check if I lost the term here. There should be. I lost the term here. There should be, I have to remember this 203, but anyway, this is it. It's doing over q hat. So q hat is the green player, um, q hat plus q, and there is no. Oh, yeah, it's okay. I think it's uh, yeah, I see what's going on. I'll come back to this when I get to here. So, this is the green player was basically the same, except there's uh two terms as three terms, okay, and then the Okay, and then the fixed point, okay, the argument is that the total production, the total production should be so mean field match equilibrium. Total production is the average of the Q stars from the first problem averaged according to the player types, according to the density L. Okay, so what is the Q here? The Q here is the average production of all the Is the average production of all the fossil players? The Q here is the average production of all the fossil players. What's the average production of fossil players? It's their Q start from this problem according to the density of the types M. Okay, so once I plug in this in here, I'm going to have a fixed point. Okay, so let's. Okay, so if I maximize this term here. If I maximize this term here in terms of q star x, this is a function of x because of c x term. I'm going to have as we had on the board, same thing here. I have one half one minus q minus the q hat. q hat is the green player version minus c x, which is matches exactly what I have here, same. Okay, so that's the q star for a So that's the Q star for a player of type X. Q hat star, this is the maximum for the green player from here. So that's one half. Again, one minus Q minus C0. So that is exactly the same as here again. Okay, now I take this and I plug it in here. Integrate and that should give me q which should match this q okay so let's see how that looks like when we do this on the blog let me try to merge this stuff here okay so q star x is over there one half there one half one minus q minus q hat star minus c o x okay now i want this to be a fixed point so this should be becoming a q star and q star is the integral of um q star x m dx. m dx. Okay, so how I'm going to make this equal to each other, I'm going to solve for what's happening, right? So I'm going to integrate this side to the span to the types. So if I integrate out q star x m d x, okay, if I integrate out to the right hand side Okay, let me move this way minus one half CX minus one half U star. Maybe something can go in there. Just integrate out both left hand side, right-hand side. Okay, so this is supposed to be to start. Okay, now here constant, constant, constant. These are all constants. So while I haven't integrated all this thing here respect to the distribution of the players, what do I get? One, right? Because this is a density function. So the integral is one. It's all constant with no x anywhere. So a constant with no x anywhere, so that just comes out. So I'm going to get back one half, one minus q star minus q half star. And now what happens here? This is one half. And it is what it is. This is the average cost, the average cost of all those players, the population. So this is the average population cost. Population cost. Right, so this is essentially saying in the mean field limit, the only thing that matters is no individual pairs become negligible, but I care about the average cost overall. That shows up. The average cost overall. Average production cost. Okay, so, right, Q star here, Q star there, I can solve for Q star. I do this, I get. I do this, I get this thing here. Okay, again, you can see that there's a Q star here and there's a half Q star. We always have three halves. Once you do three halves, we're going to have the one-third showing up, fraction, one-third. Okay, now I do this, and then I substitute again, one is q star. One is q star. This is q star. I plug this into here, right? To be able to, you know, because this looks like circular logic, right? What is q star? q star is a function of little q star. But little q star is a function of q star. So what's the point of doing that? Okay, so I'll slide it back in. You can see this has a half. You can see this has a half minus a half cube, and this is a third, so I'm going to have five-sixths. That's why I want to plug it by six and divide by five. We're going to have one-fifth showing up. All kind of cool fractions happening. So, this is one fifth. Okay, here's the one more one. The one one C0 stays around. And then this thing here gets modeled by a factor of two. So, here it is. This looks very similar to what I had on the thing that's erased. The thing that's erased, which had one plus. So I care about my cost, my population cost, double as I care about the green producer, which is the other player, the plus one. Okay, so this is the aggregate production equilibrium. And then once I have this, I plug this whole thing back in here to get Q star. You should get another one F. That's how this looks like from just subject. This looks like from just substituting this whole thing into q sorry into here and into there. So I have two things. Okay, so and again I have this issue that the green player, this whole thing are just guesses. So this only things make sense if both of those terms are positive. So depending on this integral in C0, this may or may not be positive. And so this can have that when the green player When the green player, this thing is too high, too high, then they don't do anything, and then this term disappears, and then I have a monopoly of only fossil features. Okay, but the main point I want to get through is this fixed point argument again. So I solve for q star x, which depends on Which depends on the aggregate. The aggregate is just the integral. And so I can integrate out and match things up to get what q star is, and then go back and substitute to get what little q star is. So I solve for the population and I plug it back in to get what the representative producers are. Okay, so this thing I think is a nice static model, right? Nice static model, right? No dynamics. There's several papers that looked how to make this dynamic. So if you see X, you can imagine this can be made dynamic somehow. So X can represent different things. X can represent the reserves of a producer. So reserves get depleted over time. And as you get fewer reserves, your costs go up. So that's a story about exhaustible resources. You can also think about this cost. You can also think about this cost as about talking about efficiency or technology. And so maybe you can improve your technology by making efforts, but trying to figure out a better way to produce energy. And so you can make of X to be a sort of technological progress. So you can put effort to reduce, to make X smaller, make CX smaller, and therefore, you know, yeah, more efficient and therefore make more profit. No profit. So, this is a starting point to make things dynamic by making apps to be a stochastic process. Okay, so 10 o'clock. Let me see how. Okay, so I think this is a little bit There. A little bit of a preview of what's, you know, for numerics. I think I want to do numerics next tomorrow afternoon. But here's a little summary. So this, the next couple of slides are, so Matthew Laurier has a bunch of very nice lecture notes on his website. So he has too many electronics. He has too many election notes. If you go to his website and you click on teaching, there's about six different mini courses. All the lecture notes are there. There's like, you know, 80-page summary papers. There is one set of slides. There's six sets of slides. There's eight pages of slides. There's too much. But this is anyway. This is based on his. I'm trying to use his notation. So here's the basic thing I want to get across. This is linear quadratic. This is linear quadratic, right? So, I just want to have a little some preview of how the structure looks like. Okay, so I think Roxanne talked about the general theory. I want to have, I think this might sound, this might be a review essentially what you already seen on Monday. Okay, so I think about everything's been linear, linear, quadratic. Okay, so I'll keep emphasizing the linear parts and the quadratic parts. So, here is the state variable, it's called x generically. Of x generically. So everything is generic, it doesn't have an immediate interpretation. This is the pay arrows called X. Okay, how does X change? It's a stochastic differential equation. It has constant volatility. Okay, so nothing happens with it. This is just some noise fluctuations. Okay, it has the drift has three terms. It has some linear effect of sort of momentum effect of where you are and where you're going. So this is. Are where you're going. So, this is this first term here. It's linear in X. It's linear in the average of RDL. So, mu bar represents the average x of the population. So, again, you can think of this as, for example, reserves or technology of producer eye, and then how the reserves change depends. How does it know the reserves change depends on what everybody else is doing on average? Linear. And then one third term, also linear, is my action, my control, my effort, VI. Okay, the objective function has five terms. It has five terms because this is trying to be general, and so in practice, most of those terms will be either zero or have some dashboard structure. Be either zero or have some special structure, but the possible things are everything is quadratic. Everything is quadratic. All the letters are constant. So this is a constant, constant, constant, constant, constant. Try the names, constant, constant. It's all constants. Okay, now the running cost. So what you pay between time zero and capital T is quadratic in where you are now. So essentially, this is. Are now so this is essentially this is saying you want to stay close to zero, you don't want to deviate too much, so you must, you know, control where you are, you don't want to deviate from everybody else, the crowd, you don't stay close to the crowd. Okay, so you don't stay close to zero, that's your target. You don't stay close to everybody else, you want to be on your own, second target. And third is you're lazy, you don't want to act, effort is. Act effort is hard, so any effort is quadratically parallels. So I ideally do nothing. Okay, so ideally, do nothing, so you don't control anything, and then you just drift her along, but you have to act because you want to be close to zero, so you don't like the shocks getting you away from zero, and you want to be close to the crowd, so you have to follow where the crowd is going. Okay, that's the writing costs. In the terminal cost, it should look the same, except that. Looks the same, except that this is over time. This is at the end. So, at the end, I want to be again close to zero, maybe with a stronger penalty, and I want to be close to everybody else with some other penalty. Okay, so with this setup, you can solve the problem explicitly. Now, here is the idea. Now, here is the idea. If I have a linear quadratic control problem or linear quadratic anything, okay, what should I be doing? Okay, so this is my expected cost. I don't like to use those terms. I want to make everything close to as close to zero as possible. Everything is quadratic. So you think this is all, of course, as an expectation. So this depends on where I'm right now. On where I'm right now, so it depends on x0. Okay, so because everything is quadratic here and linear there, your guess is that this whole expectation is a quadratic function of your state, which is called x. Okay, so that should be a quadratic function of x. So you make a guess that j i of x Of x is quadratic in x plus constants. And you make a guess that your action, your best action, should be what's the simplest format. How should your action depend on your state? Linear. The easiest way is to say, okay, so this is going to be a function of where you are. Okay, it's a function of where you are. And you want this to be linear in what you're doing, in where you are, because that's the simplest possible function you can think about. Fast as the in simplest. Sorry? Fast as in simplest. Best as the insect? No. So best is an optimality, but because everything is going to be quadratic, we're kind of what happened here means we're linear in. So not in here, because I don't have the, well, it's linear in C, C1 is like the X. Okay, so my action, because of the linear parallel structure, is going to be linear in my state. In case you make a guess at V V i t is something to do with xt linearly. Okay, so the way you solve this problem, you make a guess and you verify this is going to work out. Okay, so this is a slide from Lau Von Laurier's notes. And so here is the answer. So Told you, right? Action is linear in the state. Okay, I'll get there. And then here's the value function, the expected cost is quadratic in X. All the coefficients are time dependent. So I have now three coefficients, S, R, and P, but they're all time dependent. So we're going to satisfy ODEs. ODEs. Okay, I have one more, four ODEs to think about: one, two, and three. And the fourth one is here because, right, this whole problem was about, I want to be close to the crowds. I want to be doing what everybody else is doing or thinking that they're doing. And so I have to tell you about what mu bar t is. Now, again, at this point here, mu bar t is the average of the axis, averaged out over a huge population. average out over a huge population okay now at the when average is doing optimally at equilibrium optimally then mu bar is deterministic because all these noises i don't this is the randomness it's going to get erred out and so i can just track where the mean is going deterministically so this is a fourth Would E. Okay, so what is the structure you get? If you get these four equations, it looks a bit a lot, but that's the least you can get because you have to have at least three coefficients here. Okay, and plus you have to check the mean. Okay, so let's talk about what kind of equations do we get. This is the main point. This is the main point. This is kind of to illustrate the mean field logic. Okay, so first of all, the quadratic coefficient, okay, this is kind of a standard thing and anytime you do control, the quadratic coefficient is going to be a Riccati equation. What's a Riccati equation? Quadrat. Okay, so P autonomous, these are all called constants: A, B's, Q's, Q bars. B's, q's, q bars, these are all constants coming from this formulation here. They're all coming from there. So there's here the q's, the c's, the b's, the a's, it's all different coefficients. So the p equation is autonomous and has a p squared. Ricardo. Okay, well understood. Okay, now the p equation is retarded backwards in time. It starts with capital T and it goes back to, you know, I give you a terminal condition to solve for PT from zero to capital T. Back in time, that's why this is a minus zero. Okay, now everything else depends on P. Okay, so now. So now let's talk about the first equation. The first equation for Z is linear in P, which P is just solved. So if you first solve this, you get P. You can just plug it in here. Now, this looks like a linear D, because this is time-dependent coefficient times Z. That's not bad. Plus another linear term that has an R. Now, this thing here. Now, this thing here is a forward equation. You start from S Z0 and you go forward. Okay, now this equation is the one easiest to explain. What is Z? Z is supposed to be the localization mean. How do I get Z? Well, I have to start somewhere, right? This is supposed to be the average of the axis. The axis starts at some x 0 bar. That's the initial condition. Okay, so mu. Okay, so mu sorry, Zt is the expectation of X star T. And now here is the change of X. Okay, so if I take expectations, roughly speaking, of this equation, I'll get the change in Z. Z. The expectation I'm going to have an OD. Okay, so how does Z change? Well, the change in right, so I take expectations. I'm going to have the change of D, DZT, then DZ DZDT. That's what this is going to become. It's going to be A times Z. That's going to become Z. That's going to become z. That the expectation of x is z, so it's going to be a times z. A bar, this is z also. This will become the same terms. This is also z. Okay, plus b. What is v? V is my optimal action, which is here. Which is here minus B T X, this is X T that Z again, not the expectations, plus R, that's something else, divided by C. So look, the total, this is DZ is linear in Z A plus A bar, the first two terms we just talked about, minus B, sorry, plus B, right? So plus B V I. This guy here is exactly because exactly because b is right there so b d i is minus b squared over c b squared over c times p that's a linear term plus b squared over c times r here's the other term that's it so this equation is just take take take the expected value of this dynamics plug again the optimal z okay so this tells you how z Okay, so this tells you how the mean evolves. Let's go forward. Now, the bad news is it has P, which is here, it has R. And what is R? R is there. And R goes backwards. And it has Z. That's bad. That's a forward-backward coupling. So I have an ODE. So I have an ODE for Z, which goes forward and has R, and I have an ODE for R, which has Z and goes backwards. So you cannot solve this in a normal way. You have not seen this before. You would not know how to solve this immediately. Essentially, you know, sort of logically, you have to say, okay, let me try to like fix. Let me try to like fix R and do Z, and then try to plug in the Z in here and do R and iterate something like this. Okay, but they have two ODs which go in different directions and they couple each other. So that's quite nasty. The P is just there autonomously and the S, the S is a nuisance term. The S is once you have, that doesn't show up anywhere else. The S is once you have the R and the Z. The R and the Z, you plug in here, you get S, and it's actually, it has no S here, so this is just integral. S is just an integral of something. So the S is a boring equation. Just the integral of something to do with Z and something with R. So the main point is Riccati for P plus a forward, back, backward of these for Z and R. Okay, give me one minute, I'll stop on the next slide. Ones in the next slide. So, this was explaining how the allocations here look like. Now, the other thing to say is, okay, so what happens to how does my state look like, right? So, my representative agent state, x star, right, x5. X star, right? F I again X star stays here. This gets replaced. This is Z now. This was new bar before, the average. This is a Z from the ODE. And this is the P and Z R from the ODs again. So I have a, this is my equation for optimal state representative agent. State representative agent. It still has a sigma, still stochastic. And then finally, there is, I just want to sort out. I don't know sort of the details, but that Roxanne talked about this. So the forward-backward SDE version is you have to introduce another variable called y, which represents essentially the gradient of u. u as a value function from before from here. This is u. When you plug in x star for the x variable of the value function. Okay, and so that thing here is essentially an expectation. So that's why it looks like a backward SD. So the Z is just an auxiliary part of what comes from the BCD piece. Okay, nothing to do with the Z here. But I have the x and the y, and so the other way to solve this problem is to try to directly solve for x and y. So y depends on x. And yeah, so solving for y will give me another way to solve the mean field, linear quadratic mean field. Okay, so I'm going to stop here because I'm out of time. Since I'm out of time, the part I didn't get to is a whole carbon example. So I guess this is for tomorrow. So I'll do this in the afternoon, but I just want to be a quick, just a preview to what's coming tomorrow morning. I want to get this. Where you go. Okay, so this is that put on the Google Drive. Google Drive, there's two papers by Gilksha Dynikli and Matthew Laurier to talk about carbon taxes and applying the linear quadratic mean field to this problem, which is about emissions from fossil generation versus no emissions from renewable energy and trying to minimize those costs. And so the part I want to preview is that I'm going to go through its details in a minute. With details tomorrow, but what ends up being happening is that this other problem is that there's an interpretation of what the different terms mean, but it looks like this, which essentially looks like the same linear quadratic I had before. So there's more terms, but again, it has everything either squared or linear quadratic. Okay, except there's more variables. Same logic, the basically vector version of the before. Basically, the vector version of the before, and you end up with a result like this, where you see lots of equations, but I want to get to this one here. Okay, so I'll talk more in this detail tomorrow. But here's again, ODE systems where they look more letters with the same structure, Ricati and forward-backward coupling. Okay, so. Okay, so the reason I want to use linear quadratics right now is to give you a preview of how it's going to look like here, because I'm not going to go through this. I don't want to explain this again because it's just same idea, except more heavy notation. Okay, but that's so we'll see this linear quadratic structure in this carbon tax example tomorrow. Okay, and then afternoon, I'll go through this competitive. This comparative capacity expansion setting and my previous paper from 2018. All right. Let me stop. We'll do this. Well, I'm going to find the right spot. You got to here. All right. Questions from the audience or from people from the remote zoo? This thing's my explain very clear. Okay. Thank you. Thank you. Yeah, so now it's now it's copy break.