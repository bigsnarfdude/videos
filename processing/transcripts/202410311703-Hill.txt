Maybe out of the back here for the most pretentious title. Uh, thank you very much. Thank you for Mark for chairing. Thank you to the organizers. Thank you all for still being here. I'm very aware that this is the last session of today and I am the only thing stopping me from going to dinner. So I'm going to try and keep this light. I also don't really do water waves, and so that might be why it's slightly light, but we'll see. We'll see how I want to talk about his. What I want to talk about is some recent work that Mark and I have been doing, and how you can maybe extend this to actually studying water waves, and in particular, general 3D problems in cylindrical geometries. Again, like in Paul's talk earlier, when I say cylinders, I don't necessarily mean circular cylinders, but that'll be what most of my pictures are because they're easier to plot. Okay, do I use anything here? No. Okay. Do I use anything here? No. Okay, so axisymmetric waves, we've already seen a talk by Yoga today about axis symmetric waves. But I come from more from pattern formation. And so when I think of axisymmetric waves, what I think about are things more like this. So this is a target pattern in wave binar convection. So here you've got water in a dish, a circular dish, it's being heated from below, and you can generate these waves that are rotation invariant in some plane. Invariant in some plane. If you're thinking about in a plane rotational invariants, you can also think about ferrofluids. These are now magnetic fluids. You put a magnetic field vertically up through the fluid, you can generate surface instabilities, and you get these anti-symmetric spikes. This, by the way, very much the same equations basically as a gravitational wave equation. So this is the closest I'm going to get to water waves, probably. Likewise, if you saw Cat's poster, you can look at these. Cat poster, you can look at these bouncing droplets, you can think of these as being axisymmetric as well. And I guess if you wanted to study, say, the ripples coming out of them, again, water weights, axisymmetric water weights. In Jerk's talk, we saw kind of a different definition of axisymmetric, I guess, right? So now we don't have a plane solution that's axisymmetric. We're now talking about solutions that are on some sort of like column and are axisymmetric around the column. Axisymmetric around the column. So here, this is basically what Jerk was looking at, right? These are capillary waves on some jet of water. So this is water coming down, you have capillary waves here. There's also more interesting, slightly more complicated sort of problems that you could talk about in this sort of framework, so vortex rings, where you have these two jets firing into each other and generating these rings. Again, they're sort of axisymmetric along the line of action. Along the line of action. And if you're feeling really fancy, there's also Taylor bubbles, where again, everything's happening in this axisymmetric direction down some sort of tube. Now, axisymmetry kind of implies polar coordinates, right? We want to use polar coordinates because axis symmetry means that you've reduced the 3D problem to, let's say, a 2.5, 2 plus epsilon dimension instead. You just have the radial coordinate and you have whatever you're. Just have the radial coordinate, and you have whatever your length is, which much like the I'm going to call Z, for no real reasons other than, I guess, symmetrical coordinates. But of course, beyond axisymmetry, you could still do polar coordinates in other problems, right? And so there's sometimes it might be useful to think about things in terms of polar coordinates. So mostly to fill up my picture quota, let's do some more of these. You can talk about helium short patterns, right? So here you have two plates close together, and you inject some fluid into the plates, and then as Some fluid into the plates, and then as you inject the fluid, you get these sort of fingering instabilities that occur. Or you can think about rimming flows. So here you have a tube with fluid inside it, you're rotating the tube, and if you get a certain rotational frequency, like speed of rotation, you can generate waves along the fluid around in the circle. Again, polar coordinates makes a lot of sense. You can also think about Faraday waves. Here I've chosen a Faraday waves. Here I've chosen a very non-Faraday wave picture, mostly because it's a really fun table that I saw recently. So, this is Faraday waves where you have a very small amount of fluid, and at certain frequencies, you can actually make them generate polygons. So, you get a pentagon here and stuff. Again, it's not so clear that polar coordinates would work in this example, but it's much closer to something that I've been doing in patterns, and so to me, I'm like, ah, this is cool stuff. Okay. Okay, I'm not going to talk about any of these stuff anymore. Again, this is mostly for keeping everyone awake right now. So, cylindrical domains, I'm going to focus on basically two types, right? Again, kind of how we broke up the two sets of pictures. I'm going to focus partly on jets of fluid in this sort of regime, much like what Jerk was talking about. I totally stole this picture from Martin Dawk's paper. I stole it from another paper. It's a good one, it's a good picture. It's a good figure. Ignore, for example, this here. This is obviously some magnetism things that I don't care about right now. The idea is you have three coordinates, right? You have X, Y, and Z coordinates. And you replace your X and Y by R and theta. And what you found is a domain that is a cylinder. There's a free surface going on here in the R direction. You have a cylinder that is bounded in width, but it's In width, but it's infinitely extended in length, right? So Z goes from passing infinity to minus infinity, but R is bounded, in this case, by a front surface. The other case, which I couldn't find a picture for, so I had to make a quick one, is the other case where you have planar patterns on the top of the surface, or planar weight. And in this case, you're thinking about a cylinder that is infinitely wide, so r goes off to infinity, and you have bounded heights. Rounded height set. So, this is more like if you wanted to look at the 3D water wave problem and pose it in polar coordinates, you could have a cylindrical coordinate system that looks like this. Okay, like I said, these are normally known as jets. I like to think of these more like a shark tank. So, you have jets, you have a shark tank. Here's my shark, shark tank. So, I like to call these jets and tanks for shorthand, but then that sounds like I'm being funded by DARPA. So, instead we'll have jets and we'll have sharks. And we'll have sharks. And much like any good rivalry, what we'll find is that they have a lot of similarities, but some key differences, like any good rivalry. So the key thing here is, right, water waves. You have a 3D problem, but in these cases, you can reduce to just the free surface. We've seen so many ways of reducing to the free surface. The key thing here is in the JETS case, if you go to a free surface and you assume anti-symmetry, all of the action is a very simple All of the action is happening in just the z direction. So it's basically one dimension. If we instead go to the shark's case, we can reduce to the free surface, and we can assume axisymmetry, and we now have a planar problem that only depends on a radial dimension. But anyone who's worked in radial coordinates knows that it's significantly different trying to still study those problems. And a lot of the normal analytic tools you want to use. Analytic tools you want to use aren't really there. So, we're going to cover both of these in this course, right? So, in theory, most of the actual formulation of these things is just talking about cylindrical coordinates. So, they don't really care if you're like this or like this. There's only a few technicalities that will change later on as we go on. Okay, so sharps and jets. That's all we need to remember for now. So, just to mention, again, I come from pattern formation. To mention again, I come from pattern formation, and recently there's been a fair amount of work using polar coordinates. So, there's the concept of like spatial dynamics in radial variables kind of got pioneered in 2003, it's quite common now. And nowadays, we use it to look at, say, like spiral waves. I've got some recent work with David Lloyd and Jason Brownberger, where we look at localized patterns on the plane that have these dihedral symmetries. And even more recently, there's some seen by perturbation theory that. Recently, this has seen the perturbation theory that Eileen in previous life was a part of. But the point is that in pattern formation, these are all very simple models. They're all semi-linear, parabolic PDEs. There's not a lot of fancy techniques you need to use. There's not much actual function space theory or anything you actually need to read in this. Now, when you come to do water-based problems, of course, this is very different. And so this is not an exhaustive list. This is not an exhaustive list, but just to note a few recent studies involving axis symmetric waves. So I had a semi-analytic formal paper looking at these ferrofluid spots. This is with David Lloyd and Matt Turner. As Jurg mentions, there's been a lot of work on these ferrofluid jets. So much like the capillary waves JÃ¶rg was talking about, we now have some extra magnetism terms. And so there's some numerical work by Martha Memeleon. America work by Mark and Emilian, as well as some work by Alex and Jean-Mark. And then there's some analyst work where the previous picture was stolen from by Mark and Dork. And I think this is another picture stolen from Mark and Dorke. They really are my pictures. There we go. And so in these cases, you're finding more like KDV or NLS type waves. Also, in the Curie wave case, so both of these co-authors are Eric and Both of these co-authors are Eric and Jerg. Again, I mean, we've seen JÃ¶rg's talk, I don't have to go too much into the detail. So, notably, I'm going to focus on the two analytic, or the three analytic papers here. In the ferrofluid case, there was some theory Mark and Dork were using about radial function spaces and stuff, some slightly older work by Bernardi and our collaborators. But really, when it came down to doing stuff like taking certain When it came down to doing stuff like taking certain estimates, it was a case of just brutally fighting those estimates. I think that's fair to say. When in doubt, just keep taking estimates until you get the right ones. Whereas, as we saw in Juris's talk, there's some quite a tricks going on here where you go up to these higher dimensions, you look at things as just Laplacians, and you're able to actually deal with a lot of this operator theory without actually having to touch the radial problem mostly. Both of these are kind of square pegs to round holes, right? It's like the titanium thing. That's where the title comes in. So, why use a square peg and a round hole if you can just build a round peg? So, that's the idea of what we're trying to do here. Can we find things that are specifically designed for radial problems so that we can then do radial problems more easily? Okay? And so, I'm waiting more time. Okay. So, to have a little break and do some analysis for a second, so this is now going to be. This is for a second. So, this is now going to be a brief, very brief overview of some work that Mark and I did recently, and the preprints there if you're interested. And so, the point here is just, let's say we have two functions. So I have a function f hat k, which is a function of x and y, and I have a function f of k, which is a function of the radius r. And let's say, for example, they satisfy this relation here. So, if I take my function of x and y and I put it in polar coordinates, And I put it in polar coordinates, the R and theta dependency decouple, and you get this sort of behavior here. So I put this K in, so I have a C to the ideal theta. And I hope you will all accept that if K is zero, then you get an axisymmetric function. So we have this function. I'm going to introduce two sets of derivatives. So over here, I have complex derivatives that are verting a derivatives. So it doesn't really matter too much, right? There's one up here, and then this one's just a complex conjugate. Up here, and then this one's just a complex conjugate. On the radial case, we have these non-autonomous operators or spatially heterogeneous operators that depend on this index k. And the point is, if we take our f hat k, which we'll call something like, let's say, a mode k function, and you take a derivative with zeta or zeta bar, what you find is that you shift your Fourier mode by one, either up or down, and you the rest of your operator becomes these dk and d minus k operators on your function fk. On your function fk. So dzeta and dzeta bar are mapping you from a mode k function that satisfies this to a mode k minus one or a mode k plus one function. And likewise, the dk and the d minus k map you to, let's call it the radial coefficient of a k function into a k, a coefficient of a mode k minus one or a mode k plus one function. And what this tells you is you can write your is you can write your variables as Fourier modes, and if you start taking derivatives in this way, they remain Fourier modes at the cost that they have to shift in their frequency. But it doesn't break the structure, right? It just moves it. Much like cleaning a room, you don't actually clean the room, you just move everything until eventually, until everything's back in the right box. And you can generalize this up to higher order, right? So if you're in a higher order, you can look at anything. If you're in a higher order, you can look at n derivatives with some amount of dzs, some amount of dzerbas, and you can write down something. Where you now shift into the k plus n minus 2i mode. It's not super important here. Note that if I, let's say I want my function to be well defined at r equals 0, right? r equals 0, always the problematic term in all of this. So if I need this to be valid for r equals 0, then I need that f sub k at 0 must be 0. At zero must be zero, except in the case where k equals zero. So if k equals zero, then we lose this term over here, and then there's no theta dependency. But if k is non-zero, you have to have fk equaling zero in order to kill the theta dependency on the right-hand side. So what if I want my derivatives to be well defined at r equals zero? Well, I mean, I have my formula now. I know what my frequency is. So actually, I also have that to equal. So actually, I also have that for any derivative, I know exactly which terms have to be zero and which terms don't have to be zero, basically on the fact of whether or not they map as into an anti-symmetric function. So if you map into a mode, k function where k is non-zero, you have to be zero at the origin. Right, so you can basically, it's a very easy way of writing down all possible boundary conditions as it is. Let's say you're not doing derivatives, but you want to do like Fourier transforms. We want to pose things in Fourier space. Fourier space. The numericists in the room might be more familiar with this. I found in analysis we weren't so familiar. You have instead what are called Hankel transforms. So here you take your FK, you do this Hankel transform, you basically just multiply it by a Bessel function and integrate it with this factor of R. So you take a two-dimensional Fourier transform of your f hat k, you pose in polar coordinates in Fourier space, and you get a function which is just the Hangle transform of your F K in your Fourier mode. In your Fourier mode, with now just a slight rotation in Fourier space. So, again, we have a Fourier mode structure, we apply basically our Fourier transform, and we preserve the Fourier mode structure. Again, in this case, at the cost of slight rotation, or a factor of I, if you want. And Hankel transforms are useful, they're self-inverse, and you have this great property that's these differential operators corresponding to basically multiplying by the coordinates. So you can define all your derivatives as, I guess, Hankel. Derivatives that has, I guess, Hankel multipliers rather than Fourier multipliers. It's all very cautious. If you define a weighted L2 norm like this, this is like a natural L2 norm in two-dimensional with axisymmetric functions, then you also recover pass rule. You can also write down a Puncher L version as well. And so what you find is that actually this HK is defined isometric isomorphisms on your L2 roots in this case. Okay. What does this all mean? Okay, what does this all mean? It means we can take now the collection of projections, we have some form some normal function u hat, and we can now write it as a sum of all of these u hat k's based on this projection, where at each stage, if you pose in polar coordinates, you have this relation here. This is a really fancy way of just saying that you can write down a Fourier series in the angular direction. But it also means that you take the Fourier transform, you also can write it down with this angular transform. So any angular So, any analyst here might ask the question: if we know what function space U hat is in, can we say what function space these UKs are in? And when we first started thinking about this, we kind of thought that someone would have done this. It's not super clear. The literature is not the easiest to find. But basically, the very, very, very broad sense of it is: let's say that u hat lies in some space X. That u hat lies in some space x, where x is the function sort of r2 into, say, c for simplicity. Then we can say that u hat k lies in some space x hat k. This is just the proper subspace defined by your projections. And then u lives in some new space that you've defined, the functions from zero to infinity, again to c, where these two spaces are then related by this relation here that forms an isomorphism. So, in this way, we can basically categorize all of the functions. Way we can basically categorize all of the function spaces we need based on what function spaces our original function was in. So, for example, you can do continuous differentiable functions, or here I've got test functions, class class functions, and particularly sopholov spaces. And we have these great properties. So, let's say you have two functions and you want to take their product, then all that happens is, so I'm using CM as for simplicity here, basically what happens is you end up with a Basically, what happens is you end up with a shift in your Fourier mode. And likewise, if you know what uk is in some CMK, when you take derivatives, you lose the derivative, and you also have to shift by plus or minus. This is now a closed system where if you consider all of the k's, everything remains within this family of spaces. And notice what this also means is if you take actually symmetric functions where k is zero, when you start taking derivatives, you still have to pay attention to the You still have to pay attention to the non-zero index spaces. So if you take n derivatives, you might be in the n-mode space at the end of it all. So you can't just define actually symmetric spaces, you have to define all of them. This was something that also wasn't obvious to us when we started. And so if you define the sub-love spaces, the sub-love norms in a vessel potential sort of way, you can also then write down this isometric isomorphism between U-hat and the Between U hat and the sequence of all of your UKs. Okay, so the sums are all well defined as well. Okay, that was the break for analysis. Let's leave that for now. Let's all take a nice deep breath and move on. So we're doing three-dimensional problems, right? So in a three-dimensional problem, not only do your variables depend on your spatial coordinates, but they are also represented in your spatial coordinates. Everything has a direction. Everything has a direction. So if you have your velocity u, you have to write it down in terms of some sort of basis functions. Those basis functions depend on your variables. So you have to be very careful in how you do this. So let's say we have our relative, like our cylindrical polar basis vectors. These are invariant under rotations, right? So if I have the R hat direction and I rotate my frame, my R hat. Direction and I rotate my frame, my R hat direction also rotates, whereas X would stay where it is, right? So if I want to expand U as a Fourier series in theta, I need to do it in these coordinates, otherwise I'm not actually getting, I'm not really decomposing all my theta dependency from my system, right? So, sorry, this is just to note that this is what the gradient of course would be in these coordinates. But I want to write down my Fourier series in this form, but I now have to somehow relate. But I now have to somehow relate, let's say I want to relate, you know, if u equals the gradient of some function, I need to somehow relate these together. And we know that these derivatives aren't going to be in the same, they're not going to map you to the same Furri mode if it works in the same way as what we were doing before. And so what you can do is you can scrap this system. So we don't want R, we don't want R hat, we don't want zeta hat, we now have some zeta hat and zeta bar hat, which in the physics. Which in the physics literature exists and is called spherical basis vectors. Don't know why they're called spherical, but hey-ho. These are now, so they're complex basis vectors, a bit bizarre. But what they do is when you take this expansion here and you reformulate it with these two terms, you're basically hiding some of your theta dependency in your zetas. But what that does is it causes a shift in the furium. In the Thurium moments again. So you have these two terms down here, these are completely independent of theta, and they get written in this way. And so when k equals zero, this is an anti-symmetric solution because the Ethi minus theta and Ethi theta counteract the theta dependency in these two basis vectors. Very strange, I know, but if you do this, what happens is, let's say we now explain. Is, let's say we now expand our F as a Fourier series, we expand our U as I just said. If I now take the gradient of F and I write it in these basis vectors, I get exactly means which are exactly mapping into the same form as we were taking derivatives in the scalar case. dk of fk maps a k mode function or a coefficient of a k mode function into a k minus one mode and so if k is in a particular function space if k is a set In a particular function space, if k is instead of a particular function space, this is now in the correct space in one of our function spaces again. And likewise for each of these. And I can start defining more things. So the Laplacian of F is just in this form. So this is exactly the Laplacian for a given mode. You can prove that it's elliptic, or at least has elliptic regularity in certain regimes. You can define the divergence in a similar way. You can also take the cross product. I don't want to go into the cross product. To take the cross product, I don't want to go into the cross product, it's a nightmare. And what you can do is you can also then write this in a spatially invariant way, coordinate-free, at which point now each of these operators is mapping you into the cross-product of a k minus one function space, a k plus one function space, and a k function space. But you can keep track of where they're all mapping to because you know how these derivatives act. You know what they map in terms of each of the different Fourier rules. So, I want to do a very simple example. So, I'm going to use Stokes Flow because it's super easy to write down. So, we have our u's as the velocity, we have our pressure, we have some data f, and we have some boundary data g. It's not super important what the domain is to describe. And here, this Laplacin is the vector Laplacin, right? So, it's the Laplacity of each lift. If you write this down in terms of your spherical basis, so R hat and theta hat, you generate a hat and theta hat, you generate a system that looks like this. You have your Laplace and your plane Laplace, but then you have all these cross terms over here. And you have your divergence issue right here. If you instead use this formulation and you plug this in, you get exactly this, which is the Laplacian for the K plus 1, the K minus 1 mode. This is the Laplacian for the K plus 1 mode. This is the Laplacian for the K mode. And everything is diagonalised. So to me this was the thing that told me this is exactly what you should be doing. Told me this is exactly what you should be doing when you do these sorts of problems. Like, I've seen people work with this, horrendous, why would you? And of course, again, you could write this down in a very compact form. I mean, due to the nature of notation, this looks super compact because it's designed to look super compact. So I won't go too much into that. But let's say now we do Stokes flow, but I'm now going to just replace my F by some nonlinearity, and I'm going to ignore the boundary conditions because I want to consider both. Boundary conditions because I want to consider both cases of sharks and jets. So the boundary conditions will be different. But now, so let's say I have this system, right? So this is just the first three components and then the divergence-free case here. And I want to write down a spatial dynamics for P. In the case of jets, my spatial dynamics problem should be in terms of z, right? Because that's the infinitely extended direction. What you can do is you can solve for p, introduces some new conjugate variables, and you can write down a system where it's an evolution equation for z, and you have a bunch of these k or k. Z, and you have a bunch of these k operators in here that you just need to keep, take control of. In the case of sharks, I want to write this down in terms of an evolution problem for R. And what happens is you can do the same thing, but now your evolution equations depend on these operators here. And you get pairs of operators, basically like a mapping up and a mapping down in the Fourier modes. If you wanted to look at the linearization of this of the right-hand side and study the The right-hand side and study the operator, you end up with a problem where you're looking trying to solve this plus the boundary conditions. And this, as we said, is an elliptic operator in the radio radium. Likewise, if you do it in this case, you just get a nice operator like this. So you can write down these spatial dynamics problems very, I want to say simply, I don't know if you want to call it simply. There's a lot of algebra going on here to get to this point, but it all works, is what I'm saying here. How am I doing on time? One minute, sure. That's fine. So I have some questions about flattening and whether that preserves everything. I was going to talk about the stoke stream function, but JÃ¶rg also sort of talked about that. I will just mention, so in Jerk's example, right, we had this problem up here, where I've written in my notation, this is exactly the lambda star that JÃ¶rg had before, and this is one of those boundary conditions in his problem. And so what they did was they recovered. And so, what they did was they rescued like R squared. They could write down a system where now this is an axis symmetrical plastic because we're in R5 Cartesian space. And actually, this adjective doesn't seem to get much better in terms of R squared. What I would suggest is actually you define it by R times some variable phi. And so your U becomes a system that looks like this. So I've left it written in R and theta. In theory, I would write this in zeta and theta bar, but for the sake of ease right now, this is now, if phi is a Is now if phi is a mode one function, then this is an axis symmetric function. This will be a mode one function, but it's in the right framework. And what happens is when you write down these equations, this is exactly the Laplacian for the one mode, throwing modes. And everything on here, or at least this and this, and maybe if you define this slightly differently, are very easy to bound by just like H1 modes. So to me, I mean, this is then exactly an elliptic operator. This feels like the way that you can solve this. This feels like a way that you can solve this in a very doable way with using function spaces. Okay? So, yeah, just to say, function modes are the way to go, filter modes are the way to go. It seems like everything works very well. There are a few little issues, but I think this is definitely a way that you can really move forward in looking at these sorts of problems. And with that, I will say thank you very much. Any questions? Can I ask a question? Maybe I haven't properly understood what you've said.