Well thank you for first of all to the organizer for inviting me. My name is Ainzo, I'm a second year PhD at Tri Holloway. And today I will be giving a talk about a project I've been working on together with my supervisor Glenn and with contributions also from other staff. So the topic of my talk is uncertainties on systematics and a possible suggestion of how to deal with them in a particle physics analysis. in a particle physics analysis. So we have already seen that some of our systematics are well estimated, and this is usually the case when we have some real data set of control measurements associated to them or some self-data set of Monte Carlo events. But there are also quite nasty situations where our systematic uncertainties can be themselves quite uncertain. And as we have seen, this is for example the case of series systematics. Of series systematics and also the very famous case of two-point systematics. So, in this talk, I will suggest the possible approach of dealing with these uncertainties, ununcertainties that we informally call errors and errors, using the frequentist approach. So, just to formulate the problem, suppose we have some measurements y described by some probability density, doubled by some parameters of interest mu and some meters parameters zeta. Now, we also suppose that we have some. Now, we also suppose that we have some auxiliary measurements that we use to provide auxiliary information on the parameters, on the Lussa parameters. And we assume these auxiliary measurements to be independently Gaussian distributes. Now, the exact value of the auxiliary measurement can be the real outcome of an experiment, of a measurement, or can be just the best guess based on some theoretical reason, for example for serial systematics. Now, of course, if we take the log likelihood, what we get is that our What we get is that our Gaussian constraints are quadratic term, and this little sigma ui is what we will refer to as the systematic error. So the standard deviation for the auxiliary measurements are what we call the systematic error. Now, what we want to do is to try to extend this model so that we can allow for some of the systematics in our model to be themselves. Model to be themselves uncertain. So, in order to do this, so to implement our zone errors, our assumption is that our systematic variances are adjustable parameters of the model instead of being the fixed parameter of the model. And we use the systematic variances instead of the standard deviations because, as I will show, they are just a little bit mathematically easier to now. We assume that the value we were assigning to our systematic errors before, so the best estimate of our systematic variances, we assume that. Of our systematic variances, we assume them to be gamma-distributed. And now, gamma-distribution are family of distributions for positive defined quantities, and they are labeled by two parameters, alpha and beta. But we can equivalently reparameterize them in terms of two other parameters, which are the expectation value of the distribution, which are the systematic variances themselves, and this little epsilon parameter, which is a little bit more involved definition, but it is basically. But it is basically the relative error on the systematic uncertainty. So it's what we informally call the relative error on the error. Now, having an epsilon parameter equal to 0.2, for example, means to have 20% uncertainty on the systematic uncertainty. Now, if we plug this in our model, we find that, well, first of all, we have promoted against We have promoted against systematic variances to be adjustable parameters of the model. And we have multiplied our original model by these new gamma terms. And this model, and we have called this model the gamma variance model due to the presence of these gamma terms. Now, the nice feature of this model is that we can profile over these parameters in closed form, and therefore we don't actually double the number of recent parameters. Because once we have profiled over these parameters, what we get is a profile likelihood that the parameters. What we get is a profile likelihood that depends on the original parameters of the model. But the only difference is that we have substituted the quadratic terms with logarithmic terms. We have, let's say, generalized the quadratic terms with logarithmic terms, because these logarithmic terms depend on error parameter axion, which is a fixed parameter of the model. And if we take the limit axion going to zero, we recover the contracted terms. Now, just to visualize this at the level of the likelihood, this is equivalent to substitute the Gaussian. To substitute the Gaussian constraints for the control measurements, we put students' constraints, which are distributions that are shaped like Gaussians, but they have taller tails. And therefore, the way this model treats outliers will be different. Now, to see an example of this at work, let's suppose to have a very easy example. So, suppose that we want to average four measurements, and we assume this measurement. And we assume these measurements to have all the same statistical and systematic errors equal to one. And we also assign an epsilon parameter, so an error on the error, equal for all the measurements, equal to epsilon. And we want to study how, if these measurements are compatible, are compatible, as they are, how increasing the epsilon parameter, so how turning on zeros on errors, would affect the result. So now if I change the value of the epsilon parameter and I increase it to 30%, to 60%. To 30% to 60%. Well, you can barely see any changes. And this is because our measurements are compatible. So, what actually happened is that the estimate of the mean stayed constant, whereas the confidence interval on the estimate of the mean was only slightly inflated, reflecting, of course, the extra degree of uncertainty introduced by erosor errors. So, if data Errors errors. So, if data are internally compatible, results are only slightly modified. But the picture changes, if we instead consider an example where in our data set we have an applier. So, if now we have an applier in the data set, what happens when I turn on errors on errors is that something this time is happening. And more specifically, well, if I increase the value of epsilon, what happens is that the estimate of the mean. What happens is that the estimate of the mean is pulled less strongly by the presence of the applier. Of course, if you do an average and you have an applier in the data set, this will bias your result. But once you consider the presence of errors and errors in the model, well, what happens is that the estimate of the mean moves back toward towards a group of measurements that are compatible with each other. So the model correctly identifies the presence of an outlier in the data set. An outline in the data set. And so the mean is not pulled by it anymore, but the tension in the data set is reflected by an increased confidence problem, which now is increased by more than what it was before. So to conclude, the model is sensitive to internal compatibility of the data. Now, a very nice physics application is a W master. Since W master is one of the fundamental parameters of the standard model. And quite interestingly, the latest TDF measurement. Last thing is the latest CDF measurement displayed quite significant tension with the other measurements and with the standard model prediction. And so it's a very good application to see the features of our model at work. So if we look at the discrepancy between the CDF measurement and the standard model prediction, we have quite a big discrepancy. We have a seven sigma discrepancy. And what we would like to study is how this seven sigma discrepancy would be affected if we consider the presence. Consider the presence of errors and errors on the systematic uncertainty of the CDF measurement and on the theoretical uncertainty of the standard model. Because we know these terms might include components that are themselves uncertain. So if we assign to them the same epsilon parameter, so the same error on error value, we find that the significant significance, when we plot the significance as a function of this epsilon value, the significance is quite strong. Strongly affected by the presence of uncertainties on the uncertainties. And in particular, if we already consider a 20% uncertainty, what we see is that the reduction, you obviously have the reduction from 7 sigma already below the 5 sigma threshold. So uncertainties in the assignment of systematics can account for some of the tension between the inputs. And of course this like very large reduction is also due to the fact that we have a large tension in the data set. Fact that we have a large tension in the data set, and therefore the gamma-vinus model, the facts of the gamma-vinus model are particularly useful. If you consider another case, so we want to this time see the discrepancy between the Aftas measurement and the CDF measurement. If we don't consider uncertainties on the systematics, we have a full sigma tension. And of course, these are measurements of the same standard model parameter, same physical quantity. So it's definitely. Physical quantity. So it's definitely an interesting problem to, it's definitely interesting to check if uncertainties on the systematics may account for some of this tension, which has to be motivated by some reasons. So again, if we assign uncertainties on these two systematics, these two systematics terms, what we find is again we see a reduction in the significance. Even though it's not as strong as before, because now we have a four-sigma tension, it's a data set is that of a seven sigma tension. Instead of a seventh potential. Now, you may think that, well, this is a quite strong statement to say that both the systematic uncertainties of the two measurements have an error and error associated with it. But the point is that if you consider a less aggressive approach, and so we see we say, for example, let's assume that just the Apple's measurement has errors on its on its systematic uncertainties. And in this case, we see a very, very similar plot. See a very, very similar plot. This is because when you have multiple components that have errors and errors associated to it, and you have a big tension in the data set like we have here, well, then errors on the largest uncertainty dominate. So in the previous case, the errors on the uncertainty were dominating, and this is why we observe a very similar trend. And so if we did something similar, so if we made a breakdown of the systematics, we wouldn't need to have all the systematics. Need to have all the systematics to have a large error or error on them to see such a trend. But we would really need just one larger systematic, maybe some large series systematic, having a large error or error associated to it to observe a trend similar to this one. Now, we can also see what we did in practice on a combination of the two measurements. Of course, this is a very trivial combination without assuming any correlations between the measurements. And if we consider again the two. And if we consider again the two measurements to have the same uncertainty on the systematic uncertainties, we see that the outdoors measurement, since the outstress measurement has a bigger systematic error, it is treated by our Erros and Errors model as the outlier of the data set. And therefore, our estimate of the mean is shifted towards the CDF measurement. And the tension is the data set is reflected by inflected error part. And a similar conclusion can be drawn if we just consider error on the atasimeter. But instead, The atlas map. But instead, a different picture is observed if we set the atlas error on error to zero and we consider just an epsilon parameter on the CDF measurement. And in this case, for sufficiently large error on error, what we see is that this time the CDF measurement is treated as the outline of the data set, and the estimate of the mean is shifted towards measurement, and so also towards the standard model prediction. And the fact that Standard model prediction. And the fact that now we have an outlier with a very small error bar is reflected by a quite inflated confidence interval. Now, of course, I've plotted everything as a function of this epsilon parameter, which of course in a real analysis on a real combination would be a fixed value. Because, well, we didn't do the analysis, so we don't have expert knowledge about what is actually what actually is epsilon parameter may be. And so we plotted everything in terms of it, and then it's up to the experts to say. Of course, to the experts to say what value is a correct assumption. So, what causes the funny behavior around epsilon equals 0.3? Yeah, well, it's a mathematical property, and it's probably related to the fact that at this point, up to 0.3, in the model, the estimate of the mean stays constant. And for sufficiently large, when the errors and errors become sufficiently large, When the errors and errors become sufficiently larger, then the CDF measurement is finally treated as that outlier. And this also has a consequence a readjustment of the confidence interval that considers this change in the position of the estimate of the NOOOO. So if we want to do a mental experiment and imagine that part of the this This huge for sigma tension is due to some uncertainty on some of the systematics. So let's say we want to reduce this for sigma tension to 2 sigma 1. Well, then we would need quite large uncertainties on at least some of the systematics. But we know that these measurements have QCD modeling effects or PDF uncertainties. So it is possible to have such an effect for at least a couple of them. And this would also imply. And this would also imply that our knowledge about the true value of the W mass, based on our experiment, would be significantly affected by the presence of uncertainty on systematics. Because the conclusion and the knowledge we claim we have on the W mass would definitely change, and especially our confidence interval would be largely inflated, of course, depending on the assumption we've now another important point of how to use this model. How to use this model is how we can compute confidence intervals using the gamma-viance model. Because this is a not true problem, because the gamma-viance model, so the model with errors and errors, was found to deviate from the asymptotic limit when we have sufficiently large errors and errors parameters. And we have seen that this epsilon parameter can easily be up to 50%, for example, for serial systematics. And since we want to provide a tool that everyone can use without necessarily have to do some heavy Suddenly, we have to do some heavy computation, heavy simulation. While IRO del Symptotics proved to be quite an elegant solution to avoid the needing of Monte Carlo. So, well, we all know how to compute confidence interval using the Lightning Ratio. So, well, we compute, of course, the p-value, then we construct of these regions based on its value. But, of course, the problem is always how to compute the density function for the lighting ratio. And while solutions are, of course, using Monte Carlo to do an exact estimation of each. Monte Carlo to do an exact estimation of it or to use a asymptotic limit. Now, the asymptotic limit is defined as a limit where the MLEs are Gaussian distributed. And in this limit, we can apply the Wick's theorem to know the distribution of the likelihood ratio, which is a chi-square with error terms that goes like n to the minus 1. However, when we have a small sample size, because n is a, well, n usually refers to the sample size. But for example, for our Errolson-Errows model, n is related to the n is related to the Aroson Errors parameter epsilon. So for big epsilon, we have small n, and so we are in the situation of being away from the condition of the Wigg's theorem. And so what we do again, we can use Monte Carlo simulations or a solution we found very convenient for our application is uh instead using uh hydrody asymptotics. Well now I I will just give like flavour of how hi hy rotor asymptotics work. Of how high radical asymptotics work. This is not meant to be a formal introduction, of course. So, hydrogen asymptotics are corrections directly to the first-order statistics, so in this case to the likelihood ratio, to make it more chi-squared distributed. So, we don't want to correct the chi-square distribution. What we want to correct is the likelihood ratio itself. So, we want to correct the statistic so it will be more chi-squared distributed for small sample sizes. And the two options we considered are the vertex correction and Are the vertex correction and they are let's begin. Now, we will apply this correction to our Eros and Errors model, but in general, they are a very useful tool whenever your asymptotic distributions are poor approximation. So, for example, if you have small sample sizes. Now, the battery correction works in a very intuitive way, because basically what you do is just you just take the you just take the like good ratio. You just take the like-hood ratio and you multiply it by this correction factor. This correction factor is just the ratio of the expectation value in the asymptotic limit, so the degrees of freedom of the chi-squared distribution, divided by the exact expectation value that you can compute with some analytical technique or you can estimate with a music Monte Carlo simulation. But nevertheless, this is like a correction factor that makes your likelihood ratio better approximating by a cast. Better approximating by a chi-square distribution, even for small sample sizes. So it has a reduced error size. Another possible solution is instead using R-star, which instead is a correction to the likelihood root statistic. And as the name suggests, the likelihood root statistic is just the root of the likelihood ratio multiplied by a sine function. And this statistic is, instead of being Tasque distributed, is normal distributed. Now we can correct using R star. Correct using R star, this statistic to make it more normal distributed. Now, this R star statistic has a quite involved structure, but a relationship I find quite intuitive to understand is that at this perturbative order, R star is just like a standardized version of R. So it's R minus its expectation values divided by the standard deviation. So it's quite intuitive uh this this I find this quite intuitive to understand how how we are correcting how we are correcting the uh likelihood route. Likelihood root. So, as a likelihood, as I was saying, as a normal distribution, but with a smaller error term. And we can also think about squaring the likelihood root if we want to get the correction to the likelihood, something like a correction to the likelihood ratio. Since this statistic will be chi-square descriptive, since it's a square of a statistic that is normally descriptive. Now, why our errors on errors model deviate from the asymptotic limit when we have big errors per big error? Big errors para ep big epsilon parameters. Well, to understand this, we can look at a very simplified example where we have just one measurement which is normal distributed. And we assume that the variance of this normal distribution has a relative uncertainty associated to it. And so we construct the likelihood just as a product of the normal distribution and the gamma distribution. Now, if we compute the likelihood ratio, we find that the likelihood ratio has this logarithmic structure, but it can be expanded in powers. But it can be expanded in powers of epsilon square. And what we find is that the first-order term is just a quadratic term. And then we will have corrections that cause like epsilon square or bigger. Now, this is not a formal mathematical proof, but well, these terms will be chi-square distributed. Therefore, if we take the Libby epsilon going to zero, we will find the likelihood ratio is chi-square distributed plus correction terms. Correction terms. Unfortunately, what we find is that these correction terms are quite relevant. Indeed, already for epsilon values of 0.2, we already see that the distribution of the likelihood ratio, the blue curve, is already moving away from its asymptotic value. And especially, and if we increase this epsilon value to 0.4 and 0.6, well, we find that we can't really rely on the chi-square approximation to do inference of our own. Approximation to do inference on our parameter of interest. So, well, what happens if instead of the likehead ratio we look at the improved statistics that I just presented? So, well, in this situation, we find that the chi-square distribution is a much better approximation to our higher-order statistics. So, if we increase epsilon value this time, what we find is that everything is very What we find is that everything is very well approximated by the chi-square approximation. So, in principle, we don't need to do Monte Carlo. Equivalently, we can look at a plot on the confidency interval on the parameter of interest versus our epsilon parameter. And what we find is that, well, if we just use the likelihood ratio and we assume that. The likelihood ratio, and we assume that and we assume it to be ti-squared distributed. Well, what we find is that we really get the wrong answer. So, we are getting the answer that the confidence interval is shrinking instead of the exact solution. But if we use high-order syntax instead, well, everything is much more precise and the correct trend is captured by high-order statistics. So now with Clarence Alessandro, we just published a paper on the archive concerning different implementations of iron draw symptotics that applied to different errors and errors model, including averages using the Hubigabians model, which is the one I've used before for, for example, the W mass measurement example. Also, for more simple averages where you don't have systematic errors, but just a standard deviation, which is Errors, but just a standard deviation, which is uncertain. And also to correct goodness of statistics, which of course we need in physics. So, some conclusions. First one, concerning errors and errors. Well, including errors and errors into a particle physics analysis has really non-trivial consequences. It's not just like multiplying the confidence intervals by some constant. It has a relation with the internal compatibility of the data set. And so if your data are internally compatible, And so if your data are entirely compatible, results will only be just slightly modified and there won't be uh substantial changes in the results of your analysis. But if your data are incompatible, then errors and errors will modify both your center values and your confidence intervals in a quite non-trivial way. And the second point is that higher-order asymptotics provide a very angular method to correct deviations of this model from the asymptotic limit. And so that And so that proved to be a very valuable tool, at least in this application, and definitely worth it to explore other applications in other branch of power physics. So, well, thank you for your attention. Okay, let me close the second picture.