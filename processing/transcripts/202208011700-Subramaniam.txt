Your attention to this talk, and my regrets that I could not be there in person. I think this workshop would really, you know, I would have benefited more by being there in person. But so I appreciate your interacting with me virtually here. So to get right into it, so I'm going to talk about the problem where you have clustering or preferential concentration of particles. Preferential concentration of particles, typically, or could be droplets as well, or bubbles in turbulent multi-phase flows. And obviously, these have multi-scale characteristics. And so, what I'm curious about is how do I represent and model these phenomena? And the principal result that I'm going to show you uses this filtering approach, filtered fields. And I think it provides us a basis for a theory of multi-phase turbulence that's applicable. And I'll try to limit the scope here. These are collisional, already dispersed multi-phase flows. Already disperse multi-phase flows, so atomization itself is not being covered by this kind of a theory. And it has, I believe, the capability to deal with both what we call intrinsic turbulence and pseudo-turbulence, where that's the velocity fluctuations in the fluid generated by the particles. And it is technically applicable from dilute all the way to dense cases. So, with that kind of a preview, let me move on to describing a little bit Move on to describing a little bit about definitions in this multi-scale description. Broadly, I'm going to talk about three kinds of scales: a micro scale being on the order of one to ten particle diameters. And in this case, you'll see velocity fluctuations in the fluid and the particle. The images you're seeing there are from experiments by Schaefer and Gopalan at the National Energy Technology Lab. And these are very nice particle tracking velocity measurements. Tracking velocity measurements near the edge of a riser. And so you can see the characteristics here. You know, obviously, these particles are influenced by drag and drag fluctuations. They undergo inelastic collisions. They may be fluid-mediated particle-particle interactions, particles experiencing the wake of particles ahead of them. And obviously, shape and rotation are important. Moving over to, they were able to zoom out in this set of experiments and look at. Set of experiments and look at things at the mesoscale where you see organized particle structures in this middle panel interacting with the fluid. And then you also, of course, the role of inelastic collisions is important even here. And then, of course, they're finally able to zoom out to the macro scale, which is the order of meters. You can actually see that the reactor schematic is shown on the right. The entire height is about 22 meters. And we're looking at actually a fairly small section of the rhizo there. Small section of the rhizo there, and at the macro scale, you do expect to see more effects of particle-wall interactions and the influence of the device geometry. So, that's just to kind of give us an idea. And the kind of structures I'm curious to see if the theory can represent are at the mesoscale. So, this clustering and preferential concentration is characterized by fluctuations in the number of particles in a volume, or if you look at particle volume itself, that Particle volume itself that fluctuations particle volume in a region. And as I said, there are two ways that this we are familiar with this. One is the glute particle-laden homogeneous turbulence, which has been studied for a very long time, where actually you have an intrinsic turbulence, which is dominant, you have fine particles. And here you're seeing both experimental images and simulations. And this is a turbulent duct flow, results from Andrew Banco, 2018, and S. Miley. 2018 and S. Miley in 2020. And this is a fairly high Dark Reynolds number, but particle-Stokes number is about 10 based on the Komnerov scale, I believe. And then you can see here experimental images of preferential concentration. And then on the right, you see point-particle direct numerical simulations at the same conditions showing this kind of preferential concentration. In contrast, we also see a different kind of clustering. Here we see cluster-induced turbulence. In this case, this is sort of pseudo-turbulent. In this case, this is sort of pseudoturbulence-dominated. So, on the left, you have the image from the experiment of Schaefer and Gopalan. And in this right panel, in the left figure, where my cursor is pointing, if you can see that, these are simulations by Jesse Kiappa-Silatro of particles settling in quiescent fluid. So, in this case, there is no intrinsic turbulence, but you still see a formation of particle structures, and we call that clustering, and it does generate velocity. It does generate velocity fluctuations of the fluid, and we call that cluster-induced turbulence. And another example here is an even simpler problem where there's not even this isotropy of settling. This is a homogeneously cooling gas solid flow. So the particles here are in a thin two-dimensional bed in some sense. These are simulations by Shalong Yin. And basically, it's homogeneous. They're just cooling. They start off with a given particle velocity fluctuation, they collide. Fluctuation they collide through inelastic collisions and they cool down. So, very high density ratio of a thousand. Um, and then you the Reynolds number based on the gramular temperature, that is the capital T there, which characterizes the trace of the particle velocity variance, is about 30 and is in elasticity. So if it was one, you'd have elastic collisions where E is 0.8. So every time the particles collide, they lose some energy, and then you see them form these structures. So I believe that this is very central to a lot of. This is very central to a lot of multi-phase problems, and so appropriate for us to talk about here. Before we jump into some of the math and the basics of this theory, I'll just make sure we are on the same page regarding some basic terminology. I'll use an averaging operation, the angle brackets here represent that. So if Q is any quantity, be it in the fluid of the particle, it's a function of space and time. In general, we refer to this as an integral over a sample space, omega, in which we have. In which we have elements little omega with which we associate some probability, capital P here. So, this is the definition of mathematical expectation at invariant space and time. Under appropriate conditions, we can estimate this average by using an ensemble average, which means we perform several trials of the same experiment. Or in a statistically homogeneous case, we may use a volume average. If we know it's statistically stationary, we may use a time average to estimate this. And if ergodicity holds, then And if ergodicity holds, then all these should converge to the same quantity. Now we're going to look at something called a random field-based characterization, which often is seen in what we call Euler-Euler codes or a two-fluid model. Sometimes you see that terminology. And what I'm basically referring to here is you have an indicator function, which is also called a color function or a characteristic function. I sub p represents the indicator function of the particles. So if a particular point in space and So if a particular point in space and time is occupied by the particle, this is one on a particular realization omega. And if it's not occupied, it's zero. And its complementary piece is the fluid indicator function, which for two-phase flow will be one when it's in the fluid and zero otherwise. And so the sum of these is always unity at all points in space and time for a given realization. So if we look at statistics, it's useful to look at the expectation of this. So the expectation gives us the average solid volume fraction. Average solid volume fraction. That's a single point quantity. So mean phi at X is nothing but the expectation of I sub p. So you would think of this as a solid average solid volume fraction. It's also interesting to look at two point quantities. So if we look at the expectation of IP at a given spatial location x1 and another spatial location x2, I can form this quantity called CPP, which is a kind of a covariance function x1 and x2. And what we've seen. And what we'll see is that when we look at device-scale simulations, we usually require tractable single-point closures. So, that is in itself quite a challenging simulation problem for many device-scale problems. And so, very often we are not able to go to this two-point level of description when we do statistical models. So, however, it's worthwhile to kind of understand what these two-point quantities are, because as we'll see, they're important to characterize. Because, as we'll see, they're important to characterize clustering. So, if you look at a distribution of particles or droplets or bubbles in physical space, I've shown here a three-dimensional plot in which we're looking at different slices. So although it's mono-dispersed because of the slicing not going through the equator of the particles, you can see they seem to be of different size. But then these are basically the color function or the indicator function. You can see the blue is one. Well, in this case, it's IF. So IF is one in the red and blue in the particles. The particles. And I can consider an ensemble of these identical in every other way. And what I do is I take an expectation of IP1 at x1 and x2. And so I form the CPP. So this is really how we form this two-point correlation function. And if I do this for, say, this kind of settling problem that we looked at, which I mentioned by Jesse Capisolatro, starting off with particles nicely, uniformly distributed in this box randomly, and then you can see it underneath. And then you can see that under the influence of gravity, these particles fall, they experience drag, and then they form these complex structures. And this is at a later time, say 40 times the relevant particle response time. And so what I'm curious to know is, you know, what will I find if I do CPP on this thing? And this is a homogeneous system. So I can replace my X1 and X2 with an X and an X plus R. So R is a separation vector. And what I find is. I can, what I find is that the presence of clusters does not show up in the average solid volume fraction. So if I start with what about 10%, I think it was much less than that, maybe 2% volume fraction in this case, particles are rearranging themselves, and I'm still going to get an average volume fraction of 2% at late time, but really there is clustering. So the question is, the signature is actually found in this two-point particle covariance function, which I alluded to earlier. And the peculiar thing is when I take the limit value of this. Is when I take the limit value of this when r goes to zero, I just recover the mean average solid volume fraction. So the average solid volume fraction is the same at the beginning and at the end, and so is the zero point limit of this. So single point models really cannot, single point theories cannot capture the fluctuations in number and particle volume. And however, they're very important, obviously. So here's a plot for I just encourage you to focus on the blue line. So we take data from a Line. So we take data from a particle field, monitors for spheres. This is a very small box from DNS, which is about maybe eight particle diameters. And we have maybe a grid that we make, which is 20 grid points on a particle. We know the average volume fraction is 20%. And so when we plot the CPP at r equals zero, r is the separation between the particle centers on the x-axis. CPP is the quantity I told you, expected value of the indicator at Expected value of the indicator at x1 and x2, and it reaches the value 0.2 as we expect at the origin. There is also a nice result that tells us that in the far field, when things get uncorrelated, it should go to mean phi squared. And in this case, that's 0.04. So you see that goes to 0.04 at the far field as well. So the single-point limit of this covariance of volume fraction does not contain information about clustering and in. In this problem. So, what we've learned so far in this sort of short summary is that turbulent multi-phase flows are intrinsically multi-scale. They result in clustering preferential concentration. The challenge is that we want a single-point theory for ease of modeling, but the two-point statistics are needed to characterize fluctuations in particle and fluid volume fraction. So, that's what makes the theoretical treatment of multiphase turbulence uniquely challenging. And it really has not been satisfactorily addressed. Been satisfactorily addressed from previous statistical theories. We do have Drew's one-point theory from 83, which is perfectly reasonable theory, but it doesn't capture the 2.5. We have a 2.3 from Collins and co-workers in 1999, but that again is a full-blown 2.3. So you have to go to a six-dimensional space and it's not very tractable for industrial devices. Since then, we've seen some work by Sundarayson's group on coarse graining to fluid theories, which has been very widely. Which has been very widely used in industry, I think. And then there's also the Fox mesoscale theory from 2014, but that kind of starts off at the mesoscale. And I want something that goes all the way from the micro scale to the macro scale, because we want to reconcile with particle resolve DNS at the micro scale. So let's take a look at filtering, which I'm going to, I told you at the beginning, a little preview is that filtering might help us here. So we look at a quantity. So we look at a quantity phi bar. So what we take, we take this indicator function and we filter it. So f script df is some filter function, which is a homogeneous function. It depends on the separation x minus y, characterized by some characteristic filter width delta. So if I take my indicator field and I filter it, I get these nice smooth fields, which you would expect. In this case, I believe this is a Gaussian filter with some filter widths. And what I'm curious about is I know the filtered field has some information from Some information from away from the filter point because it has stuff at y away from x. So, my question is: can filtered fields give us some information about the two-point statistics that the standard fields don't? And we know that, as I said, single-point limit of the two-point covariance does not show the signature of clustering. So, the question is: could the single-point limit of this two-point covariance formed from the phi bar reflect clustering? And so, we would play the same game on. So, we would play the same game on phi bar. So, we take an ensemble of realizations, we filter each of them, and then we form the ensemble average on phi bar. So, we want to form C, and the notation here I'm going to use is P bar, P bar, meaning that each particle field has been filtered, and then I'm forming its two-point statistic. And this is, of course, parameterized by this filter with delta, so it will kind of change as I change delta. So, that's something to keep in mind. So, it turns out that there's a very simple manipulation that we can do. manipulation that we can do for C P bar p bar we write out its definition expectation of the filtered fields product of them we have to be careful to do the filtering over y in and over z space in the other one but what is interesting is because these two operations commute that is filtering and expectation both being integral operations you get a nice answer that says C P bar P bar is nothing but the C P doubly filtered and so it just says I can take this into the expected Can take this into the expectation inside from CPP, and then I get a double filter. So, in the homogeneous case, this simplifies further, and it shows me that, in fact, that the P bar, P bar does pull in two-point information due to the filtering operation. And so this gives me some hope that maybe I can use filtered fields to actually get signatures of clustering. And we look at the zero point, the zero r equals. The zero point, the zero r equals zero limit, and we can show that, of course, this is not the mean volume fraction. And in fact, it contains information from points which are distant from zero in CPP. So that's very promising. And if you actually plot this quantity, that's this quantity CP bar, p bar, the red line, for a particular filter width. I see that obviously it doesn't go to 0.2, but this difference is to me, This difference is to me in some sense a signature. The departure can tell me something about clustering. And of course, depending on as I vary the filter width. So the key result, central result here, which is pretty obvious to me once I've first found it, is that the single point limit contains information about clustering at the filter scale. So the question is: now, can I use this to develop a theory which would be able to explain clustering? Explain clustering in a single point representation. And I think the answer is promising. We can analyze turbulent multiphase flows, which are introduced in multi-scale using filtering. And in fact, it's not just numerical expedient because this filtering has led to a remarkably successful volume filter Euler-Lagrange formulation due to Capisolatro and Desjardins. But it can actually lead us to a new theory of multiple turbulence because of these following points. Points. So we can build models for single-point closures, C-Dragon heat transfer, but they will now be a scale-dependent. They'll be a function of the scale-dependent single-point metric of clustering, which would be the filtered volume fraction variance. And we can also look at its two-point counterpart, which can maybe explain the mechanism of scale-dependent clustering in cluster-induced turbulence. So, first, I'm going to just show you what this filtered volume fraction variance evolution looks like. And it looks remarkably like what you would see. It looks remarkably like what you would see in any turbulence problem. But so, first, some definitions, we have to be a little careful how we define this quantity, which I call the true fluctuation, which is both random and centered. So the true fluctuation phi check here is nothing but your filtered field minus the ensemble average of the filtered field. So it's filtered minus ensemble average of the filtered quantity. And if we take the two-point covariance of this true fluctuation, so phi check at x and y, it's a It's a centered quantity, it's centered compared to CPP, which is a covariance. So, this we call covariance function of the filtered field. And so, this I call Kp bar P bar, which is really C P bar P bar minus this piece that is making it centered. And its single point limit will give us the filtered volume fraction variance. So, we can derive an equation for this volume fraction variance, filtered volume fraction variance, and this will give us a measure of clustering at the single point level. Of clustering at the single point level, and it has very typical, you know, it has a mean flow convection piece, it has a triple correlation, very similar to what you see in turbulence, it has a production piece, a transport of flux, but it has some additional terms. It has a non-commutative flux term, which arises because of the difference between these quantities, mean UMP and mean UP bar. It has a change of measure term due to the fact that. The fact that mean UP bar is not divergence-free, and then it has some boundary terms because when you do filtering and you hit boundaries, you have to be careful to account for the boundary effects. So it's not exactly like turbulence, but it has some additional terms. And I believe that this could be a useful way to characterize clustering at a single point level. It also explicitly depends on the filter width, which is attractive in a sense because, you know, depending on what size your clusters are, you'll be able to dial that in and out depending on. Dial that in and out depending on how you choose your filter width. And this will typically augment a mean and second moment equation for filtered quantities in the particle and fluid phase. So if you're looking at an industrial size reactor, you could actually transport this quantity and it would give you an indicator of clustering. What is more interesting, I think, is to look at the two-point version of that because we can use it to analyze the DNS data. So the question I want to ask is: can the evolution equation for its two-point covariance function? Two-point covariance function helps us gain a mechanistic understanding of clustering. And towards that end, we look at this quantity, which is phi-check at x, y. In a homogeneous case, I can zero out the x and make r equals y minus x. And then I can call this quantity in a Monin and Yaglom type notation a B, B Phi check, phi check. So we can ask what is the evolution of B phi check, phi check, and it evolves in this separation space as a function. In this separation space as a function of time, and it has certain characteristic quantities, correlation of phi-check with the velocity fluctuation in the particles. And so at so the two-point correlation, velocity and volume fraction fluctuation, volume fraction fluctuation at origin versus in the velocity fluctuation at the distance. So you see these symmetric terms, and then you see triples. And then you see triples. So velocity fluctuation at zero, and then the fluctuation at R, and the velocity, volume fraction, fluctuation at the origin, volume fraction, fluctuation, R, and the velocity, and a symmetric term like that. Now, what is very interesting is this looks very similar to the predecessor of the Carmen-Hawarth equation in turbulence, single-phase turbulence, but it unfortunately doesn't lend itself to a simplification like an F and G, which you see in Carmen-Howard, just two scalar functions. Just two scalar functions, that simply is not possible because this is not isotropic. We want to apply to anisotropic problems, and the velocity field is also not solvable. So we cannot use some of Chandrasekhar's results or axisymmetric turbulence either. So we kind of have to deal with this messy equation. We can't just go down to simpler functions like f and g. Nevertheless, we can use this equation to form budgets from data. From data. And so, my summary here is that this key result tells us that maybe we can look at two point statistics of filtered fields. And next step would be to analyze the CIT data and even the point particle DNS data from PK Young has recently been generating some of this data for inertial particles. So, we hope to quantify the two-point statistics and form budgets of this equation. And hopefully, that'll give us a more mechanistic understanding of why. Mechanistic understanding of why clustering happens. And hopefully, it's the basis for a new theory of multi-phase turbulence that lifts some of the limitations of the earlier theories. And why I call this unifying is because, oh, and before I move on, there are also experimental data that we can use to analyze. So here are some experiments done by Filippo Colletti and his student Fong in 2021. These are particles falling in a duct. And again, And again, you see formation of particle structure clusters falling. And they've been able to extract some of the velocity and concentration statistics, which again, although these are not fully two-point quantities, the measurements are not two-point. They're kind of frozen measurements. But there is some hope that we may be able to analyze these to extract the budget of that two-point equation. Now, to give you a kind of brief history landscape of where this fits in, if we look at filtering, you know, Anderson and Jackson introduced this a long time ago in 1967 and then later in 97. And this led to the famous two-fluid simulations. And then Drew in 1983 gave us an ensemble averaging approach of the two-fluid theory. And the kinetic theory kind of was more at the mesoscale, which is a good theory for collision. Which is a good theory for collisional multiphase flows. And Sundaram and Carlis in 1999 gave us a two-point theory for statistically homogeneous flows. And now the goal here is, you know, we now have particle result DNS like Luca talked about. We are well established now for drag, pseudo-turbulence, et cetera. So the question is, you know, we really do need something that will connect all the way from the micro scale to the mesoscale and connect random freedom. Scale and connect random fields and the filtered approach, and you know, show us clustering. And so, you know, we've seen, as I told you, Sundarayson's mesoscale structures, he showed that in 2001, that when you run these two fluid simulations, you do develop structures at the mesoscale. And then that motivated his development of filtering these two fluid equations. And at the same time, you know, around 2012, we've seen Tiapa Silatro and Dejardan. Capisolatro and Dejardan developed the volume-filtered VFL method deriving from Anderson and Jackson. And 2014, we've seen turbulence theory from Rodney Fox using a sequential phase and Reynolds averaging approach, starting from the meso-scale. So where this kind of fits in is brings in the Sundaraman-Collins two-point theory through filtered fields. And then I think that it can relate very nicely to both the Sundarasan and Fox theories, as I'll show in a second. Fox theories, as I'll show in a second. So, one thing is if I just take my filter to be a delta function, I can recover Sundaram and Collins' homogeneous two-point theory, which is really nice. I can also recover Drew's single-point theory because that's just a special limiting case. And in fact, if we look at Sundarayson's coarse graining, this theory will give us what I call the missing random fluctuations in that, in his course screening approach. So, after filtering, what happens is we get residual terms. And these residual terms. Get residual terms. And these residual terms can arise from either random fluctuations or they may be non-random unresolved fields. And this true fluctuation that I talked about, which is the filtered minus the average of the filtered, actually captures both the randomness and the under-resolution. Whereas if you look at the two-fluid fluctuation in so the Racence theory, it's actually the average that's filtered, and then from that the average is subtracted. So this captures a non-random. This captures a non-random under-resolved fluctuation, but it doesn't capture the random part of that under-resolved fluctuation. So, in some sense, if you add these two, you get the true fluctuation. And so, that's kind of an interesting result, I believe. So, the true fluctuation also, yeah, it's random and it's so its expectation is zero and it's also centered. So, that's a nice feature of it. So, going ahead, I think that. I think that I'll skip ahead to my final slide. It incorporates signatures of clustering for two-point statistics at the single-point level, fully reconcilable with representations of the micro, meso, and macro scale. And we can extract closure models directly from the DNS. But this will require very large particle-resolved DNS because these clusters are often 90 times the particle diameter. So the domain sizes have to be very large. And the closure models will be filtered and And the closure models will be filter and filter width dependent, and new closure models will be needed. The mathematical structure with these equations still needs to be explored, either hyperbilic, et cetera. But the numerical implementation should be easily accomplished in standard multi-phase RANs and multi-fluid fluid model solvers. So I'll leave you with those summary and conclusions, and I'm happy to take any questions that you may have. Thanks for your attention. Thank you, Professor Subramanyam. Excellent talk. I'm pretty sure there are going to be lots of questions, despite the fact this is the last talk of the day. So let's see. Okay, Charles. I want you to be wrong. Yeah, really interesting. I guess I was looking at this from the standard LES point of view, where indeed. Where indeed, I guess there's an analogy with the kinetic energy of the filtered velocity field in LES, where that kinetic energy has a power law dependence on the filter size. And it's some power law, two-thirds, I think. And that information is equivalent to the Kolmogorov spectrum. So there you have an example of dependence of the filtered single-point variance of a filtered field that does provide you indeed two-point information. Provide you indeed two-point information. But I think the caveat is that you need it at various filter scales. In other words, the two-point information is contained as, you know, how does this vary as you vary delta? So, whereas I got the impression that you're proposing this at sort of a fixed filter scale. So it seems to me to get really information about clustering, you probably need a description as a function of filter scale. And maybe as a color area of the question. Maybe as a colorary of the question, a lot of these clusters sometimes have power law and fractal type distributions. So I was wondering whether you would expect sort of a power law dependence of that single point variance of I as a function of filter scale. It's a long question, sorry, but it's a long day. Thank you so much, Charles. That's a fantastic question. And indeed, you're right. I mean, so the choice of the filter weight is important. So, the choice of the filter width is important, you're right. And that's something that I think the preliminary analysis of the DNS data will, I'm hoping, shed light on what that choice should be. Because as the filters, as the clusters grow in time and become larger, you're going to have stuff information moving from subfilter to the result scales, and that's going to be critical. So, I completely agree with you. This is still a nascent. This is still a nascent idea, but I appreciate the feedback and the warning. Yes, great, thank you. All right. Let's see if there is any question on Zoom. I think my Zoom audience probably is here. So Shankar, kind of following up from Charles' questions about, you know, if you have, let's say, two different filters. If you have, let's say, two different filter scales in the single-phase LES by using two different filters, there was a very nice way of closing subgrid closures using Germano identity, right? So is that something like, let's say, you had one filter that was, let's say, right below your cluster size and above cluster size? Could you get some information through that? Absolutely. That is the hope. Right. Thank you. Yeah. Right, thank you. Yeah, yeah. So, rather than explore the whole range of filter sizes, you're right. So, it should, but of course, as a caveat, I think in single phase LES, we have some stronger assumptions that we can make about the structure of the subfilter velocity field. Whether we can make that in the multi-phase context is yet to be seen. But absolutely. Yeah. Yeah. I should point out that the analysis of these statistics from the DNS data is itself very computationally intensive at this point. And part of our effort has been going into developing very efficient ways to compute these statistics. If you just do a sort of blind calculation, you can see how rapidly it'll get out of hand. Of hand. But there's some hope of doing these very large data sets using more particle-based techniques rather than the grid-based techniques. So, and one of our jointly sort of advised student with Jesse Capisalatro has a talk upcoming at APS where he has some hope that we can get these statistics much cheaper. And that will be very important to answer the kind of questions you're saying. Yeah. Whether we can interrogate it at two different filter widths. Interrogate it at two different filter widths and infer a model that's based on some universality of the sub-filter motions. Thank you so much, Shankar. It might have been an equally long day for you too, but you accepted our invitation at a very, you know, thank you for that, finding time on your busy schedule and giving a very exciting talk. So we are going to be closing the day one of our births workshop tomorrow. Workshop tomorrow. Focus of most of the talks will be on about data-driven models, AIML, type of approaches that have been used in multi-scale, potentially multi-phase systems also. So I would look forward to all of your attendance tomorrow also. It's going to be a pretty exciting day of some of the talks that we have listed for our day two of the BIRS workshop. So thank you all for participating today and we'll call it the close of the day one. Call the close of the day one and enjoy your day. And we'll be heading out for the tour of the beautiful city of Oahaka. Right, take care. Bye-bye. Thank you very much. Enjoy.