For inviting me to this conference. And so today I'm going to speak about some of the new, the last works we are doing here in Tolvergada. And so, in particular, the work has been done by me and in the group of Luca Piferale. Also, I would like to acknowledge Fabio Bonacorso and Patrizio Clark Tileone, that now is at Buenos Aires University and has contributed a lot to these results. So, the main idea of this work. So, the main idea of this work is to try to understand the potentials of the new data-driven tools to try to solve some non-trivial problems in data analysis of complex systems. In particular, we are considering turbulent flows. Of course, when I'm talking about data-driven, the new data-driven that looks more promising are the ones that come from artificial intelligence, so machine learning tools. And the kind of problems I would like to discuss. Problems I would like to discuss, at least today, even though we are considering even more broadspread spectrum of problems, are the reconstruction of partial information. So, when I'm talking about reconstruction, I'm thinking about a system that we can measure only locally in space. For example, let's imagine we have a velocity field that we can measure with sensors that are spread everywhere in the volume. And then we want to understand what is the velocity field, even in the regions where we don't. Field even in the regions where we don't have access. So, in some sense, we want to reconstruct the missing informations that with the information that we don't have in the measurement. On the other hand, we also consider another problem, which we call classification problem. Classification is a little bit taken from the machine learning jargon, but what we mean is a similar problem. So, let's imagine we can measure we can observe a system and our observation will be limited anyway. Observation will be limited anyway. And out of this partial measurement, we want to understand, for example, what is the physics of the system. So, classification in the sense of understanding inferring parameters of the flow, for example, Reynolds numbers or other properties, as I will discuss more in detail in the sequel. So, these are the kind of questions I would like to discuss. And as you can understand, these are really Really application-oriented in some sense because we have satellites that are observing the Earth, and every time we do an experiment, anyway, we will have a limitation in our observation capabilities. And we will always need to have a reconstruction, or we would always like to have a better understanding of the system. So, even though this is true, indeed, our first idea when we started to play this game was not only really application-oriented. Only really application-oriented. So, we didn't have in mind a particular application where we wanted to improve our data analysis or our data acquisition ability. But instead, we were also interested, so first of all, in trying to assess the potentials, as I said, of these machine learning tools. And second, we were also interested to try to use these tools in a reverse engineering approach, try to understand some more theoretical questions. Like, for example, let's imagine that now we have. Let's imagine that now we have a tool that is good for reconstructing a velocity field or for inferring the physics of a system. Then we can try to change, for example, the input data that we give to this tool. So the kind of measurements we give. We can measure a flow close to the boundaries or inside far from the boundaries or spread everywhere or maybe more focused on a region. And we can, depending on the question, on the features of the flow that we are interested in reconstructing, we can. In reconstructing, we can try to understand what are the degrees of freedom that are more important. So, somebody is not there. Excuse me? Okay. So, yes, the question is trying to understand what are the degrees of freedom that are more important in the system. So, what is our approach? Our approach is based on direct numerical simulation. So, as we said, we wanted to use a data-driven tool. So, we need to produce Data-driven tools, so we need to produce data and we choose simulations. Simulations allow us to do two things. First of all, first of all, we can choose the system setup. We can play with the equations, we can change forcing mechanism, we can, for example, here I have written a forcing that acts locally in space and time. So we can simulate many different things, many different systems. And on top of that, we can also, the simulation gives. Can also, the simulation gives us the opportunity to know exactly what is the ground truth, what we call the ground truth that we want to reconstruct. For example, let's imagine we have a system that we want to deteriorate a little bit. We have a simulation, we can remove some of the information we have simulated, and we can try to evaluate these data-driven tools to reproduce the system in a better way. And because we know during the simulation stage exactly what is the property of the field that we have simulated. The property of the field that we have simulated, we can quantitatively measure the properties of, so the potentials, in some sense, of these data-driven tools in reconstructing the fields. So the first problem I want to discuss is what I was calling a classification problem, so inferring the physics of the system. And the case of study that I want to consider is turbulence on a three-dimensional space and under a rotation. Space and under a rotation on a rotating reference frame. So it means that we have Navier-Stokes plus a Colliery term. So this is a simpler case when compared to the first set of equations, even though this is still a very complex system. So the equations are a little bit simpler, so it's easier to simulate this flow, but the velocity field of such a system is extremely complex, as probably you will already all know. And indeed, what we have is And indeed what we have is that when the Coriolis force is added to the Navier-Stokes equation, we have that the energy that is injected at a given scale in our system, it does not only go in the so-called direct cascade, so from the filter scales toward the smaller scales, but it also experience a dual cascade of energy. So simultaneously, the energy goes forward, the direct cascade, and also backward in the inverse cascade. Backward in the inverse cascade. So the energy is distributed from the forcing scale towards the larger scales and the smaller scales simultaneously. The fact that we have this inverse cascade gives rise to a very particular field. If we look at the velocity field here, there is the formation of these large vortex structures that are also strongly energetic. And also this inverse cascade breaks a symmetry of the field, which is now almost constant in the Z in the in the in the z in the in the in the in the direction of the of the rotating velocity um and then at the same time so we have this sort of two-dimensional system which anyway develops all the three-dimensional intermittent features that we know that are generally observed in a in a in a three-dimensional turbulent flow and indeed we have uh statistics which at small scales become strongly non-Gaussian how this uh forcing so this Codiolita forcing this Collioli term changes the dynamics is not is non-clear and it is also more interesting if we think that this is not it is not it's not enough to add the Coriolis force such as to have this split cascade regime but actually the rotation rate of the reference frame must be strong enough such as we have we see the effect of the rotation on the flow so in some sense we have a sort of a phase transition whose nature is not even so clear must be Nature is not even so clear, must be should be even better characterized that we observe. And so, if rotation rate of the system is small, we see a flow that has a sort of three-dimensional behavior. So, we see that here it's a plane, a two-dimensional plane of the velocity module. We see that there are only small scale features. And then, at some point, increasing omega instead, we start to see that the flow starts to develop these two-dimensional behaviors. And then, indeed, we see if we. And then, indeed, we see if we look at the energy spectrum, that in the direct cascade case, we have that the energy is not accumulated at large scales, while after the transition to this new behavior, we see that the spectra have a lot of energy also at smaller scales. So, while it's very easy, looking at the energy spectrum or even looking at a simple snapshot of the velocity field, to understand in which side of the transition we are, it is much more Transition, we are, it is much more complicated if we want to distinguish the right value of omega if especially the two omegas are in the same side of the transition. So, if we want to distinguish omega 6 from omega 8, this becomes a problem that is extremely complex. And in particular, as we can see, there is no trend in the energy spectrum. The field looks extremely similar. And even looking at more complex statistical quantities, like for example, here I'm plotting. Like, for example, here I'm plotting the eighth-order moment of the velocity field versus the fourth-order moment. We can see that these statistics really is very well splitted looking at direct and inverse cases, but looking inside the same regime, we see that different omegas have statistics that is extremely similar. So, the first question we want to ask to one of this data-driven tool is: is it able to understand? Is it able to understand what is the rotation rate just looking at one of these, analyzing one of these planes of velocities? To try to answer this question, we set up the problem in this way. So we design a neural network whose goal is the following. It has to analyze in input one plane, one single plane, two-dimensional plane extracted from a three-dimensional simulation on a rotating frame. We used 10 different omega values from below. Omega values from below to above the transition. And then we asked the network to give an input exactly the rotation value that was corresponding to the simulation that has generated that particular frame. So in particular, this neural network has to map the input space of the two-dimensional plane at a given time. So there is no time information and we only have a velocity module. In output, we want to have a prediction for the To have a prediction for the rotation of the reference frame. So the network doesn't know anything about the physics. We don't have to implement anything that tells to the network what are the Navier-Stokes equations, the fact that there is a Coriolis term. We don't know anything, of course. So the only way we can ask the network to reach this goal is providing to the network a lot of data. And to do that, we have built up a training set composed of 200,000 different two-dimensional planes. Two-dimensional planes. To each of these planes, we know from which simulation it has been obtained. So we have a label that is the corresponding omega. And then we ask the network to optimize its weights such as to minimize a loss function that is exactly the difference to the square of its prediction of the rotation value minus the true rotation value. And we see that indeed the network is able to minimize, to at least minimize this loss function as a To this loss function as a function of the training. So here we have the loss as a function of the epoch. The epochs are one epoch is when the network has seen all the eight hundred thousand planes of the training set. We see that keep repeating this analysis many times. We see that actually the loss function decrease of different order of magnitudes, and that is reflected on a better estimation or better prediction of the network of the rotation value. To give you just an understanding of what I Give you just an understanding of what happens here. We see the PDF of the predicted values versus the predicted values normalized to the true value. So, when this PDF is one, if we have a delta to one, it means that the prediction is always correct and it's always equal to the true value. Here we see that at the beginning, at the epoch 50, we see that there is a still probability that is quite large. And then we see clearly that continuing in the training, we have a probability that becomes very sharp and very peak. Probability that becomes very sharp and very peaked. So, this is the first result that tells us that actually this network is working with some error. And to give you, to try to understand how actually this prediction is, we tried to find a different data-driven tool that could give us a similar prediction of the omega value, starting from a similar approach. So only considering a two-dimensional plane without time information and only the velocity module. Time information and all the velocity module. And the other possible way we had was, for example, to have an estimation that came just from a simple standard Bayesian inference. And so what is the idea here? The idea is that we can have an estimation coming from bias that depends on a measure of a particular observable. Let's imagine that, for example, here we consider, we explore the different observables, and then we found that from actually one, the best results were obtained. Actually, the best results were obtained looking at the mean squared velocity coupled joint together with the value of the mean squared velocity gradients, where the mean was obtained on space over the plane. So we can take one field, we can make this measurement of the velocity module square and its gradients, and then we can imagine that we know the probability, the conditional probability of having a particular omega value once obtained. Omega value once obtained this result to this measurement. So, all the goal is to have this evaluation for this conditional probability that we can obtain using the bias theorem. And so, we can measure the probability of having omega once having this particular result on this observable just calculating the probab the inverse pro conditional probability, so where we have the probability of having such measurement for each omega. Measurement for each omega. And then we can use the bias theorem to have this estimation. And in particular, why this is easier, because this particular probability here, the probability of having a particular measurement for each omega, we can compute numerically out of our training set. So we took our 800,000 different planes. We have made our measurements for each simulation of each of the 10 omegas. Of each of the 10 omega values, and we have estimated this probability. So, in some sense, then in this way, using the bias theorem, we could have another prediction for the omega value that is just this following the Bayesian approach. So, now to compare a little bit, to give you the feeling of how the the the machine learning was indeed able to improve uh the standard uh Bayesian inference, here I'm showing a few examples, just s six examples. Just six examples of the rotation planes that have been analyzed. Here, there is the first rotation value of the simulation, so the true value. Here, we have the neural network estimation and then the bias for each of these examples. So, you see that even in all cases, there are errors, but anyway, the machine learning, at least neural network, was always better to give an estimation that was closer to the real value. And sometimes, I mean, you can see in this case. Sometimes, I mean, you can see in this case, the error was extremely small. And also, in the case of only direct cascade, the neural network was very close to the true value. So, to be more quantitative, here I'm showing a scatter plot of the predicted omega values versus the true values. So, here on the diagonal line, we have the exact predictions. If all points are on this line, If all points are on this line, the network or bias is perfect. What we have here is that these hexagons are actually the mean values over the prediction of the full planes. And we see that actually, so the neural network on average is able to give a prediction which is very close to the true value. At the same time, bias is also not too bad in some sense, it's able to understand that there are differences between these. uh there are differences between this these uh at least between the the the the extreme values of omegas but in the intermediate range we see that bias is sometimes completely not sensitive to to the different uh rotation values so here i'm showing the mean value and also uh these are the distribution of of the prediction with the size of the symbols that are proportional to the density of points so in some sense we can have a we can calculate a PDF We can calculate a PDF for all of these predictions and compare for each omega the prediction obtained by the neural network and the bias. So, this is what I'm going to do here to give you a little even more quantitative comparison. And here, so I'm showing for each of the 10 omegas used in this analysis, I'm showing the PDF of the predicted values by the neural network, that is the solid line, and the bias, which is the dashed line, also always normalized. Also, always normalized, so the predicted values normalized to the true value. So the red dashed line here would be the perfect prediction. So I would like to distinguish, maybe it's important to distinguish two different regimes, I think. So when we are in the direct cascade, we see that actually both approaches are not so good indeed to they make quite big errors in the prediction. Even though this is understandable, because as you can see, the Understandable because, as you can see, from these examples, as you know, the effect of rotation is minimal in the dynamics. So it is indeed very difficult to have a correct quantification, so regression on the right omega value. Anyway, I mean, we see that the neural network is always as a mean value of this PDF that is always closer to the correct prediction. Then there is the extreme case where instead we are on a rotation rate that is very large. Rate that is very large. We see that both techniques look quite good, even though, again, so they are quite peaked around one, even though again, also here the neural network is better. And then there is this intermediate range of omegas, where we see that indeed the neural network is able to improve much better the prediction that we can obtain otherwise with the standard Bayesian inference. And especially for 15. And especially for 15 or 7, we can see that bias is really pointing towards the different omega values, while the network is always good to give the right value. So this is a first result which tells us that these tools indeed can be interesting, can be very powerful in some sense, and can give us access to data analysis ability that we were not able to get otherwise with the standard. not able to get otherwise with the standard statistical techniques. So now I want to also to show a little bit and discuss a little bit the second part of the second part of this talk, this second problem of the reconstruction. And I'm always considering the same case of study. So I'm always considering the turbulence under rotation, but now I'm not interested in understanding what is the right omega or the omega used in the simulation. But let's imagine we fix also omega. And instead, what we want And instead, what we want to do is we want to reconstruct part of the velocity field that are missing in the measurements. So, to be more precise, again, we are going to consider only two-dimensional planes extracted from a three-dimensional volume. And here, we are applying different types of damages. So, let me stress that also again here we are trying to reconstruct a field that we know that is That we know that is out of really a small limited knowledge of the full velocity field, because not only we have a two-dimensional plane, but also this two-dimensional plane is only on a very coarse grid, a coarser grid with respect to the grid used in the DNS. So we imagine to have access only to a very large scale velocity field. So in this way, even if we remove a pixel, this velocity field, if we want to reconstruct one pixel at this point, To reconstruct one pixel at this level, a simple linear interpolation would not work very well because the field is not differentiable at these scales. So the field is already difficult by itself. And then on top of this, we want to study what happens when we apply a damage that is extreme, that is quite large. Like in this case, we see that the amount of area that we remove is very important with respect to the characteristics scale also of these large scale vortices. And then here we also tried to play this game. So we always keep the same fraction of missing information with respect to the dimension of the system, but we split the damage in different shapes. Such as we want to understand if the machine learning is able to reconstruct, is sensitive in some sense to the structure of the damage that we have in our data. So we could expect that probably here we have much larger scale. Much larger scale information that are missing. And so, probably the network should be more, it should be more difficult for the network to reconstruct the velocity field in this case than in this case. So, we want to understand how much the network is actually indeed sensitive to the kind of information we give input, such as we can try to understand if in future, I mean, we can try to use this system such as to evaluate the quality of data that we are giving in input and so trying to understand what are the degrees of freedom. Understand what are the degrees of freedom, as I was saying, that are more important in order to have a better reconstruction. So, how can we try to do this, to play this game, to reconstruct this field? Of course, now the problem is more complex because we don't have to infer a parameter, but we need to generate data. And this can be done with this following this approach of the generative adversarial networks that are also called GAN. And these are networks that are more complex. I just want to give you a little bit of the idea. Want to give you a little bit of the idea here, and so what happens is that we have the network works in this way: we have two different networks: one is called the generator, and the other is called the discriminator that are trained in an adversarial way. So, the generator network has the goal to take in input the data with the missing information and to reproduce in output the missing data. So, to actually reconstruct the velocity field that we don't have at the beginning. So, this is the real. Beginning. So, this is the real generator of data. The second network is the discriminator. The goal of the discriminator is to evaluate the quality of the generated data, because now we need to understand how good the reconstruction was. And this we do using not a simple loss function, but also using actually a second network. Use goal is the one to distinguish whether the data were taken from the original measurements that we have to know at the beginning during the training stage or were generated by the. Or were generated by the network. So, and this non-trivial adversarial training system is able to give us the results that I will show you now. So, first of all, as before, I want to show you what happens during the training is evolving. At the beginning, we see for this particular example that the network is not able to produce any realistic reconstruction of the velocity field. Of the velocity field. It looks almost noise. But then at some point, we see that the network learns that here, in this point of the volume, the velocity field should have a large scale structure. This is already a non-trivial result, because as you can see from the damaged image, the large-scale structure was completely missing. So the network has learned that here there should have been a vortex just looking at the frame. Looking at the frame, and then we see that going ahead with the training, we see that also the small-scale statistics are improved and they start to look very similar to the original field. To be more quantitative, here I'm showing the a comparison of a reconstructed with what we call the ground truth, so the original data coming from the DNS. And here I'm showing a velocity profile along this vertical line for the original field. For the original field, that this is the solid line, and the reconstructed field in the dashed line here. And as we can see, also in the case of this large gap, this large damage, we see that the neural network was able to reproduce at least the large-scale feature. And then we see that indeed, splitting the damage in smaller regions, we see that the network was able to produce an error that was better and better. Error that was better and better as we expected. So it actually shows that it was sensitive in some sense to the type of damages that we are applying to the image. Here I'm showing the error normalized to the energy of the flow measured over different thousands of images that were not used during the training. And we see that even in the case of this large gap, we have an error that is of the Gap, we have an error that is of the order of 0.15, and then this error decreases as we were expecting when the gap becomes smaller and spread everywhere in the volume. Of course, 0.15 doesn't mean too much at this level, but what we did, and it's well described in this work, in this paper, we have tried to compare this reconstruction with other tools, also with equation-informed tools like matching, for example, where we use the equation of motion, the knowledge of the physics to reconstruct. Motion, the knowledge of the physics to reconstruct the missing information. And what we have seen is that even in that case, we couldn't obtain a reconstruction with an error that was better than the one obtained with this approach. This is, of course, an error that was estimated on average over the volume. Here, I'm showing also the PDFs of these errors. So, I'm taking the pixel-wise errors measured over different configurations, and here I'm showing that actually the mean values is the mean value. actually the mean values is the mean values are representative for the for the statistics of these errors because we have the pdf is quite peaked depending on on the on the size of the damages here the red line is for the of is for this damage the black line is for the the the large gap we see that they are all peaked and uh also in the case of the large gap we have an the error that is of the order of the of the energy only for a point every uh more than one thousand so this is a 1000. So, this was quite an encouraging result. And then, just to give you the idea, we can even be more quantitative because once we have the field that is reconstructed, we can go and measure all the typical turbulent quantities that you know are interesting and complex in a turbulent flow. Like, for example, the energy transfer, which is a non-linear quantity that depends on the velocity gradient. So, it's strongly a non-trivial quantity. A non-trivial quantity that has indeed is characterized by a heavy-tailed PDF, non-Gaussian. And we see that actually, and if we measure this quantity only inside the reconstructed region, we see that the statistical features of the produced reconstructed data are very similar to the ones of a real turbulent flow under rotation. And this is true also if we look at higher-order statistical quantities like structural functions, we see that. Quantities like structural functions, we see that the generated data satisfies on a statistical sense the right turbulent properties. So, there are many more questions that we are trying to ask. Also, the ones that I have discussed now were mainly focused on a Lagrangian point of view. We are also trying to use other artificial intelligence tools like reinforcement learning to study, for example, how to drive particles autonomously in a Autonomously in a turbulent flow to achieve non-trivial goals, like for example, finding the emitting source of a pollution or of a concentration field or other things. And what we have seen is that actually in more or less the different cases that we have studied, the reinforcement learning or so the artificial intelligence tools in general were always able to find a solution, non-trivial solution to the complex questions that we have been Complex questions that we have in turbulence. So, in some sense, we didn't have up to now, we were not at the stage where we could say that we have understood some of the fundamental questions of turbulence through these new tools. But what we have seen is that these tools can actually, probably in the future, probably give us some more insights to such complex systems. Thank you. And I leave you here with. And I leave you here with some references if you want. Thanks, Michele. So, I don't know if there is just one quick question or we can move them to the discussion session later on. I have a quick question considering the considering the the reconstructing the reconstruction of the fields I was wondering if you need some can I say some scale matching between the structures that you want to reconstruct and the portion of of the plane in this case that was missing so so it seems it's clear that it when you have the smaller When you have the smaller portion, even if then the total area is the same, but you split them in smaller portions, it's clear that then since these portions are much smaller than the part of the volume you want to reconstruct. Yeah, so in some sense, what we have seen is that the size of the damage is somehow connected with the size of the somehow connected with the sign of the with the side of the structures that you are able to reconstruct in this in these uh in this um approach so in some sense if you want to reconstruct the very the very small scales looking at the at the gap that is so large it is difficult at least it is difficult uh doing it uh exactly with the with the solution that corresponds to a one-to-one pixel comparison while their statistics in some sense is good so in a statistical sense you can have a good reconstruction also at scale A good reconstruction also at scales that are much smaller from the damage, you don't have a one-to-one reconstruction. While on the other end, it's easier to reconstruct, of course, if you have damages that are spread everywhere in the volume, to reconstruct structures that are at smaller scales, because in some sense, you are able, you can perform a better reconstruction. You can see them better, you can see where is their position better. So, I don't know if it is a So, I don't know if it is a but what is interesting, I think, is that if you so if you're interested to do a one-to-one reconstruction, yes, it's important what we have seen to have to have a damage that is smaller in some sense. So you have a connection between the size of the damage and the characteristic size of the structure. But what is interesting and what is important, I think, to stress is that if you want to reconstruct on the other end the smaller scales on a study. Smaller scales on a statistical sense, with this approach, you can do it well. So, you are really not too much connected to the size of the damage. So, you can reproduce a field which looks like a turbulent flow. Maybe it's not exactly this particular realization, but its statistics is very similar. And this is something that is not common with other reconstruction tools. So, what we have seen generally is that either you are able to reconstruct that particular That either you are able to reconstruct that particular solution or your reconstruction is very bad, or you are only able to reconstruct the very large scales, but then you lose everything about the right statistics at the level of scales that you are not able to reconstruct. Thank you, Michele. And so I think we now have a break and then we recover in half an hour. Thank you. Thank you.