That we can give the opportunity to be the first talk because that means I can get my talk out of the way and then just enjoy all the rest of the stuff that we have. So, I was tasked with giving a sort of physicist summary, actually not of this workshop conveniently, but of a previous workshop that we had, as Olaf mentioned, this remote workshop we had, PhiStat Systematics in 2021. And so, this is just going to be a summary of that workshop. Obviously, I'm not going to be able to go into all the details, but I think we're going to hear a lot about what we're going to do. Hear a lot about what was discussed there again during this workshop, and hopefully, you see some new things as well. So, okay, so just to remind everyone, and maybe for the statisticians in the room, when we talk about particle physics or experimental particle physics, this is really kind of a rough idea of the landscape that we're talking about, the kind of experiments, the kind of areas that we're thinking of. So, my personal area is down here in the sort of energy frontier. Here in the sort of energy frontier section, so I'm on CMS. But as I've mentioned, we have a number of people from MATLAB as well. But then, of course, there's things like the dark sector, so dark matter searches, neutrino physics, logophysics, astroparticle and cosmology, and we have a bunch of experiments all in these different areas. And the sort of key thing is there's many different opportunities for sources of systematic uncertainty. Of course, these experiments are all very different and all subject to very different uncertainties, but also the types of procedures. The types of procedures by which they're included in the analyses are very, very different to fit. Okay, so I stole this kind of analogy from Firestat talk in 2003, but I kind of like it because it gives a rough idea about broadly the categories of systematic uncertainties that we have to live with. So we can kind of think of them as these three groups. So there's the good, the bad, and the ugly. And so the first, the top here are kind of the the easy ones. These are the ones we think of as good. These are the ones we think of as good. So maybe it's something like a calibration answer to maybe you've gone and measured to some degree of precision your calorimetry, how well you can measure certain particles and their energies in your detector, and that calibration will come with some statistical uncertainty just depending on the amount of data that you've used. So those are usually relatively easy to incorporate and we sort of know what we're doing. Then there's the next class which we call the bad. And this can involve things like using other people's results. And I don't mean that using other people's results is bad. And I don't mean that using other people's results is bad, of course. I mean that these are just difficult to deal with uncertainty. So these can typically be things, these things like theoretical uncertainties where some calculation is made and it may not be obvious as to how to incorporate uncertainties in that calculation into your analysis. Then there are things like poorly modeled data or some analysis technique that you may be 100% sure that is completely unbiased in some way. Or there may just be simple model assumptions about the thing that you're using to extract or infer from the data. Or infer from the data, and you know how to actually incorporate those as these quality tributaries. And then finally, there's the ugly things where you really just have a completely different assumption underlying what you're doing. So, for example, a really difficult one are things like cut-offs and calculations of theory when you go from leading order next to leading order, and so on. Anyone's guess as to how to really deal with that properly? There are some very nice ideas, which I think we might hear more about in this workshop, but certainly they. About in this workshop, but certainly these are the really true ones to deal with. So, these are roughly the categories that we have. And then, this slide is just showing, well, it's actually a set of links. So, if you have access to the PDF at some point, you can click on the links here. And they're all the different talks that we have from the different sort of physics areas in the FistTack systematics workshop. So, again, they range from things like dark matter experiments to searches and measurements of flavor physics. And there's also, there was a very nice talk actually on trying. There was a very nice talk actually on trying to deal with uncertainties from theory calculations in a sort of statistical way. So I do invite you to have a look at those links once you get hold of the PDF. Broadly speaking, however, I think this is kind of a nice way to show how systematic uncertainties enter into analyses in general. So this is an example from a sort of Atlas style analysis where we might be trying to measure some physics related to a hard interaction process. So here's just an example of a VBF fusion and a Higgs decaying to two Z bosons. And a Higgs decaying to two Z bosons. And so you really want to get at the parameters, these theta, that actually describe this physical process. But in between this physical process and the things that come out of your detector are a bunch of different effects, like stochastic evolution of the parton shower and things that deal with the PDF of the protons. Then there's, of course, the fact that the particles have to interact with the detector in some way. So you have a ton of systematics that appear there. And finally, you have your detector readout at the end. So in order to actually estimate So, in order to actually estimate your sort of probability of your data, you have to do this rather horrendous integral over all these different terms that appear somewhere in this chain of x's. Olaf told me that there was potentially a mistake in here somewhere. But this P, this P of data, isn't that ZZD? Ah, B of Z D, P or Z D. Okay, the good news is I stole this from Lucas, so it's probably okay good. Alright, so then I'm just going to go through a few different sort of systematic recipes, if you like. Different sort of systematic recipes, if you like, or ways in which we try to include some of those systematics into our physics analyses. And the first one I wanted to start with was by sort of splitting these recipes into two, again, two broad kind of categories. So the first one is what I like to think of as error propagation. So this is the idea that you might have some measurement. Let's say you're measuring something along this x-axis here, and you want to incorporate a systematic uncertainty that might come from changing a parameter. Uncertainty that might come from changing a parameter in your model or changing the model in itself in some way. And one way to deal with that is just to re-derive what that measurement is and then do that a bunch of times and assign something like the root mean square as the uncertainty on that measurement. So this is done often enough that it actually comes with a name. So it's going to be referred to as OPAP or one parameter at a time. And this really helps you sort of understand where certain sources of systematic uncertainty might appear in the measurement that you're trying. Uncertainty might appear in the measurement that you're trying to do. So, this is a rather common method. There are sort of variations on that where instead of just evaluating at different points in the parameter space, you might throw random values that represent that systematic uncertainty and again build up a distribution of your measured value. But often the main sort of outcome is that you end up defining essentially a difference or a width of a distribution of the difference compared to your nominal measurement. Difference compared to your nominal measurements as your systematic uncertainty. Okay, so that's a very common procedure in pathogens. Then, of course, the other way we all know and love is a sort of more likelihood-based approach, where your systematic uncertainties are encoded into your likelihood via nuisance parameters. And then, of course, you can employ constraints or priors from some preliminary analysis. Again, it might be a calibration of a detector or something like that. And then you construct something like a profile-like. Construct something like a profile likelihood or marginalized likelihood when you want to deal with those parameters. So I'll leave a proper discussion of these kind of approaches to Sarah, who can tell you the sort of real statistics behind it, but just to say that for physicists, these are the kind of more or less the two broad categories of dealing with sort of simple systematic uncertainties. So I just want to go through a couple of examples. So for this one parameter at a time, we had this nice talk by Alexander at the Fistat workshop where he tried to explain the Where he tried to explain this example on a measurement of a cross-section. So we'll see where that comes out in a second. But again, just to reiterate, the idea is that you measure some parameter, here I'm just calling the parameters mu i, and you measure them at some nominal value of the parameter, or the parameter of systematic uncertainty, and then you add a small shift to that parameter. And you take the difference between the resulting measurements of ui, and you quote that as something like Something like certainty. And of course, if you have a bunch of different sources, you can do this one at a time. So the J could run over the different sources of nuisance parameters, assuming it's a systematic uncertainties, and the I is running over the different measurements that you're making. So of course, this is very good for simplicity of implementation. You know, it's almost the simplest thing you can do, as long as you keep track of the signs of the shifts of what that means for your measurements. Then there was this MC method that I mentioned where instead of making these simple shifts, Where instead of making these simple shifts, you incorporate some random variable r now, where you're basically building a distribution and you take something like the standard deviation or the root mean square of the measured values as your estimate for our certainty. So you'll notice as I go through the talk, I'm going to raise a few of these things that I'm going to label discussion points. I'm going to list them at the end as well. But one thing that's worth thinking about, or something we think about, is whether just providing something like this simple covariance or just the variance of a single parameter is. Just the variance of a single parameter is enough for actually doing something useful with that measurement later down the line. So that's something that's probably worth discussing. Okay, and here's a great example of that in use. So again, Alexander showed us this example of a differential cross-section measurement. This is looking at bosons decaying to electrons. So we have a Z or a W decaying to electrons. And then this is the resulting correlation matrix that you get from considering those different sources of systematic uncertainty. So, of course, actually measuring these differential cross-sections relies on very well, or very really understanding the correlations across different regions of the detector, different types of events that we're looking at, and all that kind of thing. So, actually building up this matrix is a very important part of that analysis. Okay, so then coming on to that other class that I talked about, so the sort of likelihood-based approaches, now we're really thinking about nuisance parameters, where these nuisance parameters are kind of built into the statistical model, so they're built somehow into the public density. Built somehow into the probability density. And we all know how this works. You basically enhance your probability model to include the new sense of parameters in the parameter set that you have for your PDFs. And so, for example, let's say you have a simple Poisson process. You might want to build in some kind of parameterization of how the mean of the Poisson, the lambda, depends on your nuisance parameters. So, a very common example in high-energy. In high-energy physics, it is to use a sort of log-normal form for this. So you sort of take your nominal value of the plus-a-mean, multiply it by some factor, and raise it to the power, reduce it to the factor. So in some sense, what we're trying to do is actually build up this function where we can evaluate what that term is for our PDF at different values of this Newsen parameter. So the black points here represent specific values of the Newton's parameters. Represent specific values of the Newton parameter, and we want to interpolate so that we know what happens at some unseen value of that parameter. And of course, this is actually just a choice which we can, of course, validate using simulations. But again, just to sort of raise a discussion point, it's a question we have to ask ourselves. What do we do, say for example, if you now want to go to two Newton's parameters, how do you build up this parameterization in two dimensions? You have to have many more points to fill in this space. Many more points to fill in this space, and so how do you extrapolate from this point here? This was a point that Lucas made very nicely in his talk. So, the sort of discussion is: how good is this factorization assumption that we have to use where we just assume that we can split these two effects into two terms? And effectively, how can we test these kind of assumptions when we have not just two parameters, but hundreds of useful parameters? Are there actually decent ways that we can say statistically that we're sort of doing the right thing for this parameterization for a large Okay, then going a bit more fancy than just a simple price on mean, if we now think about distributions that we want to use for our analyses, so maybe histograms or stuff like that, then we have ways of looking at how those histograms change as a function of our uses of parameters. I think Lydia is going to talk a bit more about this in her talk, but there was a nice set of slides by Idindo, if you want to go and look at them, that sort of showed these kind of To go and look at them, they sort of showed these kind of common approaches where we try and construct some interpolation between the templates at different values of the parameter we're interested in, or the nuisance parameter we're interested in, and try and build up a parametric morphing, if you like, between the systems. Now, of course, this doesn't always work. So, for example, if you have very large shifts due to your nuisance parameter changing, you can get these kind of very weird effects. Changing, you can get these kind of very weird effects when you use these kind of linear interpolations. Okay, and then you might think: okay, what about sort of non-Gaussian effects? How do we include those in our nuisance parameter modeling? So, often we're using something as simple as taking a nuisance parameter and then evaluating the effect on our probability distribution or Poisson mean at fixed values such as minus one sigma, plus one sigma of that parameter. On sigma of that parameter, and then we use that to interpolate. And we tend to sort of assume, I sort of naively kind of assume a Gaussian behavior. So we kind of think, okay, if I shift the parameter twice, its value, I'm going to get two times the shift that I saw before. So two sigma is roughly two times one sigma. That's a fair assumption to make. But it's not necessarily true that this holds for all kinds of uncertainties. So again, for example, if you compare, what do I, for this Poisson mean, if I compare lambda with a sort of linear intellectual, Lambda with a sort of linear interpolation versus this log normal interpolation, you can see that the results that you get, as shown by Alexander again, can vary quite differently in terms of the relative difference that you're getting for your, say, your PT or your differential cross-section in PT. So this is actually a very important thing to say. This is not saying, you know, have I understood the source of systematic uncertainty? You know, that might be very well understood, but the way that it's incorporated into the into the analysis is is very important actually for this particular case. Actually, for this particular case. So, again, a discussion point might be something like: should we sample many different parameter values to build that parameterization up and not just include sort of ad hoc parameterizations? And are there smarter ways, for example, using Gaussian processes or machine learning methods to actually build these parameterizations in a kind of automated way just that we don't have to sort of plug in fairly simple ones and don't need to worry about this so much. Okay, then Monte Carlo uncertainties. So, this is something which plagues us. So, this is something which plagues us certainly at the LHC, but it's becoming more and more common in other areas as well. So, often we rely fairly heavily on Monte Carlo event simulation to simulate or estimate the probability densities when we're constructing likelihoods. And generating Monte Carlo can be fairly CP expensive, so often we have to account for the fact that we can only generate so many events, and therefore there's some statistical uncertainty that comes with that. Now, these are statistical in nature, which makes them in principle easy to model. Of course, that's not always. Model. Of course, that's not always the case. And there are a nice level of tricks that some people in the room will be very familiar with that we use very often to help us with those kind of approaches. So there was a really nice talk by Christoph at the workshop in Feistat that kind of looked at this, one of the issues that's related to this Monte Carlo simulation statistics, and that's the fact that if you're trying to measure something like the change in an energy scale and how that affects some distribution, for example. Affects some distribution. For example, here's the energy we constructed for an energy scale for the momentum distribution in the T2K experiment. So this distribution is built up using Monte Carlo, and there's uncertainties due to limited Monte Carlo sample sizes. And so what they find is if you try and include a shift in the energy reconstruction of the energy scale, it's not just a case of having variations up and down, but you get events that jump between bins and cause. Events that jump between bins and cause very non-smooth behavior when you try and put it into your delta chi-squared that you're likely. So, what Christophe was looking at was whether or not you can actually assign statistical distribution to the weights of those Monte Carlo events such that you can actually smooth out these curves. So, this is a really nice talk that's worth going to have a look at to see, try and look at another way to deal with this issue of Monte Carlo. Oh, sorry, sorry, yeah. So, here's the version, here's the version without the smoothing. So here's the version, here's the version without the smoothing, where you just take the weights as they are from the Monte Carlo, and then if you include a sort of distribution on these weights, then you can see that you actually end up smoothing the heis weight cursor. Okay, so then another kind of type of nuisance or type of uncertainty that we often have to deal with but are typically very bad at knowing what to do with is several cases where it's not really obvious how to construct a parameterization at all for a systematic uncertainty. So a classic one is For a systematic uncertainty. So, a classic one is the so-called sort of two-point systematic. This is typically related to something like where we have two simulations of a physical process coming from two different generators and you get a different answer. So, how do we account for this uncertainty? A Bayesian might assign a prior to each of those two and then try and marginalize it. Apprentice might construct an average model. But what was nice was Philip Litchfield gave us this sort of nice example that kind of explained something like an average model might not actually necessarily constitute. Might not actually necessarily correspond to anything meaningful. So if you look at, say, traffic going down a road in a two-lane road, your average model would have the car driving down the middle right into realm. One shouldn't do it. So it might actually be the case that this average model that you construct is meaningless physically. So you might not want to do this procedure. This is just to show you an example where it can really make a difference. So this is a really nice example from a study at the CCM. A study at SNCM is performed on TTB, where you can see that the different choices of generator and tune can lead to drastically different predictions in terms of this, it's a particular variable which is important for this analysis. But the important point to note is these ratios are quite large values as a parameter, and how do you incorporate those as uncertainties into the analysis is really a question we have to deal with very often on path analysis. I might skip this one just to save a bit on. Might skip this one just to save a bit on time. So, this is just to point out another type of problem where you might have something like a background function where you have to make a choice between the different functions. I think we're going to hear more about that in the model session. Okay, so then thinking about a little bit about how do we know what actually matters when it comes to these systematic uncertainties, one sort of very nice diagnostic, which I'm showing an example here from the Atlas experiment, is to look at the effect of the At the effect of the nuisance parameter on your measured value. So, this is kind of similar to the one parameter at a time construction, but here you're really using it more like a diagnostic. How does my measurement change if I vary some nuisance parameter? And it sort of gives you a way to sort of figure out which are the actual use parameters which are important, which are the uncertainties that are important to our measurement. And the only one I wanted to highlight here was again, so this one at the top actually comes. Was again, so this one at the top actually comes from something that I was just talking about. So, this TTB uncertainty, where you have different models that can lead to different predictions, and you can see the uncertainty is actually very large for this particular case. The thing I wanted to point out was that when you look at these kind of plots, one thing that you can see is that you can see, if you look at these black points, anytime that you see that the error bar on the black point is smaller than one, that tells you that the data has constrained that parameter. So, there's some information in the data that's able to constrain that parameter. The data, the data would constrain that parameter. And that, of course, is a good thing. But you might ask yourself: do we want certain analyses to update our knowledge of certain parameters? So, in this example, do we want the TT, this is the TT Higgs measurement, where Higgs goes to BB. Do we want that measurement to be able to tell us something about our jet energy method solution in our detector? So, this plot would tell me that that's going on, but that might not be something that's sensible. And of course, And of course, is there a better way of incorporating systematic planning like this rather than just taking the difference between simulations? So then, of course, can we avoid what doesn't matter at all together? So, can we actually use something like machine learning? I think we're going to hear a lot more about this in the workshop, but can we use advanced methods like machine learning to try and avoid these regions where we have large systematic uncertainties? With plots like this in hand, we sort of have a good idea which things are causing us trouble. Things are causing us trouble, and so it may be that we can actually use smarter approaches to reduce the effect of those uncertainties using these kind of new modern approaches. So, the example here is using sort of adversarial methods to train the machine learning model to actually reduce the impact of a particular uncertainty. So, here's an example where you have this parameter Z is causing a large shift in your function output. But if you train in the right way using this adversarial training technique, you end up with a much You end up with a much larger resistance to changes in that parameter. Effectively, the model knows not to try and use that information. Similarly, there's a paper here you can read about something that was used for CMS in houses. Okay, the final point I want to make, just because I think it's quite important, is actually, especially for this workshop, communicating what we did. So, physicists are great at showing you very nice plots that have x sigma for this new particle we discovered. Here's how great our measurements are. We're not so good yet. Measurements are. We're not so good yet at actually telling you how we did all of that and giving you all the information that goes into making those measurements. This is a slide that I stole from Kyle a while back, which I think really nicely shows the complexity of the kind of models. So this graph, which is, okay, the resolution is nowhere near good enough for you to see what's going on, but if you can make out all the Wiggly lines, these are effectively connections between different parts of the model. This model actually comes from, I think it's a Hicks combination. So these models are. So, these models are complicated, large, and often private. But there's been a lot of work recently from the experimental community to actually push for publications of these full statistical models. This was an idea that actually came out of a five-step workshop originally and is now realised more and more often. So, that's good news for the statisticians in the room that you can get your hands on our analyses and tell us what we did wrong or how we can improve things. Okay, so I won't go through the summary of my summary. This is really just to flash. To flash up the discussion points that I had, maybe some questions for a statistician, and then this might be something that we can come back to in the discussion session later. So, thanks very much for listening, and I think I'll stop. I think it's best if you take questions right now. Is there any question in Zoom? No one? No. No, no. Any questions or some comments here from the team? Yes, we got some. The second part of physics experiment, there are many situations where you have the opportunity to trade off some systematics for statistics. Do you think this is an interesting approach? So, for example, I don't know, jet energy systematics, right? You could a lot of this systematics, for example, comes from the neutral fraction of the On the neutral fraction of the jet. If you choose jets which have only charged particles in them, then you get less systematic. Yeah, so there's another thing in the equation. Rather than just trading up between statistical and systematic, you've also got to trade off between, well, there's a terminal that accounts for your sort of your model dependency. So what are you trying to capture? So that's a very good example where you might say, okay, I understand these types of jets very well, and so maybe these are what I'm going to focus on. And so, maybe these are what I'm going to focus on. But if you took that approach, you might never discover a new process that involves magnet particles, for example, because your jets are just completely different in that example. And so, that's another axis of the... Ah, I see. And of course we do because, you know, in TDPAR you have W use. W use is a great standard candle for dependency scale, right? Yeah. I think that means your time's up. No, no, I see your point now, I see your point. So it comes to this question: are we okay with our TT analysis? I mean, yes, it could be that this is telling us that we should actually go and look at a delicated sample of TT bar and then really... Of course we do. And that's how plots like or studies like this come about is looking at TT bar. In this particular analysis, this is actually a search for TT Higgs to BB. So if I'm doing a search for a process TTHigger BB, in the In that search, do I necessarily think it makes sense that my jet energy scale and calibrations are being updated? So it's, you know, as you said, you could see it as a good thing, but I don't know if it's necessarily something that you'd want for this kind of analysis. That's the question I'll ask. One questions? Comments? So, just to great talk, Nick, very nice summary. Just to mention two things with respect to some of your slides. One was you were mentioning. One was you were mentioning Barlow and Beeston as a method of dealing with statistics in Monte Carlo information. So here we have Barlow sitting next to me, who's going to talk a bit later in the week, but not about that. So it has to be more to discuss that bit here. The other thing is that the theme that you slipped over about profiling with respect to different parametric forms, the relevant paper that you didn't mention was by somebody called War. didn't mention was by somebody called Wardle Letown. So if you want to ask any questions, you can feel free to ask me about that. Thanks, man. Other more questions? So I may also want to... Just one thing I forgot in my postwalk I should mention. People may be confused that there are session shares. They appear in the agender like there are speakers at talks. This is simply for technical reasons because only this facilitates the automated recording. The automated recording if you are a speaker or to talk. But it doesn't mean that you should really take over the session and just ask all the questions or show slides. No, it's of course a democratic event. Everybody has the same priority here. So I still have some questions. I mean about this horizontal versus vertical warping. I mean, I think still people mostly do in our community like managing more for horizontal, vertical. And I think if you have a precaution with just slightly shifted, I think you still can do that still, right? So maybe it's a question, but if there's a clear guidance when this breaks down which point. Yeah, yeah, so that's yeah, maybe that should have been a discussion point. But that's exactly what this kind of Point. But that's exactly what this kind of toy show is a case where this breaks down. So this is a clear really separating exactly. Or it's just one version that's broader than the other. Right, right. So this I guess can come this could be a problem where you have that case, for example, where you have two completely different simulations. So you can only evaluate the shift at those two points. Then if you're going to try and build a morphing, then you're stuck. A morphing, then you're stuck with that, so you should probably not use something like horizontal. But I mean, that's a good question to I'll say some more about that tomorrow. I think then one remark, I think in a way we have physicists, you always try to make everything look like Gaussian, finally write the Gaussian cooler and what's on. But there's a model building you do, like when you compared on the slide 60, you compare this. You compare this change delta times this log normal thing. Yes. And there I often think we are just doing this in your reserve blindly. You're not checking this. Exactly. So this check is exactly the kind of thing you want to make sure how sensitive you are to this choice. And it really shows you that you get it. So just to go and explain what this is, this shows you the ratio of the sort of different shifts that you get for different parameter values. Different shifts that you get from different parameter values, whether or not you make a log-normal assumption or Gaussian assumption. And you end up with effectively very different answers. That's maybe it's more common than the question, but I think maybe our field would have served this paper. If you would do more, more better studies, bigger studies, maybe really kind of evaluate the coverage. I I think we started working on fixed bio tools, CMS, which is usually people use with normal certainties, but it's just dedicated. But it's just sort of an educated guess, right? Right, right. So that was kind of the point of this as well. Is that yes, we should be doing these checks, but maybe there are even smarter ways that we can build these parameterizations from a more physics standpoint. I think we're discussing this also, like in 21, we were discussing these problems, but I still think we need more studies. In the meantime, more questions or comments? I have a question which I think is going to be relevant for both of the two talks. How are we doing this? Both of them talk server. How are we doing this here? Oh, maybe we can. I think now it's time to switch to the all loopers and logic. I wanted to go back to this distribution on the weight. I didn't quite get it. Or I don't remember from the discussions. Yeah, so you can follow the link here if you want to go to the talk. The idea is to, so when we usually calculate these shifts, so this mid. We usually calculate this shift. So, this middle plot shows what the distribution of the momentum looks like after applying the shift. And the way you do this is you apply the shift to every event that you have in your monte-colored sample, and then you just see what the distribution looks like. And if you just assume that that weight is a fixed number, you'll end up with a jagged curve because depending on how many events you have, events will move in and out. So, the idea was to instead of running a single weight, you sort of think of these, you think of each of these events as having. Of these events as having some kind of distribution of weights or distribution of outputs, if you like, that span, bin, boundaries. Yeah, so that's what I was trying to understand. Is it like weights in terms of like the weight in the histogram, like the vertical weight, or is it like a distribution, like the location? So it's a distribution of lo on location, but the weight is is if if you like the height of that distribution per K D E, yeah, you can think of it like that. Yeah, you can think of it like that. And there's also kind of like an interesting entire field on perturbation, like stochastic perturbation analysis and so on. But yes, to me it looks similar to a KDE or what people did in machine or like in like into indifferentiable likelihoods with like differentiable histograms with inferno, which also to also call linear benefit, right? So when you basically So when you basically not do square button, I think now we uh switch to the sound so we stopped the recording, we stopped.