About underserved localization or potentials generated by hyperbolic transformations. Thank you. Thanks to the organizers for organizing this meeting. Yeah. It's very cool here. It is. Okay, so yeah, my talk is about Anderson's transition for potentials generated by hyperbolic transformations. So this is a joint project. This is a joint project with Atlanta and the David Dynamonik. So yeah. So we've already seen this in Anton's talk. So we considered one-dimensional discrete shading operators like this. And free Laplacian and the potential. And here is a real valid boundary sequence. And the main question that I'm interested in in this talk is about. I'm interested in this talk is about the anesthesia position. So, also we have seen and talked about it, right? So, we see this operator has anesthesal position if you have pure point spectrum and the eigenfunction decreases exponentially. But I'm going to approach this slightly differently. But I also, it was covered by Anton talk a little bit. So, I'm going to consider that the potential. Going to consider that the potential is generated dynamically. Okay, so basically, what I mean is the following. So yeah, here I'm taking this potential to be something like this. So what is this here? So first of all, I have a so-called base dynamics. Usually we assume this base dynamics to be a compact metric space. Then we have a homomorphism on this compact metric space, or sometimes continuous F-map. And also we take a T-ergodic probability. T ergodic probability measure Î¼ on this base space. Then I'm taking this f, which is so-called as the sampling function, to be a bounded measurable function. Then we sample this f along a trajectory of some point, little omega, in this base space, capital omega. So this omega, I should see that this little omega is a point in this base space. Then when you start to iterate this base point, you use t to You use T to generate a trajectory of this omega. Then you evaluate this F along the trajectory of omega. You get a new vector sequence, bounded neural vector sequence. So that's how you get your potential. So this is the so-called ergodic shedding operator because you have ergodic base dynamics and use this ergodic base dynamics, you generate the potential. So the main question that I want to investigate is: I want to study anesthesis for this. Want to study Anderson localization for this dynamically defined ergodic shading operate. So, how do we study localization for this kind of operate? So, yeah, also this was also covered by Anton Stuck. So, we are going to do it via the React North exponent of the corresponding Schiller echo cycle. So, let's mention, let's introduce what is the Schilling echo cycle. So, basically, you have a map from the base space to special linear group. To a special linear group, SL2R, which takes this form. Then we consider the product of this SR2 matrix again along a trajectory of some base point. So first you evaluate at omega, then followed by evaluate this matrix at T of omega. Basically, you evaluate F at T of omega. Then the so on so forth. I denote A sub M. I denote A sub M, the product of M matrices starting at A of omega ends with A of T n minus 1 of omega. So yeah, then what's the relation between the Johning operate and the Schooling cycle? And I found it's very nice to give this talk after Anton because Anton has already mentioned several of them. So yeah, the limitation is given by the following, right? So yeah, how do you study shooting of it? So how do you study the spectral property of So, how do you study the spectral property of Schilling operate? Basically, you need to study this eigenvalue equation, right? So, you need to understand the long-term behavior, I mean the behavior of the solution of this eigenvalue equation as n goes to plus or negative infinity. So, the asymptotic behavior of this of the solution of this eigenvalue equation will tell you the spectral property of this shoulding operator. So, it turns out that, as we've already seen in the enterprise, As we've already seen in entrance talk, you can move this solution by this Schuling cycle. Or, in other words, if you give initial data u0 and u negative 1, then you propagate this vector via this Schrodinger cycle. And that will actually give you the solution of this eigenvalue equation. So, because of this, of course, we can say that the solution space is two-dimensional, and so on and so forth. And so, like, to study the asymptotic behavior of solutions of this eigenvalue equation as n goes to positive and negative infinity, it turns out that we can instead consider the dynamics of the Schrodinger cycle. In other words, we want to study this long-term behavior of this product of n matrices as n goes to infinity. So, it turns out that the key object here to The key object here to understand this dynamics of the Schrodinger cycle iteration is so-called as the Lyapunov exponent. So the definition of the Lyapunov exponent is, so here I'm going to consider the integrated Lyapunov exponent. So basically you take the product of n matrices, you consider the norm. Here I'm assuming we are using upper norm. Then you take a log, natural log of this norm, then you take the integral of this with You take the integral of this with respect to the gotic measure on this base space, then one of n, and you let n goes to infinity. So the limit always exists because this sequence here with respect to n is a subadditive sequence. So it's like exercise in real analysis that you can see if this is subaddive, then this limit exists. Moreover, this limit equals to the infinum of this sequence here. Of this sequence here. Because we are in SL2R, the norm is always bigger than or equal to 1. So when you take a log, it's always bigger than or equal to 0. So in this case, the Lyapunov exponent is always bigger than or equal to 0. So then how do you use the dynamics of the Scherding cycle to study spectral property of the Scherding orbit? In particular, how do you approach questions related to Anderson localization? How do you use this definite exponent? Lapinoff exponent. So it turns out that usually the first step to establish Andersonal position is to prove so-called uni positivity of the Lyapunov exponents. And usually we needed more than just positivity. We needed some kind of uniform positivity. So which means the following. So if I take a compact interval on R, then we see this this Lyapunov exponent, we have uniform positivity on this compact interval if the infimum of the Lyapunov exponent. Of the definite exponent on this i is bounded below, bounded away from zero. So this uniform positivity. So, but merely uniform positivity is, although uniform positivity is a strong indication of any single condition phenomenon, but it's not sufficient. For example, you can, in the class of quasi-periodic operator, when you consider uh supernuvial frequency, uh you can when you consider analytic analytic potential, you can still get uniform positivity. You can still get uniform positivity. But because of the Louvinian property of the frequency, you can see the operator has single, does not have pipeline spectrum. So, in other words, uniform positivity, although it's a starting point, but it's not sufficient. You need additional properties to prove any syncretization. So, the second thing that we are going to investigate for the Lyapunov exponents is so-called larger deviation estimate. So, first of all, So, first of all, by Kieman's subactive ergodic theorem, we know that although I define the Lyapunov exponents as the integrated one, but we know that because the base dynamics is ergodic, we know that for almost mu almost every omega, this individual Lyapunov exponents exists, this limit exists, and almost surely for mu almost everywhere, this limit actually equals L V. So, this is a consequence, direct consequence of K-man's sub- Consequence of K-man's sub-addict theorem. So, but to prove Anderson localization, we need more than this almost sure convergence. We need to, in some sense, we want to measure the range of the convergence in the following sense. So again, I consider this compact interval. And we see that we have uniform latitude estimates on this compact interval in the following sense. If for every action, If for every action positive, I'm trying to measure the set of omega so that this individual exponent deviates from the average one or the integrated one with a fixed area epsilon. Of course, by K-means subactive ergodic theorem, we know that the measure of this set must go to zero. There's no doubt. But we need something more. We need something more. We want that this set not only goes to zero, but goes to zero with an expected rate. And we want this rate to be independent of the E on this compact interval. So in other words, for each action positive, we are hoping to find a constant c, little z and big c bigger than zero, so that for every n bigger than or equal to one, we have this set decase expectations. Have this set decrease exponentially. So, this little C and the capital C, they are independent of the E throughout this compact interval I. So, if you have this kind of estimates, then we see that we have uniform latitude estimates. Then, once you have uniform positivity and the uniform latitude estimate, then this is indeed a strong indication to prove Anderson localization for this kind of operate. I mean, I don't think there are any, so far there's any. I don't think there are any so far there are any counterexamples that where you have uniform positivity and uniform larger division estimate, you do not have an operation. So I mean so there are very strong indications. So yeah, but yeah, how do you use this kind of to combine the two properties, positivity and the large division, to prove and some position? So yeah, let's go to the again, go back to the Anderson model, which have already extensively discussed. Have already extensively discussed by Anton. So the Anderson-Bernoulli model in particular, I want to focus on the Anderson Bernoulli model. So basically, again, we take an alphabet on the real line and we take a probability on this alphabet. And to avoid a trivial case, we assume that the support of this new is at least two. Then we can consider the full shift space. So here I'm going to introduce this. So, here I'm going to introduce this ideal variable a little more dynamically. So, yeah, then we consider this full-shaped space where this capital omega is just take the product of the alphabet. Then this mu is basically generated by this mu on this alphabet. So, then we consider the so-called shift operator. So, basically, if you have a sequence, so here every element in this omega is a sequence whose Here's a sequence whose nth position belongs to alphabet A. So this is a double sequence. So T of omega, you basically just shift to the left position by one unit. So in other words, the nth position of T of omega is n plus 1 position of omega. So you shift to the left by one unit. So then how do we take the sampling function f, right? So in fact, here we are going to start with a function that is defined on A function that is defined on the alphabet. So I have a function only defined on the alphabet. Then use this function f hat. I generate an f defined on the base dynamics omega in the following sense. So f of omega equals f hat of omega 0. In other words, my f depends only on the zeros position of this omega. So then use the way we generate the potential. You know, the potential. So, f of t n of omega is going to be my potential, so which is nothing other than f hat of omega n. So, because of this, we can say that this potential is ID learning variable, with stationary ID learning variable in terms of entering language, because we have a fixed probability on the alphabet. We do not change the probability measure. So then So then, so if the potential is generated in this way is added random variable, then first as a very seminal work, which basically says that in this case, the Lyapunov exponents is going to be positive everywhere. So, for all E, this is going to be positive. So, moreover, later, Firstenburg and Kiefer, they proved that also. And Kuiper, they proved that also in this case, the Lyapunov exponents is going to be continuous. Since it's continuous and it's everywhere positive, so it's going to be uniformly positive because you can fix some compact interval. So on any compact interval, it's uniformly positive by everywhere positivity and by continuity. But as E goes to negative positive infinity, you can easily see this is uniformly hybrid with growth with the Leipzig exponents growing. The with the Lyapunov exponents going to infinity. So you can easily see that. So, because of that, you can see that in this case, actually, the Lieutenant exponents is actually uniformly positive on the whole year line. Okay, so we have uniform positivity. That's the first step. Then, in 1982, La Pete proved that actually uniform large evidence estimates also holds true in this case on any compact intervals. So, using PLE, uniform positivity of Uniform positivity of the Lyapunov exponents and ULD, which is my notation for the uniform laggy division estimates. Then we use some improved strategy of Bagenstra, which I will introduce a little later. So then we proved the following results. So we have so-called the expansion dynamical coefficient for this NS model, which again, Anton mentioned this already. So in fact, this is, of course, I mean, the NSA. Of course, I mean, as also, as Anton mentioned already, the localization for the Anderson-Bernoulli model was first proved by Komona, Glene, and Martinelli in 1980s via matrix scale analysis. And around the same time of our work here, Jechen Skaya, Xiao and Ju Anton, and also Go and Also, Gu and Zhao. So around the same time, we came up with all kinds of different proofs of these kinds of results. Basically, but I think the key ingredient in all this work is the Levant of exponents, the positivity and larger deviation estimates. Although you might use different approaches to do that, but the key input is how do you use Frustenberg theorem and these larger deviation estimates. Estimates. So basically, yeah, so in the Anderson model, you can use this too to do suitable illumination of double resonance and to get this kind of results. So this again, this is a workshop on quasi-periodic and randomness. So and Anton combined the um randomness and uh quasi-periodicity by adding them. Quasi-variability by adding them. So, here I'm going to move away from randomness slightly in a different approach. So, we can ask the following natural questions, okay? So, in particular, we can try to remove the independence in the Anderson model in some way, right? So, Anderson model is very restricted in the sense, first of all, in the base dynamics, you have a full shift space, and also this potential F is not only local constant, but only depends on the single on a single site. On a single site. So maybe we can try to remove the independence in a suitable sense. For instance, how about we consider more general F, keep the base dynamics to be a full-shaped space. But instead of considering F that depends only on the zeros position or on any single site, maybe we can consider more general F that depends on more than one site. Maybe we have some capital M bigger than equal to zero so that F of omega depends. than equal to zero, so that f of omega depends on the position from negative n to capital n. So if f is like this, then we call f actually, we call f is locally constant. Or maybe more generally we can consider maybe f to be more holding continuous functions, more general holder continuous functions. But we need to note here that merely continuity is not enough. You need some modulus of continuity, otherwise um exotic behaviors of uh spectral behaviour will occur. Of spectral behavior will occur. So you need some modules of continuity. So this is one way to move away from independence. Another way is maybe you can also change the base dynamics. So for instance, how about instead of considering a full-shift space, we consider Markov shift with Markov measure. So there are already works for positivity of the Leyfnov exponents by Ilia, by other people. So how about Anson looks like? So, how about Anderson localization phenomena? So, what if we move away from force-shift space to mark shift? Can we improve similar results like Anderson localization, like the Anderson model? Or maybe we can even move away from this full-shift space a bit further. So, let's consider the following version. So, this is the W map. Okay, so we consider the base space to be the unit circle, and we consider the circle W map. The second W map. So basically, T of omega is just two omega. So in this case, because this is a self-map, we have to consider half-line shorting operate with deleted boundary conditions. So then if we consider this model, there are some previous works. So I think the first Android localization results for this model is given by Bokashark in 2000. So they showed that in this case, if you consider That in this case, if you consider the sampling function f to be a C1 function, then for each data positive, you can find a constant, so coupling constant basically. Lambda 0 depends on f only. Small. So that's for every coupling constant between 0 and lambda 0, you will have uncentral coefficients for this operator for almost every omega. Every omega, but you will have to remove something from the spectrum. So you have to remove a small neighborhood around zero, and also you need to remove a small neighborhood around the negative two and two. So once you remove this small neighborhood around zero and plus minus two, once you consider this very small coupling constant, then Bogashrock, they prove that indeed you have any single condition. So their key So, their key ingredients to prove Anderson localization, like the Anderson model, is again, first of all, you have uniform positivity on this interval. So, this is due to the work of Spencer and Chenkovsky in 1995. And in the same paper, I think Bogashirak proved a uniform larger deviation estimate on this interval, on this from zero and plus minus two. So, then they can find some suitable version of elimination of double resonance. So, you can Of double resonance, so you can go ahead and then prove any sort of position. So the question is: so you can see this results, it's a first result, which is nice, but also it has a lot of restrictions in the sense that, first of all, you have to consider very small lambda. And also, you don't get full spectral condition because you have to remove something from the spectrum, from the center, and from plus minus 2, basically. So, the natural question is: again, can we? The natural question is again, can we improve more general and satisfactory results when we consider the hybrid system? So why hybolic system? Because once you move away from Anderson model, so in terms of the base dynamics, you try to move away from forceship space. Then hybridic dynamics is a very natural next step. So it has very strong mixing property. It's very random. So compared with quasi-periodic, you know. Compared with causing periodic, you know, almost periodic, or a general weekly mixing or mixing, it has more randomness in it. So, yeah, so it's very natural for us to expect that if the potential is generated by hybolic transformations, maybe we can still prove anti-localization, right? Like the answer model. And yeah, so to answer these questions, I mean, how can if we can get more general results and go back to the previous slides? You know, what if we consider more general F, or what we consider maybe Markov shift with a Markov measure, right? So it turns out that we can to attack this kind of problem, we can combine the base dynamics in the full sense. This is called a general hybrid transformation. The definition is a little bit technical, but let me just go through this slowly. So now, instead of considering a full-shaped space, we consider a Shift space, we consider a subshift phenotype. For those of you who are not familiar with subshift phenotype, basically you have some forbidden words, then you are not allowed to appear in the shift space. Like for instance, maybe the alpha batch is 1, 2, L. Maybe you are not allowing 1, 1, 2 to appear. You only allow 1, 3, 1, 4. You will never have C12. Or you will never have C23. So you take away some finite words. With some finite words, and this is the subshift of finite type, basically. Then to define, so subshift of finite type. So, the key thing to encode this hybridity is you need to find some suitable ergotic measure that detects this hypercity. So, to do that, we actually need to introduce omega plus, which is basically you forget about the past. And omega minus, you forget about the future. So, then we have projections from the full shift. We have projections from the full shift and from the two-sided shift to one-sided shift, respectively. So then you take a ceiling set which you only fix the zeroth position. So Z to J0J only means that you fix the zeros position to be J. Anything else, you have no restriction at all. Likewise, we can do the same thing for the one-second strips. So then to encode hyperbolicity, we need this T-agodic. We need this T-agodic measure. So, first of all, we assume the measure to be fully supported. This is not an essential restriction because you can always do that in some sense. So, we let mu plus minus to be the push-forward measure of the measure mu onto the positive and the negative side, so single-side shift space. So, then we see that this mu has a local product structure. If when you restrict to every cylinder set where you only fix the zeros position, Where you only fix the zeroth position, you can see that this is equivalent to the mu plus times mu minus. So this is called a local product structure. And this actually allows you to see the hyperbolic structure. So this will basically so yeah, you know, basically in some sense that yeah if you your local stable unstable sets Your local stable unstable sets, they intersect in your non-trivial way. You should be able to see it. Okay, this definition is slightly technical, but this is how we basically encode hyperbolic transformations in a subshift of finite type. So yeah, again, subshift of finite type can be defined by an adjacency matrix. We are, you know, this is a standard way people generate subshift of finite type. So then, yeah, so you may want to. Um, yeah, so you may want that this looks very technical, but uh, what really is this kind of base dynamics? So, if you want a concrete uh concrete examples, for instance, you can consider C2 transitive, so-called anosov diffimorphism, which in dynamic systems is a very important type of dynamic system, which is called a uniform hyperbolic system. So, whose inventory measure view is, for instance, absolutely continuous with respect to the volume. So, if we have a C2 transitive anosop diffimorphism, whose invent measure is H2. The inventory measure is AC with respect to the volume measure, then we can, there's a standard process in dynamic system. You can convert this system into this system, basically. In particular, like the W map on the unit circle or for instance the another cat map on the two-dimensional torus can be reduced to the setting in the previous slide. Also, irreducible Markov change. Irreducible Markov change with a mark of measure is a special case of our general setup. You can easily, you can, I mean, not easily, but it's again, it's a standard thing in hybrid theory. You can see that. So basically, this is, although it looks a bit technical, but you do have a lot of interesting concrete examples that represents this kind of space. So then what we do for this base dynamics. Base dynamics. So, how do you prove positivity and the large dimension estimates? And eventually, maybe hopefully, you can prove localization results. So, in fact, we can prove general results. For positivity, we can prove much more general results. But for the purpose of large deviation estimates, I have to restrict myself a little bit. So, in this talk, I'm only going to consider a special type of F. So, basically, so the cosine. So basically, the cross cycle map A is said to be five bunch if you have a n zero. So that this is true. Basically, as long as A has the upper norm A is small, you will have this. Okay, so basically, so this alpha is the, so A is here. I'm going to assume A is wholly continuous. So this alpha is exactly the holder exponent. So then going back to the Going back to the Schoenie cook cycle, right? How do you ensure that you have this fiber bunched condition for this co-cycle map? So you basically just need F to have very small super Riemann norm. So if you have F has small super Riemann norm, then you can, it's not very difficult to say that the corresponding Schrodinger encounter cycle is actually five branched for all E in this interval, which automatically covers. Which automatically covers the almost zero spectrum. Then we denote by sh the set of all such f. Then again, locally constant, I have already mentioned this. So we say f is locally constant if there's capital N, so that F of omega depends only on the position from negative N to capital N. We let L C denotes the set of all such F, L C local constant. SH, hold continuous F that has more superluminum. Super luminum. So then we denote Zf to be the set of energies where we have zero L exponent. So to prove positivity, basically what I want to show is that I want this set to be small. So basically, in this setup, so we proved that in our first paper of this project, we proved that in fact if I have a base dynamics like this, if T in addition T has a fixed point. If t in addition t has a fixed point, and if f is um code-continuous with small superluminal norm, or if it's locally constant, then actually we have zf is a finite set. So in fact, we have something more, not just zf is the finite set, because in this case, you can also, there are also known results that the definite of exponents is continuous. So once you can show that zf is finite, then away from any abstraction of this finite set, you will have uniform validity. Of this finite set, you will have uniform positivity. So, this is our results in our previous paper. So, then once you have, so by the way, this is already a good starting point for you to eventually establish localization results because this finite set really carries no weight when it comes to the spectral results. So, then you need to show large deviation estimates. Large division estimates. But when it comes to large division estimates, merely a local product structure is actually not enough. You need something stronger for the base dynamics. So you need to see some basically like a near independence of the base dynamics. So basically, this is technical, again, a technical definition. We see that this mu has a so-called bounded distortion property. So basically, you see some kind of interval. You can imagine if mu is a full-ship space, then this is precisely. then this is precisely this is precisely equal. So you take one cylinder set at one finite set, you take another cylinder set far away. So if you consider Fussia space, they're independent. So the intersection, the measure of the intersection is a product of the measure. So but of course to be able to consider more general hybolic type of base dynamics, you do not hope to have independence, but you can have this bounded distortion property. Bounded distortion property, which is basically like near, almost independence. So, again, you may ask that if you put this kind of restrictions on the base dynamics, what can you cover in your system? So, in fact, all the concrete examples that we mentioned previously has this boundary distortion property, like C2 and also transitive with the organic measure to be AC with respect to volume, or Markov shift with Markov measure. Or Markov shift with Markov measure. Actually, they all satisfy this boundary distortion property, actually. So then, so in our second paper of our project, we first proved this result. So we consider the base dynamics subshift penetrate, where Î¼ is a gothic measure with this boundary distortion property. Suppose you have positivity, uniform positivity. Positivity, uniform positivity, and uniform logic estimates on a compact interval. Then we can show that from your almost every omega, you do have expansion dynamical position on this eye. So basically here, yeah, you again, so once you have PRE and URD, what's the additional step? Yes, you need some suitable version of so-called elimination of dark resonance. Because of this near independence, eventually we are able to prove. Eventually, we are able to prove a version of elimination of double residence, which enable us to prove exponential dynamical localization if you have this. So, again, this is a general result. Can you apply them to some concrete examples? So, then again, let's go back to either the small holder potential or locally constant potential. I mean, assembly function. function. Then you can take any connected compact interval containing the almost zero spectrum. Then what we can do is that you can find a finite set which contains the finite set where you have zero Lyapunov fax point because due to our first result we know that the set of zero Lyapunov X point is finite. But yeah, to show uniform large division you actually need to consider a slightly big set but definitely Consider a slightly big set, but still finite. So then, once you move away from this finite set with an actually small distance, so by this notation, I mean an open neighborhood of this finite set with radius eta. So when eta goes to zero, it's going to be actually small. So you remove this finite set, a small neighborhood of this finite set away from this j. Then we show that we have u uh uniform large deviation estimates on this uh on this interval. On this interval. So yeah, so then together with our PRE result, with this URL result, we can and this general theorem regarding localization, we can immediately draw the glory that we have full spectral localization for almost every omega. So here again, the base dynamics is the hyperbolic transformation where mu has bound. The hyperbolic transformation where Î¼ has bounded distortion and where this f is a small holder or locally constant. So, for instance, this result fully covers the previous results of oh, sorry. Yeah, so these results, for instance, fully covers the previous results that we mentioned approved by Bogenstruck. It's more general. And it also covers the Anderson Panoli model. And also, you can consider the markup shift. Is the markup shift. And also, you can consider shift space and consider F to be locally constant or small holder continuous. So, anyways, this is already much more general results than all the previous results in this line. And yeah, only five minutes. So, yeah. So, I mean, I just want to quickly mention some of the tools that we used in our proof. So, yeah, so the main tool that we used in our proof is so-called as a stable, unstable holonomy. Is so-called as a stable, unstable holonomy. So basically, you need to define the stable, local, stable, and unstable sets. So this is standard saying in hyperbolic dynamics. In hyperbolic dynamics, for every point, you can have the stable manifold and unstable manifold. Unstable manifold, and after two points, on the same stable manifold, when you go forward, they will come close expansionally fast. And on unstable valuation, when you take any two points, then you iterate backwards, they will tend to each other expansionally fast. Like, it tends to each other expansionally fast. So, this is stable and unstable variation. Then, you can move things around on the stable and unstable variation by so-called stable and unstable holonomy. So, basically, yeah, here, again, it's a bit technical, but you can show that if we are in the scenario of local constant or small hold, then you can show that these two zero converges because the hybrid. The hybridity on the base dynamics dominates largeness on the fiber, so that you can get a convergence of this. And this is called a stable and unstable holonomy. Use this stable and unstable holonomy, you can actually conjugate the original cycle to something that is independent of the past or independent of the future. So is this like a generalization of locally constant? So then using this, Using this, so another very important ingredient of our work is so-called the SU state. So basically, so yeah, so NTA invariant measure on the projected dynamics is called a SU state if some disintegration of this measure is invariant on the both stable and unstable holonomy. So, yeah, it's because of this property, as you state. So, then the invariance principle. The events principle, which is a first version approved by Lijpedia, says the following: if you have zero Lapnoff exponent, then this co-cycle will certainly have a so-called SU state. So SU state is actually a very, so in some sense it's a very rare phenomenon because you have too many invariants attached to it. So because you have so many invariants, you can easily perturb it away. So, but on the other hand, Away. So, but on the other hand, if you have zero exponent, you will have this kind of SU state. So, which in the end tells you that the events of have zero exponents is rare. And in fact, the finite set I mentioned previously that contains the Zf where you have zero exponents is precisely the set of energies where this School cycle has an SU state. So, you can, because of this lemma, you can say Zf is a subset of FF, because ZF has zero. FF because Zf has zero exponent and a zero exponent implies the existence of SU state. But you can show actually this is the finite state, actually. So yeah, this is also the key ingredient for us to actually prove the uniform latitude divination estimate. But I'm running out of time, so I'm going to skip the last two slides. Sorry. Watch that? Questions? So for the number, zero is special, right? You can just screw it, you can screw their influence on the zero with analyzation. You can explain why. You know, zero, you can screw it. Zero was excluded in the work of Orchestra, right? Yes. So in your work, in person, that's what you're talking about. No, I don't. Yeah. I have four special position results. Yeah. Did you think about a a continuous version where you have a dynamical system, hyperbolic continuous dynamical system on a manifold and you have ddx? I want to get away from the discrete case. So you have ddx squared. Oh, continuum squared. Oh, yeah, we have to think about it a bit. Yeah, I think it's very possible. Yeah, it's possible. In some ways, those things are easier. You don't have special energy. That's true. Yeah, I agree. Yeah. And now we have lunch, and after lunch, there is a group photo, and then we're giving here. So now there's a lot of customer. There's a photo and it's here because it says TCPL foyer. So I'm guessing that explains. So, I'm guessing that it's right here. Oh, we have lunch here. No, no, the photo, the photo, lunch we have in the same way in the same place that we had breakfast. So, hopefully, they will not make us stand outside for the photo as they usually do. So, if so, then please bring some work also. You won't be able to see anybody's face. How we will look like I have one association right now, and that's not a good one. I think we still have a few people as well. Should we tell everyone or just boom? Okay, so just move the juke photo to Winsteck and we can just talk. Same again. So because not everybody has arrived yet, we want to push back the juke photo. And so we've agreed on having it taken at the end or right after the last talk on Wednesday. So just before the launch break and the free after. So after the last talk on Wednesday, we'll do the photo taken. We will make the change online soon. Temperature-wise and seasonal. I'll change it on the schedule. Yeah, make a bit more change. Yeah, make a bit tomorrow. Yeah, I'll change your schedule. Yeah, two and two thirty inches and then they change it. So you'll have to push for yourself. Is it sure? Yes, yes, yes, but they call it. So it's proper. Yeah, exactly right. Yeah, exactly, right? If I did, I guess. No, no, but it's not a problem. In a measure sense that you have built it. But it doesn't mean like the metal crisis. Like for example, it's really not a final measure. So non-deterministic really just means that you have a split, but the other thing means that once you Once you cut the measure into two halves and project the measure to the two halves and then form the product of the two things, then you have something that resembles the original measure. I mean for the promise measure it's equal to it, but for our expected just means that you estimate what it is. No, it's really just a common definition, but it's stronger than all the jobs. Okay. He said the first one to kind of honestly. Yeah, so okay, so yeah, I think the way he introduced things was backwards relative to Use things with backwards relative to the normal things, right? Holistically, you initially define the double measure, it's really just a feature of the dynamical system, right? And then you look for measures that sort of like that, I don't know what I call that.   Yeah, there are some more products, but there are more people like that.    