So very happy and grateful that I have this opportunity to present recent research projects which are with my collaborators Sodani Lbatl and Matthias Beigelberg from the University of Vienna and Stefan Eckstein also from the ETH. So I want to start my presentation by motivating why I'm interested into these things. Into these things. So, I will first talk shortly about stochastic processes and the weak topology, then introduce what we call the adapted weak topology. I want to view this topology more from the perspective of optimal transport and thereby shortly introduce optimal transport, but in order to be able to talk about adapted transport. Talk about adapted transport, then I want to introduce what we call the Wasserstein space of stochastic processes and then talk about its properties. So this will be the first part. In the second part, I want to talk about numerical aspects. And there, I want to introduce a new variant of synchronous algorithm in order to solve adapt that. Self-adapted transport problems. And finally, I want to talk about convergence and show you some pictures. So let's start off by motivating this. So stochastic processes are frequently used as models for phenomena in nature, society, and economy. As an example, in mathematical finance, we use models, we use stochastic processes as models for asset prices. And then these And then these models are used in order to support the decision-making process, which typically also involves solving optimization problems. Now we face then the following challenge that, of course, in order to support the decision-making process, we require a certain sense of continuity of the decision-making and of also of the optimization problems with respect to the With respect to the model. And here we see that in many instances, these optimization problems are not, or many optimization problems are not continuous with respect to the usual weak topology. So I want to emphasize this point by the following example, which some of you might already know from sequential decision making. So let us consider two processes. So we have on the left. Processes. So we have on the left-hand side a process X in pink, on the right-hand side we have a process Y in green. So we are in two time steps on R2. So what does X do? X is a martingale which starts at time one in zero with probability one and then it splits with probability one over two. You go either up to Over two, you go either up to plus one or down to minus one. Whereas on the right-hand side, we have a process which where at time one we are either a little bit above or below zero, and then we go either deterministically up to one if we are above zero at time one or down to minus one if we are below. Now, when we compare the laws of these two processes, clearly, if we now send epsilon to zero, Now, send epsilon to zero, then we can all agree that these two laws are similar. So, mu, which is the law of the pink process, is in the big topology similar to nu, the law of the green process. But when we think of these processes, for example, as models of assets, and we want when we consider a sequential decision-making problem such as optimal stopping, then clearly we see that. Then clearly we see that the left-hand process is the marketing deal and therefore its optimal stopping value is zero. Whereas if we consider the right-hand side, then the optimal strategy would be to either if you are if you are at time one and you observe that you are above zero, then the model will tell you that the value will only increase, so you would stop at time two. Whereas if you observe at time one that the value is Observe at time one that the value is below zero, then you would stop immediately because the model would tell you that the value will only decrease. And therefore, what we obtain here is that for epsilon arbitrarily small, the optimal stopping value is always about one over two. And therefore, these, yeah. So therefore, if we send epsilon to zero, the right-hand side is not converging to the left-hand side. Of course, we are not. Of course, we are not the first ones who are observing this. In fact, in the past 40 years. Just one quick question. You don't see my Zoom overly, Red. You only see my presentation. Are you muted, Martin? Sorry. Yes, we see only your presentation. Okay, sorry. It's annoying me a little, but if you okay, then it's fine. Okay, so in the past 40 years, numerous mathematicians from different Numerous mathematicians from different fields saw this problem and tried to define an extension of the weak topology that respect the error of time of underlying filtrations. So as a short disclaimer, in my presentation, I will only talk about discrete time. So and for simplicity, the path space will always be R to the power of n. So we consider stochastic processes in n time steps. Stochastic processes in n-time steps. Now, here I compiled a short and probably incomplete list of different topology sheets. So the first one was Aidos in 1981, who introduces his extended weak topology. Then in 1984, Hoover and Kiesler view this problem more from the perspective of logic and introduce what they call adapted distribution. Introduce what they call adapted distributions. And in particular, they also try to formalize when two stochastic processes have the same probabilistic properties. Then in 1996, Helvig viewed this problem more from the perspective of economics and he defines what he calls the information topology. Then one can also define the optimal stopping topology, so the coarsest topology which makes Topology which makes optimal stopping for a certain class of non-anticipative cost functions continuous. Then, more recently, people also viewed this from the point of view of optimal transport. So, in independent workload-siland, also Bakov define a causal Wasserstein distance. Then also people define what I would call an adapted Wasserstein distance. What I would call an adapted Wasserstein distance. So the first one was Russendorf, I think around 1990. And then later, Fluge and Picle and also Bionodal. And very recently, last year, Bonnier, Liu, and Oberhauser define a topology where higher rank expected signatures. Now, it's kind of astonishing that, at least to me, that in discrete time, all of these approaches yield the same topology on the law. On the laws on the path space. So, this kind of tells us that no, if you want to fix this issue, which I presented on the previous slide, and if you want to be close to the weak topology, then you will always end up with kind of with one topological object. But I don't want to focus too much on the topology, but I would like to view this topology from the point. Perspective of optimal transport, and therefore, I will give a short introduction to optimal transport in order to define adapted transport. So the main question optimal transport is, of course, how to transport a distribution mu to a distribution new. And a simple answer can be given that this can be done via transport maps. So a transport map is a map which maps map is a map which maps in our case the path space R to the power of n to R to the power of n which satisfies this push forward condition that when t pushes mu then we end up with mu. Of course in optimal transport we usually don't we usually consider the set of couplings for multiple reasons. So a coupling is a probability. coupling is a probability measure on the product space so on Rn times Rn which satisfies that its first marginal is mu and its second marginal is mu and one can connect this to this definition of transport maps because one can view couplings in fact as randomized transports between um these between mu and mu and i like to think of couplings as the set of all possible Set of all possible relations which optimal transport allows us to use in order to compare to measures mu and new. Now, given a cost function c, we can define the optimal transport problem as the infimum over all these couplings, so the infimum over all relations between these two measures. And then we so and then we take the cost of this relation, which is here given by the integral of the cost function with respect to the coupling. Respect to the coupling. Optimal transport has, of course, many interesting and beautiful properties, but I want to just mention one, that if we choose as a cost function c, for example, the Euclidean norm to the power of p, then we obtain a distance on the set of probabilities, which is called the Wasserstein distance or the pth order Wasserstein distance. And this distance then can be used to measure as the weak topology. The weak topology. Now, when our measures are not just laws of random variables, but laws of stochastic processes, then comparing these stochastic processes with respect to arbitrary couplings may not be a good idea because it may violate the error of time of underlying fluctuations. Of course, the center. Filtrations. Of course, this sentence is not very rigorous. So, for this reason, I want to show you again the following example. Let us consider again on the left-hand side we have our process X, on the right-hand side, we have our processing green Y. And T now is a transport map between these two. And what does it do? It takes the upper path of the left-hand process and maps it to the upper path of the right-hand process, and at the same time. And at the same time, it takes the lower path of the left-hand process and maps it to the lower path of the right-hand process. Clearly, if x is distributed according to mu, then t of x is distributed according to nu, but we have only one issue that if we consider what happens at time one, we see so at time one, we just observe on the for the on the for the pink process, we just observe a Pink process, we just observe. So we are just in zero. But what the map wants to do is it wants to split this into two points at time one on the right-hand side. So for this reason, somehow this map is not adapted. And in adapted transport, what we do is we want to fix this. So, heuristically speaking, we want to restrict to transport maps that are adapted. That are adapted. And for the sake of my presentation, I would like to symmetrize this definition. So, I want one might want to consider maps T, which are bi-adapted transport, meaning that T is a transport between mu and nu, and both t and its inverse are adapted. Of course, these definitions or defining a transport problem like this might be clumsy because Clumsy because, for example, in the example above, there doesn't exist an adapted map from the left-hand process to the right-hand process, and therefore we have to pass on to, in fact, similar to an optimal class, but we have to pass on to couplings. So for this reason, I would like to introduce the notion of a bicausal coupling. So a bicausal coupling And I will only introduce it informally. So, bicausal coupling, you may think of it as randomized bi-adapted transport between its marginals. And again, I like to think of now the set of bicausal couplings as the set of all possible relations when you want to compare laws of processes. So. So now we can define the adaptive transport program simply by given a cost function C, we may define it as you take the infimum overall bicausal couplings or all relations between these two processes, and then you integrate this cost with respect to the coupling. Similar to an optimal transport, we have that if you take C as, for example, as the Euclidean norm on Rn. Norm on Rn, then we again obtain a distance on the set of probabilities, which is called the adapted Wasserstein distance. And again, this distance has nice properties. For example, it can be used to matriz the not the weak topology, but now the adapted weak topology. Sorry, and Sorry. And, but this is not the only thing it does. It can also, it is also a good distance in this in the view of sequential decision making because it leads to quantitative continuity results in optimal stopping, hatching, and utility maximization. So, next, I would like to talk about shortcomings of this space. So, if we now consider the set of probabilities on Rn, and if we On Rn. And if you equip it with the adapted Wassofstein distance, then we end up with a space which is not complete. And if we think of these as laws of processes on Rn, then you might think of these elements as stochastic processes which are equipped with the natural filtration. So one might also ask: can we also deal with processes with general filtration? Processes with general filtrations. So, to show you why this space is not complete, I would like to go back to our example from the beginning. So, if we consider this sequence in the middle in queen, so if we send epsilon to zero, then it's easy to check that this is a Cauchy sequence with respect to the adapted Wasserstein distance. But its only limit point is, of course, or its only possible limit. Is of course, or it's only possible limit point rather, is the left-hand process in pink. So, but we know that it doesn't, or in order to fix the example from the beginning, this doesn't converge. So, this space cannot be complete. But what you can think of, what happens in the limit is, in fact, that if you send epsilon to zero, you converge to a process which can already decide at time, which knows already at time one. Which knows already at time one, if the path will go up or will go down. So, somehow, in order to have a limit, you have to admit larger filtrations. And in fact, these two shortcomings, and this example I would say, was the starting point for the joint work with Matthias and Daniel, where we show this, the following theorem, that if you consider the completion of Completion of this, of the probabilities on Rn equipped with the adapted Lasso-stein distance, then in fact you end up with all stochastic processes living on generic filtered probability spaces. And of course, this theorem now is not very precise. And in order to make this statement rigorous, I want to introduce some notions. First, I want to talk about what we call a filtered process. So, a filtered process with So, a filtered processor with paths in Rn is a five-tuplet consisting of a filtered probability space, so omega F, F T and P, and a stochastic process X. On this class of filtered processes, we can naturally define, again, the adapted Wasserstein distance, which is very defined very similarly. We define it as you take the infimum of all by. As you take the infimum of all bicausal couplings between these two filter probability spaces. And here I want to remark that now a coupling pi is, so this filter, these stochastic processes X and Y, they may live on different filtered probability spaces. So this coupling pi is really a coupling between two different filtered or potentially different probability spaces. And then we take the expectation. And then we take the expectation of our cost. So, if you consider now this class of filter processes, then it's of course a huge class and which contains a lot of redundancy. For example, if you take a process, you could always add something independent to this process without changing its probabilistic properties. Therefore, I want to remind that what we want to use is Use is what Hoover and Kiesler did in 1984. They formalize when two stochastic processes have the same probabilistic properties. And this leads, I would say, to the first main result of our work, that if you consider two filtered processes X and Y, then they have the same probabilistic properties in the sense of Hubert-Kiesler, if and only if the adaptive Assassin distance vanishes. And this is important, of course, because. And this is important, of course, because this tells you somehow that the adapted Wasserstein distance already can distinguish whether when two stochastic processes have the same properties or not. And now this allows us, in fact, to consider the following factor space where we factor out this equivalence relation. And this step is now very similar to what we do for example. We do, for example, when you consider LP spaces, where we also factor out a function via almost sure equivalence. In fact, now we call this factor space equipped with the adapted Wasserstein distance. We call it the Wasserstein speed of stochastic processes. And on the next slide, I would like to talk about. I would like to talk about properties of this space. So if we consider now this Wasserstein space of stochastic processes, then we end up with a Polish metric space which is isometric to a classical Wasserstein space. So meaning that the adapted Wasserstein distance is now complete and this space is not too large since it's a polished space, so there exists a countable density. Is a countable dense subset. So, somehow from this large class of filter processes, by factoring out this equivalence relation, we obtain something tractable. Then now we can also make it more precise what we mean with that these the probabilities on Rn are the completion. So, really the yeah, the wash and space of the The Wassachian space of stochastic processes is the completion of this space when we feel the space or the so that when we feel the laws of on Rn as the when we identify them with processes equipped with the natural filtration we also can characterize compact sets in a similar fashion to Pokhoff's theorem so we have that So we have that a subset of filtered processes is AW compact if and only if, of course, it is a closed subset and their laws are tight. So now this really means that as a subset of the probabilities on the path space. And of course tightness is something which is very well understood. We also have that Martnigels form a closed subset. Form a closed subset in the space of filtered processes. We have that a lot of families are dense, for example, Markov processes on finite probability spaces. And we have continuity and even quantitative continuity of the dupe decomposition of optimal stoppling, the Snell envelope. And finally, we also develop a machine. We also develop a more geometric point of view. So, assume that p is bigger than one, then we have, in fact, that the Wasserstein space of stochastic processes is a geodesic space, meaning that if you have two stochastic processes, you are now able to consider geodesics. So, you can talk now about an intermediate point of two stochastic processes. And we also have that martingale. Also, have that martingills form a closed geodesically convex subspace. Yeah, meaning that if you have two martingills, then any point on a geodesic between these two guys is again a martingale. So, and I think this point is remarkable somewhat because if you consider the Wasserstan geodesics between martingals, then they don't have to be martingals anymore in between. So, interpolation, Wasserstan interpolation between Interpolation, was such an interpolation between martingales don't have to be martingales. And I will show you this by an example at the end of my presentation. So in the next part of my talk, I would like to talk about numerical aspects. And for this reason, I would like to consider an entropic regularization. So here I defined, so here I consider again. Find so here. I consider again the adaptive transport problem. So I take the infimum overall bicausal couplings, integrate the cost function with respect to the coupling, and add now epsilon times some irregularization term, which is here the Hubert-Leiber diversion, also known as the relative entropy of pi given the product of its marginals. The relative entropy. The relative entropy has very nice properties. For example, what you can do now is that you can put this integral with respect to the coupling. You can put this inside of the entropy simply by changing the reference measure. So instead of considering this product coupling as reference measure, we may consider as reference measure this measure PC. measure this measure pc epsilon whose density is given as written down below here and now one might wonder why it is of any interest to consider this regularized optimal transport problem and the reason for this is that you have in a certain sense a consistency meaning that if you have a continuous unbounded cost function then and if you now send epsilon to zero so the regularization parameter zero, so the regularization parameter to zero, then you have that the unregular or the regularized value is converged to the unregularized one. But you have even more, you have we even have that limits of optimizers of the regularized problems are optimizers in the limit of the unregularized problem. So this consistency really tells you that in order if you want to compute the Want to compute the unregularized problem, then it really makes sense to instead consider for epsilon sufficiently small, of course, the regularized problem. Here I also want to mention some other algorithms which one could use in order to solve this adaptive transport problem. One way is, of course, you could reformulate this problem as a linear program. Another way would be to Another way would be to view it as a minimax problem, since it is, in fact, an optimal transport problem with additional constraints. One other option would be to solve the problem via a dynamic programming principle, meaning that you could solve it backwards by solving a nested sequence of classical optimal transport problems. So, what is the reason why we want to consider the regularized optimal transport problem is the following that in 2013, Koturi proposed to use this entropic optimal transport in order to solve the optimal transport problem because the entropic transport problem can be efficiently computed. Compute that by using Synquence algorithm. Now, our approach is very similar, in fact. So, we want to introduce now adapted variations of synchronous steps. And in order to explain you what I mean by that, I would like to recall how the synchronous algorithm looks like, which is used in order to solve entropic optimal transport. So, in the first step, you start by defining pi zero as simply as your reference measure. And then you iterate the following. If k is odd, you minimize the relative entropy of pi given the previous iteration step where you Where you minimize overall couplings pi, where you only fix the first marginal and you let the second marginal be arbitrary. Whereas when k is even, you again minimize the same problem, just now you consider O couplings where you fix the second marginal and let the first marginal be arbitrary. Now, this is really an algorithm because. This is really an algorithm because the optimizer of this, so if you only fix one marginal, then and you know pi k minus one, then you can explicitly write down how the optimizer looks like. Now, together with Stefan Ekstein, we propose to use the following algorithm, which is very similar. So, in the first step, the first step is again the same. We initialize pi zero as the reference measure. Pi zero as the reference measure. But what we do then afterwards is we instead of doing classical synchron steps, we add an additional constraint. So we also want that pi satisfies some kind of adaptedness constraint. Now, yeah, together with Stefan, we show that we show that this is a That this is again an algorithm, meaning that these optimizers pi k can be explicitly written down even with this adaptedness constraint. In fact, in order now to show convergence of this algorithm, we were following the footsteps of what Callier recently did for classical entropic optimal transport. Transport and we show convergence of this algorithm via duality, via arguments based on duality. So for this reason, we had to show duality. So let C be a continuous and bounded cost function. Then we show that the adapted transport problem, the value of the adapted transport problem coincides with the supreme. Coincides with the supremum of all functions S, which are in a suitable class of functions and which are dominated by the cost. And then we integrate these functions as with respect to the product coupling. Of course, if you know the duality from optimal transport, this looks very similar. The only difference is that this Yeah, this family of dual functions is a different one. So, in the case of classical optimal transport, this family would be given by a functions which are the direct sum of the function only depending on the first component plus a function only depending on the second component. And similarly, we also show for the regularized optical. Regularized optimal transport and adapted transport program duality. And again, the so the term which comes after the supremum is the same which you also have for entropic optimal transport. The only thing which we change is that we have to take the supremum over functions in a suitable functional class. We also show stability, meaning that if we have a sequence of marginals, mu k converging to mu and mu k to mu in a suitable sense, meaning here in the adapted weak topology, then we have convergence of the optimal values of the adapted transport problem. Problem and also of the regularized adapter transport problem. And this can be even made quantitative if the cost function, for example, is Lipschitz continuous. Whereas we only have a quantitative convergence of the optimizers in the case of the entropically regularized adapted transport problems when we assume additional. Additional assumptions on the marginals, they have to satisfy Talacron's concentration inequality. And these results are very, yeah, very much in the spirit of classical optimal transport. So I want to remark that I think stability is somehow an essential property in view of doing numerics because it tells you more or less that you. It tells you more or less that you are allowed to discretize your problem as long as you are discretizing as long as you are reasonably or as long as you discretize reasonably well. And it also tells you that, in many cases, when you don't know how mu and new precisely look like, that as long as you know them, as long as you know them reasonably well, what you are computing. What you are computing still makes sense, of course, even if you don't know what how mu and mu might look like. So given this, we showed the following theorem. So given a cost function c which is bounded and continuous, we showed that there is a rate rho in 0, 1, which depends only on. Which depends only on the supremum's norm of the cost epsilon and the number of time steps, such that we have that the value of the regularized adapted transport problem is the same as the value which you obtain after the k-th synchron, kth-adapted synchron step, plus some big O of rho to the power of k. So here So here pi k is another optimizers which we obtain in the kth synchron step. And we also show that we have the same rate of convergence for the logarithm of the densities, so for the corresponding dual potentials in L2. So yeah. Yeah, this is in fact, we are almost at the end of my presentation. So, next, I would like to show you this example which I talked before. So, here we start with two, we consider two processes. Stop it. So, as you can see, we start with a martingale and we end with a martingale. But if we consider the Wasserstein two geodesic, then we can see. To geodesic, then we can see that it is not a martingill. Whereas, when we consider in this case, the adapted Wasserstein interpolation, then really this property is preserved. But this is not the only thing which we can do. For example, we can also define now a new family of new way of taking averages. Averages so we can define adapted Wasserstein barycenters. So, an adapted Wasserstim bary centers again is defined very similarly to an optimal transport by simply taking this infimum over all processes and we minimize the distance of this process X to these other reference processes XK and down below. And down below, we computed the adapted Wasserstam bary syndics of three CRR-type models. So on the top, you can see a process or CR-type model which has a kind of a big volatility in the beginning. And then I think at time three or something, or time four, the volatility is smaller. Whereas on the bottom left, you see a process which has a small volatility in the beginning. Small volatility in the beginning, and then the volatility is bigger after time three. On the bottom right, you see a process which has a small volatility up to the very end. And yeah, in the middle, we computed the barycenter. And what we end up with is something which almost looks like again a CR-type model. Just what happened is that the volatilities were smoothed out. So, I want to end my presentation. Here is a slide of the different papers which I was mentioning during my presentation. And of course, I'm very happy to take questions. Thank you very much for this very nice talk. Are there any Are there any questions or comments? Thank you. Very beautiful talk, Goody. Thank you so much. First question in the previous slide, when you take this vari center, just out of curiosity, this minimization over filtered probability. Filtered probability filtered processes, correct? Yes. Okay. And is there a way to define like what is the convex combination of filter processes or is there a naive way to combine two filter processes? I mean, for me, a barycenter is like a convex combination, right? I mean, if you change. Combination, right? I mean, if you change, or is in a certain sense a convex combination or a way of defining a convex combination on a set of processes. Sorry, what I want to say, I like your complex combination. I wonder what if there's another one. I mean, you could always go to this classical Wasserstein space, which is isometrically isomorphic to the Wasserstein space of stochastic process. Basel stands based on stochastic processes, and there you can just deal with it in the usual way, right? Just take the normal convex combination of two laws. Okay, okay, I see. So these are the only two things I can at the moment think of. And how is this translated then back into the space of filtered processes or is this not so direct? I would say it's not so direct. Well, I guess it's actually simple. Wait. So maybe it's like if you do this, you would simply consider the product of these two filtered probability spaces. So you take the products, but also the products of the filtrations. And then, yeah, like this, I would, and then you have to wait, then you do a normal correct simulation of the weight. Correct simulation of the bits or of the parts of the process. Okay, thank you. And if I may, another question just about duality. Okay. This one. So out of curiosity, in the first duality formula, why does the product measure appear or how come the product measures play such an important role in the first formula? The first formula I mean I mean maybe this is a little miss oh well I think the important part is so for the so you could replace the product coupling here with just the law of new at time one and the law of new at time one. Time one. So this is even an overkill to compute this bit like this. So you couldn't make sense what I'm saying. But because the function s is now incorporates terms, cross terms with x's and y's, right? Yes. Because I mean on the Because, I mean, on the one hand, the product coupling is always, of course, a bicausal coupling, so it fits also to this setting. So, not only to optimal transport, but it's also a good coupling for adapted transport. And the other yeah, I mean, the other thing it has to do with the set of The set of the dual functions because they are you could have taken any B Causal transport prime with the right marginals and that would have spit out the same value for a test. Of course, yeah, they will always, yeah. Good, thank you. Are there further questions? There's no questions from the from I mean this is related to Julia's question because I was thinking I understand why the entropic regularization works and that it's convenient because you can do this Breglon projection and compute the optimizers explicitly but on some level it it feels also that it's not the natural way to regularize this precisely because Precisely because of the product measure, also which I mentioned before. Then actually, the exponential of the entropy one, we really don't respect this adaptively. So is there another way to regularize that? With some other penalty terms which also be introduced to complexity, but which will be somehow adapted? I think that the entropy is in a certain sense adapted. Entropy is in a certain sense adapted because of the product property. So for me, yeah. So in my opinion, it is an adapted way of regularizing because precisely because of the product, how do you say product rule of entropy? Product property. The standardization property of the entropy. Property of the entropy. Yes. Because, yeah, I mean, I would have to write it down, but you get this kind of an adapted sum in a certain sorry? Just repeating that you think that is the natural way at all. No, I'm not saying that it's I think it is a way you one can do. I don't want to say that it's maybe the most natural. Say that it's maybe the most natural one, but in but I'm not saying that it's a bad, my point is just I want to say it's maybe it's not a bad one for doing it because it is in a certain sense adapted. You are not destroying your any structure by doing that. But yeah, but I mean, of course, you were also thinking. I mean, of course, we were also thinking if you can do other things, but yeah, we at least didn't come up yet with something else. I have another question. So in your assumptions of your convergence results, you assume that you can approximate or you have a sequence of measures which converge in adapted Vassar Stein distance to your measure mu. Now, if you have a given problem, Problem, yes, is it how difficult is it to find a nice sequence which is very low-dimensional in a certain sense? I know, I mean, I mean, you do want to find nice discrete measures, right? So that's lower dimensions. So, I mean, you want to find nice discrete measures which approximate your process. So, how easy is it to find them? I mean there is this I mean concerning approximation of measures there is this paper on the by Matthias Julio Daniel and so on on the adapted empirical measure so somehow yeah if you can sample from it you can find an approximate reason and it is And it is, yeah, and the convergence is more or less the same which you have for the classical empirical measure, the same rate. And on the other hand, I think, anyways, if you consider processes, then any natural way of approximating a process will yield also an approximation in the adapted big topology. Or at least, this is the feeling which I have that if you do something That if you do something in kind of which is adapted in time, then you will have, then you will end up with an approximation in a good sense. Okay, thanks. So I have another question. So I mean, this is, I guess, this is very much discrete time, what you're talking about. Yes. Yes. So, how much of this is available? In continuous time? Yes. I mean, of course, numerical, these numerical things are purely, I mean, I guess every all the results are purely discrete. Fair enough, yes. And I mean, continuous time, we are working on things. We are working on things, but continuous time is definitely not as nice as discrete time. So, in discrete time, yeah. I mean, in discrete time, optimal stopping is much nicer than continuous time optimal stopping. So, this is kind of already the first point which shows that things might get uglier. Let's see, uglier. So, I mean, at the moment, we are trying to get a better understanding of the topology, from the topological perspective in continuous time. And I would say that we are at the moment far away also from a geometric point of view on stochastic processes in continuous time. Yeah, to be very honest. Honest. Okay, thanks a lot. Are there any further questions? I asked another question about this P strictly greater than one in the theory. The fact that you don't have P equal to one, is that just linked to the fact that you don't control uniformity currently and you don't control your limits? Well, the closeness is the problem, or do you also does the geodesic structure? Uh, does the geodesic structure also get sort of perturbed and it's not clear that you traveled? So, to my understanding here, we have the same troubles as in the classical Wasserstein setting here. So, of course, P greater than one due to our tightness result is due to this probo of type result, of course, it's very convenient. It's very convenient to have p strictly bigger than one if you, yeah, because um, yeah, we know how compact sets look like. So yeah, for example, for the existence for Asserstein bary centers, one definitely needs p bigger than one. Yeah. Okay. Are there any further questions, comments? Okay, so if this is not the case, then let's thank Gutmun for this nice talk and also Jonas and Andre Matthias for the talks of this morning session, afternoon slash afternoon session. And we'll have a break for Uh, have a break for um, well, depending where you are: lunch, dinner, or breakfast. Um, so Martin, are you still there? Are you still there, Martin? Yes. So I have a question for Jonas that you wanted to defer to after the last talk. Yeah, yeah, sure. If he's still there, all right, let's okay. So, um, so my question was that when you take. So, my question was: that when you take p equals 1, your cells have a nice geometrical structure. So, the boundaries of the cells are piecewise hyperbolas with one of the foci at the point. So, the point is contained in the cell, which is not the case for T bigger than 1. Can you use this nice geometrical structure in any way to say anything interesting about your problem? I cannot use this property. So, as you said, for p equal to 1. said for p equal to one that's the only case where you really know that the particles are actually inside their own cells else it might happen that they are not even in their own cell for example for p equal to two you have some some let's say convex body these cells are convex um and then it might happen that the the particles are outside for larger p everything looks even more weird and I don't know how I could be able to use that fact Could be able to use that fact. What do you have in mind? I don't know, but so you were saying the cells were weird. They're weird, but they're not that weird. So I wondered if one can somehow take advantage of that. I mean, in the case p equals two, one can take advantage of it because it tells you the cells are convex, which is so connected and so on. No, I wouldn't know how to use this fact to, let's say, get something cool out of it. Something cool out of it. But certainly, that is a very different area of research to, let's say, tell more about the geometry of the cells themselves. So if you just, I mean, most of the random geometry is concerned with, let's say, for the Poisson point process and look at the Voronois cells. The Voronois cell is if these dual weights that I showed you in the talk, if they are all zero. Now, we have a very particular Very particular, let's say, tessellation or diagram here that is, let's say, restricted to the fact that all these cells have equal mass. I mean, we need that to do this optimal transport. And this fact that they all have equal mass is somehow, let's say, a nice fact for us, but it's very, let's say, boring for the people from random geometry who want to prove, I don't know, central limits theorems for the expected volume, but that if the volume is always the same, then there's... If the volume is always the same, then there's nothing to prove. On the other hand, of course, you can ask very different questions. For example, what is the circumference of these cells? What is another way of measuring how spread out these cells are? How many edges does one cell have? Things like that are usual questions from random geometry that would be interesting to study here for also these Wasserstein cells. And I don't know how to do it. Okay. So answer that your question, Robert? Yeah, thanks. Okay. Are there any further questions to Jonas or Matthias or Andre or Gutmund? Maybe to give the same question back, there was something in the chat asking about what happens to P equal to infinity, and I myself not, let's say, having a strong background. Let's say having a strong background in optimal transparency, have no idea how it could look like. How does p equal to infinity look like? Gives a different topology for one thing. Becomes a very strong distance because it controls the support. And it controls the support, yeah. So it's quite a different piece. Okay, then it's better to that I don't think about it. I mean, there's some work by Snepchev and Causes who look at this problem. Okay. Okay. That's good to know. That's good to know. Well, then, see you at some other time because I will get some dinner now, okay? See you guys here. Okay, so thanks a lot. See you all in, I don't know, 90 minutes or so. Enjoy lunch, dinner, breakfast. 