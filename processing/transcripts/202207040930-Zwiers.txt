And thank you very much for the invitation and for your ears and eyes. As I said to Leonard, it's a good time to have a sleep for an hour if that's how you're feeling inclined at the moment. So this is a photo that was provided by the BC Ministry of Transport and Infrastructure of the Whatcom Road interchange on the Trans-Canada Highway. As you go through the As you go through the Abbotsford area, and that's what it looked like around about November 16th, 2021. And so if you drove through there yesterday as I did, it looks perfectly normal again. None of that water is there. Agriculture seems to be proceeding as it was before. But this was certainly a very impactful event in British Columbia, as you know, the flooding. Columbia, as you know, the flooding that occurred in the middle of November basically cut off Vancouver from the rest of the country. So all of the surface transportation network was damaged, pipelines were closed, roads, rail and pipelines. So there was basically no access to the port from the largest port in Canada for the rest of the country. Of the country. I live in Victoria and so the Malahat Highway that connects Victoria to the rest of Vancouver Island was damaged as well. We had gasoline rationing throughout the lower mainland and in Victoria. There was very little gasoline to be had, for example. So Victoria is a funny place. Gasoline gets to Victoria by traveling over this hill in trucks. Traveling over this hill in trucks every day. The depots are all north of the Malahat. The population concentration is to the south. So there's certainly risk that was realized in this particular case, substantial risks that were realized. And had we done the research ahead of time, we might have been able to quantify to some extent what role climate change. What role climate change played in that? So it's an example of a risk to infrastructure that materialized into a significant impact. So I've said these things. I recently gave some testimony to the Canadian Senate Standing Committee on Agriculture and Forestry. And so that's going to be material that will underpin a substantial part of this. Substantial part of this talk. And then the second topic that I'm going to come to towards the end, I hope I won't run out of time, is to ask whether or not we're ready to mitigate projected increases in risk to infrastructure. And that's really a question about whether or not we have the statistical tools available to actually quantify what those risks are. So that, and if we have the engineering know-how that's necessary. Know-how that's necessary in order to be able to design for those risks. And I would posit that we're not quite ready for that yet. Okay, so I'll be showing you a few loose slides like this. So this is kind of the background material that I provided to the Senate. So first, a statement on global warming. So global mean surface air temperature during the most recent two decades was about one degree C. In two decades, it was about one degree C higher than it was during the early industrial period. There's no doubt that that's the case. Canada's warmed about twice as fast as the global average, so about two degrees warming here as opposed to one degree warming for the globe on average. And that's due to processes that are well understood. So, generally, as the globe warms, you see more warming over land than over oceans, and you see more warming, particularly. Oceans and see more warming, particularly at high northern latitudes than elsewhere, because of is albedo feedback processes. And almost all of that warming can be attributed to the run-up in greenhouse gas concentrations that has occurred since pre-industrial. So the kind of language that's being used by the IPCC now to describe that and describe the causes of that warming really leaves no doubt that our industrial activities are to blame. Activities are to blame. So then the impact on extreme precipitation. So theory and climate models both suggest that the intensity of extreme rainfall will increase somewhere between 6 and 7 percent for each one degree C of warming. So that's a relatively simple theory that tells us how the atmospheric water vapor content increases as the globe warms. And it happens at about And it happens at about that class, what's called the Clausius Claperone rate, six to seven percent per degree of warming. And so, if there's seven percent more moisture available, all else being equivalent during an extreme rainfall event, you'll end up with seven percent more moisture on the ground. And so, that's the simple idea behind what the climate models and the theory tell us about. Climate models and the theory tell us about how, on average, intense extreme rainfall will increase. So, if you look at the data, observed trends in extreme rainfall at long-running meteorological stations across the globe, confirm that this is happening. So, I'll show you some evidence of that. So, you can do a simple dumb statistical analysis of annual maximum one-day precipitation amounts at many, many different places, fit simple. Fit simple extreme value models that use global mean temperatures of covariate and ask, well, how does the scaling factor on a covariate change? And with time, as temperature increases, then the answer is turns out magically to be 7% per degree of warming. So that just kind of falls out of the data. You don't need theory for that. Need theory for that. Local trends are really noisy. So I was discussing that over the coffee break. And that makes that change really difficult to see at individual observing stations. So if you look for these trends locally, you're only apt to see these trends at a very small fraction of stations, simply because the tools that we have are not powerful enough against this background. If you look at the data over large If you look at the data over large areas, then you begin to see consistent evidence over those areas that human-induced greenhouse gas increases have increased the risk of extreme precipitation events. And if you look at climate change projections for the future, they indicate that these risks are simply going to increase. So now I'll go through some of the evidence that supports these statements. So this is a nicomo. So, this is a nice. Most of the photos in this deck are my photos, by the way. I'm the guy behind the lens, but this is an iconic place that has probably been photographed millions of times, an example of buildings being built on permafrost inappropriately and then sagging as time goes along. This is in Dawson City. So, here's the observed record of global mean temperature change. Of global mean temperature changes on the right-hand side, the black curve. The kind of orange stuff that surrounds that black curve is what we simulate with climate models. So a large number of climate models. So what is these are free-running models, ocean, atmosphere, land surface, carbon cycle, all interacting together, the thing that is specified to these climate models, an initial condition. Models and initial conditions, which are rapidly forgotten. And the other thing that's specified as time goes along is the atmospheric composition. And today, these models can be run in two different ways, either by specifying atmospheric concentrations of greenhouse gases, aerosols, variations in solar output, and variations in aerosol local. In aerosol loading caused by volcanic activity. So, those are the four primary external drivers. Or you can specify to these models emissions of CO2, and they will calculate the atmospheric concentrations of CO2. And they do so relatively reliably at a global scale. The blue-green kind of band at the bottom is what these models simulate. These models simulate over time 1850 to 2020, if you drive them just with the natural external forcing factors. So, the effects of volcanic activity and effects of solar output changes. And you see that there's inter-decadal variability and inter-annual variability that is produced by those forcers, particularly the volcanic activity. So, you can see You can see the impacts of volcanic events here, for example, on the global mean climate. But you don't see a secular trend in the simulated change in temperature if you leave out the greenhouse gases and aerosols. And then the figure on the left puts the current observed period, the 150. Observed period, the 150 years for which we have instrumental data, about 1850 to 2020, in the context of reconstructed global mean temperatures that are reconstructed from proxy data collected all over the world. So think of tree rings, corals, that kind of data. And so there's a statistical problem, a relatively substantial statistical problem in producing these reconstructed. Problem in producing these reconstructions because you only have this short period when we have instrumental data that you can use to calibrate the proxies. So it's kind of a data imputation problem, right? You have instrumental data 1850 to present. You have tree ring and other kinds of proxy data in this case from the beginning of the current era to the present, but no instrumental. But no instrumental data. So then you have to impute what the instrumental data would have been using those data sources. Okay, so this is what's projected for global mean temperature for the future. And it's an interesting presentation that was first used in the IPCC fifth assessment and was used again in the IPCC sixth assessment. So up until 1919, you see observations. So the black legals are the global mean temperature. Black wiggles are the global mean temperature variations from instrumental data. The gray band is what climate models simulate. If you provide all of the known external forcings to the climate models, you see there's a pretty decent correspondence. I pasted in the time scale at the bottom. What is actually on the x-axis is the Q. X-axis is the cumulative CO2 emissions since pre-industrial measured in gigatons of CO2 equivalent. And so you see there's almost a linear relationship between the temperature that's realized at any particular time and the amount that we have emitted up until that point. And so then you can project into the future and you can ask, well, where do you get in 2020 or 2050? Or 2050. And all of those endpoints are endpoints for 2050. And they depend on where you get to, simply depends on how much more you emit. And so the IPCC has used this collection of emission scenarios that it has prosaically called the SSPs. I even forget what the acronym stands for. Stands for. There's storylines about how emissions and society evolve as constrained by those emissions. And the numbers indicate essentially the cumulative emissions up to 2100, the amount of the strength of the warming effect in watts per square meter at year 2100. So the one that keeps us below about two degrees above pre-industrial, which is a number that the Which is a number that the Paris Agreement has targeted, is the SSP 1 2.6 scenario. So it's a scenario where emissions peak and then decline pretty quickly. Okay. Okay. So I don't have very many pictures of extreme precipitation, but this is a snowfall event in Victoria. And in fact, just off to the right of the sign there and beyond. Right of the sign there, and behind that little yellow box that's showing on the pole, is where our offices are located. So, we have we're located in perhaps the nicest real estate on the University of Victoria campus. It's a form of president's residence, but presidents no longer want to live in residences that are provided by universities, except maybe at the University of Western Ontario. Okay, so the observational Okay, so the observational studies suggest that this predicted intensification of extreme precipitation is occurring. So that if you look at the annual maximum one day rainfall event, so it's prosaically known as Rx one day, there are many, many, many studies that look at this particular indicator. So that number is broadly consistent with the Clausius-Clapeyron relation that says six to seven percent per degree of warming. And there's a glow growing number of And there's a growing number of long-term studies of changes in extreme precipitation that point to greenhouse gas emissions as the cause. And so I've listed some of them here. I think I'm involved in most of those in one way or another. But nevertheless, local detection of change is still very hard. And so here's an indication of what the challenges are. So let's having a discussion over coffee about where. Having a discussion over coffee about where precipitation data are available and how densely they're available. And you notice that there's not very much data available that is available for international exchange for study over Canada. Our north is really sparsely populated. There's good coverage in the United States. There's very good coverage in Europe, South Africa. China has a very dense coverage. China has a very dense observing network that has been, in essence, in operation since about 1950. Their observations have some challenge in the sense that almost all of their observing stations are in urban areas and areas that have urbanized very rapidly over the last 20 or 30 years. The colors indicate the direction of change. If they're If there are dark blue spots, those are changes where a man-kendl test tells you that the change is statistically significant. Okay, and so question is, well, at how many points? So does this simple test operate well? And so those two bar charts on the right-hand side, first, if you look at the one that's labeled significant decrease on the lower. Decrease on the lower right-hand side. There's a red dot there. 2% of stations show a significant decrease. And this is with a test that operates, two-sided tests that operates at the 5% significance level. And so you would expect about 2.5% of stations to show a significant decrease just as a consequence of false discovery. Okay, and that 2% is entirely consistent with what you would expect. Consistent with what you would expect in resampling of the same process over and over again. So that's what the bar chart tries to indicate is what the uncertainty in that number should be. Okay, whereas 9% or so of stations show a significant intensification of this number, and that lies well outside that expected uncertainty just due to false discovery. Due to false discovery, and so you might think, well, there's something going on in the background. There's about two-thirds of stations showing intensification, and at a few, you can actually see that the trends are significant. But these are tests with very low power based on relatively short time series. Nevertheless, you could bravely go ahead and fit simple extreme value model to the time series, the annual maximum time series at each location. Maximum time series at each location, non-stationary one, and then try to first deduce whether or not there's a significant relationship with temperature, which you see at many places, again at about 10% of locations. So you don't get any further in terms of making the connection with warming. It's simply another way of interrogating the same data. But you do learn from this that the little That the and the little bar chart on the right shows that the mean intensification per degree of warming that you deduce from fitting this little extreme value model to all of these data is 6.6% per degree of warming. That just falls out of the data without having to rely on theory at all. And that's much, much larger than you would expect. You would expect by random chance. So, those uncertainty bars there are determined by a bootstrapping approach to where you break the connection between temperature and extreme precipitation. Okay, so then we have a collection of studies that so-called detection and attribution studies that ask whether, more formally, whether or not the change in extreme percentage. Change in extreme precipitation that is simulated by climate models over the historical period is what you find in observations. So, this is a well-exercised collection of techniques that are used to study the impacts of greenhouse gas emissions on temperature. So, we have spatial patterns of change that we're looking for. Basically, these are linear These are linear regression type of problems where the climate models provide you with an estimate of what the pattern of change should look like, and you ask the data whether or not you see that pattern of change in the data. So these have been adapted to extreme precipitation using various kinds of approaches. And so recently we undertook a study with a postdoc who's just returned to China. And we undertook this study by using the station data directly as opposed to pre-processing it in some way or other. So we're asking, we're fitting extreme value models at individual locations, maximizing likelihood across the entire domain. And at each location, we're taking an estimate from An estimate from a collection of climate models as to how the location parameter in that extreme value distribution should change as a consequence of increasing greenhouse gas concentrations. And so we've been able to successfully demonstrate this technique. And this allows us to answer this question without having to first heavily pre-process the data. So almost all of these detection and attribution studies first rely on First, rely on reducing the dimensionality of the problem very substantially by spatially averaging and averaging cross-time, that kind of thing. So, we've been able to avoid that, which is very nice. The thing we study is the log of the annual maximum one-day precipitation amount rather than the annual maximum precipitation amount itself. And the reason for doing that is the Clausius-Glaperon relation that then makes the connection. That then makes the connection with temperature linear and therefore easier to deal with. Okay, so the little model that we fit, we considered about 5,100 stations across the globe. But remember what the globe looks like. It's sampled in some places and not sampled in many other places because of the data that's available. Available. And at each location, we fit an extreme value distribution with a time-dependent location parameter, time and location-dependent location parameter, a location-dependent scale parameter, and location-dependent shape parameter. And so it's via the location parameter that we bring in information from the climate models. So that's what the capital X is. Capital X is. You can consider multiple signals at one time due to greenhouse gases, due to aerosols, for example, if you wanted to, or due to human-induced forcing, natural forcing, a two-signal problem. That's often how these kinds of problems are solved. We make the scaling factors on these signals constant across space and time. So we're anticipating that. So we're anticipating that Clausius claparone will apply everywhere, or something like Clausius claparone. And then we use bootstrapping to determine uncertainties in the signals and in the account for uncertainties in the signals and in the noise in the observations. We're using a combined score equation technique that was developed by a student. It was developed by a student of Jun Yang at the University of Connecticut that makes this efficient, as opposed to we had an earlier version of this where we were using profile likelihood, and that turns out to be very, very expensive if you have more than one signal that you need to consider. We don't consider spatial or temporal dependence, although that's possible with a combined score equation technique, but we do. But we do account for that in the bootstrapping process. So the estimators could be a little bit more efficient by taking spatial dependence into account, but the inferences that we make are probably okay. So the data 1950 to 2014, this is typically the kind of period that's considered that instrumental data become much sparser prior to 1950. To 1950. We're considering there's always a lot of missing data when you're thinking about precipitation data. And so we deal with that. We allow data to be missing. We insist on having at least 45 years of data in that period and at least having three years of data during the last five years, so that the time series are anchored in the recent past. In the recent past. And the regions that are considered, we consider a global domain, some continental domains in blue, and then some subdomains in red. And these are areas that are of particular interest to the IPCC. We use signals that come from two kinds of climate models, from the Canadian climate model, for which there were very little. For which there were very large ensembles available for study. So, one of the things that you can do with climate models now is you can run them over and over again. Kind of like undertaking a Monte Carlo study of climate change, starting from different initial conditions, essentially producing independent realizations of the same transient process that brings you to the past and then to the future. And so, in any particular year under transient climate change, you have not one In climate change, you have not one annual maximum extreme precipitation amount that's simulated by that climate model at that location. You have 50 amounts. Okay, and so that from a statistical perspective, that helps out a lot. And then we also considered models that participated in the recent climate model intercomparison project that was used by the most recent IPCC report. And in that case, we're using We're using about eight or so different models that had both all forcing, that means greenhouse gases, aerosols, land use, volcanic forcing, aerosol forcing combined, and natural forcing only. And so if you consider those two signals, then you can deduce from that what the influence is due to humans acting on the climate, assuming linearity. Assuming linearity. Okay, so we used the individual station data, but it's a little bit hard to see what it is that we're looking at. And so what this shows is a gridded version of both the observations on the left-hand side and then corresponding model output from, in this case, the CMIP6 models. In this case, the CMIP6 models on the right-hand side from the off-forcing simulation and from the natural forcing only simulation. And so on the left-hand side, on those numbers indicate global average intensification of precipitation, I believe, per degree of warming that's deduced from the observations. Deduce from the observations, from those gridded observations, and the number is 6.1%. And from the climate models, using the same period and the same coverage, you get 7.0% per degree of warming. And if you take the natural forcing into account, only you'll only get a tenth of 1%. So that's basically flat. So the lower. So, the lower right-hand graph, you see that the signal just fluctuates a little bit randomly up and down near zero. It has some structure in the models on the right-hand side when you take greenhouse gas forcing into account. There's actually a lot of structure in the signal, but the way, because of where the observations are located, we can't see a lot of. Observations are located, we can't see a lot of that structure. So, we have places in the world where extreme precipitation weakens over time as a consequence of human-induced forcing on the climate system. But those areas tend to be in the subtropics, and you see just a little hint of that along the coast of South America in the simulations. What the model signal shows is that it's relatively smooth in the areas where we have data. In the areas where we have data, whereas in the observations, it's kind of noisy. But there's only one realization in the observations, or 46 realizations on the other side of the same thing. So the models are able to filter out a lot of the background natural internal variability. The area over China is interesting. So it's an area that's really intensely studied, and you see that there's a characteristic pattern there with an area. There with an area with decreasing extremes flanked by an area of increasing extremes. And that's a topic that has preoccupied Chinese scientists for the last five or 10 years. So it has very serious implications for them, for how they conduct agriculture and so on. We don't really understand yet what the causes are of this kind of bimodal structure, this drying-wetting pattern in China. Wetting pattern in China, but we think it has to do with changes in precipitation frequency as opposed to changes in circulation. So we just have a paper that's in the final stages of being accepted on that. Okay, so if you use this detection and attribution machinery, you end up with these scaling factors and their estimated uncertainty. And this is one of those MAC things that doesn't translate. It doesn't translate well to PC. I thought that had all gone away. There are two panels on this figure in the Microsoft world. They're overlaying on each other. So just concentrate on the one that is shown there, which is ignore the little inset at the bottom. So it shows these scaling factors for the global mean. And the red indicates the scaling factor. Indicates the scaling factor on the anthropogenic signal. The blue indicates the signal, the scaling factor on the natural signal. The closed images are when the signal comes from the Canadian Can ESM2 model. The open symbols correspond to the signals that come from the CMIP6 models. And you can see that both cases human-induced forcing on the climate system is detectable, and you may or may not be able to detect. Or may not be able to detect a weak natural influence on the climate system as well. Okay. Oh, this is, yeah, I'm sorry about that. I should have converted this to the PDF and anticipated that this was going to happen. This graph is supposed to show you an implication of these detected changes. These detected changes. So, because we did this using extreme value model, you can start to estimate return levels and/or return periods for a fixed level. And so you could take from this what you thought was a 20-year event early in the record, during the first, say, five-year period of the record, and deduce what the waiting time for the recurrence of that event would be at the end of the. Uh, at the end of the analysis period during the last five years, okay, and so the numbers that you see here, depending upon the scale that you're looking at at the global scale, remembering what the globe looks like, a 20-year event became a 16-year event with some uncertainty according to this model. So that's a kind of number that an engineer might actually be able to make a little bit of use from, and somebody who's thinking about risks of various. Who's thinking about risks of various kinds might be able to make a little bit of use of okay? Okay, and then so projections of future change. So this is the Dempster Highway. So it goes from runs up to Innevik from Dawson City. So somebody was So, somebody was certainly thinking about the, or should have been thinking about the future when they were thinking about this road. So, it overlays permafrost. And so, how do you keep that road in place in a warming world, a warming world that looks like this? So, this shows the link between local and global projected warming. So, the simulated change at two degrees global. Simulated change at two degrees global warming looks like the pattern on the left, where over Canada you get something like four degrees. The simulated warming at four degrees above global warming looks like the pattern on the right, where over Canada you might get as much as eight degrees or so. So there's a lot of the reason we're plotting these things in this particular way is because there's many, many. There are many, many emissions pathways to think about now. And people who are designing infrastructure often get, and dealing with adaptation kinds of questions, often get stuck on the question, well, what emissions pathway am I on? And so that kind of presents a roadblock and a way in which to circumvent at least part of that roadblock is to rephrase the question and ask, well, what should I? Phrase the question and ask: Well, what should I expect at this particular location if the globe warms by two degrees? Or what should I expect if the globe warms by four degrees relative to pre-industrial? And then for the particular adaptation question that you have in mind, you might ask, well, do I have to make, do I have, how many opportunities will I have to make decisions that pertain to that piece of infrastructure between now and when it gets to four degrees C? It gets to four degrees C. And so, if I'm making decisions on road surface material, for example, once every 20 years, then I don't need to worry about which emissions pathway I'm on. I just need to worry about how much warmer it's going to be in 20 years' time. And that's almost emissions pathway independent. But if I'm building a bridge or planting a forest that I'm hoping to harvest in 60 years' time or a bridge that's going to be in service for a century then. Be in service for a century, then I do need to think hard about what pathway I'm going to be on, whether it's going to be two degrees warmer or four degrees warmer. How do I hedge my bets in the process of doing that? Anyway, we've been trying to get the users to think about climate change in terms of impacts at a particular level of warming, as opposed to starting the conversation by, well, I don't know what path I'm on, so what do I do? Okay, you might pose the question a little bit differently. You might pose the question a little bit differently. So, what paths are compatible with assuring that I've managed risks in this particular way? And then make a judgment about whether or not those paths are likely to occur. You'll have to bring an economist into, so Paul would be an important guy to include in this discussion, because climate people know very little about how the global economy and the emissions that it produces. And the emissions that it produces, and how society is going to arrange itself is going to evolve. Okay, so here's a figure from the IPCC that shows what the link is between global temperature change and local mean precipitation change. And you see that these patterns are very similar at two degrees and four degrees. You see that there are places where mean precipitation increases. So in general, Canada is projected to become richer. General Canada is projected to become richer in water in a warmer world. There are also places where there are characteristic decreases. And one of the places that's been talked about a lot in the literature is the Mediterranean area where there's a lot of people. Even now, it's being remarked that these are climate change hotspots where there's a lot of drought stress. There's a lot of drought stress, and that is projected to increase in the future. But more water doesn't necessarily mean more forestry or more agriculture in a particular place because there's also more atmospheric demand. And so, although this is less certain, what the IPCC shows in these figures is how soil moisture, as it's represented. Moisture, as it's represented in climate models, changes as a function of global mean temperature. And again, you see the same pattern of change, whether you get to two degrees or four degrees, but you see some interesting differences between this and this. So mean precipitation in Canada is projected to increase, but that doesn't mean that That the land surface will remain as moist as it is at present. So, there's still projection that over a large part of the Canadian land mass, over a large part of North America, and particularly in the Amazon, soil moisture will decrease. You have to take this with a grain of salt because the land surface models in the climate models haven't been studied nearly as intensely as other components of the climate models. Components of the climate models. Okay, so now what about the future of extreme precipitation? So, this is something we studied with a former Chinese postdoc who's in Shanghai now. And we produced in this process a lot of material that was used in the recent IPCC report. And so, what this is showing is projected changes in 50-year, one-day precipitation. 50-year one-day precipitation events at one degree above recent climate, so about two degrees above pre-industrial, and at three degrees above recent climate, so about four degrees above pre-industrial. And you see it's kind of noisy. You see a pattern of change that we've seen in previous Previous generations of climate models as well. So, first of all, you see intensification of extreme precipitation occurring in many more places than in places where mean precipitation increases. That's maybe not surprising. But you still see some places where intense precipitation is suppressed in the warmer climate and in characteristic places. And so it's useful. And so it's useful to think about the physics that underlies this. So, if I go back and show you this pattern of simulated change in mean precipitation, you see on the equator, a very large increase in mean precipitation. You would also expect there to be, therefore, a very large increase in. Increase in extreme precipitation, simply because these exponentially shaped distributions, the variance and the mean are linked to each other. Physically, what's going on, this is an area called the intertropical convergence zone. And so this low-level convergence of moisture on the equator, strong rising motion on the equator, so heavy precipitation as the water in that air rains out. That air rains out, you end up with dry air entering the stratosphere, and then that dry air spreads out and sinks. And it's these dry places where it sinks. And that sinking motion enhances and strength and widens in a pattern that's such that you see some places, fortunately, or where not too many people live, where extreme precipitation actually Extreme precipitation actually decreases. Okay, you can also express this as a change in the frequency of the occurrence of an event of a particular magnitude. Okay, and so that's how those patterns are shown here at two degrees above pre-industrial and four degrees above pre-industrial. So where the there's dark. So, where there's dark blue on the right-hand side, for example, a 50-year event at three degrees above the recent climate is projected to occur perhaps once every 12 and a half years, four times as often or six times as often as at present. So, if you have infrastructure that's designed to deal with what is currently a 50-year event. Is currently a 50-year event, and its capacity is going to be exceeded often in the future, at least in the substantially warmer climate. So that was the background on climate change and extreme precipitation that I provided to the Senate. And so now about the November 2021 events. So we have a paper that was recently published in Weather and Climate Extremes that came out of a And climate extremes that came out of a rapid attribution study of this event. And so, what that means is it's scientific ambulance chasing. An extreme event occurs and a team forms to analyze that extreme event and tries to post a paper within six weeks or so of the event. And so, in this case, a paper was posted at the end of December, early January. Early January analyzing the event, which is a pretty complex event, and then that paper went through peer review, and it's now appeared in a journal called Weather and Climate Extremes. So this event was caused by an intense atmospheric river, which is a flow of atmospheric water vapor from the subtropical Pacific Ocean, sometimes called Pineapple Express, because it often looks like it's originating around the It's originating around the Hawaii area. But if you see satellite photos of these things, they're narrow filamented flows. And basically, what's going on is that there's a low pressure system, rotating mass of air, and that rotating mass of air strips off moisture from the subtropical band, which is always very heavily moisture laden. It's actually fun to watch movies of these things happen. So this particular So, this particular atmospheric river was aligned with the Fraser Valley in such a way that it was able to drive moisture quite a long ways inland. And so, with that particular alignment, it's estimated to have been something like a one in 12 year event. So, we have pretty good data from recent reanalyses that can be used for that kind of analysis. So, we have some confidence that that number is more or less right. So, there's all around the Well, there's all around the Fraser Valley, there's topography that lifts up the air. And so the uplift of that moist air by the mountains resulted in large amounts of precipitation over a two-day period. We had some places, some locations with more than 300 millimeters in southwest Vancouver Island and in the mountains around the Fraser Valley. The average amount across the affected area, so if you draw blocks around the area, spatially average. Spatially average, calculate that number for each day. Do an extreme value analysis on that spatially average number. Then, depending on the data set, you would estimate this to have been somewhere between a one and 50-year event or one and 100-year event, a pretty big precipitation event over a relatively large area. And so, the heavy precipitation and a warm atmosphere during the event produced high. During the event, produced high damaging stream flows in multiple river basins, and there's a list of them there: the Nuk Sac, the Chilliwack, Coquihalla, Coldwater, Smilkamine, Tulamine. When you drive up the Coquihalla, for the first half of the trip, the highway follows the Coquahalla River. And for the second half of the trip, it's following the Coldwater River, which discharges near Merritt. The Nook Sac is a big. The Nuk Sac is a basin just south of the Sumas Prairie, and it normally doesn't discharge into the Sumac Prairie, but under these particular circumstances, it did. That's where the water came from that filled the Whatcom Road interchange area and other areas. So the recorded flows in some of these basins are estimated to have exceeded one in 100-year events. And we have to take those numbers. And we have to take those numbers with a huge grain of salt because you can imagine a river basin like the Chilliwack. There's a gauge installed at a particular place. And what the gauge is doing is it's measuring water velocity and water height. And based on those two pieces of information, it's inferring what volume of water is being carried past that point. But that requires knowledge of the shape of the channel. The shape of the channel, and during an extreme precipitation event, the channel is being reshaped. And so it's only after the fact that people go back and actually determine what the new profile of the channel is that you get some notion of really what the flow was during that event. I don't think the data have been updated yet. There are also instances when the gauge itself. When the gauge itself just entirely gets washed out. And so these are photos provided by the BC Ministry of Transport and Infrastructure, but they give you some idea what the damage was like. So in the upper left-hand corner on the Coquihalla Highway, in the lower right-hand corner, the Malahat Highway that serves Victoria, Highway 1 near. Highway one here with Lytton, you see a rail line dangling across what used to be a piece of land. Okay, so here's the atmospheric river and the circulation situation. If you look at the situation on November 15th or November 14th, what you're looking at are What you're looking at is an indication of what the 500 millibar geopotential surface looks like in the middle of the atmosphere. So it provides you with some information about the flow. And the flow is more or less basically lined up with those contours that you see. The tighter those contours are, the stronger the flow, and the greater the amount of. The greater the amount of moisture that's being transported from the Pacific inland. And so, if you look at the November 14th or November 15th moisture transport maps on the right-hand side, you see what a characteristic atmospheric river looks like. And there's a little box that's drawn there. That's where the extreme precipitation occurred and where a lot of that moisture was delivered. They're called rivers because they're sometimes able to. Rivers because they're sometimes able to move as much water as the Mississippi does at the same rate as the Mississippi moves through. So it's really large volumes of water that the atmosphere moves in in this particular way. So compared to all atmospheric rivers in the region, the 2021 that occur in the region, the 2021 event wasn't particularly unusual, a one-in-three-year event or something like that. Compared to at the Compared to atmospheric rivers that line up more directly with the Fraser Valley, something like a one in 12 year event. This shows you the upper panel, the two-day precipitation amounts, contoured by location. This is from a two and a half kilometer resolution precipitation analysis produced by Environment Canada called CAPPA. Canada called CAPA C-A-P-A and there are some dots that overlay that and those are those are measurements from station data from rain gauges. So you see there's a reasonable but not perfect correspondence between the rain gauge data and the Kappa data. And in the lower panel, there's also a time series of annual maximum two-day precipitation. Two-day precipitation amounts over that area calculated from a recent reanalysis called the European ECMWF, European Center for Medium Range Forecasting Reanalysis, version 5. It's a mouthful. The ECMWF is the world's premier weather forecasting center for medium-range weather forecasts. There is no center in the world that can produce forecasts that are skillful. Produce forecasts that are skillful. Canadians actually come pretty close. So, our Canadian weather forecasts are pretty good and are quite competitive with ECMWF forecasts at the global scale. You see that these two data sets have a lot of common variation, so they're agreeing on the representation of precipitation. Kappa has the CAMPA has the event that occurred in November as being more intense than Euro 5 does by a substantial amount. And so, we'll say more than that. We tried to deduce that based on era five, this would have been something like a 50-year event based on the shorter Kappa record, something like a 100-year event, but with very wide uncertainty. 100-year event, but with very wide uncertainty. Okay, so then the causes. So the atmospheric river-induced precipitation was the dominant factor driving the extreme stream flow. So when you talk to hydrologists, they spent a lot of time thinking about in these particular river basins that were affected by what the snow stored snow in the basins. Snow in the basins could have contributed to the effect. So, this is in mid-November, and at elevation, there's already snow present. So, it warms up and rains at the same time. Then not only is the atmosphere warm, but the warm water falling on the snow also accelerates snow melt. And so, you can have a substantial snowpack reduction even in mid-November during this kind of an event. Okay, so the hydrogen. So, the hydrologists on the team determined that, depending upon the river basin of interest, snowmelt contributed to something like somewhere between one-sixth and one-third of the water that entered the river basins and produced flow. So, rainfall was a dominant thing, but snowfall is certainly important. So, a lot of discussion about river basin preconditioning that may have influenced. May have influenced the event. And so it was a very wet fall. If you look at cumulative precipitation during the six weeks running up to the time of this event, that runs at almost the upper quantile, or top or second quantile amongst or top two or three of these cumulative precipitation time series. Precipitation time series. So the land was pretty saturated. And then there are also questions about what the heat dome event of June 2021 might have done in terms of changing the land surface structure and its ability to accommodate moisture. And also some speculation about whether or not wildfire in some of these basins had affected their behavior. Their behavior. So, we were really fortunate to have available not just climate model output, but also hydrologic model output. So, we use the CAN ESM2 large ensemble climate model simulations to drive our hydrologic model at PKX. So, we have 50 simulations of daily stream flow on the Fraser basin from. From each of 50 simulations over a period running from 1950 up to 2100. And so we're able to interrogate this data and ask this data empirically about whether or not we're seeing changes in extreme stream flow as a consequence of warming in the system, as it's simulated by that combination of models. And so that was an aspect of this study. And so that was an aspect of this study as well. When we're using climate models and this streamflow model, we deduced that the atmospheric river event and the resulting precipitation event, their likelihood had been increased by roughly 50% as a consequence of human influence on the climate system since pre-industrial, and that the The streamflow event had been made more, the probability of a similar streamflow event had increased by something like 100%. These are purely climate model results, hydrologic model results. So they're not constrained by observations. And so in these event attribution studies, the role of the observations as The role of the observations, as in this case, is to define the event. And then we ask the tool that we have available what the odds of that event were in the unperturbed climate or in the recent climate and in the climate that has been perturbed. And then we compare those likelihoods. And so this is really very, very model dependent, which is something that has to be appreciated when you're reading this. Be appreciated when you're reading this literature. Okay, I think I'm going to skip over this analysis of the importance of some of the hydrologic features. I'll go to the summary of the event, and then there's another topic that I want to broach briefly. So, in terms of the rarity of the event, In terms of the rarity of the event, or roughly speaking, the atmospheric river about one in 10 years, the two-day precipitation somewhere between 1 in 50 and 1 in 100 years. The stream flow maybe in excess of 1 in 100 years, at least in some locations. Dominated by precipitation and the moisture that was provided by the atmospheric river, but with important contributions from snowmelt, human-induced climate change appears. Human-induced climate change appears to have increased the probability of the atmospheric river event and the resulting precipitation and flooding. And we can only speculate really about whether or not the HETOMA event and the 2021 wildfires or earlier wildfires had been a factor in affecting drainage basin function in the basins where this occurred. Okay, so looking forward to infrastructure design. So, the infrastructure design challenge and ensuring future reliability of the stuff that is built. So, here's a picture of the road that connects Inavikta Tukta-Yuktuk as a new road. I think we drove on at the second year in service. Pretty exciting. This is in the north, they still know how to build things quickly and relatively low cost. So, this was, I believe, this road was built for. Built for under $200 million, stretches quite a distance through the Mackenzie Delta. It's been learned that the way in which to build these roads is by laying fill over top of the existing permafrost. You don't disturb anything that lies on the surface. You simply lay fill on top of it. And the layer of fill is about two meters deep. So it's quite impressive to be driving along on this two-meter thick layer of gravel. But that's what it takes to insulate. But that's what it takes to insulate the road surface from the ice underneath so that the ice underneath continues to support the road over the long term, they hope. Okay, so reliability of structures in a changing climate. So in a changing, ideally a structure that is built, I mean, there are many reasons for a structure to structure's reliability, probability of failure to change over. Failure to change over time as it ages and so on. But ideally, you would like to think that the reliability of this structure is constant over time so that the people occupying it today are people who are occupying it 20 years from now or 30 years from now are just as safe as the people who occupy it today. So, but climate is changing, and so some climate loads that will affect climate change. climate loads that will affect climatic loads that that affect this building will change and so it's we shouldn't expect that the reliability of the structure will remain constant. There's a complex question to try to answer because some loads will increase and others will decrease. Think of snow load on this building, that's apt to decrease over time. But if you think of thermal loads on this building, they are apt to increase over time. Apt to increase over time. And if you think of wind loads on this building, we're maybe not able to answer the question yet. So it's hard to know exactly how to guide engineers in thinking about this. But probability of failure due to climatic load seeds is not going to vary monotonically over time. And I think that's something that I don't think they really have their heads around yet. Heads around yet. Current practice assumes a constant probability of climatic load exceedance, but that's really no longer appropriate in a changing climate. That's starting to be recognized by engineers, but it's not generally part of their practice yet. There's a question of what target they should design for. So their responsibility is to produce a building or a fleet of buildings that are Are collectively reliable, and they succeed in doing that in Canada. So we have very few instances of bridge or building collapses that make it into the headlines. But in a world where reliability changes in a non-constant fashion as a consequence of changing climatic loads, the question arises: well, how should they go about designing these structures? Should you think about the initial Designing these structures, should you think about the initial probability of failure? Should you think about the average probability of failure over the design service life of the structure so that it's similar to current average probability of failure? Or should you think of end of service life probability of failure? So, depending on who you are, your interest in what the design target should be might be different. So, the question arises. So, the question arises: well, can one design in such a way that routine periodic maintenance and refresh can maintain a given reliability structure over target, over time? Assuming a 20-year maintenance refresh cycle. So, every 20 years you might have to replace a heating plant or cooling plant. So, can you design the interior of the building in such a way so that you can move adequate, so there's enough space so that you can move adequate volumes of water or air? Or air in order to keep people comfortable. So I'm not sure that kind of thinking is presently being undertaken when a building is designed. And how do you approach such an issue and socialize it in an acceptable way that's acceptable to engineers, where engineering practice is very highly structured. They know what they have to do in order to exercise due diligence and sign off on their program. Diligence and sign off on their projects, the user and the investor communities. So it's a very broad issue that hasn't been broached yet. And this issue is being complicated by a move in the engineering community to move from current engineering practices in Canada, which are so-called uniform hazard engineering practices, to uniform risk. Uniform risk engineering practices. So, just to give you an example of a simple uniform hazard engineering practice, if you're building a dock, you might design, do an analysis to determine what the highest estimated wave height might be during some period. Okay, and so you want the structure to be larger than that so that it doesn't get knocked apart by. That doesn't get knocked apart by waves arriving at the shore. And then, just to be sure, you'll add another two feet. Okay, so that's now a uniform hazard approach because the reliability of that dock will vary from location to location. That extra two feet, that extra half meter, will buy you a bigger increase in reliability in some places than in other places, depending on the way. Places than in other places, depending on the wave climate in that particular area. So, engineers have recognized this and they want to move to a uniform risk approach. So, a uniform risk approach involves using the local climatic information in a more detailed way in order to be able to build structures here in Kelowna, in Victoria, in Calgary, Toronto, in a way. In a way such that the reliability of the building can be expected to be the same in all of those locations. So, taking the local climate into account. And so, rather than adding a fixed load factor or multiplying by a fixed load factor everywhere, what they're pushing us as climatologists to do is to think about: well, how do I estimate points that are much further in the tail? Okay, so that's how they're taking. Okay, so that's how they're taking into account uncertainty. It's not to design for a 50-year wind load, for example, but to design for somewhere between a 700 or 3000-year wind load, depending on the importance of the building. And so they want this really localized. This is happening in the United States now, it's going to happen in Canada. They would really like these localized information about what these climatic loads are. About what these climatic loads are for points that are very deep in the tail of the observations, which in Canada are really scarce. So we don't have a lot of long-running, in the case of windlows, anemometer records. The records that are available cover relatively short periods. They're almost certainly inhomogeneous as instruments change and stuff around the anemometers changes and so on. Okay, so with those short records, So, with those short records, you might kind of hesitantly provide an engineer with a 50-year wind load, but you certainly wouldn't want to feel comfortable giving an engineer an estimate of a 700 or even a 3000-year wind load. So, that consideration led us to think about the kinds of tools. The kinds of tools that we often use in climate science to estimate points that are far in the tail. And so, this is work that a postdoc did with me. And it took advantage of a large on again, a large ensemble in this case of regional climate simulations for North America that are available a little bit higher resolution. And the question here. And the question here is about again about extreme precipitation, but you can ask this question about extreme wind speeds as well, as simulated by that model. And so we had in this particular case, 35 of those runs had hourly precipitation archived. And so in any particular year, you have 35 annual 35 annual maxima of hourly precipitation simulated by that model, 1750 annual extremes in total for the period 1951 to 2000. Okay, so if you take this period as being roughly stationary, the climate change signal in extreme precipitation is still relatively weak at individual locations. Then we have a lot of data. Then we have a lot of data that we can use to estimate 100-year return level or 1,000-year return level, fitting GEV distributions, for example. So we're sampling at the annual maximum point, block maximum is one year. And we can compare that directly with the empirical quantiles that you can extract from that data. And so those are the biases that are shown here. The biases get to be pretty large when you go deep in the tail. when you go deep in the tail using the kind of tool that you might that an engineer might junior engineer at least might use to to to estimate tail values okay so here are a couple of examples where that illustrate how these biases come about and so showing here two locations one for vancouver one for Vancouver, one for so grid box that covers Vancouver, another the grid box that covers Denver. And there are on these plots two GEB distributions that are displayed. One is fitted to 50 annual maxima from one of those 35 simulations, so the blue. And the other is a GV distribution fitted to all 1750 available. To all 1750 available annual maxima in the red. Okay, so the uncertainty in the fit as a consequence of parameter estimation certainly goes down. But the quality of the fit doesn't really improve. And it doesn't improve. So there's something particular going on in Vancouver. You have basically two tail shapes. And one of those tail shapes corresponds to atmospheric rivers that that climate model simulates, intense atmospheric rivers that occur very occasionally. So you kind of have a mixture problem. There's a different situation in Denver. Benalia asked, well, how does the estimated shape parameter depend upon where you're Shape parameter depends upon where you're sampling in the distribution. So we have 1750 years of data. And so you could sample in different places. And when using observations, you sample by using the annual maximum. There's lots of reasons for that, but one reason is that you don't want to sample more frequently generally because of strong annual cycles. So you want to sample the time of year when the biggest event occurs. So you could sample. So you could sample every, you could sample the annual maximum, you could sample the biannual maximum, once every five-year maximum, so on. So you could make the blocks longer. Okay, so what happens when you make the blocks longer? And so at Vancouver, you see that the shape parameter basically keeps changing as you vary the block length from one year all the way out to 20 years. So it takes a very long time until you're consistently sampling. Consistently sampling that second part of the tail in every block. Okay, and that's where you need to be sampling in order to be able to then be able to extrapolate out to a thousand years, assuming that the tail behavior doesn't change beyond that point again. So that's something we simply don't know. And so that makes this Denver it looks like you need to have blocks of length five years. You need to have blocks of length five years in order to be able to sample all of the processes that produce extreme precipitation in that model. The effects of all of those processes consistently within each block. Okay, and so this is a kind of question that doesn't get asked very often yet about how we're applying the tools that we have. Engineers have rational methods, they have lots of methods for. Lots of methods for doing storm system-dependent kinds of extreme value analyses, but we really don't, they're not studied nearly as extensively as or have nearly the same strength of mathematical foundation as the standard tools that we use. And so maybe we don't know a lot about the reliability of those. Okay. And then so. So Benelias asked a question, well, what happens? So those are only two places. So what happens if I use 10-year block maximum rather than one-year block maximum at all locations that this model simulates? And the answer is you do a pretty good job. But of course, we can't do that with observations because many places we only have 30 years of data. And so we wouldn't be able to do an extreme value. And so we wouldn't be able to do an extreme value analysis by fitting a GEV distribution to three numbers. Okay. A little bit more about the same time, we were thinking about another approach to this problem. And so how do you bring a little bit of physics into this problem? And the thought was: well, maybe you could do a bivariate extreme value analysis. So partition. So, partition rainfall into a couple of components. And one component is precipitable water. It's the amount of water vapor in the atmospheric column that could potentially fall down at any particular instant. And the second is precipitation efficiency, which is precipitation divided by precipitable water. So if you think of an atmospheric river that's continually bringing moisture into the atmospheric column. Into the atmospheric column, and you can have precipitation efficiency well above one because the atmosphere keeps replenishing the rainfall that falls out on a time-step basis. So you could fit a bivariate extreme value distribution. We use the semi-empirical method of Heffernan and Ton to do that, which is hard to apply. And then Once you fitted that bivariate distribution, then by sampling from that distribution, you could deduce what the distribution of the marginal distribution for precipitation itself is. So you end up with a more flexible tail shape than you would just by fitting a GV distribution. And so this figure on the left-hand side shows the bias in estimates of thousand-year levels that. Estimates of thousand-year levels that result by doing that when you're using only a 50-year sample. So, with a lot of work, you can actually get around some of these issues. Whereas the analysis of precipitation amount directly using just the GEV distribution and sampling at the annual maximum point, produces for this particular number, which is a different number than the number we studied previously. Than the number we studied previously, but produces for this particular number again large biases. So there's a potential solution by thinking a little bit about how to bring physics into this. And by splitting precipitation into two components, we're splitting it into a component, the precipitable water that has a thermodynamic interpretation. As it gets warmer, the amount of water vapor in the atmosphere column increases. Water vapor in the atmospheric column increases, and where the other component is dynamic. So, what does the atmosphere carry into the atmospheric column over the period when the event occurs? Okay, so what role does climate change play, has climate change played over the last 50 years when we're thinking about risk quantification? And so, certainly in Canada, we have a lot of data limitations, but generally we have a lot of data limitations. But generally, we have a lot of data limitations. So it's really hard to fit non-stationary extreme value models to short records. You run out of parameters and information really quickly. You need much larger samples than we have access to to do that. We have climate model limitations. So our climate models do not run at convection permitting modes. So we're not simulating the effects of convection on extreme precision. Convection on extreme precipitation. We're parameterizing the effects of precipitation at the resolution that the climate model resolves. So, climate model, a high-resolution global climate model has 50 kilometer, 100 kilometer resolution. And so, over in a 100-kilometer grid box, the climate models are using physical reasoning to try to understand what the mean effect of convection is at that scale. At that scale. They're not simulating the individual thunderstorms that take place. There's in doing risk quantification for engineering purposes, but I'm sure for other purposes as well, there's a huge demand for specificity. So stuff gets built in a place. And so the questions are about that place, you know, and investments get made in a place and so on. So the questions tend to be more. So, the questions tend to be more or less local, and yet that's exactly so, but we run out of steam locally. We might be able to say what human influence on extreme precipitation has been when we consider all of the data that's available on a continental scale or on a half-continental scale, but we can't do so with any reliability locally. And then we're being asked to extrapolate very far into the deep upper tail, but I don't think we really know how to do that yet. How to do that yet. So that hasn't been researched very much. So projections indicate that risks will increase, but it's still hard to be confident in specific projections. There are, as I mentioned, some serious climate model limitations. So there's really a lot of excitement about convection permitting models. They are very, very expensive. So at the moment, people are So at the moment, people are the largest convection-permitting experiments that are available are single simulations for the upper two-thirds of North or the lower two-thirds of North America, for example. There's another experiment underway running for about 15 years for the present climate and 15 years for the future climate. So you're interested in something that's going on in Cologne. In something that's going on in Kelowna. And so you both point at the four-kilometer grid box in that model that covers downtown Kelowna. And the question is: well, what can I say about 100-year precipitation event simulated by that model based on 50 years of data simulated by that model? And then, you know, how does that change in the warmer climate? Okay, we're not there yet. So, to understand the economic challenge that goes with this when the climate Goes with this. When the climate model resolution goes from 100 kilometers to 50 kilometers, the cost goes up by an order of magnitude. There are four times as many grid boxes in the horizontal. I need to increase vertical resolution somewhat to go along with that. And I need to shorten the time step in order to keep the model numerically stable. So it's at least an order of magnitude more expensive to run at 50 kilometer resolution than it is. At 50 kilometer resolution than it is at 100 kilometer resolution. And I really ideally to answer questions about localized extreme precipitation and how convective events might change extreme precipitation and localized winds and wind blows on structures. I want to be running at two kilometers or one kilometer resolution. So many, many, many orders of magnitude. And in order to have confidence in what comes out of that model, you need many groups being familiar with those models, being able to run them to be, there are still things in these models that need to be parameterized and understood. The land surface, we need very detailed information about the land surface in order to run these models. But in order to have confidence, But in order to have confidence as a community in what they're telling us, we need the entire community to be using these tools. And so that the computing that's required to do that just doesn't exist yet. I think maybe I'll stop at that point. But just to say that there are lots of challenges and still lots of research to be done in order to be able to support. To be done in order to be able to support risk assessment.