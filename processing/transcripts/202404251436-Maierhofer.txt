Actually, also for putting me after Trushan, because that makes my talk all easier. Because a lot of the topics in this presentation look fairly similar or closely related to what we just saw. So, this is some joint work that I did together with Yvonne, Valeria, Ivan, and Katrina, who are based in Paris and in Nancy in France. And we're interested in constructing what are called structure-preserving lower galaxy integrators for dispersive non-linear systems. So, what are dispersive non-linear systems? Two prime examples. The first one is the non-linear Schrodinger. The first one is the nonlinear Schrodinger equation that we've seen extensively this week already. And the second example would be the Curdovic-Lebries equation. But what we will see in this presentation more generally applies to dispersive nonlinear systems that take the shape shown on the bottom of the side here. So you have some dispersive linear operator and a polynomial type non-linear. And what we're interested in is low-regality integration is really to try and solve the initial value problem for these kind of equations numerically when our initial data has limited differentiability in the spatial direction. In the spatial direction. And I think it's appeared as a question a few times this week already. So, why low regularity does it have any applications? And I'm personally not an application guy, but my background is more from the mathematical perspective. But I still wanted to give you a few examples where I think this sort of lowering value phenomena appear and I'll develop it. The first example I wanted to give is the so-called Talbot effect, which appears when you simulate the non-linear Schrodinger equation with step function initially data. And it's a localization effect. So essentially, what happens And it's a localization event. So essentially, what happens when you simulate this, the dynamics look quite incoherent for most times, but if you look carefully at some isolated time instances, your solution localizes. And just to make that a bit more explicit, I've taken a few snapshots here. And so essentially, when you hit a rational multiple of pi in your time evolution, your solution localizes and becomes something that looks almost like a linear combination of step functions. So if you want to simulate a phenomenon like this accurately, you have to deal with initial data that is rough, that it's not even continuous. Graph, that it's not even continuous. Second example I want to give is something I'll come back to at the end of my presentation, and it has to do with the simulation of vortex filaments in ideal fluids. So imagine you smoke a pipe and you blow a smoke ring. Now that's a vortex filament propagating in air, in fluid in front of you, and you might be interested in understanding how that vortex filament evolves as time progresses. And that's governed by what's called the vortex filament equation that you can see on the slide here. Now, if you blow a smoke ring, Now, if you blow a smoke ring, a ring is a stable solution of that equation, so that won't change the shape. But in many of our daily encounters with engineering devices, there are actually vortex filaments that are being generated that have different shapes. And one of these examples would be a jet passing through an anungated nozzle like you have in a hairdryer or an air purifier. And the voltage experiment evolution of the kind of flow that's generated here looks a bit like what you can see on the left of the side here. Now, this looks fairly innocuous. Now, this looks fairly innocuous from the outside, but what we're dealing with here is really again a low-regularity initial data. And the reason for that is that the initial data is generated or determined by the shape of our engineering device. So it's piecewise straight along these straight edges. And then there are something that looks almost like a semicircle fashion, the bottom and top. That means that the initial curvature of our vortex experiment is p-size constant, constant zero. And this piece is constant, constant zero along with strategies and constant non-zero along with semicircles. So we have again to deal with initial data that's not even twice continuously differential. And the final example I wanted to give is something that's not exactly a low regularity taxonomy, but it turns out, and I hope that will become a bit clearer as I progress in this introduction of my talk, that the kind of challenges that I encountered with classical methods when they applied to lowering data data, I actually also encountered. Data are actually also encountered when you try to simulate data that has, in some sense, large derivative values. And some examples where that might appear are highly oscillatory solutions or localized solutions such as solitons or domain hole states in paramagnetism. So why can't we use the classical method for this? And luckily we've already had extensive introductions to this, but I just wanted to break that point down once again. And I'm going to hit on the splitting methods once more. I'm sorry for that. I really do think splitting methods are excellent. I really do think splitting methods are excellent techniques for solving these kinds of problems, but they do suffer, like many methods, from a phenomenon called water reduction in this low regularity cell. And so, the numerical example that I want to show you here is the simulation of the non-Linear Schrodinger equation with low regularity initial data. And I'm comparing the convergence properties of two splitting methods here, the strong splitting and the least splitting. Now, the plot, just for those who are not used to this kind of plots, just quickly is a convergence plot that we've already seen a few times on the X. That we've already seen a few times. On the x-axis, we've got the time step, y-axis is the error at fixed simulation time. So, if your method converges, as you go to left, you should see a store of decay in the error code. And if you're in a smooth setting, so with highly regular initial data, then you observe indeed second-order conversions for the strong splitting, first-order conversions for the splitting. So, that's one thing. But if you start to reduce the regularity in your initial data, then the convergence properties of the Then the convergence properties of these methods decay and decrease. And so, if you reduce it to H3 initial data, for example, this transcription can no longer attain its optimal second-order convergence rate. And if you reduce it further to, for example, H2 initial data, neither of these methods is able to attain its optimal rate of convergence. So why does that happen? One way to see it is the constant solution of these types of dispersive non-empty systems, and we've already seen that on Tushang's slides. We've already seen that on Chushang's slides earlier. The idea is that you express the non-initiating equation instead of as a partial differential equation as an integral equation here. And so I haven't solved anything here. This is still an implicit equation, handing on the unknown on the right-hand side. But what you can do now is you can take this variations of constant solution and you can iterate it into this expression. So you can replace all the terms in the integrand with the same formula, and you end up with an expression that I've shown you here. And now, what I've neglected here are terms that What I've neglected here are terms that essentially involve double integrals that do not contribute to the regularity assumptions that you have to make when you use the numerical method to approximate the solution of these equations. So we'll forget about the ordered tau squared terms and we look at this term in the integral in red, which is the important contribution to the regularity assumptions that we have to make when we use a numerical integrator to approximate the signature. And these are terms that are called the non-linear frequency interactions of the dispersive part of the non-linearity now. Of the dispersive part of the nonlinearity in Naon division. And so, really, the essence is how well can we approximate these nonlinear frequency interactions in Naundo-Markov scheme. And so, it turns out that this is the kind of issue that classical methods encountered. They are not correctly approximating these types of interactions, and as a result, incur some regularity assumptions or high regularity assumptions on your solution of the equation. So, in particular, least splitting, and we've seen that again in Tushan slides already. And we've seen that again in Tushan slides already, corresponds to a linearization of that nonlinearity around the dispersive part of the flow. And exponential integrators entirely include the presence of this exponential of the dispersive part of the flow inside the nonlinearity. And in both cases, now I've simplified a lot, but the local error of these types of methods or these types of approximations leads to an error term that is of order tau squared in this integral here, with a constant that's related to the Laplacian of the solution. Passion of the solution. And that means if you want to make sure that your method indeed attains that first order, that second order local error in a given norm, you need to ensure that the solution has at least two additional bounded derivative values in that same norm. And that's exactly the source of problem in the order reduction of these splitting methods when we cannot provide the boundedness of these coefficients. And it's also the reason why solutions with large derivative values provide some struggle for these kinds of methods because these error constants are real. kinds of methods because if these error constants are really large you might not actually observe the asymptotic rate of convergence in the regime that you're interested in. So in low regularity integration we try to come up with better approximations of these dispersive non-linear interactions and I just want to highlight that that's possible and that there is already extensive work on this. And so just to give you an example of one of these successful low regularity integrators is something that's published just two years ago. It's a second order local integrator that achieves optimal rates of Integrator that achieves optimal rates of convergence with really low regularity initial data. So, just the same convergence fault that we had earlier with stronger E splitting. Now, we have guaranteed convergence rates for this new integrator in the black curve. And this field sort of started maybe about seven years ago with two very influential papers constructing these kind of low regularity integrators for the K D V equation and the non Linux Ringing equation, and since has grown really extensively to construct these methods in various settings for a range of equations. Settings for a range of equations. And quite interestingly, also in recent years, it's sort of reached the stage where people are starting to come up with general constructions of these low-range integrators for a broader set of dispersive monumental systems. But unfortunately, one of the things that may be a little bit missing in our work until recently is structure preservation problems of this mechanism. Just to highlight that, I've got a quick plot here of the relative energy in the relative error in the energy. Relative error in the energy as a function of time evolution. And I'm solving this, in this case, the norm in the Schröding equation with a fixed time step. And I'm looking how the error in the energy progresses as time evolves. And unfortunately, so this black curve that we saw earlier was performing so well in terms of approximation property in the low regularity regime, can no longer preserve the energy in that provolong times. And that's in stark contrast to structure preserving methods, which we've seen in many talks already today. So, for example, in David's talk. So, for example, in David's talk and in Jushan's talk as well, if you have some sort of structure preservation in your scheme, typically you're able to achieve a good preservation of the energy. And what I mean by good preservation is that you can attain an approximate preservation of the energy at a certain small level, not at machine accuracy, but at a small level, over very long times. And for this purpose, what we want to construct are so-called symmetric schemes. And the idea of symmetric schemes. The idea of symmetric schemes is simply for single-step methods, that those numerical schemes are symmetric if they are equal to their adjoint method. It's just a theoretical construction. But why do we like symmetric schemes? Well, one of the reasons we like them is this very central result that was proven just in the last decade, that essentially if you use this symmetric method on an integrable reversible system, then you can preserve conservation laws in that system over a long time. Laws in that system over a long time in your numerical integration. And this is a rigorous result currently only proven for the ODE case, so for finite-dimensional systems. But it turns out when you do your numerical simulations in the PDE case, you find similarly good behavior for the symmetric methods. So our idea was to try and construct symmetric low-regularity integrators. And in order to get there, I have to start by explaining very briefly how construction of low-regularity integrators work. Construction of low regality integrators worked from the beginning. So we'll start and come back to this idea of the iterated Duhammes formula, the iterated variations of constant solution. And so the place to look at on the slide is the button here, which is exactly the same formula that we've already seen on an earlier slide, now just expressed in free coordinates. And so it's the red integral, that's the non-linear frequency direction that we have to describe in a good manner. And the nice thing in this foul coordinates setting is that you can write down what the integral is. That you can write down what the integral is quite explicitly, and this looks fairly simple, very innocent. So you can ask yourself: well, what would be a good approximation of that integral for an American method? And one naive way of doing this is to say, well, maybe I will not approximate that integral and just integrate it exactly because I can do this in Fourier coordinates and generate a numerical scheme based on that. That would be wonderful because in this way you don't make any approximation that requires regularity assumptions. That requires a regularity assumption on the solution. But the problem with this approach is that when you construct a time stepper, at every time step you have to compute a convolution that involves this integral as a convolution kernel. And that doesn't separate. And as a result, you have to compute it directly, which is extremely expensive, especially in higher dimensions. An alternative approach is to say, well, maybe I will just take some of these operators in here, the exponentials, and maybe linearize them, maybe tailor expand them. And maybe tailor-expand them. And that would allow us to integrate, for example, just the first term here exactly and then have approximations on the remaining parts of the integrand. And if we do that, then we can use fast retransform-based methods because essentially the kernel in that convolution is separable and therefore we can construct a much more efficient numerical scheme. But unfortunately, the error that we've incurred by making this approximation is essentially related to the first term in the Taylor series expansion of these terms that we've neglected. Of these terms that we've neglected, and that's related to a re-index squared, which corresponds to a Laplacian of our solution. So, we've made a regularity assumption when we made that approximation. And so, what we'd really like to do is try to find in low regularity integration a balance between the two approaches. Provide fast computations and at the same time not make too much high-regularity assumption on our solutions. And this has been done with an approach called. Uh with an approach uh called uh uh which which uh is essentially termed resonance-based methods. The idea is that you collect the exponent exponent in your non-linear frequency interactions, these are called the resonances of a differential equation, and you split it into a dominant part and then into a lower order part. And the dominant part is capturing essentially the most important non-linear frequency interactions, but it's capturing it in a simplified manner so that you can use fast free transform-based methods to compute your time step. And the lower order Your time step. And the lower order part is kind of the remainder terms that do not contribute too much on the regularity assumptions on our solution. And so the construction that I've just gone through is essentially the construction of the first low regularity integrator for the non-linear Schrödinger equation. And in physical space can be written down just in a simplified manner. So now we've got a scheme that's wonderful because it requires less regularity assumptions to convert it to first order than, for example, To converge to the first order, then for example, a least printing word or an exponential integrated. And you could ask yourself, well, if I look at that scheme, maybe that's already a symmetric method. And the answer, the obvious answer is no, unfortunately it isn't, because if you look at the adjoint method, that's an implicit scheme, whereas the original method was an explicit integrity. And so what's missing? How can we come up with a symmetric scheme? Well, if we want to ensure that we are equal to the adjunct method, we have to allow the original method to be implicit to begin with, right? We have to allow Be inviced to begin with, we have to allow for a few additional degrees of freedom to be present in that construction. And so, one way to bring in these additional degrees of freedom is to notice that the non-linear Schrodinger equation is time-reversible, so we can iterate Dohamnisform not just around the left endpoint of that domain of integration, so around zero, that's what we've done so far, but we can also iterate it around the right endpoint, around tau. And if you take a linear combination of those two iterations and you plug it into that iteration that moves. Plug it into the iteration that we've done before, you end up with an expression that has very much the same resonances as the original construction that we've already just seen. And you can play through the splitting into dominant and lower part and end up with a lowering larity integrator that has very much the same convergence properties as the previous methods, but is now asymmetric scheme. Now, I grant you that felt like a little bit pulling a rabbit out of a hat. I mean, why should we use exactly these kind of iterations? Why should we use exactly these kinds of iterations? Why do we do the right endpoint? Why do we do the left-hand point? And it's a little bit unsatisfactory to just come up with symmetric schemes in this manner, which is exactly the motivation for us to try and come up with a more structured manner of characterizing and constructing symmetric low-regularity integrators. And so the sort of brief outline of what we tried to do in this work is to come up with a general expression of what it means to be a low-regularity integrator for a dispersive non-linear system. And out of that general expression, to And out of that general expression, to characterize those methods which are symmetrical. So, to get to that general expression, I just have to very briefly mention how we can construct higher-order methods in this sort of low-regularity setting. So, what we've done so far in the construction of these methods is we've iterated Duhamme's formula and we've neglected the double integrals, so sort of higher-order iterations. But if you want to get higher-order methods, you have to keep more of these terms. So, you have to keep on iterating through Homosforma and Keep on iterating dual misformer and retain more of the iterated integrals that you find. But that also means that your resonance structures in these repeated integrals become more complicated. And therefore, you have to come up with a structured way of collecting those resonances and then separating them into a dominant and into a lower order pump. And so, just to quickly summarize where we're at, if we want to come up with a general characterization of what it means to be a low regularity integrator, It means to be a low-regularity integrator, a resonance-based scheme. We have to firstly understand how do we iterate two homos formulae, what are the resonances in these iterations. We have to be able to account for the left and the right and for the iteration of two homos formula in order to allow our methods to become symmetric in one. And then finally, that's something I didn't mention in much detail. We have to be able to discretize also the lower order parts in a high-order sense in order to get high-order methods in the overall machine. And so in terms And so it turns out that the first part has already been resolved in some very nice work by Corneille and Schwartz in recent years. And essentially, they've come up with a formalism that allows us to characterize exactly these kind of resonance structures and to split them into dominant and low-order parts. And this is a decorated tree formalism that we'll then use to extend into this more general symmetric low regularity setting. So, how does the tree formalism work? I just want to briefly run through a couple of examples. Essentially, we For a couple of examples, essentially, we can use these decorated trees, so trees, which are essentially colored nodes and decorated hedges, or decorated edges and decorated nodes, to characterize all the terms that appear in the iteration of the homes formula. So, for example, a tree with a single colored edge in red corresponds to the dispersive flow operator that appears in your iteration of the homos formula. If you color the edges in blue and you have MOLDs, Glue, and you have MOLDs, and that corresponds to a combination of integration time and dispersive flow of rich operators. And you can keep on repeating these constructions, and in the end, represent all the terms that you find in your iteration of two Hammerswoma in a structured manner in these three homes. And you can collect this and come up with a general expression of what it means to be a resonance-based scheme in this setup. Okay, and now this is a sort of maybe a bit of a complicated expression. You don't need to understand it. Complicated expression, you don't need to understand in full detail, but essentially, what this represents here in this expression is the most general expression of a resonance-based scheme for a given dispersive non-linear system. So, essentially, what we have, we want to compute the value of the solution at the future time, and we do this in terms of the value of the solution at the previous time, plus some terms that come from the iteration of the Homes formula. So, in the sum over the black term here, that's essentially the iterated integrals that we retain. Iterated integrals that we retain, so trees that represent these integrals. And then in the colored terms, we collect all the degrees of freedom that I've mentioned before. So, for example, in blue, we have the possibility of iterating around the left or the right-hand point into a homosforma. In the purple part, we collect the resonances and split it into dominant and lower-order parts. And in the green term, we interpolate the lower-order parts in each of the integrals correctly to achieve the correct order of our integral. To achieve the correct order of our integral. Now, all of those parts that I just described are shared between all resonance place methods. But what characterizes an individual method is the coefficient functions here shown in red. And that means you can use the coefficient functions to characterize those schemes which are symmetrical. And that gives rise to a characterization of symmetric resonance-based schemes, so simply in terms of these coefficient functions, which is very similar to how you can characterize. Which is very similar to how you can characterize symmetric Morning-Putter methods or symmetric exponential Morning-Putter methods. Now, this might have looked a little bit complex, so I just wanted to break that down, come back to the example that we've seen at the beginning of this presentation. And that was we iterated Durham's formula just once and retained a single integral. That integral is represented by the tree that I'm showing you here. And if you retain simply that term, the general formula reduces to this expression here, which is simply a sum with. simply a sum with eight degrees of freedom. These are the coefficient functions in our Mercosur. And the symmetry conditions are simply a relationship between these degrees of freedom that characterize which of those methods are actually symmetric. So how can we use this in practice? How can we use that to construct a symmetric scheme? First of all, we choose coefficients such that they match these symmetry conditions, and then we ensure with the remaining degrees of freedom that With the remaining degrees of freedom, that we are consistent to the order required. So, for example, if you take the first coefficient function to be equal to this expression, which is related to the low regularity assumptions in the first integrator that we've seen, then the symmetry condition immediately tells you what the conjugate coefficient has to be in order to obtain a symmetric scheme. And this is the scheme that we've already seen at the beginning of the presentation. But it's not the only choice, and in fact, through this characterization, we can come up with the whole plethora of Up with the whole plethora of low-regularity integrators, which are all symmetric. And one of those examples in terms of higher-order schemes is the one that I'm showing you here, which I would like to compare against the low regularity integrator that I've shown you before as kind of the state of the art in terms of convergence properties. And that's the black curve. So the new integrator is shown in yellow on this convergence plot. You see in the lowering glass machine, we have fantastic convergence properties, but now we're also able to retain long. Also, able to retain long-term preservation of the dimension. And even in lower galaxy regimes, where first of all the low regularity integrators that were out there before were not able to do this, but also this trunk splitting breaks down because we're dealing with low-regularity solutions. So for the final five minutes of my presentation, I wanted to quickly show you an application of this to the simulation of low regularity vortex experiments. So what we have to solve is this Wart experiment equation that I've shown you at the beginning of my presentation. That I've shown you at the beginning of my presentation. And we won't solve that exactly. Instead, we will solve the equation that's describing the tangent dynamics of that work experiment. And that's what's called the isotropic under the fluid equation. So instead of studying the evolution of the coordinates of our curve, we will study the evolution of the tangent. Now the isotropic Lambda-Lift equation more generally is interesting in its own right. It can be used also to Right, it can be used also to describe spin dynamics and paramagnetism, and there is theoretical interest in understanding solutions in the low regularity stream to this equation because currently there is a lack in understanding or a gap between when people understand the existence of solutions and well posing on the system equation. So if you were to discretize this equation directly, you might think of doing something that's perhaps an implicit scheme in order to avoid the sort of stiffness. Scheme in order to avoid the sort of stiffness problems with this stiff operator inside the nonlinearity, or perhaps even better, a semi-implicit discretization. That's sort of what was the state of the art in this field until recently. But if you use such a direct discretization of the equation, you end up with methods that essentially leads to order reduction, much like we've seen with the splitting methods for the non-linear equation. And so instead, we wanted to come up with a new idea to try and simulate solutions of this equation. To try and simulate solutions of this equation, especially for the lowering value. And our idea is to use an existent non-linear transform that relates this isotropic vandal equation to a non-linear Schrodinger equation, and then to simulate the non-linear Schrodinger equation instead. And this essentially leads to fast, unconditionally stable methods, which have very nice convergence properties in low-regularity machines, and are also time-symmetric because we can use the previous constructions that we've already discussed. Instructions that we've already discussed in this presentation. So, what's the non-linear transform? The non-linear transform that we consider is the so-called Haas and Motor transform. And the idea is that we make the problem a little bit more complicated before we start making it simple. And how do we make it more complicated? Instead of simulating just the tangent vector field, we will at every point on the curve complete that single vector to an orphanormal basis of R field. That gives rise to A free. And one way of doing this is with the normal and binormal vector that's giving. Normal and binormal vector that's giving rise to the Frenet frame. But you can also rotate that selection of vector fields around the tangent vector field, that gives rise to what's called the parallel frame. And if you try to simulate the combination of these three vector fields, so the tangent vector field, and these two elements of the parallel frame, turns out that that dynamics is governed by a linear OD system. And that linear OD system simply has coefficient functions which are the solution of the non-linear Schwering coefficient. The solution of the non-linear region. So, this gives rise to a very nice natural two-step process now. In order to simulate the dynamics of our isotropic Lander Lichens equation, we can start by solving the non-linearity equation, which we already know how to do, and then we can solve simply a remaining ODE system, which is much easier than solving a PD. So, for the first part, we pick a low regularity integrator, and we want this to be semantic, so we pick one of the favourite low regularity integrator that we've just seen in this presentation. Integrator that we've just seen in this presentation. And then to solve the remaining ODE system, we construct what's called a Mango's integrator. So essentially, we write down the exact expression of the solution in terms of an exponential of a series of integrals over matrix combinators of these coefficient functions. We truncate that series and we approximate the integrals. Now, an important thing to note here is that these coefficient functions are actually related to the solution of the non-linear Schrodinger equation. So we only have the approximate value for those. Have approximate value for those, and we only have approximate values of those at isolated time steps. That means we have to use some sort of quadrature in order to approximate these integrals. And in the classical setting, if you were to use a classical quadrature on these integrals, then you find that the quadrature error is, of course, related to a derivative of the integrand. But in this case, the integrand is the solution of the Schrodinger equation. So a single time derivative is related to a Laplacian of that same function. A Laplacian of that same function. And therefore, when you apply a quadrature to these kinds of integrals, you inherently have to make some regularity assumptions. And it turns out that classical quadrature wasn't good enough for this, so we had to come up with a better way of doing it. And the way we came up with a better way of discretizing these integrals is by switching to what's called the interaction picture, or the twisted variable. And in this interaction picture, it turns out that the non-linear frequency interactions again become really explicit in Fourier space. And now you have to represent these non-linear frequency interactions again showing in this red integral in an appropriate manner. But the additional challenge that appears here now is that neither of these frequencies appears to be dominant, right? So how do you select the dominant and goal or the part? And the way we've got around this is by separating the convolution into two regimes. The regime where the first coefficient is dominant, and then the regime where the second coefficient is dominant. Where the second position is coming. And if you make those approximations correctly, you come up with a suitable low-regularity quality rule for your Magmus expansion that you can plug back into your Magmus integrator and get a low regularity numerical method for the overall discretization of your OE system. And just an important point to highlight here is that because we have separated the convolution in Free Space into two regimes, we can no longer use fast Free transformations. We can no longer use fast-free transform-based methods to compute damping. But instead, we can rely on Bog-Deplitz decomposition of certain matrix vector products that appear in the computation of these quadrature rules. And so you can, as I said, plug that all back together and you can come up with what we call the fast low-regularity Hasimoto transform, the Flower transform, that is a really efficient integrator for the isotropic Andolichi's equation, especially in the low regularity machine. And maybe I'll finish on a quick numerical. Maybe I'll finish on a quick numerical example of this, coming back to that simulation of the low-regularity voltage filament that you've seen at the beginning of my presentation. We are able to outperform previous working both in terms of convergence properties and in terms of CPU timing, because we have this fast implementation. And so this brings me to the end of my presentation. So what we've seen today is a construction of structure-preserving symmetric low-regularity integrators. And the essence is we build on what's out there in terms of previous constructions of low-regularity methods, but we bring in more degrees of. This, but we bring in more degrees of freedom to allow for the symmetry of this numerical schemes. You can also extend this to the construction of symplectic methods, but I didn't have time to talk about that here today. And so I will be very happy to answer your questions, but maybe I can start with a question. So I'd be very interested to see if in your work, maybe there are examples where lowering larity methods can be useful. And I'd be very happy to learn about that and have a chat about that after the talk. And a quick bit of advertisement: if you're interested in lowering large methods, Advertisement: If you're interested in Laura Gladstone experience, we've got a Git repository with some codes and method codes so you can experiment with these kind of methods, but with yourself as well. Thanks very much. Um I I would like to comment that on you. that when you uh substitute expansion in the integral variation of constant for this, and at the end if you build those popular structures, where you get those roads labeled, we are many years ago, uh Andrew and myself developed Venerable theory to obtain course expansions and course expansions and this kind of expansion into a different kind of expansion referring to other papers thank you that's an excellent comment yes I should look at those thank you very much In this example, we did only a second-order scheme, but you can use a similar approach to get higher-order schemes as well. So, essentially, you just have to keep on discretizing or finding suitable quarters for the terms in your Magmus series. And it turns out that you can do that in a similar methodology as well, in high-order methods. So from your presentation, you're sort of suggesting that these kinds of approaches can work for KDV, non-linear shrding, or anything that could be transformed into those. Are there some limitations there? Is this only going to apply to integrable V D E's or do you think it could be more general? So I mean sort of the symmetry property uh I think it's essesh especially beneficial. Especially beneficial for integral systems. I think there are some results that show that symmetric methods, in principle, lead to slower error growth as time progresses. And I don't know if that's limited to integrator systems. The construction of low regularity integrators more generally is more beneficial in the case of dispersive equations. So if you have a dispersive operator. But I think we've also seen in Guoyang's talk actually that kind of comparable. That kind of comparable approach worked quite well for wave equations. So, certainly, you can't use it on anything. I think there is some workout there where people use this for parabolic equations, but I'm not quite sure what the benefit is there. Thank you for your talk. In time-dependent lensive functional COEs, sometimes you have non-linearities which depend not only on side, but on the gradient of side. Are there ways? Are there ways to extend this? I think so. So, for example, the KDB equation would be an example. Oh, I'm sorry, the KDB equation would be an example, right, where you have a derivative value, right, in the nonlinearity. So, maybe I can just come back to this at the very beginning. So, I mean, obviously, it would be interesting to understand more carefully the example you mentioned, but this would be an example where you have a kind of potential term of deteriorating. So now it is last year, but yes, we realize it okay. Okay, yeah, so it might be more complex than this, but but on the outside it might be costing me. Well, thank you, okay now about coffee, so I think I'm gonna add the cash flow.