not going there is nothing differential in this talk i will be speaking of different construction of invariance and those will be global invariants but the methods the algorithms we describe the construction we described are very much in influenced by the um by the moving frame method originally but uh quite spectacularly actually there will be no moving frame in this talk i mean the title is a bit In this talk, I mean, the title is a bit misleading. So, the thing that will appear will be, I mean, the emphasis is going to be on sections and how they help you construct invariants. So, there will be three parts. The first part is I will recap a very general construction that we had worked out with Irina Kogan. Had worked out with Irina Kogan to construct a rational invariant. So it's a very general algorithm. The one reason I want to go back to this algorithm is that really it's all the ideas that I used in the, I mean, the ideas I used to construct more specific invariants that were coming from applications. I mean, yeah. So invariants which were needed in application. I mean the Which were needed in application. I mean, the original ideas all stem out of this general algorithm. So that's why I wanted to review that. So the second section will be about some application in mathematical biology, parameter reduction. And this is something I worked with George Laban. And the last question on those are invariants that were needed in. We are needed in brain imaging. That was a problem that was brought to me by some colleague in my institute. And I worked on that with the then student Paul Gorlach. And I hope to get to the end of this three major section. So let's start with, so my setting is that, you know, no big surprise, I will be in an affine space. So that will be Rn or C. So that will be Rn or Cn. So I will write K for either. So sometimes, you know, you will want results over R, but you will need to check what's happening in C. And I will mention that. So my group is here noted G, and this is an algebraic group. So that means it's the variety of some polynomial equations. So these equations are going to be in the polynomial. In the polynomial ring in lambda one, lambda l, and it's um so this set of uh polynomials by define it are form an ideal. And so the action I'm going to consider is going to be noted with the star. The group parameters are noted lambda. The coordinate of the fine space are z. And so the actions I consider are rational, are given by rational function in lambda and z. In lambda and z. So this is the algebraic setting. So but that will allow me to do some computation and some algorithms that I can then implement in a symbolic computation software. So for instance, simple, very simple basic example, the orphan or group, the equations are given by two parameters, lambda and mu. Two parameters lambda and mu and the square of these two things sum to one just like sine and cosine. So this is the linear action and where the orbits are the so-called of course. Now if you have the multiplicative group, this is also an algebraic group. The equation says that for your parameter any parameter lambda you have an inverse mu. So this is equal to one. And so we are going to see quite a bit of this group. To see quite a bit of this group as they act as scaling, as this the orbit of, I mean, as this represented here with the orbit of the scaling that is described here. So what I wish to achieve is to compute the rational invariance. So let's see what are their initial properties. So I have my action star. I have my action star and if I have a function which is a rational function in my n variables, this is an rational invariant, just if when I substitute z by lambda z, I get the same thing. So these invariants, they form a field. And in stark contrast with polynomial invariance, this field is always finitely generated. And so that's one good thing. Another very striking result is that for a set of invariants, R1 to Rk to be generating, it is necessary and sufficient that this set is separating, at least generically. And this means that if you have two points, Z and Z prime in general position. In general position, then the invariance at these points will be equal if and only if these two points are on the same orbit. Okay, so again, this is in stark contrast with polynomial invariance, where in some where sometimes the polynomial invariance don't separate everything. They don't at all. So rational invariance, there are some. There are some open sets, some no, sorry, there are some closed, there is some sub-variety on which the separation property is not true, but this is, you know, this is at most an hyper surface. Okay, so I already kind of announced it. I mean, a major tool will be Major tool will be our major concept: is the concept of section. So, we are not looking at the local section but at the global section, but we kind of give it an attribute, its degree. So, the degree is, so the variety is a variety is a section. If every generic orbit intersects this section in finitely many points, and this finite points and this finitely this finite number of points e is the degree so it doesn't make it a very uh elusive property i mean if you take any sufficiently generic affine space of dimension complementary to the one of the orbit i mean you have a you have a section uh how does this work over car sorry what how does this Sorry, what? How does this work over what you have just said? Over reads? Yes, I'm coming to. Yes. This is the number you have to count. Indeed, I was going, I'm remarking on it. You have to count the number of intersection over C. So here, I mean, these are the two examples that we saw. So here for the rotation, the simple section y equals zero is a section of degree. y equals zero is a section of degree two, you can't do any better. And now before this scaling, I mean, you can't choose y equals zero because it consists that this is a line that consists of orbits. And so if you take then you take y equals one. And so you have, if you look at the colors, I mean, it intersects each colored orbit only once. But actually, I mean, if you look at the equation, if you look at the Equation: If you look at the equation that gives you this intersection, then this equation x for so y is equal to one and x free is equal to something. And so you see that here you have actually two intersection points over C. And this is the degree of your section. You have to count the degree over C. So now if you take instead x equals one, it's kind of better because you can see that. Of better because you can see that the degree is two, and this is also the degree that is revealed in the picture. So, if you want to do better, you actually have to take the section x equal y. And if you go to x equal y, the equations that give you this intersection that you can see at the bottom, it makes clear that you have only one point of intersection. So, this is, you know, this is nice. This is a global section or also. This is a global section, or also called a rational section, and that's a section of degree one. So you, the thing is, so let me go back. I mean, here, as we saw, I wrote the equation of the intersection of the section with an orbit, the orbit of the point little x next to y. Dexter Y and you could see here the invariance appearing so in purple. So this intersection, I mean this equation of the intersection, I mean this is this, if you take them all, I mean it's it forms an ideal. And this is a key element. So we are going to call it the intersection ideal. So generally it's going to be an ideal in the polynomial. An ideal in the polynomial ring in capital Z variables, and the coefficients are going to be rational functions in the little z variable. So what this ideal shall mean is that if when we specialize the little z to a point in our fine space, then we obtain a polynomial ideal in just a big z variable. Z variable. And that's exactly the ideal of the intersection of the orbit of this point Z with the section. So that's, so this ideal is kind of the generic, is the, sorry, this section is the ideal of the intersection of the generic orbit, the generic orbit of Z with the section. Okay, I'm not saying how I'm computing it for the moment, I'm just thinking of. It for the moment, I'm just thinking of it as such. And the thing I want to remark is that you see that if you take the orbit of Z or the orbit of lambda Z, it's the same thing. So the intersection with the section is going to be the same. So these two ideas, these two specializations to Z and to lambda Z should be the same. So that means that if you have a canonical That if you have a canonical representation of i, that means a unique representant, then when you special, then its coefficient should be invariant, because when you specialize, you should obtain the same thing. So a canonical representation will have coefficients which are rational invariants. Now, if you think of it a bit more, you see that with the separation property that I mentioned, That I mentioned, you can actually claim that these coefficients are going to generate the field of rational invariance. So there was such a construction that didn't quite use the intersection ideal, but the orbit ideal, that used form as the canonical representation. So that was in 56 by Rosenwicht. But for us, we are not going to use this. We are not going to use this exactly, we are going to use something a bit more practical. So, let me say first: what is this ideal? How do you find equations? I mean, how do you find its generators? Well, you should express this intersection. So, first of all, you're going to say that capital Z is the image of little Z by a group element. You're going to add the equation for the group, and you're going to say that Z belongs to the That z belongs to the section by saying that P, by putting here the equation for the section. So P is an ideal, is a prime ideal in, so which is generated by polynomial in capital Z variable. So that says that links everything. So in the example of a rotation, I mean, so we had the equation of a group. group this thing here was uh written is just the linear relation between the x and the capital x and the capital and the small x and y and p is just y equals zero so when you have a rational action i mean you have to kind of take care of the of the denominator that appears here then but now i mean what i describe But now, I mean, what I described is a polynomial in small z, capital Z, and lambda. So what you want to do is eliminate lambda, so that is, take the intersection of this ideal with this polynomial ring. So this is an operation that, so we have generators here, and we want to compute generators of the intersection for this elimination ideal. And this is something that can be done with the algorithmic tool that is. The algorithmic tool that is called Robnabasis, and that is fortunately implemented in any, any absolutely any computer algebra system. So, and furthermore, this problem basis gives you a canonical form. So, this is really the go-to tool. So, we can compute. I mean, you just feed these equations to your program, I mean, to the command from the basis. Command Robner basis in Maple, for instance, and you compute a reduced Brogner basis of I. So this is a canonical representant. You pick its coefficients, which are rational function in little z. So we know that those are rational invariants. And so by the argument I gave earlier, we can also show that this is a generating set. But so in this original argument, But so, in this original article by Irina, I mean, we actually prove something more. We actually gave, I mean, the proof of generation is obtained by showing how we would rewrite any other rational invariant in terms of this generator, in terms of this coefficient. So, I'm not going through this algorithm. It's not a simple replacement. It's not a simple replacement. It's not a simple replacement. Instead, it uses the normal form algorithm that is associated with the problem basis and that tests membership to an idea. So, but that's it. That's a very easy to implement strategy to compute in variance and also obtain rewrite rules for that. So, with this strategy, we can retrieve results, you know, well-known, I mean, kind of well-known results like invariance for classical action of SL2. So, for instance, on binary forms. I have to check the time because I think this is a rather, oops. So, here the section, I mean, the section, I mean, the usual section is to take the first coefficient to one, the second coefficient to zero, and then Second coefficient to zero, and then the third coefficient becomes exactly this discriminant, and that's what we obtain. Um, I like also this other example. I mean, there is another example of similarity of matrices. This is an example I used in a later paper. And so I'm just going to say that you here you really have to refrain from using the diagonal. The diagonal matrices as a section because I mean, so the elements on the diagonal can be permuted. So, this is a section that has degree at least n factorial, I mean, and exactly n factorial. So, the computation become very unwidely. What you have to use is the Frobenius normal form, which is given by these matrices in companion shape. And this is a section, this is a nice section of degree one. And what will come here are And what will come here are the coefficient of the characteristic polynomial of your matrix M. And that's what your rational invariants are. So I'm going to pause for a minute to drink and to give you the opportunity to ask questions on this first part, which was the general construction. All right, preparing my tea for the end of the second section. All right, so the second part of this talk will be about specializing the construction for scalings. And this I'm going to use to Going to use to do parameter reduction, the most parameter reduction that occur in the mathematical models for biology. And so we bring a number of we make a number of contributions to the subject. This was, as I said, work with George Laman. But I think that a key point is that there was some kind of There were some kind of partial answers to the question, but what is really new in the method we bring is that we bring a solution without fractional powers. So there will be no square roots in our solution, no square roots, no cube roots. We will keep everything with integer exponents. So that's a new part. And still it's algorithmic. So the okay. Okay, so let me first speak of scaling and introduce. I will just kind of give the main idea on this scaling with scaling in the planes. So scaling is going to be defined by a matrix here. So here, so there will be this matrix will have two columns, like the space we are acting on, and one row for the, because we are just using the k star. You have the k star, so the multiplication group. So, this integer a and b they give you the exponent of lambda when scaling x and of lambda when scaling y. So you see, but you have different pictures. We've already seen the middle picture where we saw that y equal x was the section of degree one. The section of degree one. So instead of writing question on the middle picture, Evelyn. So I think you said generic orbits because that doesn't intersect the vertical line, right? Correct. Yes, it's always the generic orbit. It's most orbits. Yeah, I should say. So instead of thinking y equals x as the equation of my obvious section, I'm going to write it as a monomial, x times y minus 1 equals 1. And you see that the left picture, I mean, y equal 1 was just perfectly okay. And that's again a monomial equal to 1. And you see on the third picture, where this made where the This made where the exponents are two and minus three, you actually cannot use any line. I mean, you can't hope that the line is going to be a section of degree one. You have to go non-linear. It's non-linear, but still, it's the level set of a monomial that is found here. So, as you see, I mean, so it's a general thing that the section of the green one will be. The section of the green one will be given by monomial equations, by monomials, by level set of monomials. And now at the bottom, you can see what are this intersection ideal I mentioned earlier. So you see that because it's the green one, it will be x equals something and y equals something. And this something is an invariant, and this invariant is a monomial. So with positive and negative powers, but integer powers. And here it's maybe. And here it's maybe the most representative of what I want to say. There will be one invariant here, a generating invariant, x3, y2, but arise in with different powers. So the invariance we would have computed with the general algorithm would have been these two things. And then we would have a CCGs, but it would be way better to compute directly this thing, the generating indicator. This thing, the generating environment. All right, so let's see how let's. I mean, this is a this is actually linear algebra as it's kind of known, but let me look at it with the integer point of view. So I have my scaling that is defined by my integer a and b, and I want to look for a monomial that would be invariant, so that my monomial is given by the exponent c and d, also integers. C and D, also integers. So I write for myself the fact that it is invariant by substituting for X and Y, they are transformed. Pulling out lambda, I obtain that it will rise with a power AC plus BD. And so I want this exponent to be zero. So that means that I want. Exponent to be zero. So that means that I want C D, the vector C D to be in the kernel of the scale of the matrix by define by scaling, AB. So I mean, I can take the easy thing is to take minus B and A. But if A and B have a G C D, I mean, that's no good in the sense that it's not going to generate everything. So what you want to do is to take to divide. is to take to divide A by this GCD and B by this GCD to obtain these powers and then you have a generating invoint. Okay. So okay I have a generating invoicing handbook but what about this rational section that I've been kind of promoting? Well what I can do with this once I have a GCD I can write a bazoo identity so that's writing the GCD So that's writing the G C D as a linear combination of A and B. And then you can work out, you know, you can with a few lines of computation, I mean, you can see that the coefficient of this Bezo identity, alpha and beta, are actually the exponent that give you this rational section. So that's pretty neat. Now, if you want a moving frame, I mean Now, if you want a moving frame, I mean, one additional line of computation shows you that this is the equation for your moving frame. So you see that if H is not one, then you have isotropy and it's not unique. Okay, this is the only time I think where I speak of where I explicitly mention a moving frame here, though it's underneath many things. So let me say how this. So let me say how this planar case is generalizes to all dimensions. I'm going to write all my what I have here as in matrix form. So I have the matrix of the scaling first, AB. On the other side, I have something that is actually a Hermit form given by the GCD. And here I have these two vectors that I put in a matrix. Matrix and it's a matrix that you can observe that is of determinant one, so it's invertible over the integers. So that's the property of this Pezu coefficient. So this multiplier, so on the right, this is the kernel, minus DC is in the kernel of the scaling matrix. And this should be a plus D. Sorry. And here, this is something that gives you this. That gives you this full rank matrix H somehow. So as I said, this is how we generalize it. I mean, so you can define for in any dimension a matrix to be in her mid form. So this is a kind of triangular form where you put all the columns to be zero, all the zero column to the right, and the left columns are kind of triangular. And then you have some additional constraints of decrease. Additional constraints of decreasing and positivity. But this is really not the important part, but I want to mention. What is important is that whatever your matrix A you have of integer, you can bring it to a hermit form with a unimodular matrix. So the unimodular matrix represents some column operations that are invertible over the integers. Its determinant is one or minus one. So, what is more, if you split the column of this matrix B into on one hand the n, the n being of size n minus r, where r is the size of h. Then these columns, they not only generate the kernel of the kernel of they span the kernel of A, but they actually span the kernel of A over Z. So any other integer vector in the kernel is a linear combination over Z of the vectors in this part of the matrix. So that's how you find the generating invariance. So if A is so let me just give you in a nutshell the recipe and I hope that what I said on the plane kind of Plane kind of makes you understand why it is so. So, if A is defined the scaling, then the column of Vn are going to be the exponent of some monomials that form a generating set of invariants. The column Vi here, they actually give you, they define the rational section, so that's pretty neat. And now you can look also at your And now you can look also you can concomitantly compute the inverse of v and the bottom row are going to give you the rewrite rows. So which will be given by each z can be replaced by a monomial in your generating invariance. And the exponents are given by this bottom. Okay, so this is really in a nutshell, this how you use. This is how you use this Hermit form that is also implemented in Maple to compute invariance and to do the rewriting for scalings. And now the application. So the application, for instance, if you take, so those are, so if you look in the book by Murray on mathematical biology, so typically when they have a model for some, so here, for instance, the prey predator model. Here, for instance, the prey predator model comes with a lot of parameters. And what is said in this book is that the first thing you should do is reduce the number of parameters. And they kind of give, you know, and it's basically an art form or kind of a rule of thumb, though. I mean, the Buckingham Pi theorem gives you a bit more indication on how to do that. So, but you know, artfully, you figure out that if you take this. Hopefully, you figure out that if you take this combination of the state variable, the time, and the parameters, then you can rewrite your system like that with less, free less parameters. Okay, so the goal here was to kind of make this all algorithmic. And how it works is as follows. First, we start from this system. We're going to look for a scaling symmetry. So, who's RB here? So, who's RB? Here, it's a free parameter scaling symmetry. And we represent this scaling symmetry by a matrix, which are the so the column of this matrix are the exponent of this monomial that give you this that rescale each variable. And how you obtain the scaling a matrix is a bit the same as before. You look for some kernel, but now a left right kernel. Right kernel to the matrix of the exponent that appears in these equations. And if you use it with the, if you do that with the Hermit form again, once you look at the Hermit form of A, you will obtain IR, which is good. It says no isotropy. It describes the scaling without isotropy. And then the multiplier that you compute on the way gives you. gives you the new variables as invariants. So those are these things. So the invariants give you these, are given by this row. And the rewrite rules are given by the inverse of this unimodular multiplier. And once you have these two things, well, voila, that's how you can determine algorithmically this reduced system. And what is more, actually, we can figure out how to lift a solution. Figure out how to lift a solution of this system to the original system. So that was the application shown on an example. And I'm going to make another example to re-emphasize the fact that we avoid fractional powers. So this is another model that you find in Marais, and the suggested change of variables that it uses, you see that it has fractional powers. So, and it gives this reduced model. But actually, you can use some pretty nice rational invariants, some nice monomials with no fractional powers to obtain a very similar reduced system. And so that's, I think, that's what I wanted to emphasize, the fact that we can keep everything with integer power, which has the benefit, but you don't have to question whether the quantity underneath your power. The quantity underneath your power is positive or negative, you don't have to split cases, so that makes for a more global approach to this parameter reduction. All right, so this is my second pause before I start the third section. And I have only 10 minutes to do that. So let's ask a basic question for the second part. Is there a question? I'm not hearing anything, but yeah, I have a question. Maybe I'm missing something basic. How metrics arise in scaling? Because it seems like you have a scaling vector, right? You have variables and you scale lambda to the power. So I understand how if you have more variables, you have a longer vector. you have a longer vector but how matrix arise on the section what is the um what is the correspondence between matrix a and section so okay i'm sorry i don't hear you very well but i'm going to answer what i think i heard your question is so i'm sorry if i missed it completely so the um Yes, so here the matrix AB that describes your scaling. So A and B are two integers and they describe the power of lambda that scale X and Y. Right, but in your big example, when you start to talk about multiplies, your metrics start to have more than one rule. Yes, so. So, what So, so what what what are the corresponding yes so sorry yes so here you see i mean here you have a scaling with three parameters so you have a eta mu and nu so and you have nine variables that you're going to be scaling okay so the matrix has three rows that correspond to the three parameters and nine columns and in each column you have the monomial that we scale. The monomial that rescales this variable. So you have, sorry, so the first column here is minus one, zero, zero because it corresponds to eta to the power minus one. So that gives you the exponent of the monomial. So there is several scaling parameters. And also in your first example, you just had one lamb. First example, you just had one lambda, but now you have many scalar parameters, and each row corresponds to a scalar parameter. Yes. Okay, well, thank you. Sorry, yeah, maybe next time. Okay, thanks for the question. So, so this, okay, I'm starting with the further. So, now, I mean, so for. So now, I mean, so for scalings, we managed to find a section of degree one, and that's really what kind of was brought success. So now we are not going to find a section of degree one, but we are going to use a section with, we want, so a section with certain property. So when we were looking at this problem, so with Paul, I mean, we were looking for something, so our section clearly had the property of environment. Clearly, at the property of invariants and the subgroup. And we were looking for some results that looked like what I'm describing here. And it was really lucky that we kind of found it in the literature. It's mostly, I mean, it's kind of a not, it certainly doesn't appear in the computational literature. It's more in the, it's a result that is used for proving the rationality. For proving the rationality of some fields of invariance. So, let me just go through this result. So, a subspace, so we have a group G, a big group G, that acts on a space omega. And we are going to consider a subspace lambda to be a B slice where B is a subgroup of G. So, first of all, if it is a, it's not a First of all, if it is a, it's not exactly a section. It just means we just want generic orbits to intersect Lambda. So every, I mean, most orbits should intersect Lambda. We don't say it's a finite number of points of intersection. We want the V to be the biggest group, subgroup of G that leaves the lambda invariant. And then here is a A condition that you have to check, even if you want a result over R, you need to check this condition over C. If you have an element in your slice that can be taken by a group element to again your slice, but the slice understood over C, then this group element should belong to the complex extension of P. Extension of P, of your group. So again, B and omega and lambda, we are all algebraic varieties defined by equation. So you can always think of this equation over C and look at what is the condition of, I mean, what's the solution over C. But so that's the definition of a slice. It's different from the definition of a section in several Section in several aspects. But so just the invariance of it gives you this first implication, but if you have an invariant of the big group, if you take its restriction to the slice, what you obtain is an invariant of the small group. So this is this restriction is an invariant of the small group. Is an invariant of the small group acting on the slice. So, this lemma of Shechadri tells you that actually this operation of taking the restriction is injective. I mean, it's actually bijective. And that means that if you take a rational, an invariant of B on the slice, there is a single invariant of a big group on the big slice. Of a big group, on the big space, of which it is a restriction. So, you really can do a lot. I mean, we will do a lot by using this strategy of looking at the invariant of the small group on the small space to say something about the invariant of the big group on the big space. So, let me give you the first example that we will actually expand on. That we will actually expand on afterwards, if I have time, actually, which is not happening. So, if you look at the action of the orthogonal group on symmetric matrices, I mean, I wrote for dimension free, but any dimension works. So here we're going for the slice. I mean, the natural slice is a section, is the diagonal, it consists of the group and of the space of. Group of the space of diagonal matrices. I mean, you know, you can always find a group, an orthogonal matrix, but will bring your matrix A to the slice, to this section. But now you have to figure out what is this lambda invariant under. So you can check that if Q is the matrix of plus and diagonal matrix of plus and minus one. Diagonal matrix of plus and minus one, you won't change your matrix, lambda one, lambda two, lambda three. But also, if q is a permutation matrix, all you're going to do is permute the lambdas on the diagonal. And it's actually the whole group. The whole group is this group that consists of the symmetric group and the sign changes. I called it B3 because it's in its name in the COPSATER classification. It's also the It's also the symmetries of the cube, if you like it better that way. So here is the setting. So we have a big space, big group, the small space, a small group. And what we need is to figure out what is the action of the small group on the section, on the slice. So a small group is given by a permutation and a diagonal matrix of plus and minus one. Well, actually, Minus one. Well, actually, the plus and minus one matrix doesn't act. It's only the permutation that acts. So the invariance on the small space of B3 are, for instance, you can take the Newton sums. So the sum of the lambda i to the power k for k equal one to n. And you can observe that these. Can observe that these invariants are actually the restriction to the slice of the invariants, which are the trace of the matrix and the trace of the square of a matrix and the trace of the cube of a matrix. So you can apply this lemma, earlier lemma, to say that this Q invariance generates the field of right. generate the field of rationality of the field of invariance of the big space under O3 because you know that this Newton sum generates the invariance of the small space by B3. So had you chosen the symmetric function, I mean here you would have had the coefficient of the group, the coefficient of the Of the character polynomial of characteristic polynomial of the matrix. So I'm coming to the application, and maybe I'll just conclude by saying what we achieved. So the application was in brain imaging. So the first model that they used to model the diffusion of water in the white matter. White matter was a tensor, the symmetrical matrices that we looked at. And so they would model this diffusion at each voxel by that one matrix. And so at each voxel, they kind of had a direction. This would give you a bit what would be the direction of the fiber. And so that's how they managed to do some tractography in the brain. And what they had come up with are two what they call biomarkers, which the value of which can give you an indication of whether your white matter is healthy or if there is an anomaly. So these two biomarkers, so the sum of the eigenvalues of your tensor, or this variance around this mean. mean there are two invariants there are invariants so the one the first one is the trace of the tensor and the other one can be written as the trace of a square or and the trace so these so that's so i mean of course these are the two by two three by three matrices so things are very well understood but when they started to want to model fibers Fibers with more model the crossing between the fibers and they wanted more expressivity and they had their protocols and their machine allowed them to have better measurements. They wanted to model the diffusion with now a four-folder tensor, and that is a fourth, a degree four form of in three variables. So, and you see that the forms you can bundle are kind of Forms you can bundle are kind of better, and so it allows you to see the crossings. So the question was to have this diffusion with this tensor at each voxel, what you want is to kind of have a normal form because I mean, if the patient whose brain it is is turning, you want the model is going to be different, and so they wanted to find. And so they wanted to find among the invariants of those tensors some relevant biomarkers. So that's where my project was to find these invariants first. And so you can see that now we are dealing with an action of O3 on a 15-dimensional space, and the action is given by polynomials of degree 8. So that's the representation. So that's the representation of the matrix representation is of degree eight. So this becomes really kind of, you don't feed that to the general algorithm. So there were attempts to find polynomial invariance and they are unsuccessful for different reasons. I won't say now. And what we managed to do, and I'm sorry, I mean, this is very interesting. I mean, this is very interesting, and I'm so sorry I'm rushing through it. So, using this strategy of looking at the invariant on a slice under a smaller group, we were able to say that we found a minimal generating set of invariants. That's 12 of them. We showed how to, we don't have the explicit expression for them, but we have a robust way of evaluating. Robust way of evaluating them numerically, which is more important. We can also say what is the image of this invariance. So that's where is the possibility range of this invariance. And we also can solve the inverse problem. So if you give yourself values for the invariance, can you reconstruct a degree four ternary form that will give you this invariance? view this invariance. So that's so I'm sorry, but I'm running out of time here. But that's my final, yeah. And what I wanted to say that, you know, on Monday after Michael's talk, I was wondering if this same strategy of the slice could be applied to the type of action on integrated integrals that he showed us. So this Showed us. So, this is something I might think about. Okay, thank you for your attention. And I'm open to question, of course, though. I'm already impacting on the time of the next speaker. I think we have the time for maybe a short question if someone has a question for Evelyn. Yes, I have a question, Evelyn. Evelyn. Evelyn, to what extent is this technique of using the slice in the last? I know you were having to rush through a little bit at the end, but to what extent is this essentially related to factoring out continuous group orbits and getting down to sort of something that you can say, okay, each one of these is represented by an equivalence class on a slice. Is it pretty much the same thing? thing well i mean the thing is that you you have several into you don't have a single point of intersection with your with the slice so um so that's why you need the invariant i mean if you want to say something about the class i mean you need the invariants of this you know of the uh of a smooth on the slice again yes of course but are you getting down to are you getting down to sort of something that you recognize Getting down to sort of something that you recognize as an algebraic object on the slice. I mean, what is the structure of the orbits on the slice? Well, I mean, it's what is the structure? I mean, it's a fine, I mean, this is a finite group. So on the spike, it's a finite group. Yes. So it was our set of points. Okay, thank you. We can talk about. Thank you. We can try to do that. I mean, on this case, I mean, for this specific case, I mean, it's a set of points. I mean, it doesn't need to be that way. Yeah. Okay. Thank you. This is Debbie Lewis. Super quick. Just familiar with using slices. I think slightly, not the algebraic, but just on manifolds and bifurcation theory. And usually the slice is matched to the isotropy at a particular point. And so if you have different size continuous isotopes, Size continuous isotropy groups. They'll use different slices in different places. So, is this something that works globally? It looks like it will for the symmetric case with the plus minus. Not clear what slice you'd want to use for something like the umbilic point situation that came up yesterday. So, a very vague question. Maybe slice-interested people can bring one of the breaks with something. So, I'm not sure which. I'm not sure which talk you're referring to from yesterday because I went to bed at some point. There was one where someone's using choosing a basis on surfaces and Peter raised the question of, okay, what happens at an umbilic point where you don't have this unique choice determined by your Riemannian geometry? Riemannian geometry. And that was something where, again, in the context I'm familiar with, you would use a slice to analyze what's happening local to a point like that where you have a change in the isotropy. But that slice would be kind of a pain in the rear to work with. Yeah, so here, I mean, it's pretty clear what are the so here we so the generic orbits are given by by By when the diagonal form when the have all its eigenvalues which are distinct. So, for symmetric matrices, the generic thing is the symmetric matrices with distinct eigenvalues. As soon as you start to have two eigenvalues which are equal, then the isotropy is bigger, and that's your yeah. Yeah, and all the results I gave are kind of outside of some variety, and typically that would be the variety here where you have two eigenvalues which are the same, at least two eigenvalues which are the same. Thanks. Yeah. So thank you very much, Evelyn. So let's thank you, the speaker, one last time. 