Yeah, I'm happy to present one of our recent work on parameter estimation in finite mixture models. And this work is based on a joint work with Shen Nong Wing, who is my PhD advisor, and Cheyenne Mukherjee, who is my postdoc mentor. And here's the outline of the talk. Firstly, I will introduce the finite mixture model, and then I will also introduce. Mixture model, and then I will also introduce the estimator, minimal distance estimators. And I will talk about the uniform convolution rate. If I have time, I will also cover how to estimate the number of mixture components and also how to calculate the point-wise convention rate. Okay, yeah, so here is like a representation of some data sets. And roughly, we can see there's a heterogeneity of this data set. Of this data set. And roughly, we can say we maybe can divide the data into five components, where each circle represents one component. And such data is not uncommon in practice. For example, if each data point is the width or length of the seedl or petal of sunflower, then each circle here might represent a specific flower species. If each data If each data point is some kind of image, then maybe each circle here might represent a certain object. And also, if each data point here represents a text document, then each circle here might represent a latent topic for the text documents. And typically, when we see such heterogeneous data, one common choice is to model this data by financial models. So just assume that we have some latent variable z, which indicates the label or the cluster of the data. And let's say this z, when z equals i, the probability is pi star. And the data is assumed to, given that it comes from the i component, the density is px given theta i star, where the theta i star depends on the components. The components. And if we just write down the density for this observed data x, then it would be just like a convex combination of each component. So we just have this finite summation of pi star times the density p x given z i star. And if we model data by this finite matrix model, we can just do classification or clustering easily. Essentially, when we observe some data, we just calculate. We observe some data, we just calculate the latent variable z equals i given this data. And by Bayes' rule, it's not difficult to see it's proportional to pi star times the density when we plug this data x into the density. So px given theta i star. And then we just assign this particular observation to be the cluster that has the largest this product pi star times p. This product pi star times p x given zeta i star. And here is a one-dimensional graph illustrations. So, assuming that we have three components, then we can see that on the left of this intersection point, we can say it to be class one because this density is larger. And from this method, we know that to perform this classification, we have to know this true parameter, pi star and theta i star. So, typically, what people do. So, typically, what people do in practice is that when they give the observation, they firstly use the data to feed this parameter, k star, pi star, and i star. And then in the second step, they perform the classification. So therefore, we know that in order to do the classification, we have to estimate this parameter first. And in today's talk, we will just focus on how to estimate this parameter. Okay, so now. Okay, so now let's formally define our problem. Assuming that we have these mixture densities, we know the unknown parameter is the number of components, k star, and also the mixing proportions for each component, pi star, and also the component parameter, theta i star. And we can actually write this all the unknown parameter compactly as disquiet distributions. disquiet distributions. So we just write this define this G star to be the disquiet distribution where this delta means direct measure. Basically it takes when it takes value theta i star the probability will be pi star. And if we are able to recover this three star then essentially we can potentially know all the information about the unknown parameter k star pi star and theta i star. So by adopting this notation So, by adopting this notation, we can also rewrite this finite summation as integral with respect to theta to the parameters. And we will denote this mixture density by pg star x. And here, we assume that the component distribution is null. For example, it could be Gaussian, it could be gamma. Yeah, even though we assume it's null, but it's a general. And as one typical example, you can think of location causation mixture in this talk, even though we are considered just the general finite mixture models. Also, I want to introduce some notation. So by capital P theta, we just mean the probability measure corresponding to the density Px given theta. Also, we use this notation P of G to denote the mixture probability measure when the mixing. Measure when the mixing measure is G. And this PG star will be the true probability measure. And because we think this parameter as a discrete distribution, so essentially we have this map go from the discrete distribution to the mixture distributions. And also, here I want to emphasize that this component parameter is assumed to be on the general space. So it could be multivariate or So, it could be multivariate or it could be a more general space, say some polished space. And traditionally, when people trying to do estimation, they will assume the number of components, k star is null, and they also assume some separability between each component. Then they try to study the convergence rate for this pi star and theta i star. And because actually, we here we consider all the unknown parameters as discrete distributions. Know parameters are discrete distributions, and we're trying to estimate this discrete distribution directly. So, we don't need these two assumptions, so we don't need to assume that the case dies no, and we don't need to assume there's some separability between each components. So, formally, what we observe is just an IID data from these two mixture distributions. And based on these observations, our goal is to try to estimate this mixing distribution G star. So, we will introduce some estimators. We will also study the commodity rate. And we will also talk about in what sense it is optimal. Okay, so let's firstly introduce the concept of pointwise convergence rate, also uniform convergence rate. By pointwise, yeah, actually, we need to firstly assume we have some kind of distance on the space of mixing mirror, which are discrete. Mixing error, which are discrete probability distributions. So assuming we have this distance W1, then by pointwise convergence rate, we basically just mean the G star is fixed. For this particular G star, assuming we have some kind of estimator, we just measure the distance from our estimator to the true mixing value of G star, and then we calculate the expectation. And by uniform convenience rate, we basically just mean. We basically just mean this G star can actually take not a fixed value, but it can take any value in a particular subset. So to introduce the set, we will assume that we have no upper bound k for this k star. And we use this curly g k theta to denote the space of all discrete distribution with at most k atoms on this support set theta. On this support set data. So with this space, then we can just formally define our uniform convenience rate. Essentially, just a pointwise rate. And then we let this G star vary in this set with all discrete distribution with at most k components. Essentially, this uniform convergence rate is just to study the worst case convergence rate. Okay, so in Okay, so in the paper, we study both the point-wise commodity and also the uniform commodity rate. And now let's briefly go over some of the existing literature here. So the early literature only studied the point-wise conventions ray. I believe the first one to study the conventions ray in finite matrix model for the mixing measure is this paper by Is this paper by Professor Jaharsen in 1995, where he studied the minimal Komograph's mean of distance estimators? And then in 2013, my PhD advisor, he has a paper where I think it's the first time to use Wosen distance to measure the error of the estimation in finite matrix models. And recently, they were more interested to study the uniform commodity rate. Yeah, and actually, it's kind of surprising because the financial model is used a lot in practice, but the uniform commodity is only studied like very recently. So, you can see the first one is in 2018. This paper by Henry Chekahan, they also studied the minimum chromography mean of distance. And they showed that for the And they show that for the worst case convergence rate, it's n to the negative 1 over 2 times 2k minus 1. And let's recall this case the upper bound for the true number of components. So you can see actually this uniform rate is quite different from the point-wise rate, and actually it's much slower. And two years later, there's this paper by Wu and Yang where they studied metal moments. They also recovered. Moments they also recover the same uniform rate, but only for Gaussian nature models. So, you can see these two papers are quite interesting, and also they generate some questions. For example, we want to ask whether the result by Henry Chikahan can be generalized beyond the univariate mature model where they consider there. And also, for common graph distance, it's also hard to evaluate. So, can we just also overcome this issue? Also, overcome this issue. For these metal moments, the limitation is that they only apply for Cauchy mixture model. And also, I want to mention all these four existing work, they are all published in the Annals of Statistics. So in our paper, we try to address the red points here. So essentially, we propose a general framework in the Work in the finite mixture model. And this will extend the minimum Kamograph's mean of distance to multi-value case. And also, this will extend metal moments to beyond the Gaussian mixture models. And also, our framework can also produce new estimator. So one example is maximum mean discrepancy estimators. And also, we provide a unified theoretical guarantee under this general framework. Okay, so let's. Okay, so let's also here is the like the rough structure the brief structure. So we are we know the parameter is a mixing measure and then we once we have this mixing measure we have the mixture distributions and then we observe data from this mixture distributions and when we try to do estimation essentially And when we try to do estimation, essentially we're going backwards. Based on this data, we firstly do the density estimation for the mixture distributions. And then afterwards, we're trying to go from the density estimation to the mixing area estimations. So roughly for estimation, we can consider there are two steps. The first step is to do the density estimation to estimate the mixture density. And then the second step is to go from the density estimation to the mixing distribution. Density estimation to the mixing distributions. So the first step here actually is quite flexible. We can consider MLE maximum likelihood estimator or maybe we can also consider the Bayesian estimators. In today's talk, I will just use the minimum distance estimators as one example. But I think the first step is flexible, and you can also consider other estimators. And the technical part is the second step. Technical pie is the second step where we try to go from the dense estimation to the mixing distributions. And this is the key contributions in the paper. Okay, so now I will formally introduce the definition of minimal distance estimators. Okay, so let's first consider the population version. So let's consider this population sorry. Yeah, let's consider this population version. For here, we need to assume that there's some kind of distance on the space of probability variables. And if we just consider any mixing distribution g and so therefore we have this mixture Have this mixture distribution corresponding to this mixing measure g, and then if we just measure calculates its distance from the two mixture distributions, pg star, and if we do the minimization, of course, we will get the g star because when g equal to g star, this distance will be zero. And this is the population version. And the issue is that it contains the true mixture distribution PG star. Distribution PG star. So empirically, we just replace this PG star by the empirical distributions P of n. And if we replace it and still perform the same minimization, we will also obtain some kind of minimizer, g n hat here, which is our estimator. And this is so-called minimal distance estimator. Let's give some graph representation here. Some graph representation here. Essentially, we have this Pn, which is the empirical measure, and we know this Pn is not necessary on the space of mixture distributions. So, this surface represents the space of mixture distributions. And the empirical mirror is not necessary on the space. So, what we do essentially we just project this empirical mirror down to the space of all mixture distributions. And this projection point is called. And this projection point is called P gn hat, which is our density estimation. And the corresponding mixing measure will be our minimum distance estimators, g n hat here. And the benefit to consider this minimum distance estimator is that it's very easy to obtain the density convergence rate. We know that this will be our estimator for the density. And if we want to calculate the density estimation, And if we want to calculate the density estimation rate, essentially we calculate the distance from this projection point to the true mixture density, which is basically the length of here. And just by simple triangular inequality, we know it's smaller than the length of this solid line plus the length of this dashed line here. And then because this PGN hat is the projection, so we know this the length. Projection. So we know this: the length of the solid lines should be less than the length of this dashed line here. So essentially, we get this density information rate should be less or than the two times this length of the dashed line here, which is the distance from this empirical distribution to the true mixture densities. And here we are still in the general setting, so we haven't specified this distance d, but typically when we specify But typically, when we specify this distance d, and by the theory of empirical process, we are also where you typically can show this rate is of the order n to the negative one half. So we see that, so far we introduced the concept of minimal distance estimators, and also we see that it's very easy to obtain the density confidence rate. So essentially, we already complete the first step. Already complete the first step here. Now, let's see how we can perform the second step here. How can we go from the dense estimation to the mixing mirror estimations? So let's recall we have this map. We have this map go from the mixing area to the space of mixture distributions. And we assume this mixing area takes value in this query distribution with almost k components. query distribution with at most k components and we assume there's some distance w1 on this space and for the mixture distribution we also have this space we also have assume there's some kind of distance on the on this space and we already obtained the density convergence ray so we what we hope is that if the mixture distribution are close then the corresponding mixing measure are also close if we uh have this result then we can go from this mixture Then we can go from this mixture density estimation to the mixing measure estimation. And formally, we're trying to establish such an inequality. Yeah, so the distance between the mixing layer is upper bounded by the distance between their corresponding mixture distributions, possibly raised to some power S. And we call this inverse bound. And of course, once we establish this inverse bound, then we can go from the density estimation rate to the mixing meal estimation rate. But generally, such inverse bound will be difficult to establish for like a general distance W1 and general distance D. So now the question is that for what pair of distance W1 and D, so such that this inequality is possible. And also, we also want to make this D as general as possible because we know we are considered a minimum distance estimator for each particular choice of D that corresponds to a different choice of estimators. So we want to have a general framework. So we want to make this D as general as possible. In terms of W1, we can just make one particular choice. As long as this W1 can tell us, As long as this W1 can tell us information about the parameter convention ray, that should suffice. Okay, so these are just some kind of criteria on how to choose the distance D and W1. So now let's formally introduce how we choose this distance D. So what we consider is the so-called integral probability matrix for the distance D. Assuming that we have some kind of function. Assuming that we have some kind of function class f1, then this integral probability matrix with respect to this function class f1 is defined as follows. Just take any two mixture distributions, pg and pH. So we just take any member from this function class and then we calculate its integral with respect to pg and also with respect to pH. And then we just calculate the difference between the integral and take the shoop. So in two So, intuitively, you can think each F1 here represents a certain direction. And this turns basically calculates how these two probability distributions are different along this particular direction F1. And then we just calculate the maximum deviations. As we mentioned, we want to, so this will be our choice of D. And as we mentioned, we want to make this D as general as possible. So let's see how why this D is. Why this is general. So, if we choose this function class to be the indicator function, then we recover the common graphs mean of distance. And if we choose this function class f to be the boundary function, then we can also recover the total variational distance. And if we choose this function class to be Lipschitz function, then we can recover the Wolfenstein one distance. And if we choose this function to be the unit board in reproducing kernel Hilbert space, Ball in reproducing kernel Huber space, then we recover the maximum mean discrepancy. So, as we can see, this particular choice is quite general. Now, let's introduce the distance for W1, which is on the mixing mirrors. Okay, due to the time constraint, I think I will skip the formal definition, but we essentially just. We essentially just use the Wolfenstein one distance on this mixing measures. The formal definition is not important, but I just want to emphasize that the Watson stand distance essentially will be very close, like approximately equals to the difference between the component parameter plus the difference between the mixing proportions. So therefore, once we obtain some common Once we obtain some commodities ray in terms of the Watson-sun distance, then immediately we can get the commodity ray for the component parameter and also the commodity for PI. And that's why we use the Watson-sun distance. Okay, now let's formally rewrite our model in terms of the new distance. So we still consider the minimum distance, but we just replace the distance D by the integral probability matrix. Probability metrics. So we just take any mixing measure, we just calculate the difference between this PG to the empirical distribution. And then we perform the minimization, the corresponding minimizer we obtain is the minimum integral probability metric estimators. As we see that it's very easy to obtain the density convergence rate. And actually, it turns out for this general distance. For this general distance F1, we are also able to establish this inequality, the inverse bond. So we can show that the Watson-Sun distance between the mixing distributions are upper bounded by the distance between their corresponding mixture distributions raised to the power 1 over 2k minus 1. So therefore, firstly, we obtained the density. Firstly, we obtained the density condition rate and then we use this inequality. We can recover the covid straight for the mixing distributions. So, now let's formally present our results on the uniform covid street. So, here's the statement of the theorem. Assuming this space data is compact and just let our estimator to be a minimum integral probability metric estimator. And then, under some regularity condition, The sun regularity condition, we can show that for any true mixing layer G star in this space of all discrete distribution with MOSK components, we can see this the estimation rate is upper bounded by this right-hand side here and right-hand side is the so this part basically just the you can see it's the empirical distribution and this part is the mixture distribution. Is the mixture distribution. Essentially, this is the distance from the empirical distribution to the two mixture distributions. And because we are using this integral probability metrics, and if we write out this definition, essentially it's just some empirical process, which depends on the function class F1. So, what we show here essentially is that the estimation rate is controlled by this empirical process. By this empirical process. And typically, once we specify this function class, this empirical process can be showed to have the order n to the negative one half. And if we plug that into this right-hand side, we will get this the convoyance rate for mixing measures will be of the order n to the negative one half raised to power 2k minus 1. So, so far, we can see that we are able. So far, we can see that we are able to obtain the convergence rate for this general minimum distance estimators. Actually, we can also show the lower bound. For any possible estimators, the best, the smallest uniform rate has to be at least of the same order, unto the negative 1 over 2 times 2k minus 1. So by combining these two inequalities, we see that this. Combining these two inequalities, we see that this minimal distance estimate actually achieved the best possible uniform rate in the minimax. Okay, so next I will just specialize some function class and see how can we recover the previous result and how can we reproduce new estimators. So, firstly, if we choose this function class F1 to be the indicator functions. Class F1 to be the indicator functions, then our estimator reduces to the minimum homograph's mean of distance estimators. And the regularity condition essentially reduces to some linear independence between the C D F. And then we just apply our general theorem, then we show that the uniform convergence rate is upper bounded by this empirical process. And which is less okay than this n to the negative one over two times 2k minus 1. So this result recovered the results from this paper by Hen Shen Kahan in 2018. And also our result is more general. So our result applies to the multivariate case. And also if we look at some of the technical assumptions, our result actually require weaker assumptions. Require weaker assumptions. Okay, next, let's see how. Okay, okay. Okay, so next, let's see how can we recover the metal moments. So for this one, let's just consider the location Gaussian mixture models. And we will consider the homopoly polynomial of degree R and our function class F1 will be the home polynomial HR where the degree Where the degree goes from 1 to 2k minus 1. And by choosing this particular function class, we will be able to recover the metal moments. And also, our general theorem can be applied so that we also obtain the uniform cognitive state for the metal moments is of disorder. And this recovered this result by Wu and Yang in 2020. And actually, our result is slightly more general. So we can go beyond the Gaussian maximum model. In particular, Gaussian matrix model. In particular, our results applied to natural exponential family with quadratic variation. And finally, let's see how we can also use this to produce new estimators. If we consider there's some kernel on the sample space, and if we take this the function class to be the unit ball in the reproducing kernel Hilbert space, then this integral of probability metrics reduce to the maximum mean discrepancy. The maximum mean discrepancy. So then basically, we have a new estimator: minimum MND estimators. And this estimator is normal, at least in the literature of matrix model. And also our result can be applied so that we can also obtain the commodity straight for this estimator. And there are some benefits for this estimator. Firstly, it's easy to compute. Firstly, it's easy to compute compared to Komograph's mean of distance because we don't have the shoop anymore on the right-hand side. And also, comparing to the metal moments, it's more general. It can be applied to general mixture models. Okay, I think I will skip many of the results. Here, I just want to briefly mention we also have this how to estimate the number of matrix components in result in our paper. And different from the existing literature, we can. Different from the existing literature, we can actually contend, we can also show the estimation rate. So, for some of the existing literature, they only show the consistency on how to estimate the number of metric components, but we also have the estimation rate. And finally, we also have this point-wise estimations. So, once we estimate this number of mixture components, we just plug them into the minimum distance estimators. Then, we can also Distance as meters, then we can also obtain the point-wise convergence ray, which is unto the negative one-half. And yeah, sorry, I cannot cover too much details for the later parts. And let me just quickly have a summary. So our paper proposed a general framework. And this general framework basically established the convergence rate in terms of some empirical process. And when we specialize, And when we specialize different function classes, we will be able to recover many of the existing estimators. So, for example, we can recover the minimum comma gross minimum distance estimator. We can also recover the metal moments. And our result can also produce new estimators. Okay, and if you find the presentation interesting, please just go ahead and read our archive paper. Archive paper. And also, I appreciate any comments because this paper currently is under review. So, if you have any questions or have any comments, just let me know. Thank you.