Thanks for the invitation to speak here. So I'll be talking about some recent work with my collaborator, Nanjing Lee, on some initial steps towards understanding learnability for transformers. So as we all know, this is sort of the architecture behind a lot of kind of incredible empirical successes over the last couple years. But as with most things in deep learning theory, In deep learning theory, empirical success is not really met with corresponding understanding in theory. So we're trying to remember the scale. So in this talk, I want to tell you about sort of a clean theoretical sandbox where we can hope to try to rigorously probe the learnerability of these kinds of functions that arise in these practical contexts and prove some of the first learnerability results of this functions. I should mention that this comes in the wake of a long line of theoretical work on learning BFOR networks, which is some argument. Some architecture that came before. And I want to emphasize that this particular line of work, there are still many interesting mathematical mysteries. But in today's talk, we'll focus on this more modern architecture. And it'll be kind of more outward-looking towards what kinds of questions we can even begin to pose from a theoretical perspective. And I hope to come into that the theory for learning transformers might shape out to be quite different from the final theory for the feedboard setting. The feedboard setting. So I need to define for you what a transformer is. So here's one slide. So kind of the main building block behind these functions is this notion of detention. So you can think of this as just a function that maps k by D matrices to K by D matrices, where it takes as input some matrix whose rows kind of correspond to tokens, their input. So each token is a d-dimensional vector input case. Each token is a d-dimensional vector, you get k. And this function will now transform that sequence of k tokens into some other sequence of k tokens. And so I'm just gonna use some words that, you know, I'll give some names to these parameters. So these functions are defined by what I'll call an attention matrix and a projection matrix. These are just names. Need not be projectors, and I think this terminology is a little non-standard. Okay, so these are just D by D matrices, and so the corresponding, you know, So the corresponding self-attention mechanism is going to operate as follows. So you're going to take your input sequence and you're going to compute essentially inner products between pairs of tokens. But the inner product is going to be defined with respect to this matrix theta. You're going to then apply a row-wise softmax to this matrix of quadratic forms. And so this results in a matrix so that every row of that matrix is a problem. row of that matrix is a probability vector. And each probability vector then tells you a way to sort of mix the different tokens in your original sequence. So that will give you now a new k by D matrix, which is the product of softmax and x. And then finally you have some linearity on top that you use to transform these new mixtures of turbulence. Concretely, if I look at sort of the i-th row of this output, I'm essentially looking at how the i-th row is the rhythm of the Looking at how the ith token, so xi is just some d-dimensional vector, some row in my original input. I'm looking at its inner product with respect to this detention matrix theta across all the possible tokens, and then I'm applying a softmax. I'm just exponentiating every entry and normalizing, so that I get that probability vector that specifies how to mix together the rows of x, and finally I apply a linear transformation to get another d-dimensional vector. Okay, so that's the function. So data. So data doesn't so we do this to sequentially produce the next token? Is that ah so in this talk we're only going to focus on sort of uh sort of sequence to sequence functions. So instead of imagining I mean in practice what you would do is you would imagine like to generate the i plus first token, you'll look at the output of this function under the kind of input sequence which is given where the first i rows are given by some tokens corresponding to your sequence so far. And maybe you have Your sequence so far, and maybe you have some padding at the end because k might be longer to find. Okay, so f transforms a list of k tokens into another list of k tokens. Exactly. And data doesn't depend on the relative position in the... Right, right. So in this talk, we're going to sort of sweep those issues under the rug by kind of thinking of this data as given to us by nature. And our guarantees will apply to fairly general data. But you could encode positional encoding. Fairly general. But you could encode positional encoding in some way. Brown truth. But so, yeah, alright. Okay. Yes. Anyway, so this is just some family of functions. It can, with a certain setting of thetas, you can encode, you can get things like just check encoding. Am I correct in thinking that if I permuted the input sequence, the output sequence would be just underdo the same permutation in this model? Because theta is the same for all pairs of positions. It's not. Ah, you're saying if you permute the rows of x, would you permute the rows, the corresponding rows of the outputs? Yeah. That's true. But not if you permute catching matrix. Yes, yes. All right, thanks. Because it's one constant d by d nature. Yeah, that's right. There's no, there's a reference to K. Great. Okay, so that's single-head attention. So the title of the top is multi-head attention. So the title at the top is multi-head attention. What is that? That's nothing more than a, in this notation, it's nothing more than a sum of single-headed tensions. So now instead of giving a single attention matrix and a single projection matrix, we imagine maybe n of them. And the corresponding function is just the sum of all these different sequence-to-sequence functions that I defined on the previous slide. Okay, so this looks a little different from what one might encounter in a typical machine learning paper or machine learning blog post. Rest assured that kind of the formulation in practice is. The formulation in practice is one special case of this particular parametrization. But for us, this will be fairly clean moving forward. So yeah, just think of the function classes defined as well. Okay, so before I tell you about the kind of general setup that we'll work in, let me just tell you what's known about theory for transformers. At the risk of omitting. At the risk of omitting any of the works, at a high level, the bulk of this literature focuses on representational theoretics. So, really saying that there exists some setting of weights for which this particular function class can implement some sort of interesting task, and potentially showing separations from other architectures. There are, of course, works on sample complexity. These are really drawing upon existing tools in the statistical learning theory literature to say, well, if I ignore issues of computation, Say, well, if I ignore issues of computation, how many samples would I need to learn? There are some interesting works recently kind of giving this dynamical systems perspective on sort of the evaluation of these kinds of functions across many layers. These are going to be orthogonal to the focus of this talk. Probably more relevant to today's talk will be this, you know, the extensive line of work just in the last two years on training dynamics for small. Dynamics for small transformers. So the focus here is you hook up some clean toy model for data, which if you kind of ignore aspects of transformers, this is a toy model that is kind of trivially wearable. But the really hard part is understanding why if I train a single attention head with reaching the descent, it will be able to solve this task. So that's kind of been the focus for a lot of these works. There's this nice paper that dropped, I think, attention yesterday, understanding Essentially, yesterday, understanding how transformers like a single attention head composed across two layers can learn some interesting causal structures and distribution. On the multi-head attention front, the work is comparatively sparse. So there are some works in kind of the linearized regime where either you imagine that you freeze the attention matrices and just train these linear functions that are sitting outside. That's kind of like the transformers analog of all this work on this interesting work on random features. Interesting work on random features setting. And then there are also some NTK-like results in this computer. The reason why multi-hat attention is sort of hard to learn in some sense, or hard to kind of pin down the train dynamics, one aspect is, you know, it's a little tricky how to reason about when the student network is actually kind of breaking symmetry in the sense that the two heads that you're training kind of start to diverge. So these things are generally quite hard to analyze, and perhaps it's. And perhaps this explains why currently there hasn't been much work in this. So, let me tell you about the setting that we consider. We're going to kind of take a slightly different tap. So, instead of imagining that the data distribution is coming from some toy model that's kind of trivially learnable, we're only going to make the following assumptions. We're going to assume that the input data, the input sequences, come from some nice distribution. And I want to make as mild use of this nice distribution as possible. Of this nice distribution as possible, my sort of end-to-end learning guarantee. And secondly, the output sequences, my labels for my data, are somehow perfectly explained by some simple ground-truth transformer. So in other words, there exists some choice of weights for which you would get zero test loss. And furthermore, that particular network would be exceedingly simple. So, this is a setting where you would have no doubt whether or not creating a setting of brackets would work. Really, the question is: can we come? Work, really the question is: can we call it with any algorithm with approval guarantees for this? So, this is really the setting of realizable pack learning with specifically distribution-specific realizable pack learning, where you assume some nice input distribution and you assume that there is some ground truth function that perfectly explodes. So, just to be more concrete, right? So, we imagine that we have some input sequences that are samples from some nice distribution. From some nice distribution. And we imagine that the output sequences are just given by some simple transformer f, some simple multi-head attention layer. And our goal is just to output any function, f hat. In this case, we output something from the original function class. So with high probability over the randomness of the samples in the algorithm, we achieve small square laws in the sense that if I sample some random input sequence x from my nice distribution d, and I look at the square distance between my output and Distance between my output and sorry, my output and the true output. This is a very, very, very simple setting. So here I'm going to make a maybe controversial distributional assumption. So we're not going to try to leverage very strong features of the input distribution. So, as a highly stylized proxy for kind of the discrete nature of real-world tokens, I'm going to assume that the tokens are essentially. I'm going to assume that the tokens are essentially uniformly sampled uniformly in reference. This is definitely a very sort of unnatural distribution from a practical perspective, but kind of the main takeaway from the guarantee we're going to show is that we're not going to leverage too much about this particular distribution. And presumably, if you impose additional helpful correlations in this data distribution, the learning problem should become even easier. In particular, we're trying to understand what are features of the function class itself that make the learning task possible. I should also mention that. Possible. I should also mention that this also carries over to the setting where the tokens are Gaussian, which has been kind of the focus for the bulk of the work in this literature. And there actually, the analysis is strictly Eastern for reasons I don't think. Okay, so let me tell you what we show at a high level. But first, let me kind of contextualize this under the more general backdrop of deep learning theory. Okay, so there's been a ton of work on path learning. Path learning speed-forward networks, mostly focusing on one-in-layer networks. So, here the underlying function is essentially just a linear combination of non-linearities. So, you might have some weight vectors, w1 through wm, and the function is computed by projecting your input along those directions, computing some non-linearity, maybe sigmas like a Raylu or something, and then taking a linear combination. Essentially, all of the guarantees that work outside of The guarantees that work outside of kind of linearized regimes focus on the input distribution being Gaussian. And there are some guarantees. I won't really get into details about this. I do want to emphasize, however, what are structural features of the problem that this literature heavily exploited. So one very nice structure, which we'll hear more about in the next talk, is this kind of latent, this low-dimensional structure that's sitting under this function. Function. So the function only depends on sort of the projection of the input along these weight vector directions, W1 and Wn. So inherently, this function is going to be only depending on a low-dimensional projection of the input. So this is, it's called the multi-index model. The structure is something that one could hope to exploit. Another feature is that the input distribution itself is, you know, it's nice and rotationally invariant. And of course, all the coordinates are product. This is kind of unique to the Gaussian distribution. This is very useful. Distribution. This is very useful also in conjunction with this low-dimensional structure because it allows you to, for instance, rule out certain directions of space. I won't get into details about this. And lastly, a lot of this work implicitly is making use of this rich algebraic structure in the moments of the joint distribution between input and rabble. So in particular, if you wanted to cook up a particular low-ranked tensor that's essentially some layer combination of tensor powers of A sun layer combination of tensor powers of these unknown parameters wi, you can do that. You can cook up some kind of moment that will tell you this information. And so I just want to quickly mention that in our setting, none of these kind of features hold. So this multi-index model structure breaks down because the function ultimately doesn't depend necessarily on a low-dimensional projection. Of course, if theta i itself were low-ranked, then we do have something to leverage, but our guarantees will not exploit this. The product notation. The product and rotation variance, of course, that's specific to Gaussian. And in this work, we really wanted to try to get something that worked beyond the setting. And then finally, this useful moment structure, for all we know, it could hold, but at least for us, we found it very difficult to reason about moments that involve the soft maps. In general, it just seems quite hard to get any kind of analytical handle on expectations involving this thing. So, really, the kind of meta-question we're trying to understand is what are The kind of meta-question we're trying to understand is: what are structural properties of this function class, if not these, that actually enable efficient learning, maybe not by gradient descent, by some plug-ins like that? Yeah. So in our setting, we yeah, well, we're going to work in a setting where it's almost identifiable. There's some slight degree of ambiguity at the end, which we did not resolve. So we give proper learning, but not prioritizing. Learning, but not prior learning. It should be identical. Great. So let me tell you our results. So we consider kind of some regime. This is not going to fully capture what goes on in practice. In particular, we're going to imagine that the transformer is actually very simple. It's just a multi-head attention layer with a very small number of heads. In particular, the number of heads m is going to be much smaller than the ambient dimension. And the number of tokens will also be smaller than the ambient dimension. Will also be smaller than the ambient dimension, maybe by some polynomial factor. So, this is some setting that we'll operate in. Of course, it's not going to capture everything, but it's sort of our starting. So what we show is the following. We show that there actually is an algorithm that runs in time d to the m cubed, or path learning this function class, at least if you make certain non-degeneracy assumptions on the function class, up to error d to the negative omega m. To our knowledge is this. To our knowledge, this is new even for single-head attention. I should specify somewhat what these non-degeneracy assumptions are. So, we need to assume certain incoherence across the different parameters of the heads. We also need to assume that they're not too low rank. And furthermore, in order to handle some of the issues that come up, for instance, anti-concentration for the hyperQ, we need to assume some density assumptions on the rows or the columns of its matrices. So, I won't get into the Of its matrices. So I won't get into the details of this, but suffice to say there are a significant number of non-degeneracy assumptions we need to make. Finally, let me mention a bit about why this runtime is maybe, you might be wondering why there's this exponential dependence. It turns out that if we, at least in the worst case for this problem, even if you're working with, say, two tokens, k equals two, you can show lower bounds that suggest that this is actually some kind of exponential dependency is necessary. Some kind of exponential dependencies necessary. So we show this via a statistical query lower bound. And in general, we show under some variants of learning with errors that you can't really hope to get fully polynomial, at least in the worst case. So it could be, however, that in this sort of nice regime that we're working with, where you make some non-degeneracy assumptions, maybe you could build a polynomial, but at least in the worst case, this is not the case. I want to say a bit about the object. I want to say a bit about the algorithm. Before we get to the algorithm, did you consider if you would take a polynomial approximation, what's the runtime? Or we don't know? I think the answer is just we don't know. For all we know, you could have Fourier decay, but it's not hard. Okay, so let me tell you about the algorithm. This is not something I would suggest you run in practice, but it's something that at least we can prove works. Okay, so it has sort of six stages. Sort of six stages. In the first stage, we're going to estimate some partial information about the parameters of the function. So we're going to crudely estimate the sum of the projection matrices that sit outside. It shouldn't be immediately clear why this is sort of helpful information, but it turns out that you can use this partial information to then construct a very crude estimate for the following object, which is essentially the affine hole of the estimate. Of the attention matrices, theta i. So it's going to be the set of all kinds of linear combinations of the theta i's where the linear combination coefficients. Here I want to emphasize the error with which we estimate all these objects is quite bad, actually. And it turns out that with this kind of crude estimation of this apine hole, we can't really get too much information. But it turns out that you can then kind of take your crude estimate of this apine hole and then back out a more refined estimate for what you, you know, completely. Refine estimate for what you computed in the first step, and then use that to refine your estimate that you computed in the second step. And at this point, your estimate for the affine pole is sufficiently fine that you can do some kind of epsilon net argument over the space spanned by this affine hole. So ultimately, the most interesting part of the analysis really comes in the first three steps. I think I'll probably only be able to cover maybe the first two steps. So let's just see what slides I end up sexing quite quickly. What slides I end up sexually. Okay, so let's start with estimating the sum of the WIs. This part bears the closest resemblance to some of these methods of moments-based techniques that show up in this literature, but with one crucial difference. So my goal is just to output something that's somewhat close to the sum of the WIs. And it turns out that there's a very simple algorithm that already achieves this. You can think of it as follows. I'm going to train a single attention. To train a single attention head to try to match a multi-head attention layer. This seems like a fool's errand, but it turns out that if you initialize the parameter of your student network that you're training to the all-zeros matrix, and you just train the outer linear function w, it turns out that with just a single gradient step, you can already get something with appropriate step size. You can already get something that's reasonably close to the sum of the wi's. So, if you don't care about gradient descent, really the main expression that you want to understand is the following. You want to look at the correlation between, okay, so where is this? This is the all ones matrix, k by k. Where does that come from? That just comes from taking the softmax of an all-zeros matrix. That's why this term emerges. And you want to basically look at the correlation between your output sequence and this particular mixture of the rows. Of the rows of x according to the all-ones matrix. And what you want to show is that this particular matrix is a good approximation to the sum of the projection matrices. This is sort of all you have to do. I want to really emphasize that this approximation, you can't hope to make an equality. In fact, even if you had an infinite number of samples, you would not be able to get sort of arbitrarily close to some of the wi's using this approach. So, this is, I view as kind of a subtle distinction between. kind of a subtle distinction between this technique and method of moments in that the population statistic you're targeting isn't actually an unbiased no it isn't actually exactly expressible in terms of some simple function of your parameters. Okay so let me skip some technical details and get to like maybe the main estimate that we want to show. Ultimately what you want to show is something of this form. So I kind of mentioned that it's kind of hard to compute expectations involving software. Expectations involving softmaxes, this is kind of the best thing that we could accomplish. So we're ultimately showing that some particular matrix product that involves a softmax in the middle is approximately the identity matrix. So let me kind of briefly sketch the intuition for how to handle the fact that we're taking an expectation that involves a softmax. Let's pretend for the moment that this inner term, softmax of x theta, x transpose. softmax of x theta x transpose is actually a deterministic matrix. It doesn't actually vary in the randomness of x. Then, okay, let's say the expectation of that is delta, and it's actually deterministic, so it actually is equal to its expectation. If we plug this into the above, then things seem not so bad. So now I just have an expectation of x transpose times the deterministic matrix times x, and an expectation that's going to give. And an expectation that's going to give us just the identity times some trace product. And this trace product you can work out to be just one. The reason being that the softmax, every row is a probability vector. And so each row sum is just one. Okay, so this is some vague intuition, and it's actually horribly wrong. The reason is that you don't actually get such a nice concentration. So you could think, okay, maybe softmax, it's nice and lips shit, and I'm taking. Ellipsids, and I'm taking things over the hypercubes, things are fine. But you'll note that the variance of this thing is just too large, potentially. So if I had theta i, which was, you know, had very large norm, and I plotted sort of the expected value for this soft max, you might get sort of like a dense matrix. But in reality, because every, you know, for a typical sample, you know, theta has such large norm, the soft max essentially behaves like a hard max. And so instead of getting It's like a hard max. And so instead of getting some nice dense matrix for a tip triple, you're just getting something that's essentially one sparse in every file. So this is no good. There turns out to be a hack where instead of arguing that the softmax kind of globally concentrates, we argue that it sort of locally concentrates in the sense that if I condition on the handing weight of some fixed substring of the input, and I look at the remaining randomness in the input, that corresponds to some conditional expectation on the soft method. Corresponds to some conditional expectation on the soft maps. And we argue that this conditional expectation actually concentrates in the randomness of the hand results. So this is maybe just a very high-level overview of the argument. So we go from, instead of trying to use global concentration, we just use this sort of local concentration. And here we only make use of upper and lower tail bounds for the higher Q1. Okay, let me briefly mention why the sum of the Briefly mention why the sum of the projection matrices is actually useful for anything and tell you a bit about the second step. Okay, so let's say we produced some estimate for the sum of the wi's. Now I want to produce an estimate for the affine whole. So let's consider kind of a hairbrain scheme where we imagine we saw, you know, we encountered a very lucky event. We saw some input sequence that induced the same attention pattern in the first row across all the different heads. So let's say that the softmaxes that you've produced Let's say that the softmaxes that you produced for some particular example, x, if you look at the particular, you know, the first row of each of these softmax matrices, they happen to be the same probability vector. Let's say we're, let's call that v. What you can show is that in this lucky event, you can actually express the first row of your output sequence as a linear combination of rows of x times the sum of the wi's. And that's the only reason why we want. And that's the only reason why we want the sum of the wi's. Because in this lucky event, if we can produce an estimate for the sum of the wi's, then in this expression, the first row of y is approximately some linear combination of rows of xw hats. We know everything except the linear combination. And so I can certify whether I've actually encountered this lucky event where all the heads have the same pattern in the first row by just running linear regression. And it turns out this won't introduce any false positives. Turns out this won't introduce any false positives. I'll be able to exactly certify when I get this lucky event. And then once I have this lucky event and I know the vector v, then I can back out information about the attention matrices. This part I think I'll call scale. But once you get this lucky event, you can extract out certain linear constraints on the attention matrices. And by collecting enough of these vectors that come from these lucky events, you can construct, you can sculpt out sort of an appropriate convex body that's guaranteed to contain approximately. Guaranteed to contain approximations to the parameters of your network. Okay, so I think I want to let me just skip ahead to a conclusion and take questions. All right, so I guess the high-level takeaway is that, at least to me, it seems like realizable hack learning is sort of a flexible language for really probing learnability of these functions in settings where you can actually prove things. We leveraged We leveraged kind of fairly different features of the function class in question. Instead of using this low-dimensional structure or sort of the Gaussianity of the inputs, we ultimately used kind of simple facts like this idea that every now and then you encounter this lucky event where a particular sample induces the same attention pattern across heads, and that allows us to extract information about the parameters. So, in particular, we only use ultimately So in particular, we only use, ultimately it turns out we only use sort of upper and lower tailbounds for dense linear functions. Okay, we have to make a lot of non-degeneracy assumptions on the underlying function. I think it's a very interesting question to see to what extent these can be removed. And of course, the million dollar question is to what extent, you know, gradient ascent you can prove works for this problem. This is kind of not clear to me even for two heads, how to argue about this issue of the heads diverging. This issue of the heads diverging during your training. And maybe like a very speculative and vague question: to what extent can we hope for these theories that have been developed for learning feedback networks and the ones suggested by this work for learning multi-head attention layer, to what extent can these theories actually be composed? Because ultimately, in practice, we imagine a transformer block is going to consist of a composition of both this attention mechanism that I described, as well as a feed force. As well as a feed-forward layer. And if we have kind of learnability guarantees for these two different function classes, can we get learning guarantees for their composition? This would already be quite interesting because even with a single transformer block, we know that you can generate coherent English. And so this seems like within the realm of the kinds of things that we can have. So that's all I want to talk about. Happy to take questions. But yeah, thanks for listening. Two questions, but yeah, thanks for all. So one of the assumptions is that the number of tokens has to be pretty small compared to deep. Is one of the reasons that you have to wait until you get enough collisions? Yeah, so it is indeed in that stage. The reason is that I claim that somehow if you can regress y, to get a linear comp, express y. Like to get a linear combination, express y as a linear combination of rows of x that we had, then you know that you've encountered this lucky event. If you imagine that k is like much larger than d, you might get a lot of, I mean, you'll get false positives. Like, you'll always be able to express y as their combination of the things or those. And that's like, you can imagine some kind of noise model, like there's a softback on the top of the network that generates word or something. You think probably this kind of algorithm will work for eventually? Yeah, I think the first stage is the one I'm kind of worried about. Yeah, it is quite different. It is heavily using the linearity of the math. Yeah, yeah. But that's for this particular algorithm, for all we know there. Yeah, there's some other approach. Here's object refers to the exponential of every entry and then normalizing the okay, so I mean this is not a point new function. But you know, like on some But you know, on some level there are polynomial functions that are sort of similar, right? You take high power and then the expectation that this should behave would make it theoretically much easier, would it behave in practice similar? Is really important if you take this model polynomial. You're saying take some polynomial approximators to the softmax. Yeah, the simplest article is take every integers from log power. Take every integers from large power and then normalize it. So it was getting probability back into the property that you want to lose large. So I think the issue might be the normalizing aspect, because you would run into a rational function. It seems a little tricky to me how to get handled. Either the TZ or the camera. Yeah, yeah. I mean, I should say, ultimately, we're not using too much about the softmax, except that it is nice and lift shits, ultimately. So it's really this concept. It's really this concentration of the conditional expectations that we're exploiting, and that I expect should work for more general kinds of attention mechanisms. Yeah, but it is a good question whether for polynomial-ish attention mechanisms, any of the theory becomes clear. Yeah, because right now it's a little messy. Could you press turn on the recording button? Yeah, if I uh understand which one it is something about the red button. Uh that's the one, yeah. Yeah, and then you