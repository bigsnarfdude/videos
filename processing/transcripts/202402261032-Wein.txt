To be here. So, this will be about, I'll give kind of an overview of two different joint works. So, okay, just to set the stage a little bit, let me talk about the Plantec-Clique problem, which maybe some of you are familiar with. Let's imagine I give you a random graph, so n vertices, each edge occurs with probability one-half, but then I've also planted a clique, meaning I've taken these k vertices and connected them all to each other. And connected them all to each other. Your job is to find this clique in the graph. And this problem is kind of famous for being maybe the canonical example of a statistical computational gap in the sense that we have this picture. Okay, so as the size of the clique gets larger, the problem goes from statistically impossible. On one end, it's sort of easy, meaning there exists a polynomial time algorithm that provably works with high probability. Provably works with type probability. But then there's the sort of middle region where, statistically speaking, you can solve it by brute force search, but we don't know any fast or any polynomial type output. Or in other words, statistical and computational thresholds are different. And as we know, this is just one example out of many. And so I want to talk about this hard regime. How do we actually convince ourselves that it's hard? Or might there just actually be a better algorithm out there? Actually, be a better algorithm out there waiting to be discovered. So, you know, we don't have so much in the way of average case complexity theory. The existing approaches are usually kind of average case reductions relating these statistical problems to each other or showing that particular classes of algorithms can't do it. And so I want to focus today on the particular framework. So I'm going to talk about low-degree polynomial algorithms, so some restricted classes. Polynomial algorithms, so some restricted class of algorithms. And I will talk about the hypothesis testing question, which is maybe the simplest version of this that you can imagine. This is just like distinguishing is there a pledge fleet versus is there not. I'm also excited about applying this low degree framework to like other types of problems like recovery or estimation and so on, but I won't say about this in this talk. Okay, so let me give you a brief overview of what is. Give you a brief overview of what is this low-degree polynomial algorithms or framework. So let's imagine we have these two distributions on random graphs, right? The Q or the null distribution is just purely random. The P or the planted distribution is where there's a planted clique. And we're going to talk about a polynomial degree D test that tries to tell these apart. It's a multi- Tries to tell these apart. It's a multivariate polynomial of some degree, and its input is going to be kind of the whole graph expressed as the adjacency matrix. So this is like a multivariate polynomial that has n choose two different input variables, and those are either each 0 or 1, telling you whether the edge is there or not. And the output will just be some number. And so, what does it mean for such a thing? Or, first of all, some examples. So, examples of low-degree. So, examples of low-degree tests could be things like counting the total edges in the graph. That would be a degree one polynomial. Counting the triangles in the graph would be a degree three polynomial. You can count other subgraphs and do some various spectral arguments and some other things. What does it mean for such a test to succeed, to actually tell these two distributions apart? Well, this is kind of the definition that we tend to use for this. Use for this. So, you know, here's kind of in math terms what it means for a polynomial to tell these two aparts. But in pictures, it kind of looks like this: where if I take this distribution and feed it to my polynomial, I'll get some distribution of real-valued outputs. If I take the other distribution and feed it to my polynomial, I'll get a different distribution of real-valued outputs. And I just want these to kind of be well-separated so I can threshold them in the middle. And specifically, we're going to kind of ask it to be separated in terms of the. It to be separated in terms of the first two moments. So the difference between the means needs to be larger in order than either of the standard convictions. Okay with this. So this framework has by now been applied to a bunch of problems. For instance, Fred mentioned beta-beta in the last talk and kind of is a way we can. Of as a way we can prove kind of these concrete lower bounds about these statistical problems. In this talk, I want to ask a couple of questions that are somehow more fine-grained than the standard approaches. So the first one is runtime. So let me first tell you kind of the conventional wisdom or the traditional approach to this. So we tend to use this heuristic that a degree D polynomial or the class of degree D polynomials is roughly. Or the class of degree polynomials is roughly as powerful as algorithms of runtime, like n to the d, where n is the problem size, which is kind of roughly the number of monomials or number of terms in such a polynomial. Or more specifically, we kind of like to informally say that the class of polytime algorithms is like more powerful than constant degree polynomials, but less powerful than logarithmic degree polynomials. Because a constant degree polynomial. Because a constant-degree polynomial can be computed in polytime, whereas log-degree polynomials can capture various spectral methods and things, and these kind of tend to be the best algorithms we know for these types of problems. And more, if I also care about runtimes that are bigger than polynomial, I care about something like e to the power of something, you know, there's also a correspondence between degree and runtime. A correspondence between degree and runtime there. And so, informally, we speak sometimes of this kind of low-degree conjecture, saying that if low-degree polynomials fail to solve something, then algorithms of the corresponding runtime should also fail. You know, this is, of course, kind of a conjecture, and it's a bit problem-specific, and there are some caveats. Another way to think about it is just that we're proving unconditional lower bounds against specific class of algorithms. Class of algorithms. Okay, so the question that I want to kind of ask in this talk, that I think has not really been asked before, is about whether we can differentiate some kind of more fine-grained time complexities. For instance, order n versus order n squared. And can we pose such a question using low-degree polynomials? Okay, so that will be sort of part one of the talk. Part two is about the testing error. So, this strong separation is the condition I mentioned earlier about how, you know, if your polynomial is telling, is kind of separating the two distributions, this implies that by thresholding the polynomial, you can distinguish with high probability these two things in the sense that both your type one and your type two errors will go to zero. There's also a corresponding notion of weak separation, where you put a big O instead of a small O. Where you put a big O instead of a small O, and this implies sort of that you can do non-trivial hypothesis testing. So you can tell these two parts better than just a random guess. And so the heuristic we tend to use is that, okay, if we're able to prove that low-degree polynomials fail to achieve one of these two notions, then we sort of should conjecture that fast algorithms fail to solve the associated detection task. Detection task. This is sort of the conventional wisdom. And the new question I want to ask today is something more fine-grained: is can we actually identify the exact optimal trade-off between some of these things, between type 1 versus type 2 error, in a regime where weak detection but not strong would be possible. So, you know, we're going to be able to tell these two things apart with some sort of constant probability. Some sort of constant probability, and I want to nail exactly what is that constant. Makes sense so far? Okay, good. So let me do the first part then. So I want to return to the planted clique problem, and let's think about the distinguishing version. You know, is there a clique versus not? Is that a question? My question is: what is type 1 and type 2? What is type 1 and type 2 and what are type 1 and type 2 is? Oh, yeah, so it doesn't really matter which one is which, and I don't remember myself. But yeah, like, right, you have two distributions, I guess you have a null versus, yeah, so one of them is false positives and one of them is false negatives. And, you know, I don't know which is which, but yeah, but I don't know either. Either. Yeah, thanks. Good. Other questions? So let's think about Pletit clique, but now in the easy regime. So the clique is going to be larger than root n. So we do know polynomial time algorithms to find it. But let's ask exactly what is the runtime that we need. So sort of the naive methods would be something like either count the total edges in the graph. Be something like either count the total edges in the graph or count the look at the maximum degree vertices. And naively speaking, this kind of would need runtime like n squared because you have to read the whole input in order to do this, right? By the way, I'm imagining that I'm really like kind of far above the threshold. I'm n to the power of half plus epsilon. So even these kind of naive things like counting total degree is going to work. You can't, there is. You can't, there is a way to do better than this. You can kind of do maximum degree, but you can do it in a slightly clever way where you don't end up reading the whole graph. You only need to pick some set of vertices to estimate their degrees, and you estimate their degrees by only looking at some of their neighbors. Okay, so you can make this exponent a little bit better than two, and it depends on sort of this delta. It depends on how far above the threshold you are. So if you're more above the threshold, you can have. If you're more above the threshold, you can have a better runtime. And the main question I will ask here is: is this optimal? Can we do better than this? Or if not, how can we kind of argue that you can't? How do we even approach this? If we go back to kind of the conventional wisdom, we might hope that there's going to be some degree of polynomial that corresponds to order n time, and maybe a different degree of polynomial that corresponds to O. Degree polynomial that corresponds to n squared time or something like this, right? But I think this is kind of not going to work because even a degree of one polynomial can read the whole graph, right? It can count the total edges. We have to take a sort of different approach. And the approach is to notice that the bottleneck in this case seems to be actually just reading the input. So we will think about a model where you are explicitly limited to only reading some of the input. And this has, in fact, been kind of And this has, in fact, been kind of studied before from a more information theoretic perspective. So, this will be non-adaptive edge query model, which means I'm going to think about algorithms of the following form. They upfront, they have to choose which entries of the adjacency matrix they're going to look at and which ones they will not look at. And we call this mask. So, the mask is only going to depend on the problem size. This is something you have to pick before you see. You have to pick before you see any of the input. That's why it's not adaptive. And then once you've decided which edges you're going to view, now the planted clique instance is generated, and you get to see the presence or absence of only the edges that you've chosen to observe, right? Or to query. Okay, and then we can ask: given that limited information, can you tell is there a blended leak or not? Is there a point to click or not? And there's a few different variations of that question, right? I could maybe give you unbounded runtime to do that second step, or I could restrict you to some restricted class of algorithms. Okay, so the main result here is going to be that if you restrict your, if you do it as non-adaptive kind of queries, followed by a low degree polynomial test, where low degree means like, well, or log n for me. Means like order log n for me, then you actually have to observe this many bits of the input, right, which exactly matches sort of the best known runtime, right? So this is kind of like saying if you want to improve the best known runtime, you cannot do it within this class of non-adaptive low-degree ones. Right, because those ones have to look at this many inputs, yeah. So you can just say that. Anyway, yeah. So you can just say that like the bound uh f to three uh better zero than three bars, which seems to be basically good. Uh good, so delta is strictly bigger than zero here. Delta is a constant bigger than zero. It's much better than the best approach gets weird, for example. Yeah, this is a good question. Like, what happens right at that threshold? It seems potentially like maybe it kind of jumps. Like, maybe it kind of jumps when you get close that if you're really at a constant times root n clique size, maybe as far as we know, you actually need n squared time there. But if you're at n to the half plus a little bit, you basically need like n to the three halves, actually. So is there a constant depending on the delta in front of that sort? A constant that tends to infinity has delta infinity. It tends to infinity as delta goes to sign. Oh, yeah, something like this must be the case. Yeah, I'm kind of hiding in kind of, I'm treating delta as a constant here and hiding that difference. Yeah, if you allow not a low degree test, you could create where it's just. Yeah, you can do better with... Yeah, so I'll get to that on the next slide, but yeah, great. So the information theoretic threshold will be different, yeah. Good. So here's a more formal version of the theorem, which I won't really spend too much time on, but you'll. Time on, but you know, if you're there's a positive result saying, you know, there is some mask of this size that kind of works and lets the low-degree test succeed, but on the hardness side, like no matter what mask you pick, you're not going to be able to do it if you're below the threshold. Okay, so to put things into sort of a broader context, here's like kind of this phase diagram where I'm looking on this axis, this On this axis, this gamma is parametrizing the size of the mass, like how many entries you get to view. The y-axis is parametrizing the clique size, so like how easy the problem is getting. Okay, and so the green region is easy in the sense that we know this kind of degree sub-sampling algorithm. And in the green region, we know sort of an algorithm whose runtime matches the number of queries, basically. So in the green region, you kind of think. Basically. So, in the green region, you kind of think of query complexity and runtime as being kind of synonymous. Okay? And so, back to John's point: you know, maybe the first thing you would try if you wanted to show this is optimal is to just ask like information theoretically, how many entries do I need to observe? If I have to pick a mask, and then I'm allowed to do any test I like, maybe you would hope that sort of matches this, but actually, that threshold is different than the one we'll end up getting. One will end up getting. So, the information theoretically, the best thing to do is to kind of query a small subgraph and then do a brute force search to find a large clique inside that. Okay, and so our result is kind of this hardness part. I guess if you're below this line, the hardness was known already because this is just the usual planted clique hardness, right? That even if you observe the whole graph, low-degree polynomials can't do it. And so, our new result is kind of filling in. And so, our new result is kind of filling in this last triangle up here. And so, probably one of the biggest open questions here is, does adaptivity help? Everything I'm doing here is restricting myself to these non-adaptive algorithms where you have to choose upfront what things to observe. Potentially, adaptivity could be better, but it's not even clear to me how exactly to phrase this question in terms of something about Lovi-Greek polynomials and so on. About loading repo, I mean, so on. But from the formulation of the question, do you think the answer is different? Probably, for detection, probably not as far as we know. But for recovery, if you want to care about recovering the cleat, the best known algorithms are actually adaptive and need to use like a few rounds of adaptivity. So, yeah, it's hard to say for sure. Yeah, I'm saying this is like. Yeah, I mean this is something, right? I mean, given the property testing regime, by the way, the number of queries you're asking is smaller than the size of the input machine. Correct. So you could you could imagine a property testing lower bound independent upon these low degrees carved, right? Because of the thing. I mean, I think that's kind of what this red region is: is saying that, yeah, if you get to query a limited subset and then do any computation you like, this This is impossible below this line and possible above this line. So that doesn't give sort of a sharp answer to the original question I asked yet. Because you can do kind of this brute force thing. Like you can query a small subgraph and do brute force. So like one way to say it is that the query complexity is kind of one bottleneck on the runtime, but it's not necessarily the only bottleneck on the runtime. Are you going to talk about the likely not in the interest of time, but uh I hope maybe you'll ask me about it later if you're interested. So you know, here's the proof there. You have 20 minutes, we'll just see what's thinking. Okay, well, I can go back, yeah? Can I just ask a question for you? Can I just ask a question for you? Yeah, please. So, like, is the are the is the hardest, or sorry, is the best strategy to, like, pick a subset of nodes and then, like, query the entire graph once I say something? Or, like, yeah, so what we're basically going to show is that the best type of mask is one where you pick, um, I guess, two different subsets of nodes and look at the complete bipartite graph between them. They're going to be sort of of different size. So, you think of this as like approximately estimating the degrees of the ones on this side. Estimating the degrees of the ones on this side. The way roughly we're going to show it is by using this kind of standard tool, the slow degree likelihood ratio. But we are going to show that if you start with any mask, there is some operation you can do that sort of simplifies the mask and only can increase this low degree likelihood ratio thing. So we're somehow showing that you can reduce the question to masks that are very structured. But I guess I understood your question to be: do we know the best thing is a vertex mask? The best thing is a vertex mask as opposed to I would also choose at the outset a different earner in the end graph and just check to see whether those edges are present. You're talking about doing like pick a subset of vertices and query all the edges among them? Yeah, taking all vertices but picking a subset of their edges, like an error, masking. A masking the input graph with an error-training graph of lower density, but still with all the vertices. And looking at maximum degree of vertical graph. Yeah, I think that will not work as well as this one where you kind of specifically concentrate on these are the vertices whose degree I want to estimate, and I'm going to focus all my energy on those. Great. Other questions? Other questions? Um okay, so yeah, maybe let me try to do an overview of the other part in whatever limited time I have left. So I want to talk about a different problem now. The details of the problem won't be terribly important, but if you are familiar with the spiked Wigner model, you know, this is some kind of problem where either I'm giving you a random matrix. Where either I'm giving you a random matrix or I'm giving you a random matrix that kind of has this rank one spike. And the prior on the entries of this spike is going to be kind of some arbitrary thing, not depending on n. But I won't limit what it is. Okay, so this is this problem in this particular regime is a situation where weak detection is always easy, right? You can always do kind of better than random guessing about. Do kind of better than random guessing about testing these things just by looking at the trace of the matrix. Now, for strong detection, if I want to really test with high probability how to do this, well, this has kind of this usual picture we're used to. There's kind of this BBP eigenvalue transition where the leading eigenvalue will sort of let you distinguish. But below that, depending on the product, But below that, depending on the prior, we often have kind of this hard region here. And so I want to ask the question about what's the best you can do in this hard regime. So in the hard regime, we think you cannot do strong detection, but what is the best weak detection you can do? And what is the best algorithm we know for this already? It's something called linear spectral statistics. Basically, it means you take your matrix. Basically, it means you take your matrix, you compute all of its eigenvalues, you apply a particular carefully chosen function to each one of those eigenvalues, and then you add up all of these values. And this is known to achieve a particular curve, this thing we call ROC curve, which is like the trade-off between the type 1 versus type 2 error. So you can kind of see this in the picture. Yeah, so let's maybe talk, you know, give us some equation for the curve, which is not so important, but just to get you oriented about what this picture means, right? Here's your kind of false positive rate, here's your true positive rate. So for any fixed x value here, we want to be sort of as high as possible. And this dotted line here is sort of trivial. Like even if you don't look at your input, if you just output null with some fixed problem. Output null with some fixed probability and output planted with some fixed probability, these are sort of the points you will be able to achieve. This very top left. This seems like this doesn't care about your priority at all, this algorithm. That's right. This turns out to only use the eigenvalues, which don't depend on the spike. Yeah. And so it doesn't matter what the fire is. Good. So this is trivial. Perfect would be right. This is trivial, perfect, would be right in this very top corner. This is, you know, this is like strong detection. And so we should imagine that, you know, there's going to be some kind of optimal curve of things you can achieve. Or in other words, maybe think of the region under the curve as being like the achievable points, the achievable points of like type 1, type 2. And this region of achievable points is going to be concave, because if you can achieve these two points, you can also achieve. Achieve these two points, you can also achieve the line between them by doing sort of a probabilistic combination between the two types. Is that okay? And so as the signal to noise lambda gets bigger, you should expect this curve to kind of move outwards, right? So here, this 0.9 is the higher lambda value, and 0.7 is the smaller lambda value. This picture, okay? Okay, so if you're kind of in this, you know, this is not about really the regime I care about, but this is known to be information theoretically optimal in some regime when you're sort of in the impossible regime from the last slide. But the question I want to really ask is: when we're in the regime that is possible but hard for strong detection, is Is this the best possible weak detection you can do? Right? And this is really a computational question, right? Because information theoretically, you can do strong detection. You can really hit this very top corner. But I want to argue that computationally, this is actually the best possible curve you're handy on scale. Is that okay? And so basically, we're going to prove this conditional on an appropriate strengthening of this kind of low-degree conjecture. Kind of low-degree conjecture, which I'll try to state now. So, you know, this thing is what we call the low-degree likelihood ratio. This is kind of by now a standard tool being kind of used in this area. And the way it's defined is it's some measure of how well you're able to distinguish these two things. So it's like a supremum over degree. So, it's like a supremum over degree d polynomials of like having you want to have like a good large expectation under one distribution divided by sort of the size of your fluctuations in the other. So, this is some kind of ratio, and roughly speaking, if you're able to achieve a bigger ratio, you are doing better at testing between these two. Okay, and so one thing that you can prove that's really a theorem is that in the Spike-Wigner case, we know exactly the Case, we know exactly the limiting value of this thing, right? This is going to be equal to some very specific constant. So that parts of theorem. And since this is like bounded, since it remains bounded as n goes to infinity, as your problem size grows, this implies that you can't do strong separation. And so the standard conclusion you would draw from this would be that strong detection is hard. In fact, it requires exponential time because the degree here is basically. Exponential time because the degree here is basically hand. Okay? But I want to say something more than this. You know, I want to say something about precise power of weak testing, and that's going to require kind of a stronger version of the conjecture. So the conjecture I'm going to make is that if I restrict myself to fast algorithms, to f that can be computed quickly, then the best value of this ratio you can The best value of this ratio you can get is going to be exactly that same value. Right, so I've proven that low-degree polynomials can only do so well at a particular task of maximizing this thing. And based on that, I'm going to conjecture that all polytime algorithms or all fast algorithms can only do that well at the same task of maximizing this ratio. So, this I think. This, I think or hope that it is a reasonably reasonable. Sorry, I'm just like having trouble parsing this. Just the sense in which you're strengthening the typical degree injector is like you're really saying the degree D polynomial should take time and corresponds to algorithmic time and didn't. That's the sense in which this is strong. Okay, so that's something. Good. So that type of correspondence, I'm not trying to change. Yeah, the kind of heuristic correspondence. The kind of heuristic correspondence between degree D versus runtime end to the D. That I'm leaving as is. The stronger part is that I'm conjecturing something about this exact limiting value of what you can get for the ratio. It's not just about like, it's not just saying if this thing is bounded, then it's hard. It's about saying like, if flow-degree polynomials can only get this so big, then all fast algorithms can only get that same limiting value. Limiting value. Thanks. Okay. Time, I guess. So this is the optimal F here is the thing that kind of writes down the low degree Fourier coefficients. Yeah, that's right. The optimal F here is what we call the low degree likelihood ratio, which is exactly you take the likelihood ratio and you project it onto the space of low degree polynomials in a particular sense. In a particular sense, which, yeah, is a little bit of why these expressions, one minus random spray, and minus one fourth. Oh, why this particular value? I don't know if I really have any intuition for that. That's kind of just something that comes out when you calculate this, I guess. But the strong low-degree conjecture thing, okay. The one minus lambda squared thing, this is like specific to this problem, but my understanding to make sure I'm Problem. But my understanding, to make sure I'm understanding right, of this strong low-degree conjecture thing is you say for any problem you want to talk about, whatever the optimal value of this ratio is among low-degree polynomials, that's the optimal value achievable by any polynomial time computable f. Yeah, that's exactly right. That's the strong low-degree value. So yeah. And for this particular problem, that value happens to be 1 minus lambda squared, which is also minus 1/4. Yeah, right. So yeah, here's, you know, to be sort of as safe as possible, I've stated this conjecture kind of just for the Spike-Wigner model. And as you say, yeah, you could imagine. As you say, yeah, you could imagine making the analogous conjecture for whatever your other favorite problem is. And so, you know, maybe just to wrap up, the conclusion basically is what I alluded to before, that if you assume this strong low-degree conjecture, then it follows that this trade-off curve that the linear spectral statistics gets is the best possible trade-off between type 1 and type 2 area that you can get. One and two are that you can get, meaning if you like fix any point, if you pick any like alpha beta pair that lies above that curve, there is not going to be a fast algorithm that achieves that thing. So intuitively, the best degree D thing, or the first degree D thing people would think of doing is just, you know, dth power of the You know, the dth power of the matrix is looking at its trace, so looking at the dth moment of the eigenvalues. But I'm really curious what the optimal polynomial of the eigenvalues is. So let's say that the spike prior is spherically symmetric. Then the optimal low-degree polynomial will be some polynomial of the spectrum. It will be something. The spectrum, it will be something involving traces of powers of the matrix. Do we know what that optimum polynomial is? Does it depend in interesting ways on the prior? Right, so it is exactly the same as this function I've put here, right? Like when I defined linear spectral statistics, I defined it as a sum of a function applied to eigenvalues. But another way to say that is that it's the function applied to the matrix power, you know, the trace of the function of the. The function of the matrix powers. You see what I'm saying? So I think this f lambda is exactly the function you're asking about. So is it some low-degree approximation to the threshold function, some chubbyshuff-like thing? Oh, what actually is it? Yeah, good. I should have remembered that someone's asked me this question before and I forgot to go look it up. I think it's not like a threshold. It's just like something that kind of like maybe is sort of exponential, like it just kind of increases, but yeah, maybe don't. Increases, but yeah, maybe don't quote me on that. I don't exactly remember what it looks like. But yeah, it is known what exactly this thing is. But does it depend on the prior? No, it doesn't depend on the prior. Yeah, so good. So kind of a conclusion I'm making here is that if you are restricting yourself to computationally efficient tests, the best thing you can do is only to use the spectrum, is to only look at the spectrum. The spectrum does not depend on the prior. And so you are effectively. And so you are effectively, you know, the answer here doesn't depend on the prior. Now, if you're allowed to use brute force search and stuff, you can exploit the prior. But yeah. Can you recover this f lambda function from the loading v calculations? Could it be that if you posit that this is optimal, could it be that somehow you look at this and then you find a different optimal test? That's a good question. That's a good question. So, when we did this, it didn't directly come up for us. The proof, yeah, our proof kind of does not actually, as far as I can tell, really involve this. But yeah, potentially one could do something like this. I don't really know, yeah. One thing I that's I'd like to ask is that is there are there additional conditions needed to s to say that like once you have the expected value uh of the polynomial under the plan distribution? Of the polynomial under the planned distribution, that this gives you this success probability? Right. I mean, I haven't talked about the proof of this, and I don't really have time to do it. But yeah, I'm not making any, I haven't really hidden too many assumptions, you know, when I stated this theorem. There's some, okay, the spike prior, you know, assume it's like sub-Gaussian. And that's basically the only, and it doesn't depend on n, it doesn't depend on the debt. And it doesn't depend on n, it doesn't depend on the dimension n it's like fixed. That's really the only simple entries are important. Oh, yeah, right. This the hidden vector has IID entries drawn from this. It's like order one so gasing. Like you're not allowed, for example, to have a pie supported on a single port. I mean, it's like having these recordings with respect to yeah, right, because yeah, so those priors would like depend on n, would depend on the dimension. But you can have anything that doesn't depend on n, you know, within yeah. Within the reason. I think I'm well over time and have said most of what I wanted to say. Yeah, go ahead. Okay, you know, now you have this, from the strong Lodegree conjecture, it tells you that any polytemputable, you know, 0, 1 value test has some upper bounded ratio, the E. And then is the meat somehow transforming this ratio bound into this trade-off curve? That's like the. Yeah, good. Yeah, good. I'll try to say the proof in like two sentences. The proof is like: if I told you you could achieve this curve, then I would be able to kind of use those tests you gave me to achieve a certain value for the ratio. You know, there's some equation for it that depends on the curve. And so, what we're basically going to do is: okay, let's skip this part: is to say, for our problem, okay, we know this is. Okay, we know this is achievable. We have an actual algorithm that achieves this, the linear spectral statistics. Suppose for contradiction that some point above it is achievable. If you could do that, then you could also kind of achieve this whole upper convex envelope. But if you are able to achieve this, this would allow you to get a good value for the ratio, you know, better than the one that sort of corresponds to this, right? So the better value for the ratio. Right, so the better value for the ratio that sort of contradicts the one we know. That's kind of the proof. So, how much does it use that function? Are you actually proving something pretty general about detection error for a lot of the problems in the R regime based on load rebounds? Yeah, it's pretty generic. Like, it's not important what this specific function is. But what's important, yeah, so the ingredients in the proof, you need a Yeah, so the ingredients in the proof, you need a positive result showing that some curve is achievable, and you need the value of that curve to match the value of the low-degree likelihood that you've separately computed on. With those two ingredients, then you can do it. Yeah. Good. All right. Let's take the rest of the questions offline and thank Alex. Now we're going to do an excellent ice cream break for the lobby. Excellent ice cream here from the lobby. Wait, maybe I'll explain here first. So, the purpose is that we all, since we're coming from different communities, we all make sure we at least say hello to each other and introduce ourselves. So, I have here a bunch of cards that I made on the airplane last night, and they come in pairs, and you will each receive one, and you must find your pair. It may not be completely obvious what it means to be your parent. Means to be your parent. Sam will stand at that. Everyone exit through that door and Sam will be there. Exactly. Exactly. Thank you.