I I I try to keep it minimal, so but it's too minimal. Okay, so we'll talk about resolution and in particular about this question. So I will talk about just resolution or subsistence and size reverbs. And I'm interested in what are the strongest possible resolution size reverbs that we can prove. And the torx actually boils down to And the talks actually boils down to two open problems and one proof sketch. And okay. So just to be sure we are on the same page, so this is just some notation that I will use on resolution. So resolution, that's the inference rule. I will always use F to denote a CNF formula, which will always be a KCNF and in n variables. And with this W of F indicates the width needed to refute the formula resolution, and with S of F will be the size. So the subscript indicates if I'm restricting myself to either unrestricted, so all the possible resolution proofs, or just I look at just the minimum size needed over the proofs that are tree-like, or I restrict myself to the resolution proofs or refutation of f that are such that the That are such that the directly acidic graph corresponding to the proof is regular, which means that no variable is resolved twice along any path. So, these are just notation. I guess everyone is familiar with the solution. But I will use this notation a lot, so okay. Just to simply just write down steps. So, I'm interested in strong overbounds, so let's start with mild exponential overbounds. So, lower bound of the form two to the end to some. Of the form two to the n to some constant. We have a lot of examples of these, and most of them are, let's say, from combinatorial principles, visual principle, fractional principle, multilateral sensors, a lot of formulas. We are now able to prove lower bounds of this form, but we want to go further. So, okay, so let's go maybe formulas for which we can prove lower bounds of the form 2 to the number of variables divided by some constant, which is positive. And again, we have a lot of examples, but we need to. We have a lot of examples, but we need to move away from the combinatorial world somehow to some uh probabilistic, randomized or expander graphs uh constructions. So some examples are random trigger formulas or random tricksor or sighting order and expander or the graph vision principle or choose your favorite one or something. But we want to go further. So, okay, so can we really go up to the worst possible thing? So, really, like the there are there formulas for which the Are there formulas for which the worst possible refutation is really the complete binary tree? And the answer is no. And this is essentially due to the switching lemma. And so this is, we have this upper bound. So it's true that for every unsatisfiable formula in n variables, there is this upper bound on the size of three like refutation, which is two to the end, but there is some savings. I will always use the notation sigma k to denote some savings. Sigma k to then all some savings, and these savings will always go to zero as k goes to infinity, and this is also the asymptotic of this savings. So, this is a result we proved with a bit, but I guess maybe it was already for claw or something. Okay, so how this proof looks like, so there is, they look like, so the picture is like that. So, there is, here is the false, I brought it like top down, so there is the like the binary decision tree or something. So, it's a complete binary decision tree over a set. It's a complete binary disjoint tree over a set of variables we call the set S. And this set has size N minus L, where L is N over 14K. Of course, here I'm optimizing over the parameters. And the problem is that when you reach the leaves of these complete binary zone G over these variables, for most of these leaves, then you need to query not up to the worst case possible L many variables, but only up to L over 2 for most of them. Up to n over 2 for most of them, except for a fraction of n over 2 of them, in which you might go all the way up to the worst possible depth. And then you count how many you so have, and that's the size upper bound. This is the upper bound, and so, okay, can we match this upper bound? So, can we show that there are similar formulas in n variables such that we have this lower bound in resolution? So, this is open. Solution, so this is open, and we are not really focusing on the asymptotics, but of course, here is two questions in one. So, the asymptotic is that tight or something. So far, I mean, it's also already open if the we don't care about this sigma k constant zero. So, the ACATH from the title is because of this connection that if you prove this open problem, then you show that no CDCL solver can refute the strongest potential time hypothesis. And this is because And this is because, as it was pointed out in, I guess, Paul Beam tutorial, it was recalled that resolution refutations are equivalent to the execution traces of CDCL solvers over unsatisfiable formulas, modulus of some assumptions. So, this is my first often problem. And then there is a second one, which is actually, if you look at the literature, sometimes, so this is the Benza-So-Bittgerston inequality, right? Not really, because Right? Not really, because usually, I mean, there should be an omega here to get the Benson-Sov-Wittgerson inequality. And sometimes people actually, when they cite this Benson-some-Wittgerson inequality, they forget about the omega. But I saw it more than once, but it's not true. I mean, there is a constant hidden in the Benson-Som-Witterson inequality, and it's open to prove it with a constant that is actually one. And in particular, as we will see later. As we will see later, this open problem two implies open problem one. So it might be more difficult in principle, but so it's not a big, I mean, I don't feel these open problems are super big, I feel they should be attackable, they should be doable. But yeah, I mean, I mean, it's just an improvement of a constant factor, right? I mean, if you refute, if you refute or Refuge? If reflector can prove one, then it's very big. I mean, um, well, as long as you don't know, and what does this happen? I mean, I'm going, I mean, I mean, the first one is not reputing a pH processor, at least. No, no, no, no. Okay, so okay, before going to why we should kind of believe this all. We should kind of believe this in problem one. I want to see why we fail to prove it with some standard techniques or formulas. Next slide. So like what are the candidates of these formulas? So random CNF, they are not a candidate because there is an upper bound which is of the former over some constant. Random XR is not a candidate because also there is an upper bound. Candidate because also there's an upper bound. And also, sighting formulas, normal sighting formulas, then there is also an upper bound. And that's a recent paper by Navid Roder and someone starting with. And okay, so we need to go further this kind of usual kind of expansion properties over expander graphs. Maybe sighting over hypergraphs could. Maybe citing over hypergraphs could be a candidate, but I will show some more reasonable candidates in a moment. Okay, why we cannot just use the size-width inequality? I said it would be because of this loss in the constant. So, even if you are able to prove a realistically high lower bound of width, which will be like all the variables will be n, that's not enough because of this constant that is killing everything. So, the constant Everything. So, there's the constant in the Benzo-Som-Biderson inequality smaller than 1 over 5. Last time I tried to calculate it. So, in theory, you could just improve it by a constant factor, and then you are done. Or you can also use maybe some gadget if you are gadget-oriented. And also here you have a kind of a lifting thing. Okay, if you have a very strong width lover bound, then you have a strong size lover bound, but again, there is a constant loss. Again, there is a constant loss. So maybe one could also think of improving this kind of relationship, maybe for a different gadget which is not parity, some other gadgets. The omega depends on the two, obviously. Well, I mean, if you have later I will use a different gadget with L, like a larger number of pi. Yeah, it depends on this L. Yes, yes, the omega depends on that. Yes, sir. So this is somehow such Yes. So this somehow suggests, okay, something more m tricky is going on, so we might need something extra. So what's known? So this was, so the F here for every of these is like a different formula. So the first lower bound for three like was proven by Pudilak and Paliazzo in 2000. I also write the asymptotic of the savings, just what it got. Okay, so for tree-like, the formula is citing over some hypergraphs, and they need to do it's not just applying the benzos for the tree-like, they need to do some work, some non-trivial work. But this result will follow trivially from a couple of slides because of new techniques. Then in 15, back in Paliazzo, they proved it for regular, some similar thing with electrosymptotics, with Navi. Symptotics. With Navid in 2016, we improved it and simplified this argument for some system that I will define in a moment, which is intermediate between regular and full resolution. And what happens for full resolution? We have there are formulas for which we have this type of lower bound, which is, so this is the constant that we inform. So it's just that we need to improve it of a factor of two, some control. I guess it's kind of believable that we should be able to. It's kind of believable that we should be able to do it, and we have a lot of techniques in resolution, so this is a good playground to test maybe some techniques. Okay, so this is the width-lover bound that is needed, and it's a strong width-lover bound. This was proven by Mackin Palazzo. So, it's almost all the variables. So, the width is n times 1 minus sigma k. With a width, we can prove it through some slightly better asymptotics, the savings. So, thanks to these. Fix the savings. So, thanks to this width-reward bound, then the second open problem on improving the constant in the size-width inequality implies the size-reward bound, which was the open problem one. And also the result for three-like resolution will follow immediately because of the Benson-Wid-Gerson for three-like, which is just there is no constant there, it's like two to the width minus the initial width. So, the next idea I want to So the next slide I want to to say how this psi n is constructed. It's not easy, it's not needed for what follows, but uh since it's not super well known, maybe it might be interesting to refresh how these formulas are constructed. So if you don't care, wait for one slide and we will be back. Okay, so these formulas, we take a prime, p, sufficiently large, and now we want to construct a system of linear equations. And now we want to construct a system of linear equations of mod p variables, m variables with this following property. And this system of linear equations will be constructed through a random process in a kind of standard way. And we want that each equation as a random number of variables appearing in it with non-zero results. Every large enough subset of equation is unsatisfiable, and this large means linear in M over some P. And we want some expansion property. And we want some expansion property, which means that for every intermediate set of equations of size intermediate, every linear combination of this equation must contain a lot of variable mention. And now this is R mod P variables. We translate it to Boolean variables and we do it in a redundant way. So in making polynomials, we do it with P squared Boolean variables. Of course, you need P Boolean variables or even log P Boolean variables to describe log p Boolean variables to describe a variable mod p, but you need this lag to your argument, so you really want some redundant encoding. And you encode it as a CNF using these many volume variables for each mod P variable and you get a CNF which is a K CNF with Kp to the four and these many variables. And with Navid where we give a less manual construction and we've improved on the on the constants. On the constant. Right, and then once we have this thing, then the Witt-Rober bound is kind of standard technique from the progress measure medium complexity type of argument in which the axiom will have small complexity, the conclusion will have large complexity, and then there must be a clause of intermediate complexity, and that clause of intermediate complexity must have large weak. And yeah, so this is kind of. Yeah, so this is kind of the structure of the argument to prove the Witt-Rover bound, and this is the formula. Right, so I kind of believe that this formula is already hard, it's already giving an example for which we have a strong lower bound, but we were not able to prove it for a solution. So, this might be a candidate formula to look at, or we can make it even harder using some gadget. And so, I want to. And so I want to talk how to to prove this strong seismic bounds like in a black box way somehow. So this is the result we want to prove. Now we just want to a formula which has a large width of a band, we don't really care how the formula is constructed. And just from some analysis of some gadget thingy, we will be able to prove this all around for reason. We will be able to prove this all around for regular resolution. And also for some slightly stronger system, which is delta regular. And delta regular means that it's kind of like regular, but in every part you are allowed to resolve a set of variables multiple times, and this set must have size at most delta n. And it doesn't have to be the same set in every part, it can be a different set, but it just doesn't have to be too large. And the syntotic of delta is also going to. asymptotic of delta is also going to zero as k goes to infinity. So the proof of these results follows from the Whitlower bound and this result and this lemma. So this lemma is completely unaware of what is the formula that we are feeding it, which is completely generic. So f is a case in f in a variables. Case in F in variables, we have a width-logger bound, doesn't have to be very close to n, it's just some w. And then we are able to prove the size in regular solution of this formula. So this plus XOR L means that every Boolean variable of F is substituted with the XOR of L uh variables and um and that's it. And then you you expand it as a CNF again and then you get this formula. Again, and then you get this formula. So we are able to prove this lower bound of the size over boundaries from 2 to the WL times 1 minus epsilon, epsilon will be small. So from this expression, it's easy to see that L constant will not work, you will need L growing with K. And yes. So how this proof goes? So Proof goes. So, this is kind of the proof kind of sketch. And so, you assume that the width is large, and then you have a regular refutation of this formula. And now you want to show that for every assignment in the variable of the formula as NL variables, so for every total assignments, there exists some clause in the proof mentioned. In the proof, mentioning at least W full blocks of variables of this formula. And the negation of the clause, which is an assignment, disagrees with W on at most W variables. Like in every block, they disagree on at most one variable. And this user regularity. Polarity and then it's just the counting at. So why is one true? So this views the gain characterization of size and width. So the Azerias-Dalmau family and the Budlock characterizations of size. So since we have a width-rubber boundary, we know that there exists some Azerias-Dalmau family. And now this Atzerias-Daldal-Dalmau family can be The Mao family can be seen as a winning strategy for some game on the formula F. And now, using this strategy, which we don't know any structure because F is completely generic, we build many strategies against the formula F surfied, and each of them somehow is trying to be as close as a given beta as possible. So whenever you are queried the variable in the game for this larger formula, you try to always answer according to beta, unless this is a stupid choice. Maybe that's the last variable of one block, so you're fixing the value of some parity. And now you look at what was the strategy in the logar formula in F, and the strategy will set you, okay, that parity must be set to one for some reason, so you set according to the last. You set according to the last variable to get the parity right. And by regularity, then you disagree with beta in every block, only in one value. Of course, this gets more complicated if you allow too irregularity, but if the regularity is not too large, then it can be done. And yeah, so that's the So that's the that's the argument. I have a slide m on uh the on the games, but I guess um people are familiar s with the games or otherwise I um I think I'm kind of done. What I would um say more are okay, so this argument is seems to be kind of so it's kind of an easy argument, so this is kind of suggesting, okay, there is a lot of room for improvement, so For improvements, so maybe just some better analysis or counting intermediate clauses towards a large clause of large width could help, something like that. Or even maybe some of these hardness, how is called hardness for condensation kind of thing might help because maybe you are saving variables or maybe use a different gadget so that there is a lot of Use a different gadget, so that there is a lot of freedom to choose to construct your hard formula. This part is just a possible choice. And yeah, so this is what I wanted to talk. And it's allowing, it's trying to encourage people to maybe spend a couple of days thinking more about this problem, which should be attackable, should be open. And um so I think it's a good test for uh some techniques that we have for the solution. And We have for the solution. So I forgot to say how we constructed the formula. So the formula in this theorem is not the formula for which we have the large width. It's the solidified version of that. And this solidification is of these many parts. In Pagliazzo, they use directly the Paliatzo they use directly the formula psi n, but and they get a similar uh lower bounds on size. And uh and yes, that's it. So these are the relevant papers if you are interested in the Becking Paliatzo paper. There is no journal version of that paper, there is only the stock version of that paper. And um th the paper is uh nice, but there are some typos in some places which makes the reading not so Which makes the reading not so trivial sometimes. So, if you're interested, maybe these other two papers are easier to read than the original one. So, there we go. Right, so the the kind of savings that you have are kind of the typical savings that you get that you have in algorithm for for K sales. They usually involve one over K the savings. At PTSD, this kind of savings. But in this recent paper by Navid, I think it's kind of su suggested that maybe you can get suggested that maybe you can get have better settings for the solution. Like maybe not k, maybe k squared or something. I mean I think there is some intuition. The settings very fine. I think that would be very good because it would show the difference between but that would mean improving on the switching lamm adjusting this particular random picking uh algorithms for random picking it will be better. Another open problem that I forgot to mention is for instance also the grid over bounds for in polynomial calculus which are very close to n, that's also interesting. That's also interesting. And someone here I know is working. Like this kind of the width lower bound. So instead of having this width, having the degree, and maybe some different formula, something very close to the number of variables, so you know, this the PPA SZ algorithm, can it be interpreted? The algorithm, can it be interpreted as a resolution? I mean, it's not a complete algorithm in the sense that it doesn't find a satisfying assessment, it starts over. But can it be completed possibly? And another question is uh your upper bound via switching mana. We just apply naive one this tree and then we pop it so So because it's just as direct, we could ask: does this thing have overlaps? And have you analyzed whether terminating to attack this users all the time. Just one room before? Any more questions? Thank you very much.