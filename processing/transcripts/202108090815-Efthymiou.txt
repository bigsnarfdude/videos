So yeah, the title is Sampling Symmetric Gibbs distributions on sparse radon graphs using a contiguity. So okay, so what is a Gibbs distribution to start with? So basically this is a distribution over spin configurations on the vertice of a graph. Basically we have a graph G and a set of spin S. Our configuration is basically assignment of spins to the vertice of the graph, right? To the vertice of the graph, right? Each configuration sigma is assigned its own weight, the weight of sigma, and basically the gift distribution is chooses a configuration sigma which is with probability which is proportional to its weight. One example is POTS model, for example, where we have a graph G and the set of spin is the set of integers one up to two or two different colors if you like. We have another parameter beta which We have another parameter beta which we usually call inverse temperature. So the configuration space is basically the set of Q colorings of graph G, not necessarily proper. And the weight of configuration sigma is equal to E rise to the inverse temperature times the number of monochromatic edges that sigma induces in the graph. Notable cases of POTS model is when Q is the same. Is when q equals 2, we have the Ising model. When beta is minus infinity, we have the coloring model. Basically, the support of the Gibbs distribution includes only proper colorings, right? So, here we look about the sampling problem, right? So, we have a Gibbs distribution mu on some graph G, and we want to efficiently generate a configuration sigma which is distributed as a mu. And we know that this is a what. And we know that this is a worst case, this problem is computationally hard. So the next best thing would be to generate sigma efficiently, of course, in some distribution which is close to mu. But turns out that we cannot always do that. So then we focus on the range of the parameters of mu in which we can have a good approximation from Gibbs distribution. For this talk, particularly, we can. For this talk particularly, we care about the case where the other line graph is sparse out on graph G and M. So this is a graph on N vertices and M edges. We denote the expected degree with the letter D. And for us, D is going to be a fixed number, any fixed number bigger than one, essentially, right? So, which means that the number of edges is a linear function of the number of vertices. So, how about So, how about the sampling problem on GNM? So, still we focus on approximate sampling, so there is nothing easier here. But what makes the problem interesting is that we can use ideas and concepts from physics to get somehow better, more interesting algorithms. Okay, so a popular approach to sampling, everybody knows, I guess, Markov Seyn-Monte Carlo method. Physicists have suggested this method passing heuristics. This methods passing heuristics algorithms in many cases. We have what we call weights is an algorithm for counting and sampling. We have Barbinox abroads. There are some algorithms that use the Lobas-Local lemma. So here we don't consider any of the above, right? So it's somehow a different approach. Let's see what is the idea. So we have the input is a graph G and some Gibbs distribution u. Distribution u and initially we create this sequence of graph g0, g1, gr and one is a subgraph of the other. Particularly, if we have gi plus one, we can generate gi by going to g i plus one and removing randomly one edge. And the sequence is such that z0 is the empty graph, right? So now what we are doing is we generate a sigma zero. We generate a sigma zero, a configuration sigma zero, which is distributed according to the Gibb distribution at G0. This is an empty graph, so the GIF distribution at G0 is just a product measure, so it's very easy to generate such a configuration. And then the idea is to use sigma i, the configuration for gi, to generate efficiently sigma i plus one. So ideally, we would like sigma i and sigma i plus one to be distributed according. plus one to be distributed according to the keep distribution of gi and gi plus one respectively or somehow to be close to these distributions. So basically if we have a configuration sigma i like that what we would like to do is to apply some efficient method which we call update from now on and generate some kind of configuration schema i plus one which can be considered considered like a configuration Considered like a configuration of GI plus one. And of course, at the end, the algorithm outputs the configuration of GR. Perhaps I forgot to say that GR is the G. So basically, we output the configuration of G. So yeah, this idea is what I'm proposing here is not new, appeared back in 2012, and there was an algorithm there. It was there was an algorithm, therefore, which was specific to graph colorings, right? And there is another work, subsequent work, by Blanca and Tal, who considered the POTS model for the related radiograph. And from there, actually, we can adopt appropriately the algorithm for the ferromagnetic pulse model to have it for GNM, but not the adiferromagnetic one. So, what is common in today? One. So, what is common into this approach is that somehow the design of the algorithms, so these two algorithms are quite different with each other, and the design somehow heavily takes into consideration spatial properties of the distribution we are sampling from. So, the idea now is somehow to avoid that. Somehow, we would like an algorithm that does not apply one algorithm for any distribution. Any distribution to avoid running the algorithm again every time we have a new distribution to sample from. Essentially, what we want to do is to have an algorithm that the sampling distribution is a parameter, right? What we are going to do here is we are proposing an algorithm for symmetric distributions, symmetric Gibbs distributions. And this includes some very And this includes some very nice examples like the AZIN model or the POTS model, includes colorings, sampling random solutions of what we call the K not all equals R. Interestingly, we can use it to sample from spin class distribution like the K-spin model, right? And of course, this applies to both graphs and hypergraphs. So, okay, let's see what our approach. Okay, let's see what our approach. So, the main challenge is to define this process update, the process that gives us sigma i plus one from sigma i. So, let's see a simpler setting than the general one. Assume something distribution and let's take two fixed graphs, G and G prime, which such that G prime has an extra edge compared to G. And we assume that both graphs are. We assume that both graphs are of high Gibbs, like no short cycles there. Okay, for the sake of convenience, also assume that mu and mu prime are the Gibbs distributions of G and G prime respectively. So what we assume we have a configuration sigma, which is distributed as the Gibbs distribution at G, this new, and we want to generate efficiently want to generate efficiently a configuration tau which is distributed close to mu prime like the the the more complicated graph somehow okay so let's see what would be an ideal solution to our problem so this is g this is g prime so this is the the extra red suit highlight who is blinking your monitors so let's also call the incident vertices u and w and let's say that And let's say that this is the configuration sigma we have for G. Also, to avoid technicalities, let's assume that we also have configurations for V and W for the G prime. Since the gift distributions in the two graphs are different, we expect to have some disagreements between the configurations at V and W. So in this example, I just have one at the W, right? And this involves a special. And this involves spins blue and yellow. So, what is the idea here is basically iteratively we visit the vertices at G' and decide their configuration under tau by just looking what happens at the other side, the left side of the monitor. So, we give priorities to vertices which are next to in this iteration, we give priority to vertices that are next to disagreement and whose configuration on the left side Whose configuration on the left side are either blue or yellow. In a specific sample, these are X1 and X2. Let's say that we pick X2. Now we are going to decide the configuration at X2, but a priori I'm telling you that this is going to be one of the two colors, blue or yellow, right? The colors of the disagreement, essentially. So the choice we are making is radomized. The choice we are making is a randomized one for x2, and somehow it makes sense to try to minimize the probability of disagreement at this vertex we consider, at vertex x2. And we can do that by doing something we call a maximal coupling. So maximal coupling essentially tells you that the best thing you can do, if you want to minimize the probability by just looking at x2, is that you assign x2 color blue, thus creating a disagreement with this probability. A disagreement with this probability here, max of zero over one minus some ratio of marginals at x2. Right? So with the complementary probability, we just assign x2 yellow. So assume that we do the experiment and we decide that x2 is going to be blue. So we have a new instrument there. Then we repeat, we look at the neighbors. We look at the neighbors, blue or yellow neighbors of the disagreements, and then we decide again choose one of them and repeat as before with this maximal probabilities. And we repeat in the same way until there is a point where disagreements cannot propagate anymore. There are no vertices next to disagreement to consider. So it's possible that there are vertices whose configuration has not been decided yet. Has not been decided yet. So basically, what we do, we copy the configuration, it's going to be the same as sigma, right? So we what they have at the left side. Actually, if you haven't messed it with the UNW's, you can show that the resulting configuration, working like that, the resulting configuration is a perfect sum for mu prime. And of course, this is too good to be true. This is too good to be true for our case. And what is the cuts? The cuts is how to compute this maximal coupling probabilities, right? So, note that these probabilities involve computing these Gibbs marginals here, right? So, if you essentially, if we want to compute these maximal couples, we need to be able to compute marginals of additional marginals of Gibbs distributions efficiently. Efficiently, and this is open how to do right. So, now what is the idea? So, the idea is to somehow replace these Gibbs marginals we need to compute with some good approximations of them. And let's get some intuition before making decisions. Okay, so the marginal SAIA text tree, let's look at the specific step, is a two-copy. Step is too complicated an object, and the reason is that we have some configurations from some vertices. In specific case, we have u, w and x2, and this configuration somehow influence the distribution of, right? So, because of the get assumption, it's easy to notice that all but one vertices are far apart from the vertex we are focused. The vertex we are focusing on, in specific case, the vertex extreme. So, this could be that choosing appropriately the parameters of our distribution, essentially we can make the influence from other vertices apart from x2 somehow negligible. So essentially, we can ignore the influence of u and w in this specific case on the vertex x3. And essentially, we compute the marginal attack. Essentially, we compute the marginal at x3 using considering the graph within this dust curve, which is a very simple graph, just a single edge with some fixed assignment at x2. And it's very easy to see that dissimplified marginal can be computed in order runtime. It's very trivial to do that. So, what's now the idea? We go back to our example, and in this maximal coupling, And in this maximal coupling marginals, we substitute these marginal marginals, right? And we repeat the same experiment as before, right? So as long as we have disagreements, we continue to repeat. If the disagreements cannot propagate anymore, we just copy the configuration to the vertices that haven't decided. Decided, of course, now the cuts here. So, basically, we are not allowed to in this iteration with the we are not allowed to touch neighbors of a vertex u the vertex opposite to this agreement. And also, we are not allowed to cover all the vertices in one of these log cycles of G prime, right? Where phenomenon, then we have some kind of phase. And on, then we have some kind of failure, right? And because of failure, this update process is not an exact output, is not a new prime, but somehow is an approximate, is an approximation algorithm, right? And I don't with details, but essentially the probability of failing for the update process is an upper bound for the L1 distance. L1 distance between the output distribution of update and the ideal distribution, the distribution we'd like the output of update to have. So, and what is interesting here is that somehow we created property for the update, but I think this is a detail now. So, the sub-link algorithm, right? So, the sub-link algorithm uses update as I write here, but since update. Right here, but since update is an approximate algorithm, all the sampling algorithm is an approximate sampling algorithm, of course, right? And basically, the L distance of the output from the distribution mu can be shown that is essentially the probability of failure in some of these iterations of the algorithm. And the running time, of course, if you just see the pseudocode, is just the order of number of edges times. Order of number of edges times number of vertices. So there's nothing exciting here. Okay, so but I assumed before that this is for high-girth graphs. So graphs, typical instances of radon graph are slightly different. So we have some short cycles here and there. They are far apart from each other, which pose some extra challenges in our analysis, which I don't plan to explain here. It's due to time. And here it's due to time restrictions. I think the interesting aspect of the algorithm is for parameters of the gift distribution on G and M, do we get good approximations? By good approximation, I mean we have this L1 error, which is a verse nominal over of vertices and size of the root. So then we should. So the update doesn't change this, and this and these are somehow untypical. Assume that the distribution satisfies this gibbs that we have this on the left side slide. Assume that this is some kind of assume that we have the level H of the tree, right? So basically, we want to compare the Gibbs margin at the root with or with the end with the configuration at the larger h. And particularly we care about what happens when h is large. So this limit has essentially two value or some value which is bounded away from zero. A uniqueness says that basically when this limit When this limit is zero for any configuration at level eights, somehow is equivalent to having uniqueness. So here we are using some different conditions. We don't use uniqueness. The reason why is that for many distributions we consider here, all the distributions essentially I showed you before, for many of them we don't have established uniqueness. We only have conjectures. We only have conjectures about them, right? So you cannot use them directly. Also, for hypergraphs, uniqueness seems to be restrictive. So the first one is what we influence with scar said before simplified material. So basically, what So basically what proposition is that we take this we compare the marginals at the left side, the extreme. And we want this L1 distance to be always smaller than one over T. And T is the exp of our graph. So this is a simplified version. If you have a more intricate distribution, it can You have a more intricate distribution, it can be a bit more involved. So, with this condition, you cannot really go far. So, as we are going to see, so in the analysis, to move on, we somehow apply what we call the planted trick, which essentially amounts to reconsidering the randomness of the analysis. So, the natural way of doing the analysis is following what we call the uniform model. The what we call the uniform model, where we take a radom graph G and M, then take a radom configuration sigma radi with respect to Gibbs distribution, and then analyze the possibility. Sound is breaking. Why don't we switch off our video? I don't know whether it helps or not, but because the sound is not smooth, let us try whether it improves. So I will also mute and stop my video. I can switch the network if you okay. So one example is of this student model is what we call the platinum coloring, where we have this graph here. Where we have this graph here, we take a radon coloring of the vertices, the colours are red, green, and blue, and then we take the radon graph so that the coloring is a proper one. And then we analyze the update process. Why we should do that? Why should use this model? Basically, the process is much easier to analyze. And actually, with the influence condition, The influence condition, we can show that the failure probability is very small. So, what we would like to hope here is that somehow this also implies that for a uniform model, we have a small failure probability. And this would require what we call the contiguity condition to hold between the Gibbs distribution and the teacher-student model. And contiguity, just to avoid the technical definitions, implies that basically. Technical definitions implies that basically the typical configure, the typical properties of the uniform model and the teacher-student model somehow have the same typical property. So the second condition we require is essentially the Gibbs distribution or the uniform model, if you like, and the teacher-student model are contiguous with each other. So some remarks of what we are doing. So basically, Marks of what we are doing. So basically, influence condition is a more strict condition than continuity. So another remark is that somehow the influence condition coincides with what we say gives uniqueness condition. For hypergraphs, influence condition is less restrictive than uniqueness, so this allows further hypergraphs. Let me conclude with some remarks. Conclude with some remarks. So, we present some algorithm for sampling when the underlying graph is a typical instance of GNM and K-uniform hypergraph HK of NM for any expected degree D bigger than one for any symmetric distributions and the running time is order n log n square. And the accuracy is a reverse polynomial of the size of the graph or the hypergraph. So, as far as the performance, so I didn't show details, but for the different. So detailed, but for the adipheromagnetic distributions, I version of the distributions I gave you, it outperforms any other sampling algorithm in terms of the range of the parameters it allows. And interestingly, it applies to spin glasses, and I think this is the first algorithm that samples spin glasses in a non-trivial region of the parameters. And also, it uses concepts from other areas like contiguity and some other areas. Okay, thank you. 