A few people who are coming in, so you might want to start slowly. But welcome, everybody, to the Thursday session. Alex Viaverde will talk to us about, well, you see his title on the slide, Finding Breaking Least Symmetries. And go ahead, Alex. Okay, so good morning to everyone in. morning uh to everyone in in north america and good afternoon to those of you in in europe um and uh well not sure about uh i think there was someone from australia and other parts too so so in in this talk i'm going to talk about uh structural local identifiability actually and uh some related properties the the concept of structural identifiability was already introduced by gleb in his talk yesterday Talk yesterday. I will also talk about observability, which can be seen as a generalization of structural identifiability for state variables. So basically structural identifiability refers to the possibility of inferring parameters which are constant, and observability refers to the possibility of inferring the state variables, which are time-varying, of course. And both are important concepts in dynamic modeling, and I'm going to relate them to the concept or the theory of symmetries, and in particular of least symmetries. So, the structure of the talk will be as follows. First, I would like to motivate, to provide some background on these concepts and motivate their study. Sorry. And then I will give an example here to try to show what is it, although there have been some examples yesterday already. And then I will talk about the relationship with least symmetries and I will show a way of finding the least symmetries in a model and also of transforming the model to remove those symmetries. To remove those symmetries. And then I will show some illustrative examples and we'll finish with a brief discussion and one or two open questions. So I would like to start with a disclaimer. I'm actually not an expert in least symmetries at all. I have encountered it only recently. And I found these two books to be very helpful. Very helpful. So if you're curious about this subject, I think they're a good place to start, and there are probably many others, of course. And then on the topic of structural identifiability and observability, I do know a bit more, fortunately for you. Otherwise, you would be wondering why I'm giving this talk. However, I don't know of any good book to give us a reference on this. It's true that there are. It's true that there are some very nice review papers, and some were already cited yesterday. But since this talk is specifically about structural local identifiability and observability, I think it can be useful. I thought it would be useful to cite my own recent review because it follows the same approach. It's not probably not the best review, but it's the one. Probably not the best review, but it's the one that follows this approach. Okay, so let's start with some background on these concepts. So we will consider ordinary differential equation models, which are in general non-linear. So the models are of this type, or they can have state variables which we denote. State variables which we denote by x, parameters theta, known inputs u, and we may also consider the presence of unknown inputs, w. These unknown inputs can be seen as disturbances that we know that are present in the model, but we cannot measure them, so we don't know the magnitude. And we assume that f and g are vectors of analytical functions. Vectors of analytical functions, and for the purpose of this talk, they will be assumed to be rational functions. Okay, and we may also consider specify some initial conditions. So, broadly speaking, we say that a model is observable if it is theoretically possible to infer its states or its state vector x by observing its output. By observing its output. And we say that it is structurally identifiable if it's theoretically possible to infer its parameters by observing its output. So by theoretically, I mean that we're only looking at the constraints that are placed by the model equations. So it may be theoretically possible to infer these quantities, but But maybe because we don't have enough data or the data is very noisy, maybe in practice this is not possible. But this would be what it's called practical identifiability, for example, but we're not interested in that now. Okay, so well, I mentioned structural identifiability, that's the title, but it's more specific. The title, but it's more specifically structural local identifiability, and this can be considered as a particular case of observability and it can be studied with the same methods. So, to this end, we have to extend the state vector so that it includes not only the states, but also the parameters. And in this way, we obtain a new version of the model. To simplify the presentation, we will forget about the unknown. We will forget about the unknown inputs for now. In fact, they are seldom taken into account in modeling applications and their presence complicates the expressions. But if needed, they can be considered with this approach too. And they will be considered a bit later. So, a more rigorous definition than the previous one is this one. We say that a model variable That a model variable, which can be a state or a parameter, is structurally locally identifiable or observable if there is a neighborhood in which it can be distinguished from all its neighboring values by looking at the output. That is, two different values of the variable cannot yield the same model output. This is not a global property, it doesn't hold for Property, it doesn't hold for all possible values, but only locally in a neighborhood. And of course, this is not the most powerful claim. It would be better if it was a global property, but it's still very useful. And in many cases, in many applications, it's all that we worry about. So if this condition does not hold, we say that the variable is structurally unidentifiable or unobservable. Or unobservable. Okay, so to see why these concepts are important, let's have a look at this example here. This is a model of a glucose insulin system. So this models the absorption of glucose in our blood and its evolution. So we have U here, which is a U here, which is the external input of glucose, and then G stands for the glucose concentration, I for the insulin concentration, and this beta here is the beta cell mass. And so we have these equations here, and there are five parameters in the model and three state variables. So here we can see the time evolution of the three state variables. time evolution of the three state variables for two different parameter vectors. And I'm only plotting the values of two sorry, I'm only writing the values of two parameters, which are P and S i. And we can see that for two different values of these parameters, the glucose and the beta cell mass are exactly the same. However, for insulin, However, for insulin, the two parameter vectors yield two different time courses. So this means that if we can measure glucose or even glucose and the beta cell mass, these two parameters are structurally unidentifiable. And also insulin is unobservable, while the remaining parameters in the model are structurally identifiable. So this means that, for example, So, this means that, for example, if we wanted to use this model or a similar one to try to infer the concentration of insulin, we would be making probably a huge error. Also, we wanted to use it to infer the values of these parameters, which have physical meanings because they represent some sensitivities, we would be getting them wrong. Wrong. And this has many practical implications, of course. So, these kind of models, for example, can be used in these artificial pancreas devices that are being designed right now, which are very useful if you have diabetes. And this result tells us that if you can only measure this, this wouldn't be a good model. Okay, so now let us turn our attention. Turn our attention to the concept of least symmetries. So, a least symmetry is a continuous group of transformations of the states and the parameters such that the output is unchanged for each of the parameters in the Lie group. And we are interested in symmetries here because the cause of a lack of structural identifiability or observability. Identifiability or observability can be explained by the existence of Lie symmetries in the model equations. It was shown by Yates and Evans and Chapelle that the existence of symmetries amounts to the existence of similarity transformations. And in turn, well, similarity transformations are It was a method. The similarity transformation method was an approach for studying structural identifiability, and the existence of these transformations implied that there were some structural identifiability issues. So in this talk, we will see how to find least symmetries using an approach which is based on Sat's polynomial. Which is based on Sat's polynomials. This is based on a paper from a few years ago by Merckt and co-workers. And we have recently proposed a few modifications of this approach. So I'm not going to go into many details about the basics of this. They can be found in the books or in the article. Or in the article by Merket that was cited before. We just need to know now that this is the expression of a one-parameter Lie group of transformations, and this is the parameter epsilon. And we call, we will use these etas here, which are infinitesimals, and this x, the large x, are the The large X are the infinitesimal generators, and we also have these infinitesimal transformations of the Lie group, which are of this form. So, in order to find the symmetries, we want to build these infinitesimal generators. And to this end, we will augment the state vector as we already did before. Now we're going to consider the unknown inputs too. So we're going to augment the state vector so that it includes the original states, also the model parameters, and also the unknown inputs. Okay, and then we need to consider the type of polynomial and such that we're going to use. And such that we're going to use for the infinitesimals. And this can be of different types. In our paper, we consider, following previous works, we consider three different types of polynomials. So the simplest ones are univariate infinitesimals, which are of this form. And in this, the idea of The idea of univariate infinitesimals is that a variable, an x, or an x y. So, a question has arisen in the chat that I'd just like to bring to your attention. So, when you introduced these Lie symmetries, you talked about a group, a Lie group, which is acting, which seems to be somehow attached to your system of differential equations. To your system of differential equations. So, can you say something about what that group is and what its connection is to the system of equations that you're considering? Okay, yeah, first. Well, sorry, I wasn't looking at the chat. Yep, sorry. Yeah, so yeah, so about a league groups. Basically, a league group is a group whose elements are organized Elements are organized continuously and smoothly as opposed to a discrete group where the elements are separated. So, a Lie group is a differentiable manifold. And yeah, well, I think that's the basic idea here. But I think the question was: which group, which Lee group is it that you're interested in? Group, is it that you're there one in particular that you're considering that's connected to your equations, or are you talking now just in generalities about in general yeah okay yeah I think I think it maybe I don't know maybe with an example this will be clearer sorry can I intervene here Alex you you seem Alex, you seem to be, you start with a system of equations, and then you talk about a Lie group. What is that Lie group? Perhaps I can jump in since I'm a little familiar with the method. If you have a Lie group acting on something, then its tangent space acts on it. Rather than starting with the group itself, one can start with something that looks like the tangent space of the group. Like the tangent space of a group. And that's the method that's being used here. So rather than looking at a continuous action, you're looking at the action of a vector space, which may or may not be associated with a finite dimensional group, a vector space of derivations. Does that help a little bit, Anand? Yes, yes, it does. Thank you. The point is what he's discussing right now is that. Is what he's discussing right now is actually how do we find Lie groups that leave this differential equation invariant? So that's exactly the problem Alex is currently studying. At the beginning, you do not know the Lie group. And what he's now trying, we are certain answers is trying to construct a Lie group that leaves this differential equation invariant. Okay, thank you. That we do not know the group. Well, yeah, thanks for the. Yeah, thanks for the interventions. I think they were very helpful, so you explained it better than I would probably. Okay. Shall I continue? Please do. Okay. Yeah. Okay, so I was mentioning that in order to find this infinitesimal. Find these infinitesimal generators, we are going to consider some polynomial ansats, which can be of several types. To begin with, the simplest one will be univariate infinitesimals. And the idea here is that the expression is this one, so we have this one variable, which can be a parameter or state. And the idea here. And the idea here is that each state or each variable can be involved in a symmetry that only depends on itself. And here we have some maximum degree of the polynomial and we have these coefficients, these r's, which are unknown constants that we want to determine in order to define this. This infinitesimal. So now there may be more complicated symmetries, and that is why we also consider what we call partially varied infinitesimals and multivariate infinitesimals. So in partially varied infinitesimals, each variable can be involved in symmetries that include not only itself, but also any other. Any other parameter in the model. And so this applies for the parameters, for the states, and the unknown inputs. And in a multivariate infinitesimal, this is the most general case that we consider. We consider that states can be involved in symmetries, also with other states and with parameters. Parameters can only Parameters can only be involved in symmetries with other parameters, and unknown inputs can be involved with any type of variables. Okay, so using this formulation for the infinitesimal generators, we obtain this explicit criterion for the admittance of a Lie group of transformations. So we say that our system admits one parameter. Admits one parameter Lie group of transformations if and only if these two conditions hold. And here this x comma here is the derivative of the infinitesimal generators which is obtained in this way. So by applying this condition to our system, To our system, we have these expressions here a bit more detailed. So, this expression for the dynamic equations of the M states, and these are the expressions for the outputs, for the n outputs. And we're going to assume, as I said before, that our model is rational. So, both the dynamic and Both the dynamic and the output functions are of this form. And in this way, we can transform this system of partial differential equations into a system of ordinary differential equations. And this is what we obtain. So, depending on the type of infinitesimals that we considered, we have these expressions for the univariate and the partially variate, and we have a bit And we have a bit lengthier expressions for the multivariate case. Oh, and if we specified some specific initial conditions in our model, we can include them also in these equations in this way. And we have some additional equations here. Okay, so this is not that important. Important. Okay, so now in order to obtain the transformations, we are going to reorder all our coefficients in a vector R, so a vector R and we obtain this equality. The coefficients are linear in R, so finding the symmetries is equivalent to solving a linear system of equations. We can find them by finding the kernel of this matrix C. And as a result, we obtain these vectors, R, that we replace in the expression of the infinitesimals, and we obtain the infinitesimal generators. Now, the next step is to build the expression of the Lie group with infinitesimal generators. Group with infinitesimal generators. In some cases, the transformation is given by powers of one variable. And in this case, they are called elementary transformations. And these are the typical examples. So, the simplest one is the translation like this. Then, we have the scaling, the movies, and what Movies and what we call higher-order transformations, which are excuse me, Alex, there was another question raised just on, I think, on the previous slide. You have an in star. I mean, so there's a there's an in star which appears in your expressions, and it was asked whether it has some relation to Dmax. Oh, oh, this n star here. Yes. No, no, actually, no, this depends. No, no, actually, no. This depends on the number of variables. So, it's the total number of variables that we have in the model, including the states, the parameters, and the unknown inputs. Okay, thank you very much. Thanks. Okay, so yeah, so I was mentioning that typically what apparently what the most common symmetries in models are the simple. In models, are the simplest ones, the translation and scaling, but the others also appear sometimes. Okay, so in summary, the idea is that first we choose the type of polynomial and such that we want to use. It can be univariable, partial, or multi-variable. And then we create the infinitesimal polynomials with them. We build the corresponding expressions for the states, the outputs, and the initial conditions if they are present. And we recast these expressions in this form and solve them by finding the kernel of the matrix. And then we replace the vectors that we found in the infinitesimals and we obtain the transformations. So this will be. So this will be, I think this will be clearer with some examples, but first I would like to mention the available implementations. So there are a number of software tools that we can use to find symmetries. I have found the ones listed here. There may be more. All of these were developed during the last decade and Decade and they differ a little bit in the methods that they use, but also in the language. I think maybe from a practical point of view or from the point of view of the user, this may be even the most important thing. So if, for example, if you're used to working in Python, you may want to try this one first and so on. Because I don't think so all of them use similar ideas. Similar ideas. But that said, I'd like to mention a few features of our own implementation which are not provided maybe in others. Oops, sorry. But on a second thought, on the interest of time, I think I'm going to skip this and go to the examples. Okay, so this is a very simple example which we used to provide an intuitive illustration of the methodology. So here we Of the methodology. So here we have a very, very simple chemical reaction. We have one species that is degraded, and we have these equations. We have three parameters, one parameter K, which is this rate constant, and then we have two observation parameters. So we're not directly measuring this concentration, but a function of this. And if we If we use an univariate for this, we used an univariate polynomial and we set this maximum degree to two and we obtained these infinitesimal generators, two different infinitesimal generators, which are of this form and they correspond to this transformation of the variables. So we obtain two new sets of variables. Sets of variables. In this case, all the transformations are elementary. So we have this set here and this other set here. Note that in the first one, we transform all the variables in the system, but in the second one, only two of the variables are involved. And well, this is just the output that our system prints on screen, not very interesting. Okay, this is a bit more. This is a bit more, a bit larger example. This is also a model basically of chemical reactions. This is a pharmacokinetic model. We have four states and two observations. And in this case, we find a more complicated infinitesimal generator. And also, we have rather complicated expressions for the transformed variables. Okay. Okay. And well, just a final example that I'm going to skip because it's not that interesting. Okay, so I will jump straight into the conclusions. In summary, we have seen that these symmetries, they can inform us about the lack of structural identifiability and observability, and also about its source, because they give us an idea of the relationship between. An idea of the relationship between the variables that are involved in these deficiencies. So, their study can replace other structural identifiability tests. They can be used as a way of analyzing structural identifiability, or they can be used to complement other tests. And this is done by symbolic computation. And I have illustrated the use of a tool that does this in MATLAB. It's an open source implementation. It's an open source implementation. Don't know if I mentioned, but it's included in the Strigold toolbox, which is for structural identifiability and observability analysis. And one thing that it does is that it computes the corresponding transformations automatically. I think not every software tool does it. There are other tools available in Mathematica, Python, Maple. And yeah, and I have actually Yeah, and I have actually an open question before I finish, which is that these symmetry breaking transformations, they give a new set of variables that are observable. So they fix the lack of observability or structural identifiability. So this is good in principle because we want our model to be identifiable and observable. However, the cost is that the mechanistic cost is that the mechanistic meaning of these the variables is lost and this is something very important so we want our model because generally we want it because our variables have some physical meaning and and this is in generally in general is lost so my question would be how can they be used I mean apart from from finding from detecting lack of structural identifiability can we Lack of structural identifiability, can we actually use these transformations to find a new model that is also not only observable but useful? And this is something I've been thinking about, but I don't have a question now, so maybe in the coffee break we can discuss it. And oh, yeah, and before I finish, let me know that there are other possible uses of symmetries in biological modeling. There are actually in this There are actually in this symmetry journal in which we published our paper, there are a couple of open special issues about symmetries in biological modeling. And they play a role in phenomena like, for example, in morphological development in animals or in homeostasis processes in biology in general. Or other like this. This is a very recent example that was out last week or a couple of weeks ago. last week or a couple of weeks weeks ago. And yeah, I would like to thank my this is the work of a master's student, Gema Masonis, who did all the computational work. And I'd like to thank my sponsors and thank you for your attention. Thank you very much. So there have been a few questions raised during the talk, but we have time now for a few more. For a few more. So, anyone who would like to ask a question, go ahead, unmute yourself and just raise the question if you like. May I ask a question? Go ahead. All right. So in Gleb's talk, he said that there is a notion of definability, and identifiability was equivalent to definability over the outputs. And so here definability. And so definability was defined by having automorphisms that preserve the outputs, would preserve the, was that right, would preserve the parameter in question. So what Alex is talking about, is it exactly the same thing? So from my side, first I have to admit. First, I have to admit that I could attend the first part of the talk by GLEP, but not the end. I was just checking it now, the slides. I saw this definability notion, but I couldn't read it. So I'm not exactly sure if we're talking about the same thing. Maybe Gleb has more information about this, but I cannot say. Sorry. I'm not familiar with this I wasn't familiar with this notion of definability. With this notion of definability. All right, so maybe we can discuss more during the coffee break. Yeah, sure. I mean, from what you said, it looks like it should be basically the same thing, but I'm not sure. All right. Okay, thank you. Can I ask? So, I lost your first two, three minutes of the presentation, so I don't know if you have already clarified this point, but But the unknown input variables, W, that describe the disturbance, for example, usually they are described as statistical variables, which in general are not differentiable. So do you have to assume that these W are differentiable variables to proceed with your methods? Yes, that is. Methods? Yes, that is true. We assume with this formulation that the Ws are differentiable, continuous and differentiable, and that. Yeah. Okay. Thank you. All right, then. So, oh, I see that there is a erase thing in the chat. Did I miss it? Oh, no, that's just a comment. So maybe in the interest of taking a short break before the next talk, we can cut off the discussion now and continue what's in the during the coffee break. But before we do that, let's all unmute and thank Alex for this fine talk. Thank you.