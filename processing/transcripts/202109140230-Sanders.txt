It is a pleasure for me to introduce Sam Sanders. He is one of the spiritus rectors of Grey Eminences in beginning this project which led to this nice conference. And he is in general somebody with very unorthodox ideas. So please, Sam. Thank you, Matthias. Thank you, Matthias, for the introduction. So, as you can see, this is joint work with Dak Norman, and I'll be mostly talking about the computability theory of the uncountable today. I'll mention reverse math here and there, but I thought this would be apt for this workshop on New Frontiers Improving Computation. Yeah, so. Like Norman and I, we've been looking at functions of bounded variation, and this is all very recent stuff and connecting it to the uncountability of the reals. I'll show you some interesting results about boundaries variation, like Jordan decomposition theorem. In the sense of computability theory, cleanly S1 to S9. So So now people have studied the Jordan decomposition theorem via codes. I'm thinking of Andre Nis, Keta Yokoyama, and Alex Kreutzer. And as you will see, the second order approach via codes is vastly different, as you may have come to expect. So what do you do then? And well, given that it's one to S9 can. Given that S1 to S9 computability is so different from Turing computing with codes, I thought it was time to introduce a notion of is computationally harder than for theorems of third-order arithmetic based on Turing machines. So as you well know, Turing machines, you can't cram a third-order object in there, second order only. So yeah, you have to work with codes, but as I said, that has its problems. But I'll show you. Problems, but I'll show you a way of curing computing with third-order objects. This is very new, so it may be frayed at the edges here and there. So, Turing and Kleeney, Alan Matheson Turing, Stephen Cole Kleeney. Turing made the revolutionary breakthrough of coming up with the first intuitively convincing notion of computing with real numbers. That's what Computing with real numbers, that's what the machines do. And cleaning his S12X9 extends this to computing with objects of all finite types. S12S8 is universally known to all here and to every person on the planet, Harvey Friedman might say, is just primitive recursion for higher types. You know, primitive recursion, just bump all the types up, and yeah, it's a kind of Up and yeah, it's a kind of iteration as one to s8 formalize higher-order primitive recursion. This should not scare you if you know classical recursion theory and you've read Bob Sorr's book, you know that the or one of the fundamental theorems is the recursion theorem. And that's what S9 does. S9 just essentially hard codes the recursion theorem. If you don't believe this, ask Dak Norman and he will tell you S9 is. And he will tell you S9 essentially hard codes the recursion theorem. So that's perhaps not the most natural approach because for Turing computability, the recursion theorem is derived from the SMN theorem, which is again derived from first principles. But cleaning with cleaning, he could get away with this. He said, look, we'll just hard code the recursion theorem and let us compute. Now, as nine doesn't come to the four that often. Don't come to the fore that often. Generally, if you have A computes B via S1 to S9, you can also witness this S1 to S9 result, computability result by terms of primitive recursive terms, terms of Gödel's t, maybe with a couple of extra bits and bobs. But in our world, the Norman Sanders world, if you can compute something S1 to S9, generally you can also witness it by. Generally, you can also witness it by primitive recursive terms. I will show you one pretty exception. And those of you who pray to the gods of set theory, for our purposes, or and this includes the study of ordinary math, infinite time Turing machines are too strong. They compute outright many of the interesting things we have studied. So, yeah, there's that. Since recursion theory mostly deals with non-computable objects, let's recall some of those. The canonical one, of course, being the halting problem and the Turing jump. So for a set of natural numbers X, X jump is the set of all programs E, such that the Turing machine with program E and input E holds. And input E halts, and oracle X halts after S steps with output M. So that's the Turing jump. Yeah, so program is a finite set of instructions. You can compute it as a number E. You take the input E as just a natural number. X is the oracle. That's X jump. So, well, actually, this is actually kind of third order because you map second order thing to a second order thing. To a second-order thing, that's sort of third-order. And that's why there's something called the Sturing jump functional, E, in the higher order approach. So you just take as input some F, and E of F decides whether F has a zero. This is also called Kleene's E2. It used to be called 2E. And it's, of course, discontinuous at 11111. Yeah, so that's the highest. uh yeah so that's the higher order approach and yeah cleanly uh formulated this quantifier e2 of course uh that's not the only one like zieg and pfefferman based on hilbert bernayes' new functional they introduced new k and s2 k which decides the truth of pi1k formulas e2 decides the truth of arithmetical formulas Of eric medical formulas and Siege-Pfeffermann's new k, which we call S2K, decides the truth of pi1k formulas. I'm talking about conventional comprehension here. So when I say pi1k, I mean pi1k. So only first and second order parameters are allowed. I call this conventional because the only papers I could find, there's one paper by Ulich Kulenbach in the Pfeffermann Festschrift, which deals with unconventional comprehension. And there's one unpublished paper by Saul Pfefferman that deals with unconventional comprehension. And then there's, of course, what Dach and I do. So we will always assume E2 because then we don't have to worry about how the reals are represented. And we just assume that subsets of the reals are given by third-order characteristic functions. If you have E2, all the If you have E2, all this works very smoothly. And yeah, I should say the following: S2K can, of course, decide pi1k formulas. So you have pi1k comprehension. If you take them all together, full second-order arithmetic. And that can also be had by Aklini's E3. So that's a this is a type 2 object, so third order, type 3, so fourth order. E3 takes as input. E3 takes as input some third-order object and decides if it has a zero on the bear space. And that's E3, and it gives you full second-order arithmetic in one fell swoop. It's rather discontinuous, as has been characterized by John Hartley. Yeah, in I think his PhD thesis. So, yeah, that was the introduction: computability. Introduction, computability, and all that. So let's look at some analysis. Camille Jordan, Frenchman, he was studying Dirichlet's convergence theorems for the Fourier series, and he was all, ooh, I can do better. Dirichlet's results were limited to functions that can only have finitely many points of discontinuity and some other conditions. But the notion of bounded variation is better. It is allowed to have countably many. To have countably many points of discontinuity on, say, the unit interval. That's good, that's great. And Jordan also realized, yeah, abundant variation is interesting. He proved some interesting essential properties. And to paraphrase André Nis, this is ordinary math, if ever there was such a thing. Because, yeah, well, it's like slightly outside of the continuous, you know, not finitely many points of discontinuity, but countable. Points of discontinuity, but countably many. So, yeah, what does bounded variation mean? In constructive math and reverse math, there's a distinction. So, one says that a function has bounded variation on the interval AB if there is one natural number such that this sum is bounded for any partition of that interval. So, yeah, this measures the So, yeah, this measures the variation, and you just bound it by some number. So, this is what you use because there's a stronger definition. You say, look, it has a variation if this supremum exists and is finite. So, yeah, this is not very suitable to do reverse math or constructive math. So, that's why they use this definition. Although, Jordan can be said to have used A2 because the A2 because the paper is sort of hand wavy, but it's not clear whether it doesn't use supremum that much. And first of all, bounded variation function, I will refer to A unless explicitly stated otherwise. However, hold your horses, you critics. None of the hardness results depend on this choice A or B. So we will prove that some operations on boundary. Operations on bounded variation functions are hard. It doesn't matter whether you use A or B, but the choice does have a difference if you want to look at the power of certain operations. So for now, just assume item A, which is used in, as I said, constructive math, reverse math, maybe even Jordan himself. But many of the results are independent of whether you use A or B. So the central theorem. So, the central theorem is the Jordan decomposition theorem. A function of bounded variation can be written as the difference of two non-decreasing functions. And this was in this paper too. G and H are what is called the Jordan decomposition of F. And let's look at the computability theoretic results of this. So, we will assume E2, as I mentioned, and in the form. E2, as I mentioned, and then the following are computationally equivalent, the following tasks. For a bounded variation function, find a Jordan decomposition. So take as input f, output g and h, which are monotone. For a bounded function, enumerate the points of discontinuity. So, yeah, a bounded variation function has countably many points of discontinuity. I want to see them. Enumerate them. Or Or, which seems more elementary for a bounded variation function, find me the supremum. I mean, it's Riemann integrable anyway, so there must be an upper bound, so there must be a supremum. These three are computationally equivalent. So finding the supremum, finding the points of discontinuity, so enumerating them, give them in a sequence. And yeah, a Jordan realizer will find you this, of course, this is fourth order. Of course, this is fourth order. Find you this monotone FNG, we'll find you the Jordan decomposition. These three are all the same. And E2 is there, among other reasons for coding, but also it can enumerate the points of discontinuity of a monotone function on an interval. Maybe you can do it less, we don't know. But so E2 can do here, and so if you look at the And so, if you look at the results by Nies, Kreutze, Yoko Yama-san, if you represent bounded variation functions via second-order codes, one way or the other, then finite iterations of the Turing jump of that code will suffice to compute a Jordan decomposition. So, yeah, that's with codes. So, it's all very Turing jump E2, Turing jump functional. So, how hard can it be? So, how hard can it be to compute this supremum or this Jordan composition in the sense of Klini's S1 to S9? And yeah, there we go again. No functional S2K can compute a Jordan realizer. S2K decides pi1k formulas and all of them together yield full second-order arithmetic. E3, which also gives you a full second-order arithmetic, can compute. Arithmetic can compute Jordan decomposition and the suprema. So, once this is what I mentioned at the beginning, finite iterations of the Turing jump will give you coded Jordan decomposition stuff. But if you work with actual third-order objects, functions of bounded variation, then it's very hard to compute in terms of comprehension functionals. So there's that. So there's that. And as I've said in previous talks, if you look at reverse math, the Jordaw decomposition is hard to prove in terms of conventional comprehension. E3 can, of course, give you this, but yeah, that gives you full second-order arithmetic. All S2Ks together don't, but they also yield full second-order arithmetic. Of course, that's conventional comprehension's fault, but nonetheless, if you believe the world is measured Believe the world is measured, hardness is measured in terms of comprehension functionals. Well, that's then you your Jordan stuff lives way up there. Right. And this hardness does not disappear if you say, well, sure, Sam, item A. Wait, let me, what was it again here? So no. So you used item A, and that's where. And that's where the hardness comes from. No, if you formulate things with item B, you get the same hardness results. So that's what I was going to write here. If you say, well, in the definition of balance variation, I want the supremum to exist, and I want this to be an input of whenever I compute something about functions with bounded variation, the supremum has to exist and has to be given. The hardness doesn't. The hardness doesn't change. That's my point. It doesn't matter whether you use item A or item B. It's all very hard to compute. There's no way around this. Put your criticism back in a box, please. So the weak definition of bounded variation, namely item A, does give you tremendous explosive power, as follows. So this is a theorem that I minted, I guess. That I minted, I guess. So here we have: so a Jordan realizer computes a Jordan decomposition from a function of bounded variation. And we add S12. That's the Susslin functional that gives you pi11 comprehension. And if you combine those two guys, you get S22, which gives you pi12 comprehension. So you can jump from pi 1, 1 comprehension to pi 1, 2 comprehension. Comprehension. All you need is this Jordan realizer. Of course, this is also true in reverse math, as I've said in during previous talks. It's very explosive, and that does depend on the fact that you use the weak definition. So I proved, yeah, I proved this. And Dach Norman said, oh, but I can do better. And he did. So you can go from E2 to E3. So from So, from essentially arithmetical comprehension to full second-order arithmetic, you can compute E3 from E2 and the Jordan realizer and a little bit of extra. A well-ordering of the unit interval. Clean ES12S9 can take anything and the kitchen sink as input, so why not a well-ordering of the unit interval? The set theorists have plenty of those lying. Have plenty of those lying around here. And so these two things, to be able to decompose a bounded variation function and a well-ordering of the unit interval, throw them together with E2, and you have E3. I think that's a beautiful result. And yeah, I mean, because it just is. Now, there is one little ugly thing. We don't know how to find. We don't know how to find a term of Gödel's t. So, here from these two things, you can go there via term of Gödel's T. However, for this theorem to get E3, you have to use S9 to use the recursion theorem. And we don't know how to do that without S9. So, there may not be a term of Gödel's t. So, but that's the only, yeah, not so pretty possible. Yeah, not so pretty power. Now, so that was bounded variation. Now we shall look at the uncountability of the reals. Cantor, as you can see there, in this false color picture, I guess, in 1874, in his first set theory paper, he proved that the reals are not countable. And you can prove, you can formulate this in various ways. What is often called Counter's theorem means that there's no surjection from... means that there's no surjection from n to the unit interval, but you could also, well, the official definition is that there is no injection or bijection from said set to n. And what is interesting, the first one, Candler's theorem, has a constructive proof with an efficient algorithm. Given a sequence of reals, there's an efficient algorithm that computes a real not in that sequence. I think it's even published in the I think it's even published in the AMS Monthly. So, yeah, you can find this on the internet. And it's constructive in the sense of Bridges, Bishop, and you can prove it in RCA0. Yeah, so that's good. Now, let's formulate some third-order stuff because, yeah, I mean, this is third-order. There's no injection for any mapping from the unit interval to n, and that's third-order. interval to n, and that's third order. So let's have some witnessing functionals. What's a canter functional? A cantor functional takes as input a set and a bijection on that, an injection on that set, and then returns a real nod in that set. So again, we have a set that is countable because there's an injection to n. And so, yeah, that set is actually quite small, it's countable. Small, it's countable. So, a counter functional gives you an element not in that cell. I mean, it's almost impossible to miss, right? A has countably many elements. There's an injection to n. So, C just has to pick an element not in A. That's a Cantor functional. And that's similar to Cantor's theorem, but without the sequence. You just say, well, here's a countable set, give me an element, none, and that's it. As here, you say, Here's a sequence. Here you say, here's a sequence. Tell me an element not in that sequence. A little bit stronger is this Nin functional. So we know that y cannot be an injection to n. So yeah, give me two x and y that are different and that map to the same natural. So yeah, there is no injection from the unit interval to n now. So all right, show me. Given this y, give me x and y. Given this y, give me x and y. A weak counter functional y is even bi-adjective. So y maps injectively and it's surjective. Again, find me some x that is not in a, and you know it's bi-jective. So yeah, this looks all very weak, but again, no s2k can compute a weak counter functional. So again, s2k decides pi1k. Again, S2K decides pi1k formulas, and e3 can compute a weakant or functional. And this is similar in reverse math. The principles NIN and MBI, they tell you basic facts about the reals. They're hard to prove in that, yeah, E3 can prove it if you assume the axiom E3 exists. Then you can prove it if you assume the axiom. You can prove it if you assume the axioms S2K exists for every k. You can't prove it. And so, why yeah, why am I telling you this? I'm telling you this because as far as we know, AFAWK, as far as we know, these are the weakest third-order theorems that are hard to prove and the objects therein hard to compute. So it's this basic stuff, really, right? I mean, the reals are uncountable, and these are the... The reals are uncountable, and these are the weakest third-order theorems that are hard to prove. The objects they're in, hard to compute. So, this weak counter-realizer seems very weak. I have a set, it's countable, there's a bijection to n. Find me an element outside of that. Apparently, that's hard to do. And that's what makes third-order arithmetic special, as opposed to, I mean, or different from second-order arithmetic. And then, yeah, here. And then, yeah, here I shall show you some very basic operations. And all these operations compute cantor realizers or weak cantor realizers. So remember, the cantor realizer, if you have a countable set, and you are given the injection and the set, all as characteristic functions, find me a real not in that set. So assuming E2 again, which is not a blip on the radar compared to E3. Dark compared to E3. If you have a bounded variation function going to Q, well, it can be an injection. So find me X and Y that F maps to the same rational. So if you have, if you say, well, is there an injection? There's not even a bounded variation injection, but that's already hard to prove because you can, or hard to compute because, well, we can't. Or hard to compute because, well, we can't realize it can be computed. So, we know that the bounded variation function only has countably many points of continuity, so discontinuity. So, I'll give you a bounded variation function. You give me one point of continuity. Can't miss almost. There's only countably many. And yeah, yet that computer can't realize. And the next one is also pretty. Suppose you're given f from the unit interval to the unit interval. F from the unit interval to the unit interval, which is Riemann integral or even bounded variation with Riemann integral zero. If the Riemann integral is zero, then the function has to be zero almost everywhere, even yeah. Only countably many exceptions if it's bounded variation. So then find me a point where f is zero. That's hard because it computes a counter realizer. And this one is pretty. And this one is pretty. Given two bounded variation functions, so that the Laplace transforms exist and are equal everywhere on the half-line. Now find me a point where they're equal. So f and g are equal, and f and g, the Laplace transforms are equal everywhere. That means that the functions have to be equal almost everywhere, or again, in this case, only countably many exceptions. Find me a point where the Exceptions, find me a point where they are equal. All these operations are hard because they compute a counter realizer. And a weak counter realizer can be computed if we restrict to the stronger supremum definition. So this is again with this definition in item A. Then we can compute cantor realizers. If you say, well, the supremum has to exist and has to be given as an input. As an input, you can still compute weak Cantor realizers, which are hard to compute. So, yeah, this is third-order arithmetic. It's different from second-order arithmetic. If you believe that second-order arithmetic is the only true way, we're sort of talked out mostly. If, however, you think, yeah, yeah, yeah, so if third-order arithmetic is the next Arithmetic is the next step up towards set theory, and we believe there is some independent mathematical truth. Yeah, then this is a problem. The third order world is so different from the second order world. So, but how are we going to bridge this world? How are we going to do that? And that's the second part. The second part is my own crazy discussion. My own crazy discovery. So please do not harass Dach Norman for that. So again, Turing versus Kleene. So there are a lot of pros and cons to Kleinier and Turing. So a con of Kleene's approach is that, well, it hard codes the recursion theorem via S9 and it has no T predicate. So the T predicate expresses that something is a computation on a Turing machine in so many steps. On a Turing machine in so many steps. It's primitive recursive. S12S9 doesn't have that, unfortunately. Now, however, if you code stuff and shove it in a Turing machine, you do change the logical and the computational properties very much. There's a huge difference to those who believe that full second-order arithmetic is huge, which I hear is most of you. So, right. Right. Yeah, PNP can be formulated, was originally formulated with Turing machines, complexity theory. I am told that the cleaning approach does not have a canonical complexity theory. That's not so great. The Turing approach, well, the countability of the reels, all the stuff I mentioned, counter realizers, etc., even in time, Turing machines would outright compute many. Would outright compute many of these things. So they're not a very suitable framework for analyzing these things. Now, is there a way, can we have the best of both worlds? Like take a little bit of cleaning, take a little bit of Turing, and we can compute with third-order objects. Perhaps as follows. So I'm going to show you a way of doing this. Doing this. So if you look at third-order arithmetic and you assume E2, you ignore all the technical details of representation, then you see that most theorems have this form. For all third-order objects, there is a second-order object such that something second-order with third-order parameters happens. So A only has quantum. So, A only has quantifiers over the Cantor space, or could even be arithmetical, bear space, or could even be arithmetical. But so, for all third-order objects, there is a second-order object, something, something. And of course, reverse math, in reverse math, you say, well, does this imply that? So, this could be the convergence theorem, monotone convergence theorem for nets. Does that imply Heine-Boyd alpha uncountable covers? Alpha uncountable covers. So you study these kinds of implications, preferably over Kuhlenbach's base theory, because that's a great framework. That's what higher-order reverse math does. Does this implication hold or not? And cleaning computability theory, you would study the following. So if you could compute x from y, could we then compute y from z? And so this data here, data is fourth order. Theta is fourth order. Theta gives you x from y. That's what it does. If I give this theta and if I have a z, can I then compute this real y? Where e theta z is a clean e s1 to s9 algorithm. As I've said, oftentimes, or in many cases, you can, in all cases, almost you can replace this by a term of Gödel's t. But the point being, I have this theta, it computes x from y. It computes x from y. Can I then compute y from z given theta and z, of course? That's what you do in higher-order recursion theory. And yeah, that's what Dach and I have been doing. So this is nothing new. So this is the canonical form of third-order theorems that Dak and I have studied. This is the kind of questions you ask in higher-order reverse math, and this is. Ask in higher-order reverse method. And this is the kind of question you ask in higher-order computability theory. So we want a version of one involving Turing machines because, yeah, this is S1 to S9. This is fourth order. This scares the hibi-jeebies out of many of you, I hear. So, yeah, so here we have it again. How do we get a special case of two? And yeah, here it is. So take it all in for a second. So you bring Z to the front. You get rid of theta. Here is a term of Gödel C. And here is a Turing machine with some index. And yeah, there you have it. Instead of theta, you have this X here for a specific Y. Specific y, and then you can compute your witness there. So let me try to explain this a little bit more. The intuitive meaning of five here, so what do I mean by this? The point is we fix this z and we bring it outside. We fix this z, we bring it outside because, yeah, well, here it is outside. And then the antecedent of 4 is way too strong. Is way too strong. So here you have for all third-order objects, theta gives you the right one. But for all third-order objects, like there's a lot of those are here. Uncountably many, some might say. So we don't need that. For one specific z, we don't need for all y a y theta y. We just need one specific instance of y, namely t of z. Of y, namely t of z, and any x. So for fixed z, we bring the z outside, and then this is total overkill. Like you never need all that. Just you replace this by one specific z, one specific y, namely t of z, and any x that's the solution. And yeah, then with some Turing computing, you can find the witness on the other side. That is. Is the idea. So bring Z outside. This becomes overkill because you only want the witness for one specific Z, for fixed Z. So you can get rid of the antecedent. You can weaken it massively. One specific T of Z and some X, and then you can compute your witness. And of course, you assume it has to exist, like this has to be a real number, right? This is, yeah. Number, right? This is yeah, yeah, this is either a real number or element of the pair space. So in this case, we would say that the pro this, I mean, in line with the nomenclature of recursion theory, solving this problem n reduces to solving that problem, n for neutral between Kleene and Turing. So you have this problem. How to compute Y from Z? Well, if you know. From Z. Well, if you know how to compute X from Y in a very specific way, you can do that. That, so this is N reduction, if you like. This theorem n reduces to that theorem if you have this, if there's a term that is primitive recursive and some Turing machine index. Yeah, so this is in reduction this formula. There is no cleaning recursion theory, there's only primitive recursion. That's yeah, as I've mentioned, positive S1 to S9 results can be replaced by primitive recursive terms. And that's, of course, that plays in the background here. But this is mostly Turing, a little bit of higher order primitive recursion. But I mean, like Kurt would have liked that, I guess. Of course, then you ask, all right, Sam, assume. You ask, all right, Sam. Assume this is transitive, by the way. Exercise. Uh, yeah, how general is this eight? I mean, sure, why not? You can define this. Maybe you can even compute with, Turing compute with third-order objects, but how general is eight? We want to know. Tell us. And so, Dak and I have about 10 papers by now, and many of our S1 to S9 and reverse math results carry over to N reduction. Carry over to n reduction. So at least for our neck of the woods, we can generally replace S1 to S9 computability by this special case. So yeah, that's nice. There are exceptions. I've showed you one. I've shown you one, this, the one with the well-ordering of the unit interval. But that's the only one we know so far. Moreover, There's such a thing called Grigliot's trick to derive E2 from discontinuous functions. And Urik Kolomak has shown how to formalize Grigliot's trick in weak system, namely RC0 omega, his base theory. And these results seem to have the right syntactic form to treat discontinuous functions. So that's nice because, yeah. Because yeah, the existence of discontinuous functions does not have this obvious form, but if you look at Gilio's trick, it has that form, and we can say something meaningful there. So that's nice. So you can treat these continuous functions and well, the kind of third-order arithmetic Dachnorman and I have looked at. And if you're being lazy, you could introduce a weak n-reduction where you allow E2 here. E2 here. And you could look, you could say, well, but what about other extensions of Good LST or primitive recursion? Yeah, well, let's take a look at that, shall we? But so we're not married to primitive recursion here. You could plug in E2 in case it's necessary. That we call weak end reduction. Well, actually, you have to use Pfeffermann's mu, which actually goes back to Hilbert and Bernice. It's in the Grundlager with the. It's in the Grund blag with the letter U, in fact. But so this has to be Pfefferman's mu, which was introduced by Fernando Ferreira in the previous talk. Yes, there has to be Pfefferman's mu, but mu and E2 are equivalent in reverse math. So there's that. Now, what is our hope and dream? Witness? Why did we introduce this N reduction? Most questions in S1 to S9 computability. In S1 to S9 computability theory cannot be answered. And so we hope that in reduction, which is a special case, that these kind of results can help settle two hard questions. I'll give you an example. ADS, ascending-descending sequence, tells you that if you have an infinite linear ordering that is countable, then there's a strictly descending or increasing sequence. Of course, the second-order arithmetic. Of course, in second-order arithmetic, everything is countable anyway. So the linear ordering is given by a sequence. Now, let's assume that it's third order. It's a set of reals, and there happens to be an injection, or even a bijection to n. That's ads3. Infinite linear ordering that is countable. Nin is still, there's no injection from the reals to n. And Counter's theorem, given a countable set, so there is an injection. An accountable set, so there's an injection to n, there is one element at least outside of a. So we conjecture that nin does not n reduce to ads3 and vice versa, and nin does not n reduce to Cantorst theorem and vice versa. So nin and Cantorst theorem are equivalent, trivially, in reverse math, but we believe that because you use the counter positive, one does not. The counterpositive one does not interred to the other. But if you say, well, a realizer for this does not compute a realizer for that, we have no idea how to prove the S1 to S9 versions of this conjecture. So S1 to S9 computability, interesting as it may be, is very general. So negative results are hard. So N reduction is a special case of S1 to S9 reduction, and it uses Turing machines. Uses Turing machines. So maybe this is easier to show. So somebody have a go at this, please. So time-wise. You have 30 minutes including. Sorry, you have 20 minutes including questions. All right. So here's another. So this is a conjecture, obviously. We conjecture that. And of course, we have to break out the Heine-Borel theorem, aka Cousin's lemma. Cousin formulated it like this, essentially. If you have a mapping from the unit interval to the positive reals, that gives you an uncountable covering of the unit interval. And well, of course, there are a finite sub-covering, so there are reals x1 to x0. There are reals x1 to x0 to xk, such that this finite sub-covering still covers the unit interval. So that's the Heine-Bodel theorem for uncountable coverings. And again, NIN is there's no injection. Counter's theorem as previous. So the problem NIN N reduces to Hbu. Hbu, of course, you have to formulate this properly, and it's all prissy because it's. This properly, and it's all prissy because it's recursion theory. But you can show that min n reduces to Hb, which is cool. And if you restrict this size to bare class 2 functions, still Cantor's theorem n reduces to HBU for bare class 2. So this is for n epsi. There's many of those. Let's restrict to bare class 2, and then you can still have Cantor's theorem. That works. Again, the details are pretty. Again, the details are prissy, but yeah, again, that's how it is. And we believe that LIN does not end reduce to HBU for Bear class 2. So yeah, that's our conjecture. But again, the S1 to S9 version, well, we have no idea how to prove this. I mean, and Dachnorman can be said to know higher-order computability theory. I would say that. I have said it. I will say it. I will say it, but he does not have an idea how to tackle s1 to s9 that yeah, a realizer for this does not compute a realizer for that. So, hence in reduction, maybe that does work. Let's hope. And I should, yeah, so these are some conjectures. And I mean, it's all very basic stuff, right? ADS, Heine-Borel, unaccountability of the reals, Cantor's theorem. This is like ordinary math, it's just that. Man, it's just that people don't like. Let me put it this way: since we're recording anyway, many people have spent a lifetime learning tag tree forcing and all that to show negative results in second order. They're not going to start learning S1 to S9 computability theory, county selection, all that. That is true for most people. But here you have a way of studying third-order arithmetic. A way of studying third-order arithmetic, the language in which mathematics is written, not coded. And you can use your Turing machines, your tag trees, and your forcing. So that is all cool. However, I should show you that there's a backbone behind all this. Like, why? Sure, but why does this all work in the first place? So I've shown you E2. E2, E3, and all that. That's higher order comprehension. However, there's another way of writing down higher order comprehension. In particular, if a given theorem or object does not give you a discontinuous function, say on Cantor space, then you can prove it from this we axiom, NFP, the neighborhood function principle. It's actually a fragment of the axiom of choice. Actually, a fragment of the axiom of choice. So, if for all there exists, then there's a choice function. That choice function is a reverse math code or a Kleene associate, and A may involve third-order parameters. So this is another empirical observation by Dachnomen and myself. If it doesn't directly imply the existence of a third-order discontinuous function, then we can prove it from NFP. In fact, because NFP. In fact, because NFP is classically equivalent to comprehension, and it comes from Brouwer's intuitionistic mathematics. Trustra and Kreisel called it special bar continuity. It's classical and it's actually equivalent to comprehension. But of course, yeah, this is a second-order object. You can feed that to Turing machines. Can feed that to Turing machines. And that's the entire point. So the third-order parameters are implicit. For every parameter, there is such a choice function. And this is the canonical form almost of n reduction. And you can just feed this into Turing machines. That's sort of where you start. And essentially, this is an alternative way of writing down comprehension. Way of writing down comprehension. And this is how the core idea. Any of these theorems, if you have a discontinuous function, you can use Gligiostric. If there's no discontinuous function, then some associate or other exists, and you can feed that into a Turing machine. And that's how you arrive at n-reduction. Arrive at n reduction. And yeah, that's your NFP, the neighborhood function principle. Yeah, so I should say thanks, of course, to our breadlords. And yeah, are there any questions? So thanks for the speaker. And question, please.