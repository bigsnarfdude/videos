Okay, so today what I'd like to talk about, and I'll give basically just like a pretty pretty broad overview of some results, is kind of a different perspective on a version of results that Chenlin talked about, which in particular is trying to come up with a framework for understanding the non-equilibrium behavior of some physical systems. Behavior of some physical systems. And so, in particular, we'll focus on particle systems today, but the type of equations that we'll see, they arise also in some aspects of fluid dynamics. We'll see at least some pictures. And yeah, the aim is, again, to go kind of beyond the equilibrium theory to describe fluctuations in these physical systems, where, at least from our perspective today, the fluctuations will be described in terms of a central limit theorem. Like a central limit theorem, or some kind of large deviations principle. And so, just to kind of set the stage of where we're going, you know, the main thing, like I said, that we'll talk about is really so-called like an interacting particle system. And so we'll focus mainly on a model that arises in statistical physics, like the so-called zero-range process. But everything I say has also been done in the case of the exclusion process that we saw in Chen Lin's talk. Lin's talk. But also, you could think of other examples. So, you have voter models, contact processes. So, if you think about this picture below, this is the so-called voter model. And so how does this kind of random process evolve? Well, you can think of each little square in this box as representing a person, and maybe the color represents some kind of belief. And so, uniformly at random, what's happening. Random, what's happening is that is that sort of a single person sort of loses their belief and takes on the belief of one of the neighbors. So maybe it's like their favorite basketball team, say. And so then, and so as these dynamics evolve, what you normally see is this kind of like clumping behavior. So now is people kind of lose their belief and they take on the beliefs of their neighbors, what you see are these kind of patches where What you see are these kind of patches where, you know, I mean, as you might expect, everyone likes the same team, for example. And so, what we would, what we'd like to understand is what is the probability or what do the kind of rare event behaviors look like in the sense that you might ask the question, maybe the purple team, what's the probability that that just no one, like in in a short amount of time, just nobody likes the purple team? Or what's the probability that maybe a huge majority of the population suddenly suddenly likes this team? Suddenly likes his team. And so, this is the sort of idea that we want to understand. So, we have this kind of dynamic fluctuation in the sense that we're looking at a space-time event. And for example, and what's the probability that you see really a large deviation away from the average. So, before we talk about these fluctuations away from the average, let's think a bit about the single particle process and then also what, like, And then, also, what does the actual average behavior of a particle process look like? And so, this is the so-called zero-range process. And it's called zero-range because particles only interact if they're sitting on the same site. And so, how does this process evolve? So, we fix a non-decreasing function from the integers to the integers, which is zero at zero, and then positive for positive integers. So, a good example to Positive integers. So a good example to keep in mind is just the case when this is the identity function. So g of k is just equal to, is equal to k. And then we have these, these, so for every k, we have like an exponentially distributed random clock with rate k. So what you're seeing here is that as k is going to infinity, the distribution of these random variables is approaching a delta function at zero. So somehow as k is going to infinity, it's extremely likely. To infinity, it's extremely likely that this clock rings very quickly in this way. So, for larger and larger k, you're getting something that's more and more concentrated at zero, so it becomes increasingly likely that this clock rings very quickly. And so, how do we evolve this process? Well, the process evolves in the following way. So, we start with some kind of initial distribution. So, imagine that what we have here is really like a family of particles. Really, like a family of particles sitting on some kind of discrete torus. So we have like a discrete torus with four sides in this case. And what we have is maybe four particles, three particles, one and two. And so to each site, we attach one of these random clocks, which is distributed according to the number of particles on each site. So if you think about the case when g of k is equal to k, then it would be most likely that Then it would be most likely that this clock rings first, but it's entirely possible that one of the other clocks rings first. So, just for example, suppose that the second clock rings first, and then say, I mean, it can be more complicated, but say with equal probability, you know, a particle either jumps to the right or to the left. And so, say we're in the event where the second clock rings first and the particle jumps to the right. And so, then we find ourselves in this configuration. In this configuration. So now we have clocks of rate, you know, in the case when G of K is K, rate 4, then clocks of rate 2. And so now we just restart the process and we do exactly the same thing. So now we update the rate of each of the clocks and we just wait for one of them to ring. Maybe the rightmost clock rings, the particle jumps to the left, so when the configuration below. And then finally, at the third step, maybe the most likely event happens where, say, the left clock rings first. Say the left clock rings first, the particle jumps to the left, but we're on the torus, so that just ends up in this position here, and we just continue this particle process. And so, what we want to ask is, okay, what happens with this process on very large scale? So, if we think about taking a very large version of this process and sort of rescaling it to unit scale, but then scaling time parabolically so that we see a lot of jumps, we have to ask ourselves, okay. See a lot of jumps, we have to ask ourselves, okay, what do we expect to see? And in this case, well, of course, you know, okay, don't worry too much about this, but the only thing I'm trying to say on the top two lines is that on average, you know, so this is like the jump kernel, P. And so all I'm trying to say is that at least for the moment, the jumps are centered. So on average, you know, so just think in your mind that like with equal probability, a particle jumps to the right or to the left. But more generally, To the left. But more generally, what you think is: okay, on average, the jump is zero. So you have this like average zero jump kernel. Okay, and so we do what I just said. So what we do is we define this process maybe on a Taurus of size n, and then we rescale, we rescale space, so we rescale space to unit scale, but then we scale time parabolically. So this allows us to see really a lot of jumps. So we have this. Really, a lot of jumps. So, we have this parabolic rescaling of the particle process. And then, okay, most likely what happens is that, okay, if we have really a lot of particles, what we would expect to see is some kind of averaging in the sense that, again, I think a good example to keep in mind is this case when the clocks are exactly distributed according to the number of particles. So, we expect to see really a lot of jumps away from regions of high concentration, you know, very few jumps from regions of low concentration. Concentration, and so the most likely event that happens would be some kind of averaging in the sense that we might start off with kind of a jagged initial profile, but then as time evolves and we see really a lot of jumps, we see some kind of like averaging behavior. And this is like the version of the Law of Large numbers in this context are what you would call a hydrodynamic limit. And it turns out this is true in the sense that, okay, here we have the parabolic, we have the empirical. We have the empirical density of the particle process. Okay, if you think about starting with initial data, which is one, this is like just putting a particle on every site. So if we're in B dimensions, they're n to the D sites. So we just normalize by 1 over n to the D to preserve the mass of the system. And now this is just saying exactly what I said. So we first define the particle process on a torus of size N. The particle process on a torus of size n. So then we rescale everything to unit scale. But then we parabolically rescale time so that we observe really a lot of jumps. And so we're rescaling time so that we observe a lot of jumps and see this kind of averaging behavior. And then this is the result: if we take, say, a continuous function on the torus and we average this function against these empirical densities, then Densities, then these averages are converging to the integral of our test function against the solution to a deterministic PDE. So this is a nonlinear diffusion equation here where phi is the so-called mean local jump rate. So this is semi-explicit in terms of the phi. So the phi here, it's determined by these jump rates, g, in this way. And so you see, so this is really like a very And so you see this is so this is really like a version of the law of large numbers in the sense that at least weakly somehow the the empirical densities of these of this particle process are converging to you know to the density determined by the solution to a deterministic PE. And we can actually see some interesting behavior in terms of this nonlinearity in the sense that if we take this model case that I suggested, so this is the case where the jump rates are just The jump rates are distributed exactly according to the number of particles. And if you compute a bit what this looks like, well, then this is actually equivalent to just looking at independent random walks on the torus. So if the jump rates are distributed exactly according to the number of particles, then really what you have are just independently diffusing particles on the torus. And what you see in this hydrodynamic limit is just the heat equation. Because in the scaling, what you're saying is just kind of. In the scaling, what you're seeing is just kind of like independent Brownian motions on the Torah. But you can also kind of define a more kind of a weird dynamic, which is that you can also define your jump rates to be just the G just to be the indicator function that there's a single particle. And so in this case, this would be like, in that case, there, this would be like saying that, okay, I'm equally likely to observe a jump from To observe a jump from any site that contains a particle. So now you see a much weirder kind of averaging effect because it doesn't matter if there's a million particles on a site or one particle. I have equal probability of seeing a jump from either of these sites. And you end up with this kind of strange looking diffusion equation here, where phi is like rho over one over one plus rho in this way. Okay, so this is like in the context. So, this is like in the context of particle systems, this would be like a law of large numbers, or at least the version of the law of large numbers. And what we see here, when we compare it to the talks of yesterday, is that, well, okay, I mean, maybe this is a bit strange, you think, if you're just seeing this, because when I look at this hydrodynamic limit, so I see convergence to the heat equation, well, this is actually exactly the same hydrodynamic limit as I saw when we were talking about this exclusion. When we were talking about this exclusion process. So, like, in terms of a law of large numbers behavior, you know, at least if I take initial data bounded between 0 and 1, this 0 range process and the exclusion process, they're the same. But of course, their behaviors are actually quite different in the sense that the zero range process really allows for the particles to sort of clump up and form concentrations. Where in the exclusion process, you know, what you're seeing is that. Process, you know, what you're seeing is that the number of particles in each site is capped at one. So if a particle tries to sort of jump on an occupied site, we just don't allow it. And so what I want to try to understand now is how do we understand, you know, so how would you, you know, how would you try to tell these two processes apart? Well, when you think about telling these two processes apart, what really kind of differentiates them is exactly Of differentiates them is exactly what I just said. You know, it's not necessarily like the mean behavior in the sense that for these two processes, the mean behavior is exactly the same, but it's really their fluctuations about the mean behavior in exactly this way, in the sense that the zero-range process really allows for these regions of concentration where the exclusion process kind of caps the particles in this way. And also, here too, you see that these. And also here too, you see that these, like in terms of their convergence to the heat equation, you see very different things, you know, in the sense that, okay, if we take this intuition from the zero-range process, on the one hand, the zero-range process is really, you know, it's defined by these independently diffusing particles, and this looks like Brownian motion, and this is something we associate with the heat equation. But also, the zero-range process has aspects that don't look like the heat equation, you know, in the sense that it doesn't satisfy maximum principle, for example, because if you start with Principle, for example, because if you start with initial data, which is one, these particles, like I said, can form a big concentration. But for the exclusion process, it's a bit different in the sense that now the exclusion process, it does look like the heat equation in some sense, because if you have a region of occupied sites, then sort of heat or energy can only escape out the boundary. And you do have this kind of maximum principle of behavior, but you have less independence. So, how do we understand now? So, how do we understand now? How do we come to understand the fluctuations of these processes about their mean behavior? And the idea is what I want to refer to, what I'll refer to in this talk is a notion of mobility, which I think in Chen Nin's talk was referred to as compressibility, but it's the same thing, just a different name. And what we want to ask ourselves is: okay, so I have this random. Okay, so I have this randomness that is like that's determining the jumps in my process. And what I said when I derived the hydrodynamic limit is that this randomness was centered. So on average, when I thought about the particle jumps, these jumps, the average jump was zero. But now what I want to say is, okay, what happens to my process? Let's say I observe some kind of fluctuation. Observe some kind of like fluctuation in my system. So instead of, so my noise does something that looks like a rare event in the sense that my noise, instead of like kind of pushing my particles like in a centered way, my noise has a big fluctuation that now forces my particles to say move to the right. And so what I'm saying here is, okay, suppose that instead of looking at a jump kernel that's centered, so on average, my jumps are to the right. So if you think about this, Are like to the right. So if you think about this, if you think about like flipping a coin, this would be saying that, okay, instead of flipping like a fair coin, what I do is I introduce like a bias. So maybe my coin is more likely to land heads than tails. So then if you keep flipping, you know, in the long term, you're going to see a drift in the direction of the heads. But now if I introduce this bias into the particle system, the parabolic rescaling doesn't really make sense. Rescaling doesn't really make sense, and it's a bit like if you think about a stochastic differential equation, if you add like a constant drift to a stochastic differential equation, and the point is that in this parabolic rescaling on very large scales, the drift just kind of dominates and takes you off to infinity. And the same thing would happen here, actually. So, if you looked at the if you didn't have a centered jump kernel, but you have this, and then you looked at the parabolic rescaling, well, then. The parabolic rescaling, well, then the process would just kind of wind around the torus in the direction of this bias. So instead of looking at the parabolic rescaling, what we do is we look at the, for this system, we look at the hyperbolic scaling, which is exactly the same, except that instead of rescaling time parabolically, we do it hyperbolically. So we see nt here instead of n squared. And then you have a very similar result in the sense that. Similar result in the sense that you again have kind of like this hydrodynamic limit result, but instead of converging to a diffusive equation, you converge to a conservation law. And so what you see is that these hyperbolically rescaled empirical densities, they converge to the solution of this kind of conservation law, where now what we say is we again see this phi of rho where this will be our notion of mobility. Will be our notion of mobility, this five row. And so, think about what's going on. What it's saying is that, okay, suppose I see like a fluctuation in my system in direction gamma. Then I ask myself, okay, what is like the net effect of the flux of my system by this gamma? Well, okay, if there are like no particles sitting here, it doesn't really matter, you know, because if I'm in a region with very few particles, then it doesn't really matter what's going on. Like, the noise can go completely crazy. On, like, the noise can go completely crazy, but there are very few particles for the noise to actually move around. So, I see, so in a region with like very, like a very low density, you know, the effect is small. But now think about what happens if I have very high density. So, you can think of five rho being equal to rho, but you could also think of like a porous media type equation where five rho was like rho squared, or rho to the tenth, or rho to the m for some very large m. And what this is saying is that, okay, And what this is saying is that, okay, depending on the behavior of this phi, which is exactly the phi we saw in the hydrodynamic limit on the previous slide, you know, the effect of this kind of fluctuation can be quite dramatic, you know, because what this is saying is that, okay, kind of somehow for very large densities, maybe the diffusive properties of my system become, you know, can become larger and larger. So five rho is just the identity. This is kind of like this. This is kind of like the situation we expect where we have these independent particles. Each particle experiences this drip gamma. So the net effect on the flux is just rho times gamma in the sense that each particle is just being independently moved in this way, gamma. But now say five rho is like rho to the tenth. But now what we're seeing is we're seeing like an enhanced diffusivity. So this is what you see in like the porous media equation, if I have very large density. So now if So now, if five of rho is like rho to the tenth, like the effect on the flux is going to be, is going to be much larger than if five of rho is just the identity somehow. So you're no longer in this independent case. And so, okay, this is what I'll define as the so-called mobility for this process. And this is where you see the difference for the diffusion process, because for the exclusion process. Because for the exclusion process, what you see is you can repeat the same computation, but then what you see in the But then what you see in the limit is really rho times one minus rho in place of five rho. And so why does this make sense for the exclusion process? Well, so on the first case, if rho is zero again, you know, there are no particles in this region. So the mobility is just zero. It doesn't, you know, the noise can do whatever it wants in some kind of region because if there are no particles to move there, it just doesn't matter. But here we see also that the mobility vanishes if rho is equal to one. If rho is equal to one. And the reason this is the case is that, okay, if all the sites are occupied in my process, so the density is exactly equal to one, these particles, they can't jump anywhere because we have this exclusion rule that says, okay, my particle can't jump to an occupied site. So once again, if I'm in a region where my density is equal to one, I can experience huge fluctuations in my noise, and it just has no effect on the system whatsoever, because the particles aren't actually allowed to move. The particles aren't actually allowed to move. And so, this is where, you know, so we saw that these two processes have the same like hydrodynamic behavior, allow of large numbers behavior. But here, we actually see a big difference. And the difference, like I said, is kind of the sensitivity of the system to fluctuations. Where in the zero range process, if the density is equal to one, it's still quite susceptible to fluctuations because these particles can really move around. But for the exclusion process, if the density is equal to one, things just can't. Equal to one, you know, things just can't move. So it has like the mobility in that case is zero. And this will be, this will be kind of like the, in the next two slides, I'll describe kind of a framework for understanding kind of rare events in particle systems where this mobility plays like an important role. And this is the beginning of so-called like macroscopic fluctuation theory, which I'll describe in the next two slides. But the idea is the following thing. So now, again, in our model, Again, in our mind, think of the zero-range process where, say, g of k is equal to k. So we're just in a situation where we're likely to see jumps from regions of high concentration and unlikely to see jumps from regions of low concentration. And okay, what we happen, we say, so say, you know, okay, when we evolve the zero-range process for a large number of particles, you know, on average, what we You know, on average, what we expect is to see some kind of averaging. The most likely event we expect to see is some kind of averaging, in the sense that here, if we're in this configuration, it's most likely that particles jump from this region and less likely from this region. But of course, it's entirely possible that we see a lot of jumps from this region. So, as this process evolves, you know, it's entirely possible that we see this kind of like concentration effect. And what you'd say is, okay, if I If I observe some kind of concentration effect like this, then this looks nothing like the solution to the heat equation. If I think about this green line as being a solution to the heat equation, well, the heat equation is just going to rapidly average this initial data. So, somehow, the hydrodynamic behavior or the kind of law of large numbers behavior is not at all capturing this kind of behavior or this kind of behavior. Of behavior or this kind of event. And so, what we'll see, and I'll try to explain this a bit on the next two slides, is that how can we understand the probability of observing some kind of concentration effect or some kind of event like this? And the idea will be that we introduce this kind of controlled equation here, where we view G as being sort of an L2-valued control, and the square root of rho in this case is the square root of the mobility. The square root of the mobility. So, more generally, you would have, like in the zero-range case, you would have like a phi of rho here and a five to the one-half of rho here. And the question we ask is: okay, if I observe this kind of event where I'm seeing a concentration of particles, then the probability of observing this event depends on the amount of energy I need from this control G in order to sort of overcome the averaging properties of the heat equation. Properties of the heat equation enforce this kind of concentration. So you should think of the control G in this case as kind of acting to, again, overcome the diffusive properties of the heat equation and enforce the kind of concentration that you're seeing here. And then what we'll see is that this particle process actually satisfies a large deviations principle with rate function determined by exactly this energy. And what I mean by that, at least a bit informally, mean by that, at least a bit informally, is that, okay, if I take the particle process at scale n, and I ask myself, okay, so the rho here is like a space-time density fluctuation. So think of rho as describing exactly this kind of space-time event where the rho starts from this initial data and then forms this kind of like concentration. And so, what we're going to see is that, is that the probability that my particle process exhibits this kind of Exhibits this kind of event, becomes exponentially small in the number of particles times the rate function i of rho, where i of rho is equal to the minimal amount of L2 energy I need from this control to sort of create this event in this way. And this will be, and so this is what we mean by like a large deviations principle, at least of it informally, which gives us just a sense of. us just a sense of measuring the probability of rare events on this kind of exponential scale. Where in this case, again, what we're asking is, you know, how much energy do I need from, like, in this controlled equation here, where again, here we're seeing the square root of the mobility from the previous slide, how much energy do I need from this G in order to overcome the averaging we'd see from the heat equation and force this kind of concentration? This kind of concentration. Yeah. Yeah. So sorry for what? Zero range process. Yeah, zero range. With mean zero, with mean zero. Yeah, yeah, with mean zero. Yeah. Yeah, without mean zero, you would see just you would you would like to see just the conservation law. You'd like to see distant conservation law. So you would see, yeah, in this way. And this, so this is, this describes like a kind of a more general idea, which is that, okay, what we're thinking about is we're in a situation where we have, say, a particle system that's observing some kind of like law of large numbers or averaging behavior. And so we're thinking, okay, just think about, so this is exactly the case we see for the zero range or exclusion process. And so we have. And so we have some kind of deterministic limit rho bar and say that this law of large numbers behavior is determined by this kind of deterministic PDE where say j of rho is this is kind of like the effective flux. So j of rho bar is the effective flux. And what we want to ask ourselves is, okay, what's the probability of observing some kind of space-time fluctuation rho? So you think of rho as being like this density fluctuation. So again, rho is like this event where we see this. Rho is like this event where we see this concentration of particles, which is determined by some kind of flux J. So this is just kind of a continuity equation, and it's just saying that, okay, the rate of change of mass in some region U is just given by the flux of J through the boundary. And in the kind of like fundamental, so this is just this kind of a survey, there have been a lot of papers on this topic. The fundamental equation in this theory is the following one, which is to say that, okay, then I expect. One, which is to say that, okay, then I expect my system to satisfy this kind of large deviations bound. So the probability that my, say, particle system looks like rho becomes exponentially small in the scale n times the rate function i of rho, where what i of rho says is, okay, I of rho just gives me the L2 energy between the distance between the sort of average or hydrodynamic flux and the flux that I actually observe. Actually, I observe. But the point is that this L2 distance is a weighted L2 distance in the sense that I weight by the inverse of the mobility. And this is kind of the essential point. And this is what, we'll see some more examples near the end where we'll see some examples. But the idea is that really, when you think about the probability of these events, the The formation of the event is affecting these probabilities. Because now, what you're thinking is: okay, is I say the concentration is forming in some region, well then the mobility of my system is also increasing. So somehow it takes less probability or less energy to move my particles around in that region. Or conversely, maybe like a vacuum region is forming. So if I want to move particles around in this region, then it really costs a lot because my mobility is somehow very. lot because my mobility is somehow very low. And this leads to basically a PDE in the sense that if you think about just rewriting the difference of the actual flux against the effective flux is the square root of the mobility times a G, then what you see is that the I of rho is exactly the L2 energy of this G. And then if you just rewrite the equation defining the density fluctuation, what that gives you is just, okay, I just add and subtract the effect. I just add and subtract the effective flux J of rho. So I see that the dt rho is equal to the hydrodynamic limit, so the Laplacian of sigma of rho. So think of this as being the heat equation, now minus the divergence of the square root of the mobility times g. So if you think about the zero range process, this is exactly this equation here that we saw in the previous slide. Whereas for the exclusion process, you would expect that the large deviations behavior is given by. Deviations behavior is given by this equation. Where, once again, you're seeing sort of the hydrodynamic behavior modified by this control term, wherein the control term is exactly the square root of mobility. And just, and this, so I'll say more about this coming at the end, but this has been made, this is true, and there's some caveats in the case of the zero-range process. So if you look at the zero-range process, you actually see that this process does satisfy a lot. Does satisfy a large deviations principle, but I'll say some more about this at the end. But the point is that this process now satisfies a large deviations principle with rate function given by the rate function here. But I'd just like to say one thing is that this is actually, now I want to say, okay, so this is kind of from the discrete side of things. And now what I'll say for the remainder of the talk is kind of a continuous analog of these results in the sense that what we'd like to do. That, what we'd like to do is to try to come up with basically like sort of a continuum approximation of these particle systems, which sort of accurately describes these systems. And the starting point for this, and I'll also write this on the next slide, is really that if you just formally ask yourself what are the large deviations, as epsilon's going to zero, this kind of stochastic PDF. Going to zero, this kind of stochastic PDE, which I'll explain more in the next couple of slides, what you see is that formally these stochastic PDEs should be somehow an accurate description of the particle system, but I'll get to this. But the point for this is just that, okay, this kind of framework of microscopic fluctuation theory is true, at least in the case of the zero range or exclusion process. And okay, so where do SPEs come into the picture? To the picture. So I won't go into great detail here, but there's kind of a complementary theory to this microscopic fluctuation theory, which is so-called fluctuating hydrodynamics. And the idea of this picture here, which are some numerics done by Alexander Donovan, is that, okay, in this picture, what you should imagine is that I have like a layer of blue water and a layer of red water. So I have two separate liquids which are capable of forming. Which are capable of forming like a homogeneous mixture. So it's not like you have water and oil, but you have two liquids that can be mixed together into some homogenous mixture. But then you start them at like an equilibrium situation. So initially, the boundary between these two liquids is flat. So from an equilibrium point of view, this is sort of like a fixed situation. But of course, Situation. But of course, when you think about the microscopic behavior of these systems, the microscopic behavior is never at equilibrium. And these two liquids exhibit some kind of thermal fluctuations at the microscopic level. And this leads to the formation of these kind of rough boundaries between this water. So the SPDs that I talk about don't exactly model this, but this is the idea. So this is what we want to understand is the formation of these kind of rough boundaries. The formation of these kind of rough boundaries are, if you like, in the zero-range process, it's kind of like saying if we start with initial data one, you know, if we're before we pass to this continuum limit, you know, the process will exhibit sort of fluctuations about this mean behavior. And the idea of this theory is that it's very closely related to macroscopic fluctuation theory in the sense that to describe events like this. To describe events like this, this theory postulates a stochastic PDE where you again take the hydrodynamic limit, but then you modify the hydrodynamic limit by a conservative noise here, whose covariance is determined again by the square root of the mobility, and in this way. So, this would be kind of the stochastic PDE predicted for the zero-range process, and then this would be the stochastic PDE predicted by the exclusion process, and the connection between. Process. And the connection between these, which I'll describe in the coming slides, is that very formally, if you take a small noise, large deviations principle for the solutions of these stochastic PDEs, then this gives you a connection with macroscopic punctuation theory from the previous slide in the sense that formally, these solutions, rho epsilon, satisfy large deviations principle with exactly the same rate function is predicted. But there are a few problems. But there are a few problems with these equations, in the sense that, as we saw in some previous talks, is that somehow these equations are extremely singular. And in the language of sort of singular SPDEs, even in one dimension, this is a supercritical equation. So these renormalization theories, even in one dimension, we don't expect them to apply. And there are even some kind of negative results in the sense that if you start this equation, That if you start this equation, there's a notion of martingale solution for this equation. And if you start this equation from like a delta mass, well, what happens is that is that the noise just completely wins. And so this is like a very strong statement of supercriticality, which is like the supercriticality in some sense is saying that if you look at this equation on small scales, then the noise just dominates the diffusion. And actually, that happens in this case, whereas if you look at Martingale's solutions with space-time white noise, The space-time white noise, you know, when you start from a delta mass, what happens is that the delta mass just evolves like a Brownian motion. So the law of the solution started from a delta mass, you know, it just remains a delta mass for all time and just evolves like a Brownian motion. And this, of course, is very different than the heat equation. You know, if you start the heat equation from a delta mass, you immediately see a ton of smoothing because the solution is just like the heat kernel. And so somehow what this is saying is that the noise just. What this is saying is that the noise just completely wins, and you see no regularization at all. I mean, you do see some kind of like dynamics and some kind of evolution, but you see no regularization. But the idea now is, and there are other reasons to introduce this. So I won't go into a great amount of detail about why you consider the type of noise you want to consider, but there are certain gradient flow formulations of the equation where a Stratonovich noise makes sense. Tonavish noise makes sense, or sometimes even different kinds of noise. But the idea that I want to point out is just that: okay, if what I'm really interested in is looking at a continuum limit of a particle system or some kind of coarse graining that you see in fluctuating hydrodynamics, then it makes sense to introduce some kind of correlated noise. And what you should think of is: okay, if I have my particle system of scale epsilon, and I think about taking a continuum approximation of my Continuum approximation of my particle system of scale epsilon. Well, then, you know, what I'm thinking about the noise enter, when I think about the noise that enters my system, the point is that if I look at scales delta, which are less than epsilon, then the noise entering my system is just constant. You know, my particle system on scales smaller than epsilon, you know, my discrete system is just constant. So somehow the noise entering is also constant. I mean, the noise entering. Also, is also constant. I mean, the noise entering the system doesn't look like white noise necessarily in space because, I mean, you do have correlations on scales smaller than epsilon. And so the equation we will consider is to avoid this supercritical behavior, is the same equation, but where we have like a space-time white noise that is still white noise in time, but has some kind of spatial correlations. And the point is that you can come up with a we'll pose. That you can come up with what pose in this theory for these types of equations. So, I mean, you can do it quite generally. But what I want to point out, just for the sake of this talk, is that when we think about an equation like the zero-range process, then you can solve these equations for any delta. So, I mean, you can allow for, say, degenerate diffusions like porous media, but also pretty irregular noise coefficients here, including the square root. Coefficients here, including the square root. So you don't show you can go beyond like a Lipschitz regime and the randomness and include terms like the square root that we saw in the Dean-Kawasaki equation. But there is one aspect of the theory that is extremely useful, which is that when we show the well-opposedness of the solution, you can actually prove that you have kind of an almost sure contraction property. In the sense that when you show uniqueness, you don't need to take. You know, you don't need to take an expectation, but actually, for almost every realization of the Brownian motion, your solutions satisfy this kind of contraction property. And this is a profit. So when you think about these questions in stochastic dynamics that we saw in Shen Cheng's talk yesterday, this really helps when you construct these invariant measures and understand stochastic dynamics because now it makes one aspect of the arguments that he was talking about. Of the arguments that he was talking about yesterday was like this coupling argument. But now, now, if you have this kind of contraction property, the coupling is quite simple because as soon as the noise turns off, the diffusion drives you to the average. And then as soon as you're close for one time, because you know you have this almost sure contraction property, you just kind of remain close for all time. And this is the idea. But for the remainder, the point is that we just have a good well-posedness theory for these kind of solutions. For these kinds of solutions. Okay, and the point is that, and this goes to Chen Lin's talk and also the results of Professor Finaki: is that if you look along appropriate scaling limits, what you can see is that these, is that these, at least to the order of a central limit theorem, the solutions of these SPDEs give you a good approximation to the particle system in the sense that you see the same kind of hydrogen. You see the same kind of hydrodynamic limit, our law of large numbers behavior, but you also see some kind of central limit theorem. And this is exactly the process that we saw, this Ernststein-Ullenbeck process that we saw in Shenlin's talk yesterday. But the point is that if you would be happy with just getting a central limit theorem and law of large numbers, you would say, okay, you're wasting your time a bit, and you don't really have to. Uh, and you don't really have to work very s so so much. Because what you think of when you think about like a central limit theorem, this is really like a more universal statement. So, what you could think of doing is say, okay, instead of looking at the solutions to this nonlinear SPDE, what I could say is I look instead at this SPDE, where the difference is now the noise coefficient. I just insert the hydrodynamic limit. So now this is just like an additive noise where, okay, this is like the heat equation. Okay, this is like the heat equation, so this noise coefficient is quite smooth, and there's no kind of issue understanding this equation. And what you would see is that, okay, and you would be right. You'd say, okay, actually, this equation is relatively simple to analyze, relatively speaking. And you would see that the solutions to this equation here actually satisfy the same law of large numbers and central limit behavior, you know, because this is the CLT is kind of a, you know, it's like a universal statement. Statement. But where do you really see the difference? So why do we insist on making our life a bit difficult and setting this really nonlinear term here? And the point is really in the large deviations. And so what you can prove is that the solutions to this equation satisfy the right large deviations principle at least along appropriate scaling limits. And so one way to think of this scaling limit, which is not the scaling limit, Scaling limit, which is not the scaling limit we don't claim is optimal and we think it can be improved, is in some sense, this is we're identifying a scaling regime where we stay in kind of the subcritical regime for the SPDE, you know, because we're scaling epsilon sufficiently quickly with respect to the smoothing of the noise to kind of stay in the subcritical regime and recover the large deviations that we see for the particle system. So, um, it's okay. What you see is that, okay, indeed, if also you want a large deviations principle, then along the same scaling limit, the solutions sort of sort of capture the large deviations behavior of the particle system. And this is really where you see a difference between these kind of linear hydrodynamics that we saw in the previous slide and these nonlinear hydrodynamics. Because now, the rate function for this linearized version, or this semi-linearized version, you The semi-linearized version, you know, it differs from this rate function in the sense here we have this row, and here we have this row bar. And this is essential from the point of view of the theory, in the sense that, remember, the whole point of this theory is that somehow the probability of observing these events is very much affected by the formation of the event itself. Because as you're seeing a large concentration of particles, this means that your mobility is also really increasing. Your mobility is also really increasing in this region, so somehow it's going to be a bit easier to move particles around. Whereas it's conversely, if you see like a vacuum region, well, then your mobility is kind of going to zero, so it's very hard to move particles around. And so you see that this actually does quite a bad job capturing this kind of behavior. And there are other reasons why this is not so good. I mean, for example, these solutions will become negatives. The density, of course, never becomes. Negatives, the density, of course, never becomes negative, but this is very much more like a CLP type equation. And just okay, I won't give much details here other than to say that also for the exclusion process, you can do some numerics and you might ask, okay, the CLT and the LDP are really like asymptotic statements. It's okay, maybe, okay, like theoretically you see this difference, but maybe numerically, you know, it's not so apparent. But we just did some numeric. But we just did some numerics to show that, okay, if you compare these two equations, then you really do, as soon as you go beyond the CLT scaling, you really do see a difference between the linearized, the more linear version of the equation with additive noise versus the nonlinear equation. But I won't say too much. Okay, just very quickly in the last five minutes of the talk, I'll just say something very briefly about some analysis that goes into this result. This result. And the main analysis really is analyzing this equation that enters the rate function, which, if you look at the scaling of this equation, it's exactly an energy-critical equation in L1. And so what that's saying is really that the drift G is somehow on equal footing with the diffusion. And so you don't have, for example, you can't close LP estimates, and the only estimate that you really have is some kind of like decay. That you really have is some kind of like decay of entropy, where now what you're seeing basically is that the entropy of your solution plus the Fisher information is bounded by basically the L2 norm of the control plus the entropy of the initial data. And this energy criticality is not so surprising in the sense that you know what it's saying basically is that if you're willing to put in kind of an arbitrary amount of information to your energy to your system, you know, you can't leave the System, you can't leave the space of L1 functions, but you can sort of get arbitrarily close. And that's also something you see in the particle system where somehow if you would think about looking at a measure-value deviation, this would be an event where more or less every particle sits on the same site, but this is sort of super exponentially small in probability, and this is something that the large deviations just don't see. And I'll just say just very briefly that the Wilposinist theory. Briefly, that the Woposinist theory is very much based on a, we normalized in the PDE sense the so-called kinetic formulation of the equation. And what this does is it allows you, because we have sort of irregular coefficients like the square root, it allows you to sort of first kind of cut out very large values of the solution and very small values of the solution, put yourself in a region where things are relatively regular, kind of do the uniqueness proof, but then you have to remove these kind of errors. And so you And so you can define this notion of renormalized kinetic solution based on a kinetic formulation of the equation. And then you can prove, and so also the analysis of the SPDE very much uses this theory, and then prove here that you have well-posedness of these kind of so-called renormalized solutions in the PDE sense. But there's a problem with this kind of solution theory, which is that, okay, for the large deviations, We kind of essentially need one property of this equation. And the property we need is that sort of a weakly convergent sequence of controls induces like a strongly convergent sequence of solutions. But in this term, once you pass to this renormalized solution theory, you see this product of the gradient and the control, which when you pass to the limit, we don't have any higher regularity. So the gradient is just converging weakly and the control is just converging weakly. So in the limit, we don't have this kind of stability. And the limit, we don't have this kind of stability property. And so, an essential thing is to try to show actually well-posedness of weak solutions. Because if you just look at sort of like the standard weak formulation of this equation, then you don't see this kind of ill-defined product between the control and the gradient. And so, what we prove is actually that sort of renormalized solutions are equivalent to weak solutions, which is not. To weak solutions, which is not totally obvious in the sense that, you know, if you think about these works of Ambrosio and DiPrent-Leon's, so they look at this equation without the diffusion term, and what they show is that if the drift G is like a bounded variation, then you can show this equivalence. But they need, but these conditions are almost optimal in the sense that there's a counterexample for drifts which are bounded variation everywhere except for a hyperplane. Except for a hyperplane. And so then what you see is: okay, I mean, now we have some potentially degenerate diffusion, but we have much less regularity for the drift G in the sense that we only have an L2 drift, but we can try to use this kind of entropy estimate to still prove a statement like this. And so this is possible. And then you can recover this kind of weak, strong continuity, which is essential to the proof of large deviations. And just in the last 30 seconds, Deviations. And just in the last 30 seconds, I'll say something very quickly, which is that sometimes when you look at large deviations principles for equations like this, what you see is actually that the large deviations principle is defined in terms of the dual norm of kind of a weighted Hilbert space, in the sense that now if I think of an L2 Hilbert space where now the L2 norm is weighted by the mobility of my system. Is weighted by the mobility of my system, what you can see is that, okay, in some sense, if I look at my diffusion equation here, then the equation basically defines this equation as an element of this dual space. And so if you think about what's going on with the equation, then you can actually rephrase the large deviations principle as really like the norm in this dual of this weighted hilbert. This dual, this weighted Hilbert space. And actually, using just like Rhies' representation techniques, you can say that the minimizer takes the form, has this kind of form, and you can rewrite the skeleton equation in terms of this kind of supercritical equation. And so oftentimes you'll see large deviations phrased in this way. So, this is my last slide. And so, but why is this difficult? Well, the point is that. Well, the point is that this equation doesn't have good sort of analytic properties. And this leads to some difficulties if you look back at the original proof of large deviations for the zero range process. Because what they see is that they get a large deviation's upper bound with the rate function we've described, but the lower bound is obtained by sort of a change of measure. And for this, you need to restrict to a space of smooth fluctuations. And so the lower bound is restricted to the boundaries. The lower bound is restricted, is obtained with this rate function, restricted to the lower semi-continuous envelope of the rate function, restricted to these smooth fluctuations. And then using the analysis, the skeleton equation and this interpretation of the rate function, what's remained open, I guess, and what you can solve using this analysis is the characterization of this lower semi-continuous envelope. And so you can show that this lower semi-continuous envelope, you can characterize it. Characterize it by constructing kind of a recovery sequence and then show that this is equal to the original rate function i. So, okay. So these are some of the references and the rest of the references. But yeah, with that, thanks a lot. Yeah.