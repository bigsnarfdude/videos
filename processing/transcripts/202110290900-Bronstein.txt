With us here today, Michael Bronstein. He's a professor at the University of Imperial College London and also the University of Logan. He's the head of graph learning research at Twitter. And in these areas of the intersection between geometry and learning, he is very well known. So, a lot of you already know about his groundbreaking work. I will just I will just mention some of the latest accolades because he received the Silver Medal of the Royal Academy of Engineering from the UK in 2020, and he is now a fellow of the British Computer Society and a member of the Academia Europea and an IEEE Fellow in 2019. And his work in this direction has pioneered a number of different research areas, and he even coined And he even coined the term geometric deep learning. So, thank you so much for accepting our invitation. And thank you very much. Thank you for this kind introduction and the invitation. So today is a little bit for me, well, a little bit stressful in the sense that it's the first time I will be presenting some recent works. So it's also some learning experience for myself. So I will be more than glad. Experience for myself. So, I will be more than glad to hear comments. So, some of the works already appeared in ICML and will appear in new reps, and some other results are still work in progress. So, I would like to talk today about graph neural networks from the perspective of differential geometry and differential equations. So, apparently, not exactly related things. Exactly, related things. Well, maybe some of you are there related, but I would like to show some of these related. So I would like to start with this broader view that we call geometric deep learning, and it really boils down to the notion of symmetry. And here I'm quoting Hermann Weil, who said that symmetry, as wide or as narrow as you may define its meaning, is one idea by which men for the ages has tried to comprehend and create order. Try to comprehend and create order, beauty, and perfection. And another quote is from Philip Anderson, a physicist, Nobel Prize winner, which will say maybe even more categorically that it's only slightly overstating the case to say that physics is the study of symmetry. So this is exactly the idea that we try to build into neural network architectures or in general to approach the relation of The derivation of new architectures and the analysis of existing architectures in deep learning from a geometric perspective. And here our program that we call geometric deep learning mirrors and is inspired a lot by the Erlangen program of Felix Klein, who approached geometry as the study of symmetries. So we can show, for example, that we can derive operator learning architectures such as convolutional neural networks, graph neural networks, transformers, and you name it from And you name it from these basic notions. And if you want some more details, I will just very briefly overview it here. You are welcome to see my lecture that I gave at ICLIA earlier this year and the website geometricdeploning.com. And we also have a book that will be coming next year that I'm writing together with Joan Boomer, Tanko Koen, and Peter Dilichkirich. So, just to give you an idea, basically, we think of machine learning. Basically, we think of machine learning problems, at least in some settings, we can think of them as function estimation, right? So, we have some input that may be very high-dimensional, like an image, and an output is, let's say, a label of this image. So, most of these problems are dimensionality cursed, right? Because of the very high dimensionality, the standard results and the standard bounds from approximation theory become meaningless. So, we get, for example, the phenomenon that the number of Phenomenon that the number of samples that we need to sample our space or our function at grows exponentially with a dimension. So it means that in practice we cannot apply any of these results. So the idea of geometric department is really to use a geometric structure of the data in the sense that usually the signals that we deal with have some underlying domain. So if you take an image, for example, it's not just a multi-dimensional. It's not just a multi-dimensional vector, it has an underlying grid structure. So, the idea of geometric reporting, we separate these three instances. So, we have a domain that I denote here by omega. We have a signal that is defined on this domain, I denote it by x of omega, and then we have functions that are defined on such signals, right? So, again, think of a grid, an image that is defined on nodes of this grid, and then we have, for example, a function that assigns some label or a number. Some label or a number to this image, right? Like image classifier. So the structure of the domain is modeled by its symmetry group, for example, the group of translations. On the signals, the action of the group on the domain is manifested as what is called the group representation, and it imposes certain inductive priors on inductive biases on the functions through the notion of invariance and equivariance. So, for example, I would like to say that my So, for example, I would like to say that my function doesn't change under the action of the group or maybe changes in the same way as the group acts. So, a good example, again, in the case of images, convolutional neural networks are probably the most well-known manifestation of these principles. So, we have a two-dimensional plane as our domain on which we assume the translation group to be the symmetry. Then, the action on the image is the shift operator that we can apply on the image and. On the image, and a class of functions that are equivariant to shifts are convolutions. You can actually work both ways, so you can define convolution as a linear shift-equivariant function. Here is another example that we will be dealing with today. So these are graph neural networks. In this case, our domain is a graph, which has a set of nodes connected by edges. The symmetry that we assume here is the permutation group, and it comes from the fact that a graph. And it comes from the fact that a graph being purely topological construction, we don't have a canonical order of notes. So we need anything that we define on the graph needs to account for this thing. So the representation of this when attacks on textures is the permutation matrix. And the functions, this is what is implemented in graph neural networks, are some form of message passing, where I aggregate the features of the node. The features of the node and I pass them to the neighbors. So basically, there are multiple domains that can be looked at from this perspective. For example, grids, meshes, graphs. This is all what we study in our book on genetic economy, but there is somehow, there are somehow continuous counterparts, right? So for a grid, for example, we can think of it as a discretization of the place. We can think of it as a discretization of the plane, or more generally, we can think of homogeneous spaces where you have a transitive action of the symmetrical. For meshes, we can think of many forms, right? Two-dimensional surfaces that are typically discretized as triangular meshes. We don't have an immediate continuous counterpart for graphs. And to me, at least personally, this is disturbing. So we can do a lot of things in the plane. For example, we can develop a continuous theory and discretize it properly. And we obviously have continuous conversions. We obviously have continuous conversions and discrete conversions, same thing on manifolds, not on graphs, right? So it would be cool to have something continuous for graphs as well. Now, the second thing regards graph neural networks themselves. So let me give you a brief overview. So the typical way that we deal with a graph in graph neural networks is we look at the node and its immediate neighbors. So other nodes that are connected by an H or some node I. We can take the features of these nodes. Can take the fishes of these nodes and together they form what technically is called the multi-setoral band. This is maybe a technical subtlety because even though the neighbor nodes themselves are unique, the features might not necessarily be unique, right? So you can see here, for example, two of the blue nodes that have the same feature vector. So together with the feature vector of the node itself, we have some description of the neighborhood of the node and we can aggregate node and we can aggregate this information by some function phi. Now a reminder that we don't have canonical ordering of the nodes so this function must be permutation invariant. It must ignore, must be agnostic to the ordering of the neighbors. And the typical way that this aggregation looks like, we have some permutation invariant aggregator. It could be some, minimum, maximum, average, whatever. Whatever. And we have some learnable function that I denoted here by phi. And there is a lot of nuances and different versions of how these graph neural networks work. So in the simplest case, we have what is called the convolutional graph neural network, or what we call convolutional flavor of a GLN. So in this case, we just have a linear aggregation of the neighbor nodes with weights that depend only on the structure of the graph. That depends only on the structure of the graph. And again, in the case of a grid, this becomes the standard convolution. So that's why this is the name. So this is how some of the early works on GNNs try to generalize convolution from grids to graphs. Slightly more general architecture, here we now have, it's still linear aggregation, but now the coefficients are dependent on the features themselves. So this is what is typically called the tension. The best representative of this architecture is the The best representative of this architecture is the GAT or the Graph Attention Network, the work of better Wilich Page from 2017. And the most general flavor is what is called message passing. So in this case, you have some non-linear function that transforms the features of the node and the neighbor. And you can think of it as the message that is sent from the neighbor to a node. And this has been popularized by the work of Justin Gilmer from Google and Peter Vatalia from DeepMind. Atalia from Deep Mind. And again, there are multiple works that probably more or less the same, just with different names. And probably significantly earlier than these references that I cited. In particular, if you look at the domain of computational chemistry and chemoinformatics, some of the prototypes of graph neural networks already appeared in the 90s. Now, it is interesting to see that in many cases, it is common to use positional encoding. Positional encoding. So, actually, this form of message passing, or maybe it simplifies a form of attentional flavor, is exactly the transformer architecture that uses positional encoding to represent the order of nodes. We're dealing with sequences of text in particular, in nature language processing applications. And this has also been applied to graphs. So, bottom line, we have the Bottom line, we have this local function that takes aggregation of neighbor features and produces a new feature for each node. So if we apply this function phi at every node, we get the output of this proputation activant, even though the function itself at every node is invariant. And its choice is crucial to the expressive power of the graph neural network. And in particular, it was shown, well, here I'm Was shown. Well, here I'm citing the words of Schuen Morris, but it has been observed before in other contexts not in graph neural networks, that message passing graph neural networks where the segregation function is injective are at most as powerful as the by square 11 graph physical fusing test. So, this is also a classical work from Graph Theory from Rifra McCropraft 1967, and it was also inspired by applications in chemistry. So, actually, In chemistry. So, actually, the story goes from what I know is that Georg Vladimirs, who worked in the Soviet Union, he was a computational chemist, came up probably for the first time with the idea of representing molecules as graphs in the beginning of the 60s. So in the West, I think this is usually credited to Morgan with the eponymous molecular fingerprints. And then he came to Weiss Verde and Lehman with the question: how would you distinguish it in different molecular graphs? And then they came up with. And then they came up with their famous algorithms. So the algorithm goes like this. So, what it is is an iterative color refining scheme. So, it starts with a graph where every node is labeled in the same way. So, this is represented by color. And it looks at the structure of each neighborhood and then applies an injective function, you can think of it as a Hessian, to change the colors. So, initially, here we have two types of neighbors, right? So, we have a blue node with two blue neighbors and blue nodes. With two blue neighbors and blue node with three blue neighbors. So, this will, because of the injectivity, they will become two distinct colors that they denote by green and yellow. I can apply the same procedure again. So, now we have three types of neighborhoods. We have green with two yellow neighbors, we have yellow with one yellow and two green neighbors, and we have a green with one yellow and one green neighbor. These will become violet, red, and gray. If I do it again, then it doesn't change. At this point, I can output the distribution of the labels, and if I take an Distribution of the labels. And if I take another graph and I get a different distribution, I can for sure say that the graphs are isomorphic. But if I have the same distribution, actually, I don't know. So it's a necessary but insufficient condition. And in fact, we can see examples of graphs that will be deemed potentially equivalent by the vice-per-devon algorithm. And we can actually see why it happens. So if I take this node in the graph, basically what the algorithm will do, it will explore progressively its neighbors and it will construct. Its neighbors, and it will construct this kind of a rooted tree. And if I look at the corresponding node in another graph that is not isomorphic, then I will see that this kind of tree will be completely indistinguishable. So what positional encoding tries to do is actually tries to color the nodes in a different way. And we can see here that if I had some encoding for the nodes that will be different in these two different graphs, I will see different trees. This will allow. See different trees. This will allow me to basically increase the expressive power of the graph neural network. And there have been multiple ideas how to use positional encoding in graph neural networks, whether it's using random features, whether it's using, for example, Laplacian, graph-Laplacian eigenvectors, whether it worked or recounted the primitive graph substructures such as cycles or cliques and so on and so forth. But there is no really an agreement of what is the right position. Of what is the right positional encoding? So, this will be the second question we will try to approach from this perspective of differential geometry and differential equations. Now, there is another thing that somehow is considered problematic in graph neural networks, and these are the phenomena of oversporting and bottlenecks. So, if you happen to have a graph that has the structure of a small world network, where the volume grows. Where the volume grows exponentially with the growth of a bow of certain radius. And you also depend on long distance information. So what happens is that you need to propagate information from distant nodes, from a lot of distant nodes, right? You have many nodes and you need to squeeze this information into a single node. So you get a phenomenon that is called oversporting. And the remedy for it that And the remedy for it that has been proposed is to try to decouple the input graph from the graph that is used for information propagation for message passing. And implicitly, it has been done before for other considerations, such as, for example, sampling the graph, sampling neighborhoods, such as the famous Graph Sage work from the group of UL Escobes using multi-code filters. So one work from our group. From our group that we call sign using a complete graph, where it has a fully connected structure or some other rewiring technique, for example, based upon diffusion on personalized page rank. Or you can also make the construction of the graph learnable. And this is what we did in the NameGraph Synons. So, bottom line, there is no really a canonical way of dividing the graph. So, we want to try to understand. We want to try to understand how to do it. So, these will be the three guiding questions that I would like to talk about today. And I will try to address them in the following three sections. We'll talk about continuous models. So, we'll be talking about the reinterpretation of graph neural networks for differential equations. So, this was our CML paper that we presented in December. Then we'll talk about positional encoding and the generalization of this diffusion PD model. Diffusion PD model to non-nucleinid case that we call the Goldtrami flow on graphs. So, this is a paper that will be presenting at Europs later this year. And then, if time remains, I will also talk about graph rewiring and we'll be using another form of diffusion equation that is called the Ricci flow that also comes from differential geometry. And this is a work that is still unpublished. Still unpublished. So let's talk about graph neural diffusion. Just second. Sorry, my son is reaching for my phone to deal with this disturbance. So yeah, so let's talk about graph neural diffusion. So this is a work that was that we did at Twitter, and the first answers were Ben Champerlin. first authorizer Ben Chamberlain and James Robertson who was an angel in our group. So the history of the understanding of diffusion and in particular diffusion of heat actually is very long and well probably one of the first formal ways to try to deal with it is a work that was published anonymously in the Transactions of the Royal Society and in 1701. In 1701, published in Lee. Even though the paper was anonymous, as it was not signed, everybody somehow knew that it was written by Isaac Newton. He would become a serf, I think, four years after that. And this is what is now called the Newton law of cooling. And it says in modern language that the temperature of a hot that a hot body loses in a given time is proportional to the temperature difference between the object and the environment. So it's a kind of global property, right? Global property, right? So if I have a very hot object, it will be losing its temperature faster than the cold object, right? Now, it is also probably clear why Newton was reluctant to sign this paper, because at that time the understanding of heat and temperature was not really great. And he used the latent term color that literally translates as heat. And now, by heat and physics, in modern times, we understand the flow of. And the flow of thermal energy that has the units of Jaus, right? The units of energy. What he was referring to actually is the notion of temperature that was not invented yet at his time. And this is what we use to describe the average kinetic energy of molecules. We usually measure it in degrees Kelvin, for example. So some time has passed, and well, there are some people that are actually disputing Newton's priority and why the law of Boolean. Like the law of cooling faces, then why it was actually Fourier that described it properly. So, Fourier came with the local differential version of this law that is called Fourier heat conduction law, and it says that the heat flux resulting from thermal conduction is proportional to the magnitude of the temperature gradient and opposite to it in sign. So, Fourier, as you might know, he was actually a politician most of his career. He got He got an offer that he could not refuse from Napoleon, and he was the prefect of Chernobyl. And that's why, actually, his publications, this work about theory of heat, is from 1822, even though the work was done at least a decade before. And so, in modern terms, what this law meant is the following: so, if this is our temperature distribution. This is our temperature distribution. So, we have some hot area and a cold area. So, this is modeled as a scalar valued function or a scalar field. So, this difference in heat creates heat flux, right? So, there is a gradient. So, this is a vector field that tells us how the heat flows from one point to another. So, the gradient is a vector field. You can visualize it as a lot of little arrows at every point. And it tells you that in this direction, or in the negative direction of the gradient. By the or in the negative direction of the gradient, there will be a flow of heat. Actually, it's not only heat, this diffusion description of diffusion works in other things. You can define and describe, for example, change of concentration in chemical solution or Brownian motion, right? So these are all diffusion equations, even the flow of people, right? So there could be also some socio-economic model. Now, coupled with another thing that is some kind of conservation. thing that is some kind of conservation condition basically we can express it as follows that no heat created or disappears out of nowhere right the formal way to describe it mathematically is to look at the divergence of these spectral fields so divergence tells us the overall flow through some infinitesimal volume in the domain right so basically what we say is that the change of the temperature is only explained by the divergence of this of this heat flux right Heat flux, right? So nothing is created and nothing disappears. And together it gives what is called the heat diffusion equation, or simply the diffusion equation, which is a partial differential equation that looks like this. So the left-hand side is the temporal derivative of the change of temperature at point u at time t. The right-hand side is the divergence of the heat flux, or the gradient of the temperature with some coefficient A. With some coefficient A. That can, in general, can be something rather arbitrary. So it can depend on position. It can also necessarily be scalar. It can be also vector. We'll talk about it in a second. Okay, but you can think of it as a kind of local differential form of the Newton law of cooling because in the simple case where the C is constant, right, we can write the divergence of the gradient as delta n. Of the gradient as the Laplacian, right? And this is what we call the homogeneous or isotropic diffusion. And in this case, the Laplacian measures the difference between yourself and your neighbors locally, differentially. And the left-hand side is the change of temperature. So here in that this differential form of the Newton law of cooling. Now, the more interesting diffusion equations are non-homogeneous equations where the diffusivity, this constant A, depends on the position. A depends on the position. So that's why it's called non-homogeneous, meaning that the conductivity property of the medium are different at every point. And the most general form is what is called theisotropic diffusion. So it's not only position dependent, it's also direction dependent. And this is modeled by writing the diffusivity function as a matrix failure function. So it rotates gravity. Okay, we'll be primarily talking about this. This non-homogeneous but isotropic diffusion. And it has been very widely and for many years explored in the image processing domain because you can think of diffusion as convolution with the low pass filter. And if you make this diffusion nonlinear, you actually get age-preserving properties. And in particular, what was interesting to look at at diffusivity that is based on the age indicator. So you diffuse less across ages, across discontinuity. Across edges, across discontinuity of an image. And we can think of an image that has, for example, bright colours and dark colours, right? So if my neighbors are all of the same colour, it will be standard linear diffusion, right? So it will be just convolution with Gaussian, with the heat kernel of this equation. But if I get close to this discontinuity, I will actually not be mixing pixels with different colors, right, or with different intensities. Colors, right, or with different intensities, because at this point the gradient of the image will be large, and my diffusivity is inversely proportional to the gradient, so I will be diffusing less. Okay, so that was first described by Piedra Parona and Jitendra Malek in 1990. They called it anisotropic diffusion, which is obviously wrong, right? So it is isotropic, but non-linear, non-homogeneous diffusion. And there are follow-up works that really did an isotropic diffusion. Did anisotropic diffusion equations, but it has become a very popular idea and it has been explored in the image processing domain probably for the next 20 years until deep learning changed everything. And it is probably ironical that we are now going back to these models. But here you can see the difference between the standard homogeneous diffusion, which is just a low-pass filter, versus non-homogeneous diffusion that removes and smoothes within the continuous parts of the. The continuous parts of the image and preserves the edges, the discontinuities, which are important for visual perception. So you can remove noise, for example, without removing, without destroying somehow the cartoonish look of the image. Now, I should say that this idea of looking at diffusion equations on other objects is not new at all. So we'll now be talking about diffusion on graphs, but just to give you an idea that on meshes, for example, Just to give you an idea that on meshes, for example, in computer graphics, it has been done for at least 15 years. So, we also have some works in this domain way before deep learning has become popular and way before graph neural networks have become interesting or even this term was invented. So, one of the classical works, for example, is by my colleague Max Ovsanikov called heat kernel signatures. You can think of them as some multi-scale version of the Gaussian curvature. So, if you think of page rank, for example, this is also some form of For example, this is also some form of diffusion property, right? So it's steady state of a random work process, which is also related to the diffusion equation in machine learning. For example, in non-linear dimensionality reduction, Laplace and Eigen maps or diffusion maps by Matthew Koufman have been used since almost 20 years. And if you want to go very deep in differential geometry, heat trace expansion is a work from the 40s. And sorry that I have always Sorry that I have always troubled to pronounce the name of the first author on this paper. So, bottom line, to some extent, we're not doing. So, that's the main message. So, let's see how the diffusion looks on graphs. And here we can generalize or reduce in some sense what we do in the continuous domain pretty straightforwardly because there is a one-to-one analogy between the continuous and discrete operations, in particular between the gradient. In particular, between the gradient and the divergence. So we assume that we have this undirected graph, it can also be directed, but just for simplicity, we have node features x, right? So this would be the analogy of this colour field. We can compute the gradient that for every edge in the graph, we look at the difference between the node at the two endpoints of the edge, right? And this will become the analogy of the vector field. So it is, these are features that are. These are features that are attached to the edges of the graph. We can similarly define divergence. So, if the gradient takes known information and maps it to the edges, the divergence does the other way around. So, it takes the fissures on the edges that I denote here by Y and computes a weighted sum. So, A here will be, in the simplest case, just the adjacency coefficients of the graph, but these could be also weights if the graph is going. If the graph is weighted. So the divergence goes back to the nodes. And formally, these operators are adjoint, so you can switch between them with another respective inner robots. Okay, and the Laplacian is defined as the divergence of the gradient in many references defined with the minus. So it depends whether your Laplacian is positive semi-definite or negative semi-definite. And essentially, what it does, it looks at Essentially, what it does, it looks at the difference between yourself and the weighted average of your neighbors. So that exactly corresponds to the geometric situation of what the plus an operator is, right? And we can easily see it from this formula. So it's not very surprising. Now, this gives us the possibility to do pretty straightforwardly to generalize or to write the formal. Or to write the formal analogy of diffusion equation on the graph. So on the left-hand side, we have the temporal difference of the temporal derivative of the feature at every node. And on the right-hand side, we have the divergence of the gradient. So again, the gradient here will be the difference between the features at node J and node I. The diffusivity here will be this function A. We assume it to be. This function a, we assume it to be normalized to one just for convenience, and the sum over the neighbors will be the divergence. Okay, now we can to solve this diffusion equation, so it is non-linear, right? So A in general will be something that depends on the features, we can use, we can discretize this diffusion equation in time. So in the simplest case, we assume fixed step size. case we assume fixed step size. I denote the step size by tau. So we have instead of continuous time t, we have now steps k. And we can use forward difference, which gives us the explicit or the forward Euler discretization. So you can see that the difference between the subsequent steps will look like this. I can write it in matrix vector form in this way, right? Or maybe more briefly as a matrix Q. So you can see that it's a linear aggregation of the Is a linear aggregation of the neighbor features. So Q has exactly the same structure as the adjustency of the graph, but the matrix itself is a non-linear function that depends on features x, right? So once I'm given Q, I can write it as a linear diffusion, but Q itself depends non-linearly on X. Now I can also see, well, I said that it has the graph adjusted structure, so usually it's a sparse matrix, and we can Matrix and we can show that this explicit scheme is stable in the sense that is usually given total PDs for this tau, the step size being between zero and one. So essentially, it guarantees that if my step size is sufficiently small, no disasters will happen. You can also see that because I assume this normalization for the diffusivity function, if tau is equal to one, If tau is equal to one, we get the graph attention network, we get the gutter collection, right? Maybe a special version of gut where there is no non-linearity between layers, right? And every step of this explicit Euler scheme will become a layer of gut, right? Now, of course, we can do something smarter. Those of you who are working in numerical analysis will say, whoever uses explicit schemes, why not to try, for example, a backward discretization? Why not to try? Discretization, why not to try a semi-implicit scheme? So it's called semi-implicit in the sense that we assume that the right-hand side, the linear part will be taken from the following step, but the diffusivity function will be taken from the current step. Okay, and we do the same trick. We can write it also in this form. We can now see that the matrix multiplies x k plus 1 from the left. So the solution will be given by the By you will need to invert the matrix B, right? So usually it's done approximately using by solving approximately a linear system. And you can interpret this matrix Q, right? The inverse or the pseudo inverse of B. It will usually not have the structure of the adjacency. It will usually be dense. And you can think of it as a kind of multi-hole filter. So I'm not only propagating information from my immediate neighbors, but maybe from multiple. But maybe from multiple neighbors, multiple hopes away. Okay, and the advantage of these schemes is that they are unconditionally stable. So we might incur a bigger error, so in the solution of the diffusion equation, but no disasters will happen even if tau is very large. Okay, and of course, we know other schemes. So this is the explicit scheme on the left and the implicit scheme on the right, which you can think of as a kind of feedback loop. But we can also, for example, have multi-step. Also, for example, have multi-step schemes such as the Rungi-Kuta method, or we might have also implicit multi-step schemes. We might have schemes with adaptive step size, so the step size doesn't need to be to be fixed. So you probably know better than me that there is a lot of literature and a lot of different, sometimes exotic methods in this domain. And this is how we propose to do learning on graphs. And this is the graph. Learning on graphs, and this is the graph genome diffusion or grant for short. I know that this is not a very modest accounting. So, we are given a graph with input-node features, and let's say that our task is to node-wise classification. So we use the input features as the initial condition for the diffusion equation that we run for time equal capital T. And then we use this as an output. And additionally, we can also have some initial function that. Some initial functions that are ignored by phi and some read-out function psi that transforms the input and the output. So the dimensions of the features that are used in the diffusion do not necessarily need to be the same as the input dimensions. A here is a learnable function, so the diffusivity, so it has exactly the same structure of potential. And here, that was the choice. We make it time independent. So this A in principle could depend also on T, not only on X, but we make it. but we make it we can make it time time independent so if the basically by analogy if you discretize this equation using uh using explicit Euler scheme it means that and we know that it is equivalent to gut it means that all the parameters across gut layers will be shared okay so why I don't bother to do it well I think there are multiple reasons first of all we get new perspectives on old problems such as overschool On old problems, such as overschooling, bottlenecks, and so on. We can get new architectures. So, first of all, many existing GNNs can be formalized as some version of discretized graph diffusion equation. We can avail to more efficient solvers that are maybe do not have immediate analogies in the GNM zone, such as multi-step adaptive, implicit, maybe multi-grid solvers. We can, for example, interpret implicit schemes as multi-hoc filters, which in some cases are quite popular. In some cases, they're quite popular and advantages. We also gain theoretical guarantees, so we can use some classical results on stability and convergence from the PDF theory. And also, there are deep links to other fields that are probably less known in the GNM literature, such as differential geometry and algebraic topology. So, here's one thing, for example, that is considered to be problematic to graph neural networks. And I even wrote last year a blog post. Wrote last year a blog post that attracted a little bit of criticism about depth considered control in graph neural networks because it is known that at least with some architectures, it's difficult to make deep graph funeral networks for a variety of reasons. One of them is over-smoothing and another one is information bottlenecks. So, in our case, we don't really have the notion of depth. So, the depth is roughly equivalent to the diffusion time, right? And with simple explicit schemes. And with simple explicit scheme, when we have fixed time step, indeed we can think of k, the number of layers, being just sorry, it should be t over tau and not tau over t. And with adaptive scheme, we can basically trade off between the number of layers, right, or the number of iterations versus the step size, right? So we can have bigger steps, but less layers, right? So the diffusion time will still be the same, but maybe. Time will still be the same, but maybe the depth, at least as understood literally in deep learning, the number of layers will be smaller. And with implicit schemes, we can trade off depths and width, right? So less equations, but bigger filters, right? More hopes. And here is an experimental result from our paper. So here we show the test accuracy on standard data sets such as cora, carbon method, and Data sets such as Cora, Providment, and SITES here versus depth of the grand architecture and standard graph neural networks, GCM, particularly in residual version of GCN. So you see that the performance of standard architectures deteriorates with depth, whereas in our case, it doesn't. So I'm not necessarily claiming that we have solved this problem, but at least it shows the advantage of this mindset. And here are some results. Set. And here are some results. So again, I'm always skeptical about showing half percent better performance on Quora or PubMed. We do show better performance, but probably what is more interesting is that, for example, if you compare the GRAND model to the GAT model, we get more or less the same results, but we use something like 20 times less parameters. So 1.6 million parameters for GAT and 70,000 parameters for GRAND. And this is because we use this shared Because we use this shared time-independent attention function intentionally. So, basically, this is a much smaller model that potentially overfits much less. Now, there is another question that will actually lead us to the next point of the discussion is the following observation. So, if we think of the diffusion equation in the plane, right, let's say that we discretize the plane as a grid, we have a zillion of different versions of constructing the Laplacian. Of constructing the Laplacian operator, right, or diffusion operator. So I can use, for example, my four hop neighbors, the top, bottom, right, and left. Right? I can rotate everything by 45 degrees. I can look at the more distant neighbors. Now, because this is a linear operator, any convex combination will also be a very discretization of the Laplacian. So, what is the right discretization of the Laplacian or the diffusion operator in the green? So, I think the answer is there is no right one, right? It depends what you want to do. Right one, right? It depends what you want to do, right? Everyone, each of these operators has different properties. So, why the same doesn't apply to graphs? So, we want somehow to get rid of the graph. So, we want to do graph neural networks where the graph is not something that is sacrosanct or a holy structure, but it is some convenient construction that we can change as we like based on. Like based on some numerical convenience or maybe some information propagation convenience and so forth. Okay, and this brings me to the second paper that I wanted to show today, which is the graphical trauma flow. And again, the same first authors, Ben Chamberlain, James Rohlbotton, and this will appear at New York's later this year. So if we look at this non-linear If we look at this non-linear diffusion equation that we've seen before, right? So we had basically a diffusion kernel that was adapted, right, or the diffusivity function that was adapted based on the gradient of the image non-linearly. So we had this equation. So an equivalent model, as I will show in a second, is to consider a linear but non-Euclidean equation. So in this case, you can see exactly the same picture, but from a different perspective. So we can Perspective. So we can think of a surface, right, or a manifold on which we run a diffusion. And basically, this manifold is embedded in a joint space that represents the positional, the spatial coordinates, and the feature coordinates, right? Or in this case, it's intensity. So the metric comes from this jointed bed. And you can see that the kernel somehow wraps on this non-Nucleian surface. Non-Euclidean surface. So, more formally, this is what is called the Giltramic flow. So, we consider an image as an embedded mainfold. So, it has coordinates u and x, maybe with some scaling factor alpha. And so these are x are the feature coordinates and the u are the positional coordinates. And considering this as a manifold, basically we can pull back the metric from the ambient space, which in this case is Euclidean, and the metric is And the metric is given by this two by two matrix. So that we denote by g. Okay, and the Beltravi flow is given just as a diffusion equation that involves a non-Euclidean version of the Laplacian operator, which is called the Laplace-Beltravi operator, named after Italian mathematician Eugenio Beltrami. So Beltrami is credited as one of the inventors of the differential calculus, sorry, differential geometry, and also one of the And also, one of those who proved that hyperbolic geometry is self-consistent. So, basically, that was probably the first time that people became interested for BL in non-Euclidean geometry. And that was probably the last nail in the coffee of Euclid. And deltrami flow is essentially the cradled flow of an analogy, a non-Euclidean version of the Dirichlet energy that is called Podikov energy or Polykov action. So it is common. So it is common in high energy physics, in particular in bosonic string theory, and it measures somehow the smoothness of this embedding. So I should say that the space in which this manifold is embedded doesn't itself need to be Euclidean, it can be another manifold. So this was the work of my PhD advisor, Ron Kimmel, and his colleagues in, I think, 1995. So again, it's a little bit historically ironic that after That after so much time we're getting back to these kind of ideas. So basically, you can see that this is how the diffusion equation looks like. So the evolution, again, if you ignore this normalization, so the change in the fissure rise or in the temperature of whatever it is, whatever X models, is proportional to the divergence of the gradient normalized by some. Normalized by some expression, and this expression can be interpreted as an age indicator. In particular, if alpha is large, then what we get what is called the total variation. So we'll get the gradient flow associated with total variation. And we can see that the peronomalic equation is a special case of the no-travi flow that can be obtained by certain choices here of this metric. I should say also that the metric doesn't necessarily need to be pulled back, so we can define it separately. Pulled back, so we can define it separately. So, in a sense, what is considered in this paper is also a particular case of what can be done more generally. How am I doing on time, by the way? Yeah, all right, you have 10 minutes left. I mean, if we could leave maybe five minutes for questions, that would be great. That would be great. But don't worry about it. It's interesting to finish all of this. It's fascinating. Thanks. Okay. Thank you. Yeah. So, obviously, as we've done previously with the diffusion equation, we can also write the graph version of Builder and Flow. So in this case, the graph, we can think of it having both positional and feature coordinates. So each node is equipped with a vector u and a vector x, right, and we denote it together by z. Right, and we denote it together by z. And now the graphical triangular flow evolves both. So it evolves the positional coordinates and the feature coordinates. So before that, we didn't have positional coordinates, right? We have a fixed structure. And you may wonder why to do it at all, right? So this is how it may look like. So you see the evolution of the positional coordinates. And this is the feature diffusion, as we had before, but the evolution of Z, you can think of it as graphy wire. So if I have So, if I had an isometric way of embedding the graph in some continuous domain, then I could completely forget about the graph, right? And I could say that basically the proximity structure, right, the nearest neighbors of a node is essentially my graph, right? So the U would be an equivalent representation of the graph. Unfortunately, this is often not the case, even though there are some better spaces that are better suited for certain graphs. Used for certain graphs, but uh, yeah, so it's not so it's not uh exactly the case. We still have a graph, but we know we don't necessarily need to stick to the same graph. So the graph might evolve, so the positional coordinates might evolve as a result of this diffusion based on the downstream task that we are trying to solve. And maybe at some point it will make sense to change the graph to evaluate. Okay, now what we show in the paper, and this is, I should say, that this is a little bit not completely to my satisfaction, is Completely to my satisfaction is, and this is one of the points that were heavily criticized by Livy Duris at Eurips. We say that with certain structural assumptions on the diffusivity function, we can write down a discrete analogy of the Polykov energy and show that this equation is a gradient flow of this energy. Now, why I'm saying that this is not completely satisfactory? Because there is no really an immediate analogy of the pullback of. An immediate analogy of the pullback of the metric manifolds that can be straightforwardly defined on graphs. And that's why it's a little bit of hand waving. We somehow fit the theory for what we actually do, for what we actually do in device. We now believe that we have a better idea how to do it. But yeah, so that was something that was largely left for future research. So this is an example of that I like. So that was done by James and this is Was done by James, and this is uh, this shows how everything evolves on core. So, this is the Motromi flow on the core graph. So, you see multiple things here happening. So, the nodes, the points represent the nodes, the colors represent the features, so it's RGB version of the features, so they're somehow projected for visualization. The positions of the points represent the positional encoding again. Represent the positional encoding again, two-dimensionalization. And then the graph changes. So we do rewire during the diffusion. So basically, what we try to do here is, of course, is to classify, right? So it's no classification task. So we want to see clear, distinct communities with distinct features. And that's exactly what happens, right? And you see that why rewiring the graph makes sense because maybe at some point in this diffusion, we have neighbors that completely should not. Completely should not talk to each other, right? So, this is how it looks like. And of course, we can think of other popular GNN architectures as instances of this framework that we call blend. I wanted to call it blend with A, but my co-authors outvoted me in the decision. So now it's called Blend. And you can see that, for example, GAT or transformer architecture or dynamic. Or dynamic rough CNN or GRAND. They all are particular cases. Here we have evolution of both positional coordinates and the features. The diffusivity depends on both the positional coordinates and the features, and the graph is adaptive and based on the positional coordinates. And the discretization can be done using either explicit or implicit methods or other numerical solvers of the of this diffusion P D. This efficient PD again, some results. So, well, we perform better than other methods. Without this, I'm afraid it's impossible to publish in conferences like Europs. Again, take it with a grain of salt. I think the bigger advantage of this framework is it's a nice theoretical grounding and maybe potential avenues that it opens rather than half percent background core. So with the So, with this, I think I'm out of time. So, let me just briefly mention the next work about which I will not be able to talk today, unfortunately. So, this was a work of a PhD student that I have under co-supervision with Xiaoben Dong at Oxford called Jake Topping and a colleague from Twitter, Francesco Di Giovanni, who recently graduated. He finished his PhD UC. He finished his PhD at UCL and he's a differential geometer. So many of the ideas are actually come from him. And basically, just in two words, we look at another kind of PDE that looks a lot like the diffusion equation. And this is what is called the Richie flow in differential geometry. So Richie flow is often described as the diffusion of the Riemannian metric, even though it has very different properties from the standard diffusion equation. The standard diffusion equation, and it's a little bit counter-intuitive thing, but basically, you're changing the space on which you live by this equation. So, the Riemannian metric on the left-hand side, right, that I denote by G, is essentially it's a bilinear form that tells you locally, it defines angles and distance structure on the manifold. And the Ricci tensor on the right-hand side, the Ricci curvature tells you how the volume changes, how the volume is different from the... Changes how the volume is different from the Euclidean from the Euclidean setting, roughly speaking. Okay, so if you think of maybe the one-dimensional analogy, you have one-dimensional curvature, and basically what happens is that you have a curve, arbitrary curve, it will become more and more convex, and eventually will collapse into a circle and then into a point. So, this idea was proposed by Richard Hamilton to study high-dimensional manifolds. Study high-dimensional manifolds. And the idea was that in some cases, we can show that we have something complicated like this, it will become more a sphere and then collapse into a point. And if you're familiar with the Poincar conjecture, so this is exactly what Poincar conjectured. In two dimensions, basically you can characterize spheres as a manifold where you can take any circle or closed curve and collapse it into a point. You can do it on the sphere, you cannot do it on a torus. You can do it on the sphere, you cannot do it on a torus, for example. Right, so the conjecture was that you can do it in three dimensions, and this exactly this instrument that was invented by Hamilton called Nietzsche Fall was used by Grigori Perelman 20 years ago to finally prove the concurrent conjecture. So that was really the breakthrough of the year at that time. Here is the cover of science. Here it's the cover of science. That's what's ecstatic about this proof. But it is a very useful technique that is used in differential geometry and some adjacent fields, such as, for example, astrophysics. So I think you can describe black hole behavior using these ideas. I think it's practically unknown in machine learning, or in particular, this field of machine learning on graphs. So we try to make this tool maybe more. To make this tool maybe more understandable and usable to this community. So, what we try to do in this paper, and I will probably stop at this point, is to relate the over-squashing and bottleneck phenomenon that I mentioned in the beginning, which is essentially the failure of message passing to efficiently propagate information due to some structural characteristics that are commonly known as bottlenecks. And as I said in the beginning, that As I said in the beginning, that typically we see it in problems where we have long-distance dependencies that require information from nodes that are multiple hops away, right? So, if you think of problems in chemistry, I have a molecule and I need to predict its property. And often you will see that some physical or chemical property depends on the properties of atoms on two opposite sides of this molecule. So, I will need to crawl this information from one side to another. It occurs in graph. It occurs in graphs which, unlike it, have exponential volume growth, right? So, too many neighbors, essentially, like social networks. And it tends to deteriorate with the depth of graph neural networks. And empirically, it was shown in particular by Uri Allon that we can alleviate it with graph rewiring, maybe even as simple as a fully connected graph. And what we try to do is total formally define over squashing. Formally defined over squashing related to the curvature or some notion of curvature on the graph. So we have an analogy of Richie curvature on the graph and then propose a surgical process that is inspired. It's not exactly the Richie flow, but it's inspired by the Richie flow that allows to do this rewiring. And as a byproduct, we also show that the diffusion-based rewiring that was proposed in the Deagel paper by the group of Stefan. The Deagle paper by the group of Steffen Guinemann doesn't always improve learning. So, the name Deagle Diffusion Improves Graph Learning is technically inappropriate. So, I think I will stop here and thank you very much for your attention. We'll be more than glad to ask questions. Well, thank you very much, Michael. That's a fascinating talk. Always pleasure to hear about your work. Does anyone have a question? I think David is asking. I think David is asking. Do you want to open your mic, David? Sure, sorry. Yeah. So, my question was: on this continuous perspective of graphs, is there any way to get a continuous analogy of the WL lemma? Something that would look like a manifold isomorphism test? I don't know if it even makes sense to ask that question. Yeah, good question. So, first of all, I think I don't have a First of all, I think I don't have. So, this is a really surprising question. So, I don't have an immediate recall answer. So, in case of manifolds, probably what you want to look at is not isomorphism, it's isometry, right? So, it's probably will be some form of metric equivalence between manifolds, so some form of embeddings. So, here the answer is usually negative. So, usually, you cannot represent a graph or a manifold in some fixed space, even though you can approximate. Even though you can approximately do it, so results like Bougain's lemma or the Johnson Lindage charge, right? So, with you can represent some object, some metric space, for example, in another space or in Euclidean space, even with a certain bound on the distortion and a certain bound on the dimension that this bound is lograpec. I don't know. I don't know. It's range. Okay. Maybe just a quick question. What about this idea of getting a better embedding into, say, an ambient space where you realize the isometric properties of the graphs? I mean, how would this actually improve on the performance of some of these systems? Because you mentioned that in kind of the second project, I think. Yeah, it's a good question. So it does improve. So, and well, we see. Improve so, and well, we see it also empirically from other works. So, among these exotic spaces, hyperbolic spaces have become popular in some applications. And the reason is the following. And there is actually a field in traditional network science that is called network geometry that actually tries to study graphs as continuous objects. In particular, in hyperbolic spaces, so for these small world type of graphs with exponential volume growth, you can show that you can produce such graphs. Produce such graphs as nearest neighbor graphs in a hyperbolic space. And you can see actually that, so hyperbolic space is a continuous analogy of a tree, right? So if you think of the volume, the expression of the volume of a sphere in the Euclidean space, it depends polynomially on the radius and exponentially on the dimension. In a hyperbolic space, right or in such graph, it depends exponentially on the radius. Exponentially on the radius. So it is difficult to embed such objects like trees in Euclidean spaces. Basically, they are too crowded. That's why you need to go to a high dimension. So in capabolic space, you can have a low dimension and there is enough space somehow for your neighbors to fit in. And this was shown to be, I think, NLP. People like these kinds of embeddings for different ontological structures or knowledge graphs or whatever. Or knowledge gaps, or whatever. And there are multiple works. So, of course, it involves certain exotic kinds of optimization on monuclide manifolds. But what we see also, we tried actually hyperbolic spaces in blend. So we've seen some advantages in the sense that you usually need less dimensions to get equivalent results, which is probably in line with what is observed in other works. Okay, well, that's great. Okay, well, that's great. I don't know if anyone else from the audience has a question. Open it up.