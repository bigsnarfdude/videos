Yeah, can you hear? Can you see me? You can see the talk. Okay, very good. Very good. Okay, so first of all, I'm really sorry for not being able to be there. I'm quite envious. There, I'm quite envious to be honest. That you are there, right? It promises to be a very nice meeting, right? Due to voice, can you hear me? Yeah, shall I go on? Shall I go on? Okay. Well, try to interrupt me if you don't hear me because I'm not really sure how well you hear me. Right. So as Daniel was saying, our idea as organizer was to have these introductory presentations trying to, well, meant mainly for students. Meant mainly for students, but also meant to emphasize the different cultural differences, let's say, between the more physics and the more mathematically oriented people and trying to bridge an understanding of the other community. Right, so tellingly, my title is More Ambitious. The title is more ambitious than that of Daniel is about the mathematics of soft matter. While in the end, it will be much more narrow, just about pneumatics overall and a very focused and rather heavily biased perspective on the the mathematical issues that are of interest in matics. Okay, so um Okay, so this is the real thing. Well, nearly real. This is really a simulation of Claudio Zanoni at the molecular scale. So this is how the molecules are supposed to look like, these rod-like molecules whose specific feature is that the local alignment. So if you look on a small region, you see that they are more or less in the same direction. So, radar comprehensive way of describing this is through these probability measures that assign a number between zero and one to each region in the unit sphere. And the idea is that this number tells you the probability of finding vector, finding molecules pointing. Uh, finding molecules pointing in that region, right? And then, since these are rod-like molecules, you'll have as many molecules pointing in the region A as in the region minus A, right? So we have this required mu of A is equal to mu of minus A. And of course, this is a rather complicated description to have at each point in space this problem. Probability distribution functions. So, the genius of DeGen was to say that you don't really need the whole of it, but it suffices to have a moment. And because of that symmetry that I mentioned, the first order moment is gone. So, the next most relevant Next, most relevant moment is the second-order moment. And since you have here a tensorial situation, this is something like this, P tensor P, and then you take the average over this measure. And in here, this is a 3x3 matrix whose components are Pi Pj for Ij going from 1 up to. For ij going from one up to three, and then p is a unit vector. So if this is the uniform distribution, so there is no preferred direction, this is precisely one-third of the identity. And thus, if you normalize this second-order moment by subtracting one-third of the identity, what you obtain is a rough description of the The second of the alignment, right? And in particular, if you have a uniform distribution, then this becomes zero, but you can also have this non-zero, a zero, even if you don't have a uniform distribution. So you are losing a lot of information. If you were to think of a periodic function, you identify on the Identify on the unit circle, you are taking out of the Fourier coefficient just the second Fourier coefficient and say that that describes it all. So you are losing a tremendous amount of information. Nevertheless, you still get a pretty good, pretty predictive theory, apparently. And okay, and then the acute answers are referred to by their referred to by their um optical um optically well because of optical reason let's say as being isotropic if it's all components are zero uniaxial if it has two eigenvalues and biaxial otherwise okay and there is this issue if you go through this uh say um statistical uh Statistical description of the Q tensor, you'll have that the eigenvalues should be located between minus one third and two thirds. So they are bounded and maintaining this through suitable theory is challenging. It can be done with this Bohmat-Jundar potential that was designed, was mentioned. That was designed, was mentioned before, and actually, it was also seen by other people before Slastikov and Fatkulin, and even before that, Slukin and others. And it's not clear, though, how important that is from a physical point of view. Of course, it's important that it's bounded, but whether you need those precise values is this. Precise values is disputable. Okay, so this is the Jens theory. You can embed this theory into another very popular, well, you can embed in this theory another very popular one, Ericsson's theory, which just works with uniaxial Q tensors, and where essentially you just, I mean, if you have uniaxial Q tensors or you have two equal eigenvalues, you play a bit with the spectrum. The spectral decomposition of 3x3 symmetric matrix, and you see that you can write such a matrix with two equal eigenvalues in this special form where you have a scalar and then is multiplied by a matrix of a special type, n tensor n minus one third of the identity, where n is a unit vector. And then you have within this theory, you can have this. Within this theory, you can have this Ausin-Frank theory where S is essentially a constant, and then you are just using these unit length vectors. And really, it's important to point out that this kind of representation with the matrices automatically encodes the fact that you do not distinguish between head or tail of a molecule, because if you replace n by minus n, you get the same thing here. Minus and you get the same thing here. And now you have at least three different theory that attempt to describe at a continuum level, say the situation that we saw earlier. And the trouble is that, in some sense, there will be an overlap of what the theories are predicting, and there will be differences as well. What we would like to understand mathematically is if you take a theory, what it can predict, how is it related to another theory, which is quite different from the way physicists usually think because they can switch between theories very clearly because they always think of rather concrete situations in which you apply the theory. So they have a very good Theory. So they have a very good understanding of when a theory can be applied and what are its limitations, which is not the case with mathematicians. We start with a theory and then we try to work through it like those are the axioms, right? And then, of course, each theory uses different types of modeling. So in this first, in the Q-tenses theory, you have In the Q-tenses theory, you have a 3x3 matrix, which is symmetric and traceable. So, overall, you have five degrees of freedom. In here, you have two plus one, three degrees of freedom, and two degrees of freedom here. So, different theories are able to predict different things. And indeed, the Q tensor is richer because you don't predict just the pneumatic, but The nematic, but also you are able to predict the transition right from isotropic to pneumatic in particular, and you allow also for biaxial states, right? So you get a richer theory with a richer descriptor, but you pay a price. Yeah. So the issue is then what is the appropriate descriptor? What is the appropriate descriptor? And the real answer is for what? I mean, it's another question, right? So, appropriate depends on what your aims are. And again, for physicists, this is quite clear, while not so much for mathematicians. So it's important to have an idea of what you are trying to describe in order to formulate the. Describe in order to formulate the appropriate mathematical problem. So, in this case, when I talk about choosing an appropriate descriptor, is this issue of point defects that appears again and again in different reincarnations in different theories. So, if you have something like this, so you have like x over mod x, this would predict different things. I mean, the Different things. I mean, the way you'd match this into different series would be different. So you have this hedgehog-like defect in the Aussie-Frank, while you have a more complicated structure in the Q-tensor theory. And there is also this perspective that the Q-tensor using a more refined descriptor is able to see into finer scale. Scales. So the Ausin Frank is more macro scale, where Q-tensor is more mesoscale. That's same. And indeed, if you use more complex order parameters like the Q-tensor, you don't have constraints on the target, you don't work with functions that take values into the sphere or in a manifold. Or in a manifold, and that releases the topological constraints. So, in particular, if you have a situation like this, you have constraints in the Aussian-Frank theory, you have this degree one constraint. Well, you can happily avoid this singularity in the Q-tensor theory because you have a real. Test a theory because you have a richer or richer target space. So, this is one way of avoiding the singularity, particularly through anisotropic melting. Nevertheless, there is some topology left in the Q-tensor representation, and this was quite heavily used both by mathematicians and physicists, let's say. Let's say here. I mean, also the references that I'll make are very limited. There has been quite a lot of progress in the recent years. So again, I'm sure to have missed a lot of work and people, but again, this is supposed to be an introduction, again, heavily suited to my tastes. Okay. Okay, now the equations. What are the equations? And in here, again, the simpler, the better for everybody, in the sense that the more complicated equations you have, the more source for troubles, because there will be troubles at the computational level and that what the theory can predict. Right. And now this. Now, that's well, my favorite equations, let's say, are these ones which are kind of as simple as that, yet are able to predict something reasonable. And they come out of minimizing this energy functional in which you have this elastic part that is responsible describing the spatial variation and this spa term that is describing the isotropic. Describing the isotropic nematic transition and still able to describe both nematics and isotropics in the same framework. And in here, we call Q-tensor just this symmetry symmetric and traceless matrix. This in particular is not able to capture these Capture these constraints on the eigenvalues. But, okay, well, so in physical cases, maybe you have certainly that Q is bounded, but not necessarily with the eigenvalues where the statistical description would want them to be. And then you have the corresponding system of Euler-Lagrange equations, and we usually put And we usually put these kind of boundary conditions which are chosen such that you are precisely at the boundary in the minimum of this bulk potential. Essentially, because, well, it helps if you study the singular perturbation problems that well, many, most people are studying, let's say. So if you put If you put, let's say, we put these boundary conditions, yeah. And now, the issue of defects, this is the central thing because this is what gives the name of this kind of pneumatics, these threads that are appearing, and various theories are trying to explain them. This is quite You know, well, quite uh an ugly, let's say, uh, picture, but it's from the it's from the 70s. And the good thing for me was that in the paper, it was telling me what is the physical scale at which this appear. And I learned the hard way that the physical scales are important and something that mathematicians usually totally. Mathematicians usually totally ignore is this non-dimensionalization part because, well, it's about telling you what mathematical problems is interesting to solve, namely which mathematical problems has a meaning. And in this case, it turns out that if you take the physical constants, you try to plug in what DC. You try to plug in what these things are, these constants, and you try to non-dimensionalize in the units relative to the size of the domain. And what does that mean? That means that I am trying to say that this is unit lengths and then see what kind of units, what kind of non-dimensionalization I obtain if I want to declare that, say, this square is of size one. Square is of size one. And the point for doing this is that I'll see some sharp transitions. I'll see high gradients if I do this kind of non-dimensionalization, as opposed to doing a non-dimensionalization in which I'm looking at around the defect. So bottom line doing this kind of non-dimensionalization at the end of the day in units relative to the Units relative to the size of the domain leads you to an elastic constant which is much smaller than one. And the limit when L tends to zero has no physical meaning by itself, but the point is that in a physical context, you have L very small. And then the interesting mathematical question is to see what is the limit. To see what is the limit when L tends to zero. And what, yeah, and it turns out that you can get the Q-tensor theory out of the Zinfrank theory in this limit, but the more interesting thing is what you are left with, namely, you are losing. You are losing from the Q-Tensor theory, which has five degrees of freedom, to the Eusian Frank theory, which has two degrees of freedom. You are losing three degrees of freedom, which are kept, let's say, in the next order asymptotic expansion formally. But it is interesting to understand how, I mean, what the theory can predict using these. Theory can predict using these three additional degrees of freedom. And it's clear what it can predict for the defects, right? There is this more complicated picture of in the core of defects, but maybe there is something more to it. And I'll try to explain a bit what the mathematics can guess, whether it has a physical meaning or not. That's a different story. But again, But again, it's about what the model by itself can predict. And physicists have a good feeling of when the model is losing its meaning, but as a mathematician, that's not, well, at least it's not for me the case. Okay, so I just wanted to say here that you can choose also different non-dimensionalizations, which are more common in the physics literature. But those are non-dimensionalizations. But those are non-dimensionalizations which really say that this kind of picture is of size one, and you see that the defect is of size one, right? So if you do a non-dimensionalization where you take units of the biaxial coherence length, as it's said in the physics literature, you won't be able to, L will not be large in this kind of L will not be large in this kind of, will not be small in this kind of non-masterization. So, particularly you won't be able to see defect, but just the defect course. And that's from a mathematical point of view, the most studied problem is this singular perturbation, namely when you put this to 10 to 0, and then it's relatively easy to see that you get, at least formally, At least formally, the Q-tensor theory because you can subtract a constant from here and make this greater or equal than zero. And it will be equal to zero precisely when this is a minimizer of this bulk term. So it has this special form that we take as boundary conditions in order to avoid boundary layer problems in this formula symptotics. And well, then we usually look into understanding this as in Frank limit, and you see that you have convergence in an energy sense, let's say, to this Q naught, which will be an energy minimizer of just the gradient in this space where Q is. where Q is just of this special form, n tensor n minus one-third of the identity, with S being the fixed constant that makes Q into the space of into the minimizers of the bulk term. And then you see that if you take something of this special form, then this gradient of Q squared reduces to something very familiar gradient of N squared, which is. Familiar gradient of n squared, which is what you get in the Aussian Franc with equal elastic constant, right? So, this is formally what how you get from Q-tessor to the Aussian Franc. Now, each theory has its own troubles. The troubles of Aussian-Frank are related to the fact that you can only have point defects, namely, defects with finite energy. Defects with finite energy are just the points, and defects by defects in the Aussian Frank I mean discontinuities of the vector. And one of the main issues is that there doesn't seem to be a generally agreed definition of defects in the Q-tensor theory. And then you can try to make this convergence a bit more quantitative and what you and what you can almost what in some sense actually prove is that you are close to this Ausin Frank prediction is close to the Q tensor prediction everywhere so close means further L except for the small region the defect Small region, the defect core in which you see these Ausin-Frank defects. So, from this point of view, the Q-tensor theory is able to recover the Ausin-Franc 1 and the defect, but it's not clear what it brings in addition, right? In particular, is the Aussian Frank theory able to describe? able of to describe the the the more complicated defects, the line and the wall defects. And in order to answer that is you need to clarify what do you mean by line and wall defects in the Q tensor theory. And well for one thing you know that near the point defects you have this specific Specific feature of the acute answer theory, namely biaxiality. And now this biaxial structure is expected to be universal. There has been a lot of progress in the recent years on this issue of this structure. Including people who are at the conference. But still, I would say that this issue of universality is not yet settled in the sense that it's not clear if, I mean, it's not clear in the sense of getting just out of the equation this fact that the The baxial structure appears near a point defect. For instance, in the using Frank theory and one elastic constant, you can say that the point defects are always of the form x over mod x. And okay, now other things that are still not understood, although again, And a lot of progress has been made, especially through the work of De Pascuale, Mio, Pisant, through concerning the axiometric energy minimizers of solutions of the system that are those that describe these. These presumptive universal structures, yeah. So the biaxial core. But I would say the issue is not yet settled, in particular this universality and the role of the so-called Yuksutov constraint in the sense of understanding. Sense of understanding when you have and when you don't have this isotropy. So when Q approaches zero. So again, if you don't have isotropic melting, then you can have more topological information and you can say use. Say, use something stronger than this Luxodo constraint, namely that Q is nearly constant in norm. And there are various. There is a lot of work, admittedly, where this is proved, the lack of isotropic melting is proved again just out of the equations in certain regimes. In certain regimes. Okay, now again, what so? Still, what are the defects in the Q-tensor theory? And there is this paper which was not published, was not cited at all, really, until a few years ago, of the gene, where he was trying to explain what the defects are and Essentially, assuming that I read it adequately, he says that defects are discontinuities of eigenframe. So what is the idea? The idea is that a matrix can depend smoothly on a parameter yet have this continuous eigenvector. So this is an example which is drawn here, where drawn I mean that you identify a matrix with That you identify a matrix with parallel ellipad, and the eigenvectors are the directions of the sizes of the parallel piped, and the eigenvalues are the lengths of this parallel piped. And then you see that on y is zero, you have these eigenvectors. Yeah, so OX and OY say, and on X is equal to zero, you have these eigenvectors which are rotated with 45 degree, right? So if you were to go towards zero along this line or along this line, you'd see that there is a discontinuity here in eigenvect. And that's okay in terms of Q tensors because here at zero, you have two equal eigenvalues and then you have that any. That any pair of perpendicular eigenvectors would work. So essentially, that's what he was defining to be eigenframes. This is a very awkward definition, both in terms of expressing it analytically and in terms of working with it. With it, and nevertheless, it seems to appear in numerical simulations, you see this kind of thing happening. This is like a half of index, one half defect. And one necessary condition for this to have is that you have eigenvalues becoming equal, and that's easier. And that's easier to check numerically, still convoluted to check analytically. And nevertheless, you can have the so-called eigenvalue crossing. So two eigenvalues become equal, but you don't necessarily have eigenvector discontinuity. So this is an example where you use now ellipsoids instead of parallel pets, and you see that you have. Parallel repets, and you see that you have the same eigenframe. So these standard say E1, E2, E3 eigenframe, but the different values of the eigenvalues allow you to transform from this, say, thin parallel pad to say a fat one where you don't have three equal eigenvalues and then. Have two equal eigenvalues and then back to another scene one but with a different orientation, all throughout keeping the eigenframe, right? So only the eigenvalues vary. And well, so you can have this eigenvalue crossing without eigenframe discontinuity, so it's not clear what is the physical meaning of this definition. This definition, and if well, there is some sort of measurement of this eigenframe discontinuity, in particular if it has any optical implications. One measure of defects is related to this baxiality parameter, which is this quantity usually is. Um, quantity usually is it's one minus six trace cubed to the third square divided by this, and then it's a number between zero and one. This is slightly better because it's an analytic function of q, so this is greater or equal than zero and overall, and zero if and only if is isotropic or in the axial. So, um, and well. And well, this is one possible criterion for eigenframe discontinuities in 2D. Yeah, not really used, but principal work can try to check this. Now, what mathematicians have been focusing on are these point defects in 2D and then you have You have the director representation, and then there is this topological theory, which says that you should count how many times the director rotates as you go along a full circle. And here is where you can see a difference between using unit length vectors or line fields. Line fields, say you have these defects of index one-half, of which here is positive and negative indices or have defects of higher index. So, this is in the in the director theory, if you try to look at this into the Q-tensor theory, the main benefit that you get is that these singularities. Is that these singularities which have infinite energy will have finite energy? So they will be mollified. So corresponding to the defects of index one half and minus one half, you have these modified Q tensor versions. And the connection, the visual connection between this representation and the one that was before is that you look at the That you look at the optical eigenvector, so the eigenvector corresponding to the largest eigenvalue, so the largest size of the paraliped, and squash everything onto that to identify the paraliped with the largest size. And then you get through this say projection, formal projection, the Formal projection: the pictures that were before. And still, you won't get here in the core the infinite energy, but you'll have finite energy. So, this is the main benefit of using these Q-tensors. But on the other hand, this flexibility of the Q-tensor comes with the problem that it can produce a lot of additional. Use a lot of additional solutions. So, multiplicity of solutions is predicted by this Q-tensor theory, and it's not clear if it's because you are using a wrong description, or these things actually happen. I mean, there are some instances where you can see that different types of Defects predicted by the Q-tensor that are not necessarily there in the Aussie-Frank theory. Okay, so this is these are instances where you can have for the same boundary data different types of solutions. So this freedom. So this freedom that you have with the Q-tensor can be troublesome, right? It produces several results and you don't know which one is the right one. But from a mathematical point of view, the most studied question, let's say, is stability. And this is, as much as I understand, important auto-physically because only the stable solutions are those that you can actually detect. And from a mathematical point of view, the interest is in the fact that the stable solution, these point effects are simpler if you have this kind of answers, which corresponds to having. Which corresponds to having index k boundary data, then the whole system of PDs reduces to a system of ODs. And these are coupled ODEs through this B term, which is the one related to trace of Q to the third. Otherwise, they are almost uncoupled except for this U squared plus V squared. And then v squared and then all the system doesn't really see this dependence on the boundary data this index yeah and this b makes a difference from a qualitative point of view namely you can show that the the u and v are monotone with the v increasing or decreasing V increasing or decreasing depending on how large V is compared with the other parameters A and C. And then the stability issue is about the second variation, which is this thing. So you have an arbitrary P, and here Y is the solution, the symmetric solution, and then And then you have to see if this is positive and in what conditions. And the short answers are that it's easier to do it for B is equal to zero because the system is more uncoupled. And then you see that in that case, for any index, for any Yeah, boundary data of index K, you have stability, but not so in the more physical case when you have stability just for the least say complicated point defects. Point defects of the ones of degree one, because for higher degrees, roughly speaking, you have more variation, right? So less stability. What else? Now, this was a story about the small scale predictions. Predictions of the theory, something which is not really studied neither by the physics community, well, nor by the mathematicians are the large-scale aspects, right? So let me just show you something real, which is this experimental movie that shows how you Shows how you quench from a higher temperature into a lower temperature, you start seeing nematics. The colored bits are nematics, the black is isotropic. So as you lower the temperature, you'll get nematics overall. And then this happens through the increase of these islands of nematics that will join and the overall. And the overall at the end, the pneumatics will dominate the whole thing. So, this is the movie. And the claim is that this is a statistically self-similar behavior. So, the pneumatics island will eventually win and dominate everything. So, this is the real. The real thing. Okay. And then the claim is that even as a relatively simple-minded theory, a gradient flow of that an L2 gradient flow of that energy that I showed the before can describe this. There is this There is this paper of Bray where he argues that even this L2 gradient flow is able to describe this statistically self-similar quenching from isotropic into nematic. Well, it's actually nowadays it has much more citation than just this. And roughly the And roughly the claim is the following: that you consider a correlation function, which is this kind of ensemble average. And the ensemble is averaging over the initial data, and it's also the corresponding Fourier transform of this. And then the claim is that you have a scaling form of this. A scaling form of this type. Yeah. And then the length scale depends on the length scale of the pneumatic domains. And he argues there that somehow it depends on the symmetry of the order parameter as to what the right say scaling is. Is and the overall interest in this, for me at least, is that it tells you what is, say, an appropriate description of pneumatics at large scale, not just at small scale, just at defects. Okay, so you have this scaling either for the structure factor or the correlation factor, and these are there are. And these are there are some well the literature seems to be rather limited on this, but there are some simulations where they try to capture this coarsening and quantify it using this gradient flow for the Q tensor and well. Well, this is the claim in mathematical terms that you look at the equation that we had before, but you had now time. And it depends on what kind of descriptor you want to use. If you want to use Q-tensors or uniaxial description, so a poorer description. A poorer descriptor, and you'll get different scaling laws. Yeah, so basically, he says that different descriptors provide different scaling laws. So, this is a macroscopic way of telling you which is a good descriptor, at least for large scale for this kind of for describing this kind of quenching phenomena. And well, what we can what can show What can show is very little, namely, you can use these statistical solutions and reinterpret this correlation factor and see that essentially you have here a gradient flow and you can either have depending you can you can have different types of behavior. have different types of behavior depending on the t on a that appears hidden in here so this is the the scalar picture whether here the isotropic is locally unstable or it's sorry this is locally stable this is locally unstable depending on what temperature you use but idea essentially it's a competition between reaction and diffusion Reaction and diffusion. That's what these two terms are: the reaction, the diffusion, and the reaction. And then you can show that you have a transition between the two. So we are getting, sorry. Yeah, I need to. I'm wrapping it up. Yeah, two or three more seconds than that. So, yeah, if you have small initial data, then diffusion will win and you have a certain scaling. And you have a certain scaling if you have larger initial data than reaction wins, and you have a different scaling. Yeah, so as I was saying, for small initial data, you have diffusive scaling, so the system behaves like a rescaled, like a heat equation, essentially, with a suitable rescaling. And for large initial data, it will be like a self-expanding ball, which is expressed in terms of this. Solitons, let's say, but anyway, overall, using this averaging, you can get one scaling or the other. But still, there is a lot of things to be done and bigger to clarify the questions on this statistical scaling. So, overall, there are different theories and Theories and they predict different things, and they manifest both on small scale defects and on large scales in this coarsening. And the transition from one theory to the other is, well, it's a challenging thing from a mathematical point of view. And yeah, so basically, these are kind of the So basically these are kind of the several questions that are studied. That's about what I wanted to tell you. Thank you very much. Since you are away, maybe there is time for a short question. Otherwise, people will reach out to you. Anyone wants to ask a question? Anybody wants to ask a question, and if it is so, please be very loud. Yes, so if this DFCL, if the defect have a DRCL structure and hope, how would we be able to see this experiment, let's say, using a hospitalized set there for our eyes and also how to visualize the back-exact is there so he knows everything. So he knows everything, so he would be the person to answer that because things also evolved a lot. So I think nowadays they have much better techniques than, well, 10 years ago, say. So yeah, short answer is this is more of a physical question that I don't know how to answer. Very much. Any other question, please? Uh, any other question, please reach out to him. Thank you. Okay, thank you. Bye. Nice evening. Thank you very much. Thanks. Bye. Okay, now I think we've got 10 minutes and we're back for the next talk. But I'm going to be the P as well. So that's what I'm trying to do.