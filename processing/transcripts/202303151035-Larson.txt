So Rm of UV, the rank generating function, is the sum over all subsets of one of your atoms, your ground sets, of u to the rank of your matroid minus the rank of the set times v to the size of the set minus the rank of the set. So this is essentially this. So this is essentially the tub polynomial. If you evaluate at u minus 1, v minus 1, then you get the tub polynomial. So it satisfies a deletion contraction formula and has been studied quite a bit. The first theorem, due to how do you perceive to cats, is that if you evaluate rm at u, minus 1, then you get the resulting polynomial has all good k coefficients. And in fact, this is true even if you divide by u plus 1. So it's stronger than logic of cavity. The second theorem, proved independently by Anari, Liu, Garin, and Vidzan, and then by Brandon and Huff. And then by Brandon and Huff, is that if you set Rm of u0 is equal to, I'm going to write the coefficients sort of backwards, AR plus AR minus 1u plus A0U to the R. Then you have ultra-local cavity. So you have the statement that AK squared is greater than or equal to is greater than or equal to k plus 1 divided by k times n minus k plus 1 divided by n minus k times kk minus 1 times k plus 1. Those are two theorems. Can you read that again? If you take Rm of U0 and you, this is some polynomial in U, and so you write it as AR plus AR minus 1 times U plus through A. 1 times u plus through a0 times u to the r, then these a coefficients, which are just the number of independent sets, satisfy this ultra-low kinetic law. You said a i's are the numbers of independent sets. Yes, yes. So ai is the number of independent sets of our kind. So those are two well-known theorems about matroids. I'm going to talk about delta matroids. So let me first define a delta matrix. Delta matrix. So let me first define a delta matrix, although versions of delta matriates have appeared in Tong and Amos Cox. So I'm going to, in order to define a delta matrix, I'm first going to define a notion of an admissible set. So delta matriates are considered, at least one way to think about them, is that they're related to subsets of n and bar. So what this means is that you have a ground set of size 2n. Ground set of size 2n, and it has like a natural fixed-point-free involution i. So its elements are 1 through n, and 1 bar through n-bar, and then somehow i curved related to i-bar. And then we say that s is admissible if it has no i and i-bar. So it doesn't contain both i and i-bar for some time. For some. And I'm going to let ADS is equal to the set of admissible sets, admissible subsets of one group of N and R. So, given this, you can define a delta matroid. And like matroids, there are several different definitions of delta matroids. I'm going to give one that's quite different than Give one that's quite different than the definitions that have previously appeared. So a function g from the set of admissible sets to R is a delta matrix is a delta matrix if it satisfies four properties. So these are supposed to be similar to the rank generator. These are supposed to be similar to the rank generating function, or the rank definition of any trade. So the first property is like a normalization property, that g of the empty set is zero. The second property is like a boundedness property, that the absolute value of g of i, okay, let's say s, is less than or equal to Is less than or equal to 1 if the size of s is equal to 1 or less than or equal to 1. The third property is more interesting. It's a bias of modularity property. So it says that g of s plus g of t is greater than or equal to g of s intersect t plus Plus, and now here, if I were talking about submodularity, then I would put G of the union. But there's kind of an obvious problem with this, which is that the union of two admissible sets need not be admissible. For example, you could take one and one bar, where the union is not admissible. And so you replace this union by the minimal thing. So it's, I think the convention is to denote it disjoint union. So what this means is that S union T. Is that S union T is you take the union, and then you might have some I and I bars, and then you throw out the, if you have both I and I bar, you throw them both out. So this is equal to, yeah, I guess one way to write it is that it's E in S U D T such that E bar is not in S U D T. And here, E, you know, it could be either I or L, it's just some element. Okay, and then the last property is a parity property. So it says that G of S is congruent to the size of S mod 2. There's not really an analog of this for for matroids, although these are sort of uh close to the rank generating the rank definition of matroids. Yes. Yes. So this function g can be negative. So let me make one observation about this bias and modularity property, which is that if, rather than considering all admissible sets, I look at just subsets of some fixed set. So for example, subsets of 1 through n, well then this spatty union is the same as like the ordinary union. So when you restrict to like 1 through n, then this is just Restrict to like 1 through n, then this is just a normal submodular function. And then it's like a standard fact about submodular functions that if you have a submodular function that has its boundedness condition, then you get like a bound on its absolute value of all sets. So the upshot is that the absolute value of g of s is less than or equal to the size of s for all sets s. Okay, so then, you know, somehow with matroids, when you're interested in like bases or independent sets or something, you look at the values where the rank function is as large as possible. And then you can do that here. So I can define f, which is known as the set of feasible sets of my double matrix G, to be the set of admissible sets such that Admissible sets such that g of s is equal to n. So notice that an admissible set the size at most n. And so this is like the largest possible subset. And these are called the feasible sets of the delta matrix. No absolute value here? Yeah, no absolute value. And using this, you can connect to the definition that was, at least in EMS Huck, which is that the conduct hull of the indicator function of the feasible sets is, well, it's a lattice polytope, and it has all edges parallel to EI or EI plus or minus Ej. Um or EI plus or minus E j. So the type B roots. So here this EB, if B is like a sign subset, then this means if you have something that has a bar on it, you put like a 1 and you put a minus 1. So for example, like E12 bar is equal to Is equal to line copy. So these are all vertices of the plus or minus one cube. And in fact, this property about this polytope and its edge directions, it characterizes altimeter. So it's a little bit annoying to give examples using the strength generating function definition, the strength definition. Function definition, this rank definition. But using this polytope definition, it's easier to see things are delta matrix. So for example, if you have some matroid, you can take the matroid polytope and view it as the delta matrix. So this is the convex hull of E B union B complement bar. Complement bar, the B basis of m. So, in other words, what you do is you have the bases of your matrix M. So, these are a bunch of subsets of 1 through n of size, you know, r or something. And then for each of those, you construct a missible set of size n by you, you know, maybe phi that's not in your basis, and then you add in phi bar. And so this is a delta matrix or a delta matrix. And so this is a delta matroid or a delta matri polytope. But you can also take the independence polytope of a matroid. So this is the same thing, except you do it for all independent sets. But of course, there are many other delta matrix besides these two sets. So just a question first of all, when you have effectively homogenized, so it seems like you can't move in EI from the plus Ej direction unless you restrict to the Direction, unless you restrict to the first time elements. Is that what you mean? So that's right. So this guy has the additional property that all the edge directions, you don't have any EI plus, I think EI plus EJs, you don't have any EIs because you can, these, right, these symmetric exchanges are EI plus EJs. But I'm asking, like, if you harmonize, when you spend a complement, the sum of your coordinates is going to be n. Is going to be head. So you cannot move the EI plus either direction unless you restricted yourself to the first step orders without your cars, for example. Yes, yes. You're completely right. You only have VI messages. In this example, yes. Uh yeah. Yeah. So let me just write down like the corresponding rank function, maybe for this example. So if you have, let's say you have some set F's admissible set, and then it has an unbarred part and like a barred part, and then there are some other things that are neither barred nor unbarred. So you can express that as saying that 1 through n is equal to Reson is equal to I disjoint union S plus disjoint union S minus R. And then the rank function is equal to, for this IP of M, is equal to the size of S plus 2 times the rank of S plus minus 2 times the size of S plus. 2 times the size of S plus. So maybe not the first thing you would write down, but also not so crazy. So now that you have this G function, you can define the analog of the rank generating function of the matriarch. So this was introduced in a paper with Chris Err, Alex Fink, and Hunter Sting last year. So we could define u D, the U polynomial of Uv. u polynomial of u v to be the sum over all admissible sets of u to the n minus the size of s times v to the size of s minus g of s divided by 2. So this is sort of a reasonable thing to write down because you have this bound on the side that has the value of g b s. And this divisible i2 is And this divisible light too is reasonable because this parity recognition. Alright, so um got it. Okay, so that's that's something that you can consider. It behaves pretty like the top polynomial. It satisfies a deletion contraction, and then there's this third operation on delta matrix called projection, sort of a tuft polynomial like recursion. If you evaluate at If you evaluate at u, v minus 1, you get something like non-negative coefficients, which is not obvious from this definition, but some sort of nice invariant delta integers. So now let me state a theorem about it. So the so the the first thing is the first. The first theorem, so at the moment we're unable to prove any sort of log of cavity property for this polynomial, for all delta matroids. But we can do it with some additional assumption on your delta matroid. So let me not say what this is, because it would take a couple minutes. But let me suppose that some delta matrix D has an enveloping matrix. So, okay, so this is some condition. Okay, so this is some condition. And whatever it is, it's true in these examples, and it's true in, like, we're able to verify it in many other examples. Then we could show that the polynomial uv of u minus 1, if you write it as v naught plus v1 times u plus vn times u to the n, then we have an inequality that's slightly stronger than the cavity. So b k. So bk squared is greater than or equal to k plus 1 over k times dk minus 1 dk plus 1. So similar to how for the values of the rank hearing function at u comma minus 1, like something slightly stronger than the moment. And then you can also show that if given a similar but A similar but slightly different technical condition, which I'll call D having an enveloping matroid star, enveloping star matroid. Star matroid. Then if you look at u d of u zero, so this thing that's sort of like the independent seturating function, and you write it as like c0 plus c n u. C n mu to the n, then you have an ultra-logging calendar saving. So you have ck squared is greater than or equal to k plus 1 over k, I should have said, you write the coefficients backwards, times 2n minus k plus 1 over 2n minus k times c k minus 1 times c k plus 1. times CK plus 1. So similar to the rank generated function value at U0, you have something, some sort of plus 1 kind of thing. I think I'll call it actually. We're going to short this tank schedule of this one. So maybe one question and then how much Can you say one sentence about how you get those results? So let me say this one because it's more interesting, which is you construct, it's sort of similar to what Chris was talking about, but the same sort of technique. You construct some vector bundle that in the realizable case, there's some notion of realizations of delta matroids. You construct this vector bundle and it has some positivity properties. Bundle and it has some positivity property, you do some intersection theory and apply the Klobansky-Tesca inequality, and then this falls up. And the role of this hypothesis is that in general, these classes that we construct that were intersecting, we have no idea how to access any positivity properties for them, except when there's this enveloping matrix, when you can somehow relate these classes to the Bergman fan, essentially the Bergman fan of this enveloping. Essentially, the Bergman fan of this invalid nitrogen will then deduce positivity properties from. You can make conjectures about positivity properties. Yeah, so certainly I think it's empirically true and quite reasonable to conjecture that these statements are true for all Delta meters. Or realizable for itself. Yes, and it's uh much larger than that. Okay, let's uh take that yeah. Okay, what's the penman? 