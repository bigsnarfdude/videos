All right, so our next speaker is Jess McIver, and she's going to talk about new methods for gravitational wave data analysis. Thanks very much. Yeah, it's great to be here. Good morning to everybody who's there. Everybody who's there in the meeting room and everyone who's joining from around the world. So please don't hesitate to jump in with any questions or discussion. It would be great to generate some discussion from these slides. I see we have ScienceCat joining us here too. So I'm going to take you on a tour of what the group at UBC has been thinking about. Has been thinking about and working on in terms of data analysis. And we're going to start by taking a look at the implications from our latest LIGO Virgo Cogra results. So probably most of you have already taken a lot of note of the 39 new events that we observed, bringing our total up to 90 with a lot of dominant presence. Dominant presence, let's say, in this high-mass regime for in terms of solar mass black holes. So, a really interesting question about this, which we've been looking at some, is how do these form? And in case you're not familiar with some of the thinking about the processes, so it's a bit of a problem to form a binary of compact objects, so whether those are black holes or So, whether those are black holes or neutron stars, and to get them close enough that they'll radiate away gravitational waves and merge within a Hubble time. So this is an outstanding problem in astrophysics that we're hoping that we should expect even gravitational waves and the signatures of the masses, spins, and eccentricity of gravitational wave signals will reveal something new about how these. Something new about how these are forming. But there are two channels that I wanted to talk about, specifically focusing on this spin signature that could be really illuminating in terms of how these are forming. So one is this common evolution. So you have two stars that are born together, that live their whole lives together, and then are then in this compact binary as stellar remnants after they die. After they die as stars. So there are various ways you could think of reducing the separation between them to get them close enough to merge. So one of those is what's called common envelope evolution. But essentially, the signature that you'd be looking for if these are evolving together is that you get this preferential spin that's aligned. Hello, that's aligned with the orbital angular momentum of the system. Meanwhile, on the right, Meanwhile, on the right, we have an example of dynamical capture. So, this would be the case where you have two objects that are already compact objects, so stellar remnants, black holes, neutron stars, before they form this tight binary. And we expect that you might get more of an opportunity for capture events like this to happen in really dense environments like globular clusters or maybe close to a supermassive. Or maybe close to a supermassive black hole. But the spin signature that you'd expect to see in those cases is isotropic. They'd essentially be, there would be some fraction of our gravitational wave signals where we would see this these misaligned spins where the spins aren't necessarily aligned with the orbital angular momentum. So thinking about this, looking at the spin signatures in particular, so I wanted to show you one. Um, so I wanted to show you one of the plots from um, so this is from the main catalog paper GWTC3. So, this population of uh spins, so spin information from these 39 candidates. So, plotted on the y-axis, this is the effective in-spiral spin. So, this is the mass weighted spin that's aligned with the orbital angular momentum of the system. So, one is aligned, and minus one is anti-aligned. And what you're seeing. Lines and what you're seeing here is that really the vast majority of these signals are consistent with non-spinning systems, but there's this huge uncertainty in terms of being able to really exclude spin that's maybe not necessarily either spinning really highly or being able to exclude systems that are not spinning at all. And I'll give you a little bit of a And I'll give you a little bit of a concrete sense of this with some six interesting examples where we were able to recover the spin fairly well, either in magnitude or in angles. This is what we're calling tilt here. So if you haven't looked at these before, essentially the way that you read these, so you have zero to one. So this is the magnitude of the spin, where zero is non-spinning and one is maximally spinning. Is maximally spinning, and then you'll see these two different lobes here. So the one on the left, so this is the primary mass, the heavier of the two, and then this is the secondary or the lighter one. And then the angle that you're seeing, so this is one is aligned with the orbital angular momentum and 180 is anti-aligned. So you can get a sense of this for each one of these two objects. And what I really want to point out here is that even for systems where we are doing Systems where we are doing fairly well with estimating what the magnitude of the spin is, there's still this enormous uncertainty in terms of the tilts. And this is especially true for the secondary mass, the lighter of the two. So with that in mind, I wanted to highlight some predictions that a student in my group has been working on to think about how much better we'll be able to constrain spin, this important signature, in terms of binary evolution. In terms of binary evolution with these near-future detectors like A. So, what you're looking at here, so essentially, what he did was a series of simulated gravitational wave signals injected into Gaussian noise, and then we're looking at how well we can recover them with different assumed noise curves. So, here we're comparing plus sensitivity, so this is advanced LIGO plus and advanced Virgo plus, with design sensitivity. With design sensitivity. And each one of these dots represents an injection. So there are 10 here for each mass ratio, ranging from nearly equal mass up to 10. So 10 times difference between the masses of the two components. And the color that you're seeing here, so this is the spin magnitude. So they've got equal spin, and the color just shows the magnitude of the spin. And then, so this is showing you the width of the posterior at Posterior at plus sensitivity. And then down here, you see the difference between plus sensitivity and design sensitivity. So essentially, what we're seeing both for spin on the left and also for tilt on the right is that relative to design sensitivity, we expect to do about 50% better at plus sensitivity, which is really pretty exciting. And I also want to show you a new term that Alan introduced into the literature. Into the literature. So, this is this cost function. So, we were just looking at this posterior width as a way to characterize how well we're able to constrain spin and tilt angle. So, this cost function will reduce to essentially the standard deviation. So, looking back at the posterior width, but now we also introduce this penalty, this cost factor, that we were leveraging knowing the right answer from our simulated signals. Simulated signals. So we're comparing the median to the injected value of whatever parameter it is we're looking at. And the greater the difference here, this will introduce an additional cost. So now we're penalizing not only a broader posterior width, but also any difference between the median of the distribution and the true injected value. And this is interesting when we're comparing design sensitivity and plus sensitivity because in this study we are. Sensitivity because in the study we are using the same noise manifestation, just rescaled from design to plus sensitivity. So, what you're looking at here: so, this is a series of injections of binary black hole systems that are all consistent with this first generation and second generation system. So, the assumption here is that these are probably formed in some kind of dynamical environment where you have some stellar remnants and some dynamical capture with a second. Dynamical capture with a second generation remnant. So, this would be the product of a previous merger. So, with that in mind, so all of these have a total mass of 100 solar masses and a mass ratio of two, where the heavier system has a spin of 0.7 and the lighter 0.1. So, what Alan is plotting here, both for the spin magnitude and also for the tilts, is this cost function. So, both for design and green and plus and purple. And plus, and purple, and the difference between the two of them. And this is as a function of injected tilt, so ranging from aligned all the way to anti-aligned. And we can see that we see some interesting differences. And if you dig into the data, into the corner plots here, it's both in posterior width. And then we're also getting some benefit in accuracy from plus sensitivity. And as a nice way to summarize this, so something that Alan did was. So, something that Alan did was inject this realistic population that was drawn from one of the rates and populations curves from a past O3A populations paper. So, he injected these into Gaussian noise and looked at our projected improvements in our ability to constrain the tilts, especially of the primary object. So, what you're looking at here are sources with either moderate incline spins. This is either roughly 45 or Or roughly 105, so about halfway either closer to aligned or anti-aligned. So those are moderate in-plane spin or high-in plane spin, which is roughly about 90 degrees relative to the orbital angular momentum. So you can see this for both design sensitivity and plus sensitivity. And what we're showing here in this margin is our ability to exclude essentially non-spinning systems as we push. Systems as we push this symmetric margin interval away from zero degrees and 180 degrees. So, how certain are we that the majority of our posterior falls within this band as a function of that margin? So, what I want to point out is that at roughly about 30 or so degrees of margin, so squeezing into roughly about 30 and 150 degrees, we can start to become really. We can start to become really pretty certain for roughly half the sources that the tilt is confined in that region. So we'd be going from a regime of a very small fraction of sources where we're able to be pretty confident that there's some in-plane tilts to roughly half of them. And we can expect this, we think, for plus sensitivity. And this will be even a big improvement. Will be even a big improvement over design sensitivity, or we that's where we expect to be in the next observing run. So that's pretty exciting. And then I want to do a topics shift where we're going from thinking about nice well-behaved Gaussian noise to more realistic noise. So thinking about the challenge of detector transient noise. So you've probably seen an image that looks like this that shows part of the menagerie of glitches that we see. Of glitches that we see in the LIGO and Virgo and Kagra gravitational wave detectors. So, I'd like to point out the very different time scales that you're seeing here in these spectrograms. So, we see some very short duration glitches that actually can be very similar in terms of time frequency morphology with gravitational waves. We see very disruptive, extremely loud glitches, but then we also see these longer duration glitches that can be trickier for. Glitches that can be trickier for these longer duration sources. So, especially for neutron star black holes or binary neutron star signals, we can see a good match between some templates that are sweeping up along this front side of the scattering arch here. So, I'd like to talk about this both from the perspective of a challenge for the searches, so in terms of limiting our reach that we're able to confidently detect signals, and then also Detect signals. And then also, towards the end of the talk, we'll talk about what happens when we have signals that are occurring really very close to one or more of these glitches, which is a pretty common case. So if you've read the GWTC3 paper, there's roughly about a quarter of those events that were very close, detected very close to glitches. Well, let's start with searches first. So essentially, the problem here is that for any given Here is that for any given trigger, some event candidate, so this is where we have a candidate with an SNR, this network SNR in blue here. So some candidate we've registered in our detectors, we're trying to calculate the false alarm rate or the significance, and we need to compare this value to the rate at which we'd expect our detector network to produce this by chance, given what we know about the detector noise. So the way that we compute this is completely empirical since our data is non-Gaussian, non-stationary, and impossible to Non-stationary and impossible to model. So, we do this with time slides where we're comparing these triggers here between each one of the detectors in our network, and we're sliding those detector triggers as a function of time in time relative to the other detectors by some number that's greater than the gravitational wave travel time. So, now when we recompute these coincidences, we know that they must necessarily be. We know that they must necessarily be due to noise because we've shifted all of our true signals away from each other. So, we can do this again and again and build up these histograms, like you see here. So, this is how we calculate our background distribution. So, as you might expect, the more glitches that are in the data, the higher rate that we would expect to see these chance coincidences that we're measuring empirically. And let me give you a sense of this for the last observing run. So, this is for O3B in particular. O3B, in particular. This is one of the figures taken from the GWTC3 catalog. And what you're seeing here is glitches per minute. So I focused here on Lego-Henford and Lego Livingston versus time over days. So this is a snapshot of the entire observing run. And you can see that the glitch rates is really quite high, especially for certain regions of time. And this corresponds to weather, particularly waves in the ocean for LIBOR-Hanford. For LIGO-Hanford. LIGO-Livingston also suffers from micro-seismic noise that tends to drive a slightly different type of glitching. But I wanted to give you a snapshot of some of the worst offenders here for both LIGO-Hanford and LIGO-Livingston, which is light scattering. So coming back again to this longer duration noise transient in the data, and this is going to tend to ring up or interfere with signals that have a longer duration themselves. So especially Duration themselves. So, especially stars that are systems that are potentially interesting as kilonova targets. So, systems containing neutron stars. And I want to also zero in here and give you a sense of really how frequent this is. So, we're going to take a look at a particular day now. So, here's a scale of many months. Now, let's take a look at just one day. So, this is an image from the LIGO summary pages, a period of high weather. Period of high weather, and it's also noteworthy that these gaps in the data are also where the detector is struggling to maintain light resonance, also due to this low frequency ground motion. But essentially, what you're seeing here, so for a 24-hour period versus frequency, each one of those dots is a glitch. And they're so frequent when we have bad weather periods like this, that zooming in on what's happening here in this region where you're What's happening here in this region where you're seeing this really high rate of glitches is we're really seeing one of these scattering glitches that can reach well into the sensitive band of the detectors, this really interesting frequency region where we expect to see gravitational wave signal from these compact binary coalescences. We can see that these glitches are happening about once every other second. So it's not surprising then that we would have a greater rate of these chance coincidences and that this is also adding a lot to our This is also adding a lot to our gravitational wave background. So, thinking about this, something that the UBC team has been working on is doing a better job of distinguishing between these tricky noise features that can occur at a really high rate and our true signals. So, I'll take you on a bit of a tour of what people are working on. So, the first project I want to talk about is being led by an undergraduate at UBC, Seraphim Jaroff. So, he's leveraging So he's leveraging this machine learning convolutional neural nets and image classifier called GravitySpy. And essentially, what he's focusing on is trying to use this as a way to safely veto likely glitches. So what he's been doing is targeting, calculating how likely is it that gravity spy would classify a true signal as a glitch. So we're looking at especially the high mass regime where you would expect to see a lot of time for Would expect to see a lot of time frequency overlap, and it would even be difficult for human eyes to distinguish the difference between true signals and glitches. So, this is something of an extreme example below. So, this is a bit higher mass than we may expect to see in the next observing run. But these are the same spectrograms similar to what we were looking at before. So, time and then frequency. And you can see an example of a simulated gravitational wave signal with a very high total mass and how similar this looks. Total mass and how similar this looks to a real glitch in LIGO data. And these are one of our common types of glitches, this low-frequency flip class that happens to be in LIGO Livingston. So what Seraphim has done is do some re-engineering of this training set. So the original gravity spy model that ran in the last observing run only contained about 60 examples of simulated signals. So what Seraphim has done is sucked What Seraphim has done is supplement this training set with hundreds of examples, especially of higher mass simulated signals, and retrain the model. So I'll show you the improvement here, and I'll walk you through each one of these plots. So for both the original model on the left and the retrained model on the right, you'll see two different images. So one is the gravity spy confidence that whatever this event is is a chirp, and the other is that this event is a spirit. Is that this event, this candidate is a low-frequency blip? So each one of these dots is a simulated signal, so injected at some primary and secondary mass. So you can see those on the X and Y axis. And you can see here for the old model that there's this region of low total mass and then high total mass where gravity spy is confusing these true chirps, so these simulated signals for low frequency blips or at low mass. Or at low mass, this is actually scattering. But the retraining was really very effective. So we now have regions where gravity spies confidence is above about 50%, spanning all the way up to about roughly 200 total solar masses. So about where we're expecting roughly the limit will be of what we're going to be sensitive to in the next observing run. And we're also doing better at lower masses as well. At lower masses as well. So, this is a really promising method that we're hoping to employ in an automated way to help us better distinguish between signals and noise in the next observing run. And similar to this project is something else that Greg Ashton has been leading. So thinking about instead of using image classification, but choosing a feature set that is the parameters that we're trying to estimate for gravitational waves. Gravitational waves. So, this is taking known glitches, projecting them into this parameter space with the same Bayesian analysis software that we use to characterize our gravitational wave events, and then comparing this to the population of gravitational waves that we see. So, I'll take you on a tour here of some of these really key plots. So, the distributions that you're seeing here, so these are distributions that are generated. That are generated by running parameter estimation. So, the full Bilby parameter estimation on a set of about 100 different types of glitches. So, each one of the colors is a different glitch type. So, some of these names may look familiar to the same problematic glitches that we were looking at at the previous slides. So, we have blips in both detectors, Tomtis, fast scattering, and then scattered light. And the distributions are The distributions are the normalized sum of each one of those 100 examples. And you can see that there is quite a bit of overlap between these different glitch types and the two different LIGO detectors that we looked at. And then really interestingly, what Greg has done here is project the parameter estimation estimates for real signals detected by LIGO and Virgo. So the hope here is that we're going to be able to leverage the parameters. To leverage the parameter space where we see a lot of separation between signals and glitches to increase our certainty and potentially use this as an additional method that produces some likelihood that something is an event or a true signal versus somewhere in a problematic glitch parameter space where we know we have a hard time distinguishing between signals and glitches. So, this is a really potentially interesting. This is really potentially interesting. We only show a few different combinations of potential features: so, trip mass, mass ratio, primary spin magnitude, and different combinations of these is what's plotted here. But of course, we have many other parameters to work with since we are drawing from these full parameter estimation runs. So, something that would be really interesting to look at and to dig into is producing some sort of a trained classification model that's drawing. Classification model that's drawing from those posteriors and the labeled sets of glitches and then some simulated signals. So, this is something that we're really excited about potentially deploying for the next observing run as well. And then I also wanted to talk about a different feature set that's also images similar to Gravity Spy, but encoding coherence information now with SkyMaps. So, turning our attention to a challenge where A challenge where we're trying to distinguish between signal and noise as quickly as possible. So, here's a case study where this was the first, I think the first successfully issued automated open public alert. So, this went out May 18th of 2019. And there were a lot of elements about this candidate that looked really promising for electromagnetic follow-up. And so, the probability that the system contained a neutron. That the system contained a neutron star was pretty close to 100%. There were also some warning signs in terms of the glitch probability being pretty significant, and also the thinking about the false alarm rate relative to this estimated distance, which is about 10 megaparsecs closer than 170817. But there's a lot about this that does look really nice. So the sky map looks nice and well localized. It seems to be a close source if it is real. Be a closed source if it is real, but it wasn't. So, this was actually due to an earthquake at Lago-Hanford. And what I want you to notice about this image, so this is the gravitational wave strain data and frequency versus time, but this is a much longer duration spectrogram that we've been looking at. And some of our data quality pipelines rely on really well-localized noise sources, and they can struggle with these much longer-duration noise sources. Much longer duration noise sources that are in the order of tens of seconds or maybe even longer. So, this magenta line here, so this is the merger time of the trigger. So, it would have been sweeping right up through this roughly about 30 second long burst of excess power. And so what a student in my group is working on is trying to characterize and identify these, especially these longer duration noise sources that are falling through the cracks of some of our current tools. Of some of our current tools. So, this is a study by Julian Ding. So, he's an undergraduate student here at UBC. So, he's employing this method called the temporal outlier factor to try and characterize these longer duration noise sources. And essentially, the way that it works is it takes some time series. So, here's some gravitational wave strain data here, and it computes this outlier factor by projecting the time series data first into some. First, into some n-dimensional parameter space, and then applying some penalty factor where we're in order to have a strong TOF score, you require your nearest neighbors in your parameter space to be very close together in time. So I'll show you what this looks like. So here is a projection of that time series we were just looking at into some n-dimensional parameter space. And then you're looking not only for clusters in nearest neighbors, but then you're also requiring those nearest neighbors to be. Also, requiring those nearest neighbors to be close together in time. So, this is what our TOA factor is encoding. And if both of those are true, then you will get this outlier in this time temporal outlier factor. So, here's an example of applying this algorithm to this earthquake time. So, here again, we've got this spectrogram and frequency and time, and now we're seeing this TOF algorithm detecting this pretty. Algorithm detecting this pretty well, depending on the scale of the window that we're analyzing. So here we've got dots in red for four seconds, dots in blue for 30 seconds, and dots in black for 120 seconds. And that's just the width of the window time series that TOF is analyzing. And TOF really does a pretty good job of picking this up and picking up the majority of the excess power on the order of the tens of seconds that we're seeing this burst of noise from the earthquake. This burst of noise from the earthquake in the data. So, this is potentially an exciting complement to these tools that we already have that are focusing on these very short-duration bursts that are well captured with event trigger generators like Omicron. And then something else that I wanted to talk about coming back to Skymaps as a really interesting potential feature set is GW Skynet. So, this is a project that a postdoc at Project that a postdoc at UBC, Miriam Cabrera, was working on. So, Miriam is not a or was not before she went to work for Earth Daily Imaging. So, she was not a collaboration member. So, all of the data, the feature set data that you're going to see here is completely public. So, the goal here is to complement the information that's being provided by the LIGO Virgo Cover Collaboration. So, a lot of deep and Of deep and Sean's work that you've heard about earlier, but to take the coherence information that's embedded in these sky maps that we can leverage to try and issue this additional information and estimation about how likely a signal is to be a glitch or a true signal. So, Miriam has added some context here from the past observing run, where we do have examples of even while we're pretty Of even while we're pretty quick with issuing retractions, so on the order of half an hour or even less than 20 minutes, we still have examples where we're seeing electromagnetic follow-up happening sooner than we can retract these events. So this is part of the motivation is that GW Skynet would be able to issue some kind of a prediction pretty much as soon as we have that SkyMap publicly available. So I'll show you some of the performance. So again, this is a So, again, this is a convolutional neural net. So, it's intaking these images produced by Baystar. Also, information about the detector network and the normalization factors produced by BayStar and the performance using trained data just with O3A so far. So, GW Skynet was able to improve on the retracted event rate. So, again, this would be happening at a shorter time scale. A shorter time scale than we've been able to retract events in the past, just based on the last observing run. So, about a 50% improvement. And then also, it correctly identified six out of six unretracted events that were consistent with noise and then not confirmed in later catalogs. So, this is a pretty promising method that a team at McGill is working on extending. So, thinking about expanding these into Thinking about expanding these into different classes and potentially using similar methods to try and also distinguish between different types of gravitational wave signals. And they just put a paper on the archive out just last week. All right, so we're going to transition from detection and distinguishing between signals and noise to this case where we have signals overlapping with noise. So, coming back to this, I wanted to mention. So, coming back to this, I wanted to mention that we make this pretty key assumption when we do Bayesian inference to estimate gravitational wave signal source properties. So we're modeling our data as the sum of our signal as seen through our detector network plus some noise. And when we write an expression for the likelihood in this Bayesian analysis, essentially what we're saying is that we expect the residual of our data. So if we're subtracting that signal out of our data, So, if we're subtracting that signal out of our data, we expect that residual noise to be consistent with Gaussian noise. And I'll give you some examples of what this looks like through the lens of similar spectrograms to what we've been looking at. So Gaussian noise being on the left, non-Gaussian noise being on the right. And obviously, you have seen a lot of evidence of cases where we have data that does not have Gaussian noise. And right now, our approach to solving this problem. Now, our approach to solving this problem to making the expression for the likelihood valid is to take cases where we think we have a detection, try and try to model that noise, and then subtract it so that we get something that looks closer to the spectrogram on the left in terms of our noise. So, this is a really time-consuming process, so it can take months to generate subtracted frames. And then, we also have many examples where the subtraction is not complete, where we still have some. Complete, or we still have some residual and non-gase energy left after this process. So, something else that we're working on at UBC is characterizing, so what is the impact of non-stationarity on our ability to accurately estimate source properties? And then also, where are the safe regions where if we have a signal that's fairly close in time to a glitch, where can we be confident that we have enough temporal separation and that we can make a confidential That we can make a confident estimate of these measurements. So, essentially, what Nico Lakoche is doing is he's taking a series of simulated signals that span different signal durations. So, right now, you're looking at a high-mass binary black hole signal that's motoring its way through a clap of thunder. So, this is a thunder glitch at LIGO Livingston. So, he's injecting these simulated waveforms ranging from Simulated waveforms ranging from heavy mass systems to binary neutron stars, so very light systems. Sweeping through first data that's not polluted by the glitch. So we want to get at least 10 samples or so where we're characterizing what the deviation in parameter estimation is from this mostly Gaussian noise, and then sweeping the signal all the way through the glitch so that we're comparing each part of the signal template. The signal template to different parts of the glitch, and then we're looking at how well we're able to resolve the masses, the spins, which has implications for how we're classifying the source for astronomers in terms of is it likely to have a neutron star or not. And then also we're looking at the sky map. So where are we mischaracterizing where the source is in the sky? So I'll show you some preliminary results. And I'll also point out that Chris Penka wrote a That Chris Penkar wrote a really nice paper about mitigating glitches in this context, as did Katerina Ciaziano, focusing on Besa wave. Jade also did a really nice study of injected signals and glitches, focusing on the comparing the different glitch amplitudes. And then there's also a study in progress from the Portsmouth team looking at really rapid PE and sky maps. But let me walk you through this. So up in the top. Walk you through this. So, up in the top left, we have this similar spectrogram. So, this is an example of some real glitch. So, this happens to be a blip, and this is frequency versus time. And then, what you're seeing here for these three other plots is each dot or cross or element is a merger time. So, we're marching, in this case, this is a high-mass binary black hole signal, and we're marching that merger time across until we cross this book. Across until we cross this book glitch and then beyond. And then what you're looking at in each one of these other plots. So each element here in this plot, so this is the posterior distribution for chirp mass. So this happens to be what we're plotting here. And again, as a function of the merger time as we're marching this injected signal across. So we get the sense of what our control is, if you like, for this relatively Gaussian noise, and then we get a much more meaningful answer in terms of. More meaningful answer in terms of how far away this posterior deviates from the true injected value. And then on the lower right here, so this is the recovered SNR with pi CVC. And again, where each one of those dots is the merger time for the injected signal. And we can see that the deviation for the posterior, as estimated with a full BILV parameter estimation, is quite a lot more well localized. We have a really highly localized Really highly localized impact for a high-mass binary black hole signal and a short-duration glitch, like a blip. Whereas the impact on this re-weighted SNR is much broader, which is interesting. Something else we're looking at doing is trying to come up with this overall good or badness quantification. So right here, you're seeing this Jensen-Shannon divergence, which is the quadrature sum of the Jensen-Shannon divergence. Sum of the Jensen-Shannon divergence for chirp mass and then the two primary, the primary and secondary mass, and the spins. So trying to think about how well we're doing overall for all the different parameters that we care about. But we're thinking about replacing this Chenson-Shannon divergence, which gives you a sense of how similar the distributions are with the cost function that Alan Nee introduced, so that we're also leveraging that we know the right answer. Leveraging that we know the right answer. So, we know what the answer should be from our simulated signal injections. So, we will very likely update this overall quantification with that cost function. And with that, we'll transition to thinking about a totally different type of signals. So, something else that I really wanted to talk about is a project that's looking at recovering the That's looking at recovering the signal of a core collapse supernova. So, this is work done by Nayer Raza here at UBC. So, using a Bayes wave to fully recover or as optimally recover a signal as we can, some signal that's been identified in the data. So, to give you a sense here, so Neuer has plotted in frequency and time for five different supernova models. The injected signal. So, this is without noise. This is just showing you the features of the injected signal in time and frequency. And then also the Bayes wave recovery for a signal that has a signal-to-noise ratio of about 60. So, just to give you a sense of how well Bayes wave can do in the highest sonar limit. So, to give you a little bit of bearings here, so we've got some, so three supernova models that are solar models. That are solar metallicity and also non-rotating, so, sort of more garden-variety neutrino-driven explosions with a failed explosion here. And then the two on the right, so these are rotating higher mass progenitor sources with a magneto-rotation, strong magneto-rotational driven explosion here on the far right. We've gotten some good feedback too here. So, we're currently labeling these by the first author in the year. By the first author in the year, but we've gotten some good suggestions from Jade on how we can improve that to make it a little bit more clear. But these are the range of signals that we've been looking at. And something else I want to draw your attention to here is that these spectrograms have different time durations. So with a shorter time duration plotted from zero to about 0.6 or so seconds, whereas these spectrograms here on the right go on a bit longer. Go on a bit longer, so it's also worth keeping in mind. So, essentially, what Neuer is trying to do here, so for each one of these signal types, is to overall develop this recipe for optimization of this Bayesian reconstruction algorithm. So essentially, the way the Bayes wave works is that you're going to place a wavelet or a series of wavelets, and each one of those wavelets is going to have some defining characteristics. Defining characteristics. So, some central time, some frequency, some Q value, and amplitude. And Bayes wave is going to model this data as the sum of some coherent set of wavelets, which is the signal model, some incoherent wavelets, which is a glitch model, plus some Gaussian noise. So let's get a sense of what this looks like. So here we have Whiten strain versus time. So this is an example supernova corkal supernova gravitational waveform. Supernova gravitational waveform here on the bottom. And then we can see this process of base wave placing these wavelets, trying to recover this coherent signal. So you can see the placement of the number of wavelets up on top here. So what Neuer did is he found that fixing the sky position for a base wave really helped it to zero in with a limited number of wavelets on a really good or optimal. Or an optimal, let's say, signal recovery. He also found that increasing the maximum allowed Q factor, so allowing for this Q value for these wavelets to be longer, so you'd expect then for them to be longer duration wavelets with less bandwidth. He also had a settings for a larger than normal allowed maximum number of wavelets, and this is just to scale with SNR. Scale with SNR. He also tried introducing triplets instead of wavelets, but found that this didn't really have that big of an impact. But let me show you what we were able to do with these optimizations that Neuer introduced. So here is the recovery of these waveforms as measured with overlap. So this is just the noise-weighted inner product between the injected signal and the recovered signal. So here you have the network overlap and uncertainty as a function. As a function of injected SNR. So, this is for roughly about 100 injected waveforms for each one of these five that we were discussing. And then, so this is to give you a sense here on the right is then the improvement, the gain, and the median of this to make it a little bit easier to read. So, the network overlap, gain, and uncertainty on the left, and then the median for each one of these. Median for each one of these curves, and also the maximum to give you a sense of where we're really gaining the most and how that changes as a function of which waveform family we're looking at. But what I want to draw your attention to is that these improvements, we're really getting the most out of them in this lower SNR, this more realistic regime. And let me give you a sense of what that actually looks like in terms of what Bayeswave is. That actually looks like in terms of what Bayeswave is better able to pick up with these optimizations. So, here we have the initial Bayeswave configuration and the optimized configuration for a lower SNR case for each one of these five waveforms. So, unlike in the high SNR case where base wave was recovering most of the signal, here we see that in this more realistic lower SNR case, this optimization is really buying us some completely different. Some completely different range in time frequency space that are corresponding in some cases to different physical features of the wave the waveform itself. So that's pretty exciting. And I think with that, I will leave some time for questions. So we've jumped quite a bit from topic to topic. There wasn't time to talk about all the exciting work that's happening here at UBC, but I hope to speak with all of you again in the future and give you some updates. You again in the future and give you some updates on what we're all working on. And with that, I'd be very happy to take any questions. Thank you very much. We thank you. So, do we have any questions? Okay. Okay, just hi, yes. Great talk, thanks. Yeah, great talk. Thanks. So, I just wanted to ask you one question about the GW Skynet. It seems like to me that the glitch identification works really well from the SkyMap. And this doesn't need any LIGO proprietary product either. So, two questions I have: one is, have you done a comparison with the p-terrestrial value and the glitch score that you're getting? That's one. And the other is, are you? Getting. That's one. And the other is: are you planning to send out, let's say, GCNs that are going to be complementing the P Astro results that LIGO is providing? Yes, great questions. No, we haven't done a direct comparison with P-Terrestrial, but I think that would be really, really interesting to do. I think I saw that Deep and Sean were on the line earlier, so maybe that's something we could follow up with them about. And yes, so we. And yes, so we do plan to send out supplementary and complementary notices with GW Skynet's results. Thank you. Do we have more questions? I can ask a question since I have the mic open. Just this please. Bleach identification techniques that you talked about at the beginning. They are only based on the strain. Are there any plans, for example, of applying the anomaly detection, the outlier factor to auxiliary channels in addition to the strain? That's a really, that's a very That's a very interesting question, Margot. That's not something we'd been thinking about, but now that you mention it, so we do have some auxiliary channels that we know have these longer duration features in them that could be potentially interesting to be correlating. So seismic channels, for example, it runs really quickly, so I don't see why not in terms of computational cost. Yeah, that's interesting. That's something we'll definitely be thinking about. Thanks for this. Something we'll definitely be thinking about. Thanks for the suggestion. Yeah, I've been talking a little bit with Julian, and then I feel a bit bad because I have very limited time. But yeah, we might discuss those kind of things. I think the method is very interesting and it can really you can really get some good information on long time scale. long time scale you know glitches variability on that that Omicron or other methods they cannot they cannot catch so I think that's very very interesting yeah great we'll we'll look forward to talking more with you about that so we have another question online so go ahead hi Jess nice talk I have a question about the sky map stuff as well but it's a bit sort of Well, but it's a bit sort of tangential. I was wondering if you've ever thought about applying it to the CWB sky maps. Because the CWB sky maps, you know, well, CWB events are always a bit sort of uncertain about what they actually are. But there's a whole bunch of sky maps that you could potentially get from the simulation events, right? And that could be your training set for possible signals. Then I was thinking, I don't know if you can get training. Then I was thinking, I don't know if you can get training sets for possible noise sky maps, but maybe that would be something that could be helpful as well. It would be more complicated for sure. Yeah, that's a really interesting suggestion. So I would have to ask Sean and Deep and other experts if coherent river sky maps are public or if there are plans to do that. And then we'll maybe need to think about whether how committed we are to. We are to only using public information, especially going forward since we know that we're going to have, well, we expect that we're going to have very different kinds of detector noise, and it's not really practical to not train on the new detector noise. So yeah, that's a really interesting suggestion. Thank you. We have another question. Deep, go ahead. Oh, Deep, I think I see you're speaking, but I can't hear you. How about now? Yeah, I was saying that great, great talk, but I had a question on a naive question on the multi-classification that you said that there was this recent study. And I was wondering that, so from the sky map, so the sky map is mostly constructed out of the SNR time series, which is information. Is information in the amplitude, but to do source properties, we want information from the phase. I'm wondering that how, what feature are you using in the data in order to do this kind of multi-class classification? I was just wondering about that. That's a really excellent question. So, this was proposed by the team at McGill. So they're focused on Focused on following up gravitational wave candidates. It's certainly not a feature that I would have thought of, or that even I think makes intuitive sense to me that you'd be able to leverage this to distinguish between gravitational wave signals themselves. But I think what we're planning to do is use the interpretability maps that Thomas and Eitan have been working on to try and figure that out. So what is it latching on to? I don't think we really understand that very well. I don't think we really understand that very well in terms of either SkyMap or detector network information, what this new multi-class classifier is using to make those distinctions. Okay, thank you. Any other questions? Okay, laptop audio on. Hi, thanks for the really awesome talk. Thanks for the really awesome talk. I was just wondering. So, you talked about glitch detection using neural networks, and then you talked about looking at the source parameters you infer for glitches and the sky maps you infer and using these to help with glitch detection. So, I was just kind of curious: what information are those adding beyond what's present in the spectrogram? In the spectrogram? Like, is using the parameter estimation or the sky maps helping you somehow beyond what you just get from the spectrogram? That is a really excellent question. And I think I might myself only characterize one of them as glitch identification. So the temporal outlier factors, maybe the only one we're using to try and identify something that we're not already planning to be able to identify with existing tools. Able to identify with existing tools, they're more different feature sets, essentially. So, the idea is that we're hoping to project that same time series, which you're absolutely right, it's the same underlying data, but we're hoping that these different projections either onto the sky map where we're leveraging coherence or onto images where we can then use an image classifier or into CVC parameter estimation space. We're hoping that that gives us a better basis set where we can more easily distinguish between the Where we can more easily distinguish between the populations. Cool, thanks. So I guess it also gives you some interpretability for what's going on. Right. Yes. I would say so. Well, I certainly hope so. Okay, so do we have more questions? If not, we think yes again. And