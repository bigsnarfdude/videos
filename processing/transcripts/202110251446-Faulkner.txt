The reflected entropy version of Shidong's talk. So it's perfect to talk about it now. And so this is work with Chris, Simon, Lynn, my student, and Pratik Rath. And yeah, it's coming out imminently. Okay, so the idea is to try to find the quote-unquote page curve for reflected entropy. And in so doing, we're going to, you know, Doing, we're going to study the entanglement wedge cross-section in random tensor networks. So, the entanglement wedge cross-section is dual to reflected entropy in ADS CFT. So, there should be some similar picture in random tensor networks. So, we'd like to explore that. And we'll also, along the way, resolve some remaining questions on the derivation between this correspondence in ADA CFT involving non-potive effects. Non-profitive effects that weren't really understood before. And, you know, we'll also find along the way a novel description of something called the canonical link purification that I'll tell you what it is in a second. Turns out that there's sort of an effective description of it as a classical superposition of tensor networks with different fixed areas. And so, you know, this is some novel thing that comes out of this calculation. And we'll also. And we'll also study something called the Rennie reflected entropy. And in particular, we'll find that this sort of spectrum of the reflected density matrix is not flat as opposed to the density matrix associated to random states typically. Okay, so let me give you some background. So the reflected density matrix, you can understand it as follows. You hand me a density matrix on two parties. matrix on two parties so just a mixed state on two parties rho a b and then there's a process by which you get a another density matrix on a a star where a star is like a second copy of a more specifically you start with a purification of the original density matrix which is called the canonical purification it's basically just purification on if you double all of the hilbert spaces um and you know you can think of it as sort of the hilbert space of matrices themselves Hilbert space of matrices themselves. So that doubles the original Hilbert space, and then the appropriate pure state, which is now a matrix, is the square root of the density matrix rho A B. And everyone's very familiar with thermofield double. So this is basically a basis independent version of the thermofield double, and it works for anything. It doesn't have to be for a thermal state. Okay, so once you have that, then you trace out the BB star Hilbert. star Hilbert spaces and then you get a density matrix on AA star and you can calculate its von Neumann entropy and that's what we call the reflected entropy. So that's just some quantity. Just to give you some feeling for what it is, let me give you some examples. So if I started, if I handed you a pure state, for example, like a maximally entangled state between A and B, then the picture of that state would be as follows. It's basically a projector. Folllows. It's basically a projector. Then, this process gives me a density matrix. You can go through this process and you can calculate it. It actually gives you a mixed density matrix, which is factorized between A and A star. And in some sense, there's basically some sense in which we're taking the picture on the left and we're rotating it by 90 degrees. And similarly, if I started instead with a factorized state but mixed, so you know, product of row A and row B. A product of row A and row B, the state would look something like this in some kind of pictorial description of it. Then it turns out that the reflected density matrix associated to that is just a field state like this. And it looks like this. So again, it looks like this rotation. So there's like some interesting bounds for reflected entropy, and these two cases are sort of saturating those two bounds. Reflected entropy is basically a measure of correlation. It's a measure of correlation, both classical and quantum. You can find a bound where reflected entropy is actually bigger than the mutual information. You prove this using strong subadditivity. And it turns out that the difference SR minus I is a very interesting quantity. It's some measure of tripartite information. In law of time, I'm not going to go through this, but there's a whole bunch of interesting works discussing that that I encourage you to read. So, here I'm going to consider not just the reflected entropy, the von Neumann entropy, but I'm going to also talk about the reflected entanglement spectrum, which is just the eigenvalues of this row AA star, or correspondingly the Reni entropies of that. Another thing that I'm going to do for technical reasons is I'm going to generalize the canonical purification, which was this square root of rho eB, to some other power, rho ab to the. Some other power, rho ad to the m over 2. That's just so I can do the replica trick. m is going to be some even integer, and then I'm going to analytically continue it to one. Okay. All right. So now we want to study this quantity. It's a very well-defined quantity. Let's study it for random states. So the simplest thing we can do is just pick a random state on three parties, A, B, C. Then we're going to trace out C and we'll get a mixed and Trace out C, and we'll get a mixed density matrix, rho AB. So we pick some high random unitary and we apply it to some fiducial state, and that gives us some state psi. And then we want to sort of characterize the entropy, the reflected entropy of that state. Okay, so everyone's very familiar with this. It gives rise, this state gives rise to the if I just calculate the entropy by splitting it into two. I'm going to split it into two between C and A B. That's sort of the interesting thing to do here. Anything to do here. And then as I tune the dimension of the various Hilbert spaces, it undergoes a phase transition, which is this page and gives rise to this page curve. So just for some definitions, I'm going to have three Hilbert space dimensions, A, B, C. I'm going to take them all very big. So chi is the dimension of each of those Hilbert spaces, chi A, B, C. And then I'm going to take them big, holding fixed the ratio of the logs of the dimensions of the Hilbert space. The logs of the dimensions of the Hilbert spaces. And that ratio is going to be this xA, xb. Okay, so there's basically sort of two parameters in this problem. I'm mostly going to be interested in just xA is equal to xb. So the a and b Hilbert space are basically the same, same, same dimension. Okay, so in this language, the page curve, if I just sort of calculate the entropy of C and normalize it in some way, then the page curve looks as follows, a little bit different to Closed, a little bit different to what you might have seen because of the way I'm normalizing it, but yeah, that's what it is. In particular, if xA is bigger than one-half, xa, xb is bigger than one half, then the mutual information is non-zero in this phase. And then if xA is less than one half, then the mutual information is zero. I'm plotting just SC, but you know, you can also calculate the mutual information in the standard ways. So here, you know, we So here, you know, we had this bound where SR is bigger than the mutual information. But just, you know, it turns out that in this phase, the reflected entropy vanishes. However, in this phase, it's a question of what is the reflected entropy. So you can calculate it, take some work, and that's what I'm going to describe. But let me give you the answer. It turns out to be. It turns out to be discontinuous. There's a jump. So, in particular, as I take the infinite bond dimensional limit, there's a jump. And it looks like this. It jumps from zero over here to this. So could you explain again what the horizontal and vertical axis are in these two graphs? Yes, yes. This is some entropy. This would be SC, and this is some ratio of von. And this is some ratio of bond dimensions, of logs of bond dimensions. So it's, you know, roughly the number of qubits, the ratios of number of qubits. Okay, yeah. Yeah, and I'm just normalizing that way so I can draw simple pictures. Okay. Yeah, so the reflected entropy has some discontinuous jump. You can also calculate this difference and it's non-zero. This discontinuous jump is something that also. Continuous jump is something that also happens in ADS CFT. So, in ADS CFT, you know, there's this duality between reflected entropy and the entanglement-wedge cross-section. So, what is that? Well, you know, if I take the classic example of just Accumedia 3, split into two intervals, A and B, like this, and then I have C. Then, you know, there's a phase transition as I vary the sizes of these splits. In particular, if AB. particular you know if if a b the ab system is bigger than c then the entanglement wedge gets connected as follows and the reflected entropy is a measure of the cross section of that connection so in particular it's basically the the minimal cut that divides the entanglement wedge in two and splits it into a and b okay um and cuts you know the boundaries into a and b so Boundaries into A and B. So in this case, it's non-zero. Here it's zero because the entanglement wedge is already disconnected. So SR is zero, entanglement wedge cross-section is zero. But in particular, it's a discontinuous jump. As soon as the entanglement wedge is there, then the reflected entropy is non-zero. Okay, so as in, it's finite. It's sorry, it scales like one over G Newton, so it's large. So it turns out that this page case that I just described. Case that I described, this is the picture here, is also the cross-section of a particular entangled edge cross-section of a particular tensor network. So in particular, or it's really a graph. So if I draw a graph associated to this random state, I have my sort of random state in the middle, and then I have the boundary vertices associated to A, B, and C. And then I can cut them in various ways. In the disconnected phase, when the mutual information vanishes, Connected phase when the mutual information vanishes. The entropy of C is calculated by this part here. And the entanglement wedge is, you know, A and B tenses, but they're disconnected. So then the reflected entropy vanishes. If I cut along this bond here, which is true in the connected phase when the mutual information is non-zero, then the entanglement wedge is sort of these three vertices here. Here and indeed, you know, A and B are connected. So now I can try to split them in some way. And the minimal split, the entanglement wedge cross-section, say XA is less than XB, then the cut would go through here. And indeed, that's the, you know, the entropy of that cut, or twice the entropy of that cut is the reflective entropy. Okay, so that's the answer. Let me describe some things. Let me describe some things. So, let me tell you where I'm going now. So, the first question is: How do we derive this? Can we calculate the reflected spectrum, entanglement spectrum? What are the non-perturbative effects responsible for this transition? And can we generalize this to more complicated random tensor networks like drawn here? Okay, so the first. Okay, so the first part. To calculate this, we're going to use the replica trick. As I've alluded to before, we calculated some much more complicated object, which has two numbers, m and n. n is the usual sort of replica trick that you would use to calculate the entropy, the rainy entropies, but we also need this m, which is just sort of a generalization of this canonical purification. And by doing that, what happens is that this thing, which we're trying to calculate, just becomes some. calculate just becomes some you know monomial polynomial or uh monomial uh of of of just um sorry polynomial of homogeneous weight nm so there's just n n m state psi as long as m is twice an integer right so then i could just calculate the har average of that quantity in some standard ways using diagrammatic techniques um Techniques. And then the idea is that you do that for integer n, even integer m, and then analytically continue to one, one and one, both of them to one. Okay, so there's some diagrammatics to this. You basically, you know, there's some, this Haar average can be calculated in terms of wick contractions. I'm not going to go into details, but basically, the end result is some statinic model. So you're doing some kind of static. So, you're doing some kind of classical statinic model where you're summing over spins which are valued in the symmetric group SMN. And then, you know, the weights depend on some network, determined by some network where you have group elements in SMN on the vertices. And then associated to the edges, you have some weights which depend on the distance between the group elements in. Elements in some metric called the Cayley-Cayley distance. So, in the reflected entropy case, you have specific boundary tensors. In just calculating Rennie entropies, you would have some specific boundary tensors. Here, we have some other ones because of, you know, when you're trying to calculate this specific quantity here, this thing here, then you need to do certain contractions. And so that gives rise to certain group elements here. Raise to certain group elements here. So, you know, there are very specific group elements, GA and GB, that live on the A and B bonds. And then E is just the identity permutation element. So GA and GB, okay, there's some pictures of them. I just wanted to flash the pictures. You don't really need to know the details. So the question then becomes: what is the dominant saddle? So what is the dominant group element, essentially? Element essentially. So, naively, what you would try to do is you have these Cayley distances. So, to minimize that distance, you would just try to take the elements to be the same. It's a metric. So, if the elements, the distance vanishes. But we have sort of three different group elements. So we sort of might naively think we have three different options to put in the middle. We could put E, G A, or G B. Okay. And then we try to minimize. So in particular, by So, in particular, by sorry, and then we minimize over these three options. So, by doing this, we're trying to minimize the distance to at least one of the fixed boundary elements. And this is a little bit like the Left-Witch-Molder-Sina assumption when you're calculating the Rennie entropies in ADS-CFT. In particular, if you were to calculate the page curve instead, just the Rennie entropies, you would get a sort of a graph that looks like this. It would be a similar partition. Be a similar partition classical statistical model where you have E here, G here, and some maximal twist operator, which is just this one here. A specific signal mutation in Sn. And okay, so then the sort of the two phases that you get are just these two, and those are sort of taken care of in the Lefwich-Moldesina assumption. Assumption. But it's known that this assumption breaks down in various near-phase transitions. It breaks down when calculating negativity, as we've already heard. And it also is this other reason that it breaks down. So the claim here is that it fails even immediately here, even away from the phase transition. And it leads to some puzzling results. So let me sort of quickly describe those. Sort of quickly describe those puzzling results. One thing that you find schematically, you can draw the phase diagram as a function of m and n holding fixed certain bond dimensions. And then, you know, just assuming those two three possible phases, you have a phase where G A dominates, a phase where E dominates. And in particular, if you just take the G A phase and analytically continue to N11. Continue to n11, then you get the right answer. You get the entangled cross-section, you get what you expect. But if you did the wrong order of limits, you would get the wrong answer. So, you know, who told me to do those order of limits? Where does that come from? This is the puzzle that I was referring to at the beginning of the talk. So the resolution here is that there's another possible phase, and we call this element g x. So there's another possible element we can put in the middle, which is called x. The middle, which is called X. It's unfortunately named X because it's sort of analogous to what Xi was talking about in his last talk with cow. You should think of my X as his cow. And in particular, it's just one element here. And it has the following picture. Again, you're not going to get too much out of that. Roughly speaking, it's the intersection of the two group elements, GA and GB, to the extent that that makes sense. The extent that that makes sense. But you know, it exists, and you can actually prove a theorem which is non-trivial, which says that minimizing over the statistical mechanics model in SMN is equivalent to just minimizing over these four possible phase, four possible group elements now, GA, GB, E, and X. So we just had to add this X, and then we're good to go. Okay. This is a non-trivial theorem. And this is a non-trivial theorem. Until fairly recently, we were just working under the assumption that this is true, but we made it recently. Okay, so then if you include this X, you get updated phase diagrams. In particular, we can resolve this Mn non-commuting limits issue. This is the new phase diagram in the Mn plane. This still has the non-commuting issue. However, there's a prescription where we basically say, There's a prescription where we basically say, Let's ignore what's going on down here and replace it by an analytic continuation from m is bigger than two. And that is the correct thing to do. We can prove that in various limits. And it also makes sense. Basically, once you do that, all of your Renny entropies turn out to, sorry, reflected entropies turn out to be independent of this M parameter. And that makes sense because we're just taking powers of... Make sense because we're just taking powers of rho AB, and rho ab, at least for a random random state, has a flat entanglement spectrum. So it's just proportional to a projector. So if I take powers of projector, I get the projector up to normalization. So it should be independent of M, and that's what we find when we include this X element. Okay, so then, okay, we did the calculation, and then we calculated the Rennie reflected entropy. This is the answer. Reflected entropy. This is the answer now. So now we have the reflected entropy as a function of x. This is like the pictures that I was showing at the beginning of the talk. But now the Renier reflected entropy has this sort of new phase where x appears, and it's no longer discontinuous. However, this point here is n over 2. So if I set n to 1, it becomes this discontinuous picture that I drew to begin with. Picture that I drew to begin with. Okay. So it becomes discontinuousness limit. Okay, so you can also calculate the entanglement spectrum, you know, of rho AA star. You find something that looks somewhat familiar from the page case or, you know, Xidong had some similar pictures. You find a bunch of eigenvalues following something called the Eigenvalues following something called the Marchenko-Pastor distribution, but then you also find a single eigenvalue. And it's the competition between these two sort of pieces of the spectrum that gives rise to the phase transition. So, you know, this exchange of dominance. Now, it turns out that you can also calculate the reflected entropy near the phase transition. And the answer looks like as follows. It's basically It's basically a classical Shannon piece, where P0 here is essentially the eigenvalue of this single eigenvalue sitting here. And P1 is just one minus P0. And then there's this large entropy piece, which is multiplied by P1. So as you go through the phase transition, this probability, this P, goes from 1 to 0. P goes from one to zero as a function of x, xa. Okay, so you know, the p0 has some interpretation. I'm not going to tell you what it is because of time, but yeah, there's some interpretation for p0. But let me try to interpret this answer here for this reflected entropy, because it's interesting that we've got this sort of classical Shannon piece plus this thing that I'm calling the area operator. So, you can give the following interpretation. So, if we have our original tensor sort of network, which is describing the page case of a single tensor, really, which sits here. Then you can cut it in two different ways to produce an entanglement. So, if I calculate the entropy S C or S A B, then I can cut it in two different ways. Here are the two different cuts, and I get two. Here are the two different cuts, and I get two different entanglement wedges. Now, once I've done that, I take the entanglement wedge part of that picture, I double it, and I glue it to itself. Okay, so this is very similar to the prescription that calculated the reflected entropy in ADS-CFT. That's basically the prescription that we gave. But now we do it for both possibilities, and we get two different tensor networks at the end of the day, these two things here. These two things here. And the claim is you should think about this sort of canonical purification as being a sort of superposition of those two networks. Okay, so we have the two different possibilities, and so we should just add them. And the probabilities for adding them can be calculated. And if that's true, then this sort of picture reproduces the reflected entropy as opposed to. Reflected entropy as a function, you know, the Reni reflected entropy under some assumptions. Yeah. So, you know, that's sort of an interesting picture for what's going on. Does it still work for other tensor networks? Yeah, it seems to. So if we do a similar calculation for more complicated tensor networks, not just a single tensor, where we have some network of random tensors, we put them together in some way, and then we The mint together in some way, and then we use some boundary, uh, boundary legs. Um, we calculate the state on those boundary legs, and we calculate the uh, you know, various the reflected entropy with those boundary legs. So let's take, you know, A to be this region, B to be this region, and then C to be everything else. Then, you know, you can do the same calculation, you get a very similar statistical mechanics model with a sum over this G. With a sum over this G and the symmetric group SMN. And making now an assumption that these are the possible dominant elements, that's actually an assumption and not something we've proven, but it seems reasonable. Then you can come up with a whole bunch of different phases. And okay. And so, for example, a naive guess for Guess for the correct phase is just to fill in the tensors with group elements GAGBEE, and then you have this section which divides them. So that would be the naive guess without x, without the x element. And in fact, if you do that, it leads to the same puzzles that I talked about in the single tensor page. But now, if we include x, this is the sort of relevant phase that appears. Appears. So, in particular, you can have some balance between the sort of domain walls that appear here, which have some tension. In particular, there's some particular tension for transitions between GA and GB. And there's some particular tension for transitions between GA and X. They're N and 2n minus 1, respectively. So you have some network of minimal surfaces. Of minimal surfaces, and they tried to find some equilibrium point. And in fact, you can find that. But the interesting thing to me is the interpretation of this result. And you can give a very similar interpretation of the canonical density matrix that leads to this sort of reflected entropy. And that is a superposition again of tensor networks where we have many different tensor networks for each of the possible. Uh, for you know, each of the possible ways of putting X in here. Um, so, in particular, you know, you get many different entanglement edges, and then you take them, each of them, and you glue them to themselves. That gives you some double tensor network, and then you sort of take a summer group. And then, you know, with some specific probability, which is actually calculable from the answer. So, you find in particular that you shouldn't be doubling the entanglement wedge and gluing it to its The entanglement wedge and gluing it to itself, which is what you would usually do to calculate the canonical purification, but you should sort of think about these pinch networks and glue them together on the pinch point. And so, you know, the entropy AA star for each of these given networks is just given by this minimal cross-section, which varies now because of the pinching effect. So, in particular, you know, it seems like you get something like an It seems like you get something like an area operator, a dynamical area operator with some merging. For a different, in particular, you know, you can take this state and you calculate the Rennie reflected entropies. And for a different N, you find that different networks with different sort of cross-pinched cross-sections dominate. And that reproduces the answer that you calculate just from the statistical mechanics model. Okay, but you know, you get back what we expected as n get goes to one, you go back to the unpinched entanglement wedge and you get the entanglement wedge cross-section, and you know that the reflected entropy is dual to the entanglement wedge cross-section. It's just the Rennie entropies that are different. So I don't know, I think this is interesting. I'm not exactly sure what to make of it, but these states that come out of this calculation that naturally model this sort of these answers. Naturally, model these answers were studied in ADS CFT by Dong, Harlow, Maroff, and Akers and Rath to move beyond fixed area states that were sort of naturally dual to tensor networks. And so I think it's here, it's interesting that they're naturally arising from random tensor networks without any mention of gravity at all when computing this sort of reflected entropy. Okay, so let me just conclude now. So let me just conclude now. So there's some open questions. You know, we managed to prove that the possible phases are just the four different group elements for a single tensor network. But the more complicated tensor network, we're just sort of assuming that. So we need to prove that. It would be nice to understand the meaning of this sort of emergent area operator. The more general tensor network case, we sort of More general tensor network case, we sort of stayed away from the phase transition region, so we don't really have a full understanding of that. Then finally, in ADS-CFT, you know, how do you generalize this to work in ADS-CFT? In particular, how would you calculate this sort of Reni reflected entropy, which has this sort of interesting non-perturbative effect? It seems like you need some kind of correction to this Engelhard wall gluing, which is the sort of gravity. Which is the sort of gravitational dual of this tensor network cooling. And, you know, there's a challenge, there's an important challenge here, which I don't really know how to overcome, which is to start, you know, to take these calculations in gravity and sort of map them onto the tensor network case, you have to think about fixed area states. And in particular, to talk about this sort of entanglement widge cross section, you need to fix both the area of the cross-channel and the area of The question and the area of this surface here, which you know is the balance the entanglement wedge. But these areas are associated to operators that don't obviously commute. So, you know, what do you do? So, I don't know the answer to that, but yeah, I think that's an interesting open question. Let's thank Tom. Okay, questions. Is it clear that these two area operators you mentioned here do not commute? It's not clear, no. Because if it crossed each other, I can see that. Yeah, yeah, no, and it seems to be better if we take into account this non-perturbative effect where the sort of you get this thing happening. So, you know, maybe there's, yeah, I just don't, you know, I don't want to commit to any. Just don't, you know, I don't want to commit to anything. I don't know. It's not clear that they don't commute. It's also not clear that they do. So, yeah. Yeah. I mean, certainly, you know, if I fix the area of this surface, then you have this uncertainty of the boost angle, and this area will depend on that, presumably depend on that uncertainty. So I don't know. Other questions? I have a quick question about what you said about pinching, Tom. So you said that as n goes to one, it's back to the unpinched case. Is there also, when does it precisely become unpinched? Is it precisely at n equals one or is it away from n equals one? Away from n equals one? Well, yeah, you can see it from the tensions, right? So when n goes to one, the tension of this brain goes to zero. So, you know, this other brain relaxes back to the entanglement wedge. Yeah, but away from n is one, then you have some equilibrium that gets set up. And you can understand that as, you know, in some kind of saddle point approximation to this state that I'm writing down. Yeah, but I. Yeah, but I guess I have in mind a picture where, for example, in the picture that you have on the right here, suppose there are not too many tensors in between those two black RT surfaces. Can you still meaningfully divide them into this X region and two X regions and the region between? Have only if you only have two tensors in between, or even one, yes. Um, well, that's sort of, yeah, sorry, go ahead. Yeah, I mean, that's sort of the case, that one tensor network case that we studied, and that you still have this superposition. Yeah, so there you get this X phase certainly dominates depending on what n you are. So, yeah, it's a general effect. And when n goes to one, the x phase sort of goes away. The X-Pay sort of goes away, gets removed. That is true. Okay, okay. Thank you. Any questions from our in-person participants? I see no hands. No hands. Okay, let's thank Tom again.