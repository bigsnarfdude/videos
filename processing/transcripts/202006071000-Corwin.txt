I guess we can uh get starting. Yeah, I've spotlighted you, Alex. Welcome, everybody, to this new week of the OOPS online probability school. So, this weekly schedule is on integrable probability. So, just to give you an idea, in the three first days, we're gonna have three one and a half hour lectures of Ivan. Lectures of Ivan, and then the last two days we're going to have nine different speakers on the topic. So, I want to thank Ivan a lot for agreeing to speak, but also for basically organizing much of this week. On top of the talks, there's going to be some working groups today and tomorrow. They're going to start at 21 hours UTC, so that's five hours from now. So, basically, you're going to have You're going to have Promethe Gossel and Xuan Wu who are going to solve some problem sets that were provided by Ivan. If you're interested in participating, there's going to be a Google sheet in the chat where you should go and sign up for this. From a technical perspective, you should know that these lectures are recorded and they're on YouTube. So, in case you don't want your In case you don't want your face to be on the internet, you should turn off your video and unmute yourself. Since there's a lot of participants, everybody is going to be muted during the talk. If you have questions, you're welcome to ask them in the chat. So we, as I mentioned, we have two people here, Promet and Juan, who are going to, Juan, who are going to answer your questions in the chat. Questions in the chat as we go. And in case it's needed, they're going to stop Ivan and ask him the question live for everyone to share it to everyone. So we'll have one break at the middle of the talks after around 45 minutes where you can ask further questions in the chat directly to Iva. And at the end of the talk, we'll have another question session, which is not going to be recorded, so you can. To be recorded, so you can feel freer to ask whatever questions you want. We have another outlet which is Zulip, an online forum where you can find the information, the link on it on the Zoo website, where you can ask more questions that Ivan will be able to answer later if you're interested. So with all this out of the way, I'm very happy to introduce Ivan Corwin. Introduce Ivan Corwin. So, Ivan has done, he's a professor at Columbia, he has done his PhD with Jean-Abb√© Neros at NYU, and he's a recipient of several prizes, including the Roald Davison Prize and many others. He's very famous for his work on KPZ. And so, I'll let I'll give the spotlight to him now to start his talk. Thank you, Evan. Thank you, Evan. All right, well, thank you, Alex. Thank you, everyone, for coming. It's a little bit of a shame. I was looking forward to being in Vancouver right now, but looking forward to doing that in, I guess, two years. So let me first share my screen and then I'll try to do this. I did share with people a PNG of basically what I'm going to be writing. Basically, what I'm going to be writing. So, you're welcome to look at that if you have trouble with what I'm writing. So, this is talk one of three. And let me just start by giving you the overall message. So, this, we'll be talking about Gibbsian line ensembles. And so, and don't worry if you don't know what they are yet. We'll get to that. And so, the message is going to be that gets you in. Magician lion ensembles are two things: interesting and useful. And so what I mean by this, so by interesting, I mean that they're going to come up in a number of interesting probabilistic or combinatorial models. So, some will be obvious, like non-intersecting paths or islings. Others are less obvious, which I'll get to maybe in the third talk about KPZ equation and other things like that. And they also have non-trivial scaling limits. So, the interest in them comes from the fact that they're related to interesting probabilistic systems and that they have interesting scaling behavior. The usefulness is actually that not only is Is actually that not only is this somehow an observation about a system, but it's actually a tool in establishing regularity. And so the, you know, one of the themes that will come up in the talk is that if you can understand certain marginal information about a system and you know that it has a Gibbs property, it has the structure of a Gibbsian line ensemble, that gives you sort of spatial and Gives you sort of spatial and maybe temporal regularity with a little bit of work. And so, what I'll do is I'll explain how this plays out in the course of the years. I'm going to focus really on examples. I'm not going to give you a grand theory or anything like that. And so in the first talk, I'm going to focus on a particular example, which I'll call non-touching geometric random. trick random walk bridges and it will turn out that this is related to something called sure process my plan just to be clear is that everything will be done from first principles and i apologize to those people who already know the first principles but i want to make sure that this is accessible to people who have no background i'll build up a little bit of a theory of sure processes and non-touching geometric Non-touching geometric random walks. And I won't really talk that much about the line ensemble perspective in the first talk. I'll really talk more about the structure that sure processes have as determinantal point processes and then how you get to certain asymptotic results. Then using that, in the second talk, I'm going to construct a universal object that's called the Airy Line Ensemble. And I'm going to construct it as a limit of these non-semesse. Constructed as a limit of these non-touching geometric random locks. And that'll really showcase the Gibbs property because the first talk will provide me sort of finite dimensional distributional convergence, and then I will strengthen that in this. The third talk of the airy line on swamp, which is called the KTZ line on swamp. And I'm going to use that structure. And I'm going to use that structure to answer a question about the temporal correlation of the KPZ equation, which is a fairly well-known stochastic PDE. I should apologize. In my abstract, I said that I was going to also talk about the integrable origins of Gibbsian line ensembles. It's a fascinating subject. But as I wrote these lectures, I realized that to do so and to do it all from first principle. So, and to do it all from first principles, I would need another few. And so, I'm going to leave that out and I'll focus instead on this. Okay, so let's start with the first subject of today's talk, which is the non-touching geometric random walks. Geometric random walks, random walk bridges. Okay, so we're going to uh let me zoom in a little. All right so we're going to fix two parameters, m and n, which will be integers, and then a bunch of real parameters a1 through am and b1 through bm and they should be positive. And they should be positive. Let's say strictly positive. And their products should be assumed to be less than one. And now what I'm going to do with this is I'm going to create a random block. So let me draw the picture and then I'll fill in a little bit more notation. So I have a space that goes from zero somewhere in between. I get to time m and I get to time m plus n. Plus n. And what I do is I now start off at time zero at a certain height. Let me just say at a given height. And I'm going to take geometric random walks, so geometric steps at each intern time. So maybe something like this. Maybe something like this. Maybe it stays flat for a little while and then up. And then that's in the upward direction. And then I start moving in the downward direction. And I'm going to condition on ending at a certain height. So let's say I end at this height. Maybe it goes flat for a little while. And I'll just draw these as these sort of broken lines. So the blue dots are actually the heights, but then I just connect them. So, how do I choose these geometric jumps? So, what I'm doing is at every location, I have this A1, A2, up to AM. And then in the other direction, I have B1 up to BN. And what's happening is I'm choosing each one of these according to a geometric random variable with the parameter that's. With the parameter that sits beneath it. So this would be x bn dn minus one. Okay, so I've really I hold the starting point and the ending part point fixed, and then I consider all of these random locks according to this law. Another way of thinking about it is that for each height increase, I get a power of the parameter, the A or the B parameter, to the height. The A or the B parameter to the height change. And then I take the product of all of those things. And the probability measure is then given proportional to that weight. So let me just fix a little bit more notation to remind you. So recall, random variable is geometric with the parameter q if probability it equals to k, 1 minus q, q to the k for k, I guess, bigger than or equal to 1, or bigger than or equal to 0. I bigger than equal to zero. And so what I'm doing is I'm choosing my, I'll call this path y. I'm choosing the y in the following way. And so ys minus, so on the left hand side, ys minus ys minus one, that increment is distributed as geometric with parameter as. On the right hand side, I need to take the opposite increment, so s plus one. So S plus one, S minus Y S plus one. So this is the, and that's going to be distributed with the B's, but now I need to do a little shift. So it's n plus m minus s. This is for s is up 1 up to m, and this is for s is, I guess, m up to m plus n minus 1. Okay, and I'm going to call such a thing. Okay, and I'm going to call such a thing, I'll call this a geometric and then vector A, vector B random walk. Now, if I fix the starting and the ending points, if I fix these points, so I think these as fixed, then I'll call it a random walk bridge. Okay, so this is what my kind of core. This is what my kind of core object is. And what I'm going to do is now consider a collection. So a collection of such things, so I'll call them yi's indexed by one to infinity of such random walks, the geometric AB bridges. Be bridges now and I just need to specify where they start. So where yi at time zero is chosen to be equal to minus i, and it's also chosen to be that at the end time m plus n. On top of that, On top of that, I'm going to condition on non-touching. Ivan, there is some question. Okay. So the first question was: like this A1 up to AN on the bottom axis, can those be replaced by 1 to M? Well, no. Well, no, so the numbers one through m are the indices, but what we're doing is we're right, we're choosing our geometric random variables with these parameters, with the a parameters or the b parameters. Of course, we could choose that all the a's are the same and all the b's are the same, but this is a little bit more general and will be useful. And the other question is: like, why do we need the reason why we need this parameter and A B's being product being less than one will actually come about from a normalization. So if I think if you well, I suppose you anyway, you need all of them to be less than one for the geometrics to make sense. So really, I should say all the A's and the B's are between zero and one to you know to be totally good about this. So that's it, that's necessary condition in order for things to be defined. Uh, condition in order for things to be defined. Okay, I can answer that. Um, and the AIs are not the jump heights, the jump heights are measured in the y. So when I write y of s, this is representing the height at time s. So if I call s here, then the value above it is y of s. So it's like that's the way I'm representing this. Okay, so what we're going to do is we're going to consider this ensemble. What we're going to do is, we're going to consider this ensemble of bridges and we're going to condition them on non-touching. And what I mean by non-touching is if you have one bridge that has a trajectory like that, and you have another one that goes like that, this is considered touching. So that's disallowed. So they need to actually keep distance one apart at least. So what would an example of this look like? So I'll draw it precisely here, and then from now on, from then on, I'll be really From then on, I'll be really rushed in the way I draw these things, but it could look something like this. So you start, this is maybe m equals one, n equals two as an example. So you start with some the first curve starts off at height minus one, jumps up to a certain height, and then it makes its way back down. Okay, so these are the values. It starts off at zero, jumps up to six, jumps back down to three, to two, and then back to zero. This is y1. Now y2 needs to start off at minus two, and it needs to end at minus two. And so, but it also can only increase in the first step. And so, because the fact is, if it were to increase, it would need to touch, that's disallowed. So, actually, the only That's disallowed. So, actually, the only option is that the second curve goes flat. And if the second curve goes flat, then all subsequent curves need to stay flat from here on out. So it actually, it's only kind of finitely many excited curves. So the number of excited curves, let me say that by which I mean the number of curves that are not flat. Curves is less than or equal to the minimum of m and m. So that's an easy thing to observe. So the question is: what does such a thing look like? And what in particular happens as n and m get large? That's kind of a natural probabilistic question. So here's a picture. When M is large and fixed and n goes to infinity. And so what will happen in that case is that there'll be a total of m curves and they'll kind of rise and then they'll start to decrease eventually. But I'm only going to show you the front part of the picture. Okay, so here's this is copied from a paper of I think a paper of Urog and DeVerne and Nika. And so what we're basically looking at is this is, you know, M is somewhere to the past here. So like let's say M is over here. So it's just growing. It just grows. And later on it will decrease, but I'm not showing you that part of the picture. But I've taken M to be pretty large here. Be pretty large here. And to summarize the type of result that we're going to be proving in the course of these lectures, what happens is the following. So m is going to infinity. Now what you do is you look in a window here that is of width m to the two-thirds. And this is something that we're going to prove, or almost prove, and of height m to the one-third. And it needs to be a sort of sloped window because the truth is, is that there's a very nice Because the truth is that there's a very nice limit shape, which is being shown in this red curve, and that has a slope to it. It actually also has some curvature to it. And so, what happens is you then, in this scale, rotate the picture back, you zoom in, and now you see a collection of curves. So the top one was Y1, and the next one was Y2. So these become the sort of rescaled versions. I'll just write a tilde for the moment of the Y's. And the question will become, what happens as M goes? Come, what happens as m goes to infinity? And the claim is that as m goes to infinity, this will converge to something which is called the Airy line ensemble, which is actually a stationary object. And what's shown in this picture is not stationary, so it needs to be minus a parabola. And we'll see that that's because there is some curvature of the limit shape, and that accounts for sort of parabolic shift. Okay, so. Shift. Okay, so this is what I want to explain in the course of these first two lectures, in a sense. I'll state something a little bit more precise. I'm not going to state a perfectly precise theorem, though. So, okay, so what are we going to do? So, today we're going to do sure processes. Do SHUR processes. So we're going to connect these geometric bridges to something called a SHUR process. And then we're going to use that fact that these SHER processes are determinantal. I'll explain what that means. And we're going to use this to prove, and I won't do it in complete generality, but finite dimensional distribution convergence. Finite-dimensional distribution convergence. And what I mean by that is the following. Imagine that I take three time slices in this window. And now I look at the point process that I get by intersecting the line ensemble, you know, these curves with these fixed times. And so maybe these are these three in this window. That's what the times look like. And what I can do is prove I can do is prove convergence of these point processes at any fixed number of times. Now, what that doesn't do is it doesn't provide you with a sort of functional limit theorem. It doesn't give you tightness. It doesn't allow you to show that the geometric Gibbs property, basically the fact that this geometric model is invariant under resampling according to conditioned geometric random locks, none of that property will go through unless. None of that property will go through unless you can show tightness, unless you can deal with continuous functions and the topology of convergence of those. And so the second part that we'll do tomorrow is we'll show that the Gibbs property, which I haven't really explained to you yet, and I'm not going to talk about it really until tomorrow. But the Gibbs property, which comes from the fact that this is an ensemble of curves conditioned on some non-crossing event, that induces Non-crossing event. That induces what's called a Gibbs property. That will result in tightness and a functional convergence, which will give us the Airy line ensemble. So that'll allow us to go from finite dimensional distributions to a functional limit theorem. And that I'll focus on tomorrow. Focus on tomorrow. And that's where the gift property will really come into play. Okay, so what I'm going to, so that's kind of the introduction. What I want to spend the next few minutes talking about is the connection between these geometric random lock bridges and an object called Sherfrost. And an object called Sure process. And it's this connection, it's this reason that we are able to really do calculations involving this system. Okay, so we're going to call we're going to do a change of variables. We'll call lambda s equal to y of s plus i. And so what this is doing in terms of this picture is an This picture is initially everything is separated, but when I go to the lambda coordinates, all of the lambdas at time zero, so lambda i at time zero, all of these will be identically equal to zero. And so it kind of smushes everything together. What this will do is it will also relate these to what are called partitions. Okay, so for each. Each s, so for if you look at the lambda i's you know, the set of them, this forms a partition. Okay, so what a partition means is, um, so let me just call this lambda of s. So what we have is that lambda What we have is that lambda 1s bigger than or equal to lambda 2 of s all the way down. And this is all strictly or weakly bigger than or equal to zero. These are all integers. And so that's generally, that's what you call a partition. So an example of a partition would be something like 4, 4, 2, 1, 1, and then all zeros afterwards. Okay. It's not a partition. It's not a partition of anything in particular, it's just a combinatorial object called a partition. We'll call the size of a partition lambda, denoted by absolute value of lambda, is just the sum of all of its coordinates. And I need one more piece of notation, which is that two partitions, lambda and mu will And mu will be called interlacing. This is drawn, written as lambda bigger than or equal to often the sort of press eek or suck eek in tech. Anyway, I won't write it. If lambda one is bigger than or equal to u1 bigger than or equal to lambda two u2 so on. U2, so on and so forth. So there's a sort of interlacing of the values of the partition. Okay, so this is, I need, I guess, one more piece of notation, which is something called a skewshir, skew shirt polynomial in one variable. So So this is indexed by pairs of partitions, lambda and mu, and a single variable a. This is defined as the indicator function that lambda interlaces with mu times a to the size of lambda minus the size of mu. Okay, so what does this have to do with what I With what I was talking about, you know, these non-touching geometric random blocks. So here is an exercise, not a hard one, but I'll leave it to you to confirm. So if we take the, so I defined this this y i, these geometric random lock. Random walk or the legacy is geometric AB bridge. So this measure implies, pushes forward to a measure on the lambda i's okay, I'm going to write it as lambda i. And this, so these are the sort of line ensemble we're talking about. This is just the shift of it. And this measure has the following form. So the probability, I'll call this PAB of So, I'll call all of this thing lambda of so this is the collection of partitions. So, this is equal to the lambda of time zero all the way up to lambda at time m plus n. This has the following form. So it's a normalization constant that we'll talk about in a moment times the product. times the product overall i equals one to m of skew sure polynomials or some say s s lambda so the skew sure polynomial indexed by lambda of s over lambda s minus one to the a s and then product s equals one to n of another set of Of another set of skew share polynomials. So m plus n minus s is minus one and then the same thing with a plus one and this is in the b variable b of s. Okay, and this is a normalization that we'll talk about in a little while. So this normalizes. Okay, so what's going on here? This is kind of a cumbersome formula, but it'll be very useful. Is the following. So, you know, we had our ensemble of curves. So maybe I'll just draw another one. Oh, I guess we. Another one. Oh, I guess we had it here, right? So at each one of these times, we form a partition by looking at the point process and then doing this affine shift. And so we have a collection of partitions. And what all that's being said by this is that the measure on this collection is just given by the product of interlacing. Interlation is just the condition of non-touching being imposed times the product of. imposed times the product of A's or B's raised to the differences. And that's just coming from the geometric random blocks. So this is really just reformulating precisely what the probability measure is. There's really very little to this. This has a name. This measure is called the Schur process. It's a special case of the Schur process. And so And so let me give one other exercise. So a lot of the exercises will build up the theory of sure processes. So one exercise is to show the following, which is that if I look at the marginal law of lambda, say at the midpoint, Lambda, say at the midpoint, right? So I have my kind of ensemble of paths. This is kind of the caricature I'm drawing for them, then it stays flat after that. So I'm looking at the midpoint. I have my partition there. So the marginal law of M, which is called the Sher measure is Is given by the following. Well, this is really the first part, not even an exercise, it's just basically a definition. So if I take So, if I just sum out over everything except for the midpoint, then I can define something which I'll call a sure polynomial. A sure polynomial. So let me actually define that. So I'll call s lambda in a set of vectors a the sum over all interlacing partitions lambda zero that starting out with the empty ones. This is just all zeros and then all the way up to lambda m. m which I'll call lambda of this product s equals one to m of these Q Sher polynomials so this is exactly what I would get by summing out and so what you find is that summing out over all of the lambdas less than m in this place and then over all the lambdas bigger than m in this place what I get is the product of two of these s lambdas One of them in the A variables, one of them in the B variables, and then I have my normalization, my ZAB inverse. So this is really just a definition. This is what's called a Schur polynomial. And there are a few things I want you to show. There's probably a few more things in the notes or in the problem step, but I want you to know that these are. Stuff, but I want you to know that these are symmetric in the A variables. Now, that is not at all obvious if you think about it, right? Because the formula is not symmetric. It's not clear that if you were to permute A1 and A2, the marginal law at the midpoint M is invariant under this permutation. Same for the Bs. So that's already a kind of a beautiful fact. A kind of a beautiful fact, an invariance of the system. And then there's a very helpful formula which we'll use, which is that in fact, this S lambda of A has a formula as the ratio of two determinants. So this is what's called the bi-alternate formula. And it's basically the ratio of something that looks like a Vanderbond determinant divided by something. Divided by something that actually is a Vannerbond determinant. So it's an m by m determinant divided by the Vaneman determinant, which I'll just write explicitly. Okay, so the bottom is, of course, the Vanderman determinant. Okay, so this is an exercise. This is an exercise to understand this. But what we've seen already at this point is that there is, I suppose, actually another part of the exercise, let me add one more thing, is to show that this partition function, ZAB, has a particularly nice form, which is the product over all i and j, you know, that it can be of the inverse. That it can be of the inverse of one minus ai vj. Okay, so this is already something again that's not at all obvious. So you take these geometric random lock bridges from zero to zero or from minus i to i, and you ask what's the overall kind of probability of if you were to write this down without any normalization, what's the normal probability. Normalization, what's the normalization that you would need? And the fact is that it has this very simple product form. And this implies, or this comes from, something which is called the Cauchy-Littlewood identity, which basically says that if I sum my S lambdas and my S lambda B's, I get exactly this ZAB overall lambda. So this is the Cauchy Littlewood identity. This is the Cauchy-Littlewood identity. Okay, so what we've learned from this is that if we look at the midpoint of my ensemble, in fact, the same sort of argument goes for any given slice. So if I just look at, say, this middle slice, this middle slice, the law that I see there can be written as the product proportional to the product. The product proportional to the product of two sure polynomials. And what we've observed then is that the sure polynomials themselves emit formulas in terms of determinants. And so what I'm going to be after in the next part of the talk is I'm going to, and I think we'll take a break in just a moment, but I'm going to describe the connection between the SHUR processes and determine NFL. Point processes. And the idea will be that the SHUR process, and in fact, I'll focus just on the Sure measure, which is just this single slice, but one can generalize, is a determinantal point process. And then I'm going to use that in the next part. I'll use that for asymptotics. In this spirit, In the spirit of the type of asymptotics that prove convergence of finite-dimensional distributions in the scaling that I was describing, I'm not going to do the details of those asymptotics, but I'll give you a sort of related asymptotic problem. Okay, so we're about halfway through. So why don't we pause? We take a break until 1245 or so. Does that sound good? Yeah, five-minute break. It's perfect. We'll take a five-minute break. I can take a look at the question. Break, I can take a look at the questions and everything, and then you know, I can address some of them when we come back as well. Okay. So, let's come back in five minutes. And if you have questions, ask them in the chat. By the way, I'll just take a second to remind you that if you want to participate in the exercise session that is organized tonight, you should, well, later today, you should sign up on the Google Sheet. Um if you can put it, thank you. Now it's back in the chat. Kevin, do you have access to the chat? Or I'm just looking at         Okay, so let me get started in a moment, but let me just remark: you know, somebody was asking, what does integral mean? There is relation to integral systems, but I won't go into that right now. So somebody was asking, why is it that So somebody was asking why is it that this Cauchy-Littlewood identity, that the sum of these, this normalization, that the sum of these sure polynomials has such a simple factorized form? And it's a beautiful fact, it really is. There's a simple proof of it, which I sketch in, or you work out in the problem set. And it goes through the following idea, which is that you prove something locally. What you do is you prove what's called a one-variable skew Cauchy-Littlewood identity. And what it basically says is, And what it basically says is: what if I have the if I look at the product of an S lambda over mu and then an product of two skew polynomials and I sum over, let me write something down. So s lambda over kappa in a single variable a and then s mu over kappa. So if I sum over this, So if I sum over this, I can relate this to a sum where I basically flip the order around. So I do sum over, say, nu, S nu over lambda B S nu over U A. And what I get in front of here is a simple factor, which I think is something like Simple factor, which I think is something like AB inverse. So this is a result which is very easy to prove in a sense, because each of these s lambda a, where a is just a single variable, this is this really simple formula I gave you, right, for these skew polynomials. That's this formula, right? So this is something that you can really do. You take the product of such things. This factor one minus A V inverse would be others, right? Maybe words would be others, right? Oh, okay, okay. So, let me now it should be right. Um, anyway, so when you do this, you can really confirm this just by hand. And now, once you have this, you can actually use the combination of the definition of the Sherp polynomials as the sum of all of these skew polynomials and this one variable thing, and you can iterate that n times n m times. And each time you iterate it, you get out a factor of one minus a i b j, depending on the ij term, and eventually you end up. term and eventually you end up summing everything this way and and this is what gives you this this nice factorized form so this is kind of a a a nice trick and actually it's a trick that generalizes to a number of other models which which we won't go into okay so question was like uh what is there a reason for the geometric distribution to be a natural choice for the steps of the bridge yeah right so that's a good question so you could of course choose any distribution such that Choose any distribution such that you could rigify it. The issue is that we won't have this nice description of the measure in terms of sure polynomials and in terms of even determinants. In a sense, the formula, the determinantal formulas come from an application of the continuum, the discrete time version of the Carlin-McGregor formula, which is called the Lingstrom-Gessel-B and O formula. You'll see that in the problem set as well. You'll see that in the problem set as well. And it's through that that you have this connection to determinantal point processes, and it's through that that you can do asymptotics. So you could certainly define measures with arbitrary jump rules. And in fact, I would believe, though I don't think anyone has proved such a universality statement, but I would believe that if you take scaling limits of such objects, you will, in complete generality, assuming that the underlying jump, say, has Jump, say, has second moments, you'll converge to the Aeryline ensemble. But the scaling and centering, I don't think this has been worked out. You know, it should be, in a sense, under the same one-third, two-thirds scaling. But yeah, it would be very nice to see a result of that sort. But anyway, so I'll focus on the case which is geometric, and geometric is important because then we have this non-crossing, and then we have this ability to To write things in terms of determinants. Okay, so here's a theorem. So it's a formula, doodle kunkov. And so it says that for lambda distributed as this Sure measure with parameters A and B, so the thing that I was just defining above, this Of this measure, which is the midpoint measure. If I go back to the y's, I'll actually define what we'll call y tilde, which is just everything shifted by a half. It simplifies some notation. So this is the lambda i. So this is the point process. Now I separate everything. So I shift everything back to this affine set of coordinates. So this is really just the set of coordinates so this is really just the midpoint and then i simply add in a half and that'll simplify some formulas i is one to infinity okay so the picture is you know we have our ensemble eventually it flattens out i look at the midpoint and i look at this point process plus a half and eventually it stays it's always packed together um okay so then let me just let this Okay, so then let me just let this is a determinantal process on z plus a half. Don't worry if you don't know what that means, I'll tell you, with a correlation kernel, Kij, which I'll give you a formula for. Before I do Give you a formula for it. Before I do that, let me tell you what all this means. So, you know, how do you define a point process on the line? Well, you can talk about Poisson point processes, and that's defined by one object, which is called the intensity. You can have a constant intensity, or you can have an intensity function, so that the number of points in a given window is the integral of that function, Poissonized, the Poisson number of points. There is a more general class, which includes Poisson point process. Class, which includes Poisson point processes, which are called determinantal point processes, where similar sorts of things are not determined by a single one-variable function, but rather by a two-variable function, which is called the correlation kernel. And the way that it works, and again, look to the problem set for more details, but is, so I'll tell you what this k is in a moment, but this means that for all n tuples of distinct numbers, say x1 up to x little n. Middle n. So let me say n is 3 and I, you know, this is one value of x1. So this is like x1. Maybe x2 is here. Maybe x3. They don't need to be ordered. X3 is here. And I asked the following question. What is the probability under my measure that x1 through xn is in the point process? Is in the point process. The number between zero and one is called a correlation function. And something is determinantal if that probability is the determinant of a single kernel, K, and it's the n by n determinant. So you might think, well, you know, anything can be written as a determinant of something, but the point is that this k doesn't depend on n. Doesn't depend on n. So I have a single kernel, a single function of two variables, and I can take arbitrary determinants. So I can, in a sense, think of taking arbitrary minors if I think of that as an infinite dimensional matrix. And the determinants of those minors will give me the probabilities that those points indexed by, you know, that index the rows and columns of the minor are in the point process. Okay. Now, if I know this, then I in fact know everything about the point process. This gives me the entire. About the point process. This gives me a bit entire information. And so, all I need to do is specify what this kernel is. And the kernel in this formula is given the following way. So, it's written, I'll write it in terms of a generating function, but I'll show in a little while that it can be written more explicitly in terms of contour integrals. So, you take a generating function of the kernel. The variables are the iw minus. W to the minus J, and that this generating function takes the form of the product of these normalizing functions. So, some very explicit formula. And then finally, this normalization over positive half integer. Okay, so this theorem in principle will allow us, or if we can do asymptotics from this, it will hopefully allow us to access asymptotic information about the system, right? Because in a sense, you know, originally as the system size grows, the complexity of formulas, everything gets more and more complicated. But you see here, the kernel is not really getting much more complicated as the system size grows. These Z's are just products of terms. These are just products of terms. And so products are not that complicated when you're taking lots of them. You know, you can kind of imagine being able to extract asymptotics depending on how the A's and the B's are tuned. Now, one thing, you know, I want you to get a little bit of a feeling for what are these, what are determinantial point processes. So there's a problem in an exercise that asks you to show that the probability that the, say, the first Probability that the first coordinate of the partition lambda s, probably that this is bigger than or equal to something, can be written as an infinite sum. This is also something that's called a Fredlin determinant, so L equals zero to infinity. And then the sum of overall x1 up to xl of this correlation function. So this can be proved using inclusion-exclusion and the definition of the determinantal time process. Okay, so sorry, I'm not hearing you. I think what permit string says there is an A missing in the formula. Missing in the formula, the denominator? Oh, yeah, yeah, that's right. So, which one is which? Yeah, this one should be an A. Okay. So the story goes, there are two things left to do in this talk. So one is to show where this formula comes from. And the other is to show where this formula goes, which I mean show how to do asymptotics. And I'll just say a little bit about the first point. And again, this will be, there'll be more details worked out in the problem set. The first problem set's large, you know, it's substantial. Second one's smaller, and then there's no. Second one's smaller, and then there's no third problem set. So it's kind of works in that way. So here's a general theorem. And what it says is that a certain class of measures is determinantal. Okay, so let me give a definition of something that I'll call. So this is really definition, like not state theorem yet. So let's define. Let's define and n point something called a biorthogonal ensemble. And now this should start to sound a little bit more like random matrix theory. In fact, these sure polynomials, these SHUR measures are progenitors of measures like the GUE or the LUE from random matrix theory. And this is a more general class. So if I orthogonal. Ensemble is a probability measure on a set, so on n particle sets, so let's say x1 up to xn, so this is of the form. Now, of course, the short measure is a probability measure on an infinite number of points, but after Number of points, but after a finite number, we know that they're all zero, so we can neglect those. And so, really, it is a measure on a finite number of points. And the form is the probability of seeing the points x1 up to xn, which you should really think of as the probability of seeing lambda, particular instantiation of lambda, should be equal to a normalizing constant. And then that depends on n. And then the product of On n, and then the product of two determinants. So this is the structure. If you have such a structure, then you're called bi-orthogonal. Then another determinant. Now these psi's and phi's can be arbitrary. The only assumption is that what's called the gram matrix, Gij, which is the sum. Which is the sum and then in the exercise where should the x s be? Where should the x be? Oh, sorry, sorry. This bigger than this. Yeah, my mistake. So you're only look, you're kind of integrating over these correlation functions beyond a point. And through inclusion-exclusion, that's going to give you the tail probability. So, this you define some overall x. This is called the gram matrix. It's called the gram matrix in linear algebra. And this should be finite. Okay, so assuming that this is finite, this measure makes sense, and there exists a normalizing constant. So, this is a normalizing constant. And so the theorem is that any such ensemble Pn, that's a problem is determinantal, it's a determinantal point process with correlation kernel KXY, which is explicitly written in terms of the phi, the psi, and the phi In terms of the phi, the psi, and the gram matrix. So it's the sum i equals one, j equals one to n of phi i x and then the inverse transpose of the gram matrix and then psi j. So this is of y. Okay, so the exercise is going to have you prove this. Is going to have you prove this, and the proof of this is really linear algebra to prove this. And the key is using the Cauchy-Benet theorem, right, which tells you how to expand a determinant in terms of sums of products of minors. And then you can imagine that, well, you know, the correlation kernels are written as, or, you know, summing correlation kernels is like summing some. Correlation kernels is like summing sums of products of minors, and so it's reasonable that this would give you a way to extract certain marginals, and that's exactly what's happening. Okay, so you know, remember, what does this really mean? The fact that it's determined, right? We already know that the endpoint distribution, the endpoint correlation function, which is exactly this, this is the capital endpoint correlation function. We know that that's written as the product of two determinants, in particular if you explain. Two determinants. In particular, if you expand them, it's one determinant. But what we want to show is that for any little n less than or equal to capital N, the same holds true. And moreover, the determinant is of a single kernel, which is what this K is representing. So if you looked at, say, the one point marginal, the probability that a given value, you know, X, is in this point process, that should be just written as the trace of this kernel K. Of this kernel K, so it would be like KXX. That would give you the one-point probability, like the sort of density at that location. Okay, so that's the first exercise. And then the second exercise is to apply to the Shermasure, in particular using the bi-alternate formula, the determinant formula. The determinant formula and prove the Kij generating function formula. And for that, there's a hint which is to use the Cauchy determinant. You know, the fact that you can evaluate a certain type of determinant. Type of determinant. Okay, so this is all that I wanted to say about the derivation of this formula, this determinantal formula for the short polymers. But what I want to do now in the remaining 24 minutes is tell you something about asymptotics. So, you know, what is it that we're after? So let me take a simplifying case. Let's assume, let's go back to the Sure, to these, you know, to these non-intersecting geometric random locks, you know, between 0, m, and then m plus n. And let's just simplify everything and set m equals to n. And let's set all of the ai's equal to all of the bj's equal to the q. J is equal to the Q. Now, you know, why did we bother with all these things if we were going to eventually set them equal? Well, it's because things actually simplify when you have parameters. You know, the Shure polynomials, if we had all the A's and the B's equal, it wouldn't be so obvious. It wouldn't be so clear how to walk our way through developing their theory. But the more parameters, the fewer options there are. And we're basically forced into the theory of sure polynomials. Once we've developed that, then we can degenerate. We can start. That then we can degenerate. We can start specializing parameters. Again, this is a general concept that happened in studying systems that are in interval probability. Okay, so we're going to do this and then we're going to imagine setting n towards infinity and q fixed. And so what's the picture going to look like? Let me again try to draw, you know, draw such a picture. And of course, these are not going to be wiggly lines. They're going to be piecewise constant lines. Piecewise constant lines, but whatever. Maybe I should try once more. The second one looks like this. I hope my stylized version of piecewise constant functions is not too horrible. And then eventually, And then eventually it just goes flat and it stays flat. This bottom curve is supposed to be flat. All right, so what's happening is what you'll see in this simulation is that there is the limit shape evolving. The limit shape has kind of two different sections, right? One is the upper limit shape. One is the upper limit shape and one is the lower limit shape. And there's no reason why the two should be symmetric, why the two should be the same. In one case, you're moving into open air, in the other, you're kind of invading the space of all of the curves beneath you. But the question will be, so this is between zero, the midpoint is at M, I guess N, we've called M and N the same, and then this is 2. And so what I'll talk about is And so, what I'll talk about is what are some interesting questions about this picture. And so, the first question is exactly understanding the limit shape. So, what are these dotted lines? And for that matter, also, if I were to, let's say, just look at the midpoint, you know, like I was saying, and now I look at this point process that I get. At this point process that I get by just intersecting. Eventually, it has density one, and if I go high enough, it has density zero. There's a natural question of saying, well, you know, how does it interpolate between the two? So let me try to plot a picture of what the density looks like when n is really, really, really big. So the density is one if I go deep enough into. The density is one if I go deep enough into the picture, right? So I'm kind of drawing that the bottom part is going here, and then if I go far enough up, you know, the top part is making its way down to having density zero. And then somewhere in between, it needs to transition. There'll be some, you know, let's say here's zero. There'll be some location which will be proportional to n, so it will be something times n, presumably. Be something times n, presumably, and then something times n and then you'll start to transition. It turns out, and I'll actually explain to you how you prove this, that the bottom, that the bottom, you know, the place where it transitions to density one, occurs at minus two q plus one plus q, or divided by one plus q, and the top occurs at two q over one minus q. And then in between, we can actually And then in between, we can actually derive a formula for how you transition. This is the sort of density transition. And let me say that what if I was at position u times n? Let me give you a formula for what this density looks like. So just what this is meaning in terms of this picture is that at height, which is basically 2Q over 1 minus. minus q n that's where the top curve is living and the bottom most kind of perturbation is occurring at minus q over one plus q times n just for concreteness if i you know say q equals to a half this becomes 2n and this becomes minus 2 thirds n so there's it's you always it's much easier for you to go up than it is for you to go down that's always the case you can just kind of check Just kind of check. All right, so what does this density function look like? So the density takes the following form. It's equal to the argument of a complex number, which I'll call V plus divided by pi. Of course, the maximal argument, V plus, will be pi. So this is good. I'll assume V plus is between 0 and pi. plus is between zero and pi, where v plus and v minus are the actually they're complex conjugate roots of the following equation. So function I'll use in a moment, f prime of v equals zero, with f of v defined as the simple difference of logarithm. So q over v minus log. minus log 1 minus q b minus u times log b okay so some explicit function if I take the derivative you see and I solve for it equal to zero you get a quadratic equation it has two roots that are complex conjugate to each other one of them is v plus one of them is v minus you take the angle the part of the full angle that it subtends and that gives you the density You the density. It's a beautiful formula. Where in the world is it coming from? Well, we'll see in just a moment when I explain a little bit of asymptotics to you. So I'll actually try to prove this formula to you. Things that I won't prove but are quite relevant are local limit theorems, local limits of the point process, local limits. And in particular, if I look in the bulk, If I look in the bulk, so by in the bulk, I mean around areas where the density is strictly between zero and one, then you can show that the point process has a limit and it's something called a discrete sign process. And at the edge, you get a scaling limit to something that's called the Airy point process. And this is exactly the way that this is kind of a progenitor. This is kind of a progenitor for proving the type of finite dimensional distribution results that I was after, right? So, if instead, so what I did is, you know, if I took some region, I scaled it by, you know, n to the two-thirds transversally, n to the one-third vertical direction, and I took n to infinity with this kind of shearing of it. If I looked at finite dimensional distributions, you know, where it crosses at a finite number of times, it turns out that's also determinantal. It's slightly more complicated, but it is. Complicated, but it is, and I can prove a limit theorem for that point process to a generalization of this thing called the Airy process. I'm not going to do that, but I will, you know, because that gets a little more advanced. I will try to explain to you the origins of the limit shape. There are other things, though. You can also study things like linear statistics and large deviation principles and all sorts of other stuff using the structure of the determinantal point process. The determinantal point process. Okay, so let me spend the last 10-15 minutes trying to explain to you where this density formula comes from. Why is it that when you take these ensembles of non-intersecting geometric random walks and you look at the density, the midpoint, why do you have such a nice formula for it? Why can you, you know, why does it take this form? Well, I think it's a nice formula. And it all boils down to Down to an analysis of the correlation kernel, right? Remember, the correlation kernel, if I look at kxx, this is giving me the probability that x is in my point process. I think I called it y tilde. So that should be a proxy for the density, right? So what I really need to do is I need to analyze the Really, need to do is I need to analyze the diagonal entries of this correlation kernel. Now, doing that from a generating function seems a little complicated, but it turns out that generating functions, you know, just by using Cauchy's residue theorem, you know, for complex analysis, you can rewrite the formula of Okunkov, this generating function, in terms of a double contour integral, right? Because there's two generating variables. And so, let me write down what that is. Down what that is, and that's actually part of an exercise. Okay, so you well, actually, this kind of comes up in the proof of the Kij formula. But anyway, so for M equals N and Ai equals Bj, and there are formulas of this type for arbitrary, but I'm not writing them down for all ij. We have the following formula for the We have the following formula for the first correlation function. So I'm going to look at a location which is u times n. So I'll imagine that u, you know, that's somewhere in the bulk, somewhere between these two endpoints. Of course, I haven't justified why these are the endpoints. So just any u times n. So that's kind of parameterizing somewhere in this rescaled Lawler numbers shape. And the residue theorem goes from this formula of Kunkkov. goes from this formula of Kunkov to following formula so it's 2œÄi one over 2 pi i squared and then it's a double contour integral complex contours I'll tell you what they are in a moment with variables V and W that take the following form so V W so there's some simple rational type Some simple rational type, some simple pre-factor. And then there is an expression that takes the form of the exponential of n times f. And this is the same f function from above of v minus f of w. That difference all times n, and then dv dw. And the so v, I'll remind this f of v, this was This was, I'll just write it again. This is this log 1 minus q over v, log 1 minus q times v, then u times log v. And this comes from, you know, just from the product form of the z's in this generating function. And the contours of integration here are the following. So I'm in the complex plane, and there are Plane, and there are two places that I want to avoid, which are Q and Q inverse. And then my contours need to be the following. They're just circles that enclose one another. So V encloses the W circle. So I integrate on a small W circle, though not so small as to be smaller than Q. And then I integrate V on a slightly larger circle, but not so large so as to include Q inverse. include Q inverse. Okay, so this is the formula that you'll prove in the problem set. And now what I want to do is I want to show how you go from this formula to the density formula, density asymptotics. So what you can notice is that when you solve, you know, when you solve you solve you know when you solve this f prime of v equals zero so why would you first of all why would you want to do that well look at the structure of this so we're taking a complex integral of some function that stays fixed and then the exponential of n which is a really really big number times the difference of two functions now if i can maneuver myself in such a way that the f of v function is as small as possible and the f of w function and the f of w function is as large as possible, then I can kind of, you know, I can localize the behavior of this kernel. I can kind of show that it's very, very small. Alternatively, you know, I can, if I can't do that, I can find where the main contribution is. And the main contribution is going to come from looking at the critical points of the function, right? You know, where is it that I can't kind of decrease the value of the function anymore? So solving this equals zero is. Solving this equals zero is very natural, and when I do that, I get a discriminant. You know, I get a quadratic equation, and I find that the discriminant of that quadratic equation is zero exactly when u is equal to these two special points, q, 2q over 1 minus q, or 2q minus 2q over 1 plus q. And that's going to, the fact that the discriminant zero will represent a transition. zero will represent a transition between the roots being real or complex conjugate pairs. And so let me explain now, you know, that's one way of kind of starting to see why these values are useful, but let me explain the three cases. So there's there are three cases to consider. So what happens when u is strictly bigger than 2q over 1 minus q? So in that case, you have the following picture, and you need to trust me, you can stick it in Mathematica. Trust me, you can stick it in Mathematica and see for yourself. So, what I'm going to do is, I'm going to draw, let's see, here's Q inverse, after Q times Q inverse, and I'm going to draw the plot of the real part of F of V. So I think of V as a complex variable. The real part, or the real part of V is the horizontal, the magnitude part is the vertical. And I'm looking at the real part of this function f of v. And it has Of V and it has the following caricature. So there are three curves upon which this is equal to zero. The real part is equal to zero on these three curves. This one happens to go through one. Inside here, it's positive and then it changes value. It becomes negative here. It's positive here and it becomes negative here. So now Here. So now recall the order of the V and the W contour was that the V contour enclosed the W contour. So I could draw my V contour just like this, right? This is my V contour. And I could draw my W contour. I can deform it. Everything's analytic as long as I'm not crossing. There's an issue when I cross because of this, and I can't go through zero because of that, but otherwise everything's analytic. Analytic aside from this Q and Q inverse. And so I can just deform myself so the V contour and the W contour lie in these regions. Now, when I do that, what I see is that the V contour is in a location where F of V, real part, is always strictly negative. And so what that means is inside of this formula, this will be some sort of strictly negative value. I can say bounded by minus C. And here, the W thing. And here, the W thing will be strictly in the positive region. And so I can bound the contribution of this by also minus C, right? Because it's the minus sign and then the minus sign of the W. So in total, I find that the integrand, if I take absolute values, can be bounded by exponential of minus 2cm times some integral that doesn't depend on n. And this, of course, is going to zero exponentially fast in n. So what do I conclude? So, what do I conclude from this? Well, what I conclude is that the density, let's say, as a function of n, goes to zero exponentially fast as long as I'm in this region. So that's good. That certainly confirms part of the picture. It confirms this section of the picture. Okay. Anyway, it confirms the part of the picture. The part of the picture that's to the right of this location. So then there's the case, which is the other extreme, so minus 2q over 1 plus q. Now the picture there is slightly different, but let me just say that, oh well, so this I'll actually let me do the middle case. And the final case is done similarly as in the middle case. Similarly, as in the middle case. So the middle case is in between the two. So one plus Q and then two Q over one minus Q. So what does the picture look like now? So because there is this phase transition for the discriminant, the roots V plus and V minus complex conjugate pairs. So these are the roots. So these are the roots member of V prime of F equals zero. And so the picture looks the following. So this is, again, here is my Q, Q inverse. These are excluded points. I'm looking at the real part of F of V. And then I have the following picture. So there are two contours. Contours on which this is equal to zero, and they actually intersect each other. The points of intersection turn out to be precisely v plus and v minus. And then I have the values. So the function, the real part is negative here, and then it kind of oscillates in between. And now remember, I started out with my V, my V contour was large, right? My V contour Contour needed to be like something like this, right? This was V and the W was small, but that's not so good, right? Because if the V contour goes into the positive region, then I'm going to get possibly a very big contribution coming up here. Of course, it might not actually be big because there's lots of oscillations. The imaginary part could be quickly moving around, and so when I integrate, it could all cancel out. But I want to avoid worrying about that. avoid worrying about that. So what I'm going to do is I'd like to do is I'd like to deform the V contour so that it actually sits strictly, or at least besides these midpoints, in the negative region. So let me call this the V contour and let me call the W contour the same sort of thing, except the W, you know, W wants to be in the positive area. Now the problem is, is that as I did this, as I deformed from V being bigger than W. I deformed from v being bigger than w, I encounter residues. Remember, there was this expression, the one over v minus w, right? I had this pre-factor in my integral, right? And I have my exponential of all this sort of stuff. And so I'm going to encounter poles when I do that. And I need to use the residue theorem. So if I use the residue theorem, what I find is that this density behaves like Behaves like well, so I need to take the residue, but I only take residues between v minus and v plus. So I get an integral between v minus and v plus of the residue. And now the residue I basically get by plugging in that v should equal w. And when I plug that in, the exponential term just gives me one. And this whole expression reduces to one over. Reduces to 1 over W dw. So that's the residue. This is the residue plus, actually minus because of the order in which I'm doing things, plus a double contour integral, but now with respect to, you know, everything's the same as before, but now with respect to these contours. But by the same argument as before, this I can show is exponentially small. So it goes exponentially to zero. Exponentially to zero. And this first part, well, I can integrate one over w along a contour. I just get the argument of the angle divided by pi. And that gives me the result. And of course, if I were to take the v's all the way to the other side, if I went into the other case, the case where u is less than minus 2q over 1 plus q. And minus 2q over 1 plus q, then I would end up that the density gets expressed as 1, which is the maximal value of this residue, minus something exponentially to zero. Actually, sorry, this, I'm incorrected. This is not exponentially going to zero. There's contributions that are going to zero, but small windows around these critical points. This actually goes, I think, like one over root n. So this goes to zero, but not exponentially fast in the Fast in the in this sub, you know, in the u strictly less than minus 2q over 1 plus q, you get exponentially fast to zero with this one. Okay, so this is what I wanted to say today. But let me just kind of summarize what did we do and what are we going to do? So we looked at, let me zoom out a little. Okay, so we looked at this ensemble of Ensemble of non-interesting geometric random walk bridges. And we saw that we could rewrite the measure in terms of the skew Schur polynomials. And then by summing out, we could get different marginals and represent, say, a one-point marginal point process in terms of a product of two Schur polynomials. Then we observed that that product emits a determinantal formula because the sure polynomials are themselves determinants and we have this nice relation. And we have this nice relation between I-orthogonal ensembles and determinantal point processes. So, then in the last part of the talk, I explained to you an idea about how to do asymptotics. I didn't do the asymptotics that I advertised, right? The ones I really advertised was that if you took kind of slices, distance m to the two-thirds, and I zoomed into the kind of the top few curves of order m to the one-third, then it would converge to a continuum limit. To a continuum limit, I showed you kind of easier versions of that type of asymptotic using the determinantal kernel. What I really wanted is a result that is saying that proving that you have these kind of finite point process convergence at a few different times. Now, we'll revisit that tomorrow, but there's something that we forgot in all of this, and that's what we're going to focus on tomorrow, which is that in the beginning of the day, we were dealing with non-intersecting random locks. Non-intersecting random walks. Nowhere in the calculations did I ever really make use of that. And so, what I'm going to remember is that we had the property that the paths were in these conditioned random walks. And that induces a Gibbs property that says that if you erase a little section of the ensemble, it tells you how to resample it according to non-intersecting random walks conditioned on avoidance. And that property is going to be very important tomorrow when we construct a continuum limit of this line ensemble, which is called the Airy line ensemble. Which is called the Airy line ensemble. Once we have this Gibbs property, it will give us the sort of space, the tightness of the curves, not just finite-dimensional distributions. And then we can take that Gibbs property and we can pull it to the limit, and we'll get something called the Brownian Gibbs property. And then we'll talk about various applications. So today I gave you the kind of integrable input, the calculations, the asymptotic, the random matrix theory type inputs. Tomorrow we're going to focus much more on the probabilistic argument. Focus much more on the probabilistic arguments using the Gibbs property. And then we'll kind of focus on the version of that in the third talk as well related to the KPC equation. Okay, so maybe there are some questions. So first of all, we're going to unmute everyone and give a big round of applause for the silent. 