Okay, thank you for being here and thank you for having me. It's a really nice opportunity to see the faces behind the papers and to speak here as one of the younger generations. I'm going to talk about a talented work with Domi King, who is also a student. With Dominique King, who is also a student at the Institute for Basic Science. So Dominique over tracks. This is just put on archive last Friday. We've been interested, you can look at that paper, but now just listen to me. So what is mutual? Well, we will work around the word ground side E, which is the union of N, and the other copy of M, which we call N stars. On E, we will be able to get the number of units And on E, we have an obvious evolution starts, and we also have an induced involution on the subsets of E. We call inside a subset T inside E a transversal if T intersect T star is anti-side. For instance, over this 4 union 4 star, 1, 2 star, 3 star, and 4 is a transversal. And we denote all transversals by the curly T of n. Now what is N? Now, what is an orthogonal matrix? An orthogonal matrix is a pair where we collect information from ground set and a set of bases. The basis is a subset of the transversal satisfying the basic exchange property. If we take two bases and we take a pair of elements, x and x star, in the symmetric difference, then we can find a different pair of elements, 1 and y star, such that we take That we take not the basic change, but the basic change for symmetric difference. We take B1, symmetric difference, the first pair, and then symmetric difference, the second pair, we get a new basis. And it turned out that this axiom, this basic train axiom, is equivalent to a stronger symmetric basis change axiom. We have a different basis from the basis E2. Okay, so from the notion of basis we have independent size. It is exactly subsets of some basis. And also dependent size, circuits, duality, circuits for dual summituid, so there are closed circuits and minors. One special property for a solo matrix is that if you have a circuit C, then if you take the obvious evolution for the subset C, you get a cool circuit. Okay. Okay, but I will not use that for this talk. It's just nice to mention that. Let me give you some examples of a soggy matrix. In particular, I will give you one example that arrives from matrix. There is a projection between the matrix on the ground side 1 to n and a sub matrix on the ground side n star with certain kinetic um conditions on the basis. Condition on the basis. For example, if you take the uniform matrix U24, how can I name the corresponding orsoft matrix U24? Well, we just name the basis, if we can save the fact basis u, 4. Then we get the basis for the soft matrix here by taking 1 star, 2, 3 star, and 4. So just complete the set with the star version of the missing elements. Okay. Now, how do we study orthogonal matrix? I just just spoil it. Sorry. How do we study orsock matrix? Well, for matrix, my favorite way is to study from the geometric point of view. We look at the Grassmonian. And there's a counterpart for orthogonal matrix, which is the orthogonal Grassmannian. So if we take a field K, and for C T, let K be of current C not equal to 2. not equal to 2. And we consider the isotropic subspaces, a subspace W of the 2n-dimensional space endowed with a symmetric non-derived binary form. It's called isotropic. If your choice of binary form vanishes on that subspace. And we will only consider the maximum ones. They will have dimensions that. Now, all maximum isotopic subspaces are parameterized. is parameterized by so-called a software Gracelonian and it lives inside the project space of dimension 2 to the n minus 1 and the coordinates correspond to transversals of one set E or equivalently the subsets of 1 through N. And the coordinates are called V coordinates. And the cool thing is that this map taking the maximum isotopic subspaces to the point in this To the point in this Garcemian is injective. That means it really gives us an embedding of the summoned Garthnonian. And it is a quadratic variety cut out by certain quadratic homogeneous equations, which we call the wake equations or wake relations. So if we take two transversals, T1 and T2, and we label, well, in a order, we label elements in C. We labeled elements inside the symmetric difference of T1, T2 intersect 1 through N. Then we can write down the weight relations here. Now, why is this the counterpart of a solid matrix? It is given by the following proposition. Now, you give me some maximum isotropic subspace W, and I can give you a big vector, and we look at the support of that big vector, and that is exactly the settle basis of some. The set of basis of some orthogonal matrix. And they say an orthogonal matrix arising in this way representable over that field K. Okay. But there's a problem. I like this approach, but well, almost all matrix, almost all solved matroids are not representable over any field, which means Representable over any field, which means if we only consider matrix from a solvent Grassmanian, then it may exclude almost all matrix of interest. And that is one motivation for the study of tracks. I guess thanks to Oliver, I don't have to give much, many motivations for the study of tracks, but this is one that we can fix this problem. This problem. Tracks are not bands. Tracks were introduced in the paper in 2019. They are like fields. So we look at a pair of an abelian group G written multiplicatively. And we look at the adding relations denoted by NF such that it behaves like That it behaves like how we can solve equations over a traffic. So, NF is a linear combination of elements of j which sum up to zero, and we call it the non-set. So, how do we put axioms to make it a nice notion of non-set? What are the minimal axioms to be done here? Well, we want zero to stand to zero. So, zero is zero. To sum to 0. So 0 is in the downside. Similarly, 1 is not in the downside. And also, we want a unique inverse, unique element epsilon, such that 1 plus epsilon is seen in the downside. And finally, we want the downside to be closed under the natural action of the BN group G. Sometimes we write the unit group F star for the G and we write negative 1. For the G, and we write negative one for the epsilon. And for the experts, tracks generalize ideals, hyperfields, and partial fields. And especially if they make non-site to be closed under addition, then we get exactly ideals in the morning session. So, bands form a nice category, and so does tracks. So, a track Tracts. So a track homomorphism is a first is a group homomorphism on the group of units, such that the induced semi-homorphism takes the downside of first track to the downside of second track. And now let me give you some examples which actually showed up this morning. Not the first bad. The initial track is not a bad. This is somehow special. Uh this is somehow special in our case. The initial track I has three elements, one, second one, and zero. And the non-side has two words, zero and one plus security one. So those are the minimal required words in the non-set. We also have a fine object. This is a quasar hyperfield. Hopefully this is I really rotate this move. I agree with this move, I quote this clearer. So it has two elements, 0 and 1, and the null side consists of 0, 1 plus 1, and any other CA you can get except for the singletone 1. The regular pressure field determines regular matrix and regular solid matrix. It has three elements: 0, 1, 5, 1, and non-statics. Either you have 0 or you have 0. Either you have zero or you have the same number of ones and negative ones. Okay, well it is a convenient category to work with because it has many nice properties that we can expect for category something. Especially, I will use the categorical product for the last part on applications. So the categorical tracks admit a categorical product. Made a category product. Okay, I will talk about this later. Yes, it has a nice property of product. Okay, now we have tracks. We also have orthogonometries. Let's talk about orthogonal matrix over tracks. There are different ways to define those objects. Let's look at the wake functions. Now, f is not necessarily a field, it is a track. A wake function on ground set E with On ground set E with coefficients in F is a function taking the transversus of E to tracks such that first the set of basis is dot empty and second we have the basis exchange axiom. And in the vacuum relationship means if we write down the expression for vacuum, then it is also an expression in the non set. So why this is correct? So why this is correct? Because, oh, it's just big functions. And we say two-way functions are equivalent if they're different by a global scalar by some non-zero element inside the tract. And we call an equivalence class of big functions an orthogonal matrix over the tract F, or simply the orthogonal F matrix. Okay, now why this is nice? Because just as in the associate corresponding case, the support of every weak function gives us an assignment matrix. And we call it the underlying asset matrix of this big function. And denote it by the matrix with underlines. Okay, now we have a submitriate word tracks which form a nice Tracks, which form a nice category. So, why don't we consider how they behave under the constructions inside the category tracks? For example, we have the push forwards. If we have a track homomorphism from F1 to F2, then it gives us a natural push-forward operator taking a solo F1 matrix to a southern F2 matrix with the same underlying astroguard matrix. And in particular, And in particular, if we take the map F to be the unique tractomorphism from your any choice of track to the final object on the quasar field, you get a push forward J star m. This is precisely the same thing as the underlying asom matrix of M. And we see an under associated M is representable if we can find an oral F matrix. Well, I'm a solo f meter at m prime, such that the push forward gives you the underlying metric, underlying circle metroid. And similarly, for products, we can define products of wake functions. We take two tracks, not necessarily the same or different, regards to tracks, and we take two wake functions, these coefficients, in F1, F2. The crucial condition is that we want Fe1, Fe2 to have the same underlying associate. In other words, they are just representations of the same underlying associate matrix or different tracks. Then, we can define a product of weak functions. This is exactly the representation of your same underlying astrophilometriate over the product of tracks F1 cross F2. F1 cross F2. And we have the same underlying associate matrix. Okay, now I've defined asom matrix matrix using fake functions. But we all know matrix have different quitromorphisms. So what are they? We have author F signatures that capture the circuit axioms. But I guess this is what Oliver calls This is what Oliver calls cycles of matrix this morning. And also, we have a solo f vector size that generalize vectors of matrix over tracks by Wardolf Laura Anderson in 2019. And all of those can be defined for the weak strong weak function. That means we want all the equations to be satisfied. But also we can consider the weak version of a somewhat matrix. Osoga F matrix. Those are defined by only the four-term making relations plus the support from the osoga matrix. The main theorem of our paper is that they are actually equivalent. There are actually some quippomorphisms. And the proof involves a homotopy theorem by Bundel on the one scale of the base polysope of the oscillometer. And this results. And this result in turns generalized Moller's Homoi theorem for matrix. Okay, now let me give you just one application on how our theory of a soft matrix over tracks can be used to prove some representation theorem of classical sense of representation theorem for orthogonal. Of representation theorem for osoga matrice and also matrice, osogomatoise and matrice. Maybe we're using this Lyma or theorem saying that if P is a partial field, this is a special case of tract, then every weak orthogonal P matrix is automatically a strong orthogonal P matrix, which means if you give me a weak function, only say Only satisfying the full-term recognition, plus you know the support for my matrix, a solid matrix, then I can tell you that this satisfies all the base relations. This theorem, first proved by Gibbon in 1996 in his thesis, seems that one associated matrix is regular, so let n be an alternating associate, then the following is equivalent. The followings are equivalent. First, this slogan is representable over S2 and S3. Second, it is representable over the regular partial field. And third, M is representable over all fields. So these three statements are from field and thesis, but actually in our paper, we give two more convictions for regular matrix, but that will require But that will require some other terminology for tracks. So let's just skip that. From our theory, 2 to 3 is given by the unique track homorphism from the vector partial field to your choice of field. So 2 implies 3 is not that hard. And we don't have to do anything for 3 implies 1. So now I will do the one-slack proof for So what's my true for 1 plus 2? Okay, let's take our matrix M and we know that it is representable over F2 and F3. So by the product of wig functions and wax, we know M is representable over the product of tracks F2 cross F3. But that doesn't make sense for fields. That doesn't make sense for fields. That only makes sense in the zero tracks. Now we take the unique group homomorphism of the unit group of this product to the unit group of this vapor pressure field. This is not a track homomorphism, but it gives us a weak orthogonal matrix over the vapor pressure field. And because of the lemma on the previous slide, this is a partial field. This is a partial field, so every weak orthogonal matrix is automatically a strong orthogonal matrix. And that ends up the proof, saying that now we know M is retainable over the driver pressure field. And that is my proof and my talk. Thank you. All right, fairly high-level question. Tell me if I'm wrong, but I think the theorems you stated, their counterparts are true for matroids of retracts as well. Yes. Do you know or believe there's any differences between the theory of matroids of retracts and orthogonal