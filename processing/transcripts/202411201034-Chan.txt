To thank the organizers for inviting me, for giving this talk. So, in this presentation, I'll basically be introducing to you the algorithm that we have been developing that's called G Spino Tree S for automatic detection of glitches with segmentation. So, here's the outline of the presentations. Basically, I'll be answering four questions. Basically, I'll be answering four questions. What is G-Spinal GS? How was it changed? How well does it perform? And what are the future steps that we would like to take with G-Spinal GS? So let's begin with the first question. What is G-Spinal GS? So G-Spinal GS is actually an extension to G-Spinal Tree. Now, if this is the first time that you have ever heard of this name, it actually stands for the Gravity Spy Convolutional Neural Network Decision Tree. And JS Binary has been developed, designed, and deployed as a data quality, lygo-focal cardboard data quality reporting tasks. And JS Binary has been incorporated and has been running since the beginning of 04. So here's a simple explanation of how JS Binary works. So imagine if we have a professional candidate uploaded to GraceDB, for example, then GS Binary would be triggered. And GS Binary would then use machine learning to log into this. Then use machine learning to look into this event using the spectrograms of the event surrounding the data of this event and look into that and see if there's actually a gravitational wave signal for glitches. So most of the development work of GS103 was actually led by a group of students with pictures shown here and with support from the senior members in our group at UBC as well as the support. As well as the support from the LIBO Verbal Cardboard Detective Advisation team. So here's the latest paper on Juice Final Tree. So if you're interested, please feel free to look it up. Okay, so this is Grease Final Tree. And here's just one example, just to show you where JSPINA 3 works well. So this device is actually not a real final, it was simulated from our multiple challenge campaign that we used to. Our multiple change campaign that we use to test the broadness of our infrastructure and our search pipelines. And this event, as you can see here, this event actually passes this station narrative test. So this test found no data quality issues associated with this event. You can see that the results have passed here. But then G Spinal Chin correctly identified this actually the glitch in the spectral parameter data surrounding this definition. The spectrogram with the data surrounding this event. So, this is the final predicted label, which is life scattering. And you look at the spectrogram, you see that that's actually consistent with what's happening in the data. So, that is just one example where G-Sparatry performs well. And one other thing that is excellent about G-Sparatry is the fact that it can actually pick up multiple objects in the data at the same time. This means that it does not just do single classification, it could actually look into the sparrow program and look. Actually, look into the spatial graph and if there are more than one object there, like for example, you have a grid overlapping gravitational wave, or you have multiple bridges, then grease financing would still be able to pick it up. So this is the nice thing about Grease Binary. So this is Grease Binary, what exactly is Green Spinach S then? Here's a diagram that illustrates the motivation or the context for Greece Binary S. So imagine doing and observing one Imagine doing and observing one, our detectors, LIGO verbal carbon detectors, would be constantly taking time series data. And then our search pipelines, LIGO verbal carbon search pipelines, would analyze this time series data constantly with the configuration, with the low latency configuration. And if there happens to be a gravitational signal barrier in the data, then this search pipeline would identify, would upload this discovery to GraceDB and it. And if these discovery actually pass the significant threshold that we have in place, then this discovery would be automatically published as event candidates. Now, all this is automatic, and after this automatic publication of events, a team of researchers within the LIGO Verbal Cargo Collaboration gather together and look further into these events and see if there are actually any data. If there are actually any data quality issues associated with these events, this team of experts would have to go through a number of plots, even plots, different data quality task results to see if there are actually any data quality issues. But one thing they will actually look into is a spectrogram. So here, I'm using this very famous spectrogram for Jupiter 17017 as an example. So if this team of experts look into the spectrogram of the graphic candidate and see there's actually a Visually candidate and see there's actually a glitch here. They will also need to manually identify a very basic characteristic or physical features of this glitch. And this would include something like the time at which the glitch occurs and the time window associated with this glitch. This would also mean that the GPS times associated with the glitch as well as the lower limits. The lower limit frequency and upper limit frequency. But now, the problem with this process is that it's all done by human beings manually. So, just to recap a little bit, when we have a graphic candidate, a team of researchers would need to look into a number of things and see if there are any data quality issues. And there are actually data quality issues, then this team of experts will also need to identify the features, basic features of the glitches. The problem with this is. The glitches. The problem with this is that this is very time-intensive. Now, imagine if you have a number of events, like for example, in our future observing ones, 05, 06, and there would be many more events, and this would take a long time to do all that. And as you may imagine, relying on human beings also introduce inconsistency. So, this makes it very difficult for us to produce all the results across different events. So, these are very problematic. So, this is basically the motivation. So, this is basically the motivation of the context for the definition of G-Spinner GS. Now, I've been talking a lot about what exactly is G-Spinner. So, here is simple explanation. G-Sparnet is basically an image segmentation algorithm. So, by image segmentation, I mean like the kind of technique that you apply to an image and then work objects in the image into different groups of pixels. So, like, for example, this image. Example: This image here is an image of cars, and you can see these cars are grouped into different groups of pixels, highlighted by different squares of different colors. So, this is what image segmentation looks like. And so, G-Spinachi is actually an image segmentation technique. So, to do that, we use a machine learning technique that's called YONO, which stands for you only look once. So, the reason that Only look once. So, the reason that we use this is because it's fast, it's accurate, and it's automatic. So, it actually helps us to solve all the problems that I mentioned with the manual process that I just described. And also, what's more interesting is the fact that Grease Binal Tree, as like its predecessor, Grease Binal Tree, can identify all objects of interest in an image in one go. So, that's what we want. Okay, so just to recap a little bit, GSP. Just to recap a little bit, G-Spin GS is an algorithm that applies a segmentation technique to spectrograms of rotational detected data and see if there are any creatures of rotational waves in the data. And if there are any, then G-Sparching S would group these objects, different objects of interest into different groups of pixels. And by doing that, G-Sparaching S would also be able to provide these time windows and frequency ranges associated with these objects of interest. And because it And because it's based on Euro, you only look once it's automatic, rapid, consistent, and reproducible, of course, of different events by different users. So this solved all the problems that I just mentioned with the manual process. And the outputs from GS1HS, which includes the frequency ranges in the time window, could be used by other downstream data analysis tasks, such as, for example, voice removal and range subtraction with baseball. Rich subtraction with base waste. So, okay, so you now have a very basic understanding of what G-Spinachi S. Let's talk about the problem of how was it developed and how was it changed. Now, because G-Spinal GS deals with bridges and gravitational waves, I want to use a more general term here. I want to use this term regions of excess power to describe the kind of objects or things that we're interested in. Of things that we're interested in in a spectrogram. So now to develop G-Spiner GS, we need to develop our training data. So we need a concrete definition of region of excess power. So if you look at this spectrogram here without knowing anything, it would be very clear to you that at least two regions of excess power in this spectrogram. But are there only two? Could there be more than two? So that's why we need a very clear definition of region of excess power. condition of region of excessive power. So the way that we define region of excessive power is by doing that, doing this following here. So we first collect all the spectrograms that we have in our training data set. Train data set is just part of the entire data set that we have. And this training data set consists of more than 19,000 images in our entire data set. And then we collect all the values of all. Collect all the values of all the pixels, all these spectrograms, and make a histogram of it. And this is the histogram I'm talking about. And so the pixels of these spectrograms are normalized such that the values are ranging from 0 to 25.5. So that's why the maximum value that you see in this histogram is 25.5, and the minimum value is 0. Now, after we got this histogram, we picked a threshold. Histogram, we pick the threshold on the x-axis such that the threshold is higher than the value of 98% pixels in all our images. So this means that if a pixel has a value higher than this threshold, 10.5, then it means that this pixel is actually louder than lighting 80%, at least lighting 80% of all pixels in our image and our tape chain data sets. So this is our threshold for exact power. Excess power. Now, the region of excess power is just defined as a cluster with at least 28 such consecutive pixels. So these pixels have to be connected to each other for this to come. So this is our definition of region of excess power. So with this definition, let's look again at the spectrogram that just show you. Now, this flexible corresponds to this volume or this color bar here. And with this definition, we actually see that. Definition: We actually see the three regions of excise power. So, we have a quartet away here because we simulate the quartition away in the spectrogram, and we have a clip here as well as a low frequency light. So, after we use this definition and find three regions of excess power, we manually draw the bounding box surrounding the regions of excess power. And then, from this bounding box, we can infer the frequency rate. We can invert the frequency ranges and the time windows associated with this region of excess power, and then we label the objects within the bounding boxes based on the morphology. Now, of course, for partition waves, because we always know that the morphology is partitional because we simulate the signal. But for the other types of objects, like these pictures, we label them based on the morphology. So that's the way that we generate our training samples. Generate our training samples. Now, because I'm talking about my training samples, so let's talk a little bit more about that. So, the training samples for G-Spinal GS is actually part of the training samples for G-Spinal G. So, this is one of the reasons why we have very similar names. So, for glitches in particular, we grabbed our glitches from Graphispy. So, we basically grabbed GPS times from Graphispy, and they are And they are glitches that are observed by the LIBOR furboard detectors in the last observing 103. And then basically, now we consider only five types of glitches plus gravitational waves. And these five types of glitches include wave low frequency, wave, low frequency light, scratching, and scattering. And for gravitational waves, of course, we simulate those gravitational waves in our data. Now, in total, after we process all our scratch grants, which We process all our spectrograms, which is which consists of more than 23,000 spectrograms. We have these many regions of excess power in all our spectrograms, of which this many are simulated gravitational waste. So these are the data sets that we use to develop G-Spiner GS. Now, before I actually show you the performance, let's talk a little bit about the architecture. As you can see, this plot here is a little bit too complicated for me to make. Too complicated for me to explain in a couple of seconds. So, the main takeaway message that I want you to have is the fact that GSminer GX actually has two components. So, the first component is this very big cubic here that's called a dotlet. And this dotlet is a convolutional neural network with a lot of layers, basically 24 dotlet, convolutional layers. And each dotlet convolutional layer consists of three layers here. Of three layers here. And then there are two sets of outputs from this model, from this dotted, from its 15th layer and then its 24th layer. And these two sets of outputs will be further processed by a number of convolutional layers here before two sets of outputs are finally generated from the entire label. The reason that we use two sets of outputs is because we want one set of outputs to be responsible for a larger region of excess power. Larger region of excess power, and the other set responsible for smaller region of excess power. So, the reason for that is because we find that generally, if we design the task to different outputs, we generally improve the performance. So, that's why we have two sets of regions of outputs here. So, okay, now as you understand a little bit about the algorithm, J-Spinal Chest talk about how well actually does it perform here. Actually, the safe perform here. So let's look again at this project one that showed you just a couple of minutes ago to understand the kind of outputs that we can expect from Jean Spartan HS. So here on the left hand side, we have this spectrogram with the ground truth bounding box drawn manually here. On the right hand side, we have this same spectrogram with the bounding boxes predicted or generated by G-Spin GS. And in addition, Spinach S. And in addition to the bundling boxes, we also have this score that's called objects. Now, because GSpiner GS, when GSPS process an image, it could actually generate a vast number of binding bosses that actually do not have anything to do with any regions of excess power in the image. So that's why we use this objective score to determine whether there's actually anything inside the bounding boxes. This also indicates the This also indicates the model's gene spinach confidence that there's indeed something in the bounding boxes. This is actually a value that we could put a threshold on to change the behavior of the algorithm. So for example, if I say I would only consider the things that have an option, a score higher than 0.8 to be something interesting, then I would only look at the very confident detection, fail detection. So this is something that we could Detection. So, this is something that we could address to change the behavior of the algorithm. And in addition to this, there are also these predict object types of the things in the bundling boxes, just like your normal classifier. And from the bounding boxes, we can interpret the predict frequency range as well as the time window. So, these are the basic outputs from Driespiner GS. Now, let's have a look at the general performance of Driespiner GS on our testing samples. S on our testing samples. So, our testing samples consist of this many more than 5,000 regions of exercise power in more than 2,000 spectrograms. And here is a table that shows the general performance. Now, there are lots of numbers here, but if you just look at the percentage for these true positives and percentage for false negatives, you'll notice that for most classes, we actually have a very good performance, generally speaking, because the true positives are close. The true positives are close to 90%, if not higher than 90%. And then the false negatives are basically one single digit or close to one single digit, except this class here. You can see we have a very low true positive rate here, close to a little bit more than 50%, and then a very high fossil active rate, close to 50%. Now, this is actually a problem known to us because if you look at the numbers corresponding to this sketching, this Corresponding to this sketching. This is the total number of regional excess power in our testing sample. It is just 250, it's a very small number. Now, this number is proportional to the number of regional excess power in the training data set. So this means that we also have a very limited number of regional excess power for this type of widgets in our training center. That's why we have a very poor performance for this scheduling, this type of widget. So, this is a problem long-term, we know how to improve it. And we know how to improve it. So, this table here shows the general performance of this final GS in terms of whether the label is able to pick up the things that are actually in the image. But how well are the bounding boxes matching the untruth bounding boxes? So, to evaluate that, we need to use another metric that's called intersection over union. And so, for a different pair of boxes in the section over union, basically. And the section of a union basically is a match that values how well these two boxes match each other. So it mathematically is given by this equation here. It's given by area of overlap defined by area of union. So area of unit overlap is just the overlapping area between the two boxes. And then area of union is the sum of the area of the three boxes minus the overlapping area because otherwise you double count the overlapping area. The overlapping area. So, this is the metric that we use to evaluate how well the predicted bundling boxes match the brown truth bundling boxes. So, we calculate all this, we calculate the value of IOU for all these samples here in this table. And this is the distribution of it. So, you can see most of the predict binding boxes have a value of IOU. That's relatively high to high, but there's still a fraction of predict bounding boxes that have a value. Predict farming process then have a file that's actually relatively low, less than 2.5 example. Now, to understand a little bit better, we actually make a cumulative distribution plot to show just how large is the fraction of planning boxes that don't have a very high IOU value. So, on the x-axis here, you have this value of IOU, and then on the y-axis here, we have the cumulative distribution. Now, this dashed line here indicates where. line here indicates where is larger sent. So for example, gravitational wave is this blue curve, the blue curve in the middle here, and this dashed line intersects this gravitational wave curve at this value here, 0.68. So this means that 90% of all the predict bounding boxes have an IOU higher than this value. So this consists of 90% of all testing samples. And these are the And these are the values for our types of pitches in our testing samples. Now, it may not be very obvious to you what this value actually means. So, let's have a look again at this, the same, the same spectrogram that I've been showing during the entire talk. And so this, again, is the ground truth bounding box that we manually draw. And then this is the predict bounding box from the algorithm. And if we calculate the IOU between these pairs of bonding boxes, Between these pairs of bonding boxes, we have this value. So, as you can see, we don't actually need to have a very, very high value of IOU close to 2.5 example for this algorithm to be useful. Because even though there's actually some variation between the sizes, the predicted frequency ranges and the time window are still very consistent. So, this is the performance of this final cheat. Now, before I actually conclude, I would like to answer the final question. What are the future steps that we would like to? What are the future steps that we would like to take with G-Spiner 3S? And the first thing that we would like to do with GS Financi S is to extend the architecture of GS Final Tree S so that it takes multiple images of different durations, just like its predecessor GS Final G and Graph T spy. So the reason for this is because this would allow G-Spiner GS to look into different spectrographs of different durations so that G-Spiner GS can capture pictures of different types. Capture bridges of different time scales. So, this is one of the things that we would like to do with G-Spinage S. And then, we would also like to improve other bridge morphologies such as Tom T and Coffee Fish that are not included in the current work here. So, this is the first thing that we would like to do. And this is actually, GS5GS is actually the first step that we took towards building a very large framework for automatic bridge subtraction and noise removal. So this is just the first step. So, this is just the first step. So, to conclude, we have developed a machine learning algorithm that machine less called GS1GS using the image segmentation techniques. And it can do automatic detection of pictures and gravitational weights. It defines the time window and the frequency windows and frequency ranges associated with these things. And we plan to incorporate multiple spectrograms of different durations so that. Durations so that gene spin sheet S can capture glitches or professional ways of different time scales. And this is the first step that we talked towards building something larger for automatic removal of noise and glitches. And ask a lot is if you find this data set useful, this augmentation data set useful, please feel free to reach out. We are happy to let you use our data sets. So thank you very much. I don't know. You can call the questions. I'll let you know any kind of answer. So I was wondering about what was your resolution of your spectrograms which you used to train. What was the train? Since you were talking about this big calculation of getting the access power, which was the threshold was about 98.5% more than all the other pixels from all the other images. So I was wondering what kind of resolution were you getting? Resolution were you dealing with, and what kind of data you have to deal with? Right, so yeah, thanks for this question. So, the time series that we use to generate this metric has assembling weight of 4096 Hertz. And the image actually has a resolution of 340 by 280. And if you look at the output from GS5S, you can see that actually G-SpyJS is accurate down to the first like the G-Shajaj. Accurate down to the first digital point, decimal point, at least. So, well, from my point of view, this is enough for event validation tasks and other downstream analysis tasks, such as noise removal, cookie subtraction. But if we will, like I said, actually, a more demanding need to be more accurate, then we could actually increase those as well. Well, so, yeah. Excellent. Please touch. Thank you for the very nice presentation. I have to slide 31. Slide 31. Yeah, there you go. Because this is your discussion. But I was wondering, because from this you can see that your data set is balanced. Yes. And this your trainee set function balance. Exactly. So that's that's the exact reason why we're seeing a program. Exactly the reason why we're seeing a poor performance for this class. Now, of course, we could augment the data so that it's more balanced, but the reason for this imbalance is because when we first developed this algorithm, we just graphed whatever the bridge is provided by graphics by and then just label all the objects in the images. And we actually didn't realize that there was such an imbalance until we at this stage. So that's why we still have this problem. Why we still have this problem, but we know how to solve it because we can, like Tress just mentioned, we have this glitch pump where we could simulate more glitches, as many as we want. So we know how to solve this problem, and we're looking forward to solving it. Chris, you have a question? Question. Okay, thank you. Oh, sorry, I think you had the follow-up. Yeah, I have a follow-up question. So then again. Sorry, I forgot about it. So let's say you give GScratch as an image, which actually produces a lot of bounding boxes that have nothing to do with any objects. Nothing to do with any objects in the image. You just randomly generate things that are actually not relevant. So that's why we need to have this object score there to determine whether something that you generate is actually a valid detection by you, but just some random hallucination or something. Okay, okay. So how is this objective score determined? The objective score? Yeah. So you mean in the loss function or what? What is 100% what is zero? Send one of zero. I mean, how do you calculate? How do you calculate 100%? So, this is actually one of the outputs from the model. So, basically, you have a value from the outputs, and then you use this binary cost entropy formula to calculate the loss against the ground truth. So, this is the value that we've got. Because, I mean, one could say that there's an equally valid boundary box that includes both of them, right? That's true, and then we can. That's true, and then we calculate the. So let's say if we have two bounding bosses for this object, we then calculate the IOU. So we basically brand the detections and see which one is a more confident detection based on object score. And then we calculate the IOU. So if the IOU between two predict bounding boxes is higher than a pre-selective threshold, we discard the one that's less. We discard the one that's less common. And we only left with the one that's most common. Okay, we can talk about it. Yeah, great, thanks. Definitely appreciate it. I do remember Matt. So I think you mentioned that you can potentially inject wetters with footprints. There's also other tools. My question is, let's say that you inject a population of wetlands with footprint. They will have a certain frequency range and a certain time range, right? Like time rates, right? Um, what happens if you, if in your real data, you have glitches that have a very different bunding box, right? Because the bunding box is very limited. So, how can you account for glitches that have a different notebook? So you're basically asking whether we can generate uh data set as representative of what we can expect in real life. Uh is that the question? I think it's more like how flexible can be your framework with the framework. So that's a very good question, and I don't actually have an answer to that because glitch pop, as Jess just mentioned, was something that was only developed. So we haven't really done a lot of things to test the fragmenting of the glitches produced by glitch pop, but this would be something that we're definitely interested in looking into. Thanks for the question. Okay, I think we should probably wrap it up. Can you make Bourbon one more time? 