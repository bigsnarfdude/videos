So, this is actually my first offline talk in two years. So, it feels very weird. So, I hope it doesn't get too socially awkward. So, all right. So, the title of my talk is Towards Understanding the Expressive Power of Graph Networks. And I will kind of give a survey of some stuff we have been doing for the last two or three years to better understand the expressive power of GraphNet. The expressive power of graph networks. Yeah, so let me start with a quick motivation for graph and relational data. I'm pretty sure I don't have to tell you this: that graphs are important and that we should leverage machine learning to understand them better. One kind of graph I find interesting is graphs induced by discrete optimization problems, because you have a variable constraint interaction, and this induces a bipartite graph. Bipartite graph. And there are some interesting structures in this constraint matrix A, which you might want to exploit using machine learning. Right? So, this is kind of a different kind of graph than, for example, molecules or social networks everybody talks about. So what is the setting for the talk today? It's the standard supervised setting. I think Ron yesterday sufficiently introduced this. So we have a So, we have axis to unknown distribution. We can sample it finitely many times. For example, we can sample molecules. And now we would like to learn a mapping that maps each molecule or each graph to some Euclidean space in which you then can perform standard machine learning algorithms. For example, use a neural network to find this function represented by this dashed line to distinguish non-toxic. To distinguish non-toxic from toxic molecules. And then, if you get a molecule that is not labeled non-toxic or toxic, you hope it appears on the right side of the dashed line so that it generalizes well, right? So, well, why is this a hard problem? Well, from high level, networks are complex. So, here on the right-hand side, you see a cartoonish representation of an image. It's basically just a bunch of orange dots. Just a bunch of orange dots arranged in a rectangular fashion. And this more or less perfectly describes this image, right? And then on the left-hand side, you see a small network. And here it's much harder to describe this network using natural language, right? So you can say something, oh, well, it has sewn so many nodes, it has sewn so many edges. There may be, I don't know, five triangles in that network, and so on, so many cycles, but still from this natural language description, would not be able to, well, recall. Able to reconstruct this graph or network. And this has a number of reasons. Maybe the most simplest reason is that graphs have arbitrary size. And most importantly, there's no fixed node ordering. So your mapping has to be invariant to permutation. This is kind of a crucial constraint. So what is the state of the art in this field? So kind of graph classification. Kind of graph classification is a rather old question. So it has been investigated in different communities in the last, I don't know, five or four decades, starting in chemoinformatics under the name of molecular fingerprints. And then it has been investigated in physics and biology, in bioinformatics and statistics, in data mining, and of course also in machine learning. And you can basically multiply the number of papers times thousand or ten thousand or even hundred thousand. Or 10,000, or even 100,000 to give a rough estimate of how many papers are there, right? I mean, we all know this. But we all know that when you read these papers, well, there is not, they're all very ad hoc, so the methods proposed, and there's not really a kind of a theoretical understanding of why these methods work and why they don't work. And that's why I put this picture of people doing alchemy, you know, which is a little bit, you know, Alchemy, which is a little bit mean, but this in some sense represents the state of the art. I mean, there are lots of great ideas in this timeline, in these papers, but it's still kind of alchemy. So that's why me and my collaborators, I want to develop a practical theory of machine learning with graphs. So on one side, you have the mathematical foundations, which consist of these. Which consists of these three pillars, namely expressivity. So you want to understand which permutation invariant functions these algorithms, these neural architectures are able to approximate. Then there's the question of scalability. So how can we make those methods scalable so that they scale to large graphs and a large set of graphs? And then there's, of course, the question of generalization, which, for example, Ron and Rengie talked about yesterday. Talked about yesterday. Right, and then we want to use the inside gains in these mathematical foundations to steer the application of graph embeddings in application areas, for example, in optimization. But today I want to talk about expressivity. And as I said, I want to survey some theoretical results we had in the last roughly three years. Years and more specifically, I want to talk about graph neural networks. I guess you all know about graph neural networks. I don't have to explain this to you. I will just kind of quickly go through the idea to fix some notation. So we have this graph here on five nodes, v1 to v5. And you assume that you have at each node some initial d-dimensional real feature. And now you want to. And now you want to kind of learn features for each node that somehow represent the graph structure around this node. So the basic idea is that you look at the neighboring features of each node, for example, of the node before, and then aggregate these features together with the previous feature of the node before into a new feature. And here you see a very simple graph neural network. A very simple graph neural network layer. This is probably the most simplest graph neural network layer you could think of. So, the basic idea is that you have this sum over the neighbors. So, you do a component-wise sum over the neighboring features, and then you apply a linear map. So, W2 is, let's say, D times D real matrix. And then you map this back to the previous feature of the node V. And maybe before you do that, you also apply a linear map. Before you do that, you also apply a linear map W2. Then you feed this into a nonlinearity, applied pointwise. And you can also abstractify from this simple layer. So you replace the inner and the outer sum by true arbitrary differentiable functions, namely f aggregate and f merge. So if aggregate computes a vectorial computes a vectorial vectorial feature of the multi-set of the features of the neighbors and then the function f aggregate sorry the function f merge merges this together with the previous representation of the node b okay so so how expressive are graph null networks kind of the the high-level insight of this is that any possible graph This is that any possible graph neural network architecture misses crucial patterns of the data. And we will get into detail about this, but here just have a quick example. So here you see two molecules which have different chemical properties, bistoclobentyl and decaline. But our theory tells us that there cannot exist a graph neural network architecture that can distinguish these two molecules. So that compute. Two molecules. So that computes different features for these molecules. And this is completely agnostic from the optimization oracle use and also agnostic from the weight assignments. And then in order to overcome this, we devised a hierarchy of proably more powerful architectures, which we call two-auto GNNs, three-auto GNNs. GNNs, three-order GNNs, and in general, K-order GNMs, right? And for example, GNNs can probably not distinguish these molecules, while for example, the third level in this hierarchy can provably distinguish these molecules and can also provably distinguish almost all other pairs of small molecules. Sure. How is the quality of the identification? Um, well, the results also hold for real features. Oh, you mean 3D coordinates? Yeah, then it might be possible to distinguish them. Yeah, so let's say. Yeah, so let's say 2D molecules. Okay, so now you might be curious how we came up with these very general results. I guess a lot of you already know this. So it's all based on a simple combinatorial algorithm, the one-dimensional Weiswaler-Lehmann algorithm, which is a simple heuristic for the graph isomorphism problem. And the basic idea is that two vertices get identical colors. Get identical colors if their colored neighborhoods are identical. So, let me give you a quick example. So, here you have two graphs, G1 and G2. And initially, all nodes are colored in the same color, namely gray. And then this changes in the next iteration. For example, this node and this node get the same color simply because both of them have three gray neighbors. Have three gray neighbors. But then in the next iteration, they get different colors simply because this one has one red neighbor and this one has two red neighbors. And you can do this for a number of iterations. And in each iteration, you count how often each color occurs. And you get this color count vectors or color histographs. And if these vectors are different, you know that the These vectors are different, you know that the graphs are non-isomorphic, so they have a different structure. And now let's look at this coloring rule formally. So how does it work? So we want to update the color of the node V. So we collect the colors of the neighboring nodes into a multi-set, and we pair this up with the previous color of the node V. And then we just use some injective function to map. Injective function to map this to a new color that has not been used in previous iterations. And here you see again this general forms of GNNs. So again, you have this function f-aggregate and this function f merge. The function f aggregate computes a vectorial representation of the neighboring features. And then you merge this together with the previous feature using the function f merge. And then this is the Right, and then this is the first theorem. One can show that GNNs cannot be more expressive than the one WL in terms of distinguishing non-isomorphic graphs. So they cannot exist in the instantiation of F merge and F aggregate and corresponding parameters that if the one WL cannot distinguish pairs of graphs, then there cannot exist a GNN that can do this. So the one WL in some senses. So the 1WL in some sense is a hard upper bound for the expressivity of any possible GNN. Right, and then on the other hand, there's this, well, slightly more positive result, namely that there exists a GNN architecture and corresponding weight assignments such that it reaches an equivalent coloring as the 12. So, this is just kind of a fancy way of saying that. Way of saying that GNN architectures and the 1WL algorithm are equal in power. And a small remark. So these results have been shown in parallel by my collaborators and me, and also a group at NIT at Stanford. But there's a key difference in the results, especially with regard to the second theory. Regarding the second theorem, namely that our result implies that you have a polynomial number of parameters in the input size. And the results by Yegalka only imply exponentially number of parameters, which is a huge difference. Because Yagalka uses the universal approximation theory. But of course, the result by Yagalka is much easier to prove. Is much easier to prove. The number of weights with regard to the number of nodes. You mean the Yagalka result? You mean their Galka result? I mean, they use the, they just hammer on with the universal approximation theory, which implicitly probably uses Stonewirestra somehow. Yeah. Right, so now we have this correspondence between combinatorial algorithms and graph neural networks. So you can kind of view the 1WL. As a discrete version of GNNs, or you can view GNNs as a pluralized version of the 1WL. And the key takeaway here is that GNNs have the same power as the 1WL in distinguishing non-isomorphic graphs. And the good thing here is that the limits of the 1WL are very well understood. So, for example, there's this nice paper by Vickus Arvind et al. that precisely characterizes the power of the 1WL. The power of the 1WL. So, given a graph, they can decide in polynomial time if the 1WL captures the complete structure of that graph. So, in other words, if it computes the isomorphism type of the graph. And if you read that paper, you also quickly find out that GNNs, by our result, cannot distinguish very basic graph properties. So, it cannot distinguish cycles of different lengths, triangle counts, and it cannot even Triangle counts and it cannot even distinguish regular graphs with the same parameter. And here again, you see this molecular example from the beginning. And the basic reason why a GNN cannot distinguish these two molecules is, for example, that this atom and this atom looks the same to the GNN, right? Because the local neighborhood applied recursively is the same. Okay, so how can we make genes more powerful? How can we overcome the limitations of the 1WL? Well, if there's the 1WL, there's probably also the KWL, which is a kind of famous algorithm in the graph isomorphism community. It was apparently introduced by Babai in the 70s and then rediscovered by a bunch of people, for example, by Neil Immerman from UMass. From UMass. And the basic idea to make the algorithm more powerful than the 1WL is instead of coloring single nodes, you color K-tuples defined over the set of vertices. So you can also view this as coloring ordered subgraphs on K-nodes, right? And there is this famous result by Psy, Fuhrer, and Immermann that shows that as you increase K, the algorithm gets strictly more powerful. Algorithm gets strictly more powerful. So the KWL is strictly weaker than the K plus one WL in terms of distinguishing non-isomorphic graphs. So what is the so how does the algorithm work? So assume we have this graph here on sixth node, and let's consider this free tuple ABC. And then we say that A B D ABD is a three-neighbor of ABC simply because you have to exchange the third component to go from one tuple to the other. So you just exchange C with D and you go from ABC to ABD, right? And this is kind of the notion of adjacency we use to kind of simulate the 1WL on some high order structures. Structures. So, the idea of the algorithm: initially, two tuples get the same color if the two subgraphs are isomorphic. So, for example, this tuple ABC gets a different initial color than ABD, simply because this one has this wedge structure, this one has this triangle structure. And then the iteration is kind of similar to the 1WL. Two tuples get the same color. Two tuples get the same color if they have an equally colored neighbor. And then we can basically use this easy combinatorial algorithm to derive k-dimensional or k-order graph networks. And as you can see, this k-order GNN looks very similar to the simple GNN layer we've seen at the beginning. The only difference now is that we compute features for ordered subgraphs. Features for audit subgraphs of k-nodes, and that we use the definition of neighborhood between these audit subgraphs I just explained on the previous slide. And then you get these structural results. So here on the lower level, you have the KWL hierarchy and the 1WL upper bounds GNNs, the 2WL upper bounds 2GNs, and in general, the KWL upper bounds. The KWL upper bounds, KGNMs. And you can show that in the limit, you're universal. So there's this nice paper by Weiss Azizian and Mark Lelach that precisely proved this. And this is the picture. And now, of course, you can ask, well, you know, we have universality and everything. Are we there yet? And this is, of course, a clickbait question. And the answer is no. And the answer is no, because the k-dimensional Weiswell Lehmann and hence k-order GNNs have a bunch of problems. First of all, it does not scale because the lower bound on the complexity is n to the power of k, where n is your number of nodes. And then it does not leverage the sparsity. So when you translate this to the neural setting, it leads to dense matrix multiplication, which is not that nice. And most importantly, it's provably powerful, but it leads. Powerful, but it leads to overfitting. So it kind of too quickly converges to a too powerful function. So you don't have a good trade-off between expressivity and generalization. And the first idea we had is to derive a local variant of the KWL that leveraged the sparsity, but is still provably powerful. So, what's the idea here? So, what's the idea here? So, again, let's look at this six-note graph, and let's again look at this three-tuple ABC. And then we already know that the three-tuple ABD is a neighbor of ABC by definition of the neighborhood of the KWL, but we also know that there's no edge between C and D. C and D. And that's why we say that ABD is a global three neighbor of ABC. But for example, there is an edge between C and F. And that's why we say that ABF is a local three neighbor of the tuple ABC, simply because there's an edge between C and F. And the basic Between C and F. And the basic idea of the local variant now is that we throw away the global neighbors and only look at the local neighbors. So we only look at the subset of the local neighbors. And by that, we leverage the sparsity of the underlying graph. And now, of course, the question is, well, we throw something away. This must be less powerful. But we actually proved this somewhat counterintuitive result, namely, Namely, that there exists a variant of the local KWL, which is called for some reason delta KLWL plus, which is strictly more powerful than the ordinary KWL. And this plus version can be computed in the same running time as the ordinary local KWL. Okay, so this is a little bit weird. You throw something away and You throw something away and you get more. And the catch here is that you need more iterations to converge to the same function as the Kw. And this is actually like a quite nice graph theoretic proof about reasoning about infinite trees. So if you like graph theory, check that out. And then of course, you can derive local K-order GNNs or local K-dimensional graph neural networks. Local K-dimensional graph neural networks. The only difference to the local K-GNNs now is that we replace the standard neighborhood by the local neighborhood, right? And then, well, it's not just a theoretical result. We also implemented this. So here you see some results on the zinc data set and the alchemy data set. And it's a regression data set. So on the y-axis, you have the mean. You have the mean absolute error, and on the x-axis, the number of epochs. And the purple lines are standard K-order GNNs, and the blue and the orange ones are our local algorithms. So it also works better in practice. And then again, we have this structural result here. So again, 1WL corresponds to WL corresponds to GNNs, the two-order local WL corresponds to two-order local GNNs, and so on. And in general, the local KWL upper bounds the expressive power of any possible K-order vocal GNM. And also in the limit, you get universality. Well, of course, we're still not. But of course, we are still not there because we still consider all n to the power k tuples, right? So we have a, the lower bound on the space complexity is n to the power of k. And of course, this is not feasible in practice. So what can we do? How can we kind of escape this lower bound? Well, the idea is to, let's go even more sparse. To, let's go, even more sparse, but you cannot, well, you cannot sparsify the neighborhood anymore because when you sparsify it even more, then you lose expressivity. So I try to, I was kind of inspired by the nice slides by Liz yesterday, but I don't have the same drawing skills as her, apparently. So what I wanted to convey here is: so this circle. Is so this circle is equal to the set of all k-tuples. And now we have to think: so which subset of the k-tuples makes sense to consider? Of course, you could take some arbitrary one, maybe this blue one, but it's not clear why this blue one makes sense, right? So what we came up with is we devised the new heuristic for the graph isomorphism problem. For the graph isomorphism problem, which we call local KSWL. So it's a variation of the KWL. And the basic idea is that we only consider K-tuples inducing subgraphs with the most S-connected components. So for example, this subgraph has three connected components, this one has two, and these two have one connected component. Right? And for example, if you say, And for example, if you said s equal to 3, then you would consider all of these k-tuples. If you said s equal to 2, you would consider these three here. And if you would consider s equal to 1, then you would just consider these k tuples. And we were able to prove that the running time of the KSWL does not depend on k. Does not depend on k. So you have now, you now have the s and the exponent. So we reduce the running time from n to the power of k, which is a lower bound, to an upper bound of n to the power of s. And this tilt notation just suppresses logarithmic factors. Okay, and now you might wonder: so, what is the how does the choice of S impact the Does the choice of S impact the expressivity? And it's basically summarized in this picture. So you have on the left-hand side the 1WL, which corresponds to GNNs, and the 1WL is equal to 1,1 LWL. And then the 1,1WL is more powerful than the 2,1WL, and so on. So you have this hierarchy here. So the K1WL. So the K1WL is strictly less expressive than the K plus 11WL, which is then in turn less expressive than the K2 LWL, which is less expressive than the KK LWL, which corresponds to the local KWL I explained previous slides ago. And you can also make this hierarchy even more fine-grained. So you can show that the K2WL is strictly less. That the K2WL is strictly less expressive than the K3WL, and so on, all the way up to the KKLWL. We have not, it's not in the paper, like kind of this thing here. I mean, the more fine-grained theorems here, because it's kind of very complicated to write down these truths, gets very nasty. But yeah, that's kind of the picture. But yeah, that's kind of the picture. Yeah. Yes. So it's, I mean, it's the one WL corresponds to the one one WL. So it's, it's just notes. So the left is the size of the subject. So, the left is the size of the subgraph, and yeah, yeah, and the other one is the, yeah, you're right. Well, and we also implemented this, and this also gives a boost in practice, both in computation time, so lower computation time, but on some data sets, you also have better generalization properties because you have this better trade-off between expressivity and generalization. And you can actually also prove. And you could actually also prove this. We are working on this right now. And the last thing I want to talk about are subgraph enhanced GNMs. So in the last two years, there has been this trend introducing subgraph GNMs. For example, there are these identity-aware GNNs, nested GNNs, reconstruction GNMs, equivariant subgraph aggregation networks, and they're also. Graph aggregation networks, and they all follow the same pattern. The basic idea is that, given a graph, for example, this graph here on four notes, you color, for example, all possible pairs of two notes, right? For example, this one or this one. And then for each of these colored or marked. Colored or marked graphs, you deploy a GNN on top, and then you just aggregate over the output of these GNNs. That's kind of the basic idea of all of these approaches, like one way or another. And we came up with this framework ordered subgraph aggregation networks that kind of defines the language to precisely characterize and also generalize all of these subgraph enhanced. Of these subgraph-enhanced GNNs. So we can show that every subgraph-enhanced GNN that fits in the language of our ordered subgraph aggregation networks is strictly less expressive than the folklore K plus 1WL. So the K plus 1WL is a hard-upper bound for all of these approaches. There are at least 30 out there. And that subgraph enhanced GNNs, if you choose the subgraphs right, If you choose the subgraphs right, then they're incomparable to the KWL. So there exist permutation invariant functions that can be represented by subgraph-enhanced genes, but not by the KWL, and vice versa. And we also, this is not just a theoretical paper, we also show how to learn to sample those subgraphs. So Matthias Nippel from the University of Stuttgart, he has this IMLE framework, which is a general. Which is a general framework to differentiate through discrete structures. And you can kind of map this to learn-to-sample subgraphs. Right, and if you're interested between the connections between Weiswaler Lehmann and machine learning, we have this nice survey paper with a bunch of people. For example, Sebastian, or sorry, Sebastian is also a co-author. He did this during the strange times of COVID. So, if you're interested in this, check out the survey paper. And I want to finish with a vague question. So, the expressivity results are agnostic to the data distribution. So, how can we get the data distribution into the expressivity results? I don't know, but maybe you know. That's all. Sorry. But then you don't have the graph structure. I mean you can create a graph. Oh, I see. I see. That's interesting. Sure, sure. Sounds cool. Awesome. Do we have questions for Christopher? I'll start with Spita. Start with Spita. So, I had a quick question. When you were looking at the higher-order aggregations in terms of the three subgraphs and things like that, it kind of reminded me of the simplicial networks we've been hearing about in the past two days. Is there a connection or difference there? Yeah, all these simplicial complex algorithms can be understood in the language of the KWL. So, they're kind of a special case of the KWL. So, you can actually show that. So you can actually show that the KWL is capable of not only solve the graph isomorphism problem for binary structures, so graphs, but also for general TRE structures. So, you know, and simplicity complexes are covered by general TRE structures. So, yes. Yeah. Thanks, Christopher, for the nice overview of your work. Overview of your work. I have a question about the hierarchy of KWL because if you do like K bigger than two, sort of you, if I understand correctly, you consider all possible tuples of nodes and you don't take the union at the intersection with actual connected graphs in the true graph, correct? So you really look at So you really look at, you connect every possible node. It's like you kind of treat your, you don't use the graph structure. Oh, I use the graph structure. So this is done in the initial initialization. Exactly. It's just in the initialization. Yes. So that was one reason we developed the local KWL. Okay, yeah, because it's almost like a node feature network, right? Like you initialize your features and then you because And then you um because the updates don't really use uh the uh the structure at least in the KWL test originally, yeah. That's that's only partially true because when you update the color of the k-tuples, you you don't look at the complete graph, you don't look at all other k-tuples, you only look at a subset of the k-tuples. So, so implicitly, you're actually also taking graph structure into account. Also taking graph structure into account. It's maybe not intuitive to see at first. Wait, but I look at all possible K-tuples. Yeah, you color all possible K-tuples. Yeah, but all the information comes from the initialization. No. So kind of at the initialization, you just have kind of a local view of the graph because you only look at small subgraphs. Yeah, you take the isomorphism type, right? Yeah, you take the isomorphism type, right? You take the isomorphism type, yeah. I mean, it's something different than the isomorphism type, it's the atomic type, which considers it's an ordering, yeah. But um, and then doing the aggregation, you aggregate between these local views of the graph, so you get you can represent more functions that way. Okay, okay, cool. Yeah, thank you. I always found there is a discontinuity at k equals two somehow, because before it's very clear. Somehow, because before it's very clear how the graph structure is leveraged, afterwards, it gets a bit more abstract, like in the KW hierarchy. But yeah, thanks a lot. Thanks for the great talk. So, can you explain again what's your definition of local KWL? I sort of missed that. KWL, I sort of missed that. So you understood the definition of the KWR. Yeah. Right. And the local one only looks at a subset of the original definition of neighborhood. So it only looks at k-tuples such that the exchange vertex. The exchange vertex vertices are connected by an edge. So, for example, you have this free tuple ABC, and then you go to ABF by exchanging C with F. And C and F are connected by an edge. And that's why it's a local algorithm, a local neighbor. And if there wouldn't be an edge, then it would be a global neighbor, and you would not consider it. And you would not consider it okay. So, ABC is like a triangle, and that's the three-tuple I originally looked at. And then if I swap to ABF, I'm going to just characterize ABF as adding another edge from the three triangle. Yeah, so let's do this abstractly. So, you have the three-tuple. So, you have the three-touple ABC. And then, for example, ABD is a three-neighbor just because you change the third component. So you swap out C with D. But there's no edge between C and D. Yes. And hence it's a global neighbor. But if you swap out C with F, it's a local neighbor simply because there's an edge between C and F. Okay, so once you define global neighbor and local. Define global neighbor and local neighbor, and then how does that go into the algorithm? So, you throw away all the global neighbors. You just don't consider the global neighbors anymore. You just consider the local neighbors. Ah, okay. I see. So, that's how you reduce sort of the neighborhood size. Correct, yes. But still, after this, you still get N to the K. Yes. As upper valve. Yeah. As a profound. Yeah. I see. Okay. Okay. Okay. Interesting. My second question is: so you're your sparsity aware where you try to sort of make it to N to the S. Is it so? Besides sparsity, is there maybe other more structures that you can leverage or maybe? Structures that you can leverage, or maybe a little bit connected to the earlier question of like leveraging the original graph structure. Because there may be, for example, like motif or whatever, that's like some templates that in the graph that you probably wanted to use those as your feature to constrain your network. I mean, sure, you could also define it in application. Sure, you could also define it in an application-dependent way. So, if you know that I don't know, triangles are very important, then you probably want to consider triangles. I see. But here it's just more generic that you just had this threshold and so your triangle case would be a special case of this general thing, yeah. I see. Um, thank you. I love. Like what I mean, or is I a lot of people and just a quick question, maybe next one I often talk about what structures uh can the KWL office give you? So like you have this hierarchy, like you said in the beginning, triangle counts don't work for one WL and so on. So there's something similar for KWL, like this, what are they called, like this K rank in my graphs, for instance. So you need K plus one to do K rank in that graph or something. And sorry, I'm not sure where this k. Yeah, you proved this hierarchy results by constructing pairs of non-isomorphic graphs that cannot be distinguished by the KWL, but can be distinguished by the K plus 1 WL. And it's quite involved to construct these. There was actually like a huge breakthrough in the 90s when Neil Immermann came up with kind of this generic construction, which are nowadays called Construction, which are nowadays called CFI graphs for cipher MMN. Yeah, it's quite hard to explain these on an intuitive level. I mean, maybe we can take it offline and I can explain to you. Thanks. Very nice talk. I have maybe a dumb question. So, when you think about the generalization on these KWL models, are you thinking it in terms of like Are you thinking it in terms of like bias versus variance, or do you have like, how do you? So we think in terms of uniform and non-uniform bounds. So non-uniform means that it takes the input size into account. And uniform means that it doesn't take, that's agnostic to the input size. So it only depends on the parameters. And you can actually show both. And we can actually also show like how the graph. Actually, also show how the graph structure influences the VC dimension. This is, I mean, we're still working on this paper, still needs to be written down, but we can show this. We can precisely pin down how the graph structure influences the VC dimension in the non-uniform case. And we can also precisely quantify what role the number of parameters play in the uniform case.