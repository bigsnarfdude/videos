       And then perhaps there is another regularization term. And we studied a little bit in detail of this example that is. Example that is sparse regression, also known as the LASSO. This is you know a simplified somehow version of this example. This is a problem in which you have parameters, you know, your samples take the form of yi xi, where xi are called a feature vector. So let's say that is, you know, in a simple model is Gaussian with identity covariance, and yi is a linear function of that. Is a linear function of that vector plus noise. Okay, just to make it, you know, to make an example, this is the typical model that you use for predicting all sorts of things using statistical modeling. For instance, predicting click-through rate in an internet service. Of course, this is much simplified with respect to what is done in practice, but at the core, that is the kind And of course, that is the kind of problems that we have in mind. And the typical loss function, the typical risk function for this atypical risk function is the last so, in which you minimize y minus x theta. So x is the matrix that is n by d, and as rho is the xi's. So this is the matrix which has Which has rows. So this is the matrix x1 xn. So this is n by d and then and then you know so this is minimizing exactly the sum over i yi minus xi transpose theta square. Okay, so this is exactly this objective. Okay, so it's minimized. So it's minimizing the residual sum of square, and okay, you add for regularization a penalty of this form to promote sparsity. And we studied what is, you know, how to use Gaussian comparison inequalities for compute the limit value of this quantity. Okay, what I want to talk about today is: okay, how do you actually do this minimization problem? So, you want to minimize, for instance, this cost function, how do you do it? Function, how do you do it? And okay, of course, there is a vast literature on this optimization, but the simplest point to start from is gradient descent. Okay, so gradient descent. Okay, so this is probably something that everybody heard about. Okay, but But this is basically doing an iterative algorithm in which you go in the direction of the gradient of this n okay and this here this st are step sizes. Okay, so you can think of this as fixed or as a predecessed sequence of real numbers. Okay, now there is a subtlety here, of course, is that, for instance, in the specific example that I gave you before, this last saw, the function is not differentiable, so the gradient in general doesn't exist. But okay, you can solve that problem using a variety of Problem using a variety of methods. One is using sub-gradients instead of gradient, one other is using something that is called proximal gradient method. And there is a variety of improvement, of course. This gradient is attributed to Attributed to Cauchy normally, and since Cauchy, there has been some progress. So, for instance, there is a class of algorithms that are probably faster at the same computation speed, and these are called the accelerated gradients. And other type of very interesting algorithm are what is called mirror descent. And so on. Okay, so this is a huge literature. One thing that all of these algorithms, of course, I don't want to go into this, but one thing that all these algorithms have in common is that these are first order methods. So this is optimization terminology, and it refers to the fact that they access the loss function only through its gradient. Okay, so as you see, what you Okay, so as you see, what you do at each iteration, you compute the gradient of the function and you do something with the gradient. This is the general definition of first-order method. It has to be a little bit formalized, but can we do a general theory of this first order methods? In optimization, there is such a theory that concerns mainly convex functions, but But but here we want to you know kind of look at a theory that exploits the statistical nature of the data. So, what is the common structure of this first order methods? Well, you know, we have a little bit to specialize the problem, so I'll specialize it to the kind of problems that I described before, that is regression problem, in which again a response is problem in which again the response is data is the form yi xi where yi perhaps is a response and xi is a feature vector again think for instance of xi kind of you know all the information that concerns the patient and why i the outcome Patient and why the outcome of a certain procedure on that patient. And then you want to minimize a loss function. And in general, this, again, I write it as a function of zi and theta, but I will specialize it by writing as a function of yi and theta dot xi. Okay. Okay, so so what I'm trying now to minimize is ln, let me write it ln of theta that is one over n sum over i of L of y i theta xi and if I write this in vector notation, I can write this as 1 over n L of y and x. y and x theta. And here I use this convention that I'll follow from here on. That is when I have a function, scalar function applied to vector, is understood that it's applied or to a couple of vectors is understood that is applied entrywise. In this case, it's applied entrywise and averaged. Okay, now I need to compute the gradient to do gradient descent. Gradient to do gradient descent. So, what is the structure of the gradient of this? This is, you know, if you do, it's just x transpose times a function of y and x theta. Okay, where this function is what? Well, this function is very simple, it's just the function of first derivatives, right? Okay, so perhaps I should. Okay, and here f of y. is the partial derivative with respect to y hat of L of Y y hat. Okay, so with this, now we computed the gradient, so the structure of gradient descent is simply that theta t plus one These are some deterministic numbers like st, and then there is xt. Okay, so what you will see here is that we are doing two types of operations, right? One type of operation is that is multiplication. Or X transpose by matrix vector multiplication, and the other is we are applying a separable function. So, we are doing two types of operations here. If you look at it, Here, if you look at it, we are multiplying by x, x transpose, and then we are applying this function f. Okay, so what I want now to do is, you know, this is common to all the list of algorithms that I described before. They all proceed in this way, or many of them proceed in this way. What changes is the order in which you do these operations, and then the type of. And then, and then the type of functions that you apply, what I want to do is just abstract this and look at a general class of algorithms that are constrained to this kind of operations. Okay, so because of that, I'll define something that I'll call generalize first-order methods, and this takes the following form. So, at each operation, you do X transpose times some function of the previous iterates and then perhaps of Y. What probes are there is this? Doesn't matter plus some other function of the previous iterates. And perhaps here I have some side information. Here I have some side information. And then I update another set of vectors in the same way. Okay, so this is what we call generalized first-order methods. So I have two sequences of vectors, the u t's and the v in the theta t's. The theta t's. Theta t's. The theta t's are supposed to be estimates of my parameter vector, and the ut are auxiliary variables. And each pair, so one, you know, the uts are here. So there are two different dimensions, right? The uts have dimension n and the theta t is dimension t. And at each operation, you do one of these two things. You either These two things, you either compute a function of all the previous iterates or compute a function or multiply by x or multiplied by x transpose. And here the f are supposed to be functions, for instance, f t will be a function from r t plus one to r and when I apply two vectors as I apply it here it's vectors as I apply it here, it's a this is F1 is understood that is applied entrywise. So I form, you know, I look at all the vectors of first coordinate and apply the function, the vector of first coordinate apply the function and so on. Okay. So does this definition make sense? So this pretty much any first order method for this. First order method for this type of problem fits into this framework. And in particular, gradient descent, accelerated gradient descent, are easily seen to be special examples of this algorithm. And it's very difficult to come up with things that don't fit this type of algorithm. Now, this does include other types of algorithms that are called second-order methods, for instance, Newton method. But okay, this is the limitation. Okay, this is the limitation of the framework, and in a sense, the Newton method is a powerful method, but in high-dimensional statistics, it's not that much popular because it requires inverting a big matrix and you don't want in practice to be inverting big matrices. Okay, now the question is: can we analyze general? Analyze general and possibly find the best one. And here, when optimal one, we mean statistically optimal. Okay, there is a large theory that tries to find the best first-order method from an optimization point of view, but this is not clear that it implies really the best algorithm from a statistical point of view. Okay, that is on random instances. So we can do this in a few settings, in a couple of settings. So here I fix a specific setting for this lecture. So the answer is positive and you know. Answer is positive, and you know, in some conditions. So, I'll fix one specific setting in this lecture, and then in the future lectures, I look at another setting. The setting that I look at in this lecture is again a regression, and it's a generalization of the Lasso problem that I describe. So, I left my matrix X, so this is my matrix whose rows contains the quadrat. I'll take them. quadrate, I'll take them to be IID normal 0, 1 over n. And here the fact that they are normal is not very important, but it's important the fact that they are IID and okay centered and okay the variance is scaled in the in the right way to make things convenient. And I'll take n over d to go to delta. To go to delta, okay. So this is some Man number in zero infinity. So these are rectangular matrices, but with aspect ratio that is of order one. And then I'll assume that I have responses yi that are some function h of xi transpose theta naught and some noise. So think for instance as the noise. So think for instance as the noise as normal zero one doesn't matter really and the age for instance sufficiently regular okay so this generalized the special case of LASO where this function h is the is the sum and then I'll take this theta This theta zero, so the true vector, and then you have this theta tilde that appears in the iteration is really meant to be an initialization. So the theory includes the possibility of initializing with something correlated with the truth. And I'll take this as IID with some probability distribution mu of theta zero, theta tilde. And this is And this is a probability distribution on R2 and the Wi, okay, okay, the Wi, we wrote it normal 0, 1, doesn't really matter. Okay, in general, I can call this distribution mu w. Okay, well, I can Andrea, yeah. Um, okay, so we had a question about uh why we include uh Why we include earlier more than just the previous iterate. And I responded that, you know, methods like accelerated gradient descent require more than just the previous iterate in their update. And then we had a follow-up, which is what is the intuition behind including previous thetas? Do you have a good answer for that? A good answer for that. A simple intuition. Well, okay, so there is something that is simple. There is a simple intuition beyond. There is one method that includes a memory term. So, for instance, this accelerated method that Michael mentioned is including t plus one, t and t minus one. Okay, and if you include it in the right way, this achieves the optimum. Way this achieves the optimal convergence rate for optimization in a certain setting, for instance, smooth sterically convex functions. So why you need the term T minus one? I mean, there is one intuition, but it's not completely correct that there is this type of class of methods that were studied in the Soviet literature that are called heavy bombs. Literature that are called heavy ball methods. And so if you think of gradient descent, gradient descent is really a discretization of something like this, right? Now, turns out that when you discretize it, this overshoots. So the classical situation in which it overshoots is this one, right? You have a quadratic function that is very, you know, anisotropic and great. Anisotropic and gradient descent. If the step size is not exactly right, we start doing these kind of things, right? So we'll converge, but we'll converge slowly because at this point here, the gradient is in this direction, while the minimizer is down here. And so, okay, so because of that, you might want to add a memory term, a second-order term. A second-order term. So you can try to do instead the following: you discretize this. Okay, and in fact, accelerated gradient is basically a discretization of this. It's a second-order thing, and this reduces this kind of problems. And okay, one way to think of it is that you have, you know. To think of it is that you have, you know, again, as the word says, a heavy bowl. So, you know, a system that has some friction, and you carefully tune the friction as to converge as quickly as possible. So, you know, if you want to discretize this now, you need at least three times. I mean, that's my answer. Okay. I don't know if. Okay. Okay, so let me go ahead. And okay, so this is the setting in which I'll study this moment, this methods for this lecture. And let me now state a theorem. I state it without all the details, but just to see where we are going. So, we can prove things of this type, okay, for this model for any G FUM, so for any method of this type, we have a lower bound. Okay. Okay, so this is you look at the lower bound, uh you look at the the the the estimation error after terrific. Estimation error after t-iteration, fix t and let n and d to infinity to have a fixed ratio, and you have an explicit row where bind, okay, where to at t is explicitly given. And I will not describe it here because it's explicit but is defined recursively, and I don't want to download the recursively. They don't want to down the recursion except in special cases. But the interesting fact is that further there exists special GFUM that has called base. That is called base MP that achieves the lower bound in the sense that the limbo This is equal. Okay. And okay, so in the rest of the time today, I don't know, okay, today and tomorrow probably, I want to provide some meat to this theorem. And okay, so let me describe at high level what is the proof scheme. And not just for the sake of learning this theorem, but because you know this allows. Theorem, but because this allows me to explain a bunch of other concepts, the high-level proof idea is first step will be a reduction from the general class of GPUMs to a subclass of algorithms that I'll call the approximate message passing algorithms. And I will describe. And I will describe just the content of this deriduction, but not the reduction in detail. It's not particularly instructive. And then I'll describe the fact that this AMP algorithm can be analyzed in a sharp way in this limit. And finally, Finally, argue that this special algorithm, BSAMP, is optimal among AMP algorithms for this problem. Okay, so this is the scheme of the proof and the plan for this lecture, and at least part of This lecture and at least part of the next. And I want to, you know, I will not describe all this step in detail because it's a bit technical and not very useful, but I want to start by describing an example in which I can write things a little bit more easily. And this example is again the sparse linear regression. Okay, so now this example is instructive, and because things can be understood very easily, is a little bit the example that is most interesting because it's a convex optimization example. So there is a lot of theory for optimization and convex, for convex optimization that has already important implications here, but Here, but okay. The interesting fact about the theory that I'm trying to describe is that it applies also to non-convex examples. Here I didn't assume in any point convexity. Okay, so let me perhaps describe a non-convex example just at the level of example, just a level of defining it. Okay, so first example, perhaps. I want to give you an example. I want to give you an example of a non-convex regression problem. And this is what's called phase retrieval. It's a very nice problem in which you have an unknown vector theta, theta naught. And in linear regression, what you do is that you measure, you can think of linear regression as Think of linear regression as you measure linear projections of this vector theta in random directions. Okay, so you measure the scalar product of theta naught times xi, where xi is a random vector. So this is linear regression. And perhaps your measurements are noisy. So you measure the projection on this, and geometrically, therefore, what you are doing. So, geometrically, what you're doing is that you have this vector theta naught, and then your vector x1, perhaps x2, and you project this here, and you project this here, and you measure these two quantities. Okay, so it's the problem geometrically is basically intersecting a set of hyperplanes. Now, in phase retrieval, what you're doing is this. Doing is this. So you measure the square of the scalar product. So you are trying to solve a set of quadratic equations. And one way to solve it is to minimize the cost function. Okay, so this is the same list square objectis, but now there is the square inside. But now there is the square inside. And okay, so now this is immediately a non-convex problem, and so all sorts of crazy things happen. Okay, so this is an interesting example. I don't want to go into this. I want to use a simpler example to describe the general theory. So I'll go back to the linear regression problem and therefore sparse linear. For sparse linear regression, and therefore, my cost function now for this example will be this one. Okay, so I close the parentheses of phase retrieval. And okay, let me describe what is proximal gradient here. Let me get the proceeds in this way. Okay, so this is a specific algorithm, approximate gradient descent. Here, the function eta, we should define what it is, is this function. is constant between minus alpha and alpha and equal to zero between minus alpha and alpha and is equal to is linear it's a fine outside this is this function it's called the soft thresholding function okay so this algorithm is So this algorithm, if L is an upper bound on the maximum eigenvalue of xx transpose, then this algorithm is guaranteed to converge to a global minimum of this cost. Okay, and okay, there are various ways. It's basically gradient descent in which you take into account by this function, you Into account by this function, you take into account for the fact that this regularizer is not differentiable. So, this is a very, you know, it's probably the simplest and one of the most popular algorithms for this problem. I want to instead describe another algorithm that is an AMP algorithm for this problem. Perhaps I should say a word about how is this derived. This corresponds, you know, one way to think about it is that you. One way to think about it is that you take your cost function and you compute an upper bound that is a quadratic upper bound of this portion, and then you minimize at each step that quadratic upper bound. Now, instead, I want to define another algorithm that is this approximate message passing algorithm, and this takes instead. And this takes instead this form. Okay, so here there is very little changes between these two algorithms. Notice one thing that changes is okay is that I allow me the liberty of choosing a different threshold at each level. Threshold at each level, at each step. I remove this one over L constant here, but the most crucial part is that I added this memory term here. Okay, again, it's not hard to check that the fixed points of any of these algorithms are actually minimizers of the cost function. Okay, now the nice thing about this AMP algorithm. This AMP algorithm, so this is both first-order algorithms, but the nice thing about the AMP algorithm is that one can prove a you know a precise characterization of its asymptotic behavior. And this takes the form, the following form that you know, if you look at the empirical distribution and you know, So I look at what? I look at the pair of vector theta naught, which is the true vector, and theta hat t, theta t, which is my estimate. Okay, so these are two vector. And I look at the empirical distribution of the coordinate of the pair theta zero, theta, theta t. Okay, this converges, you know, for instance. For instance, in W2 distance to the law of a pair of random variables capital theta and capital theta plus tot z okay, where tot is given recursively by Okay, so you know, this in particular will imply that this convergence of empirical distributions in particular. Of empirical distributions in particular implies that if I look at one over d Andreash. Yeah, yeah, I made a mistake here. One second, I need to correct myself because I made a mistake. Let me call this theta tilde it, and really this theorem is about the joint distribution of these two. Okay, of course, you know, if you give me theta. you know if you give me theta tilde i t I know theta t plus one i and and but for theta tilde i t there is a Gaussian limit so and here Z okay here I should write that Z is normal zero one independent of theta and theta is as distribution P theta yes Michael was asking something. Oh, no, that's what I was going to point out. Yeah, yeah. Oh, yeah, yeah, yeah. Okay. Yeah, I made a mistake. So this gives me in particular this kind of theorem. So this recursion is what we call the state evolution. And this type of theorem is nice because it allows me to compute very precise. Compute very precisely, pretty much in this limit, but pretty much anything I want. For instance, suppose that I say, okay, I want to compute what is the distance between my estimator theta tilde t and theta zero after the iteration. Well, I can look at this. This is one over d sum over i. d sum over i one to d of theta tilde ti minus theta zero i square. Okay, very nice. So this converges to, you know, it's just integrating the function x minus y square versus this measure. So this must converge to the expectation of the function theta plus two t z minus theta squared. theta square okay and this is you know trivially is just tot square so tot as the you know interpretation of the estimate in this sense or of course you can look at one over d theta t minus theta zero square this will converge to you know by basic By basically the same argument will converge to the expectation of eta of theta plus 2t z minus theta square. Okay, and so on. So pretty much any quantity that I'm interested in, I can extract from this theorem. So this theorem says that everything is basically characterized in terms of this interesting. interesting recursion. So if I can analyze this recursion and this is a scalar recursion, I can precisely analyze my algorithm. Okay, now if you were there the last lecture, oh I forgot completely about the pause, sorry. So perhaps I'll pause in the excitement. So I'll pause one minute and ask if there is One minute and ask if there is any question about this. No problem. So we have a question. What should the argument inside the A to be in the last line? Oh, sorry, sorry. There is a second argument here. Here, of course, this is the argument. So, I don't know if the question is this one, but okay, I'll answer this one that I've wrote. Of course, I missed an argument here. There is theta plus 2tz and alpha t. Okay, so this is simply applying, you know, this general theorem gives me the limit, in other words, for n. Me the limit, in other words, for any for any test function that is Lipschitz continuous and quadratic growth, you know the psi of theta tilde ti, theta zero i, this tells me this converges to expectation of psi evaluated at theta plus two t z, and this convergence is at t fixed as and then it goes to infinity and theta. Then it goes to infinity and theta. So, this is what the theorem says. Okay. So, so yeah, so this is the argument. So, one way to think of this algorithm or of this algorithm is that at each time I have an estimate. time I have an estimate here theta hat t and then you know I compute the residual so I compute how well that is fitting the data and then I back project that on the thetas by multiplying by X transpose and then I take this soft And then I take this soft thresholding to get a sparse vector. Okay, so one thing that I want to mention is that if you remember, you know, if you were there in the last lecture, you remember that we studied the minimizer of the LASSO and we came up with a pair of non-linear equations that characterize the values of the minimizer. And you'll notice that the two, the pair of the two, The two, the pair of non-linear equations could be written, one of them took the form, took the following form. Okay, and the other one. Okay, and the other one, okay, was something like I brought them slightly different. Okay, so these two equations characterize the minimizer of the last. So, and you will notice that this second equation in particular, this first equation in particular is just the fixed point of this evolution. The fixed point of this evolution. Okay, so this is kind of a consequence of the factor is saying that if you choose the alpha to converge at some point and the tow happens to converge, then you are solving, this algorithm is really solving the last so with lambda related to alpha in this way. So if you find a fixed point of the algorithm, this corresponds to minimizers of the last. Okay, so this algorithm is. Okay, so this algorithm is really optimizing the right cost function. There are other ways of seeing it, but you see also in this way. And the idea is that after any number of iterations, the theorem is saying that after any number of iterations, this theta tilde is approximately the truth plus something that is normal zero to squared t identity. Identity. Okay, so at each iteration, you have a first-order method that has the special property that its iterates are approximately Gaussian. Now, a natural question is, can we improve over this specific algorithm that I described? So I have two algorithms I described, one that is a standard proximal gradient, and one that I can, you know, surprisingly, I can analyze exactly because there's a Gaussian limit. All right, so I gave you two algorithms: one that it's very simple, approximate gradient, and one that I can analyze, right? Now, the natural question is: can I improve over this base, this not base, but this soft thresholding AMP. Andrea? Yeah. So I actually, there's some question about some notation, which I don't recall. You wrote convergence in L V2. Is that maybe we should scroll up? Yeah, perhaps is my variety. Yeah, I think it's just a writing issue. It's just really a W. Yeah, it's a W. I started variety. Right. Okay, you can, you know, of course, this implies convergence in W2 is like convergence in weak convergence plus convergence of second moments. Okay, the reason why I'm peaky about second moments is that if I only have a weak convergence, then I cannot say anything about this test function and this test function. function and this test function is something that I'm interested in because it's it's you know the natural estimation error the square estimation error that you know many people are interested in okay so so now I described two algorithms one was proximal gradient and the other is this soft thresholding AMP and yeah okay the last five minutes I can I want to Okay, the last five minutes I can discuss: can we improve over this soft thresholding ALP? And the answer is yes. Here, there is, you know, if you see, there is this soft thresholding function that is used here, and here it appears again in the state evolution. You can choose this, change this eta in any other function that you wish. Okay. Wish. Okay. And you know, so if you change it to another function, you can consider a general algorithm that does this kind of thing. And then Now I will excite is again a scalar, before was just a sparsitive, now it's a scalar. I will not write explicitly what it is for the moment. I mean, I can write it, but it's not really important. But the interesting fact is that for any such algorithm, the same state evolution. The same state evolution recursion holds sigma square just this is replaced by h t the specific function is and then you can ask yourself okay what is the optimal function that I can Okay, what is the optimal function that I can put here? And you see that the problem here that appears in this state evolution is really that you are trying to estimate a random variable theta given theta plus Gaussian noise. So how do you minimize this square error? Well, you minimize simply by taking the conditional expectation. So h t of y should be e theta of theta. Of theta given theta plus two t z equal y. Okay, so this is you know standard, if you want, is just Jensen inequality, right? This will minimize the right-hand side over all possible function of the function, choice of the function h. Okay, so if you choose that specific h, now we'll get this iteration. H, now we'll get this iteration, what I call the minimum mean square error of the random variable theta at level 2t. So this is the minimum error that you can achieve with which you can estimate theta when you observe theta plus 2tz. Okay, there is such a thing, and this is just. Such a thing, and this is just if you want. So, this is the minimum square. Okay, so now, so this. Okay, so now this gives me an algorithm that improves over in a simple way over the algorithm that I just described that was inspired by proximal gradient. Now, what is the behavior, the typical behavior of this recursion here? Okay, the typical behavior of this recursion, okay, it's nice to It's nice to make a plot of it. Okay, so you have the function here, so two square, and here the y, the line y equal x. And then there is the function 1 over delta, you know, this function here. Typically, it looks something like this. Or you might. Or might look like something like this. And when you run this iteration here, so this is the function, this is the function sigma square plus one over delta MMC two square. And when you run this iteration, what happens is that you start at one point and then you converge to the high. And then you converge to the highest fixed point of this equation. Okay, and there is sometimes multiple fixed points. Now, it's interesting to try to understand how, you know, in the last two minutes, how the structure of this fixed point changes as a function of delta. And again, I'll plot a sketch, and this is a sketch that you will obtain if you. That you will obtain if you look, for instance, at a vector theta that is of the form, for instance, one minus epsilon delta at zero plus epsilon delta at one. Okay, so some mixture with some zeros and some ones. So very simple Bernoulli distribution. And so delta is really the sample size scaled by the dimension. And here, what I plot is what any algorithm, you know, various algorithms achieve versus data zero. And typically, what happens is the following. If you look at something like the LASSO or other convex methods, you obtain a curve like this that is a continuous curve, and so this is what will obtain by the last saw. Is what will obtain by the last saw or a convex method, and also by this soft thresholding AMP that I described, the one that uses the eta function. Okay, now interestingly, what happens is that, of course, if you look instead of the optimal, so what I described here with this choice. You know, with this choice of the function, is what I first call the base MP. Base, because at each step you are doing the base optimal operation. Okay, so the mean square error of this algorithm typically behaves like this. Okay, here I plotted that in matches. Okay. Sometimes it has a Sometimes it has a jump and then it goes down. Okay, so it's strictly smaller than the optimal algorithm, and it has a jump. Then you can ask yourself: okay, what happens if I don't care about algorithm and I just look at the minimum estimation error that can be achieved? Estimation error that can be achieved by any method, even if it is not computationally feasible. And typically, this matches the base AMP until some point and then jumps earlier and then matches the base MP after. Okay, so this, sorry. Sorry, this red line is that the optimal metal that is not necessarily computationally feasible. So, this amounts to compute the expectation of the whole vector theta given the matrix X and the observation Y. So, this will be my theta hat in this case. And instead, the purple one is the Is the optimal AMP algorithm or the base AMP algorithm? Okay, so there is this interesting pattern in which we have three regions. There is a region at small sample size or high noise. So there is two values delta opt, delta star, and delta alg. There is a region in which delta is between zero and delta star. Even the optimal method does very poorly. There is an intermediate region in which, if you use the optimal method, you do well. You do well, but if you use any first order. But if you use any first-order method, you do poorly. And then there is an easy region. So this is sometimes called a hard region. And then there is an easy region or easy phase in which there is a simple algorithm that achieves the Bayes optimal estimate. This is kind of surprising because, in general, computing Because in general, computing this posterior is m p-hart. And here there is a whole region in which this can be done efficiently. Now, there is a broad conjecture here is that this phase diagram is somehow fundamental. That is, no polynomial time algorithm can do well in this intermediate region. Okay, I'll stop here for today, and then the next lecture I will try to complete the idea, general idea of this hopeless. idea general idea of this optimality of of base mp and then i'll move to spin glasses so um we have a question which is why is there a jump why is there a jump um why is there a jump good question um well if you give me a specific so this curve of course uh here depends on this mmsc function and MMSE function. And if you give me the MMSE function, I can, I can, if you give me the distribution of theta, I can compute the MMSE function. And the MMSE function typically is not concave. This is the question. This MMSE function is not concave. And this means that, you know, at very low noise, adding a little bit of noise doesn't hurt you much. So forget sigma. forget forget sigma if you forget sigma this is what really matters is the mmsc for theta depends on theta and this what i'm saying is that this is non-convex right it's not concave so at very small noise even if you add a little bit noise that doesn't worsen your your estimation error or in other words the estimation error is super linear in tau square this is typically what happens or actually it's always what happens if the It's always what happens if the probability distribution P theta is discrete. Think, for instance, that theta is a support on three atoms, plus one, zero, and minus one. Now, you understand that if I give zero noise, if I put zero noise, the estimation error is zero. But if I put a very small noise, you can always reconstruct which of the three you come from. Of the three you come from. You observe, so you have theta and you observe theta plus tau z, where z is Gaussian noise, right? You understand that the error in estimating actually goes down as e minus constant over tau square, right? Because the probability, you know, if you have a plus one, if the r real value was a plus one, you know, and then you observe it with noise, you get mistaken only if noise is bigger than one half. Is bigger than one half in a very simple algorithm, right? So this means that actually it will go down as e to the minus one over eight to square. So this tells you that this curve goes flat at the beginning. And if it goes flat at the beginning, yeah, okay. So there are chances that has multiple intersections. And now the delta changes the slope basically of this. You know, it must saturate at some point. It must saturate at some point, this curve, because, for instance, in this case, the minimum square error is always bounded by one. So it must saturate at one, even if there is infinite, or at two, even if there is infinite noise, some constant. And so the one over delta really changes the slope of this line. So here we are looking at the fixed point of this thing, right? So the one over delta really changes the slope of this line. Of this line, and so depending on the delta, you intersect this curve at one point or at three points, right? And that high noise, you know, what matters is the highest, you know, where the algorithm gets stuck is the highest noisiest fixed point. And so depending on the values of delta, you'll intersect at the highest fixed point or only at one fixed point. So this region is the region where there is only one intersection here, and this. There is only one intersection here, and this is what there is three intersections. We have time. Maybe it's worth bringing up another question that was coming up about the type of asymptotics we're taking. So the first question was about how in optimization we typically look at fixed n and d and t going to infinity. And then a follow-up was asking whether the statistical performance for fixed n eventually gets worse as t gets. Eventually gets worse as T gets large. So maybe you just comment on the sort of fixed T asymptotic. Yeah, so the asymptotic, so fixing T is really means that we are looking at algorithm that have complexity of the order of the matrix vector multiplication, right? So each operation is a multiplication by X or X transpose. This is so the complexities of all. So, the complexity is of order ND. So, this is linear complexity. Now, the question is: can we hope to get much advantage to go beyond linear complexity? In this type of problems, for instance, this linear regression problem, there is type of problems, or in general, this highly symmetric problem with random matrices. With random matrices, there seems to be only one case in which one type of scenario in which you get a definite advantage, and that type of scenario is typically when the initialization is at an unstable fixed point, right? So, one example is the phase retrieval loss function that I was describing before. In this case, if you look at this first loss function from the loss function from the point of view of landscape, theta equals zero, the natural initialization is theta equals zero. And this is a subtle point of L and in terms of A and P algorithm is also a fixed point of state evolution, although an unstable fixed point. Okay, so in that case, in this case and in similar case, one can do a little bit more work. And one way to do this is you do an adult. To do this, you do an ad hoc step to escape the fixed point, the unstable fixed point. So you use a spectral initialization plus order one iterations. Another way to escape this fixed point would be to do. So, this has been analyzed, at least in some cases. Another case type. Another case type way to escape it will be to do order log n iteration with the random initialization. This has not been analyzed as far as I know, but yeah, it would be interesting to analyze. But you know, it ends up doing going from n to one to log n iteration, and that is enough. Now, I don't know of any single example that. Example that or any interesting example in which you know it pays off to do n to the zero.