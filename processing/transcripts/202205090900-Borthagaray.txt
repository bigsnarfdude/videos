Well, let me start thanking Barbara, William, and Diane for the kind invitation to be here. I will talk about linear and quasi-linear operators in league domains. And actually, I will spend quite some time discussing regularity for linear problems. But my mindset is that I want to apply this regularity result for approximation, okay, for deriving error estimates for finite element approximations. And afterwards, I will, if time permits, I will discuss some applications also to extensions to quasi-linear problems. Okay. Oh, this is not moving. Never mind. So, well. So, well, the prototype operator we consider is the interval fractional Laplacian, which is given by that singular interval, hypersingular interval over there, which takes an interval in the whole space and okay, that coincides with the pseudo-differential operator with symbol sine to the 2s. And the model problem I will. And the model problem I will have is fractional matching of u equal f in omega, and the function has to satisfy some boundary condition, which for these non-local operators means an exterior condition. Okay, so I want my solutions to be zero psi, or I want to take a function that is supporting omega, and when I compute this guy over there, I get some data. I get some data. And this is by no means the only way I would consider a fractional operator in a bounded domain, but this will be my model problem. And this problem, the weak formulation, is set in fractional solid spaces in which functions are required to be supported in the set omega. Okay, so functions are globally HS, but they have. Hs, but they have to be supported in omega and uh well the variational formulation we have some nice integration by part formula so we can just take a variational formulation and uh well essentially uh what we have as the bilinear form is just the standard inner product in this H tilde S space so there's no no issues regarding existence and uniqueness and continuity of solutions with respect Continuity of solutions with respect to data. And of course, we can also regard this solution as a minimizer of a certain energy, which is given by this F over there. So everything, I mean, if I take just S equals one formally, I will be taking just the standard Virus Let's problem for the Laplace. Okay, so. Okay, so I care about finite element discretizations. So this is my motivation for discussing regularity. And typically, well, if I take conforming finite elements, I will be obtaining essentially a best approximation with respect to the energy norm. And of course, once I have this best approximation. Have this best approximation property, it boils down to estimating how close the discrete spaces are with respect to the solution. So, if I can build some interpolation operator and I can use some regularity, I can create some estimates, error estimates for my finite element solution. Okay, so of course, a priori, if f is just in the dual space of this tilde space, I will not. I will not get any regularity. Okay, not expect that. But the obvious question is whether, if I have some smoother right-hand side, whether I can get some regularity for my function E. And just a little comment, for finite element applications, since I will be doing piecewise linear mostly, my domain would be a polygon or a polyhedron. Okay, so I would. Okay, so I would actually need to expect to have angles in my domain. So this will be a little bit important, actually, in what I will say. Okay. Let me first review some regularity of solutions. So we will spend quite some time discussing regularity and various forms of regularity estimates for this problem. The first one has to do with this. The first one has to do with this pseudo-differential characterization and one can use very powerful techniques which go back to the 60s but there has been very important recent progress. And apparently one would expect that everything would carry for smooth domains because one would be doing Fourier transform techniques. But actually, this very recent result by Adams and Group allows for a little bit, well, quite some improvement, although not enough for our purposes, because as I was telling you, our domains will not be C1, would not be as smooth as that, would be just digits. But in general, well, if F has some certain regularity, so we'll fit each of class. Certain regularity, so in fit each of class HR, what we can expect is that regularity order. So if R is small, we would expect to pick up 2s derivatives. We have an operator of order 2s. So that would be very natural. But as long as S plus R is greater than one half, we actually do not get that. Okay, we get stuck. So if F is infinity, so if it's super smooth up to the boundary, So, if it's super smooth up to the boundary, anyway, we will not get anything better than HS plus 1 half, even if the domain is smooth. And some other type of results had to do with potential theory techniques, in which if you have a domain, ellipsic domain satisfying some exterior condition so that you can build barriers. This very nice paper. This very nice paper by Rosaton and Sarah shows weighted helder estimates. So essentially, we're characterizing a held irregularity with some weight here that has to do with the distance to the boundary. So that weight is somehow telling us how solutions deteriorate near the boundary. And just to illustrate that these results are actually sharp. Are actually sharp. If I take the compass of all examples, so I take my domain to be a wall, and f is a constant, then the solution would be essentially like this. And that function behaves roughly like the distance to the boundary to the power s and I will get back to this in a couple minutes, but that function is not of just hs plus 1 half just because of that annoying boundary. That annoying boundary regularity. But actually, if I use this type of weighted estimates, well, I can put some weight here that has to do with equal to the boundary and then compensate for that boundary behavior. That would be a nice goal. And that's something that I did for my PhD with Gabila Costa from Muinocytes. From minusitis. We built some weighted spaces in which the weight has to do with the distance to the boundary. So these are some solid and fractional solid spaces just with that weight over there. And then, okay, under the same conditions, so here I'm just exploiting or reinterpreting the Rossoton and Sarah results. Results, we can get some higher regularity. So, t is s plus some number that will be greater than one half for any dimension at the expense of this weight distance to the power two gamma. Okay, and this is essentially the most we can get out of our solution in domain satisfying an exterior condition. Okay, there is some beta over there that will appear a lot in this talk. Just beta, every time you see you see beta, it will be that parameter over there. But just think of beta as being as high as need. Okay. Other way to interpret this Rosaton and Serva estimates is something we did with Ricardo Decetto, who I think, yes, he's in the other. I think, yes, he's in the audience. So, realistically, if I take in one t, I take just a function that is x to the s, the positive power of the positive part of x to the power s. So just a 1 d version of distance to 10 to the s. And I take a formal Riemann dual derivative, so to say. Well, heuristically, I will get some x to Will get some x to the s minus t, my fractional derivative is of order t. And if I wonder, well, this should be locally, right? L be loc in R, well, I need t to be less than less plus one over. And essentially, that is the regularity we would have for the distance to the boundary to the power s. On the other hand, if we consider On the other hand, if we consider how well we can approximate or whether we can exploit this higher regularity to get some approximation, well, we also need some inequality that has to do with the sodium numbers of the spaces we're considering. So we require that the sodium number of this WTP of omega to be higher than the sodium number of HS, and that will bring another constraint on T, S and P that has to do with the dimension. B that has to do with the dimension. So, when I put together this heuristic reasoning with this solid number argument, I see that the conditions meet at that point over there. And what we can do by doing some more rigorous argument, which actually has to do with this Rosaton and Sera weighted estimates, is that under the same conditions, we get some regularity results. Some regularity results in solar spaces. Well, now with the order less than two with no weights, and we also get the same differentiability as before. So you see here I had t to be s plus d over 2 d minus 1. And in this weighted estimates, I had the same thing, essentially. So essentially, I'm interpreting the. So essentially I'm interpreting the result in two different ways. Yes, or um you're you're using a C norm here for the F C A C norm instead of a sober F norm. So does this come from? Yes, that has to do with this exactly with this estimate over here, which I'm these are mostly of natural of held their actually I didn't write here the regularity of F sorry. I was a I was assuming I should have written that F is C beta up to the one here. So, yes, essentially, that's an issue with these estimates, actually. Since I'm using these Helder estimates and reinterpreting them as sub-level estimates, I end up with some mixed norms. So, yes, in the two cases, I'm having sub-level estimates in terms of regularity of the reference, which is Of the right hand side, which is not ideal, so to say, okay. Uh, one issue with these estimates is that we require an exterior vault condition, okay? In either these weighted estimates or even for small domains, we have an exterior vault condition. And if our domain is just, I know we've learned 2D. I know we've learned 2D and we're considering a polygon, that means the polygon has to be convex, and therefore they are not ideal, ideally suited for finite element analysis. So I want to discuss next some regularity in Beso spaces. And this is a very different technique that I actually want to spend some time commenting on. So let me begin by. So let me begin by just recalling one of the many ways to characterize special spaces by taking difference quotients. Here, just because I care about spaces of order up to two, I'm taking second differences. And if you take second differences, you essentially take your second difference of your function v. Okay, so you measure that in some LQ and then you do some weighted integral. And then you do some weighted integral over D, and D here is just a disk, okay? Unit, the unit ball in RT. And of course, if I take spaces of order less than one, I could also use first-order differences. Okay, but since I want to have something robust for sigma between zero and two, I will focus on second differences. And this is. This is the standard notation, which I don't know, but there's a dot up there. Okay, when I have the dot, I mean zero extension. So it's the analogous of the tilde spaces in the solar context. And we have this nice property that besides spaces of non-integer order coincide with the fractional solid spaces, okay, in general. And And let me let me revisit the example. Okay, I told you before the example, the nice solution distance to the boundary to Rs is not of plus HS plus one half. So let me see why. So for simplicity, I assume S between zero and one half so that I can take first differences. I take that function over there. And then I have to just, I want to measure. To measure the N2 integrability of the first difference. So I take the first difference, it's 1G, so it's pretty easy. Say the annoying term would be something like that. Okay, so I would have a couple of terms, but essentially they all behave like H to the 2s plus 1. So when you take the first difference and you measure its L2 norm, I get an HS plus one half. norm i get an hs plus one half just very easy therefore when i substitute this guy here and i measure that now i take a power q uh this q to the s plus one q times s plus one half we cancel out with that guy and i will get a singular integral so that means uh this function is not in uh is not in this space It's not in this space for q being a none. While if q is infinity, if I take just the supremum, I get a nice cancellation and that remains one. Therefore, this function is in this space, but it's not in particular, if I take q equals two, is not in the solid space for the s plus one half. Okay, so that means that the correct or every The correct or heuristically the correct framework to measure the regularity of solutions would be this type of solvent spaces rather than the fractional order solvent spaces. Okay, so there's a very, I mean, the difference between these two spaces, they have the same differentiability, but this space is a little bit larger than the solid space for the response one. Okay, so let me just state the result I want to discuss. But first of all, this result will be valid without an exterior condition. So this will be valid just for arbitrary Lipschitz domains. And I will assume F to be in some pencil space with differentiability order minus S plus one half. Order minus this plus one half, which is essentially the natural space to pick up one half derivative. I'm taking the same problem I discussed at the beginning of the talk. And what they want to prove is that the solution belongs to this peso space with second parameter, second integer ability parameter infinity, and we have continuity. And we have continuity with respect to the right-hand side. And in particular, if I did some sovereign embedding argument, I would be recovering this nice sovereign estimate. But I would also get, you see, I have an epsilon here for arbitrary epsilon. The dependence on epsilon, I can get it explicitly. So I actually get how these sovereign norms deteriorate when epsilon goes to zero. And okay, let me also say that there are some related work in which these authors obtain either nice asymptotic expansions around corners or nice analytic regularity estimates for polygons. But the result I'm announcing or commenting on is valid for libgit domains, aren't truly libgit domains, actually. Are we truly legitimates, actually? Yes, sure, please. How is it about duality between these spaces? So, if you would remove the plus one-half, would the F space and the U space be dual to each other? Well, not precisely dual. Yes. So, the duality works one way, right? Because we have a second parameter between the one and an infinity. Yes, yes, yes, exactly. So, but it was. Exactly, so okay, but it works like in the LB case, right? So, okay, so of course, if I take some p and the helder conjugate over there, you get some nice duality, but when you finish just a little bit different, okay, so what's what's the overall approach to get this regularity? Uh, this is something we began discussing with Le Carlo and Chetto, and then we also had some nice ideas by one more Li. By Wenbo Li, who I also think is in the audience. So essentially, if I want to measure some best of norm of, okay, for I'm operating everything in LB because just because, because actually this algorithm is also valid in LB spaces, and that will be useful for watching linear operators too. So if I want to measure some norm of order s plus sigma, well I can take second differences and scale Well, I can take second differences and scale them, measure them in LB and scale them by h to the s plus sigma. But actually, a second difference is just an iteration of the first differences. So actually, the first difference, I can, the second difference, I can write it as a first difference of a first difference. And then use one of the first difference to create a best of norm of a little bit higher. And let's leave that there for a second. Let's leave that there for a second. My solution minimizes this function of the Casac term and a linear term. And actually, this is quite simple because since we're solving this linear problem, we actually get this nice characterization, right? If I take the energy of any P minus the energy of the solution, I get Of the solution, I get exactly for this linear problem, I get roughly the one-half times the solvent norm, the square of the solvent norm of my function minus its pollution. Therefore, if I combine this guy, I take here I take p equal to and r equal to, so I get a solid norm there. And if I instead of taking, okay, say so if I Okay, say so if I take U and I have the first difference of U, well, I could take V to be the translation of U. And actually, if I did that, I would expect to get some regularity, right? Because I would get U here, the translation of U, and the translation of U would have to do with these functions. So if I can measure, if I can bound the energy difference when I take small translations. When I take small translations, I will be proving regularity. Now, the annoying thing is that actually if I take a translation, I'm moving out of the domain problem. So the translated function will not be admissible for the energy. So what we need to do is to somehow compensate or somehow fix this. Fix this annoying detail. And first of all, let me say that actually, in the definition of peso semi-norms, in which we took we measured in some balls, page is just a unit ball or any radius. I can replace that by a column. And if I do that, say if I have just a nice Say, if I have just a nice cone in Rd and the cone is inside some big ball, well, of course, if I'm measuring best of regularity with second parameter equal infinity, so I'm taking some supremum. So if my cone is inside some big ball, of course, the supremum will satisfy this inequality here. But actually, if my cone has an opening, and by taking differences in that opening, I can create. Differences in that opening, I can create, I can somehow recover a smaller ball. Okay, and that smaller ball has to do with the opening of the cone. So when I take the best of norms by taking differences in combs, I actually also get something that controls the best of norms by taking differences in this little book. Okay, and these two, well, they are actually equivalent up to some constant. Actually, equivalent up to some constant that has to do with this radio here. So, row zero and row. So, they are all equivalent. And of course, the equivalence has to do with the opening of the code and the radii, and therefore, and that's the radi of the balls too. Okay, so this is important because now I can restrict to some specific directions given by some column. And of course, if I have a dipsy domain and it's bounded, I have to have. Libs' domain and it's bounded, I have a uniform common property. So there is some fixed angle in which you can move nicely, so to say, in this way. If I take H in this cone, the direction, well, of course, will depend on the point on the boundary where I am, or near the boundary where I am, but the opening of the cone is fixed. So if I take points that are outside Amira and I move them. Are outside omega and I move them, I remain outside omega. So everything that is outside omega stays outside omega. And this is very desirable because I want to preserve the zero boundary condition, so to say. And now I want, of course, not every single direction will be good for all the domain, so I will need the directions to depend on where I am. So I will do. Where I am. So I will do some kind of partition of my domain, some bolts, and do some localized translations in each ball. And the way to do that is by taking some nice cutoff function, okay, which will always be the same. And it's one in this little ball and it's zero outside this little larger ball. Little larger ball, say little bit larger ball, and what I do, my translation takes v it's like an optimal transport type functions to say. You take v and you evaluate it in the translated of x. Now, of course, because phi coincides with the identity in this little ball, this is just a translation inside the little ball, and because the support of phi is in this. Is in this disk of radius 2 rho. Outside that disk, I will get a zero here, and this is identity. And if I do that, if I use this translation, and now I take h in the cone corresponding to that point x0, where I'm taking the center of the ball, I will get a nice translation function, translated function, in the sense that if I take a function in HP, in the sense that if I take a function in hv in hs sorry in h tilde s and i translate it this local translation will remain in h tilde s and this khv will be good for i mean will be in the domain of the energy and let me just say one more remark regarding this localized translation uh i could also of course i can write it like the composition between v and this function sh and And this function sh well is essentially an identity, okay, up to first-order curves. And this translation does the work we wanted to do in the sense that if I want to measure some lower order based on norm of V minus my translation, I can get the H outside of the norm and take the norm. Take the norm, the rest of the norm of V in, of course, in a slightly larger ball. Okay, so it does the same job a translation does in the disk of Radius 2 Row. It does the same thing. So it has the nice properties we want a translation to have. Okay, so let me go on. I have a function that is globally in HS, so I want to do some localization of these norms. I can do that. Norms. I can do that by taking a covering of omega. Okay, I need some overlapping there between the balls. And of course, I can take a finite covering because omega is bounded. As long as I fix fraud from the beginning, it's fine. The only thing is that this constant here will depend not very nicely on the cardinality of the covering, but anyway, it's a fine constant. And therefore, let me just. And therefore, let me just repeat this thing because I think it's important. If I can get some nice estimate of how the energy changes when I take a local S translation on each ball, on each of these small balls, by putting together the numbers on each ball, I can get a global ST. So, if I want to measure the remarkable view in some high-order vessel space, well, I can. Space, well, I can take, as I said before, the first differences in HS. Now, when I take in HS of this little ball, the translation coincides with the localized translation. And if I can prove this guy here, actually, I could get that the meso norm of order s plus sigma or two will be bounded. Okay, so if I can pull out an h to the sigma from the energies, I will get. From the energies, I would gain regularity of order sigma over two. And of course, here helps Riemann and invokes the sum or the difference between a quadratic function and a linear functional. I can argue with the two of them separately. Okay, and then just adding the supremo. Okay, so let me briefly comment on how things are done for these two functions. So the first one is So, the first one is quite clear because it's just a duality. So, if I have that F, say my right-hand side has some regularity for the T and my V has some regularity for that sigma minus T, I can actually pull up a sigma. And that's not difficult to do. First of all, Um, first of all, when I write this difference, I get this integral here, and now, well, if I want to take the sigma out of V, so to say, I can do that here directly. And that will give me one endpoint here, t sigma minus one. While if I want to go to the other endpoint, I need to take the translation to F. So I can do that just by doing it. So, I can do that just by doing a change of variables in this term. I would get something like this. So, this is just some translation of f in the other direction. And I get the Jacobian, but the Jacobian is essentially one up to higher order terms. Okay, and then once I have the estimate for the two endpoints, I can do some terpolation because this is just a bilinear operator. The operator that maps F and B to the difference. To the difference. Next, for the quadratic term, well, I wish I had some choice at this time. So this is the thing, right? So when I'm measuring this term that has to do with the so-level norm, that measures the translations everywhere. So let me. everywhere so let me let me say how here have rt times rd and the thing is this right i in principle i would have to to measure the the interval of these two guys over the whole space but the thing is this uh when i take my little ball okay so some disk there and take the disk here in principle In principle, I should need to integrate everywhere, but the translation coincides with the identity outside this disk. So when I take x and y in my integer that are both outside the disk, the two functions coincide. So there is no integration here and no integration here, no integration here, no direction here. The only part in Rd times Rd that actually matters is the disk times when times when when either x or y say are in this in this disk okay so that's why you get that strange integral in the right hand side and actually i can i can pull out a linear term so to say out of that so you get an edge here and okay so this is this is just doing a change of variables and exploiting Doing a change of variables and exploiting that this translation is stated. So it looks like a lot, but it's just a simple calculation. Once you get what you need to do, it's a straightforward calculation. Okay, so once we have these testimonies, what I have is the following. If I have my right-hand side of class of Of class of order t, you know, some vessel space with differentiability order t for t greater than minus s. And if by some reason I have that the minimizer of the energy has some differentiability order sigma minus d, this would be the estimate I would have, okay, by combining all the estimates I discussed before. And now we want to use that formula for doing an iterative argument. An iterative argument. So I just fixed some parameters. I wanted f to be further differentiated in the order of minus s plus one half. That's what I have here. Then I just take sigma to be zero to begin an iteration and define sigma k plus one like that in terms of simple k. So we have some recursive formula. I replace these parameters in this formula. These parameters in this formula here, I get something this way. And now I just run them an induction argument. Okay. And in the deduction argument, I get, in the end, I get an estimate this way. And the constant that appears here is bounded for independently of k. Okay, so in the end, what they get is. In the limit, I can take k to infinity, sigma k will tend to one, so I will get differentiability order s plus one half and some uniformly bounded constant times the regularity of f I was assuming. And this very same argument can be applied also for f in a two, which will also be of interest. Will also be of interest for our applications. Of course, if s is greater than one half, I'm already getting the best regularity I could get for a rougher F. So S greater than one half is not important when F is in L2. The interesting case is S less or equal to one half. And what we can prove is that Is that when s is less than one half and f is in l2, we can actually pick up the solution u is actually a class has differentiability to s. While if s is one half, we actually do not get that, we almost get that, but we have some annoying epsilon there, and we can actually be very explicit on the dependence of that norm with respect to Norm with respect to website, and the argument is essentially the same. Okay, use the recursive formula and do some iteration. Okay, so let me now use all of these regularity estimates for finite elements. So I'm taking a conforming finite element approximation with piecewise linear functions, which are continuous up to the boundary of the domain. So I'm solving the variation on. So I'm solving the variational problem now in these discrete spaces. And of course, again, for free at best approximation property. So now I know what the regularity of view is. And now if I want to measure that error, I can of course use both the regularity of view, but I also need to take into consideration the non-local nature of the norms. And that is something that That is something that Furman did quite some years ago in a very constructive way. So, actually, if you take your triangulation, you take the patches surrounding your elements, and whenever your element is touching the boundary, okay, you can allow it to go a little bit from the boundary. So, this ST1 would be a patch, and with the tilde over there, it will be like an extended patch and the elements start. When the elements touch the boundary, and you can get this type of localization. So essentially, here you have a local HS norm and a lower term. Okay, so in shape regular measures, you can get some nice approximation estimate. And by using that, you can directly now apply the regularity estimates and this interaction. Estimates and this interpolation estimate to get error bounds in the energy norm, say for quasi-uniform meshes. But also, if you use the regularity in weighted spaces, you can also grade, apparently take some grading of your mesh. So elements become smaller as long as you approach the boundary. And by doing that, you can And by doing that, you can actually have a little improvement on the conversions rate. So this number, of course, t over t minus one is greater than one. So this number is a little bit greater than one half. So you can actually improve your conversions if you grade your meshes appropriately. And that, if you take the CD example, I commented before. I commented before, you can actually see that in implementation. So I solved this in 2D. I'm not discussing how I did that or how we actually implemented the method, but for uniform meshes, you see an order of one-half. While if you take greater meshes in two dimensions, okay, d is two, d minus one is one, so I get, I should get. Minus one is one, so I get it, I should get linear rate, and that's what they do. Okay, so we actually see quite some improvement by doing a priori grading. Of course, the issue is that doing some a priori grading can be difficult in practice. And that's why actually with Ricardo Narcetto, we did this, we considered this constructive, this, I would say. I would say unweighted solid estimate that uses the same prosoton several bounds, but interpreting them with by changing the interiability order. So the thing is this, if you want to, okay, of course, the interpolation error, you get a bound like this that has to do with some higher regularity of the solution. And now if you use the solar use the the sovereign or sorry the the weighted helder uh rosoton and serabons and interpret them in a in this weighted in this sorry unweighted uh solar norms you can actually see that this norm has to do with the the distance between the the element and the bundle okay so we can actually use this as an error estimator and just run some adaptive algorithm Run some adaptive algorithms so the Xt is the barcode? Yes, yes, yes, sorry, yes. Actually, yes, because of course, if the element is away from the boundary, nothing to be done, but if it touches the boundary, you need to take it into account. So this, you can use it as an error estimator. So you start with any mesh and just run, well, we used a greedy refinement strategy. Refinement strategy. Okay, so you just refine those elements for which this error estimator is greater than some tolerance. This will stop and you get the same up to a lower ending term, lower ending term, you get the same rate you got with greater measures. Okay, but now this is you can do this easily in any domain. Okay, and you don't need to. Means okay, and you don't need to actually construct the grade, okay? So, I'm very behind on time actually. I'm going much slower than I expected. So let me, let me, okay, let me just say that once you have some nice best of estimates, or actually sub estimates, because you can interpret the best of estimates in terms of sub unknowns. You can also, of course, you get some nice. And so, of course, you get some nice shift theorem, and you can use that to run a standard Obini argument. So, you can actually prove convergence in N2 as well. And just as an application of that, or another way to use that, is by getting some local energy error estimates. This is something we did with Something we did with like Ado Nachetto and Dimitri Leikeman. So these local energy error estimates are discrete versions of what's known as the Cacciopoli identity. And the Cacciopoli identity roughly tells us that functions that are harmonic do not oscillate much. And you can get some fractional order of Cachiopoli inequality. Okay, which says that if you have an S-harmonic function in some model, okay, and some condition that will not be important for our application, the solar norm inside some little ball can be controlled in terms of the L2 norm in some larger ball and some term that has to do with the behavior at the field. Okay, and that is something for Is something for S-harmonic functions. The error, the finite element error, is not really harmonic, but it's discretely harmonic in the sense that if I take my finite element error, it satisfies some property like that, but only when you test with discrete functions. So if I could just take the finite element error here, I would get some convergence rating, some little ball in a chest. In some little ball in a chest, in terms of the rate in some L2 norm, which should be of higher order. Okay, but I cannot do that directly because my discrete functions or my finite element error is not harmonic, but almost. So you need to work a little bit there. But what we can prove is that on shape-regular meshes, so no need to assume meshes to be uniform, and under the Uniform and under some mesh size restrictions, you actually can have the local energy norm error bounded in terms of an interpolation error in a yes in a greater ball. I'm getting something up here. Oh no, and uh to know uh sorry an interpolation error in some Sorry, an interpolation error in some in the ball plus a global pollution term. Okay, so if this ball PR is very is compactly contained in omega, I do not reach, do not need to reach the boundary. So if U is smooth away from the boundary, I can get some improved rate here that will reflect an improved rate here. So in the local energy, in this local energy norm, I will. Energy norm, I will get something better than in the global energy norm. And that, let me just comment how things read. But for example, in what uniform meshes, when I compare the global rights that I had proved, which are essentially further one-half, well now they improve to be further oh, this should have been one actually, here we were. Oh, this should have been one actually the earlier. Now I can do this with one, yeah. But also, even if it doesn't make sense a priority, if I grade my dimensions near the boundary, I get some improved rates in the theater of the domain. So this is how they compare. So for a little less, I get an improvement of order s, while if s is greater than one half, I get an improvement of order one minus. Improvement of order one minus s. Okay, and again, the boundary behavior is somehow affecting even the interior, the interior conversions you get. Okay, so let me very briefly, I know I'm running out of time, but let me very briefly discuss some applications also to quasi-linear operators. Okay, so I take So I take fractions over spaces of order with integrity p, and now p can be any number between one and infinity. I could consider the minimizer of this energy, which when p is true, boils down to the same energy I discussed at the beginning of the talk. And now the minimizer of this energy is a solution. The other Lagrange equations for this minimizer are this system here, in which you get what's called the You get what's called the fractional p-daplacient. Okay, so now the difference between this guy and the fractional Laplacian is that they have this non-linearity or this term for the p-minus cube. So the weak formulation for or the weak form for this problem is the same as before, but they have this way here. So this is what's a linear operator. There are some regularities. There are some regularity estimates for this operator, but let me just say that since you have this nice monotonicity, in the sense that you can actually also bound the difference of the norm of your solution minus any function v in terms of the energy, you can roughly apply the same machinery I discussed about the best operation. Discussed about the beset regularity, and what you get is some, I mean, very full of parameters result, but in which essentially if f is in the correct space, now you have some derivative for their p prime. P prime is the helder conjugate of p, you pick up one over p derivatives on s on u, as long as p is greater than one half. As long as p is greater than one half, while in p is less than one half, what we can prove is that you can actually pick one half. This, of course, you can interpolate between this estimate and the well-posedness of the problem when f is in the dual space to distill space. And you can also prove intermediate regularity estimates. And also, if you take if you want to measure your regularity in terms of Your regularity in terms of solid norms, you can just do an embedding and get something very explicit again with respect to this parameter epsilon that appears there. So this is very useful also for finite elements. You can do that. And we can actually, by just mimicking some classical technique for the tiltlation by Cho, we can get some error estimates for Uh, error estimates for this operator as well, and okay. I very briefly comment on a small issue. Uh, we run a very silly example in 1D, and what we observe is that on uniform meshes, the error behaves like one over p. And one over p has to do with the interpolation error. interpolation error okay if your solution belong belong to one over to w s plus one over pp uh we cannot prove that for p less than two so we would expect from our regularity to make one half here we see something better and what's even worse is that our theoretical rates are not optimal even even in that case so uh what we can prove is sub-optimal with respect to interpretation Is suboptimal with respect to interpolation error. Okay, and again, as for the linear problem, you can improve your conversions by taking rate measures. I'm skipping this because I'm out of time. But let me just wrap up what I discussed in this talk. For linear problems, we spent quite some time discussing regularity. What's the system? What's decisive in these linear problems is the boundary behavior, and that can be compensated either with graded meshes or adapted meshes, which you can do constructively. We discussed some error estimates in the energy norm, in N2 norms, and in the local HS norms. And the technique I discussed for BESTOM regularity applies also for quasi-linear problems and actually can also be extended with two problems. With two problems with variable diffusivity or finite horizon, which are also interesting in practice. For quasi-linear problems, we can do some error analysis in the energy norm. But we have two little limitations which we are working on. First of all, our theoretical rates are suboptimal for the singular case in which p is less than 2. In which p is less than two, and the convergence estimates we get are also suboptimal with respect to the interpolation error. So we would expect some better conversions rates given the regularity we have. Okay, so that's all I wanted to say. Thank you very much for your attention and I'm glad to take questions.