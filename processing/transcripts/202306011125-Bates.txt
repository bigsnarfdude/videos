Eric Facebook classic with one time. So thanks all. Thanks to the organizers for the opportunity to speak. This has been a wonderful meeting with such wonderful people, wonderful talks, and in a wonderful setting. So I look out at the mountains and I kind of am reminded of these height functions. And the mountains kind of resemble to me these mathematical achievements that we eventually. Mathematical achievements that we eventually hope to summit. And as Balant and Nikos were kind of suggesting earlier in the week, the hope that we have for summiting these peaks is that we have these giants on which we stand on their shoulders. Well, I guess if this is Timo, he's going to have to have pretty broad shoulders. Actually, one more adjustment. Golden locks are for. Golden locks are for Duncan's benefit. Okay, so here's Timo in all his glory. So, you know, this is joint work with Timo and Louis Fann at Indiana. And so for this particular project, I don't necessarily think of myself as standing on Timo's shoulders. I want to give a different analogy. So I'd like to think that over the course of his career, Timo has built a sort of mathematical gondola that's going to take us pretty close to that peak. Going to take us pretty close to that peak. And on this gondola, as I've learned being his colleague and co-author over the last few years, he will take you on this gondola right up with you. And there'll be many enjoyable conversations along the way, and many enjoyable sights that you get to see. So I hope that this talk, apart from just introducing some results that we'd rather like, gives us a chance to see some of those highlights. Alright, so what is the model that we're talking about? And experts And experts will have to forgive the pedestrian nature at the beginning of this talk, but let's just establish the limitation. Okay, so this is a polymer model, it'll be a positive temperature model. And we'll be working on just the standard one plus one dimensional directive lattice. So let me put a starting point x. So I'm always going to use a starting point x. And then we're going to go to some distance away xn. So here my assumption is that the difference between xn and x in some l1 distance is n, just so n always. Distance is n, just so n always matches the number of steps that I take. Okay, and the restriction is that I have upright paths. So let me give a path space between x and x naught, xn. So let's say p xn, p from x to xn, is a set of paths. So let me index these by, say, 0 to n. So x0 is, let's say here, x0 is x, and then I go up to xn. And the restriction, of course, is that xi minus That xi minus xi minus 1 always lives in E1 or E2, which are the standard basis factors. So I can only take upright paths. Let's draw a couple of those. Okay, so from x, we only have two choices. Of course, we can either go in the e1 direction first, maybe go up to xn, but we could also go up to e2 first. Maybe we follow a different trajectory. Follow a different trajectory, it's up to xn. And then, of course, there are many other paths. And so, what is the kind of measure that I'm going to place on all paths that map me from x to xn according to this upright condition? Well, we choose according to some random environment, so now let me introduce the random environment. So each one of the vertices is going to receive a weight, I'm going to call wx. And these wx's will be, I id, positive random variables. And if you're used to And if you're used to not thinking about this, let me say I'm already doing the exponential. So I'm going to put e to the beta of some random variable which could be completely general all into that capital WX. So these are my weights. And then the weight, let me kind of abuse notation a little bit, the weight of some path in this path space from x naught to xn, I will define to be the product of the weights that we see along this trajectory. So, and here I'm going to take an image that I start. And here I'm going to take an image that I start at i equals 0. So I'm going to include the weight that I see at the initial vertex x, but I won't include the weight that I see at x. So that's the weight of a given path. And then of course the polymer measure, as many of us are used to dealing with, uses these weights as a mechanism for re-weighting paths, say, from uniform choice. From a uniform choice, instead of choosing a uniform upright path from x to xn, I choose one according to or proportional to its total width. So I say that here looking right at q, this is for quenched, to remind ourselves that we're dealing with a fixed environment. We lay the environment down, and then we look at the path measure from x to xn, and it chooses the path x0 to 0, x0 to n, so they say this particular path according to the weight of that path. Into the weight of that path divided by some normalizing constant from x to xn. This is the partition function, of course. So this is just the sum over all paths in my path space of the weight. All right, so any questions on the model so far? Very good. All right, so now we have this path measure that tells us what. So now we have this path measure that tells us what's the likelihood of taking a particular path given the particular realization of these positive weights that I've experienced. And then how do I sample from this path? So this is a basic just kind of simulation question. If you've never seen this model, ask yourself before. And there's actually a nice kind of inductive thing that's completely trivial but important. It's that if I want to sample the path from x to xn, then well at the initial step I only have to decide whether I'm going to the right or I'm going up. Whether I'm going to the right or I'm going up. And so the basic observation that is required is that if I want to sample a particular path, this is the same as first choosing, say I want to go from x to xn. Let's first say what's the probability that x1 is x plus the e1 vector multiplied by, so we're going to multiply this times whatever I put here. And then here's the observation: is that now once I'm at x plus Is that now once I'm at x plus e1, I don't need to ever remember that I came from x. I just need to know value by x plus e1 and then follow whatever Gibbs measure is induced from x plus e1 up to x. So I put x plus e1 up to x. And then I choose the distribution that's induced by that diet on x1 at the end. And then of course we have to add, this is now a mixture distribution, so I add. Distributions that I add. What if I want to go to E2 first, and then we multiply by the induced measure from x plus e2 up to remain xn, xn. So here the statement that is kind of, you know, requires another line of work is that the conditional distribution of this metric, of the path from 1 to n conditioned on going through x plus c1, right? Going through x plus c1 is the same as the just Gibbs measure if I just started at x plus z1. So the very basic observation. And this comes from the identity that, of course, z x to xn, so this partition function from the initial point to xn, is the same as, well, I have, no matter what I do, I have the weight at x. And then, well, the weight at x could be followed by two different things. It could be followed by a path from x plus e1 to xn, or it could be followed by a path from x. Or it could be followed by a path from x plus e2. So that's just the basic identity of partition functions that ultimately implies this. Okay, so ultimately, here's one thing I would pull off. Let me just focus, for instance, on this term and kind of underscoring this identity. Well, if I just divide both sides by this partition function, I'm kind of getting one. And I have two parts here. So I've got this guy divided by. So I've got this guy divided by the left-hand side, this guy divided by the left-hand side, and this weight just sits. And ultimately, what this is telling me is that if I know I want to go from x to xn, and I want to know what's the probability that x1, the first step, it goes in the vertical direction. Well, this is now just equal to wx times the partition function from x to c2 to xn divided by the partition function from x to xn. So, another basic application. Okay. And then, what is the kind of question that we're after? Let's say I send xn to infinity. And what does that mean? Well, so now I'm going to send xn off to infinity in this, say, upper right direction. And I have to name now in what direction I want to take it, because presumably there might be an effect of going very vertical or going very horizontal. So let's say that xn divided by the distance that we've taken. The distance that we've taken. So, this is the total number of steps. So, this now is some vector, right? On mid-delpha like this, xn minus x0. So, now this is some vector in the L1 ball in the northeast quadrant. So this, let's say, converges to some direction cosine. Then the question is, what happens? So what happens, of course, at the level of the partition functions? Of the partition functions. That's the basic thing. Remind everyone what we know. That's kind of a classical result. But then there's the more subtle question: what happens to these measures? What happens to the actual polymer measures? What are they doing? Because ultimately, I'm kind of, at least from a physical perspective, I'm interested in what the nature of these probability measures on the paths look like. Okay, so first highlight along our goggle ride. Let's talk about the straight theorem just to give our Theorem just to give ourselves the notation of what the law of large numbers for this model is. It says leading order behavior as n goes to infinity. And there are many places I can quote the shape theorem, but on this particular occasion, maybe I'll quote a very general result by Ferrost and Tino. And the result in 14, where they actually did a shape theorem for a much, much more general study. For a much, much more general setting in random walks and random potentials. But what does it say? It says that the following, with probability one, for any choice in this direction VC, then the partition functions that I'm looking at, this xz x to xn, so let's say I call this condition, so xn minus x0, or xn minus x squared, 1. 1 goes to the c. This will imply that the log of the partition function, and why am I taking log? Well, I'm taking a product of n thing, so log makes that nice. And then I'm going to divide by n. Then this converges to some function. So this converges to some function, which I'll call big lambda of casine. And I want to note that this is a statement where the probability of one of n comes before I choose the direction. comes before I choose the direction. So this holds uniformly in all directions. So that's the shape theorem. Maybe I should properly divide this by xn minus x. And here's the reason why I'm not choosing x to just be always the origin, because ultimately I want to say something about variant x as well, variants are important. Okay, so this is the law of large numbers, and what does this tell me? This tells me that. This tells me? This tells me that I have some function that depends on the direction cosi. So here I kind of am imagining, here's my north, my quadrant one. And then here's E2. Let's say this is E2 right here. One unit in the vertical direction. Here's E1, one unit in the horizontal direction. And then I've got this line segment, which is where this Cassi lives. So Cassi lives on this line segment, which encodes some direction. Some direction. And then we have this function, lambda, which is some concave function. The concavity is just coming from some superadditivity principle. I have some concave function, and this is my lamp that I can see. Maybe the blue is almost the red. Maybe not the blue. Maybe not the blue. Okay, great. Maybe I'll do some nice red one over here. Assume that's better? Great. Okay, so now I've got this concave function. Let me put that down. The concave function, you know from Yuri's talk that in certain continuous models this would be a differentiable function. But we're on the lattice, and so unfortunately, we can't use his beautiful trick. But anyway, we have some regularity. We have basic concatenity, but we don't know a priori much more than that. And there might actually be linear segments. So, as I've drawn it in this talk, I don't want to dwell too much on the details of having a linear segment, but there could be a linear segment on this. There could be many linear segments. So, we don't know much more than just current activity. Okay. So, now what's the next order question? This answers kind of our first-order behavior. So, then let me bring ourselves to a The next result that I want to quote. So, this was a very nice work of Chris Frost, which I guess goes to 2020. And so now what the statement is here, and this is really kind of getting at the question of what happens to the polymer measures, right? So, in order to understand the polymer measures, as opposed to just In order to understand the polymer measures as opposed to just what's happening to these normalizing constants, what this kind of basic insight tells me is that really what I should do is try to understand this ratio. Because this tells me how do I make a choice on a local level? How do I make a choice of going to the right or going up? And this is exactly addressing that. So here's the result. So for a fixed direction to see, subject to some differentiability assumptions, but To some differentiability assumptions, but okay, so let's say a nice C with probability one, you have the following thing. That, I'll do it like this, just write xn divided by scale one vector of xn. If this converges to concede, this will imply, and here I'm going to focus on exactly this ratio. This implies the following, that the limit This implies the following, that the limit as n goes to infinity of log z x to xn, and actually I'm going to take the reciprocal. So I want to think that the numerator is kind of the bigger one, whatever that means. And then I'm going to compare just, I'm going to choose the E2. I'm going to continue to choose the E2 by analogous things that we think could be chosen for E1. And what's the statement that this limit of the log of this ratio is equal to some function, which I'll write B. Function, which I'll write b of casi. And it depends, of course, on the fact that I did x and x plus e2. So comparing these two things. So this function, which is now the Boozman function, of course, this depends on the direction that I sent my terminal endpoint, and it depends on the initial point and the direction that I chose to move in my initial step. So that's the Boozman function. And what this statement really is saying is that no matter what the choice of xm is, as long as it's going in the right direction, the limit will be xm. Right direction. The limit will be, say, of course, there's uncountably many different sequences of xn that you could choose, this is quite a strong statement. Course about the state says. Psi is where lambda of psi is differentiable. Yeah, so okay, so what's the right technical condition? You do assume that c is a point of differentiability for the limit shape. And you also assume that if it's on a linear segment, then the boundary point. Segment, then the boundary points of that linear segment are also points of differentiability. That's the right assumption. Other questions? That's proved by a coalescence of geodesics argument. So let me not speak too much for Chris and Frost, but if I can kind of vaguely say the Danbron-Henson strategy of kind of doing some averaging trick off to some mesoscopic box at infinity, then As the stuff occurs at infinity, then that would be straight. Say more? Yeah, no, it's not checking this. Geoffreys first, and then it's weak limits first. You define the what you think is the limiting object going to be by weak limits, and then you use monotonicity to show the data. Okay, so here I want to slice that these two results kind of put side by side because this has a statement with probability one for any direction C. This says for any fixed C, we have width probability one. So of course these Boozman limits, although I can fix the Casi and then talk about a full probability event, can I reverse the origin? And of course, And of course, the interest in this question is that the answer is no, we can't reverse the order. At least, you know, we know in some cases, we definitely know. Okay, so now, more generally, let me start over here now. More generally, what can you do if you want a full probability event, but not depending on the direction? This is also what Pearson prosthes do in that case. So, what you can do is then you can say, right, some more general. On a single event of probability one, you can define two different whose one functions for the direction you see. So I'm going to put a casc minus and a casc plus. And again, I'll index that depends on the starting point x and that initial vertical step. And what are these? So, more generally, I can't just assume that for all directions, all uncountably directions simultaneously, this limit will converge. But what I could do is just kind of make something that will be defined automatically. I'm going to define the inf over all sequences that converge to the right direction, and also the s over all sequences that converge in the right direction. So, this is going to be. So, this is going to be some imps, this is going to be some soup of what? So, this would be the imp of limifs. This will be the soup of lim soups, as n goes to infinity. And then I can just put exactly what I wrote here. So I'm just not assuming that that log of the ratio converges. Log of cx xn and log z plus c2 to xn. Then I put the same thing. Then I put the same thing. So it's just the exact same ratio. I just don't know that it converges now, necessarily. But I'll just take these limsoups, liminfs, and then kind of look it over all possible choices and sequences. And then these kind of functions can be the possible sequence. Okay. And then the question is now, is b to c minus always equal to b to c plus? B to C plus, or could it be not equal for some, of course, necessarily random directions? Let's see. All right, so now we're getting into the part of the talk that very much resembles what Evan was talking about yesterday. We're going to have very analogous results, but in this lattice setting. This is a discrete setting, which I complement what Evan told me. Discrete setting, which are compliments what I told you yesterday. And I should say what these do, these could C minus and C plus, the minus kind of looks like it's recording the inf and the lim inf, and the plus kind of looks like it's recording the soup and the limb soup. I actually only arranged that to do it for your benefit. What the minus and plus is referring to is that this now is a monotone, actually an increasing function, as I vary from e2 to e1. So I'm always going to think I'm going to move my direction. So, I'm always going to think I'm going to move my direction from e2 to e1. And this minus is telling me that this is the left continuous version of this loose one function. The plus is telling me it's the right continuous version. And the fact that I chose e2 instead of e1 makes these imps line up in the imps and soups line up in the soups. Okay. So, are any questions so far? All right. So, when does this happen? Can we give some geometric? Happen. Can we give some geometric characterization? Well, just for analogy with LPP, there's an exact characterization of when this happens. So these two, so these two random variables will disagree if and only if, the following situation happens. The following situation happens. So let me draw the picture and I'll say it in the words. So the picture is that here's x, here's x equals v2, and I want to go in the direction to c. So I want to go in some, you know, send x n in that direction. So these two Boozmann functions will disagree in the LPP setting, not in our Polymer setting. But what will happen is that these will disagree if and only if there's two semi-infinite geodesics, going in a particular direction, C. In a particular direction, C, that are disjoint. They both go in the same direction, but they are disjoint. One starts at x, one starts at x plus c2. And so what I wanted to draw this picture for is to demonstrate that, well, intuitively, this sort of scenario is going to be very, very difficult if Cass C is very, very vertical. Because the semi-infinite geodesic from X is really going to want to just go through X plus C2. So if there are parts of discriminant, and LPP we know there are. LPP, we know there are. At least for the E2 direction, the points where they disagree are going to be very sparse near E2. We're not going to find very many Cassi where these two Guzman functions disagree in the E2 direction. But as I approach E1, then while now it becomes very easy for two semi-infinite geodesics to exist, easier in some sense, in a particular direction that's very horizontal. So if C is very close to E1, then it's kind of natural that you could think very long geodesics. Natural that you could think very long geodesics starting from X and one point above. If I want to go in that direction, they might have a good chance of actually staying apart for a while. And then you can kind of find some random set of directions where they actually do stay apart. Okay, so one of the other sites along the way is the previous work of my two co-authors. This is Duke Fannett. Southwin. And this is where they gave an exact description of this Boozman process as a function of Cassie in the exponential LPP case. The exponential LPP case. Here let me draw, here's my E2, and here's my E1. And here's my E1. And then here's what the booze one function looks like. So as I start at E2, so I'm going to vary this casi from e2 to e1, fixing a particular x, then what happens is it will start just at the rate, and it will stay constant for a while, and then it will take a jump. And this jump, the existence of that jump is already remarkable. It shows that you could find two semi-infinite geodesics that disagree in a particular direction, a random direction at that point. And then I keep varying, and then we'll jump again. Varying, and then we'll jump again. We'll jump again. And as I get close to E1, the jumps get larger and larger on average, and they become more and more frequent. Again, because of this idea that, well, as Kc gets close to horizontal, it's somehow easier for the semi-infinite job as its domain of parts. And so there's this function that actually diverges as you go to E1, and that's a general property that always holds. But they gave an exact description. So this was a description in terms of a Mark Poisson point process. Proson point process. So they could give a very nice description of what this Louis-Non function does as a function of this. Any questions so far? Before I state the main results? Okay. Yes. A little slow still, hurt. The previous theorem that you're about to erase is there's a limit of There's a limit of a log of z of x, xn divided by that's equal to Bozeman function. So if you take an expectation outside the limit, we should get the expectation of Bozeman function, which should be like the gradient of capital lambda. Exactly. And so I was just wondering, if you took an expectation before you take the log, do you still get the same? Sort of by analogy with anil, then you'll get something, of course, different. Let's see, if you take the expected value of this ratio, it'll be some sort of anil quantity, I would think. It's not how the ratios cancel out is not exactly clear to me. I'd have to answer you after. Okay. So here's the theorem I'd like to share with you. I said joint with Lewis Van and Gemo. Thanks, this is Louisa. Line in. But 23 plus, it's not yet out. Okay, here's the result. So with probability 1. Notice here, probability one is coming before for every direction we can see in the interior of this line segment, E2 to E1. There's two parts to the statement. Okay, the first part is that cos C mapping to the Boozman function, so this is the function of, and let me just choose the less continuous version, just to make a choice. Less continuous version, just to make a choice. x to x plus e2. So this is a function of casi as I vary down this line segment. And what we saw in the LPP case, at least for the exponential LPP case, was that this Boozman function took periods of constancy and then jumps. The jumps were where I found disjoint semi-infinite geodesics. Okay, so here we have a very different scenario. So the scenario here is that this function is not constant. Is not constant on any interval. And then, okay, let me make a qualification on any interval away from linear segments. So, on the linear segments, the Boozman functions are just constant, some sort of geometric reason. But here, as long as you stay away. As long as you stay away from the name segment, for instance, if you just assume that this is strictly concave, then this is not constant on any arrow. Okay, part B. Part B is, well, okay, if it's not constant, and, well, it somehow has to increase, we know this much, this is easy to show that it has to increase from E2 to E1, then where might discontinuities, if they exist, appear? Things. And so we might consider the set. And so we might consider the set dx. And so dx is going to be the set of directions, Cassi, for which the Bozeman function, say, x plus b2 in the casi minus direction is not equal to the bozman function in the casin plus direction. So this is the location of discontinuities of this process as I vary casino. And it depends on x. And it kind of implicitly depends on the fact that I'm choosing e2. But actually, it's easy to see that it won't depend on e2. It will be just the same for e1. So if I define this, then here's the result. dx is the same for every lattice point. So, the set of discontinuities doesn't depend at all on the particular initial point x that I chose. And morally, how I explain this to myself is that, well, in LTP, where the directions of discontinuity very much depend on the initial context, they depend in some random fashion. More than what's going on is, well, now I have a positive temperature model, so the polymer measure, right, it doesn't choose any particular path, say the geodesic. It has a range of paths that it could choose with certain polarities. A range of paths that it could choose with certain probabilities. And so, somehow, if there's some point max for which this kind of situation appeared, the Polymer measure will see it with some small probability. So the Polymer measure in a positive temperature case kind of gets to see all points with some small probability, at least those points that are ahead of it. And this is kind of the alter, the kind of mechanism that explains why it should be the same for every x. If I see a discrepancy between these two functions at any lattice point, I should see it at all lattice points. So, questions about what the results say? So, the LP result was for a fixed X, right? You were saying that. Yes, yes. But you're saying that the different X's will be different or be different. There's also a full description of that? There's a full description. So let's see, what can I say? Yeah, so the discount image will also depend on the choice of either. Will also depend on the choice of E1 versus E2. So there's a set of discontinuities that are associated to the vertex x, and e1 will somehow capture some interval of those discontinuities, and e2 will capture those intervals of discontinuity. And the unique point that they share will be the direction of the competition intervals. And that will depend, of course, on the location of this. Points where there's a discrepancy in the E1 or E2 direction. or E2 direction or like C are exactly the points this competition interface points in the X direction. And these are not all the points in the lattice here instead of okay as one closing remark I'll add that in the inverse gamma case which we're kind of all know and love which originated of course with Timo we can again give an exact description of the Of the Buzmann process. It looks quite similar to this, but now actually what will happen is that there's no interval of constancy, right? That's what the first part A says. And so at least in the inverse gamma case, what you'll see is that there's discontinuities on a dense set between E2 and E1. For every edge, for every point, there'll all be the same discontinuities, but not the same size of jumps. And so there'll be a bunch of microscopic jumps, and then there'll be these spikes. And then a more macroscopic jump, and then a spike. Max gap jumps and then a sprite. And on every single interval, there is some jump and some discontinuity. They'll be small most of the time, but occasionally I'll get big jumps. But I'll have the same general trajectory of upwards as I approach E1 with that region. But it's not a piecewise constant function anymore. And we can give an exact description. It again involves a Poisson process, not a marked Poisson process. It's kind of a Poisson process in the plane, and then you kind of look at the heights of these Poisson processes that you move. The heights of these Poisson processes that you move across the plate and you add them up. So it's a similar description, but it has some critical distinctions. So it's been an honor to speak to you. Hopefully you've learned something, and thank you very much for your attention. Okay, there are a bunch of questions for the talk. Any more open? So so my intuition for these jumps in NPP is when the coalescence point jumps around. Yes. So is it is it the case that... So we can sort of couple the two polymer measures and speak about coalescence. So all of these jumps are somehow these coalescence point jumping with different measures. So if it's sort of heavy geodesics, then these are the weak jumps. Of the Greek terms? Sure. This is at least the way I kind of intuitively understand it on some heuristic level. On a rigorous level, I'm not quite there yet. Maybe Xiao would be a good person to ask, and he could say more about that perspective. But yeah, I think that's kind of right. I mean, another kind of natural question is, right? So one thing we can do for this, the inverse gamma case is we can show a convergence and distribution of the inverse gamma Buzzmann process to the exponential. Boozmont process to the exponential LPP process. But it's not clear that you can kind of couple them in some natural way so that discontinuities just kind of, these small discontinuities vanish as I kind of claim they do as you send the temperature parameter to zero and then you kind of recover this piecewise constant thing. So yeah. So so so i in in even in the so not in the low but so this is in the lemma case, but this is for the for general right. For the general, right? So this result is general. This is not general, so you only see these kind of a dense, like a dense point of increments, right? Yeah. Yeah, so I guess the general belief is yes, we should see a dense set always, at least some mild conditions. We can actually only show that this discontinuity set is non-empty in the inverse gamma case, because we can give an exact description. But once you knew it's not empty, that would be the kind of the biggest step to going to it being guessed. Yeah, to go back to the test because we have this journal. Any other questions? Thank you.