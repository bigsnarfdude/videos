Thank you very much. Thank you very much. It's really nice to be there. Do I have to click on something for the recording? No, it's okay. No, it's done automatically. Okay. So it's very nice to be there. And I hope that, well, so far all the talks that I heard are very, very interesting from the geometric point of view. Very interesting from the geometric point of view, and I hope to bring a little bit more insight in the specific domain around curvature. Okay, so to start with, I want to set up a little bit the application context in which I'm working. So, this is computational anatomy. We're trying to do statistics on different types of shapes, basically anatomical shapes to try to analyze what is atomic. To try to analyze what is pathologic versus normal and try to use that as prior knowledge for basically personalized medicine. So, in all these cases, what we end up with is trying to do means of surfaces, means of higher order objects like the spine and website, things like that. But not only means, we want to be able to do some PCA, partial D-square, ICA. Lee square ICA, but working on geometric objects. And more generally, the concept that I'm trying to follow is that right now with data analysis, non-linearity is everywhere. And as soon as we are working with nonlinear objects, then statistics need to take into account the fact that the space on which we're working on is nonlinear. And for all that, we need tools to actually Need tools to actually do statistics in a consistent way. So that's the overall goal of the topic in which I'm working. One example is the following. If we take here 300 images of scoliotic subjects, we may try to model what is the articulation of the spine among different subjects. And here it's an example where we actually model the spine with Spine with 16 rigid body transformations and doing the statistics directly on the rigid body transformations allows us to have with tangent PCA four modes of deformation that explain more than 50% of the overall variability in all the subjects. But more importantly, they give us some good intuition about what's happening here. For instance, the fact that scoliosis is linked to the rules. Is linked to the rules of the tail rings. So here, working with nonlinear objects is not just or doing proper statistics on non-linear objects is not just, it's useful for the interpretation as well. And we are in the era of interpreting or interpretable AI. So this should give us some tools to work on that. A second framework that we're using quite often is. Framework that we're using quite often is the one of deformorphometry, where we assume that there is one atlas here, and then we deform it with some kind of diffomorphic deformation to each of the observation. And then we are lifting the problem of statistics on the deformations themselves. And so we are left with the fact that we want to do statistics on groups of on Lie groups of deformations. Groups of deformations. So, one setting is to put a right or left invariant metric. So, that's the celebrated LDMM framework if we do that for diffomorphisms. But one of the difficulty with that Riemannian framework is that in general, there is no, as soon as we, even in with wiggy body transformation, there is no left and right invariant metric. And so, if we want And so, if we want to do things on the Lie group, we like to consider the structure of the Lie group alone. And on the Lie group, we have composition on the left, on the right, inversion. And we like, for instance, to have the mean of some elements, which is, or the mean on inverse transformation, that would be the inverse transformation of the mean of the elements, things like that. And this is not the case for a right or a left invariant metric in general. So we set up a new. So, we set up a new, another method, which is relying on a non-metric structure, which is just using a collection, and that's called the Carton-Schuten connection. And it's actually fully symmetric here, but it's non-metric. And we know that the geodesics are actually left and right translation of one-parameter subgroups, which is very nice to compute very efficiently on this type of Lie groups or data leading on Lie groups. Uh, or data leading on legals. So, I would uh, well, I could spend a lot of time advertising for this uh structure, which is an affine structure and not a Riemannian structure, but it's actually very close to the Riemannian structure, and many of the elements for geometric statistics can be pushed to that structure. Uh, and I will just uh it's just important for me that what we do in Riemannian setting also work for the affine setting. Okay, so one of the examples of the application that we want to solve with that type of setting is if we have longitudinal images, so images at different time points of the same subject here, we may want to fit the geodesic in the group of diffomorphism to encode actually the deformation of that brain over time. And if we have several subjects, like control subjects and Alzheimer's subjects, then we want to normalize all the Normalize all the deformations to the reference template. And for that, we need an operation which is called domain adaptation in machine learning. But in the geometric setting, it's usually called pile transport. And here it's what we have done with Marco Lorenzi and Rafael Silvera for modeling here what is the deformation over time due to normal aging and what's the additional component. The additional component in our stationary virus ETPA framework that needs to be added for Alzheimer's disease, and then we can analyze what's happening with other types of, for instance, treatment and things like that. Okay, so I just want to advertise for a book about this subject, which is basically the foundations of Riemannian geometric statistics, application to shape spaces and the Application to shape spaces and diffeomorphism. So that's exactly the topic of what I said. But that's covering basically the basis. So you will find many interesting things on statistics, on manifolds, and the fine setting here in the foundation part of that book. But for the talk today, what I want to investigate is more the fact that most of the methods that we design for doing statistics on manifold, they rely on the Frichie mean. And manifold, they rely on the Freichie mean. And one may wonder: okay, if we compute the Freichy mean, is it what's the next thing? If we're an engineer, we want to know not only what is the estimation, but what is the confidence region around that estimation? And we may ask, is there an impact of curvature? Is the statistical test that as we would do it on the Euclidean spaces, is it still valid for the case of Riemannian manifolds? Case of Riemannian manifolds or fine manifolds. And another important point is, especially in the medical domain, is that in practice, we are not working with a very large number of data. So we used to say that there is, well, we are in the era of big data. But for studying anatomical shapes with specific disease, usually we have numbers that go from if we have 50. If we have 50 shapes, we are very happy. If we have 100, that is already a big study. And if we have a thousand, that's a world record. And we are increasing a little bit for normal controls. But as soon as you go to a specific disease, it's going to be back to these types of figures. So one of the key questions is what's happening with a limited number of questions. And the question is refresh. The question is rephrased in another way: how large should be the number, or how many samples do we need to be able to consider asymptotic results? And the second aspect is, so I've shown you an example where we used parallel transport, for that we used LADOS algorithms, so Chill's and Polado algorithms that I will present later. And well, we are using them, they are very efficient, we believe that they are very, they have. They have lots of properties, but we didn't have any analysis of the numerical accuracy that we can reach with this type of schemes. So, the second question of the talk is what's the numerical accuracy that we can reach beyond the first order? Is there something beyond the first order for this type of algorithm? So, that's going to be the two main topics of my talk. The first is on the concentration of the empirical Fouchamine. Empirical Fouch√© mean towards the population mean. And the second is the work that we've done with, or I should say, that Nicola Guigi did on the numerical accuracy of power transport algorithms. I think they nicely fit together because they use exactly the same tools for analyzing algorithms or methods on Riemannian manifolds and in particular the impact of curvature. Okay, to start with, I will need a few. To start with, I will need a few notations. The first thing I will use extensively in the talk is the notion of normal coordinate system. So if we are in a Riemannian manifold, we may assume that it's geodesically complete, in which case we can always shoot from a point with a tangent vector along the whole geodesic. And then the point that we reach after time one is called the exponential. One is called the exponential, and it's a map that depends on each point. The log is the reverse of that operation. It's what is the initial vector that is needed to shoot to a point Y with the shortest pass. These normal coordinate systems are very nice because they're actually unfolding almost all the manifold except the cat lockers on the tangent space and the geodesics become straight lines. Become straight lines, geodesics starting at the development point become straight lines. And so we can reformulate many of the notions using the expand log, like addition and subtraction needs to be reinterpreted in terms of expand log. But we can rephrase most of the operation like that. So that's for the geometric setting. I will assume first that we are in the Riemannian setting, but in the affine connection setting, it's the same thing locally. It's the same thing locally. And the second phase is the statistical tools that we need. So, first, when we are doing statistics, the first thing we want to do is some type of mean value. And Frechet observed that for exactly for doing statistics on shapes in the 30s, he observed that, well, that was difficult to do things. So, he first expanded things to infinite-dimensional spaces like here. dimensional spaces like Hilbert and linear spaces with integrals and then he realized that well an integral is a linear operator and that cannot be used for something that has a value in non-linear space so he designed another or he proposed another notion of the mean which is the set of minimizer of the mean square distance which is the the the the expression that we have here and if we take And if we take the points where this is non-sibular, where this is differentiable, then we can show that the critical points of that function satisfy this type of equation, which is basically what we call an exponential Bi center. So it means that if you take a point x, you develop everything, all your distribution in the tangent space, you're going to all your sample points on the tangent space, then you've got. Then you've got a distribution now which is in the tangent space at a point x. And you can compute the mean value in the tangent space. It gives you a vector. And if this vector is zero, then this is a critical point of the mean square distance. And it might be a minimum, or at least it might be a local minimum, but it may be the global minimum. So this idea of the exponential barycenter is introducing the first notion that is interesting, very interesting from the geometric point of view. Very interesting from the geometric point of view is the fact that if you're given a distribution on a manifold, then you have a natural series of tensors which are associated to that. And the first one is what we call the tangent mean. So it's just what I explained. You unfold your distribution on the tangent space, take the mean, and it gives you a vector. Take the expectation in the tangent space, it gives you a vector. So that's a vector field on the manifold. And the zero of that vector field are going to be the point of interest for the Frechy mean. interest for the Fresh Me. But we can also take the cross product or the Kredecker product, the direct product of these tangent vectors, and this is giving something like the second moment. So it's a tensor field. And if you remove what can be explained by the first moment, you're getting something that is actually the tangent covariant matrix field with that. With that tensor field. And we can continue like that for higher-order moments. So I will extensively use these two notions: the tangent mean field and the second-order moment, which is basically how we can observe the data from a given point X, the moments of the data in the tangent space at X. Okay, so for the Fresh E-Min, we have some very mean we have some some very nice unicity condition if we are in negatively curved spaces even in metric spaces in the sense of Alexandrov but in positive curvature we have to assume additional concentration conditions so that there is a unique minimum and in that case so Kendall Kasha first proposed some conditions and they were extended by Kendall so if the support of the distribution is in a sufficiently small In a sufficiently small regular geodesic ball, and sufficiently small means basically on the sphere that we are within one hemisphere, then we know that there is a unique Freichi mean. And in these conditions, we moreover have all the data that can be always described by a single chart, which is you take a chart with a normal geodesic, a normal coordinate system with a point within. Coordinate system with a point within that ball, and you can have all the points that live within a chart. And that's the condition. Under that conditions, Patachay and Patron General showed that there is consistency of the empirical mean. So if you take K sample or N sample and you compute the empirical Frichi mean, then this is converging towards the population Free mean. So there's no asymptotic bias. And moreover, if we normalize here. And moreover, if we normalize suitably the position of the empirical Free Shi mean for an end sample with respect to the population mean, then this is converging toward the Gaussian, but the Gaussian has some kind of unusual covariance matrix here. Sigma is the covariance matrix of the population distribution, but it is modulated by the Hessian or the inverse of the Hessian of the square distance. So this one of the So, this one of the question is: What is this doing and how does it impact the convergence or the test that we can do? Do we have an expression for Hessian? What's happening for a small sample of size and not just an asymptotic sample of size? And can we generalize results to find connection spaces? Okay, so the principle to answer that is the following: we're going to take Following. We're going to take one end sample of a given population distribution and we are going to compute the we are going to try to localize where is the empirical Fouchi mean in a normal coordinate system that we can take at any point, but in the end we're going to take it at the population mean. And then we're going to, once we have localized, if we can localize this, then we want to know if To know if there is a bias, so what is the expectation of the empirical mean for a given number of samples? What's the coverage matrix of that estimation? So that's exactly the expectation of the... So here I recall that X bar, so the empirical Freichi mean, is a random variable because it depends on which points, how you sample the points. So if you sample in points, that's a random sample. That's a random sample. So, the empirical free shmin itself for n samples is a random variable, and we can take the expectation of the first and second order moment. That's going to give us the bias and the covariance. So, the key thing for that is to realize that actually an end sample is itself a distribution. So, we would like to try to expand the first. To expand the first, the tangent mean field, this is a vector field. We'd like to be able to extend or to do a terra expansion around the points where we want to look at or around the exact or the population mean. But the key difficulty here is that well to do that, we need to do to do an extension of the log at a point xv, and xv is a point which is close to x, but which is not exactly at x. X, but which is not exactly at X. So, what we need to know is what is the terra expansion of the lock, but in a coordinate system that's going to be in a different tangent space. So, we can do that, and that's how I tried to address the problem 10 years ago using Terra expansion in normal coordinate system developed in particular by Bruin. But there's lots of terms, there are pages of Terra expansion of terms which are due. Expansion of terms, which are due to the differential of the exponential when we are going from X to a neighboring point Xv. And it's only recently that actually I found in the series of work of Gavrilov the key idea that to relay the two tangent space instead of using just a chart at a point and looking at how the coordinate system vary when we move within the chart, we can take the Chart, we can take the pile transport to actually bring back the tangent space at Txv to Tx. And if we do that, then everything becomes much more simple because we're working within a single tangent space. So we're going to have functions from a tangent space to a tangent space. So the first Terra expansion that Gavrilov proposed is based on, he called it the double exponential. The idea is that if you are The idea is that if you are at point X, you have a tangent vector V, you first shoot at a tangent vector U, you first shoot along the geolithic starting with V, and you paral transport the vector U, and then you shoot along the parallel transported vector, and you take the log to come back to a vector at point X. So, if you do that, it's basically corresponding to the composition of two exponential, and this is an analytical function. Analytical function locally, and you can show that it has a Taylor expansion as a polynomial in two variables u and v. And it's actually even more, it's actually smooth with respect to the point. So it's actually a tensor expansion. So that's the first expansion. But if we think about geodesic triangle, this is basically a geodesic triangle. There's different ways. So here we put the question if we know this side. Question: If we know this side and basically you is characterizing this side, what is this side? But we could also think: I want to shoot here and know what is the log at a neighboring point. So if I know this side and basically where I should shoot, but from a neighboring point, what is this side? And this terra expansion is what we call the neighboring log. So it's basically shooting here to get y, shooting here to get xv, taking the log. To get Xv, taking the log of Y at this point and then prioritransporting to get at the origin. And this neighboring log extension is also a transfer expansion that is expressed in this coordinate system and here. So these are expression for torsion-free manifold with the Levi-Chimita connection, for instance, of the Riemannian metric, but it's also working for affine connection spaces. And it's only involved. And it only involves the curvature if there is no torsion, it only involves the curvature, the covariant derivatives at different orders of the curvature and their product. So it's a quite nice expansion that allows to go much, much deeper in the order of the expansion than the usual Terra expansion in normal coordinate system. Okay, so coming back to the problem of the To the problem of the Frichie mean. So, the idea is to take the tangent mean here at a neighboring point and to transport it back to the point where we were originally at X. So, this is we can rewrite that this way. And here we recognize exactly the fact that the terra expansion that we had before. So, this is now instead this new operate. This new operator is an endomorphism of the tangent space, and so we can express it as we can express the terror expansion as a polynomial approximation in V. And that's what I wrote here. So I don't don't look at the formulas, they're just there to state that we can actually write it. Now, the second question is, we are at a point X, and what we know is, what we want to know is determine V such that this OR expression is zero. Such that this OR expression is zero up to a higher-order term. And so we can do that by solving for a polynomial expansion of V and solve term by term for the different orders. And we find in that way that the location of the mean is actually the tangent mean, but corrected by a curvature term and corrected by higher order terms involving the derivative of the curvature. So this is actually. So, this is actually a very nice way to locate or to have high-order expansion of what is where is the mean, knowing what we know only in that tangent space, developing everything in the tangent space at x, but knowing also the curvature and the gradient of the curvature at that point. Now, if we take one ensemble, we have a new distribution. It has some kind of moments that are determined by the sample that we take. By the sample that we take, and we have the empirical Fouchi-min, which is located exactly by the same formula as before, but with the empirical moments of that distribution. And now what we want is to take the expectation of that. However, there is a small thing is that if we take the expectation of one tensor, then it's the expectation of the population distribution. But if we take the expectation of products of empirical tensor, Of empirical tensors, then the formula is a bit more complex. But if we take that into account, we end up with these two formula on the bottom that I will skip shortly to get simplified version on that slide, where we have determined the moments of the empirical Free Shi mean of an ensemble. And surprisingly, although the empirical Free Shi mean is asymptotically unbiased, also Unbiased or so consistent for a fixed number of samples, so finite sample, it has a bias. And this bias is related to the gradient of the curvature contracted with the Currency matrix of the population distribution. So, interestingly, in symmetric spaces, the curvature is coherently constant, so it means that this term is zero, so we have This term is zero, so we have no bias up to order five, but we may have still terms appearing at higher order, even in symmetric spaces. So for the covariance matrix, it's even more interesting because what we find is the term here, one over n times m2 is basically the covariance matrix of the population distribution. So it's one over n times the population covariance matrix. That's the usual Euclidean. Usual Euclidean rate of convergence, but it is corrected by a term which is also in one over n, so which competes directly with the usual rate of convergence. And that term is a double contraction of the corence matrix with the curvature tensor. And what we can show, what we will see in the next slide, is that for negative curvature, the convergence is faster than in a Euclidean space. In a Euclidean space. And for positive curvature on the sphere, the convergence is slower. We need more samples to be in the confidence region where we should expect that we should expect with the Euclidean setting in if we are on the sphere or on positive curvature spaces. So if we take the limit when n is going to infinity, so the asymptotic version, and we can do the same terror expansion for the same terror expansion for the uh that we did for the for the hessian of the square distance it's it's written here and we can show that this is obviously consistent with the central limit theorem of uh batasha and patron general but but now what we have is we have the explicit expression of the hessian so we know exactly how this is modulated that's exactly what is written here but our expression is also valid for a small number of data and not only for just an asymptotic Not only for just an asymptotic version. Okay, so if we want to understand a little bit what's happening, we can take one archetypal example. The very simplest example I could think of is a constant curvature space. So it's basically curvature zero, it's flat. Curvature positive curvature, it's a sphere with different radius. So if the radius is going to zero, then the curvature is going to infinite positive curvature. Its positive curvature or the hyperbolic space. So think of a saddle point in 2D, when the curvature can go from zero to minus infinity when we push to the other side. And if we take an isotropic distribution in that space, then we can observe that the variance of the empirical mean is exactly Of the empirical mean is exactly related to this: the variance of the empirical mean that we should observe in the Euclidean space. But this is modulated by a factor alpha here that we can compute explicitly with the asymptotic BPCLT or with our high-concentration expansion. We have a terror expansion that gives us what is the relationship with the curvature here. So, this is the sectional curvature of the So, this is the sectional curvature of the constant curvature space. So, k kappa equals zero is Euclidean. Kappa greater than zero means that we have alpha which is above one. This is what we see here. We have a modulation which is above one. It means that we are going, we are concentrating, or the empirical mean is concentrating toward the population mean more slowly. More slowly, we need more sample to get there, to get to the same concentration than in a normal Euclidean space. And at the limit here of, at this limit here, there exists a point up to which the distribution doesn't converge anymore. And that corresponds in the case of the distribution, which is exactly a uniform distribution on the equation. A uniform distribution on the equator on the sphere. So, in that case, there is not anymore a unique Freichi mean. And we're not converging, we're converging the speed zero, not even in one over n. And that's a behavior that has been coined as smeariness for means on manifolds. On the other side, if we're going to negative curvature here, we see that the modulation factor can go to zero. Can go to zero at the limit when the variance is very, very large with respect to the curvature or the curvature is very high, either the curvature or the variance is very high, then we can converge towards something which goes immediately to the mean much, much faster than in a Euclidean space. And that's what is called sticky mean in some spaces that have singularity. Some spaces that have singularities of that type. So we have, in fact, here something which is for perfectly regular manifolds, we have a behavior which is making the intermediate or which is showing that how things diverge from the Euclidean space in positive and negative curvature and can explain the longer range that we can achieve. It's just the beginning of smeariness or It's just the beginning of smeariness or stickiness. Okay, so that's for the theoretical results. Of course, we may wonder: is it true on real data? Can we observe that? And so on simulated data, we can see that. So these are examples on the sphere here and on negative curvature here. And obviously, our small variance prediction of the Of the uncertainty of the empirical mean. So it's in green. It's working very well for low variances, but it's a high concentration, so low variance stereo expansion. And the BPCRT is working in that case with a high number of samples. However, I should state that I can draw only the BPCRT here because we are in In a space of constant curvature with an isotropic distribution. Otherwise, I don't know how to compute exactly H. The only thing that I can compute explicitly is the Terra expansion in the high concentration range. So this in green, I can always compute it for all manifolds, but this BPCLT expression can only be computed for very, very specific cases in simple spaces. In simple spaces. Okay, now if we take real data, then we can simulate also what's happening with bootstrap. And we can see that with a reasonably concentrated distribution on the sphere, we still have a modulation which can attain here 10%. And that is what is observed with real data. We really observe that it's converging 10% times smaller than in a Euclidean space. Smaller than in a Euclidean space in this case, and if we take more distributed or data that are more distributed, so here they're just going outside the Kendall and Kershaw conditions, we can achieve larger modulations. So here, about 20%, the speed is 20% lower than in a Euclidean space if we consider that the data are spherical. But if we consider that they are actually projective. That they are actually projective, which is the case for this type of data on paleomagnetic studies. We see that the modulation is actually completely exploding. And in that case, it might be actually a smeary mean, meaning that we are not even converging in one over n. We may converge in one over n square or higher order in one over square root of n, sorry, or lower order space. Order speed of convergence. Okay, so I'm left with about 10 minutes or a bit less for the second part. I will try to give you an idea of how we can use the same type of techniques for pile transport. So I'm going back to my original example. The idea is really to pile transport the initial vector characterizing this geodesic along another geodesic, which is here. Along another geodesic, which is here, the intersubject geodesic. So, when we investigated that problem with Marco Lorenzi 10 years ago to actually solve the previous problem, we thought that Cheersladder or this type of discrete version of private transport was quite interesting. So, Cheersladder is a method that was actually introduced by Alfred Cheer, but in the lecture, so it was not printed. It's actually printed in the book of the famous book. Printed in the book of the famous book on gravitation by Ehers in the 70s. And the method is to just rely on symmetries. And the idea is the following: if I am at point X, I have a tangent vector W, I'm shooting along the geodesic with that tangent vector, and I want to power transport V along that geodesic. Or we can say that it's any curve, but we're going to do it along a geodesic here. So the method that Chiefs proposed is actually to first sample the To first sample the curve along locally geodesic curves, and then we shoot for some time along the tangent vector v, along the geodesic, so to get to point xv. And we shoot from xv to x1, take the middle point, shoot to the middle point, shoot actually twice at the distance, and take the log. And in the in the 20 years ago, KFET's and colleagues showed that this is. And colleagues show that this is a first-order approximation of private transport that can now be iterated for different steps so that we can transport along the whole trajectory. So with Marco Lorenzi, we quickly came to the fact that actually there is, if we consider that this is geodesic, then we can actually use that as one of the segments of the geodesic program. And so the idea is if we just And so the idea is if we just take the midpoint here and shoot from here to here and twice, then taking the log gives us the pile transport as well, or should give us the pile transport as well. So what we showed is it was the first order. But we also found that there was a closed form expression for carton connection on Lie group, which was kind of surprising. And after several years of work, actually with the actually with the first double exponential expansion of Gavrilov, I was able to show that actually one step of pole ladder has this expression and it's actually a scheme which is of all the five which is exact in symmetric spaces. So it's quite interesting and quite appealing. But we're still missing the way or what is the order for cheese ladder, what's the order for, is this a The order for is the high order when it is iterated? And is there any results that we can say if the geodesics are just approximated? So that's what Nicola Gigi did recently. He actually took the Terra expansions that we saw before to compute a new Terra series for the midpoint rule. And from that, he was able to show that if we take this cooking recipient. We take these cooking recipes, which is actually the cheerleader iterated over different steps. We can show that the convergence is at this speed here, and there are two terms. One is related to the fact that it's a square here, and it's related to actually the way we discretize W. But here, the fact that V is quite. here the fact that v is quadratic in this expression allows us to think of another scaling we can scale differently v with respect to w and by scaling with a with the n square we can actually reach a second order approximation and not just the first order so it's actually interesting because the chills ladder the original chills ladder is first order but by just analyzing how what is the accuracy we can make it second order and we can verify that And we can verify that on numerical experiments. So, here on the sphere, so we see that this is the convergence is linear if we take the classical cheese ladder with alpha equal one, but we're getting quadratic results here with alpha equal two. And we can measure here what is related to negative curvature on the space of SPD matrices. So, for Poladder, we already have the Terrax. Have the Terra expansion of one parallelogram or one step, but iterating that, so it's an order four here, but iterating that is actually only order two. And here we have terms in gamma which involve the gradient of the curvature, but also eventually possibly the curvature itself. So here we obtain that the Poland is a skill, a natural skill for order two. For at order two. And it's interesting because it's much more efficient in terms of computing geodesics than cheese leather. So that's the one that we use most often right now. And we can verify that also on control spaces. The problem is that if we take a symmetric space, an easy space like the sphere or the hypermodel. The hyperbolic space or an asymmetric space, then there's no gradient of the curvature. So we have actually something which is exact in one step. So this is what we see here on the special Euclidean group. If we take an isotropic metric on the translation, this is exact in one step, thus in any number of steps. But if we actually take an anisotropic metric on the translation, that's The translation that's the parameter beta here, then we see that there is now a quadratic convergence, and we can see that also on Keddle shape space with the here the quadratic, well here linear convergence for cheese ladder with alpha equal one, quadratic with alpha equal two, but still faster convergence for cheese ladder here, for pole ladder, sorry, second order in the implementation. Implementation. So, this is very nice, but the additional ingredient is the fact that this can continue to work actually even with so the whole setting here, the analysis was done with knowing the exact exponential analog map. So, if we go to numerical integration of geodesics, then actually we have an approximation of geodesics. And so, what Nicola showed is that if we use Nicola showed is that if we use sufficiently strong numerical scheme or sufficiently high order Runschueta schemes to integrate forward and compute the log by gradient descent up to a certain accuracy, then we keep the same convergence result even for approximate geodesics. And that's quite interesting because it means that we can implement pile transport with Poor Ladder with approximated geodesic using this type of. Using this type of using Ruch Router, for instance, integration. And we can also generalize the analysis to other type of methods that work for possible pile transport, for instance, the one based on Jacobi fields, like the Fenning scheme. But we could actually show that this is in this case first order because the term here V is appearing only as a linear term and not. As a linear term and not as a quadratic term. And so we cannot kill the second order error term as in Chill's ladder. So this type of scheme seems to be a bit asymmetrically constrained to remain linear or first order, while Chiels and Po Ladder can be high order schemes quite naturally. Okay, so these are not only theoretical results. One of the key The one of the key contributions of Nicola Gigi was also to implement this power transport in the geomet library that Nina will present soon after, I think, this afternoon for me, morning maybe for you. So if you want to try, everything is in there. I can just make a bit of advertising for the fact that there's lots. Advertising for the fact that there's lots of manifolds, including with exact geodesic that we know, but also with approximated geodesics. So it's worth trying for your problem if you have sufficiently simple manifolds. Okay, so I'm coming to conclusion because I think I'm really at the end of the time. So the message I wanted to make in this talk is about that we now have That we now have new tools for intrinsic Taylor expansion of geodesic triangles in manifolds. And these are Taylor expansion that are Taylor series, polynomial Taylor series in several arguments. They are valid in the fine connection spaces. They are valid in Riemannian spaces. They are valid even if we take torsion, but we have lots of terms due to torsion to add in that case. But this should be a very Should be a very interesting way to analyze many different algorithms, like what we have seen here for pile transport, but also for other, I believe that this can be used for many other geodesic-based algorithms. And coming back to the first point of my talk about the empirical and population means, or the concentration of the empirical mean to what the concentration means. Toward the concentration mean. What we have seen is there is a bias which is controlled by the gradient of curvature. There is a modification or modulation of the covariance matrix of the empirical mean, which is also controlled by the curvature, which is accelerating or slowering the convergence. And one of the key maybe lessons that we can take for machine learning is if we learning is if we think that we are estimating in non-linear spaces where there is some curvature, then we should think about what is the variability or what's the size of the distribution with respect to the curvature, because if it's large enough, then the curvature will impact the way we are converging. And what we think is a large number of samples might not be sufficient actually to have the assembly. Sufficient actually to have the asymptotic results. And just a last comment. Here I expressed the bias with respect to the curvature-related effect on manifolds. We could also think that if we have, well, curvature is usually in a Riemannian manifold, it's a pointwise operation. And torsion is also a pointwise measure. So it's actually what's happening if you have a direct distribution. A direct distribution. So, if now you imagine that you have some kind of distribution to probe your manifold, we could think about what is the curvature of the manifold, but seen at the scale of that distribution. And maybe one potential proposition for the definition of distributional curvature and distributional torsion could be the asymptotic bias or the corrected for the number of. Or the corrected for the number of samples. So it's basically looking at what is the bias. This is a kind of torsion that the manifold or that the distribution will give on the manifold if we can look at the manifold only through that distribution. And here we have also a definition of some kind of distributional curvature with the rates of convergence, which is how does it change with respect to the convergence. How does it change with respect to what it should be in the Euclidean setting? Of course, properly renormalized. So, this cannot be called exactly the statistical curvature like Ephron defined it in the 70s, but this is much, much closer to the coarse curvature that Yano Lidier proposed already 15 years ago, and the synthetic Ricci curvature that Strom and Lord Villani. Strom and Lord Virony proposed also over the last years. So it may be interesting to think about new ways of, well, thinking about what is curvature on manifolds, not just with a point, but with distributions. And I think that's an interesting insight for new geometries. And I stop there with many thanks for all the members of my EOS. Of my ERC team, and especially for Nicola Gigi, who did half of the work that I presented here. Well, thank you very much, Xavier, for the great talk. Do we have questions from the audience? Yes, John. Yeah, hi. So, great talk. Yeah, so I very much enjoy that. So, now I have a question about the shoes leather in the Later, in this calculation of the GODC, so the parallel transport. So, my question is that, and you also touched upon briefly: if the connection, if your FN connection has torsion, and you said that, so first, if connection with torsion, does the shield leather method still work? That's the number one question. The number two question is that it has to do with whenever the connection you have a torsion, you have a basically a non-holonomy, you know, the you walk. You walk in two different sequence, right? Then you don't end up at the same point. So I wonder whether, how do you deal with that in the computation? So I will have two answers. One is right now, what we did is just assume that we have a connection-free, torsion-free connection. And that's what was assumed at the beginning for the cheese ladder itself. Is ladder itself. So, if it's if there is torsion, then there will be a modification at the first order already by the torsion. So, it has to be corrected for the torsion. So, in that case, I believe that there are ways to correct for it thanks to some symmetries, but we have not explored that. So, the whole thing, all the formulas that I All the formulas that I showed here only hold for no torsion, for torsion-free connections. Okay, what I just wanted to stress is the fact that the Gervilov's terror expansions, they continue to hold. And if we go in the literature of the different papers of Gervilov, he has expanded the whole thing for torsion as well. And instead of having just the curvature and the covariant derivative of the Curvature and the covariant derivative of the curvature, then we have also the torsion tensor and its covariant derivative that are appearing. And so, there is what I'm just saying is there is the tools are there to actually go in that direction, but I think everything remains to be done. So, in very specific cases, if we think about alpha connection or some duality, I'm pretty sure that we can kill the terms by symmetry. The by symmetry. But in general, I don't think that's the case. Okay, thank you. Yeah. Pablo, you have a question, please. Hi, thank you very much, Shoria, for the wonderful talk. It's always fascinating to see all of your advances.