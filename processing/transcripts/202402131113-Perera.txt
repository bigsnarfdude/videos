Okay, so yeah, let's go to our name. Hi, my name is Krishna. I'm from the University of Calgary. I'm from Dr. Long's lab. So I'm going to present on Ke, which is a software solution that has been designed by Agab. KED is currently being published in Methods in Ecology and Revolution, and it has a GitHub repo. You can access them using the Prepo, you can access them using those QR links. So, CATE stands for Good Accelerator Testing of Evolution. It is a software solution designed for conducting highly parallelized evolutionary tests on large-scale genomic data. So, I'll first give an introduction to what are evolutionary tests. So, evolutionary tests are essentially statistical tests that are used to pretty much quantify deviations from neutral theory of evolution. This allows us to assess factors such as genetic drift and also identify the presence of selecting forces and thereby get a And thereby get an insight into the evolution history of species and also identify regions of the genome that are of interest, things such as adaptive evolution and functional significance. We've designed the software around six main evolutionary tests. They were selected to cover a broad range of factors that are usually assessed during evolutionary studies. For example, deviations from neutrality within and between species selection, variations between populations, and even haptotyte-based testing. And even haplotype-based testing. So, KD is designed specifically for large-scale data sets. So, there's a main problem with processing large-scale genomic data. This is essentially because the file sizes are extremely large, which makes sequential data processing very extensive and takes a long period of time. Also, data streams are extremely long to be processed sequentially. Also, this leads to too many combinations and pairwise comparisons. This can be pretty much summarized into two major bottlenecks. Into two major bottlenecks, which is data retrieval and data processing. After that, we've pretty much come up with three main innovations: an out-of-core file structure, a novel search algorithm, and a GPU-based parallel processing architecture. The software is designed as they are keeping these three innovations as the foundation. So how we will begin is we will split and index the VCF file structures. We process every concurrent action that can be done in parallel. We use all the marginal technology. All market learning technologies such as the GPU, CPU, and even the SSE. We have optimized the algorithms for each neutral data specifically, and we provide the user with a user-friendly software parameterization interface. Okay, so the question is, why use a GPU? So the GPU is designed specifically for parallel processing. And when it comes to these evolutionary tests, we need to process thousands of SNP data in order to kind of get the final test statistic. So, this is the GPUs ideally optimized. So, this is the GPU ideally optimized for this because it has thousands of cores and you can pretty much process all the SNC data in parallel at once. In addition to this, we have an auto-core file structure, which pretty much segments the large-scale VCF files into small segments. And we are able to control the segmentation based on the number of SNCCs that are present in each file. And we're able to sort it based on the percent of the data stored. We're able to subcategorize the files based on population, so on. And this can be done using the inbuilt software. Be done using the in-built software called VCF Splitter, which is inside K. So, once we did this, we realized we kind of ran into another problem because these files are so large that it just generates a large number of more smaller segmented files. So, then we kind of need to traverse this file space and sequentially doing this, you're going back to the same old problem. So, for that, we designed our own search algorithm called Compound Interpolated Search, which uses two main search attribution search algorithms in conjunction with each other. This is interpolation search. To each other. This is interpolation search and back sort of sequential search. So, how you would do it, how this algorithm works, is you'd submit your pretty much your genetic sequence at the region, and then it will weight the gene query region under based on the start and the stop position of the search phase. And based on that, it will find a rough starting point. If the starting point satisfies the gene region under interest, it will call, it will lath on this question and spawn to sequential searches forward and backward in space. Searches forward and backward in space in parallel, and it will collect the rest of the sequence data that is needed to process the gene. So, with all of this working together, this is the overall architecture. The overall architecture is designed so that it can even run on a normal laptop or a normal computer as long as you have a CUDA-enabled GPU. It processes each gene region one at a time, and you interact with the software using the parameterization gene file and the command line. Then we have a high-performance mode, which we call Prometheus. Mode, which we've called Prometheus, where this is designed entirely for high-performance computer systems. And it processes the genes in batches or in parallel together. So multiple gene regions are processed at once. And it uses four multi-related blocks to do this. And this greatly improves the performance as well. How you interact with the software is using the command line. Also, a parameter file which is written in JSON script, which we use to pretty much set up the parameters that we want to run the software. That you want to run the software. There's also a completely documented GitHub wiki, which tells you how to use the software. There's example files, there's how to install and what the errors and the input commands and everything means. We carried out series of benchmarking to kind of prove our software in terms of robustness, speed, and efficiency. We tested against three large-scale genome data sets: the 1000 Genome Subject, 1001 Genome Subject, and GSEG. And GCE. Test conditions were carried out to cover a broad range of evolutionary test scenarios from gene voice testing, window-based SNP testing, and sliding windows. We compared it against comparative state-of-the-art software that are currently being used by evolutionary statistical population genetics, such as POPGenome, VCFKit, and SERTSCAN. Because of time, I'm just going to talk about the 1000 Genome Data Set results. So, 1000 Genome Data is one of the largest data sets that you have for genomic data analysis. Data analysis. We tested it against all known genes of the 22 autosoma chromosomes, and these are test results. So, the first window, this particular panel, is essentially K against K, K's normal mode against Kate's Prometheus. As you can see, there's a stark difference between the two as well. Then, if you compare it to like the traditional softwares that are currently being used, you see the major performance improvement. For example, to analyze this data set, Pop Genome took three days, Kate on its normal mode. Three days, K on its normal mode took three hours, and after measures, it took less than half an hour. So, it's a big performance jump of about 122 times speed to Rupon. So, to summarize, Kate is a highly parallelized software which is designed to pretty much analyze its evolutionary tests. And also, it has a number of utility functions that are dedicated to our genomic data processing. You can download it via the GitHub QR code there. And it's basically pended on a multi-pro and it. Depended on the multi-flow and NVIDIA COVID GPU system. Thank you.