So we have the company. So this talk is by Sira Gratz from the University of Koros. And she's going to talk about, she's going to give an introduction to her presentation about fundamental control algorithms. Thank you very much. And thanks for the organizer for trusting me to give this introductory talk. Just like only before we, well, I guess the other way around, this talk is aimed at people who are not super familiar with representative theory. Unlike only before With you. Unlike fully, I don't have high ambitions of showing the ones that are anything new. However, I hope you can enjoy with me some beautiful classical results and just get reminded of them. So I was actually given a list of topics that it would be useful for me to cover from the organizers. And I looked at this list and I thought, that's a really good list. And then I thought a bit more about it. And I thought, why do I really think this is such a good list? Think this is such a good list. And one of the main reasons is really that it's all about the shape of things. What do things look like? What are the pictures that representation theorists, when I say representation theorists I mean representations or finite dimensional algebra, have in mind when they talk about things? So, right, representation theorists might say something, and then what they write on the board is something else. And we want to understand this dictionary. Right, so we start. Alright, so we start with a finite-dimensional algebra. So we talk about the shape of a finite dimensional algebra first. And we're going to work throughout this talk over an algebraically closed field. Maybe I give one example where it's not algebraically closed, just to see what can go wrong. Basically, what we do here could be done over arbitrary fields. Things just get a bit more complicated. So, to get a first intuition. So, to get a first intuition, it's really useful to just stick with algebraically closed. Lambda is going to be always a finite-dimensional algebra. And I just say finite-dimensional algebra because when anyone here, I'm going to go out and abrupt you and say, like, when everyone here is finite-dimensional algebra, they need a finite-dimensional associative algebra with a unity. So, finite-dimensional algebra is just that. So, let's start with an example. So let's start with an example. And some of the representation theorists might be rolling their eyes and saying it's a bit overdone, isn't it? But you know, a lot of you aren't representation theorists, so it's fine. There's a reason why this is overdone. It's a good example. So we take upper triangular matrices over our base field. This is just a shorthand. I just write K here. That just means upper triangular matrices with entries in K. Just being a bit lazy here. So that's a finite dimensional argument. So that's a finite dimensional algebra, right? That's 1, 2, 3, 4, 5, 6, 10. Good. So how can we start to study the shape of this algebra? Well, we want to think of this algebra as having an additional structure as well. Namely, I don't know if my shading shows up very well, but that doesn't. Yeah, that's very good. So we want to view lambda actually as a round. Lambda actually as a right module over itself. And now I'm going to make the first enemies in the representation zero community. Maybe some of you prefer left modules, I apologize. I'm going to make more enemies because there's going to be choices of how to write things and which directions you want to have things in. For me, today it's right voices. But you might have to adjust if you talk to someone else. Right, so we have a right module, which means this lambda has lambda linear right action by lambda. By lambda, just given by multiplication. Good. So this is a lambda write module. In particular, as Uli told us, that means we can't decompose it into parts. And well, my ring is a boring one. I don't have this interesting ring that Uli's son was studying. I'm a bit jealous, but also it makes my life a bit easier. So I actually, in my example, we just have three and we compose for summons, that's fine. So I decomposed. That's fine. So I decompose this guy into indecomposable summons as a right lambda module. So that just means I can write this as a direct sum of these modules, and the modules are indecomposable. As in each of the PIs, I cannot again write as a non-trivial direct sum of smaller pieces. It is a complete decomposition. What does that look like? What does that look like in my example? Well, actually, the lambda as a right module decomposes into the sum of its rows. Each of that is a right module. We can let this algebra here act on the right, so we multiply one of these row vectors from the left with this matrix. And all of these are going to be invariant. And all of these are going to be invariant under this action, so they are models. But they are indecomposable. We can't decompose them any further. Good. So a little bit of a technical caveat is that to start with, later we're going to just carelessly drop this assumption. To start with, we're going to assume that our algebra lambda is. Assume that our algebra lambda is basic. That's just this technical assumption that says that these summons pi are mutually non-isomorphic. So if i is not equal to j, then p i is not going to be isomorphic to pj. That's the case in our example. Right here, all of this, this is three-dimensional, two-dimensional, one, right? These are not isomorphic to one another. But well, I think it's actually it's fine. It's fine. They're not isomorphic to one another. And that's what we want from our algebra that we talk about now. So we want to assume that lambda is basic. So these are mutually distinct summits here. Now, just to make it clear, not every algebra is basic. There are very, very nice finite dimensional algebras which are not basic. For example, if you take all matrix algebras, just to be a bit lazy, I took 2 by 2. Just to be a bit lazy, I do 2 by 2 matrices this time. This again decomposes as a sum of its rows, but the rows are actually the same, right? So we get the same indecomposable sum at twice. That's fine, but not what we want right now. So we have a basic algebra. Very good. So, in particular, if we go to our basic diagram, we have a decomposition of lambda into these indecomposable summits, which actually gives me a way to write my one. We can write one uniquely as a sum of elements EI, where EI lies in Pi, just because it's a direct sum. Now, these EIs are actually orthogonal idempotents. Are actually orthogonal idempotens. What does that mean? That means that Ei squares to itself, Ei squared is equal to Ei, and if I multiply Ei with Ej, then this is 0. So this kind of corresponds to having this direct sum decomposition. It corresponds to writing 1 as a sum of these orthogonal eigen protects. So, what I then get is that the projective Pi is just the lambda module that is generated. Lambda module that is generated by ui. So it's everything I get by multiplying lambda from the right onto ui. What does that look like in our example? We had our decomposition of this upper triangular matrix algebra into its rows, which is the sum of p1, p2, and p3. This corresponds to writing the one, which is just the diagonal identity matrix, as a sum. As a sum of these idempotents. And you can easily calculate, if you feel like it, that these are indeed idempotent. And if we multiply two different ones together, we get zero out. And then we can also see that, right, if we multiply that guy here from the left to our upper triangular algebra here, what I get out is precisely the first row. And if I do the same with that one, I tweak the second row and I picked the second row, and that one picks the third row. So we have this correspondence. Good. Now we're finally, slowly but surely getting to the point, which is that the shape of our algebra is encoded in something called the Gabriel quiver. So what is a quiver? A quiver, you could, if you want it to be a bit subtle, Wanted to be a bit subtle, you could just say a quiver is nothing but a direct graph, a finite direct graph, but it's really a bit more than that, right? A quiver is what representation theorists think about as these things. So like directed graphs comes with a whole bunch of connotations, right? There's graph theory. People think about certain things when they say studying graphs. What even is a graph? So for us, and these are quivers, at this coin, this term was coined by Gabriel. Very beautiful. Very beautiful name because what is equiver? Equiver is just a bunch of arrows and this is a very quiver. Alright, so when we start with a basic finite dimensional algebra, which we saw we can decompose into these indecomposable Pi's, which we call projective. I probably already said that word, not here, but these are precisely indecomposable projectives that how you can. Decomposable projectives, that's how you can define them. Then we associate to this a quiver, which is defined as follows. So the vertices record my indecomposable projected models. So if we have an n of those, and remember these are basic, so these are really n different ones, then the vertices of our equiver are going to be labeled by one up to n in the positive perspective. In a positive projective, gives me a vertex. And the arrows from i to j are defined as the following. So, this is quite an expression here. We're going to dive into that a bit more in a moment. But we see that it has i and j are kind of represented here by making sure we have to multiply first by EI, by this eigenpotent at i, and last by this eigenpotent at. And last by this argument j. So we kind of make sure we pick out the pi and pj in the right way. And in the middle, we have this radical model of the radical squared. So the arrows in my equiver should be given by stuff in the radical of my algebra, but not too many things. We want to throw out the radical square. We don't want to have too many arrows that record the same information several times. Right, so let's first look at our example. What does that look like? And then What does that look like? And then talk a bit more about the arrows. Right, so in our example, we have this familiar decomposition up here, like decomposition of the algebra as a right module into its projective summit, the decomposable summits, and then the corresponding way to write the identity as the sum of orthoponal eigenportance. So if we look at the radical of So, if we look at the radical of lambda, this is just the intersection of all the maximal ideals of that guy, which are precisely the strictly upper diagonal matrices. So that's the radical, and then you can multiply that with itself, and you get that one-dimensional guy here out. So, if you quotient the radical by the radical squared, you get this two-dimensional guy. And now, the And now, the rest is a simple computation that you're welcome to do if you want. If we multiply that guy, we just want to multiply it by Ei from the left and Ej from the right, and that's going to give me the errors from J to I. So for example, if you do that, if you multiply E1 from the left and E2 from the right, then you should get out something one-dimensional. That's why the dynamics are here, I think. Here, I think. And that means you get one arrow from 2 to 1. So, in this case, actually, everything is one-dimensional. So, you just get these single arrows between the vertices. So, let's talk a bit more about how we can think of these arrows. Yes, exactly. Which are essentially the things ending at I. Yes, so there is precisely the right module generated. I mean, it depends what ending means. Yeah, yeah, there's a choice there. Maybe you're going to say this later, but if you were to use a larger triangular matrix, you would get a longer absolutely. Right. So if you just take a So if you just take a, so here we have 3x3 matrices, upper triangular 3x3 matrices gives us precisely A3, upper triangular n by n matrices gives us 3. And the computations are essentially the same, or analogous. Thank you. Alright, so let's talk a bit more about the arrows. So the arrows really actually are special maps between these individuals. Maps between these indecomposable projectors. Let's dive a bit more into that. So Lamida is actually, as an algebra, isomorphic to its endomorphism ring, as a right multiplication. So how does this isomorphism work? Well, we just send an element alpha to left multiplication from alpha. You can check for yourself if you want that this is in, in fact, defines a well-defined map, and it's an isomorphism. So, in particular, So, in particular, that means, right, if we think of lambda as being decomposed into this indecomposable projective, then the endomorphism ring of it decomposes into these direct sums of morphism spaces from pi to pk. So, in other words, an element in our algebra can actually be viewed if you view it as a map from lambda to itself. map from lambda to itself, it can be viewed as an n by n matrix with entries fij, where fij is a map from pi to pj. So we can really think of elements in our algebra actually as maps. In particular, elements in this like, I'm not going to say the expression again, this EI times rod, blah, blah, blah times ej expression, right? You can think of these things as maps. That's what we want to think about now. So, right, I just like wrote that up here. I just like wrote that up here for reference. And that was the dij is the number of arrows from i to j. So if I have an element in my algebra that corresponds really to a matrix of these fijs, where fij is a map from pi to pj. What is pi, as we just discussed, right? This is the right module generated by i. pj is the write module generated by pj and then Ej and then fij is just precisely this. Well, if you think of f now as an element in my algebra, it's left multiplication by Ei then F then Ej. Having EI first guarantees that we pick out this component, right, because these are orthogonal eigenportents. If you multiply EI onto anything else in another projective, it's going to vanish. And so that guarantees we. And so that guarantees we pick we pick out this component, then we apply F because that's what we want to do, and then adding the EJ at the end guarantees that we only look at stuff that actually then ends in this component PJ. So in other words, the arrows from I to J, these are of the form EI, then some element in our ring, then Ej. So these encode maps from Pi to Pj. From Pi to Pj. But they encode very special maps from Pi to Pj, namely ones that correspond to things that live in this expression here, rad mod rad squared. So what about that? Well, the radical of lambda, well, lambda is isomorphic as an algebra to its endomorphism cell, an endomorphism ring. So we can actually decompose that further and right, we know that. That further, and right we know that is a direct sum of homes from pi to pj. So, this is going to be isomorphic to this direct sum of these things. There is some, you know, you have to work a little bit to get this. This is not all just like automatic, just to be clear. But what you end up with is that what I denote here by rad lambda pi to pj, these are the radical maps from pi to pj. These are precisely, these are anything. These are precisely, these are anything possible, these are precisely the non-isomorphisms from PI to picture. And that makes a bit of sense, right? In our quiver, we don't really want to have redundant information. We don't necessarily want all the isomorphisms recorded, right? That would mean you have to make loops and stuff. There's a lot, would be a lot of noise in the quiver. So we only want to focus on non-isomorphisms. So, right? An arrow from I to J is a non-isomorphism from Pi to Pj. Use the basic assumption. Yes, absolutely. Basic is super important here, otherwise, stuff's not going to work. Right. And now we also have this radical squared. So without going into depth here, radical squared basically means that if you have something that's in a radical squared, it means you actually factor through another indecomposable projective. We don't want that. If it factors through. If it packes through another one, it's already there in the quiver. We don't have to put in an extra arrow. Good. So the dij are really given by non-isomorphisms from pi to pj that do not factor non-trivially through a third pk. So, in our example, right, we already well, we already drew the Already drew the quiver, is this quiver 1, 2, 3 with a linear orientation? This, so yeah, P1 corresponds to P1, 2 corresponds to P2, 3 corresponds to P3, that is P1, P2, P3. And these maps here, these radical maps, are precisely the inclusions. The inclusions, Vidaka includes into that space, that space includes into that space, and these include. Includes into that space, and these inclusions are non-isomorphisms. The guy we include is smaller. So these are a non-isomorphisms, and they're radical maps. And we see we also have an inclusion from that guy, from P3 into P1. We don't want to record that extra in the quiver because that is just the composition of first including it into P2 and then including it into P1. That's where this radical square, pilling the radical square comes from. square comes from. Right, so that's the Gary equivalent. So we get from a finite dimensional algebra, from a finite dimensional basic algebra, to a quiver. So what do we do then? What do we have if we start with a quiver? So if we have a quiver, a finite quiver, I guess when I say quiver, ah, okay. Mostly I mean finite quiver. No, no, that's that's not. No, no, that's not. Let's just, yeah. Q is a finite quiver. We can construct its path algebra, which is an algebra we just construct from the data given in the quiver. So the elements in my path algebra are just k-linear combinations of paths, so things you can walk in your quiver. And multiplication is going to give by concatenation of paths. This is a very minimal definition. I'm going to kind of gloss over some things. But well, I mean. Things. But well, I mean, in the example, we see it, I hope. So, this is just putting two paths together if we can. Let's look at this in an example. Let's look at this A2 times N3 situation here. And I have this pink path, which is like go, and I maybe again, I might be offending some people. I write from right to left. Sometimes people like to write it from left to right. Write it from left to right. So I'm going to go first gamma, then delta, and then I'm going to just stay at the vertex 4. There's lazy paths involved as well. I need those lazy paths because I need an identity. So I have a lazy path for every vertex. So that's the thing I could do. Walk along gamma, then delta, then stay at 4. I also could do this, maybe it shows up as gray. Up as gray. This gray path, which is just like staying a bit at 2, then going to via beta 4, then via epsilon to 6. These are legit paths. And what happens, like I can look at the linear combination of those two, so the pink one minus the gray one. And if I multiply that from the left with epsilon, what happens? Well, I can actually concatenate the pink path and epsilon, right? We can just walk the pink path and then walk epsilon. And then walk epsilon, that's the thing that works, which is going to give me epsilon, then the pink path, and I can just scratch out the lazy path because nothing happens there. That's just the same as epsilon delta gamma. But I cannot, if I go first the blue path, I'm actually at six, and from six I can't do epsilon right. So if I multiply these two together, see. So that's how that's how the multiplication in there works. That's how the multiplication in there works. Good. All right. So if I want to really study finite dimensional algebra, I shouldn't just study past algebra. I should study quivers with relations. And that's something I think everybody here can appreciate that we might want to have some relations on here. So a relation on Q is just a k-linear combination of paths which start in the same place. Path which start in the same place and end in the same place and have lengths bigger or equal to two. I don't want to have arrows involved in there because if I had an arrow that's equal to some other path, there are a combination of paths I might just omitted in the first place. So I only want paths of length at least two, and they should start and end at the same point. For example, for this quiver q, I could just impose commutative devices, right, saying that alpha beta is the same. Alpha beta is the same as gamma delta, and delta epsilon is the same as beta mu. Right. So important theorem, studying this query with relations is actually the same as studying basic finite dimensional algebra. So if we have So, if we have a basic finite-dimensional algebra over an algebraically closed field, then there exists, so then we can construct the Gabriel quiver of lambda, q lambda. We can look at its path algebra, and then we can find in that path algebra a a nice ideal, something called an admissible ideal, such that the algebra I start with is isomorphic to the quotient. Isomorphic to the quotient of the path algebra of the priver by that ideal. So, what does nice ideal mean? It really aligns very well with what we've already thought about. It just means it consists of relations, so it's contained in the paths of lengths 2. There's no paths of lengths 1 in there. And also, if the paths start getting too long, they have to be contained in our ideal. So, from a certain n on, all the paths of lengths n are contained in the R ID. Lengths and are contained in the right. Make sure we're still in a finite dimensional. Yeah? You're afraid to tell me that this is too complicated, but what happens if case is not algebraically close? I'm glad you asked. Can I give you an example? I'm just going to go with the slides and trust that your question is going to be answered in a couple of minutes. So, we could for Um, right, we could take, for example, to just look at something that actually has relations. We could look at the dual numbers. Uh, the dual numbers are actually indecomposable, so there is just one projective, namely lambda itself. And so the quiver has one vertex, and we have a radical map that's given by multiplication by x, right? That's a non-isomorphism. So that's the quiver. So that's the quiver. And indeed, we see that if we look at lambda, if we look at the path algebra of that quiver, that is just the polynomial ring, right? Because it's just the lazy path. And then multiplication by x, x squared, x cubed, moment. So lambda is actually isomorphic to the path algebra. It's the path algebra of this diabetic curve, modulo x squared. This is an admissible idea. This isn't a mixable idea because it has links. So, getting into answering that question, what happens when it's not algebraically closed? So, what is needed for this to work well as we stated it? Well, we have to have that lambda modulates radical, this is going to be semi-simple, but this actually has to be a product of finitely many copies of K. Many copies of K. If we have that, then what we just said works. And the two conditions that make sure it works, I mean, it might work otherwise as well, but with modifications, the two conditions that make sure it works is that k is algebraically closed and that lambda is basic. Because let's first look at the one that wasn't asked yet. Let's look at the basic situation, or the non-basic situation, right? We take our We take our matrix algebra, 2 by 2 matrices. We saw that's not basic because it decomposes as its rows. Now, that guy is semi-simple. Both of these guys are actually simples. So its radical is zero. What our recipe would yield is just two points, one for each of the indecomposable summons, but no max because the radical is zero. And that is bad, right? And that is bad, right? Because that guy is going to be almost two-dimensional. You can't quotient anything out. This is going to be two-dimensional. This is not going to work for this. So, very sad. But we only have to be sad for a little while longer because it's not a problem. We can just deal with basic things. The other case which addresses the question is that if we're not algebraically closed, if we just want to naively If we just want to naively do what we did, right, if we look at the complex numbers as a two-dimensional algebra over the wheels, then C is actually indecomposable as a C module. It's simple. It has radical zero. So what our recipe would yield is just one point, and that's bad because, well, that should be R, right? And so that's also the question. That's the question. So, it doesn't work anymore. There's a fix. You can work with species instead of quivers. So, you attach some labels to the vertices and the arrows, and you can still do it. So, there is a whole theory that covers this and has some analogous results to the algebraic profosed. So, I have a question, so but my question is about the characteristic. So, you you start with equipment. So you you start with a quiver and say uh the relations are you know relations use only let's say class use thing in a company. Yeah. And then you just take a field with different characters. Number of indecomposable project would be the same. Will the yeah you look at the dimension right because you look at the dimension of these vector spectrums. Look at the dimension of these vector spaces over your base field. But will the algebra, half algebra, be very different? No, no, I don't think. No, it's not really a characteristic. I mean, there are some instances where the characteristic is going to matter, but it is sort of a relation. Exactly, exactly. But um but I don't think for the I don't think for the patholog well, I h would have to think about it, but I don't no no, I don't think for the pathological problems you have to. I don't think it would have passed that's what I'm sure. I mean, we're a presentation. Yeah, of course, like it might it might matter later on, but up here. Alright, so we talked about the shape of the algebra. So the next step is to talk about the shape of the audio cut, of all the representations as we can. We can. Right, so we kind of want to repeat the idea that we had of encoding the shape of our algebra in this quiver, in this gable query, and doing an analogous thing for all the modules, sort of for the modules over this algebra. So if you remember in the Gabriel quiver, the vertices Pi were given by the intercomposital projective modules, so by certain modules, the R. So, by certain modules, the arrows were given by special maps, this radical model, radical squared. So, we kind of want to extend and copy that approach and say, okay, we now want to look at this gamma number, which is called the Authentic-Reichenquiver, which we're going to define more precisely in a moment, where now we have as vertices all the indecomposable modules, and the arrows are going to be special maps. Arrows are going to be special maps that somehow mirror this construction we have up here. So let's talk first a bit about the indecomposable models themselves. How can we think of those? Well, we have this tool of quivers available, so we want to start thinking about indecomposable modules or modules in general, underrepresented modules, in terms of these quivers. Alright, if we have a basic algebra, Have a basic algebra, then we have this presentation of lambda as this quotient of a path algebra. And what happens is that the finitely generated modules of bright modules of lambda are actually as a category equivalent to finite dimensional equiver representations of the quiver qÎ» with the relations that come from my idea. So I might just think of my modules. So I might just think of my modules as actually representations of the quiver. The representation, right, because I work with white modules, it's going to be a contravariant functor from the quiver q lambda with these relations thought of as a category to finite dimensional vector spaces. So let's look at what that looks like in an example. So we have our equivalence relations here, just the A2 times A3 with commutativity relation. A3 with commutativity relations. So, what's a representation? We have to assign to each vertex a finite-dimensional vector space, which we did here, k squared k0, k squared k k. We have to assign to each arrow a map that works. So, a map that really goes between those two, the finite dimensional vector spaces we pick. So, here, just we pick the map lambda zero, etc. Right, they always have to go. Etc., right? They always have the right dimension. And because we have relations, we also have to make sure that the relations are preserved. So if we do alpha then beta, this should be the same as gamma then delta. So this square here has to commute with the mass that we assign. And here the right-hand square you can be very confident about because the compositions are always zero. The left-hand square should work as well. So, we can just think of modules as quibble representations. So, now we get to this joyous point where we really say we don't have to speak to basically. Because if we have, as long as we are representations here, this is the so if we start with a finite-dimensional algebra over an algebraic closed field, then, well, we can still declare. Then, well, we can still decompose lambda as a right module into its indecomposable projective summons, but they might not show up with multiplicity. It's going to be pi to the ki. Ki might be bigger than 1. But these pi's are still non-isomer. Pi is not isomorphic to pj, if i is not equal to j. So we can just pass from this very simply to a basic algebra by saying, okay, I'm just going to look at all the in the composite project. Going to look at all the inner composite projectives that show up, but now I'm just going to let everyone just show up once. That's going to give me a new finite-dimensional algebra, which is extremely closely related to that one from a representation theoretic viewpoint, namely that they are Marita equivalent, which is just a fancy way of saying that their module categories are equivalent. The module category, finitely generated modules, of lambda, is equivalent to finitely generated modules of its basic algebra, of the basic algebra. Algebra of the basic algebra associated with, and that we saw we can't just interpret in terms of pivot representations. So, studying, yeah. What's the multiplication in the new algebra? I'm the one inherited from up there. Ah, and but the one is not the same one. The one is not the same one. No, we have, I mean, yeah, you have to do, you have to think about it a bit carefully, but there's a way to do this basically. So there's a way to do this basic cash. All right. So we discussed now how we can think of these indecomposable modules, these vertices in this Auslander-Reichen quiver. So we define the Auslander Reichenquier. If we start with a finite dimensional algebra of an algebraic closed field, then the Auslander-Reichen quiver, gamma lamb. Gamma lambda has as vertices all the indecomposable right lambda modules, which we saw we can think of as indecomposable representations of some bound quiver. The quiver is the Gabriel quiver of our algebra. Now what about the arrows? Now the arrows try to mimic this idea that we had when we built the quiver of the algebra, the gateway. So the arrows from n to n are going to be, they're going to be dn. Are going to be dmn arrows from m to n, where dmn is the dimension of some space of mass from m to n. So it's the dimension of the space of irreducible lambda linear maps from m to n, which mirror this idea we have before. Yeah, you have to be a bit careful, right? It's not as simple as just looking at the radical maps. So, what specifically are these, right? Are these, right? We saw what we wanted in the Gabriel quiver was to take the radical mass of the non-isomorphisms that do not factor through another projective non-trivially. So we kind of do the same thing here. We take the non-isomorphisms from n to n, which do not factor non-trivially. So what does that mean? We take all the maps that are not isomorphisms, and if we can factor the maps. We can factor the map F via some other module, then G, either G or G has to be the inclusion of a summit or H has to be the projection onto a summit. And in this case, this is kind of trivial what's happening here. Good. So this is our outside Reichenkrimmer. So let's construct it for a run. So let's construct it for a running example. We have our quiver one to three. Somehow the orientations of the average change. At some point I realize I wrote the slides in chunks so they go from one to two to three. I apologize. But the ideas are the same. So we have the indecomposable modules of that guy and are just the indecomposable quiver representations, which happen to be Which happen to be in this case really just one-dimensional at every vertex. On the left here, these are precisely our projective modules. This projective simple, this is the projective MP3. And we also get these additional so the irreducible maps are drawn here in grey. Are drawn here in grey. And this is the outside red one. Yeah. Is that quiver always finite? No. And this is a very good question because we're going to get to it. You're really asking great questions. I love it. You're just foreshadowing things that are going to happen in my phone. So this is not always. I chose a very easy example on purpose to be able to draw that thing. However, if you stick with AN, then the answer is yes. Then the answer is yes, it's always normal. You're gonna actually just, so right here we see this triangle shape, so we're gonna just have a bigger triangle shapes like this, where the indicomposite project is still on this ray here. Everywhere you see here, I mean, you can view it as a positive. I mean, finance model, smart. Look at the positive look. Yeah, I was expecting if you have a wild representation type thing. Absolutely. Absolutely. If you have something with wild representation type, the information you can draw from the outside of the wide cover is very limited. Yeah, basically all look kind of the same. All right, so, alright, well. Alright, so, alright, well, there we have one. It's not wild, it's tale. More of that later. But we take the chronic equivalence, so just two vertices with two parallel arrows between them. This has a lot of representations. We can look at the projectives. They sit over here. They're just here, this is k0 and k square k with the appropriate maps between them. And then we have a bunch of modules. And then we have a bunch of modules we can construct from them. And we're going to be until 35, right? Yeah, yeah, yeah. Okay, we're going to be a bit more precise about what construct means, like how can we construct these guys from the projectives. So we have this component here, all the guys we can construct from the projectives, so we call them pre-projective. And then we can do kind of a dual thing. We haven't talked about that, but we have a dual version of projectives. We're going to make this. Projectives, we're going to make this tolerance a bit more precise, which are called the injectives. So we can take the injectives, there's two of those, two magical projectives, and we can construct a bunch of modules from our injectives. These are called the pre-injectives. But then we have a bunch of other ones that don't fit into these models. They're called the regular modules. And here there is a for each point on the projector. Point on the projected line, you actually get a whole family of those, one for each dimension n, and they are grouped into these tubes. So we have this one parameter families. And if you talk about tem ashrobus, well we're talking about seven. So, but here's an example of something that's not pointed. Not finite example of something that is finite, but where we have some relations to see that things can look a bit different when we have relations, like we take the dual numbers again, we have an indecomposable projective, as we've seen before, and then we also have an additional indecomposable module, indecomposable simple, K, which is isomorphic to a KX1 to X. And so the game quimmer of that thing is. The Gabriel quiver of that thing is actually this loop, but the outside and the right-hand quiver of that guy will have these two vertices, one for each indecomposable, and the irreducible maps are precisely inclusion of k into my algebra and then projection from lambda onto kx modulus. Right, so in here we don't see that loop anymore, right? That's a feature of the path algebra situation. So the loop is multiplication by x, which would be pi in the pi. Right. So an important point to make maybe is that I drew these queries in a very specific way and I didn't have to think long and hard about how to arrange them because I'd seen it just before. But also there is a good reason, like there's a systematic way to arrange the vertices in the double side. Way to arrange the vertices in the outside code. And this is encoded in the outside right translation. So maybe to revert back and motivate this a bit, because one of the core motivations of this development of this also theory, which is a very big theory in the field, was if you start with something that looks like type A, what you end up with with the outcome of the What you end up with with the Eusline Reitman cover is again a finite equivalent in this case. And it has some relations because, well, some of these maps, there's relations of the maps when you go up here and then go down here, that's zero. And same here, and then you can also check that this square commutes. So there's relations. So really what you get is equivalence relations. What's equivalent relations? We should think of that as an option. And this was really like uh the main motivations to study these algebra as we get here, they could also an algebra. Here, they call Auslan algebra. M after Auslan, obviously, who was very interested in studying them because the idea was then that you could study these algebras that you get with kind of homological methods that you have available from using this as a module category. So, this is really quite, really very insightful, this kind of translations and between these viewpoints. Um and also under writing theory it really studies that. So Apslander algebra is the clear algebra of the Apslander right inquiry. Of a finite, yeah, of a finite type one. And the relations were relations of the maps. So, right, I mean, some of these irreducible maps, there's going to be some conductivity relations in there, and there's going to be some that compose to zero. So these relations are the ones that you can have scored. Right. So Right. So, okay, so how do we arrange them? We arrange them using this Auslander-Wrighten translation. What's Auslander-Wrighten translation is basically a way, but we said before when we looked at this chronicle query, there is a whole component we can construct from the projectives, and the whole component we can construct from the injectives. So, we have a way to generate new modules from old via some procedure. And this relies on this outline and writing translation. So, what's the outline of Reichen translation? Translation. So, what's the Aussie and the Reichen translation? The Auslan-Dender translation is constructed by taking certain pluralities. So, if we look, so we can look at the functor, the contravariant functor, which takes a right module and maps it into lambda. This is going to give us a functor from right modules to left modules. And actually, it restricts to a duality between the projectives in there. So, it's going to. The projectives in there. So it's going to send projective right modules to projective left modules. So if we have a map between two projectives from P1 to P0, we're going to get a map between projectives P0 star to P1 star here. So this gives us a way to go between those. So there's duality. Now we can extend that to all of the modules. To all of the modules. Why is that? Well, any module, any right lambda module, we can approximate by projective modules. So we can take a projective presentation. So how should we think about that, a minimal projective presentation? How should we think of that? Well, the module M is going to be generated by a bunch of stuff, finally generated by a bunch of stuff, and these generators are going to be subject to some relations. Subject to some relations. That's precisely what is written on the board here. P0 encodes the generators of M, they project onto M, and P1 encodes the relations between them. So any module M we can kind of some way think about in terms of such maps between projectives. So that means we can apply our contravariant factor to this first bit, to the presentation. We get a map from P. Presentation: We get a map from P0 star to P1 star. This functor is not exact, so we're gonna end up with some co-kernel of f star which we defined to be the transpose. Good. So we can. Maybe the exactness you mentioned, like F may not be an inclusion. That's true. But we're gonna get something new and interesting. But you're right, it might not be an inclusion. So, well, okay. So, we went from right modules here to left modules. That's not ideal. We just want to think about right modules today. So, how to go back? Where it is an easy fix, you look at another duality. You just hum into your base field and you go back from left modules to right modules. It's just a corrective. It's just a corrective thing that we do. So then doing this transpose and then correcting with this duality, that's the Auslan-Wrighten translation. So without really having given you all the definition, let's still look at it in an example. Let's look at A3 again, a running example. And let's look at this module here. We can write, so this is concentrated at vertex 2, we can write this projective. We can write this projective presentation because it's concentrated at 2. It kind of has to be approximated by the projective generated at 2, which is that guy. We look at the kernel, by magic, this is again projective. So we get that projective presentation. We can apply a star to it. If you want, you can go and compute it. We get, for that picture here, the arrows turn around because you go from right to left modules, and instead of having paths. And instead of having paths ending in the vertex, we have paths starting in the vertex. And we can compute the co-kernel of that map, which you can do component-wise, like at each vertex. We get that guy out, which is the transpose of m. We can correct this step by taking the dual, which is just turning around the arrows, and we get that this is the outside. Right, so that's how this works. How this works. So in the outside and the right-hand curve, we're going to write tau of x to the left of x. That's how we arrange. Now, maybe let me gloss, maybe let me skip over that because I think I have five minutes left. Horrible. Well, anyway, so you have, yeah, so you have this actually, this outline of right in translation is going to give me a bijection between non- Translation is going to give me bijection between non-projective indecomposable modules and non-injective indecomposable. So you can start with projective indecomposable modules, apply the inverse of that thing, and get a bunch of new modules, or you could start with injective indecomposable modules and apply tau, and you get a bunch of modules. And that's how we can construct these pre-projective and pre-injective composites. This is another picture in here with tau. In here with tau, I'm also drawn in here, right? This is just tau is to the tau m is to the left of m. Right, so let's talk a bit about representation. This was already addressed in Bully's talk. So what we saw, at least in one picture, is that if we take a quiver of type A, just look at the pass algebra of that. Path algebra of that, then it actually has finitely many indecomposable representations of Johisomorphism. Also, in the right equipment, we would like these triangles before. So that's why we say that lambda is a finite term. Now we've also had an example of the Kronecker core, and we saw that we have infinitely many indecomposable, non-isomorphic plus indecomposable. Isomorphism models, but they were very well behaved. We could write them all down, right? They came in these one-parameter families. So in this case, if we can write them all down, so they come in these nice one-parameter families, then we call lambda table. And then finally, if this doesn't happen, and I mean Uli gave that all like classification of when these grid algebras are, Are of which representation type, right? If you take a big enough grid with commutativity relations, then it's wild. What does that mean? Well, a way to state that is saying that it actually contains the representation theory of any finite dimensional you can think of. Any finite dimensional algebra you can think of. So it's really, really, really wild. Like anything you could think of to write down is going to contain that representation. It's math. It's mad. In per in particular, it's like absolutely hopeless to try to classify that. Right. A very important result is draws down dichotomy, which also Woolly mentioned, the name. If we have a finite dimensional algebra or an algebraic close to, the lambda is going to be exactly one of those types. It's going to be either finite type or it's going to be tang or it's going to be wild. And that is really remarkable. And that is really remarkable. Well, okay, the first one maybe not. So, it's either finite type or it's not finite type. That's fine. But the difference here is really quite remarkable. It's that we say it's either super, super, super bad or it's really quite nice and we can write it all down. Like, consider this one variety file. One of these things are going to happen. So, so paint might always have part. Yeah. Yes, absolutely. So, really, really quite important. Really, really quite. Or is parametrization canonical? Canonical in what sense? Do you have different ways of parametrizing? No, it may not be. But wouldn't that be good? No, no, they would be no, no. And what does it mean that it's permanent? It means well okay. I mean there's a there's a Okay, I mean you can come in this same precise. You can look as you can look at bimodules of so lambda right, like a lambda right, like a Kx lambda bimodule. So you can look at Kx lambda bimodules, and then you're saying that there's going to be finitely many. Finitely many Kx modules. Am I doing this right? That there's finitely many Kx modules, so you can write down finitely many, and for each of those you have a one-parameter family of modules. So each indecomposable module of your algebra is going to be isomorphic to this module mi, modulo kx modulo x minus gamma, or some parameter number. Parameter problem. So you the one the one parameter family means you take the polynomial in one variable all its modules, you can embed it in basically to be a given algebra, but that compos kind of the same thing. One comes from one parameter, linear linear one variable. Yeah, because you're going to have your modules are going to be kx, the incomposite are going to be kx model x minus gamma for some one. X minus gamma for some one parameter gamma that runs for each fixed dimension. For each fixed dimension. Maybe no parameter for finite different things, but a cover almost all highlights or all the kind. Since you have this feature on the left. Could you repeat the question? I couldn't hear. I don't know anything about that. So, how does it go? So, for each point of the affine line, you have a set of representation, and if you fix the dimension later, you have like a finite number of independent positives about. Yes. About each way. So, I mean, basically, we can just maybe do a drawing. That was going to make things simple. You saw that you have your pre-projective and pre-injectives. They're covered, right? And so you think about your regular representation. So in the Kronecker case, you just had P1 guys of this form. So down here, you have this. This and then we have this guy, and these maps are going to be indexed by some point in P1. So this is like I don't m out and pluton. And so in general, it's kind of going to look like this. Your regular guys are going to look like tubes of this form. And all but um I think mo at most three of them actually look exactly like this. Well, I mean there's gonna be different guys written here, but they're gonna look exactly like this. Here, but they're going to look exactly like this. You have the acquasi simples here, and then guys of higher dimension up here, and then you can have some that look a bit more complicated that could be wider tubes. You could have, I think, up to three of those that look like that. You have a wider tube where you identify these dashed lines with one another, so it lies in zero linear. Or it could even be infinite. It could be even be z and infinity. Just to come back to this question, I mean it's it's indexed by k. Say k is the complex numbers, it's indexed by the complex numbers But it's an algebraic indexing. I mean the complex numbers are as I said in bijection to its I mean it you you need an algebraic moment. And then you have like a shift of RK or some vendor of RKX and you have to fix I mean the the chronicer the chronicer by is actually is direct equivalent to um the Equivalent to the projected one. So you can really think of it in chiefs of a prevalent. Exactly, exactly. So they're derived equivalent. Yeah. It's just like the regular components are going to look the same. It's just the instead of having a pre-injected or a pre-projected component, you just have more total components. We were at Financing very explicit. Can you determine that? Can I plug in your computer each? This is an excellent question. So classification is really the part. So I mean it's it's i i i i there's a famous classification result if you don't have any relations. Uh if you work with hereditary algebra, um the finite type one are classified by Gerdo, they're precisely the Classified by Verdo, they're precisely the ADE diagrams. Between tame and wild. It's in general it's hard. I mean you want to use covering techniques, you use the universal cover of the algebra, and then if you end up in a good case, there's a nice criterion. There's no really complete theory, but you would be hard to come up with an example that what about the finite type? What about the finite type with relations? Ah, okay, okay. The representation finite actually are not classified. No, they're not classified. But if you know that we have this thing, I mean there are only very few. All right, let's focus on some things that we can do with the so maybe one interesting thing to do is like to understand this better is that actually Alzheimer has shown that if we have the finite quantity component Have the finite quantity component in our outside right now, then lambda has finite tau. So if you just start naively with your projected sign, by this knitting procedure or whatever, you construct more modules by applying tau, etc., and you end up with a finite component, then your finite tai. There's even something a bit stronger that holds here, which led to the proof of the first Brouwer-Strau conjecture. Good, fine. We also have, if we have a finite type, Finite type algebra, then the Alzheimer-Reichenkoever actually sees all the information. You want to know? It even sees all the maps. So all the maps between indecomposable modules are going to be compositions of irreducible maps. Not the case even in time time. I kind of thought maybe this would be a bit of interest, a very classical. Would be a bit of interest, a very classical result due to Bombertz and Gabriel, which tells a bit more about if we start with a finite dimensional algebra of finite type, then actually every finite covering of the Auslaner-Wrighten cover is going to be the Auslan-Wrighten cover of some algebra of finite time. So this is a pretty cool way to generate new finite time algebra as well. There's many, many, many coverings, right? Um fundamental group is free. Fundamental group is free, not commuted. And just maybe the last thing I want to say, because I guess by now I'm hopelessly over time, is that I was instructed to just treat finite dimensional algebras, which I did. Some of the techniques are going to work if you're not our finite dimensional algebra, which is, I think, of interest here. For example, if you take the greater polynomial. For example, if you take the polynomial random variable, you can still construct Nausan right. It's not going to really, not everything is going to work as in the final financial case, but you have this faculty. Alright, thank you very much. A few CRMs, but it seems that you've already started the question session. You have more questions? Not also questions. Not just a question, but a short comment and self-advertisement. In Yasu and I's our paper on community vendors, we introduced these for the TV community and there's like version of academic community. I think I've heard that. Sorry. The other things were too interesting. I forgot. I can ask you a question. So, in TV, you're often interested in using non-organized technologies for actually. So, you said answering this question. There were ways to actually do it from algebraic models to non-algebraic models. So, how far can you go? I I mean, so basically everything with adjustments is gonna work. With adjustments is going to work. So you can treat, instead of working with quivers, you want to work with the species. You can still do outline or write in quivers. Now your arrows are just going to have weights. They're going to be weighted by these queries. They count some dimensions of the. Basically, each vertex in your query is going to have some endomorphism algebra attached to it. That's not just k anymore. And they're going to be basically legal by bimodules of these endomorphism rings. So you can still do it. I want to sort of add a comment to Steve's. Community is not interested in finite dimensional to write. So we actually have a quiver. We're not interested in having it out or going to a quiver where you can start with it. But but as I just said, then you you can do you can look at representations and possibly. Yeah, it's a quiver with relations. But you can do that, right? But you can do that, right? I mean we can we can do that just like just like really the space in test is not so no but I was talking also about yeah but I was talking about the the outside right and previously it would be interesting the pre-projective and pre-injective components are always finite or or no? No, no they're actually infinite in the example. You can just keep going. They're only finite for finite diagrams. Because when you find a finite component Because when you find a finite component, the whole thing is fine. But then what for projective and prejective components or components which are kind of flat. Kind of flat. Yeah. We projected a predicate used to doubt all these databases to give a finite common theory, that's the right thing to be more arbitrary finite information actually about you may not have. So computationally speaking, like how hard Moral is, is it possible? It's algorithmically undecided. Yeah, so I was kind of wondering what that means in this case. And my question specifically is if I take two indecomposable representations and I determine if they are in the same component, the Alice and the right equipment. I'll be the three parameters. Oh? Basic program that is not possible. Just because everything is hard. I mean, to classify them, there cannot be an algorithm. But to decide more of these things, I don't know. I mean, like, the outside right query is not super informative if you talk with well type, because the regular components are all just gonna be Z-A. Are all just going to be Z A infinity? You'll learn about it. And maybe there's an algorithm for determining if two are in the same quantum. I mean, no, you might be able to do that if you have two concrete modules given. Someone to ask my computer, like what business between these two outside writing correct. 