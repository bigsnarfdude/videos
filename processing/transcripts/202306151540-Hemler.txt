Um, then we'll say that that's that point P is the singularity, right? So, usually at a random point in the configuration space, the rank of the Jacobian would be the minimum of the dimension of C and the dimension of S. And at a singularity, the rank would be smaller. Okay. So these points can these singular points can cause our mechanism to behave badly. Mechanism to behave badly. In particular, we can lose a degree of freedom and lose mechanical control, but that isn't true of all of the points in this singular locus of the map. So what we want to do is try to understand this map and its inverse and the singular locus, both from a geometric and a topological perspective in a global sense. So we want to understand how they behave. understand how they behave all over the state all over the configuration space and all over the state space so to make that a bit more precise what we want to do is we want to stratify the map so what we mean by that is we want to subdivide the space C and the space S into finitely many manifold regions so that if we take any two points say Q and Q prime in And q prime in s um then the fibers over those two different points should be topologically identical in a in a precise sense. So we'll return to that in a bit. This is maybe a more precise, not a completely precise goal. So the reason that we might want to do that is then we can actually, so now we have, as I said, finitely many regions where the topology of the fibers Of the fibers over any point in those regions is all essentially the same. And so there's some results of Fred Levy that tell us actually that we can, from the topology of these fibers, determine whether the singularity in question was safe, i.e. we could pass it without a loss of mechanical control or not. So the idea would be to use these kind of To use these kinds of topological criterion on these fibers to classify all the singularities globally and to understand precisely where the safe and unsafe regions are. Okay, so now I'm going to try to be a little more precise about our setting. So I want to compute things globally and I'm an algebraic geometer. So for both of those reasons, S and C will be algebraic varieties and F And f will be a polynomial map. So, for those that spend less time thinking about algebraic varieties than I do, an algebraic variety is just the zero set of a polynomial or of several polynomials in many variables. So, it's a couple colorful examples here: a surface and a curve in R3. And from a computational perspective, we can sort of see varieties as a kind of nonlinear analog of a vector space in the sense that for vector spaces, we have an algorithm, the Gaussian elimination procedure, that lets us rewrite and solve linear systems. And for varieties or non-linear systems of polynomial equations, there's an algorithm called the Grubber basis algorithm. Algorithm called the Grubber basis algorithm that lets you do similar things, i.e., rewrite the polynomial system and also solve the polynomials to get solutions. Okay, so that's going to be our main tool. So what we're going to really want to do is phrase these geometric questions in terms of growth of basis problems. Okay, so if we're going to stratify maps, while they're maps between Well, they're mapped between spaces. So, first we need to stratisfy the spaces themselves, so the varieties. And so, we want to break them into smooth manifolds because, of course, one can kind of intuitively see from pictures of varieties that they aren't necessarily smooth manifolds. They tend to have pointy bits. So, what we want to do is, for example, in this red picture here. In this red picture here, right, we can see it self-intersects along this black line. So we want to, at very least, separate out this black line from the red surface. And we might want to do a little more separation than that. Okay, so we'll work first with complex varieties because that tends to be simpler. So a variety is, as we said, more formally written down here, just the common zero. Just the common zero set of a list of polynomial equations. And we might also write that in terms of a polynomial ideal, which is just in a polynomial ring, all the ideal generated by this list of polynomials. So algebraically, we can characterize when a variety has a singular point. So all of those singular points we saw on the pictures in the last slide, those will be precisely the points where. Those will be precisely the points where the Jacobian matrix of our list of polynomials defining the variety drops rank, sort of as expected, I think. And then, so a stratification is going to be some filtration. So starting off with the empty set and ending up at x of a variety in x, and it's going to be given by all of these things in this filtration will be varieties. And then we'll say something is a strata. If we take If we take a successive difference of two of these things in the filtration, so xi minus xi minus one, and this will be the strata m, and it will be either empty or else it will be a smooth manifold of pure dimension i. Okay, and so that will be a stratification, but we might want a little more than that. In particular, the thing that we'll want, roughly speaking, is that we're going to want to decompose X so that X, so that this decomposition is equisingular. So by that, we mean that a neighborhood in X of any two points in a connected component of one of these manifold pieces, these strata, is going to be geometrically similar. Okay, so a common example is this Whitney umbrella, it's called. Not sure it would make the best umbrella, but nonetheless, you can imagine if you flipped it. You can imagine if you flipped it upside around there, and that purple bit is the handle. So that is the singular locus of this variety. But we sort of see this point at the bottom in our upside-down umbrella here, is kind of geometrically different. We would imagine a neighborhood of that point isn't really like the neighborhood of any other point on the handle. Of course, the handle is just Of course, the handle is just artificially extended. Really, the handle is embedded in the shape all the way up. I've just drawn the handle taller so we can see it as a handle. But so all the other points in the handle, you sort of expect them to be similar, but this point at the bottom is somehow different. So how do we actually make that precise? Well, there's a condition of Whitney's, which he called condition B, and which is now called Whitney's condition often. Called Whitney's condition often, which is going to help us do that. So I'm going to illustrate Whitney's condition by looking at an example on this red surface here, which is called the Whitney cusp. So what we're going to want to see is why this central green point needs to be separated out from the black line. So it's sort of clear that this black central line is. Black central line is the singular locus of this variety. But we want to see why Whitney's condition essentially is going to fail at this central green point. So Whitney's condition is a condition on two pairs of strata. So we'll take a strata pair. Let's think of sigma as this red surface without the central black line. And we'll think of tau as the black line. And we'll think of The black line, and we'll think of a point y in tau as this central green point. Okay, so we want to see why condition b doesn't hold with this green point. Okay, so condition b tells us that if we take any two sequences of points, so a sequence xi of these blue points in sigma, so in the red, and a sequence yi of these yellow points in the black line at tau, both of them converging to our central point. Of them converging to our central point Y. And then, if we take secant lines between these two sequences and also then get a sequence of secant lines, and we take the limit of that sequence, we're going to get a limiting secant here passing through our central green point. And we'll do also, we'll take a limiting, or we'll take a tangent play in each one of these blue sequences. Each one of these blue sequences in the higher dimensional piece. And then we'll take the limiting tangent plane from that sequence. And what should happen if condition B is to be satisfied is that this secant line should be fully contained in the tangent plane. But here we see that the limiting secant is actually orthogonal to the tangent plane. So that means that condition B does not hold at that point. So that means that we. So that means that we need to essentially, or we need to precisely, when we're stratifying, we need to stratify this black line to remove that central point. So the stratification becomes the red, the black, and then the green dot. Okay, and it's not necessarily so easy to see pictorially, but at all of the other points other than the central one, this condition does hold. This condition does hold. Okay, so Whitney showed many years ago that for varieties we can always find a Whitney stratification, and that this gives us this equisingularity property that neighborhoods of points and strata are similar, geometrically speaking. Okay, so what Whitney's result does not give us is a way to compute these things. As a way to compute these things. So, I'm going to tell you a little bit about a way that we developed to compute them. So, this has been a problem that people have thought about for a while, starting, I think, in the 90s when people were able to do sort of algorithm, algebraic computers, and continuing for a while. But there hasn't actually been any algorithm proposed that. Algorithm proposed that could be implemented and actually could compute even the stratification of the Whitney umbrella prior to our work. So we of course now do have a software package that computes the stratification of the Whitney umbrella and much more complicated examples in reasonable amounts of time. So that's sort of where we're going. So I'll tell you a little bit about how we do it. So I guess the main tool is Tool is something called the conormal variety. Okay, so we're going to need a little bit of notation here. So x reg is going to be the set of all smooth points of the variety, so i.e. the manifold points in the variety. And the conormal variety is going to consist of for each point P in X reg, so each smooth point, then we have a well-defined tangent space. So we take the tangent space to X reg at that point P, and we want to. point P and we want to pair that point P with all hyperplanes that contain that tangent space and those hyperplanes are going to be represented by the normal vectors so we'll do pair each regular point with with all of these hyperplanes containing its tan space to x-ray okay so what is this actually going to capture well essentially what it's Well, essentially what it's going to capture is if we think of this, so this curve here is this X-shaped curve in R2. If we have a smooth point on the curve, like this red one, then the conormal variety is going to pair that smooth point with the tangent line. So nothing surprising there. If we have a singular point, so there's this closure operation. So we take the set, but then we close it. So we're going to add the singular points. So, we're going to add the singular points back. And when we add them back, we're actually going to add in the limiting tangents. So, at this singular point, this blue point, we're going to get the limiting tangent from this direction, this one going this way, but also the limiting tangent from this direction. So we'll get two things. So then, if we think of this projection as the, which will denote kappa x as the projection from the co-normal variety back to x, then the final. Back to x, then the fiber over the central singular point is going to be these two hyperplanes, whereas the fiber over the smooth point will just be one. Okay. And the reason that this is useful for us is that we can compute equations for it using a quite simple group of basis type calculation given equations for x. Okay, so then we're going to need one more. We're going to need one more computational tool. And I won't, I guess the main there's two things we want to sort of observe about this: is that it sort of has a geometric intuition and that it could be computed. So if we have a polynomial ideal, then this will define its zero set defines a variety. And there's a decomposition of this ideal, which you should sort of think of like a prime power factorization of an integer. So, you should think of these Q's sort of like a prime number to a power. So, we're just decomposing this ideal into these primary ideals, Q1 through QL. And then we could take their associated primes. So just think of the base of your prime number, the actual prime number without the exponent. And that will be these associated primes, these p i's. And the zero set of the ideal is decomposed into the unions of the zero sets of these primes. And each variety is sort of geometrically irreducible. So if we look at this picture here in the bottom corner, the variety is the union of this purple parabola and a teal line. And keel line. But it's defined by this ideal here. And that actually, its primary decomposition also includes an unnecessary, in some sense, geometric point, or it's not really a geometric point, but it's an, there's this extra, we've sort of covered the origin, you know, more than once in a silly way. So this kind of thing can happen and it sort of provides a bit extra insight, I guess, into the low. Insight, I guess, into the local structure is kind of the intuition as to why we want to keep track of these types of algebraic behaviors. Okay, so, but the main takeaway is that we want to decompose this ideal, and this is something we can even compute using Grober basis calculations. All right, so then now I'm going to tell you a criterion for checking if condition B holds. So we'll take So we'll take x to be a variety and y to be a subset of a singular locus. So that's just the variety with all of the smooth points removed. And actually the singular locus is also a variety. It's a theorem. So our goal is then to find all the points in the singular locus where condition B fails, essentially, or in some variety of the singular locus. So the theorem, let's break it down. So what do we do? Break it down. So, what do we do? Well, we take the ideal of our conomality that we talked about earlier, which was what we were using to sort of capture limiting behavior. And we look at essentially what we'll call the fiber ideal over y. So remember, y is contained in x and inside the singular locus. So we just add the equations of y to those of the conomal variety. And then we take that ideal and we compute its associated primes. And we compute its associated primes. So that'll be this list of prime ideals, sort of like a prime factorization. And we, for each of these, we look at the dimension of the projection back to the original space, so inside X. And those that project to something of dimension strictly less than Y, so we're looking at the fiber over Y. So those things that project to something strictly smaller dimensional, those will be our sort of back. Those will be our sort of bad points, and we'll or bad sets of points, bad varieties. So we'll take the union of all of those guys, all of those ones that project to smaller dimensional spaces, and then also the singular locus of this y. And that will give us exactly the points where condition B fails. And so then, in particular, if we remove those points, we get a pair that satisfies condition B. So this gives us a way to precisely. So, this gives us a way to precisely identify the bad points in a sort of recursive way. And we can compute it using Grogner basis calculations. Okay, so I'm just going to quickly summarize how the algorithm works. So we're given a list of equations defining a variety, say, of dimension k, and what we want to find is a Whitney stratification. So first, we compute the singular locus of x, which I said before it's a variety. It's precisely the variety defined by the, if our variety is in Cn, let's say, by the n minus k times n minus k minus of the Jacobian matrix of our given equations. And then we look at the fiber ideal for the same. For the singular locus relative to the clonomal variety of X, and look at its associated primes. So we use our criterion and we see which ones project to lower dimensional subvarieties inside Y. And then we take all of those together and we take the union of all those and we'll call that Y2. So now we've identified Y2 is essentially all of the bad points in the singular locus. In the singular locus. And then what we'll do is, and y and we can, we also show that y2 has dimensions strictly less than y1. And then you repeat this. And every time you do it, you drop dimensions. So you run out of dimensions eventually and you have a filtration as we wanted. Okay, so now let's go back to our starting point. So we wanted to globally understand kinematic maps between Between configuration space and state space. So now let's give an exact definition of a map stratification. So if we have two algebraic varieties, X and Y, and F is an algebraic map between them, then a stratification of F is a Whitney stratification of X and of Y, so that for every strata of S of X, there's some corresponding strata. There's some corresponding strata R of Y such that the restriction map of F restricted to the strata S is surjective and its derivative is also surjective. So it's a submersion, if you like, on this restriction. And a corollary is that if you have any two points in the same stratum n of y, then n of y, then the fibers over those two points are going to have the same topology, which by which I mean more precisely their hypology groups will be the same. Or in other words, this stratification makes F into a vibration if you know that word and are happy to hear it. Okay, so in our paper, we gave an algorithm that stratifies maps as well. As well. And the main ingredient is, in fact, the stratification of varieties. That's actually the hardest part. So the other stuff is kind of just messy, but not very difficult. Okay. So that was for complex varieties. So for, of course, for kinematics, we'd like to deal with real things. So for planar mechanisms, we can kind of maybe cheat and just get around it. And there's this construction. And there's this construction called isotropic coordinates where you represent x, y, and r2 with x plus iy and c, and this works more or less, but you end up introducing some unnecessary overhead, essentially. And in R3, of course, that's even worse. So currently, and hopefully it will be on the archive, I guess if we're really efficient, maybe next month, or if we're less efficient, maybe in August. Maybe in August. We have, I guess, a similar package of tools that will do stratifications of real varieties using also essentially group-de-basis calculations in quite a similar way to the complex case. And this is going to let us actually do the same sort of thing for real varieties and maps between them. Varieties and maps between them. So it'll be quite similar runtimes. So just to wrap up, so we've talked about an algorithm that can find a Whitney stratification of a variety and also I guess I've told you that you can use it to build an algorithm that stratifies maps and it is in the paper if you want to read about it or you can run the code if you don't want to read about it. Run the code if you don't want to read about it. And then, I guess the other takeaway is that in ongoing work funded by AFO SOR, so thank you to them, we are giving a similar algorithm to do this for real varieties, and we're hoping to have that available both in paper form and in code form in the next couple of months. So, yeah, thank you again for listening, and I'm happy to answer any questions. Happy to answer the questions. So, yeah, question. So I missed maybe the first slide or something. So if I missed something, tell me to get the notes. But these singular points that don't satisfy Whitney's condition, they strike me as being what. They strike me as being, you know, what you might think of as just being higher-order singularities. And so, I guess my question is: do you have some interesting pictures of just stick robots that what these singularities look like? I don't have good pictures for particular robots. And I think that's definitely like really delving into how it would look for particular kinematic systems would be very interesting. Systems would be very interesting, and I think that's on my to-do list after I finish sort of the code for the real varieties case. Okay, thanks. Right, so that's related to my question, right? So, most kinematic maps or mechanisms at least are mostly encounter not be polynomial, right? Unless you do have a procedure to have like use your results for the math we'll see if you model a know the math we see if you model a say you might always robot yes so i mean i guess the yeah i don't i don't know what um portion i mean of kinematic maps will be described by polynomials i think it would be you could extend it a little bit without too much difficulty to say like trigonic polynomials so if you want to include like sines and cosines and things like this that is fine you would just like you Would just like you just kind of call sine x, you know, a new variable x tilde or something, and you can basically do all this stuff and then unwind after you're done what the signs are and things like that. So you can do at least, yeah, I would say polynomials and trigonometrical functions, but things beyond that, I'm not sure. We seem to be working with assuming properties everywhere. Everywhere. You're coming? Yes. So we do need, I'm not sure that I actually, yes, so I'm not, I should have, if I was being really careful, said that the map, this map should be a proper map. In fact, actually, we can actually weaken that a little bit, at least in the algebraic setting. You can weaken it to something called dominant, which includes like Which includes like a lot of projection maps and things like this that wouldn't be included as proper maps. Okay, so what's the difference between your work and DNA and Yolonic one? So you already mentioned that you're your technology feature. Do you have any like computational numbers or quickly you're computing? I mean there are good stuff. Yeah, so people who like developed a lot of packages, Apple-based packages, to lose that. That would be interesting to see what you play into this. So yeah, I guess like Din and Jolenek, which, yeah, so I think I mentioned it maybe briefly on this. So they have a method that, like, they give an algorithm for computing Whitney stratifications as well. But But we've, I, yes, I made serious efforts to implement it, and I wasn't able to get it to stratify any. I mean, to some degree, the reason we wrote this paper is that I first looked at their paper and I tried to implement this code, their code, and I was not able to actually compute any stratifications. Well, it's about your programming skills. I have a co-author who implements the tools that work like Charm. And they work like charm. So, just like I'm trying to understand, you're claiming that the PM method is faster and better, and just trying to see where it's come from. So, you say that you know someone that implemented? Yeah, I mean, I have a paper. I mean, we have a paper, and you know, there is a computational component to it, and yeah, we are computing weakness classification as a part of like including points. Like, I'll focus on points in the PDC, which is sort of an extra piece of it. She's sort of an expertise, but yeah, it seems that again, like I never heard from again, I'm not quoting, but you know, my author is, and I never heard a word about difficulties here. I mean, if I'm working like, you know, like really reasonable. Yeah, I mean, I don't know. I would be happy to see someone like just because. Yeah, but I mean, like, just sort of philosophically speaking, I guess their work, their code runs in twice as many variables as ours does. So it's probably going to be slower, even just for like, you know, complex, basic complexity reasons. So do they have a code? Or they don't have a code, but like when you implement their code. But like when you implement their code, their algorithm has to run in two times as many variables. So that's that's a that's I think the main bottleneck. The other bottleneck is that they use some general position things. So theirs is actually not deterministic and you have to take like random combinations of things and this destroys sparsity. This destroys sparsity, which can make a lot of these, like, so a lot of the examples you're interested in are fairly sparse. And so, they, you should, you know, that is an advantage. It's philosophically easy, but you know, like when you make claims like that, it will be really interesting to see, you know, like people typically produce the tables comparison, you know, comparing the runtime. Okay, so I do have a table, and there is a table in the paper, but the table is literally just like actual. Actual times for our code and no times for the others. And I've included it in previous talks and had people complain that it looked silly to have a table that had really just timings for our code and took greater than eight hours for their code. Thank you. Any further questions? Let's send the speaker. Speaker, yeah. Yeah, yes, that's what we have closing discussion. Which is the boarding. I think it's not automatically with this video. Oh, I see.