Thank you. And I want to thank the organizers for not putting us in that room with the panoramic view. It would make it impossible to talk. And I want to apologize. In the last moment, I changed the subject of the talk. And instead of talking about logic, I'll talk about logic. Logic. I'll talk about analysis. So if you came here to hear about logic, then I apologize. Feel free to check your social media or watch YouTube. So I want to start simple. Maybe more concrete. So what's your new new title I think? So what's your new title? I didn't get it. The title is Distributions of Worlds in Unitarikoups. I want to start with something concrete. So SO3, okay, so take an element in SO3. An element in SO3 is a rotation in space. So what's a rotation in space? You choose a a direction, right? And you choose an angle and you just rotate around this direction at that given angle. Direction at that given angle. And if you take, okay, so these are the coordinates on the space of all rotations. Now, suppose you take a random rotation. Random rotation, so random I mean how? Random? Okay, if you look at something like this, then the axis, then the direction is uniformly distributed on the sphere, right? There is no preferred directions in space. In space. But the angles, but the angle of rotation is not uniformly distributed. And for example, it is much more likely that if you randomize a rotation, then it is much more likely that its angle is between, let's say, 179 degrees and 180 degrees than it is for this rota the angle of this rotation to be between zero and one degrees. 0 and 1 degrees. And maybe the easiest way to see this is to note that the collection of all rotations with angle 180 degrees is a two-dimensional sphere, right? Just need to pick direction for this. Whereas so being epsilon close to such a thing is Such a thing is something with probability one over epsilon. So probability epsilon. Whereas the collection of all rotations with angle zero is just a single point. The epsilon close to a single point is, so SO3 is three-dimensional, is something that of the order of epsilon q. Let me say this in a Let me say this in a different yeah. Okay, so this is, you know, this is a fact of life. Now let's do something else. Randomize your rotation and look at the square. You'll get a random matrix, a random event in random rotation, but this is a different distribution. It's not longer half. And in fact, if now you look In fact, if now you look, so the axis again is uniformly distributed, but if you look at the angle, now it is distributed uniformly. Angles between 0 and 180 degrees. That's a computation you can do. So think about it. What does it mean? Let's change perspective. Think that the how measure is what we are used to. Now we have this new measure, okay? Now we have this new measure, the square of random element. Distribution is somehow singular. It gives huge, it gives unexpectedly big probability to landing very close to the identity. So let me say differently: if you look at the distribution of random, of square of random matrix, square of random rotation, then it's Then its density with respect to the Havel measure, it has a density, but don't equally derivative if you want to be technical. But this density goes to infinity as you approach the identity. It gives unexpectedly high probability to things that are, to rotations that are close to identity. And you can okay, any question? Okay, any question? Now there's nothing special about squares, right? You can talk about the distribution of random cube or the distribution of random commutator, right? Or more generally, distribution of a world. Okay, so a world for me is just an element of a free group. Okay, we call it a world because it is just a concatenation of letters. And if W is a world, let's say commutator, you can get a distribution on SO3. What do you do? Suppose your world is in two variables. So you randomize two. Hub. Independent. Independent rotations, you plug them into the world, and then you get a random element. Okay, but so I'll denote this distribution by mu w. So, mu w is the distribution of a random. That would be fine. Okay, so again, you just think about commutator. You randomize two matrices, take the commutator, you get some distribution. What can you tell about this distribution? Let me give you a few examples. Still tail. Okay, so two weeks ago I gave a seminar talk. In the morning I talked to my kids and asked them what are their favorite quotes. So my youngest said, poop. My middle child said, Minecraft. Uh Mill Charles said Minecraft so if you if you don't have kids at the correct age Minecraft is a computer kid and my oldest kid said that his favorite word was hippopotamon stoses qui the darophobia Which I didn't know this word for the for the non-native speakers this means fear of long words So let's take these okay let's start with Minecraft so If you look at the distribution, what does it mean, this distribution again? So this is a word with nine letters, all different. So what you should do is you randomize nine half-random rotations and then multiply them together. But if you do this, it's clear that what you get is just one half-random. This one is similar. So it's not that every word, every letter appears once, but say Q, etc. Q appears only once. So even if I condition on all other letters, then when I randomize Q, I get the half measure. So it's also half. But but this world is interesting, actually. Okay, let's I don't continue. Let's call the distribution mu here. Okay? Again, like the case of squares, this distribution has singularity, the identity. It gives the density of this measure with respect to the Howard measure goes to infinity. With respect to the Howell measure goes to infinity as you approach one. But actually, it goes lower than square. Square goes quadratically, this goes linearly, one will the distance. But in all other places, the density is well defined. It's finite. For example, so here is one computation that you can do. Is one computation that you can do. You can look at the density. I write it as the Ladonic derivative. That's the work. That's the local density. At 180 degree rotation, it doesn't matter which one, right? Because they're all conjugate. And this turns out to be pi over 4, slightly less than 1. So again, it's still singular identity. One thing that you can do when you have a measure that's singular is to take its convolution, square. If you have two measures, two singular measures, and you convolve them, the result is usually less singular than either of those measures. So you can try to look at So you can try to look at the convolution with itself. And if you don't like this notion of convolution, this is actually a world distribution of a different world. What does it mean to take convolution of w with itself, mu w with itself? W with itself, it means randomize elements and plug into W, then randomize a whole different set of elements, plug them into W, and then multiply the result. So instead, I can just take the word W and then take another copy of the word W and replace all letters with completely different letters. Okay? And then just concatenate these two words. These two words. Okay, so in this case, you just need to take this word and replace the letter. So this convolution is the distribution of another world. Okay, so you can take something like this. Okay. And by the way, this is that's a Hebrew joke, not I don't mean any disrespect to the Swedish pop band. Anyway, so you can look at the density of this measure at one. Now, again, it's less singular, and now it does have density, thumbs per regular. Very label. And this turns out to be a very interesting number. This is phi squared over 8. But actually, it's an interesting number, and this is what comes into this computation. It's the volume of an important space. It's the volume of the moduli space. The modular space of SO3 local systems on a Riemann surface of genus 2. Okay, whatever this means, there is a space parameterizing something called local systems. Something called local systems on this space. It's a vector bundle. It's a three-dimensional vector bundle. And Athia and Bot in Goldman defined a symplectic structure on the space. It turns out to be a symplectic manifold. And from a symplectic structure, And from a symplectic structure, you can get a volume form based and you can integrate this volume form, get a measure. Okay, and that turns out to be this volume. Okay, now I want to slightly change perspective, instead of talking about fixing the group and talking about. The group and talking about different worlds, I want to reverse it. I want to fix the world and change the group. So I'm going to explain why this happens, the sim the I can explain. I will explain it. I didn't plan to I will explain it, but I want to say something. else before the fact that mu times mu is non-singular identity, this is called for every word to measure. So that's part of depending on the generality. So for SO3 the answer is yes. So if you convolve enough times, I didn't say... Not necessarily. Not necessarily two, yes. But you need to convolve it enough. Yes, but you need to control it enough. So I'm going to consider so fixed W consider The distribution of w on u d, okay, for every d I get some distribution and interesting asymptotic results. Let me say a few words about the history. So you can choose the trivia you can choose the world X. You choose the word X, okay? And then, again, you randomize a random unitary matrix, how random unitary matrix, what can you say about this? Okay, that's this whole subject of random matrices. Okay, that's a big subject in the, it started in the 50s with big nail, but it's a big But it's a big subject, skill kicking, in modern poetry. Maybe more connected to what I want to say is a work of Voykulescu from the late 80s. Voykulescu showed the following: he showed that if you take any non-trivial world, so non-identity, Trivial world, so non-identity. And if you look at the x, if you look at so you randomize x1 up to xr, so xi are distributed according to the hard measure of mu d. And they are independent, they are i. You look at this random variable, random matrix, and you look at its trace, and then you take the x. And then you take the expectation. And Verkulescu showed that this is, so for every d, you get a number. So this is a function of d. And the claim is that as d goes to infinity, this tends to zero. Again, take the commutator. Take the commutator. You look at the random, you take dverbic, you pick two independent random matrices, you compute their commutator, compute the trace. That's a random variable. A priori, it could get, if these two matrices happen to commute, the trace would be D. But in expectation, the trace is very small. Word is trivial, for example? If the world is. If the world is every non-trivial non-trivial world. So that will be functional. And this was several works here. This is related to something called free probability. But let me skip some history and go to a beautiful paper of Michael McGee and Doran Puddle. And they showed, so first of all they remarked, it's not the main part of their paper, but it's quite that this expectation expectation trace of w x1 up to xl. Okay, again, this is a function of d, and they showed that this is a rational function of d. Okay, so this number satisfy some terms. So here I think t needs to be big enough for x volt, depending on the world. Then... When you say rational, you mean also with rational coefficients or not? With integral coefficients, yes. What? With rational coefficients, yes. But again, it means that finitely Finitely and finitely many D's determine the list. And what they actually did later is, this is a wonderful thing. This is, you should all read this alphabet. They expanded this function as a Taylor series at infinity, so in power series of t to the minus one. It goes to zero. It goes to zero, so it looks like a1 b to the plus one plus a two b to the minus two, etc. Okay? And they found combinatorial meaning to each one of those AIs. Found meaning. So they count something discommunicable to the a To the apex. Okay, and more than this, they count something that comes from low-dimensional topology. Somehow surfaces emerge when you are asking about, you know, when you're integrating, you're computing integrators over unique telegroups. These coefficients depend on W. On the road, yes, yes. They count something that depends on W. Of course. For example, maybe the first one of those that's not zero, okay, is related to the stable commutator length. Stable commutator length of double more or less how many commutators are needed in order to add W. Okay, so but but but but the the the most systematic way to study systematic way to study so these distributions are always conjugation invariant and the systematic way to study conjugation invariant measures is to consider the Fourier coefficients so I'm not going to find I I'm going to define that's not a standard definition if If he is a capital, he is a good one is a capital of capital of Here's a character of uh is of UB. Okay, I'm going to be interested in the Fourier coefficient. Okay, but for me that I'll save you the idea of the Fourier coefficient, the corresponding Fourier coefficient of mu, mu, w. Of mu. W is just this expectation of C of W upstone up to x. For example, you can take the, you know, U D has a Berstead representation. The representation, U D is matrices, D by D matrices. They act on a D-dimensional space. And the representation, and the colour. And the representation, and the character there is just a trace. So, this would be an example of a Fourier coefficient. That would be the simplest Fourier coefficient to consider. Okay, and we are interested in these things. Okay, so let me let me put forth a conjecture. The conjecture says that for every world, for every non-developed world, there exists an epsilon that depends on the world. That for every D and every character, irreducible character of U D, this Fourier coefficient The most it can be the most this function can be is chi of one so a non-trivial bound would be a bound of this and And maybe I'll also say our result. So this is joint with Thai Glazer. It's now at Northwestern. The conjecture is true for the standard. For the standard for the fundamental representation. So what are the fundamental representations? Let me remind you. Maybe this is a complicated way to say the following. The following ID for each B, each K less than D, look at the expectation of the trace of the wedge power, the K wedge power of this one. Okay. Okay, this is less. The dimension of this presentation is d choose k one minus epsilon. Again, in an even more down-to-earth term, just to remark that this is the k coefficient of the this is the kth coefficient of the calculation. Of the characteristic polynomial of W X1 up to X1. Okay, so we bound the Fourier coefficients of random W values. Okay, so a few remarks. What they just erased What I just erased, this theorem for Iculescu, etc., is a much better bound for the case where P is the standard representation. Gives you not only that, d epsilon equals to one here. Now, the same methods, the exact same The same methods, the exact same proof will actually show you not just for the not just for the standard representation, but for other low-dimension representations as well. Now, what do I mean by low-dimensions? So, you fix a sequence of representations for each D. You take a sequence of representations, okay? And the minimal representation has dimension D. has dimension d. The next representation has dimension something like d squared. And I'm saying that as long as the dimension of the representation that you take in each d grows less than a polynomial, it's polynomially bounded, then these methods actually give you, then these methods show you this conjecture, with epsilon equals to one. epsilon equals to one. Okay, the new thing in this theorem is that we tackle representations that can have exponential dimension. Dimensions exponentially empty. Any questions? Let me Let me maybe let me answer. Maybe let me say, why is it important? What does such thing give you? Such a side give you. The first thing is that, so these bounds are rather weak. What we would like to have is at least that the decay of the Fourier coefficients go to zero. Like epsilon bigger than one. But what we say is that bad or big Is the bed or peak for the band for W if good for a bound for convolution powers of W. Okay, and the reason is that if you take in commutative, for commutative groups, if you take the convolution of two measures, the Fourier coefficients multiply. In the non-commutative group, something even better happens. The Fourier coefficients multiply, and then you need to divide by the degree of the character. Character. Okay, so it actually helps you to make things more, you know, to make the Fourier coefficient even smaller. Okay, so such thing give good Fourier balance decay, even exponential decay, for W convolved with itself several times. Okay, so this means that you do have good Fourier bounds. Good Fourier bounds for such words. They are not general words, they are words that look like I take a given word on one part of the alphabet and then the same word on a totally disjoint part of the alphabet and so on. Now, why do we want Fourier bound toilet? Maybe I should say that. What kind of questions can you ask about the distribution of? About the distribution of random commutator. You can ask things, so, about distribution, about any distribution on the unipary rule. It's conjugation invariant. So, what really matters is what are the eigenvalues. How do the eigenvalues distribute? Okay, so you may want to ask: must we have eigenvalues in, so the eigenvalues are. In so the eigenvalues are on the circle. Must we have an eigenvalue on the upper half circle? Can it be that all the eigenvalues are on the lower half circle? Or how unlikely it is. And such global such global properties of the spectrum. Of the spectrum they are controlled by the low degree Fourier coefficients. Okay, so if we know the trait the expectation of the trace and the The expectation of the trace and the expectation of the trace of the square and the expectation of the trace of the cube, I will know to answer such things. But we are actually more ambitious. What we want to do is to zoom in on some piece of the spectrum, on some very small interval inside the unit circle, and ask what happens inside this unit interval? Do I see just a random collection of points? Do this point tend Of points, do these points tend to distance themselves? So local properties of the spectrum are dictated by the high degree for your coefficients. It's very s it's not just analog, but Not just analogs, but maybe it's part of the same thing. Physicists, when they want to probe on smaller and smaller scales, they need higher and higher energies, higher and higher frequencies. The same thing happens here. If you want to grow this spectrum, you need to look at the higher and higher Fourier coefficients. What do you mean by local probability? For example, so I'm looking, so I have S, so I have UD, these, these, I don't know, Google or something. Okay, I look, so when I randomize elements and plug it into the world, I get a random matrix. I can plot the eigenvalues. I get a random process of I get a random subset of Random subset of the circle. Now I can look at an interval of length, let's say 1 over d, 1 over, maybe 2 over d. On expectation, I'm going to have two points. And then I can normalize. So I'm multiplying everything by d, and I have a process on a fixed interval of two points. Okay, of two points. And I can ask: are these two points just random points that they put here? For half matrices, they are not. For half matrices, they tend not to be too close together. They try to have distance. This is not a post-on-possess. We would like to have a similar understanding of. Of the eigenvalues of random project. And you say that this depends on, when you say I degree, even representations. Yes. Not just the representations whose dimension is polynomial in D.C. Because if you want to, so maybe why does the global properties depend on the low degree? How do you understand? You find some polynomial, find a low degree polynomial. I mean, at least I try to think to myself that the first claim is in some sense because of the Pluto-O-Machi theorem that Oh, no, no, I'm saying. No, no, you're right. So, if I have good bounds on the load and glyphourier coefficients, I can say something meaningful about the global properties. Because it's rational function, surely. No, no, no, no, no, no, because it's. No, no, no. The Puddle-McGee or Voiculescu gave me the input that I have good control on the low-degree Fourier coefficients. I'm saying that. I'm saying that in order to understand the standard way to understand global properties of the spectrum, to say that, you know, with probability 99%, if you look at this quarter circle, with probability 99%, there will be a point in the spectrum here. The way you, the standard way to understand this, is you can rephrase it as a question about Raise it as a question about the trace of the matrix, the trace of the random matrix, the trace of the square of the random matrix, and the trace of the cube of the random matrix. And these are determined themselves by low-degree representations. But if you want to shrink this interval, you want to look at an interval, you know, if you look at at the very small interval, you will need higher and hi-h. You will need to look at higher and higher powers of the trace of higher and higher powers of your material. So it's not only IO powers. No, no, no, no, no. You mean also I also can. So now uh and so for the low degree Fourier coefficients we do have uh good bounds, but for the high degree Fourier coefficients we do not. The Greek way coefficients, we do not. So, I'm running out of time. I'm just going to explain how what goes behind this, but maybe I will just answer Alex's question. Okay? Question. Okay, so let's look at this world. Now, if I look at a world and a conjugate of this world, I will get the same distribution. This is the same distribution as xxyy. Right? It's a cyclic quotation, which is just the convolution of this measure with itself. Now, so if I want to compute the Fourier coefficient, so if I have a representation and I want to understand this measure, I want to compute its Fourier coefficients. Compute its Fourier coefficients. So the expectation of heat of x squared, this is the sure indicator of freedom. This is the sure indicator. Okay, and it's it's minus one, zero, or one. So at that Zero or one. So I got lucky that this world turned out to involve only squares. That's a show indicator, but we are in SO3. SO3 also indicators are one. Now you look at so what does it mean? It means that mu x squared Squared. So as a distribution, so I'm taking inverse Fourier, inverse potential error. So this density is equal to the sum of, so the density at x as a distribution equals to the sum of one times three of x. And the density of the and now if I'm looking at the convolution, as I said, what happens, this is one, this is what happens to Fourier coefficients when you convolve is that they multiply and then you get to divide by the degree. So this is the sum. So this is the sum of chi of x divided by chi of 1. So let me, I have a minute, so let me talk about the first number that I wrote. So if you look at this mu of x squared, y squared of lambda at 180 degree rotation. That's a sum. So, what are the representations of SO3? The representation of SO3 are indexed by the odd numbers. So, m equals 0 to infinity. The degree is 1 over 2k plus 1. And the value of the character of Of at a 180-degree representation depends on whether k is odd or even. Okay, so it's minus one to the k. Okay, now maybe just one more to answer Alex's question. For SO3, the distribution of X33. Distribution of x, y, y, x is the same as the distribution. It's actually a mirror, but it holds for SO3 is equal to the distribution of the commutator. Okay? And that's because all the representations of SO3 have show indices that R1. Show indices that are one. It doesn't have show index that is zero. The Fourier coefficients, the Fourier coefficients does it have to do with the fact that these are one is the fundamental group of the toes and one is the non-orientable toes exclusively. No, because it doesn't happen for U3, for example. Other words. Other worlds. Yes, i i it that it will happen for S O oh S O four or S O five. No, no, but if you take if you take the the relation between oriental surface and genus G and unorientable surface of G. Are you going to get this the same? I'm going to get the odd values. So so so so this pi squared over eight are going to be even values of the zeta function. Values of the zeta function, right? They are related to the eigenfunction. If I'm taking an odd representation of an odd, of a non-orientable surface, I'll get even values. Did I answer the question? The Fourier coefficient. If we are coefficient of x, y is dw if we are coefficient, a tree is going to be 1 over 3 of 1, I think. Okay? And if you take a similar wall, x squared, x y, z w it's going to be one way. But if you take it x squared, y square, z square, w square, then it's going to be the zero. This is what's written here, right? But if we take the second convolution, it's going to be this. If we take the fourth convolution, it's going to be this convergence and this all. But again, this is sp this will not be true for. So this will not be true for representation if your group has representations that are complex. Okay, thank you very much. Some quick questions to the speaker. You started out SO3 and then you switched to the unit review. So does this mean to send us a message that the results are the same or the message? The message that the results are the same, or the methods are the same, or? Yes, the methods are the same, just yeah, the unitary group. So, let me say, we did not prove the theorem. We proved the theorem, we proved our theorem for the unitary group. I don't see any difficulty in proving the same for SOD. For example, we just the combinatorics is more complicated. All questions for you. Well I think we'll postpone the next talk to five minutes to make a short break and we'll resume in 1125 five minutes.