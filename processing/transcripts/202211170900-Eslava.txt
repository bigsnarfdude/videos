be together this this week and um well yeah let's talk about uh recursive twist so recursive twist was the topic for my phd project and recently i have been working with bass and myself um on extensions to wavered recursive trees so um wavered recursive trees the way um what we have done so far is still kind of very Is still kind of very divided into the classes of weight distributions that we can work with. And it also comes to a point where it's very technical. So I decided to shift back to Kingsman's Coalition, which some people will recognize in this audience. That is the construction of Union Fine Trees. So I will stay with combinatorial intuition. Combinatorial intuition, and I will tell you about a Poisson point process, which I think is the most general, nice result about high degree revertices of recursive trees. And yeah, I will tell you that intuition for the random recursive tree, classical uniform tree on increasing root label tree, and then I will point out where the recent advances are. The screen is detected. Okay. So just because that's kind of weird. Just to put everyone on the same page, we have this rooted label trees. That means that, and they are going to be increasing. So first label is one, it's the root. All edges are directed towards the root, not as Robin was saying. Not uh struggling was say, you know, suggested that the uh it just went down. For me, I just go up. Um, and the depth and the well, the height of the tree is shows how many the largest depth of a given vertex. And the depth, for example, of vertex 6 is 1. It is at distance 1 from the root, and it has zero children. So, in particular, the last vertex that arrives in Vertex that arrives into our recursive trees will have degree zero. Vertices are labeled by their time of arrival. And yeah, I think we are in a nice crowd. You all know all that notation. So now, weight of recursive trees, you have to have, well in our model, IID weights, any distribution you want, you take identical independent copies of that. Identical independent copies of that, and you will assign each weight to a vertex. And then you do the recursive process. Once you have a tree, what you are going to do is you will introduce the next vertex and you will connect to a vertex J with a probability. And I will tell you what is this probability. It's proportional to its weight. So whatever that means, that means you have to sum all the current weights that you have. Current weights that you have, and then you pick the vertex depending on that weight. This is going to make the probabilities inhomogeneous, of course, but then it's going to be independent from the current tree structure. And this makes it different from preferential attachment, because in preferential attachment, what you want to do is you want to look at the degree of vertex shape, and depending on the degree of vertex shape, you make a decision of how likely it is to connect to that vertex. To connect to that vertex. And for preferential attachment models, that are also kind of their own thing, they have this saying of the rich get richer. In the sense that the larger your degree is, the more likely it is that you will connect to that vertex. And in this case, weighted random recourse of trees are slightly different. There are going to be inhomogeneities, but still we don't care about the current structure of the graph. Current structure of the graph. Because we are not going to make choices depending on the degree of the vertices. So that's kind of puts it easier. Should I imagine that you just generate this infinite string of weights in advance? Yeah, yeah, yeah, yeah. It's like in probability, you always have a box of random variables for you to click on. Yeah, if you are if you are going to attach If you are going to attach vertex n, you only need information on the previous waves that you have seen so far. The type of difficult, I mean, what is nice to know about random recursive trees is what is the domain of traction of the weights. Depending on the domain of traction of the weights, we will have different behaviors. If you want more information, we can talk. Well so is it right to say that uh if you fix any particular vertex, as long as it exists in the in the tree, then the probability that a new vertex would join it is decreasing in time? Yes. Yes, yes, exactly. Because the weights are going to be fixed. Is this connected to this fact that you said earlier that one need not be the maximum? That sticks around long enough that breaks. Exactly, yeah. So. Exactly. Yeah. So you can see it you can see it very clearly from the recursive tree and somehow hidden in the computations on the weighted random recursive tree paper, you can see that it is kind of a pool between being born early so that you have enough time to collect degree, but also you have to have a high weight, which is going to come up at some point in the generalizations when I talk about the recent advances. Talk about the recent advances. So, here I have a simulation of a random reversive tree. You keep going. Yeah, that's it. And then, well, this is going to be my idea for the whole talk. The probability of joining to a vertex J is going to be uniform. And that means just the particular case where all weights were deterministically one. Okay, so that's the game of the day. I'm adding a new vertex and I will. I'm adding a new vertex and I will decide uniformly at random where to place this new vertex. Okay? So now this is 300 vertices in a random recursive tree. It's more sparse than what Heronimo was showing us yesterday, because the maximum degree, as I have mentioned on Monday, is going to be of order log n. So it's very sparse. The size of the vertices refer to the degree of each of the vertices. Refer to the degree of each of the vertices, and the color refers to the time of arrival. So, green vertices are the ones that arrived at the last moment, and the dark blue were the first ones. And of course, please interrupt me whenever you want. I'm here to solve all the questions. Okay, thank you, plenty. Okay, so I will give you three methods of studying random recursive trees. Random recursive trees. These are not exhaustive. Even these references to the insertion that are not exhaustive, but I like these papers. One by look was using records of permutations. And the correspondence, the way he obtains this method is super nice. You can go and check that paper. And I want to give you an intuition with renewal here. So think of, and this question was very much from the conversion. This question was very much from the computer science community. What is the insertion depth of the last vertex? So I will attach vertex n to some vertex. What is the distance to the root? And for that, you have to select a uniform random vertex that will give you this one. And then depending on this label, you will pick a uniform random vertex with a label smaller than what you just saw. And if you put this chain of decisions into a Chain of decisions into a formula, you will get whatever you have of label at time i, you multiply by a uniform random variable and you take ceiling, say, because you have to pick an integer. But if you put aside the fact that we are taking these rounding values, then the last vertex, the vertex that I want to have label one, that indicates me that I arrived to the root. Indicates me that I arrived at the root will be more or less n times e to the sum of random variables, which are IID. And then you want to fix k so that all this label is going to be 1. And this is the flavor of remote theory. So I'm not giving details today, I'm just giving intuition. So if you have questions about the intuition, people that have worked with recursive trees know things, but if it is the first time, it's kind of If it is the first time, it's kind of nice to know these strategies are there. So that was the insertion depth. And the insertion depth means I'm going to take the last vertex, I attach to the tree, and I look at the depth. But I might as well say that this is the depth of a uniformly chosen vertex. Probably, because n attaches to a uniformly chosen vertex integer. Uniformly chosen vertex integer. Another idea, the degree sequence. So again, I will draw back your memory to Heronimo's talk. For him, the degree sequence was fixed. But for me, the construction of the tree is random, and I want to know what happens to the degree sequence. So I want to know what happens to the number of vertices with a given degree. Okay? So now I really like to see. Now, I really like the structure of trees, but I will forget about the structure. So, imagine a bunch of grapes, and now you plug them out and you put it into baskets. And you will put them into baskets depending on the degree of the vertex. So, the dynamics of constructing recursive trees is very simple. Every time a vertex arrives, it has to go into the first basket because it has no degree. And then it slowly jumps one basket forward because every time somebody Because every time somebody selects it to be its parent, then it increases its degree. So it's kind of little bunnies jumping from basket to basket. And then I forgot about the structure of the tree. I just want to count the number of vertices with a given degree. That's the only information I need. And this theorem appears in the first, in the, well, where they define random course, they care. Define random because it is they get this result on the expected value of this number, and it's through a different sequence analysis. So, what has happened? Like if you want to count how many vertices there are of a given degree D at time n, well it's the same as before, except that you can have gained one if somebody from degree D1 was chosen to be the next parent, or you can lose one. You can lose one if vertex n picked a vertex of degree d, and then this vertex now has degree d1, d plus 1, and then you shouldn't be counting it anymore. Okay, so this very basic dynamic, you just keep adding vertices. The first difference equations analysis gives you the expectation. But if you go for the poly R schemes, that will give you asymptotic normality. Will give you asymptotic normality. And in general, asymptotic normality for us probabilists is a nice idea. Everything looks beautiful, we are comfortable, we know what is going on. We have an explicit covariance matrix. And this result by Svante is based on a more general result on poly orange with different number of colors. And the arms, what it means is that what I have my number of vertices divided classified. Number of vertices divided, classified by baskets, but these are going to be colors. And then I pick one at random, and depending on the color of the vertex, I will do something with the arm. This is the arm scheme. And my scheme says I will add a vertex of color zero. And this one I picked, I will change its color and place it back. And depending on the ARN scheme that can be encoded into a matrix, then you can know what is the asymptotic distribution. Asymptotic distribution of the proportion of the colors of the baskets. And then you can see that this says that if I want to look at the random variable that counts as a number of vertices of a given degree, I can do it for all d. So I have an infinite sequence of random variables that I know how they converge. But the fact that I have an infinite number of variables is a little bit tricky because in reality what we are doing is Because in reality, what we are doing is you tell me a large p that you want to imagine. And then I take this as a finite number of colors. And then I say something about the joint distribution of these finitely many random variables. Ah, this is not enough for your analysis. Then take larger D, consider a larger number of colors, then I will give you the joint convergence of this finite number of random variables. Random variables. So, how do you fix your basket impression of this process? Is at some point you will make a very large basket, which I was calling with Alessandra, the color I don't care. So, the moment my degree, my vertex has a degree larger than D, whenever I pick it, I say, okay, well, you get one more, but I don't care. You are very rich. One more. And that allows me to make a And that allows me to make a finite analysis to say something of an infinite degree sequence. So, good? Really like the best question. You have this convergence? You're taking D as fixed compared to N? Exactly. This is what I mean. Yeah, my asymptotics are going to be as the size of the tree grows, as N tends to infinity, but actually this D is fixed, which is going to be a key point in a few slides. What if you reverse this and you fix n gonna be ah well no but that's uh when you have n vertices the maximum degree will be at most n minus 1. Like imagine all vertices decides to join to the root zone. And this is not going to be very much likely because the maximum degree is going to be of order log n. We will see. Wait a second. So let me summarize Wait a second. So let me summarize what I know for well what we know with classical methods about a uniformly chosen vertex on a tree with n vertices. Well, asymptotically, the depth is going to be more or less normal with mean log n variance and the decree is going to decay geometrically. And now people interested in real-world networks will say random recursive trees are no use because we want something Are no use because we want something that decays as a power law. And this only decays geometrically. Anyway, so okay, my trees are not good for the real world. But here's the spoiler for Kingsman's coalescent. And this is going to be, the explanation of this one is going to be the last one in the talk. We have very much as in the combinatory, analytic, enumerative combinatorics community, they say we have closed formulas. Community, they say we have closed formulas, we have the number for this. Now I want to tell you: if you fix n, I have the precise law of the depth and the degree of a uniformly chosen vertex, which is nice. After that, I just take the asymptotic as n tends to infinity and I go back to probability. I do convergence of distributions. And okay, so if the height number to be a binomial random variable, would Random variables with one parameter that is going to be random. But this parameter, trust me, is going to be concentrated around two log n. So if you take two log n flip points that have success probability a half, then in expectation you have log n depth. So at least it's consistent with what we know already. That's nice. And the degree was geometrically distributed. Distributed approximately because I have to truncate it by a random variable. But this random variable goes up to 2 log n and I was trying to convince you that the maximum degree is going to be even smaller than that. It's of order log n, but the constant is way, way lower. So we have plenty of room to play around these things. More information about Kinsman's palace at the end of the talk. At the end of the day. So, what about maximum degree? Maximum degree is order log raise to n. If you want to do the computations, this is larger than log n, smaller than 2 log n. Look has a nice result that gives us almost sure convergence for this ratio. And, well, why is it too log n? I'll give you a. It tool of n, I'll give you a heuristic that is very kind of tricky heuristic because it relies on a result that we know about, on an approximation that we know for t fixed. The number of vertices of degree t is more or less, well, the proportion of vertices with degree t, it decays geometrically. So, how let me see, I'm being confused. Yeah, I have m vertices. I have n vertices. This is the probability of one of them having degree D, and I want to go up to the range in which I will have finitely many of them. Right? Because I don't expect all vertices or a large proportion of vertices having maximum. I want this number to be more or less constant, and this happens when d is login. But I have done two things that are sketchy. The first thing is to make this approximation, and the second This approximation, and the second one is to think that this approximation can be done for a D that depends on N. So precisely, D had to be fixed, and I am basing my hunches and my heuristics on something that is not reliable. So again, Kinzmann's quality. For a very large range of D that will go beyond the range of the maximum degree, I will have uniform. Maximum degree, I will have uniform bounds for this approximation. So this approximation is actually right, but I'm hiding little things over there. But it's going to be, this little old term is going to be uniform for a very large range. So it is the right heuristic, it is the right invision. I am talking about the tail, so yeah, it cannot happen, but things work. Work okay. So let me tell you one last thing about classical methods that has to do with generating functions. So if you have questions about this problem, about this result, uh-huh. Is it possible to subtract from the left-hand side with a bigger equal to Um bigger equal to D minus one and obtain the D? Ah, like counting kind of the tails but on the other side? No, I mean exactly the degree D. Ah. Because you have the tail here? Yeah, I do have the tail. Because it's uniform about it. I can't tell you during coffee time why this one is better. But if you want to take equality here, then you just take you put a plus one here. And you have the same results. The same results. Yeah. I mean, okay, so we have the same approximations either for the tail or for the exact value D, but actually, it's this one is kind of stronger for the kind of conclusions we want to make. If you make uh what was the first yeah, if like we do the economy we said. Do the equality instead? If you make the quality here, what you have to change is the constant. Yeah, but like what about the parameter range for D? Ah, no, it's the same. It's the same. Yeah. Yeah, well, it's because actually D has to be at most C log N, where C is strictly larger than 2. So you have room for plus or minus 1 channels. Possibly to have this jointly with a T and D frame? Yes. We have to talk about that English. Okay, so if you have questions about this paper, more details can be given by Keith. I will explain what I know from analytical mechanics. Okay, so these are the, this means, this one means that the distribution The distribution of the maximum degree approximately because this one is a discrete random variable, but it has a gumball distribution, which means that once I take up what I expect its value to be, the tails doesn't need to be rescaled, and it has this double exponential approximation of the probability. So this probability is the number of increasing trees whose maximum degrees are most scaled. Maximum degree is at most k for appropriate k and then divided over n factorial. So I need to know what's going on with these positions. I put them into a generating function and when I differentiate, the intuition of doing derivatives is that you will delete a vertex that is very well kind of defined. And what very well defined vertex will have in a tree is the root. So when I delete the root and I know that this tree has max That this tree has maximum degree at most k. Then I get a forest with at most k trees, and all of them have degree, maximum degree at most k. And then you go through a very kind of detailed fine analysis, technical analysis, and they get the results. That was what was known before I started my PhD. And I had a motivation for studying high-degree vertices, which is still the motivation of my open program on Monday. I have basically very few clue of what is going on. This is not recursive trip at all. But imagine I will pick three vertices, the three vertices with highest degree, or the k vertices with highest degree. With highest degree. What is the structure of this tree? How are these marked vertices connected to each other? And what is the size of the subtrees hanging out from these high degree vertices? Well, this is the center vertex. How would that can you describe the structure of the tree? Because so far, the way in which we studied the grease sequences was having this bunch of brakes and place them into baskets. and place them into basket. I'm forgetting about the little branchy thing of the grapes. We don't know about the structure. So I want to know about the structure. This is still an open problem. I will tell you about the decreasing. So these are the results of my PhD in a nutshell. Next slide is what do I mean by a Poisson point process. The near maximum degrees is that the maximum degree is a random number. A random number. So I want to take a range of maximum degree minus, it could be like plus or minus a constant number, but it's a maximum. You only go back. I know the number of vertices with maximum degree, and I also know each third. If I only count the number of vertices that are high degree, I have central limits for uh for that. Tho those are going to be Gaussian. Those are going to be Gaussian. But there is something annoying that I have to take C larger than 1. And this, I think it is technical. But somebody help me. I think it is technical. We should be able to take it at least slightly slower. And if we want if okay, I will say one more thing here. This random variable would have to be renormalized this way. Renormalize this way. But you see, close, yeah, I have to have good balance. But I put only its expectation. So if there is something different for C is 1, then I don't think it should be. But then what's going on with this? I mean, how? I think it is technical, that's what I'm saying. We can tighten the distribution. Can tighten the distribution for the combo for the test of the maximum degree, meaning that when I was talking about this generating function result, this i again needed to be fixed. But with the Kingsman's coalition, we can make this i then to be dependent on n. Here we go. Okay, so now. Okay, so now, present advances. We have more or less the same picture if the weights were distributed by a bounded weight with an atom at the maximum value possible. Which is, again, the sort of in, I mean, as long as you have a positive chance of being the one that will be attracting vertices the most, if you have. If you have weight one, then you are kind of on top of the rest of the vertices. If you have a positive chance of being that vertex, then we can track you along your history and give more or less the same pictures of Poisson-Poe process, central limit theorems. For our paper, for our joint paper, there was a technical condition regarding the distribution of the weight. The distribution of the weight near zero, but Bass is a super marvelous person and he took away that condition. Okay. There are other results for weights in the Bible domain of attraction, in the fresh check domain of attraction. It's not pressure. If you have questions, ask me in coffee. Okay, ready to see. Ready to see up at some point. The good news and the surprising, just don't read it so well. You can do whatever you want. I will focus on the picture. And for the picture, a point process, on set, is what I have been doing, right? Like I will have my tree, I will take out the vertices and I will put them into branches in into baskets. Branches into baskets. But now, these baskets, I want to shift my attention into the range where the maximum degree is actually around, which is for every degree of the vertex, I will subtract log base 2 of n, and that is going to be the location of my vertex. And here, I will have a few of them, and suddenly I will have none of them, right? Because it's the range where the maximum is. And now, this is a point process. And now, this is a point process. I have a space, in this case, it is the integers, and I put points into this space. Point processes can also be marked, which means that when I take the vertex and I place it in the basket, I add a tag that tells me, that reminds me what was the height of the vertex. Vertex. And what is the depth I want to add? This vertex, I want to add the information about its depth. And for this to be converging, I need to re-normalize already the depth. The same way I am shifting the degree, I want also to have this weight encoded already renormalized. This is going to be important. So, have you seen? So have you seen point processes before? Good, good. If you want to have a a sense of what a point process is, imagine California and fires in the forest, right? So you have a space. You have a space and then here and there are going to be fires. And then if your metric space is the plane, then you have two tags. Then you have two tags when this happens and how big the fire was. So this is a point process because you want to pinpoint where what you want occurred and you have the task of saying location, intensity, whatever. So here my points are located somewhere in the line. It extends to both sides now. And my extra information is at that point. Okay, so Poisson point processes means that if I take independent, well, this is nice about Poisson point processes. If I take independent regions of the space, I will have independent random variables. And these are distributed Poisson. So somebody asked about Poisson random variables before. Poisson random variables happens when you have something very unlikely happening to somebody, but your population. But your population is large. So maybe I'm always fatalistic. Imagine calling to 911. All people in US Canada can call to 111. We are a lot of them. But it's extremely unlikely that we will have an emergency at any given time. So how many calls does 911 get? A quasi-number number of times. The moment this probability of something happening increases Of something happening increases, then you get to the range of convergence to Gaussian random variables. So you have all vertices which we don't understand why they have maximum degree. It's very unlikely that any given vertex will have maximum degree. But you have n of them. So at least some of them will have maximum degree. They are distributed as a Poisson random variables. It's kind of expected normal, asymptotic normal components. Asymptotic normal convergence. Yeah, yeah, everything's good. But actually, hidden in this random variable convergence is what it is surprising. Because the depth, I mentioned this on Monday, the depth, once a vertex arrives, the depth, you already know, that's fixed, that never changes. So how come the depth of a vertex has to be re-normalized by something that grows in n? In N. And the reason why is that when I have this bunch of grapes and a bunch of vertices and I put them into the basket, I am actually forgetting when they came into the tree. I am actually forgetting the labels of the tree. So I'm looking at the vertices which have the highest, the largest degree, but I don't track what is the label of them. And that means that there is kind of a race. That there is kind of a trace. What's the significance of alpha and how do you choose that? It's not that you can choose it, it's a given alpha. Depending on how. I can tell you over coffee. Yeah, yeah, yeah. You will see it. I can explain it once we know about Hughesman's quality. If you remind me. Yeah, yeah, that alpha we know, that's a number that is too old to be shown on our screen. Oldie to be shown on our screen. But then there are these vertices that are gaining degree, and if you look at the maximum and you wait a little bit, well, actually quite a long time, then this vertex is not going to be the maximum degree anymore. And I think of this race as being kind of for snails, because everything is going to happen very slowly. And I want to simulate that. Somebody uh voluntaries, please. Uh voluntaries, please. Uh okay. So, questions on the Poisson process other than the alpha? This this is what I'm trying. I'm going, I mean, this is the way I want to describe in its in its largest generality I can the maximum degree vertices. If you want more technical details, please ask me. But now I will tell you about the technique I used. The technique I used to work with these models. And this is the Kingsman's coalescent or union fine tree. And the Kingsman's coalescent is a way, it's going to be a way to reverse the process. So there are instructions on there. You can read them. I will show you in real time how to build it. It's easier. So, in general, for collection. So, in general, for coalescence, you will have individuals already there. So, if I want to construct a tree on n vertices, I will put all n vertices there. And I sequentially will have mergers. I pick two at random, I flip a coin, because these forests are rooted. So I have to decide if I put two subtrees together, I have to decide which one. Trees together, I have to decide which one is going to remain the root. To do that, I just do everything independent, uniformly, and I get my first edge. So now, I don't have timestamps for the vertices because all vertices were there since the beginning. I will have timestamps for the edges. I pick two at random, I select which is the one that is going to win, and I super. And I subordinate the other subject. And I keep doing it. I will do it n minus one times because I have what n minus one edges. And that's now for this last 10 minutes. This is the name of the game. You pick two uniformly chosen trees. You will flip a coin. One of them is going to be winning. The other one are going to be losing, subordinating to the other. To the other root until I finally have a tree. Okay, so now instead of adding sequentially one vertex with its edge, I have all vertices at the same time and I will be adding edges one by one. Because everything is independent, uniformly likely, the actual labels of the vertices. The actual labels of the vertices are not really important. The important information of this random process is placed on when in which order the edges were arrived at. Okay, so can you imagine, combinatorist community, what is the relation between random recursive trees and this coalescent, Kingsman's coalescent, Kingsman's collection. And do you see the Union Fine Tree? That's also. Do you recognize the Union Fine Tree? Because I'm not that... I just know this is the name for it, but I don't know exactly how it relates. Okay, so now. Usually they're not random. Usually their edges are chosen according to the weight. Yeah, yeah, do one-to-one ancestries. But this is kind of a fair uniform. I'm interested in knowing what happened. Luke was whenever there was a time whenever binary search trees, recursive trees were studied, they were studied kind of in the same paper with similar techniques, and they also studied union-fine trees in this way of constructing them. Yeah, so Kingsman's collection is a big thing in biology, in math biology, and it turns out that this is not the construction. And it turns out that this is not the construction of Kingsman's Valescent for them. So, yeah, just it's slightly off. They don't have coin flips. What they get is a binary tree. But these coin flips that breaks symmetry, they are important in our case. And the important thing here is to see that now the labels, the timestamps of the edges have certain structure. And the structure of these labels, of course. Of this time stamp of the edges is that they are decreasing along root to leaf paths. Can you see that? Because the first edge, at some point it arrives, and then it will subordinate when it loses to another fight. But this edge has to came later in time, so that means the label of the edge is going to be larger. It's going to be larger. So now we have the same idea of a recursive tree, only that repairs. And yeah, it's the kind of combinatorial correspondences that sometimes you just see it and sometimes you need to think about it. And it doesn't help. Let me try. Let me try and do it. Okay, so now what I'm saying is that for Kinsman's collection, Christmas collection, everything was so uniform and exchangeable that these labels are no use for me. What is useful is the labels of the edges. Okay, so let's look at the label of the edges. They are monotone, but in the reversed way, which means you have to switch every sentence. So this is this one that was the first. Was the first now it's going to be the last one. This has to be changed. You have five edges, you have to reverse it another one. So this one becomes six, this one becomes five, this one becomes four, this one becomes three, and two. And the root doesn't have an edge assigned to the root. In my random recursive tree, In my random recursive tree, it has label 1. It always has to have label 1. And then I transfer these labels of the edges down to the children. So yeah, when you have work ages with your precursor. The slides are up. Anyways, that means that actually studying King's Man's Colors in the way I define it is the same as. Defined it is the same as studying random recursive trees. And what happens is that because this label didn't matter, they were exchangeable, picking a uniformly random vertex in the tree is going to be the same as knowing what happened to vertex one, any any level, vertex one. So what happens to vertex one in this construction? One in this construction. So you have all n vertices, you pick one, you name it one, and then what happens to it? And well, you will be adding n minus one edges, you will have n minus one mergers, but you don't care about that. Many, you only care about the times at which the tree that contains vertex one is put into a fight with another tree. Because then you will flip a coin and something will happen to you. Will flip a coin and something will happen to that tree. Okay, so what happens is that either the tree containing vertex 1 wins the battle and then the root of that tree will gain debris or it loses the battle. If it loses the battle, then this tree containing one will be subordinated to another thing that I don't care. But the point is that all these tree, all the vertices. These three, all the vertices subordinated, gain depth by one. Fair enough. So this is the last simulation of the talk. This is vertex one. It wins a battle. I don't care what is below that, but I know that vertex one has degree one. It loses several battles. It wins several battles, now it has degree one, and suddenly it loses degree. And suddenly it loses. So the moment it loses, the degree is going to be fixed for vertex 1. It won't change ever, ever. But now it has depth 1. And then now the one that is fighting somehow is the root, the other root that I don't care what is that. It gains some battles. But that's concerning the new root that I don't care, the degree of. That I don't care, the degree of vertex 1 is fixed, and I have to pay attention only when the root loses, because that means in particular vertex 1 will increase its depth. And I will repeat this process how many times? Well, however many times I have in this selection set. So now I have S number of trials and Trials and it either happens, it's well, it's actually kind of tricky, right? Like you gain some degree until you don't, and after that you just stay there until your representative loses and then you gain death. So that's a slide. How do you summarize this? If you want to know what is the death of a given bird, Of a given vertex in King's Man's coalescent, which is a uniformly random choice in random recursive trick, what you need to know is how many times you try and how many times you lose. And this is just a binomial random bar. And if you want to know its degree, you need to know how many times you win when you were in your good moments. Forget about school. Uh-huh. And then this is only for one vertex. But the beauty of this is, um it's like uh well like in physics. Uh there are so many particles that two of them you can study them together and they are going to be relatively independent because there are so many. So we can study we can follow the dynamics of finitely many purposes in histance policy. And that will give us what you were asking. Can you look at What you were asking. Can you look at two vertices at the same time? And yeah, I can say, okay, now follow vertex one and vertex two. And they are not going to fight against each other until the very end of the process, in which most of their structure is already there that you can say, oh, after that, I don't care how they are delayed. The other thing with the maximum degree is if you want to think of a vertex that has Think of a vertex that has high degree, then you want this geometric to be large. But if this geometric is large, then here you have to subtract what you know. I mean, if you condition on the vertex up in large degree, or any degree for that matter, that means that the first point flips were successful. Then those are used because Those are used because you use them to guarantee degree, then the binomial is over a random number that is smaller. And how smaller this is is where this alpha comes from, which is kind of fun. Okay. Questions? The last uh recent advance is by uh this student, uh Nautus Doc Vass, is that hidden in Hidden in, well, this was some point process, it's very much like the point process that Nicholas was talking about in his talk. For convergence, for his weak convergence, he says, oh, look at test functions. And that is a way to prove convergence of point processes. But for me, the way to prove convergence of point processes is through the method of moments, finite-dimensional distributions. Dimensional distributions. So there are kind of very technical things hidden in there. And if you one while you are doing these technical things, you follow the label of the vertex. So now this is kind of the timestamp of the level of the vertex. Here remember, like vertex one is one is not the level of this vertex in the real recursive tree. So if you are able to track what would it be its To track what would it be with its label, then you can recover even more information. So, now what we know about a vertex with high degree, conditional on it having high degree, we know its step and we know its label in asymptotic statements. So, well, thank you very much. This is the summary of the last question. Um so on slide eleven you have those functions apply of V and apply of C, where So I was wondering, like, so at k prime of z meaning or the decomposition k equals zero meaning that the root vertex is by itself? Yeah, I think so. Let me see. Um Let me see. Yeah, I think this gives you, if j is equal to 0, that term is 1, which is always the one that you have in generating functions where you have only one flicker. Just the tree with only one vertex has maximum degree at least. So it doesn't have any true trees, just like process, looks like it's not. Polyphonic looks like it's not constant or intra. Ah, yeah, no, it's not going to be constant. Okay, yeah, that's a good point. The fact that every the number of vertices in every each of these baskets is Poisson and independent, it's one thing, but depending on the region you are looking on the basket, you will have a different mean for the Poisson variable. So it's very complicated, right? Yeah, actually. Yeah, actually, there were two times in which I stated that n is equal to a power of 2. This is only for the presentation, but it has a lattice effect. It's for the presentation because once you want to put it formally, there are some details that have to be taken into consideration. Oh, and sorry, another very small question. I forgot which page it is on, but you have a log of E on one of the slides. Oh my god, this is log of E. I remember the two subscribers. Okay. Yeah, sorry. This is when you do is to make decisions that later. This is going to be log base 2 because if you go at the log base 2 of n where the maximum is, because when you get close to the maximum, you need to have Poisson distributions. This construction of the given qualescence reminds me a lot of the me a lot of the construction, the coalescence construction of trees with a given degree sequence. So you give me your points, both with the half edges, and then choose the half edges and merge them uniformly. Then I thought we tried something about it at some point in some book conferences about following the size of the sub-tree growing, and it happened. But and it happened in the simulations that also that if you have you follow two guys, then at the very end, so you will have a lot of atoms, small trees, and at the very end, the size starts to grow and just. My question is if it is, and I started to think about if there exists, for can't do that, trees, uh, reversed trees with a given degree sequence? Um I don't know. Because um because normally for recursive trees you you have a way to construct them, then you then you wonder about each degree. So how, yeah. Like conditional trees, like conditional having some size, would you mean that you first think of all increasing trees and then reduce your space to Reduce your space to whoever has this degree sequence. And then you know, well, yeah, maybe it can be done. Actually, for these recent advances, the first approach that we tried was to define a weighted kinsman's quality. And you can do that. I mean, you can put down the formulas and have a weighted kinsman's quality that the output of the tree will have the distribution of a weighted random recursive. Of a quadratic random recursive tree. In principle, you can do that in the formulas, but actually, then all the benefits of King's Man's Palescent, because it is not going to be homogeneous, then you lose all the benefits. And then you might as well do the computations directly. And that's why it's become technical. So I think if we were to do that, you suggest, then it will be very easy. Very quickly will become very technical. Will become very technical. I think it's related to what Esvaldo was asking. Where the weights are kind of, instead of IIT, they're given to you. Ah, okay, okay. Yeah, so the question was if you had kind of worked along that line? No, our work uh relies on this way it's being identified. Relies on this way being identified. Actually, Vas and Marcel had a previous paper, and then we joined forces and did this, made this result, in which if they don't use the techniques that were used in my work, they need to make, I always forget the name, they need to assume you are given the weights, then you compute all probabilities, condition. Compute all probabilities, conditional on the weights. And then they have to average things. And our technique somehow goes around, and that allows us to get stronger results. I I was wondering, you only pointed to this at the end. Where is the max degree node? When does it arrive? Can you say something about this? It arrive, you say something about this. Yeah, maybe I can say that here. Remember that the insertion depth was asymptotically normal. So the insertion depth would have this form. So the fact that you are adding this factor means that the label could be N Could be n to the table? Would be n to the one minute Okay, it is still an asymptotic result so it's probably it's not really that you can pinpoint the the label of the vertex and it doesn't tell you anything about the structure per link. No, more or less what it is, but No, more or less where it is, but I would like I thought it's you see how throughout the presentation there was a difference between logen and logwes to log n and log with n it this alpha comes from this conversion yeah I can show you slowly on a blackboard outside um you're almost last eight ninety Uh you mentioned uh towards the end at the bottom that uh you prefer to use moments. Uh-huh. Is it for the proof or the strategy? No, for the proof. Well, it's not it's not that I prefer that this is a technique that works. So okay, very briefly, it's more or less what I explained with the baskets. Explained with the baskets. If you want to find weak convergence for this Poisson point process, what you do is you fix a finite number of baskets and you prove convergence of the joint distribution on the information of these finitely many baskets. And the joint distribution is going to be characterized by the moments, by the mixed moments of these random variables. Random variables. And now it's a little bit involved idea that these mixed moments is going to be equivalent to looking at finitely many vertices. So all these moment convergence boils down to be able to follow a hundred vertices or fifty vertices at the same time, and I have to know. At the same time, and I have to know what is the joint probability that they have degree larger than that and height in this interval due to the computations. And because they are going to be asymptotically independent, we will have formulas of this sort. We will have formulas of this sort where we can look for the formula. Where we can look for the probability that a vertex has so and so, I'm finishing in one sentence. This has a so-and-so characteristic. Its probability is going to be more or less this. Yeah, we can talk about it. I was wondering, of course, how many moments would you need? All of them. All of them. For the method of moments, you need all of them. Yeah, yeah. And again, you need all of them, but then you tell. Is you need all of them, but then you tell me which one moment you need. I tell you how many vertices you need to keep track of.