So the floor is yours. Hi everyone. So yeah today I'm going to talk about this work I've done together with these nice people. And well as an introduction, you all know that the presence of matter in the post-merged phase of a compact binary merger is responsible for the electromagnetic phenomena that is associated with gravitational wave emission. There are several compact binary There are several compound venerability systems, venerable holes, which don't have any atomic phenomena associated, also VNSs in which some spelled matter is expected at the bottom merger phase, and also Newton star black hole mergers, which, well, these are more interesting events because it will depend the presence of neutron star matter after the merger will depend on the equation of the state of the neutron star and also on the mass and the. Star and also on the mass and the speed of the black hole. One way of computing the presence of random matter is using, of course, GR simulations, but as you know, this is very expensive and also time consuming, so an alternative is to use empirical fits. The problem is that to compute these fits, we need some parameters which are not inferred by low-related searches, which are like, for example, compactness, which is given by the equation of speed. Is given by the equation of speed. So these fits will depend directly on the choice of the equation of state. As you all know, a rapid identification of compal binary coalescent systems that can produce electromagnetic counterparts is key to undergo electromagnetic follow-up observations. In the second observing run of LIGO VIRBO collaboration, there was a first implementation of real-time data products. Data products. So, two different probabilities were introduced: probability of hashness, which is the probability that there is at least one mutant in the system, and there's also P half memory, which is the probability of having non-zero Riemann matter in the post-meter phase. The main challenge of these data products is to handle detection uncertainties in the parameter inference of rural searches, and the solution that was introduced in O2 was the use of an effect. In O2, was the use of an effective feature formalism, which accounted for statistical uncertainties, but did not consider systematic errors that came from the low-latency pipelines. So, for example, here we have an example of injected and recovered masses in the range of ENSs. And well, you can see that the distribution in the recovered space differs considerably from the one in the injected space. The one in the injected space. For example, the recovered spin values, which are given by the color bars, tend to be higher than the injected ones. And also, if you see here in the advocate between the recovered and injected primary masses in the low relative pipeline, the primary mass tends to the recovered primary mass tends to be higher than the injected one. And we have an opposite trend in the secondary mass. We have that the recovered secondary. Mass. We have that the recovered secondary mass tends to be lower than the injected one. And also, we can see here that the recovered primary masses with masses over two solar masses more or less present higher spin values, as you can see here. So an alternative approach in order to account for these systematic errors in non-latency searches is the use of a machine learning classifier to treat uh this problem as a binary classification problem. Binary classification problem. So the algorithm which is already employed, has been employed in the 03 and 04 observer runs is the KGNA classifier and this classifier is able to map between the true injected values and the recovered parameter spaces. So the true parameter space, the injected values, has labels, has an X and HASMAN, which are inferred from the, as I said, Which are inferred from the, as I said, the injected parameters using these empirical fits that I showed you before. HasNS, this label, will depend on the equation of state considered and also on the inferno of the components of the system. And has remote will be given by the empirical field from this reference, which is the recovered space, which is the recovered values of the pipeline. Will, as you can see here, will show different values of the masses. And the algorithm, what we do, will map between the injected values to the pipeline and the recovered values by the time. And well, the training features of the algorithm are the both component masses of the system, the GSPs and also the SNR of the event. Of the event. So, this is the data set that has been employed to train and test the algorithm. It's a data set with, of course, DNS, NSBH, and BBH signals. These are the approximants that were used to generate the different signals. And these signals are coherently injected into the detector data from Hanford and Livingstone detectors from the O2 LDK observing. The population, the injected population, is given by logarithmic and uniform distribution of the masses, also aligned spins, and they are injected according to isotropic distributions. The data set has more than 200,000 injected signals that are, in this case, recovered by the USDLAL pipeline. And all these signals have a force around rate of less than one per mole. So, as I said before, the machine learning algorithm. Before the machine learning algorithms are trained and tested on the injected and recovered intrinsic source properties, which are the matches and the spins, and also on the scenario of the array. So, well, as you can see here, this is an example of how the KNN algorithm works. So, we have an event here, for example, in the parameter space, and we have the different categories. So, for example, here we have Newton stars, and here we have. Neutron stars, and here we have black holes with higher masses. So we will classify the event as the type of object which is closer to the event. So in this case, if the event is here, it's closer to the blue squares and will be the square of that. So the algorithm, what it does, is it looks at the fraction of neighbors with label in this case cacheNS or cache remnant, and it will give has remnant and it will give a score. So the score will be the fraction of neighbors with a certain neighbor. And indeed these are key quantities that can be can really found on Grace DP when we look for an event. So these data products has an S, has Vernon, also has marks. These probabilities are indeed tier scores given by the classification algorithm. Or here you can see S twenty three oh five twenty nine is 30529, which clearly was an NSBH event, and here you can see that this pretty recent event was zero scores, and it was obviously a minor flow rate. So, I saw it. Okay, so the thing is, can we translate these scores to more realistic probabilities? So, this is what we have tried to do pretty recently. tried to do. Pretty recently we have undergone a work which tried to translate this scores to more realistic probabilities. This is the Pavli's paper and you can see here the archive link. And well in this case we employed two algorithms, the KNN algorithm, which is the one already implemented in the collaboration, and also the random forest classifier. We wanted to also compare We wanted to also compare the performance of different classification algorithms. And these algorithms are provided by design, KILL, and Lagrangian by Thunderbird. And to do so, we have also introduced a new labeling scheme. So the current LDK scheme works like this. The data points are labeled with two independent categories. It's a binary label problem. We have Hazenesh Venant. And they can take values of 0, false, and 1. Of 0, false, and 1, true. The problem is that when you treat these categories independently, the unphysical category hasn't, false, and hasn't done true, can, in principle, exist. So, we have also introduced a new labeling scheme in which we assign a single multi-label category to each data point. So, now we have three possible values of the single label. So, if there's no neutron star, it has an exist false, the label is one, zero, sorry. If 1, 0, sorry, if has an x is true but has remnant is false, the label is equal to 1, and then if both categories are true, the label will be equal to 2. And with this labeling, well, if there's a remnant in the event, we will have a label 2. And for events with label 1 and 2, it will mean that there's a Newton starting in the system. So, first of all, we need to cross-validate. First of all, we need to cross-validate the algorithms to find the most optimal hyperparameters. We use the data set that I told you about before, and we do this for a set of 23 equations of state, because as I said before, the labels will depend on the choice of the equation of state. And you can see here, different, for example, for K and N, the accuracy as a function of the number of neighbors we choose, the weights, and also the metric to compute the distances between the To compute the distances between the points and the neighbors. These are the optimal parameters. And this is for different choices of equation of state. And here, the same for random forests, different choices of parameters. And we found the most optimal ones. That worked pretty well for all the equations of state that we considered. And now we took a look at the performance of the algorithms and we built ROC curves for that. ROC curves for that. We split between training and testing data set using 70% percent, 30% split. And we also compare with the binary label LBK approach. So the solid lines refer to the random forest classifier, the dashed lines to the KNN classifier, but using the multi-label approach, and the dot lines correspond to the current implementation. So random forest. So, Flando Forest overperforms both KNN approaches, and it's true that the multi-label and binary label approach works in the same way for HasNS. But for Hasven, we can see that the multi-label approach performs slightly better than the binary label approach. But here, we have also used this course. We didn't introduce any realistic probability, so now after we have applied. Now, after we have applied this new labeling scheme, we want to translate, as you can see here, the scores given by the fraction of trees or neighbors, depending on the algorithm that we use, to more realistic probabilities. So now that we have three different classes, we have a multi-label approach, we have three different outputs, the fraction of neighbors that have level zero, the fraction of neighbors or trees that have label one, and label two. Unlabeled to and work me tell me what I was saying here, okay. Yeah, okay. Okay, next slide. So now what we want to do is to generalize the relation between the algorithm score, the algorithm's output width and the ground truth of the events. So we introduce these so-called Bayesian probabilities, which is the probability of an event to be actually bright or to have an NS in the system, given the escort output that the classifier assigns to an event. To an event. So we will map the output to the probability of being actually buried or having a Newton star. So we can write the probabilities as this. So this will be the probability of having a Newton star given the classifier outcome, which is given by the output of the detection pipeline. And here, the same for hash reduction. We again consider the data set D, which has a simulated events with a label assigned to each event. And according to Bayes' theorem, And according to Bayes' theorem and also applying different approximations, we get to this expression for the variation probability of having a neutron star for a given algorithm outcome and also for hash radar. So we can build them using the probability, for example, of observing the classifier output, also using the probability that a system includes an internet start, and finally also using the likelihood of observing the classifier's outcome given an event with An event with a Newton star in this case. And we build the hasron probability in the same way by introducing this factor here. Because for sure, having a random means that you previously have a new term start. So here, there's a more detailed expression of each term, but the goal of this slide is to show you that you can indeed express these terms of the expression for the probabilities in terms of the algorithm. Of the algorithm out, the fraction of neighbors or fractions of trees. So, in order to build the probabilities, we can represent the fraction of neighbors with a set of label or trees and the probability, the Bayesian probability that we compute from these outputs. And here we have the probabilities given different choices of the equation of state. The problem is that. Of a state. The problem is that these probabilities can be noisy because of the finite data set. So we use Gaussian process regression to obtain smooth curves. And well, as you can see, depending on the choice of the equation of state, we get slightly different curves. So to minimize the systematics, we consider all the set of equations of state and we marginalize the probabilities over them using these factors. So, the previous probabilities that you can compute and also obtain a smooth curve using this Gaussian process regression, you can tabulate them to compute the probabilities given a certain value of the algorithm output for any new event that happens. So, we can evaluate this marginalized probability on two new independent data sets to check how the probabilities work. How the probabilities work. So, we use simulated CBC events injected in the MDC real-time replay of O3 data, and we also use confident detections from the UWTC3 catalog. Here we have the ROC curves built with different low-retention pipelines and using these ND3OC data. And well, it's interesting that even though we have built these probabilities using We have built these probabilities using O2 data. The algorithms work pretty well with O3 data and also with different low-latency pipelines because as you know, we have only trained and tested the algorithm, so we have only built these best probabilities with the SQLAL. So, except for SP, which performs not too well, especially for HASNS, the other pioneers prefer. Has an S, the other pipeline is performing the same way. And yeah, by the way, the solid lines correspond to random forest and the dash lines to KNN. And you can see that for hash redemption, random forest underperforms for MBTA and ICBC, but in the other cases, both algorithms perform in the same way. I don't know why this works. Okay. Now. We have applied the algorithms to confident detections. So, for example, here you can see the probabilities given by the GCN circulars and also the bias and probabilities computed with our approach. And as you can see for the VNS detections, all probabilities agree. But there's an issue with EWU1900814, in which the secondary mass was about three solar masses. Was about three solar masses, and in this case, only two out of 23 equation of explanation were compatible with a Newton star. So that's where we don't have different results for KNN and random forest regarding the Hazard probability. And the reason why we have such different probabilities is why is because random forest in this case applies a hardcut on the equation of state and if only two out of context equations of state are compatible with MNS for this mass. Passible with NNS for this mass, the probability will be pretty low. The problem is that KNN depends on the neighbor surrounding the event, and since the mass gap region was not filled enough in the training and testing set, the probability was not very realistic, and that's why we have a higher probability for KNL. But in the other cases, the all all results are consistent with the GCN circular probabilities. Here you can see parameter suits for both the KNN and random forest algorithms. We have the primary and secondary images, and here we are plotting the bias and probability of having a Newton star and having rendered for different values of the spins. And you can see that they are more or less the same, but for higher values of the primary mass. Of the primary mass, we have a smoother, let's say, probability here for random forests rather than A and M, which have like small blocks for large values of the primary mass. And to conclude, we can see that this new approach, this new scheme, can provide probabilities rather than algorithm scores, which may be really useful to electromagnetic follow-up observations. And with NeoScheme, we have also developed a new labeling scheme with a multi-label approach. And Random Forest seems to outperform KNN on the O2 dataset, but when looking at different pipelines and also newer observing runs, we have seen that land of forensics looks less adaptable. Regarding feature work, we would like to refine the algorithm training and also the vagessian features. Training and also the vagasian feed estimation with updated data from different low-related C pipelines, and also with O4 injections. And we also would like to have a better coverage of the MASCAP region than the OTU set because, as you saw, there were some issues with the KNN probability. And also, we would like to use updated data products with more recent and realistic empirical fits for hash remnant. And, well, finally. And well, finally, we can easily develop this new scheme for other categories. We just need to use the output of the matching delivery classifier, and the approach is the same. We just need to map it to more realistic bias and probabilities. So, yeah, that's it. Thank you very much. Okay, I think we have time for one question. Thank you very much. Hey, thank you for the very interesting talk. I'm just interested in the motivation for using KNNs and random forests specifically. It's just both of those algorithms are, you know, they're very much constrained by their input, right? They're like a weighted, the output is some weighted average of the input. So they can't really extrapolate, for example. And is that a problem that you would expect in this problem? We're trying to detect if these queries have remnants and whatnot. So we have tried first a wider variety of classifiers. So we also tried to, well, we tested with neural networks and also support vector machine, I think. But these ones were the ones that performed better. And we wanted to try for sure. And we wanted to try for sure KNN because it's the current implementation. So we wanted to compare using the same algorithm. But I mean, I think KNN is easier to understand in the way that you just want to map between the true value, let's say the injected values, with the recovered values by the pipeline. So KNN easily does that because, I mean, you I mean, you just to look around your event and see what is the kind of event that you have surrounded, you know. So it's kind of easy to do that when you have this kind of problem. But we also tried to do a random forest and well, we saw that it performs better with the multi-label approach. Sure, w and that is probably th that was really good in the case that whatever your test data, whatever it's down to inference time, right? It's down to inference time, right? And that's that's fantastic when the data that you're going to put Nick is within the data range. So, like, and do you expect that you've covered that much? You can't, so like if you run into like an event, right? Yeah. That has a property set up just outside the range or something, like, is that something which which could happen? No, in fact, that happened like for this event, like GRG nineteen. UW19 of 24, well, I don't know what it doesn't work, but whatever. There wasn't that good coverage of the mass gap region, and that's why the classification failed, especially for KNM, because it didn't have too many events surrounding that event, and that's why it misclassified the event as a, well, in fact, as a neutron star, and but it was a BBS with that. But it was a BBS with that migration. So, yeah, I mean, the solution would be get a better coverage of these regions, such as the mass gap region. Yeah. Okay, I think there were some more questions, but I don't think we have time, so maybe you guys can continue the discussion on the coffee break. 