Okay, thank you very much for the invitation. So I'm going to speak about noise. So this is statistical enough, but I'm going to speak about linear reverse problems. Okay, so but most about linear, but there'll be something non-linear coming in a moment, very little of it. So I guess that fits into the scope of the meeting. So this is a joint work with So, this is a joint work with one of my colleagues, Sami Tyndill, who is working in probability. Okay, so we often hear in evas problems that we add X percent noise to the data and you do the inversion. So, first, if somebody has heard my talk, there are many differences, but at least this time I have a little bit more time than before. Okay, so you do the inversion, and then you get. So, you do the inversion, and then we get something like this, and then you present that picture. So, okay, it doesn't look so bad. So, you get y percent error with x percent noise. Okay, so my method is good, or maybe the image program is good, doesn't make so much noise. And now I decided to, together with my collaborator, to try to find out what is actually going on, how much noise you can expect, and what character that noise will have. That noise we have. Okay, so again, the most important question we want to understand is how does noise propagate under the inversion? And so far, this is a linear problem, but some of the noise could be added non-linear ways. So I'll get to this in a moment. And what X percent noise means anyway, and what such experiment does tell us about the problem. This is a secondary question, so that's not the most important thing. Okay, so we want to solve. Okay, so we want to solve again so far a linear problem of this kind, and we assume noisy measurements. For the time being, I'm assuming additive noise, but it can be generalized to other types of noise, at least by generalization. And you add the noise to the data and invert A somehow if it's invertible. So, if it's truly invertible, this is what we get. Invertible, this is what we get, and since it's a linear problem, we really have to understand the inverse cycling on the noise. And okay, the noise may not be in the range, as very well known, but we'll see in a moment that's not really a big problem for what we're trying to understand. And the noise doesn't need to be additive. It could be multiple additive noise, it could be Poisson noise, for example, which depends on the level of the signal. And this is the non-linear part, and this is the only non-linear part. Linear part, and this is the only non-linear part of my talk. In other words, you may not have exactly this module, and but if the noise is relatively small, which you hope it will be, then you can linearize and you don't really get exactly that, but you get something that can be analyzed with those methods. So the measurements are discrete in real life. So numerical inversions are discrete, but the models are typically not. Are typically not. So we switch between discrete and continuous functions all the time, and this is one thing that we want to understand as well. And we assume that the noise is added at the discrete stage, either at finitely many sensors, or you do numerical simulations, you add noise, you add it in a discrete way. So again, this conversion from discrete to continuous and back is one of the things that we want to understand. Want to understand. And before I say even something else, I want to say there's a newer paper with François Monard that we posted on the archive a few weeks ago, which deals with the geodesic exit transform and what happens if you add noise to the data then. And all this machinery and more is there, but I don't have time to talk about this today. So we want to model noisy discrete measurement, as I mentioned before, and we just assume that each And we just assume that each detector, at each data, we have a random variable which perturbs the data with a given distribution. And those variables so far are independent, but this can be relaxed and the machinery works. And we get something like this. So let's say this is a Gaussian noise, and this is what we get under Gaussian noise. And okay, so just to go back to the work with François. To work with François. So we talk about sampling there. We didn't do noise, but all those things are related. Okay, so this is Gaussian noise. And this is the histogram of Gaussian noise. Or the distribution, if you want, is Gaussian, okay, because we chose it that way. And this is the spectrum. So why I'm interested in the spectrum? Because very often, Because very often you want to have one measure of noise, and one measure will be standard deviation or variance. But instead of one measure, if you want to have one function, okay, that will be the spectral density or the spectrum or the power density. And here, since you have a radial symmetry, you just integrate respect to the angular variable. And this is what you plot. You see, this is more or less flat. Flat and this is actually a theorem. Okay, so this is uniform noise, uniformly distributed noise. This is the distribution, and this is the spectrum. You see, the spectrum still looks kind of flat. Okay, so that is actually a theorem, at least in the pie statistics, is called that way. And my collaborator tells me that in his field, In his field, we know how to pronounce it, and it has a different meaning anyway. In plasticists, we'll call it that way. So, the theorem says that this white noise should have a flat spectrum, and this is actually where the term comes from. Okay, it follows from the fact that the auto correlation function is a delta, so you take the Fourier transform, and that gives you the power spectrum basically and. And let's test that. So, how white is white noise? So, this is one-dimensional white Gaussian noise. So, this is the power spectrum of that. I didn't really put here square here because without the square, that looks nicer when you plot it. Otherwise, too large deviation. And you see that this kind of flat. Well, okay, on average, it's uniform, but of course, you don't really get exactly uniform. Get exactly uniform values. And you may expect at this, I expected because I didn't have any prior experience with probabilities of ticks, well, on this level and things like that before, that if you increase n, that will get flatter and flatter, but it doesn't. And now there are two regimes that you want to understand. And this is a terminology that we introduced. That we introduced. One of them is we call that temporal. So we keep the number of n of the pixels fixed and run the experiment many times. In other words, we take measurement after measurement. And then you look at some fixed n, n will be the frequency, right? And you want to find the expectation of f hat squared n. And then this turning that I mentioned before says that what we get some. That I mentioned before says that what we get something is independent of, and that's relatively easy to prove. What I mentioned before, and in that sense, the spectrum is flat because the expectation of that doesn't depend on n is the frequency, right? Okay, but in practical implementation, that means you keep doing the experiment again and again and look at the specific frequency. And the expectation for that specific frequency, the spectrum, there, the power spectrum is. The parent spectrum is constant, independent of that. But what you actually do in inverse parents, you don't take repeated measurements, you take one measurement, which will be noisy. And you won't understand what happened there. Well, you still want to understand what happened there in some probabilistic sense, of course. But you don't have the luxury of taking too many measurements. If you do, then you can just average noise. Can just average noise, you don't have to worry so much about this in the first place. So, the ergodic sense is the following: you're in the experience once, in other words, you just take one picture, roughly speaking, but with n large enough. In other words, you increase the resolution with high resolution, roughly speaking. And the spectrum stays hairy, as I mentioned before, even for large ends. So, in that sense, the spectrum is not soft, but The spectral is not flat, but on average, it is flat, and I'll show in a moment what I mean by that. So, there are basically two regimes: one of them is temporal one. Again, just summarizing what I said, keep the number of n of pixels fixed and just keep repeating the experiment. This is not what we want to do. And ergodic is run the experiment once, but with large n, and then we want to understand the properties of that mice. Properties of that mice. So, can I just ask a very quick question? Can you give me the exact definition of F hat of N, just that I can follow? So, what is F hat of N exactly? Okay, F hat is this is a Fourier transform, but that is the discrete Fourier transform in that case. Right? So, of what? Okay, of you have a vector, so F is a vector. So, F is a vector with n components. And there are ID Gaussian or units? That is F. This is F, which depends on n components, and this is the discrete Fourier transform of F. This is F hat. So F is a finite vector of independent standard Gaussian random variables? Don't have to be Gaussian, can be basically anything distributions. You have to have some finite. These distributions you have to have some finite momentum, but it's a random vector, and f hat is the discrete for a transform of f. So you compute the characteristic function at all integers in okay, so is it the expected value? So, no, I just trying to understand. It would be nice to have a definition of f hat of n, so, but anyway, Of f hat of n. So, but anyway, okay, so it's something like that. And then you have some and then e to the minus i, and then two pi. This is the engineering way of doing things to put 2 pi here, and then n and then k and over n and then you multiply here by f of k and take a sum. take a sum over the torus okay f is and f is the random it's a random vector with n components okay okay thank you yeah we think of this as being some kind of discrete version of the continuous for a transon but actually here it is the discrete for a transon and here i had some animation which for some reason doesn't work uh so Reason doesn't work, so I apologize for that. But what I mean to say that if you take averaging, take some interval here and just move it around. And then on average, you really get that spectrum. Just fix the length of that interval, take just the sum over that interval, and increase n. And then if you move it to the left and right, And then, if you move it to the left and right, eventually get a flat spectrum. In other words, the spectrum is not really flat the way it is presented here, but some kind of averaging to make it flat, which is not so hard to prove. And actually, prove theorem that is more general than that. Okay, so now let's talk about this so-called temporal averaging, which is really not what we want. Which is really not what we want to understand, but just to test it. So you take a random vector of length 128 and you plot the discrete Fourier transform at this frequency enough because it's a real, so it's enough to take out of the frequencies. And to repeat the experiment in average the power spectrum, and it really gets fast. As you can expect, this is the theorem that I mentioned before. It doesn't converge very fast, but okay. It doesn't converge very fast, but okay, we know what's happening here. So I really want to understand the regoric analysis. And then I take a random vector with length n and compute the so-called power spectral density, but it's averaged over 25 intervals. So n increases, but you always have 25 intervals. And then you move it to the left and right, and average asymptotically gets flat. Guess flat okay, so in that sense, it's flat, okay. So, and our theory must lead that to the phase space. This is relatively easy to prove again. Proper is known, but I couldn't find it written exactly that way anywhere. So, our main goal is to formulate like this in the phase space. In other words, for points and directions. And again, this is a naive way to explain what Way to explain what we want to do. So, in temporal sense, you keep repeating the experiment, you get different measurements, but that's not what we have in numerous problems. We have only one measurement. What we actually do here is increase the number of pixels, roughly speaking, to make the partition finer and finer, and try to understand what's going on then. So, this is our large parameter. It's the number n is not the number of experiments, right? Is not the number of experiments, roughly speaking, okay? Uh, so let's say you have f which is discretized on some grid, and small parameter here is basically one over n. And the natural framework is the semi-classical framework, and you want to identify functions on discrete set with functions on continuous variables. And why? Okay, because our models are usually continuous. Our models are usually continuous, they're not discrete, and there are many ways to do that. And one of the ways to do this is to use the sampling theory. Again, it's not the only way to do this, but this is a very convenient way. And I'm going to go very quickly over that because it's not the main topic of my talk. So the classical sampling theory tells you that if you have a function such that f hat is supported in some box. Supported in some box like this. Okay, this is band limited function, and then you can uniquely and stably determine based samples if the steps size satisfies this estimate, which is determined by the band limit, is basically inverse proportional to the band limit. And you have this nice formula here, and then this actually. This actually is a very nice map, it's a unitary map if it's scaled properly, and it allows for small errors, roughly speaking. And the proof is very simple. You just think of F as the inverse Fourier transform of F hat, and then the samples actually the Fourier coefficients. And the bad limited condition tells you that F, right, if you switch F and F hat, then F will be compactly supported, and then we have just Support it, and then we have just a foray series here. But the reason that we have this thing is that you can put that cutoff before you do the foray inversion. So I have some cartooning here. So this is the graph of the sink function. And you know that f hat is supported here a priori. And then you just multiply by a function which is a characteristic function of this interval. Characteristic function of this interval and then do the inversion. And if you do that, you get that sink function. But if you have support which is strictly smaller, then you can choose a function which is actually nicer, doesn't jump, and then we get a faster converging sink, roughly speaking, there. It's not going to be sink, it's going to be something else. Okay, so again, this is a classical sampling theory, nothing really. I mean, it's 80 or 85 years old. 80 or 85 years old. And in another paper, I prove a semi-classical version of that, which is basically more or less the same thing with some error estimates. So if you have a function that depends on small parameter, because you think what your F being representing small grid with finite and finite partition, and then the discrete F actually would depend. Discrete f actually would depend on the small parameter. And now I have to make this assumption about so-called semi-classical waveform sets. And what is that? I have this on the next slide. Okay, so I'm going to use some notions of semi-classical analysis. Now, basically, you take the Fourier transform, you window it, you first localize the function near some point, and then you look at the Fourier transform. And then you look at the Fourier transform in different directions. But then you scale the variable psi by dividing by H, and then look at the directions in which it decreases rapidly with H, or it doesn't. If it decreases, you declare this not to be in the Wayfront set. So the Wayfront set in a semi-classical sense is not a conical set. It can be a compact set. And actually, this is the most interesting. Set, and actually, this is the most interesting case, more or less. But anyway, this is the assumption that you make here. It is the band-limited assumption in that case. And if you have this assumption, then you have this representation with a small error. And this is product of sinc functions, but you can actually make them really nice Schwarz space functions. If you assume oversampling, you basically have to assume that this is trick inequality here. Not much more than that. Not much more than that. So now you need finitely many points, unlike the classical sampling theorem, but their number increases with age and is of disorder. And in fact, you need at least this many points. You can prove it. It's a very interesting thing. That's kind of while law for the number of points that you need to sample it properly. It's not exact, but the error is small, whatever small means. And here, nobody. Are small means, and here nobody will complain about this error. And differential operators are kind of bounded here on that space because you assume this thing about the waveform sets, right? So there's no problem to differentiate those functions. Actually, they're also infinity if they have compactly supported waveform sets with a sequence of such functions. So the simplicity doesn't have to be square, it can be rectangle, it can be even something more general. Can be even something more general because you can apply linear transforms, and the same thing creates a determined by the size of the smallest box containing the frequency set, which is basically the same as the essential support of the Fourier transform. What is H here? H means that you scale the psi variable, but psi or H in the definition of the Fourier transform. And if you And if you think of this this way, you lose the x localization because this is the worst scenario case, right? You move over all x's and then you look at the worst scenario case, but that gives you some criteria how to sample properly. Okay, so we work with semi-classically band limited functions, and one may ask why we work with sequences because they are naturally sequences. They are naturally significant, why not with fixed f because this is what we really want to understand. And if you have average measurements with small averaging, imagine something of this sort, but you can apply a Gorrow's theorem and you can basically move that thing there. Instead of having it here, roughly speaking, you can put it there. Okay, it will change a little bit. Okay, it will change a little bit. It will be so-called differential semi-classical superdifferential operator, but that will automatically make the new F first depending on H, and secondly, bent limiting in that sense. So, in other words, it's not a big deal that we work with F depending on H. Okay, so now you seem that A is a classical Fourier integral operator, and many interesting problems lead to such operators. Well, not all of them, but many, like the X-ray transom, at least in two. At least in two dimensions, and geodesic etc. So, this is the work with François that I mentioned, and thermoacoustic tomography, and so on. Okay, so now if you know the band limit of F, and I mean that in semi-classical sense, the economic correlation allows you to find the semi-classical band limit of A. There should be L here, and then we know how to sample F and to be more precise, F. Okay, if you are given the sampling rate of F, which maybe just given, we don't have even control over this, we know what resolution limit of F that gives you. And then if you under sample, then you get LISYNC, and the LISYNC artifact here actually is not local. They are very interesting, but I don't have time to go over this. Okay, now back to noise. So we To noise. So we have a discrete functions on grids like this that we identify with functions of continuous variable eventually by doing some interpolation. And then you want to characterize the resulting noise first of the continuous f and then of f applied with f applied to it. And there are many several characteristics of noise. I mean, after all, we're not claiming that we characterize everything possible. Characterize everything possible about the resulting noise, but at least in applications, you can see the power spectrum of noise is important. White noise is the one that has constant spectrum in some sense. And to explain in what sense this is true. This counts if you a priori assume independently identically distributed, for example, random variables everywhere. And now we just consider functions that depend on age. Sometimes I just skip that subscript h, which is very common in semi-classical analysis. And I think of all those functions being noise, if they depend on random variables, even though even if I don't really define what noise is. Well, it has to have zero mean, at least. And other than that, okay, they just. And other than Delta, K, they just call them noise. Okay, so we could have pink noise. Peak noise is this one definition, but generally peak noise means when the power spectrum decreases with psi, not necessarily exactly like this. You can have blue noise, this is one example of blue noise, and the distributions of the noise could be. Of the noise could be Gaussian, could be uniform, doesn't really matter as long as we have bounded moment of certain order. And doesn't depend on characteristics, distribution spectrum. And you can define standard deviation of variance, but this is again in so-called ergodic or spatial sense, right? You just integrate over the domain. Domain instead of repeating the experiments. And of course, if you forget about anything random here, this is just normalize L2 normalized. And in engineering concern, it's simple that if your signal is below the noise level, then it's lost. And if it's above, it's detectable. Well, this is not a theorem, but this is the working assumption. But this is the working assumption, more or less. And now, if you understand this, interested in the variance, you can just integrate that because you can use sparse evaluation. In other words, that object will give you everything that you need to know about the standard deviation, the variance, if this is your goal. So, this is more general. The far respective is more general. So, the problem is that we add noise with known characteristics, and then we want to understand the resulting noise in the reconstruction. Understand the resulting noise in the reconstruction. In other words, what happens if you apply your FAO or maybe the inverse of it? And inverse here means parameters. And we think micro-locally, so why not consider the power spectrum in phase space? And we want to measure its trend, a power spectrum, if you want, at each x and psi, not just to teach xi, which is the power spectrum. And the phase space is not a conic set. The phase space is not a conic set in this case here. So, if you multiply xi by a constant, you get a really different point. And we propose to the so-called semi-classical defect measures for that. And this is the definition of semi-classical defect measure, well known in semi-classical analysis. It basically tells you what is the frequency content of a function like this, which is a priori semi-classical band limited. Numbers halfway from set semi-classical, which is. Halfway from set semi-classical, which is compactly supported, compact actually, in the phase space. So you test that with pseudo-differential operators, semi-classical pseudo-differential operators, and you get the integral of symbol with some measure, and that measure here tells about the frequency content of f basically. And it's a theorem that if you have those assumptions, you can always take a sub-sequence such that this converges. Sequences such that this converges and gives you some measure. In our case, actually, we want this to converge for OH, not just for some sub-sequence. And for white noise, actually, we prove that this is true. And but again, the main point is that this semi-classical defect measure, we want to take it as a measure of the strength of the noise in phase space at any x and x. In other words, we want to do localization to the extent possible. Of course, we know that we cannot. Extent possible. Of course, we know that we cannot localize perfectly because of the uncertainty principle. Okay, so in our case, in most applications that we present, you have absolutely continuous measures. So gamma will be some continuous nice function, and this measure will look like this. But it doesn't have to be like this, and the theory still applies. Okay, so if you have that measure, what do we do with this? We can have a Uh, have a for a subdomain omega, you can integrate that measure on the on the domain, and that will give you the variance that I presented before. But you can do that locally in a small piece, small parts of the domain, and a priori it may change from point to point, doesn't have to be uniform everywhere, and you can define variance at point if you want. At point, if you want, just integrate the measure with respect to xi, there's no x here, and that gives you density, roughly speaking. Roughly speaking, you just shrink the domain to a point, and if such gamma continuous function exists, this makes sense. Or better yet, you don't integrate it and just interpret gamma's spectral density at point to direction. And this is related to the so-called Wigner function, and the Wigner function. And the Wigner function, you don't typically take the limit when h tends to zero. And that's not really a measure most of the time. But if you take a limit when h tends to zero, you really get a measure. Okay, so now we're given all these samples and we form this continuous function. I call that continuous because it is continuous, but it's a function of continuous variable. And one of the And one of the first theorems that you prove is the effect measure of white noise. If you have a white noise, white noise again means independent, identically distributed random variables, you form this function here this way. And then the associated measure actually looks like this. So this is a constant. This is the variance of the process that is used to create the noise. This should be here because it is in your formula and Formula and chi hat has large support. I mean, it's related to the band limit. And on a big part of the band limit is one. So you can just think about this being one in the interesting part of the spectrum. And if you think about this being one, that basically tells that this is constant in the interesting region. So which this is something you would expect from white nice to have constant. From white noise to have constant spectrum. So, this anti-atwell proof, and the proof is several pages long, and it's not really trivial. In that sense, spectrum of white noise is constant, and we consider more general noise, which can be correlated a priori and can vary from point to point. I don't want to present the theorem here because I don't have time for it, but what we get is you get an additional factor here if you assume that, which On that, which is related to the upper assumptions of the correlation of the noise. Okay, so next question is: how the fact major transform under an FIO? And if it's a classical FAO, you can use Garosterium. So now it is classical FAO, but this measure is semi-classical object. So you have to deal with this mismatch. But away from the zero sections, you are fine. And just apply a Gaurav's theorem, and this is what you get. Just apply a Gaur's theorem, and this is what you get. In other words, again, what is the point of this theorem? You have noise for which that measure here density is well defined. If it's a white noise, gamma will be one on relatively big region. And then the student tells me what is the power spectrum or the measure of the noise after you apply that FAL. You have to have the canonical relation, and then you take the principal symbol F star F, which is. The principal symbol f star f which actually two differential operator easy to compute, pull it back with economic relation. This is what we get. So, remember, we're solving this problem, and A may not be even invertible, but let's say that you apply just parametrics, and all those pseudo-inverses, for example, they happen to be actually parametrics, parametrices. And again, this is just parametrics, doesn't need to be actually inverse, and actually inverse may not even exist, and if And if you set f to be a inverse, this is just a reformulation of the theorem, just call f a minus. So this is how you compute the noise which is induced in your reconstruction if you're given the noise in the data. This is how you compute to be more precise that let's call it a generalized power density or the micro local measure. And if you know that you can compute You can compute the standard deviation locally, but the point here is that this, in principle, depends not only on the position, but depends on the direction as well. And now I have very little time, but at least I start with the Radon transform, two important examples, Radon transform in parallel geometry, never seen those coordinates. And so here is the picture. Maybe you have to do. You have to draw this circle, but anyway, this is your line, and then p is the distance to the origin, and omega is the normal to that line. This is one way to parameterize lines, and this is well known to be for integral operator. Here's the canonical relation. Don't have time to explain what it is and how I get it, but it's easy to compute. It's a differ morphism that actually has two branches. Each one is a differromorphism. Each one is a different morphism, and you can compute the inverse one if you have a formal for it. So, in other words, the formula there applies. And if you assume a priority that the wavefront set is here, then you take the canonical relation, apply the canonical relation, and then you see that the wavefront set of the image will be supported here in those two cones of triangles. And the smallest bounded box is the one that you see here that tells you what are the sharp. Tells you what are the sharp rates, sampling rates. And now, based on all those calculations, you can actually compute the micro measure of the noise, knowing of the induced noise in the reconstruction, knowing that in the data. And again, for white noise on a big part of the spectrum, this is just one constant. And that tells you that, okay, this is the concept I meant. So don't pay too much attention to it. So, don't pay too much attention to it. So, this tells you that the noise proportions to modus psi. And since this is a quadratic measure, so if you take square root, it will be one half here, as you can expect, because the exit transform is operator smoothing of order one half, and inverse one is anti-smooting, if you want, of order minus one-half. So, you can expect some factor like this here, but what is not quite obvious that this. Not quite obvious that there's no other depends on x on psi. The only way it depends on x on psi is through this factor, which is kind of expected. And this is blue noise. You can call that blue noise. And it's x-independent. Again, it's isotropic. That doesn't depend on direction of xi. And okay, so I don't have to talk about this. I can. Okay, those are numerical experiments. So I just take white noise in the data and convert it. And convert it, and this is what I get. This is the power spectrum, right? And this is the radial profile, which is computed. And then this can be unexpected, but the reason that it is there because the actual inversion has some smoothing properties. If you do a high accuracy inversion, then this part here disappears and it's just a straight line, as it should be, because this should be the graph of distinct individual coordinates. Okay, and some numerical experiments that show that. Experiment that shows that this is confirmed very well. This is the number I expect. This is what I get. I can have to really go very fast here. And if you put a filter in the inversion, you can compute what happens after that filter. And you can test it numerically. And again, this is what we compute with noise. This is what it should be. And the match is really nice. And I just want to go to the last few slides. The last few slides because I'm out of time now. If you go to fan beam coordinates, parametrized by point and direction, in the same transform, just parametrized difference, right? But it compute the micro-local nature of the noise. And again, if for white noise, this is going to be constant. Let's say it's one over big part of the range. And this doesn't really matter then. But what we get here is that it has a non-trivial dependence on psi, this x. Trivial depends on ψ. There's x here, and there's xi. So this is the new factor that we didn't have before. So you get a noise that changes with position and changes with direction as well. And this is what it should be theoretically. And this is what it is numerically. And just take a small square here and compute the power spectrum in this small square of the image. And of course, it's noisy, it should be. And you kind of see this feature. And you kind of see this feature there. Okay, I'll stop here because I'm out of time.