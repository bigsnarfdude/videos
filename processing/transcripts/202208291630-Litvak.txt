Well, I will thank Agalizers again. Don't recall me. Yes, yes, yes. And like Gidon, I don't belong to this community, but I'm going to speak about problem from, well, from essentially which belongs to this community, but which is also a very nice problem from the point of view of asymptotic geometric analysis. But I would like to start my talk with. But I would like to start my talk with sad news. As probably all of you know, very recently two distinguished mathematicians passed away and both were somehow close to me after my PhD. I did two past docs, one with Nicole, one with Juron. And this, of course, led to many years of fruitful collaboration. For example, if Nicole, we had 34. Example, if Nicole had 34 papers, and I want to say just a couple of words about each of them. So, Joram worked in local theory of Manach spaces and asymptotic geometric analysis, which are probably different names for the same topic. And he is known for many, well, many, many works, and probably I mentioned. And probably I mentioned several directions that he did. He has some paper with Lewis, very influential paper. They answered one of Grapenzie's questions on factorization of operators. And then many people studied so-called Gordon-Lewis property, which they introduced. Of course, they didn't call like this, but we introduced it in GL constant. Then he also worked. He also worked with Gaussian processes and he proved Gaussian minimax theorem, which had many, many applications. In particular, this led to Gaussian approach to Dwarietzky's theorem. He improved, well, Minman's proof of Dwarietki's theorem with some dependence on some constants. It's about essentially finding section of which is near. Which is near to ellipsoid, and what does it mean near? It's like in Guidon's talk: if you want distance one plus epsilon, then you want to know how a constant depends on epsilon. And Gordon improved this dependence with random approaches. Then there was some results of Gideon with non-random approach. Also, well, several results which were unknown, but Which we are known, but with best constants, it's known using the Gaussian approach. It's also probably less known result about inverse santal inequality because it was proved by Brugen Millman almost immediately after Gordon with Freisner proved it up to Lugarifon, but still it was some achievement. And also so-called decomposition of identity when you have maximum volume and you absorb it in. It was standard John's theorem, but it can be generalized to case of two different bodies. So just case several directions that he worked in. We had series of works with Stutt and Werner also on Ehrlich norms of sequences of random variables and related works on order statistics when you have. Or order statistics when you have a sequence of random variables, but not identically distributed, and you want to know how order statistics behave. It also has applications to approximation theory and to connect geometry, of course. And with Nicole, it's probably, well, I will not list directions because it's probably will be too many of them. She has many, many deep results in classical Banach space theory. By classical, I mean infinite dimensional. Classical, I mean infinite-dimensional. And also in local theory or symptotic grammatic analysis, which is a finite-dimensional counterpart of Ban Express theory. She wrote very influential book. It's a large book, I don't remember, four or five hundred pages, probably more. Also, she has a survey, a very important survey on quotients of fine-dimensional spaces, also random phenomenon. Also, random phenomena, etc. And then at the end of her career, she worked on random matrices. We had a series of papers on random matrices. So, just to mention, to say a few words. And now I probably will start my talk. And it is based on two papers which are already published, so the result is not. Which are already published, so the result is not very new, and probably some of you heard it on some online seminars talks. So I'm sorry to repeat. So what is the problem? So problem is very easy to formulate. So we have a D-dimensional cube. Well, and we want to put points in this cube to find. Cube to find some configuration of endpoints such that whenever you have axis parallel box, X is parallel means that all facets are parallel to coordinate subspaces. Then you have at least one point inside. So this integer will be denoted by n, and of course it depends on two parameters. So epsilon, which is responsible for the volume. Which is responsible for the volume, and D, which is dimension. Or equivalently, I formulate this problem differently with inverse function. So we look for super over all epsilon, such that whenever we have endpoints here, we can find so-called EPSI box, which means. So-called Etsy box, which means that it doesn't contain any point, but it's of volume epsilon. And this is called minimal dispersion and denoted like this in literature. So this is informal definition. Let me give formal definition. So we consider set of axis parallel box, which is well, like I said, that all facets are parallel to coordinate planes, which means that it's just product of intervals. just product of intervals so i denote it by rd then dispersion which was here is dispersion of a finite set so we have set p and its dispersion defined as a suprium over all such access file boxes of which which which don't contain points, then suprium of volumes of such boxes. Of volumes of such boxes. It's dispersion, and the minimal dispersion means that I take infinite, well, infinite over n over all sets of cardinality n. So then what I described here and what I will be using in this talk, it just inverse function that it's minimum ovarian, such as the dispersion less than epsilon. Personally less than epsilon. And for this talk and for proofs that we are using, it's much easier to work with n than with epsilon. But once again, I can pass from one to another. So this is the setting. And like I said, it's topic an approximation theory, but it looks very nice and very interesting from the point of view of asymptotic geometric analysis. Analysis. And let me mention known results. Well, in 1996, there was a paper of Road in Tit with first upper bound, which used some algebraic structure, which was improved by Larcher to essentially constant to the D order epsilon, upper bound for N. Rotentici got something like D to the D. It was some product of prime numbers. Numbers. So this is upper bound, which was very recently improved by Buch and Chow to polynomial behavior, essentially to d square. And for lower bounds, of course, one bound is completely trivial because if I have n points, I will split my cube in n plus one equal well, parallel tops. One of them will be empty. Tops one of them will be empty, so of course, one over n we have immediately, which in terms of n is one over epsilon. This is trivial result, less trivial result. Well, proof is not trivial, but improvement here just by constant. So in front of one over n, we can put some constant which is larger than one, five over four. But they introduce a new method, and in particular, this method implies that if I fix That if I fix d dimension and I send epsilon to zero and I multiply n by epsilon, then limit exists. And this limit, of course, function of d so in a sense if you don't care about how constant depends on dimension, the problem when epsilon goes to zero is solved. It behaves like one over epsilon up to constant, which is. Over epsilon up to constant, which depends on z. Well, this is what is known. And another non-trivial, really non-trivial result is by I. Schleitner, Heinrichs and Rudolph. They showed that not just n is larger than one over epsilon, but that this constant C D grows with dimension. So it's So it's bound essentially it's bound for C D. So it's larger than logarithm of D over epsilon. So and it was the first result actually which shows that and D grows to infinity when D grows to infinity if I fix epsilon. And another result which is in the same paper of Buchan Chow which which holds only for Also, only for very, very small epsilon, one over d to the d they proved that lower bound and benter is d over epsilon, which of course leads to bound on this limit, which is already quite good because it's between z and essentially z square. Still, we don't know, of course, what power should be here, but. Here, but at least for a limit, we know. And in general, when we don't fix one parameter and send the second to zero, in general, gap is big. It's between logarithm and z-square here. And like I said, in my talk, I actually interested in capsule to zero and z to infinity. Changing capsule to zero and z to infinity. I don't want to fix one parameter. So there is gap, as I said, especially if you don't use second estimate, but you want to work with epsilon, which are slightly bigger than one over d to the d. And another, I will show more known results. One is from 89 by these four voters. 89 by these four authors. They well, they study more general setting and classes of bodies and examples, but in particular for the cube and dispersion of the cube, the result would give what is written here. So it's bounded by D, not by D square, but by the price of logarithm of one over epsilon. So here it's you really start to feel. It's you really start to think that we need some relations between epsilon and z, what to choose, say, d square over epsilon or d times log one over d. And this was noticed by Rudolf, but Rudolph also provided a direct proof of this result using points uniformly distributed in the cube. And this estimate, I want to emphasize this estimate is better than Buchao estimate if epsilon is not. Buchao estimate if epsilon is not extremely small with respect to z. And natural conjecture, of course, was well it was much before Bukchava result. Natural conjecture was that n should behave as d over epsilon. Actually, Bukchau is in conjecture that it's d log d. But conjecture from previous time was that behavior should be like d over epsilon. But surprisingly, 0 over epsilon, but surprisingly, Snarietz proved that in fact, if you don't care on the if you fix epsilon and send z to infinity, it behaves like logarithm. So it's less than C epsilon times times logarithm of Z or epsilon less than quarter. You will see that quarter somehow important. Then Then, using lower bound, which I mentioned before, essentially we know how it behaves if you fix epsilon. It behaves like Logari from D, which is, in a sense, it's surprising. But of course, dependence on epsilon was bad. Dependence on Epson was in Cesareat's paper, it was like this, which is terrible. Which is which is terrible. But then Marioli and Jan Viberal improved this to one over epsilon squared times Logariform squared. And they also conjecture, but it was before Bukchar's result that it should behave like this. And after Bukchar, you know that it's impossible because if epsilon goes to zero, you should get D here. You should get D here. So I put years when paper appeared. Of course, the Snell's result was before, at least one year before. So, and I just emphasize that the upper bound is better if epsilon is not very small, if it's larger essentially than one over z. I actually like, I will produce a picture. That if you look at a plane which is created, say, by D and one over epsilon, then what I said before, up to some constants, we know the situation when it's bounded, say, here, C, C, here, C. If we fix one parameter D, we know what's going on here. So if D is fixed, it behaves like some constant depending on D over epsilon. Depending on z over epsilon. But if d is bounded by constant, this is just constant. On the other hand, if epsilon is fixed, you know what's going on here, it's like Lagari from D. So the question, what is both goes to infinity, one over epsilon goes to infinity and z goes to infinity. And this is a major major topic of my talk. I also before I also, before I start to say that, to say that result that I mentioned before, especially Woodworth result, they based on random choice of points. Random with respect to Lebesgue measure on the unit cube. So scenarios and then only requiberal result, they use different randomness. It still was random, but choice was on some lights. Was on some lattice. So instead of Lebesgue measures, they consider some lattice inside the cube and they choose randomly from this lattice. And surprisingly, at least for me, it was very surprisingly that it worked better than Lebesgue measure. And hope to have time to say a few words about this. And before I come to our result, I want to say also a few words on very large chapters. On very large, I mean larger than quote. Very large, I mean larger than quarter. So, of course, if you have epsilon larger than half, then nothing to do, you just take a point in the center, and any axis parallel box of larger volume larger than half should contain this point. So, we have one. Now, it in his paper also got this behavior for epsilon larger than quarter. What is important here that it doesn't grow to infinity when d grows to infinity. Grow to infinity when d grows to infinity. So if epsilon is large, larger than quarter, you don't have this effect that n epsilon d grows to infinity, which is on one hand surprising, on the other hand, probably not surprising because for half, it's fixed, so should be some transition where it starts to grow to infinity with d and well, here I just recall that we know a bound which That we know about which is Logarifon for, say, from water to one rate. And another recent result, Kutmarque, improved this behavior from one from linear in one in one over epsilon minus quarter to square root. He also showed that when you have not so many points, you first try and actually natural try to take. And actually, natural try to take them on diagonal. Of course, it's not known if it's the best choice or not, except that I take one point. But if I take such choice of points, could it also prove that this is the best possible one, which is quite interesting. And so problem, of course, what's going on when epsilon exactly equals to quarter. Exactly equals to quarter, or when epsilon goes to quarter. So, uh, would be an interesting question: how does it behave? So, and so what about known results? And here, I just combine what was known, but probably it's hard to see. We just have three different regimes. Uh, by regime, I mean relations between epsilon and z in which of them in each. In each of them, we have a different formula, which I will try to improve. And the proven is here that we got the following upper bound. And this upper bound works for epsilon less than half, but of course, it's better to take epsilon less than quarter, otherwise, otherwise, the Snarlette's result is better. And a few. And a few remarks. I say that there are two papers, and in first paper, I have additional gariffum here, and in second paper, by better construction of nets, we improve this. What is more interesting is that there is a paper of Hendrix Krieg, Kunsch and Trudol, who worked, who provided lower bound for the case when we choose random points. So the estimated expectation of To the estimated expectation of dispersion for random choice of points. Of course, random doesn't mean best, and here actually we see that it cannot be best. But because our technique is random, in our theorem, it's essentially sharp. So we get the same term here. Only double guy from here is difference with the best result if you use random choice of points. And also, And also, if you compare this with previously known upper bounds, it's also better in most regimes. Of course, if epsilon starts to go to zero, then this result is better. But if epsilon is not that small, then our result is better. And in Rodeois results, you essentially either remove D in some regime or a double gate. Or a double logarithm instead of single logarithm. So, this is an improvement. And for relatively large epsilon, when epsilon is essentially larger than 1 over d, it's a very slight improvement of the Snavier Suliquiberal result. We removed. Actually, it's my paper. We removed the square of this logarithm. It's not big enough for them. But just the same method used. And here it's not for random points. Well, it's for random points, but not with respect to Libet measure. We need to adjust Libet measure. And bound, well, if we forget about algorithms, this bound is better when epsilon is larger than 1 over z. So relatively large epsilon. And like I said, we need to adjust distribution. I will say a few words. And so I just combine in this picture this plane with one over epsilon z, we have essentially we have four hedons. And in each hegeron, some upper bound is better than other bounds. So some method of proving is better than other methods. And this and this corresponds to our results, and it's given by a rubber. And it's given by random points with respect to Lebesgue measure. This is Bukchava result, and this is from my paper. So this is what we have. And of course, it would be nice to have uniform bounds and to understand to understand the behavior. The behavior. But in fact, I doubt that such methods will give because most of them quite naturally give you that function of two variables, which is epsilon and, is presented as product of two functions of one variable. Epsilon and z. It's very similar to Dwaretzki's theorem. But of course, there is no reason that function of two variables behave so simply. So it's just byproducts of the methods, and probably we should find another method which really... Find another method which really gives function of two variables. Well, entirely now we'll discuss proofs, but of course, I will not give calculations, but some ideas. First, how random points works. It's a very standard procedure. So we take n points that P of n points randomly independently drawn from. Independently drawn from uniform distribution on the cube, which corresponds to Lebesgue measure. And we want to show that every box of volume epsilon can trace at least one point. Then, of course, we cannot check all boxes, so we will construct some net of boxes, which I call test boxes. So with the property that if each exercise Is that if each axis parallel box in n contains a point, then every angle in Rg contains a point. Of course, the simplest way to construct such net is to satisfy this property, not to construct this net, is to consider, well, just having some rectangle. Some rectangle. Simplest way to think about this and to try to prove this is to construct my test boxes in such a way that for every B in R D, of course, of volume epsilon, I can find a box in N such that zero is in B. It will be my main construction, but then I well, for some cases, I will need to go outside of B. But in this case, it's clear that if I have point inside B0, then I have point inside B. But another thing, because I have some volume calculations, I need that not so small volume. This I should uh I should be careful and then uh and then we we have what is our bet event? Uh bet event that uh it is not true that every test box contains a point. So you can find a box which doesn't contain uh points from P and this is better. So we estimate this as some, we use just union bounds. We use just union bounds, sum of bad probabilities, which means that given rectangle in our net doesn't contain any points on B. And then we need to find, of course, good bounds because we will use union bound, which means that we take a sum of bad probabilities. So essentially, we multiply the probability of bad event individual probability by cardinality of net. And then, of course, we want to get something. And then, of course, we want to get something which is less than one. So, we need to find balance between individual probabilities and cardinality of nets. It's essentially a standard way of proving such things. And here, what is good is that to count probability is very easy. Have rectangular box. Probability is just Lebiac measure, so we take product of length. So, it corresponds to volume of B0. So, we need that volume of B0 is not small. So, we need that volume of B0 is not small. And the main problem is to construct a net of not so big cardinality. And this I will try to explain. So, Rudolf used concept of so-called delta cover, which was which was obtained by constructed by Gnevic and for different, completely different reasons. And in fact, it's It's much more precise approximation than it's needed in this problem, and it's why it has not so good cardinality. So, we improved this, and actually, our approach is, well, it's much better for this problem. So, what we will do, we essentially will do what I say here. So, we construct net such that for every box. Such that for every box in Rz, there exists a box in our net such that it is inside and volume of B0 larger than some given parameter, larger or equal. And this we will call by delta approximation. And then it's relatively easy to prove that if we know size of net, then we know the number of points. So it's behaves. So it behaves like a logarithm of cardinality divided by delta. And the proof of this theorem is pretty standard to just use union bound. I probably will own it. And if you want to do it, it's just straightforward. But what is important, important here is that we see before I say that we need to have some balance between cardinality of net and volume. So here we see that the cardinality. So here we see that the cardinality of net appears with logarithm and volume, but volume of approximation. That's why I want that volume of B0 is not very small because it appears in denominator and of course I don't want to spoil anything. Well I will probably skip proof of the lemma. Let me say a few words about construction of the net. The net so uh it will be done in two steps. First, first we approximate uh so-called uncore box. So instead of considering all possible boxes of volume epsilon, we take only a size that zero is a vertex. So with vertex at a region. And of course, if I have a size box, it's just product of segments. Product of segments zero bi, so I can identify them with the upper right corner. So it will be easier to think about these boxes as a set of points B in the cube, such that product of coordinates is larger than epsilon, but in fact, it's enough to consider only. It's enough to consider only boxes of volume exactly equal to epsilon because I can decrease. And what we will do, we will consider the following set. So set of B such that the product is fixed. Actually, I will need epsilon to beta and for beta, I will choose, it will be two choices: beta equals one and beta equals. beta equals one and beta equals one plus gamma. So beta equals one exactly corresponds to set of boxes which I want to approximate. And when beta equals one plus gamma, it will be my B0 boxes. So volume will be volume of this B0, which I mentioned before, will be epsilon to the one plus gamma. So epsilon to the gamma times epsilon. Epsilon is exactly volume that I want to get at the end. That I want to get at the end. And this will be error, so we'll not do calculations, but at the end, of course, I will choose gamma is like logarithm of one over epsilon to kill this part. But for us, it's just this. And so what I want to do, I want to construct an approximation. So I want to find N0, which my boxes, but I identified boxes with upper right corner of. Right corner of such volume epsilon to the error times epsilon such that for every box, initial box, I can find box and then zero. What I need that I need that B0 is inside B, which exactly means that all coordinates of A less we call them coordinates of B corresponding coordinates of B. On picture something like this: this is A. A and to do this, we apply Logariti transformation because it's not nice to work with products, it's easier to work with sums. So, we will use this function which is written here. And what this function does, it applies coordinate-wise and it transforms point x with coordinates xi to f of xi in this way. Why it's good? In this way, why it's good? It's good because instead of product, we have sum, and some sums here essentially says that sum of coordinates lies in hyperplane, which well, sum of coordinates equals to fixed number. So, but because each of them, all of them are. Because each of them, all of them are positive, it gives me simple x. So, in fact, what is going on is that I have a set of points B such that product of B equals equals to epsilon. And I send this using my function f to points which lies here, sum of xi, in this affine plane, sum of xi equals to beta. To beta either one or one plus gamma, and here I have just simplex. Well, and now what property we need? We need, well, my initial set of initial b's of volume one of volume epsilon to power one, it comes to simplex sum equals one here, index i, of course, positive. Integer, of course, positive. And what will be my approximation? I will take beta equals one plus epsilon. So approximation will be here. So, in fact, on these pictures, let me erase something. We'll have the following. So, I have sum here equals to one. Here equals to one, have another simplex here, one plus gamma, and I want to find points here which cover and how they cover. I need to have that coordinate yi larger or equal than xi. So each y can cover something like this here. So if well, if you apply some imagination and it's quite easy to. And it's quite easy to see that what I really need, I need to cover simplex by smaller minus simplexes. Something like this. Such bounds are very well known in convergence geometry, so it gives this bound for anchored boxes. Then I probably skip next part. I will just say one word probably that what we will do next. That what we will do next, next one. If you have a box 0, 1. We know how to cover such boxes, but we need to cover all possible boxes. What we will do, we will shift it here and then use approximation which is good for this anchored box. And then we need some net for all possible shifting. So, this will be additional. So this will be additional, this will give additional cardinality and then we will multiply cardinality of these possible shifts of my box by my net which I constructed here. So these are ideas for construction. So I will probably skip set two and I will say a few words about about about case of large epsilon why uh why uh why lattice distribution worked better than the uniform distribution so well probably there are many reasons but one of them in my opinion is the following that in fact when i use some latest points and take only them And take only them. I will never come close to boundary because I don't have, I don't take lightest points on boundary. On the other hand, when I use Lebesgue distribution, it's known that I will be essentially I will be very close to boundary. So this makes difference and this leads to the following construction. Another reason for contraction. Contraction is following that I don't need really points on the boundary, close to boundary, because if I have a box of volume epsilon, the worst case from this point of view, it will be something like this. So I always will have point which is a distance at least epsilon from the boundary. Don't need to take points here. I only need to take points here. So this leads to adjusting of uniform distribution. Distribution. I will take this box one minus epsilon inside. And if my point falls down here, I just shift it here. Or from here, I will shift it here. From here, I will shift it here. So I will use Lebesgue distribution, but I will slightly adjust randomness. I will not allow a point to be here. So because Liebeau distribution, of course, on Course on the cube is a product measure. So, what I will do for each cohesion, we'll do the following: here is epsilon, here is one minus epsilon, and I have a random point. If I had felt here, I will not change anything. But if it is here, I just move it here. If it is here, I will move it here. And in fact, it works and uh gives uh gives a result. Gives the result. So, here I just written formally what I explained in the picture. Why it gives the result because if I have box of volume epsilon, it means that product of lengths of corresponding segments larger than epsilon. But then it means that many of them, many of L i's will be larger than one minus epsilon. larger than one minus epsilon so uh and if my l l l j is larger than one over epsilon it means that after such adjusting i always will have point inside so i don't need to construct nets for for this case and this essentially just reduces dimension to so i don't need to go uh to to higher dimension To higher dimension. At this moment, I will stop. And this gives improvement for large epsilon. So I'll probably stop here because I don't have time. Thank you. Any questions or remarks? So we thank you again. So we thank you again and