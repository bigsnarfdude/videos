Thank you so much. Yeah, so for those of you who don't know me, I'm Dan Fortunato. I'm a postdoc at the Flatiron Institute. And today I'd like to talk to you about some work that I've been doing recently on developing a fast direct solver for solving PDEs on surfaces. And so, why might you want to solve a PDE on a surface? Well, surface-bound phenomena really. Surface-bound phenomena really come up in many applications across a range of fields. So, in fluids problems, for instance, you might be simulating a soap bubble and have thin film hydro dynamics that you have to consider. And it turns out that solving PDEs on surfaces is kind of a very important task to get an accurate interface representation in this fluids problem. You might be on, you know, on the surface of a zebra and On the surface of a zebra and be a biologist who wants to simulate Turing patterns to see how, say, spots and stripes form on an animal. Or you might be designing a stellarator, and it turns out that the surface of this stellarator actually plays a very important role in what's happening on the interior. And there are really problems across biology and electromagnetics and fluids where geometry. Where geometry, for instance, in this cell polarization problem, really has an influence on how things move around. So, for instance, how proteins coalesce and move depending on surface curvature. Now, surface PDEs are essentially just the analog of normal PDEs, as if you're an ant living on the surface. So, surface PDEs describe the dynamics of phenomena that happen on a surface. That happens on a surface. These could be steady state dynamics or they could be time dependent. In the steady state case, you know, you just like in the flat plane, you have some maybe elliptic PDE LU equals F, where now this L is a differential operator that's acting on the surface. So this could be, for instance, the Laplace-Beltrami problem or a steady Stokes problem on some surface. And if you have some time-dependent problem, it could have. Some time-dependent problem, it could have some linear and non-linear parts where now this linear part could be you know a surface differential operator. So this could be some reaction diffusion process happening on a surface or some Navier-Stokes problem. And often you can recast the time-dependent problem as a steady-state problem at each time step through implicit time discretization. Discretization where at each time step you might have to solve some modified surface PDE. And so in this talk, I'm just going to consider kind of the model of surface PDE as a second order linear elliptic PDE on a surface, where you might have some variable coefficients, a, b, and c, that vary as you move across the surface. And now these divergence and gradient operators are kind of the analogs of the normal vector. kind of the analogs of the normal vector calculus operators to to to the the surface that you're on and if your surface is not closed you might also have some boundary conditions on the edges of the surface and so how do you do vector calculus on a surface so what are these differential operators well say i start from some reference square if i know in some local coordinates c and eta how eta, how some reference square maps to a patch of my surface. So if I have access to some mapping x here, well then I know if I'm an ant living on this surface, then as I'm as I move along, kind of the notion of lengths and angles are changing. And how are these encoded? Well, they're encoded through what's called the metric tensor, G, which is just simply a product of the derivative. Simply a product of the derivatives of this mapping in the xi and eta coordinates. So then what's the surface gradient? Well, it's just the flat gradient, so grad with xi and eta times the variable coefficients that are induced from this metric tensor. So you can kind of go through the machinations of the chain rule and some algebra to derive vector calculus operators. Vector calculus operators. Similarly, the surface divergence is just, well, it's the flat divergence pre and post multiplied by some other variable coefficients coming from the metric tensor. And so then you can form the surface Laplacian or the Laplace-Beltrami operator as just the surface divergence of the surface gradient. And so this is, you know, it's just the flat divergence of the flat gradient, but there's some variable coefficients stuck in there coming from. Stuck in there coming from the mapping. Similarly, you can define the Cartesian components of the surface gradient. So if I'm an ant on the surface and I want to know how things are changing tangentially in the x direction, for instance, then I could just look at the dot product of the surface gradient with the Cartesian X vector. So this is kind of a high-level overview of how. Level overview of how calculus works on a surface. In practice, we want to compute with surfaces, and there are many ways to do it. You could use a level set or you could use a point cloud. But in this talk, to have a really direct analog with that mapping idea from a reference square, I'm going to think about quadrilateral meshes of a surface. And meshes are a nice choice for CAD compatibility. In particular, In particular, there are some choices that you have to make when you think about a mesh of a surface. So there's kind of a range of course to find that you can tune in your mesh. So how many elements do you want to use to resolve your surface? But there's also, you know, kind of the orthogonal knob of what polynomial order do you want to use on each element to resolve the surface. So for instance, if I just have a flat mesh of For instance, if I just have a flat mesh of a sphere here, well, you could think about this as linear polynomials interpolating on each of these quadrilaterals. But I could do, you know, quadratic or cubic or other higher order versions to get a high order surface mesh. And depending on what order I choose, I can actually get high order convergence to my surface PDE solution, as we'll see later in the talk. Later in the talk. Now, you know, there are limitations to what CAD software can do. So, for instance, you could make a NURB surface or a subdivision surface to get maybe a bicubic representation on each element, or you could use surface smoothers and the like to get higher order representations. But in all of these cases, I'm just going to think about being given a set of quadrilateral patches, and on each patch, I'm going to. Patch, I'm going to have access to the surface coordinates at the set of tensor product Chebyshev nodes with each of these quadrilaterals. So my kind of surface representation is just going to be a collection of sets of tensor product Chebyshev nodes defining each of these patches. Now, why might I choose Chebyshev nodes? Well, this will then allow us to just easily apply standard spectral. Easily apply standard spectral co-location at these same Chebyshev nodes. So, if I store function values at these Chebyshev nodes, and say in 1D now, I'm thinking of being given function values at all of these red nodes. So I have values u at all of the xj points, where these are 1D Chebyshev points. Well, then, okay, I can form a polynomial interpolant of these function values to. Values to get a polynomial defined over this entire interval. And then I can take the derivative of that polynomial. It's now a polynomial of one degree less, which gives me this blue line. And then I can sample that blue line at all of those same nodes to get values of the derivative. And so I can encode this discretely as a matrix called the spectral differentiation matrix D, which just maps a vector of function values at nodes to a vector of derivative. nodes to a vector of derivative values at nodes. And in 2D, you can go through the same thing. You just get tensor products everywhere. And I can now write down matrices D cron I and I cron D, which are the analog of spectral differentiation in the C and eta directions on my reference square. Now, okay, so that's how you do spectral differentiation. Okay, so that's how you do spectral differentiation on a reference square, but we're thinking about a collection of patches on a surface. So I can just simply take my set of nodes on this surface, which define a coordinate mapping, and just go through all those discrete vector calculus formulas to define discrete surface gradient, discrete surface divergence, discrete Laplace-Beltrami operators. So, for instance, the Cartesian X component of The Cartesian x component of the surface gradient, so the tangential x derivative operator is just the, well, it's the c derivative operator and the eta derivative operator times some matrices which multiply by the appropriate coefficients coming from the metric information. And so, in general, if I'm using degree p polynomials in the x and y directions, The x and y directions, then I, okay, on this surface patch, I can write down a p plus one squared by p plus one squared linear system, lu equals f. And if p is not too large, say 16 max or something, then I can invert this directly. And so this allows me to solve a surface PDE on a single quadrilateral patch. Now, what happens if I want to solve it on two patches that are glued? Want to solve it on two patches that are glued together. So I have domains one and two, where on one, I'm solving my surface PDE LU1 equals F1, and on domain two, I'm solving LU2 equals F2, subject to, say, some boundary conditions G1 and G2 on the outside. Well, we know that, at least for second-order linear elliptic boundary value problems, what gluing means, it means that we have to match. It means that we have to match function values and the derivatives or the binormal derivatives across this interface. And so discretely, well, just like on the last slide, we can form linear systems on each of these quadrilaterals, LU1 equals F1 and LU2 equals F2. And now, hopefully, the color is coming through. I can now highlight interior nodes and Interior nodes and boundary nodes. So the interior nodes here are blue and the boundary nodes here are red. And just re-index things around. So I can put all of the blue nodes at the top of my linear system and put all the red nodes at the bottom. And then I can take what's called a sure complement to now write down operators which act only on the red notes. So for instance, I can encode. So, for instance, I can encode how to solve my PDE. So, this operator is often called the solution operator, which just takes in boundary values on the red nodes of each of these squares and gives me back the blue nodes on the interior as if I had solved LU1 equals F1 and LU2 equals F2 with the given boundary data. I also know how information is flowing out of each element once I've solved. out of each element once I've solved. So this is what's called the Dirichlet to Neumann map. And this is simply just taking in boundary data on each of these quadrilaterals, solving my PDE, and evaluating the normal derivative on all of the edges of these patches. And in particular, I'm interested in enforcing continuity of the normal derivative across this glue interface. And so I can then immediately write down using these operators. Write down using these operators a linear system which enforces continuity of the normal derivative to arrive at an operator which now acts on these blue nodes on the glue interface. So I can write down an operator s glue, which takes in all of these six vectors of boundary data coming from the outer boundary of my two squares and gives me back the solution having imposed. The solution having imposed these continuity conditions across it. And then once you're in this setting, you can immediately just recurse and keep gluing things pairwise. And so this is exactly what Gunner and Adriana did with what's called the hierarchical Poincar√©-Steklov method, where they recursively glue squares together. So now you could think of this as a mesh of a surface, but it's easier just to draw in the plane. So if I have So, if I have four squares here, well, I can construct the operators which act on these blue nodes and red nodes in, well, order n time if I have n squares. And there's actually a constant in here which scales like p to the fourth, but if p is 16 or less, I'm going to assume that that's not important here. I can then take a sure complex. I can then take a sure complement on each of these squares to reduce the linear systems to degrees of freedom, which only act on the boundary. And then I can just keep merging operators like on the last slide, taking sure complements of each of the pieces to eliminate interior degrees of freedom until I'm at the top level, where, say, I know Dirichlet data on all of the sides of this square. Sides of this square. And I can then inject that information and propagate it back down the tree and apply my operators in a tree-like fashion. And so this upwards pass, which I'll call the factorization, has this n to the three halves scaling where, well, the three halves is coming from the fact that at the top level, you have to invert a linear system that's like square root of n by square root of n, where n is the number of elements. The number of elements, and once I've factorized, though, I can solve my problem in order n log n time by doing this downwards tree sweep. And as we all know and love from fast direct solvers, after factorizing, I kind of have a representation of my solution operator stored in some fashion in my computer. And so I can repeatedly apply it for very fast solves. And this will come. Fast solves. And this will come in handy when we talk about time-dependent problems soon. Now, on surfaces, there's kind of a acute fact that arises. So it's known that the Laplace-Beltrami problem, as written here, on a closed surface is actually rank one deficient, but it's uniquely solvable if, well, if you give me a mean zero right-hand side and zero right hand side and tell me that you want a mean zero solution u. So it's uniquely solvable as a map between mean zero functions. And what's kind of cute in the HPS framework is that this rank deficiency is actually only seen in the final gluing. So if I've if I've now you know merged operators until I have two halves of a sphere here, and then at the top level, I'm gluing these two halves of the sphere together. Sphere together, well, the linear system that I have to invert on these red nodes here is actually rank one deficient, just like my Laplace-Beltrami problem. And so to fix this, we use what's called the ones matrix, which essentially just imposes the mean zero condition at the top level. And this is just kind of a cute fact, I think, that you really only see this condition in HPS until the surface becomes a closed surface. Closed surface. So, what does convergence look like in this setting? So, if I will say I just take a standard cubed sphere mesh, and well, this is easy to get a high-order mesh out of because I know exactly how to form it. And so I can form a mesh to any order I like. And I know that if I, well, if I set F to be the appropriate spherical harmonic, then the solution is also. Harmonic, then the solution is also a spherical harmonic. And so for different orders of mesh degree and solution degree P, I get different orders of convergence to my known solution. And so as I refine my mesh here, so as I'm going from a coarser high order mesh of a sphere to a finer high order mesh of a sphere, you can see my maximum pointwise error is going down. Errors going down at different speeds, and so in each of these cases, you get about order h to the p minus one convergence. So, for instance, this green line, which used 20th order polynomials on each quad, is getting h to the 19th convergence. Now, I wanted to compare the fast direct solver in terms of accuracy versus effort to a few different Laplace-Beltrami solvers out there. Laplace-Beltrami solvers out there. And nicely, this paper by Daria Malhotra has really extensive timings about these two Laplace-Beltrami solvers applied to this stellarator geometry. One solver from Mike O'Neill and one from Lise Marie and Leslie. And so here I'm plotting on the Y-axis, you know, the accuracy that I've requested, and on the X-axis, the amount of effort. On the x-axis, the amount of effort I had to spend to get that accuracy in terms of computational time. And so, you know, at least for this, you know, this small to medium-sized problem, which had, I don't know, maybe 60,000 degrees of freedom or something, the performance is actually good. So, you know, I think coming out of our panel discussion this morning, I think there's definitely a space for existing fast direct solvers for problems that kind of fall into this niche of being. That kind of fall into this niche of being small to medium size. And surfaces are kind of this nice example of, you know, having all the visualization of a 3D problem, but really being psychologically 2D. And so you kind of can get away with solving medium-sized problems. Now, nothing about this gluing really had to use smooth surfaces. It was kind of, you know, the glue conditions are kind of agnostic to having a. Are kind of agnostic to having a sharp corner or edge. So, for instance, here I'm solving the Laplace-Beltrami problem on the surface of a cube. So, this is not a 3D cube that's been divided into smaller cubes. This is the surface of the cube. And so, just like solving Poisson's equation, maybe on the square, you might get some weak corner singularities in the corner of the square. You can also get weak corner singularities in the corner of the Laplace-Beltrami problem. Corner of the Laplace Beltrami problem on the cube. And so, you know, this is just uniformly refining, but you obviously would want to do some smarter adaptive or HP refinement into these corners. But obviously, you know, in these settings, high-order convergence is probably lost. So in all these cases here, I'm using third, fourth, fifth order polynomials on each of these patches of the square, but I'm still getting second order. Of the square, but I'm still getting second-order conversions, obviously, because my solution has these weak corner singularities. Now, solving Laplace-Beltrami problems can allow you to compute some different quantities. For instance, if you'd like to compute the Hodge decomposition of a vector field, so if you give me a random vector field f that's tangent to my surface. Surface and you want to know the part of it that's curl-free and the part of it that's divergence-free, and the leftover bit which is called harmonic. So, as this illustration nicely shows by Keenan Crane, taking a vector field and composing it, decomposing it into divergence-free, curl-free, and harmonic parts. Well, this problem can actually be solved by doing two Laplace-Beltrami solves. You give me F, and then I can give you U and V. Me f and then I can give you u and v by solving these Laplace-Beltrami problems. And then this leftover bit called the harmonic part is just what's left over after I subtract off those pieces. And so here's an example of taking a torus geometry with a random tangential vector field and exactly solving these two Laplace-Beltrami problems. And what's nice is that you can get six. You know, six digits of accuracy, say, in less than a second for these types of problems. Now, nicely, Manas also gave me some timings from his recent paper, which also computed Hodge decompositions on this geometry. And again, putting this on this accuracy versus effort plot, you can see that, well, the fast direction. Well, the fast direct solver again for these small to medium-sized problems really is a big win. Now, for time-dependent problems, as I said on the first slide, you can often recast them into a steady-state problem. And this is often very useful for problems like reaction diffusion problems, where you have some diffusion term with one time scale and some reaction term with another time scale, and those times. Action term with another time scale, and those time scales can be orders of magnitude apart. And so, implicit time stepping, usually on the diffusion part, can alleviate a lot of stability issues and allow you to take larger time steps. So, for instance, if I take my time-dependent problem and use a backward Euler scheme, then, well, as long as my time step, delta t is not changing and my geometry is not changing, and Is not changing and my parameters are not changing, then I can compute a fast direct solver for this operator once, and then at every time step, I can apply it again and again in a cheap way. And you can do the same thing with higher order time steppers, like a BDF-4 scheme, for instance. And so, here I'm going to solve some reaction diffusion problems on the surfaces of these various shapes. I think the degrees of freedom range from maybe I think the degrees of freedom range from maybe 40,000 to like a few hundred thousand. And I'm going to solve two problems: one that will form spots and one that will form spirals. And so I'm going to play this movie and hopefully the frame rate is not too bad over Zoom. And so this is actually, these actually run quite fast once you've computed the fast direct solver. Direct solver, these ran at about, I think, 10 frames per second or something. And the bottleneck, at least on my computer, was actually just plotting the solution of these things. So I think the main takeaway is that fast direct solvers are really useful when you kind of have this small to medium size setting. And I mean, all of this I implemented in MATLAB. And so And so this isn't some especially sophisticated code. So there's, and there's, there's definitely speed gains that could be could be had here. So if you'd like to use this code or chat about it, I'm happy to. And you can get it at this repository called Surface HPS. And it provides a lot of Chebfun style abstractions for computing with functions on surfaces in MATLAB and the associated FastDirect solver that goes with it. So thank you. So, thank you. Thank you. Any questions? I have a question. So, when you solve reaction diffusion equation, you at one stage you are computing. You are computing that inverse. So that means you are going to store it. And these are before you invert that metric, that metric is sort of sparse, right? Now you invert it, then store it, you will have a dense matrix. Right. So this is not forming the dense inverse and storing the dense. The dense inverse and storing the dense inverse. This is storing the tree of operators. So it's kind of stored in a sparse way. So you store operators that act on each of these red and blue pieces. So it's a composition of things in a tree. And so you can apply the inverse very fast once you've stored all those pieces by composing them in the right way. By composing them in the right way, but you're not storing one big matrix, you're storing a tree of small matrices. So, so that means you still can handle a very large scale problem? Right. I mean, you know, how large in the problem you have handled here? I've handled the largest problem I've handled has about a million degrees of freedom. Okay, okay, thanks. Yeah. Any other questions? Hi, Dan. Nice talk. Thanks. What sparse solver are you using? What sparse solver am I using? To build the direct solver. So there's no sparse solver here, right? So at each of the leaves, I'm just using like an LU factorization. And then at each of the merges, that's also an LU. So it's all n to the three. An LU, so it's all n to the three halves. Uh, yeah, there's definitely some tricks you could do to exploit sparsity of communication between far elements. And you could do some off-diagonal low-rank techniques to compress these things as well. But this is all just using kind of naive linear algebra. So there's no sparse solver here. So you could formulate your HPS. So you could formulate your HPS discretization as a large block sparse system. Sure. In principle, you could throw that at some standard sparse package. But yeah, what you're doing is just doing the manual base action elimination, basically. Yeah, so it's like I'm choosing the pivots and manually inverting these things with small dense solves instead of giving it to one big, large, sparse solver. Okay, thank you. Yeah. Okay, thank you. Yeah. Do you have any thoughts about how to improve the speed of convergence for a cube if you have edges? I don't actually. I mean, I'm sure that Mike and Tristan, who have recently been thinking about corners and edges, have some thoughts about this. But I mean, at least coming from doing HP adaptivity-like things on the square with corner singularities, I think the best thing to do would be. I think the best thing to do would be to have large elements away from the corners and small elements near each of these corners and use high degree P away from the corners and low degree P near the corners. So do some sort of HP adaptivity scheme into the corner. But I just haven't implemented that in my code. Great. I'm biased, but I thought that was an amazing talk. Thank you very much. Thank you so much. Thank you very much. Thank you so much. All right.