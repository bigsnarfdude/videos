Okay, thanks a lot for giving me also the opportunity to speak. It's been a very, very interesting two days so far, and I really enjoyed and learned a lot in the first two days. So I'm going to try and add a little bit to it. It's a bit daunting, but I'm going to follow on very closely from what Yusef Mazouk spoke on yesterday. Uh, yesterday, and uh, so I'm originally a numerical analyst, and I work on algorithmic things and uh and also the complexity analysis of such algorithms, making them efficient in high dimensions. This is one such methodology that I want to present for sampling, sample-based inference in high dimensions. So, Bayesian inverse problems in high dimensions, and it's joint. And it's joint work with Karim Anaya Eskero from Bath and Sergei Dolgov, who has been the central person in this research, also from Bath, and then Tiang Sui, who has also been already mentioned by Youssef because he's been collaborating with both of us. Colin Fawkes, and then one slide will be on work, joint work with Lars Grazedik from Aachen and Harl Robert who is a PhD student in Cambridge that I'm co-supervising. Cambridge that I'm co-supervising. So you've seen this slide before because, and I put the copyright to Tiang Yang Sui so that you understand why I have the same slide as Yusuf. But it's basically from TC's PhD thesis on geothermal. And we are looking at a typical high-dimensional or PDE-based inverse problems where we have a We have a parameter that's function-valued. So it would be in this case the heat distribution and/or the permeability in the subsurface. And the data could be a time series of the steam or the water coming out of this well that we see here. So, from a mathematical point of view, we very often faced with the situation where we have, especially in engineering, Where we have, especially in engineering applications, where we have limited noise and indirect data. For example, some vector of finitely many data measurements. And we have a parameter that's strictly speaking function valued, but of course on the computer we will discretize. And then the forward operator would be a map from x to this data. And I'm going to mainly focus on this simple additive model. Model from a statistical point of view, where I say my data is my observation operator plus some model errors and observation errors. So the next few slides will be very familiar because they've been very nicely introduced by the speakers before. We look at the Bayesian interpretation of such an inverse problem, where the forward problem would be the conditional probability of Conditional probability of the data given the parameter, and that would be the likelihood. And in practice, one and in particular for this, what's important for this workshop, the inverse problem is often of much more importance or interest. And that would be the conditional in the other direction, the conditional of the parameters, the causes, given the observations x, y. And this can be framed. And this can be phrased as via Bayes' rule as a proportionality between some prior distribution on the parameter x times the forward conditional, the likelihood. And therefore, we can try and infer things from this about this parameter or functions of the parameter. or functions of the parameter by looking at this, computing this or sampling from this posterior distribution pi pos or pi x given y. And then the main task from a computational point of view is the calculation typically of expectations of under this posterior because we can't really work with distributions, we can only work with either with samples representing this distribution. Representing this distribution or with certain statistics about the distribution. And so we have some integrals with respect to the posterior distribution for some functions h. And I will particularly focus on the case where h is not smooth, for example, failure probabilities or CDFs. So, how is this linked to the classical view? So, in the classical view, you are phrasing the question very similarly, but it's more Question very similarly, but it's more a question of opposedness due to lack of existence of a solution, uniqueness or boundedness. So these stability questions that we have already addressed in previous or have been addressed in other talks, and also the ways how we compute are related. So maximum likelihood estimates are synonymous, at least in the simplest case with least square solutions. The prior distribution is in some sense the regularizer. Some sense the regularizer and guarantees well-posedness of the problem that we want to solve, and the maximum apostrophe estimate would be the least square solution of the regularized problem. But what people are interested in and why people are so interested in these statistically inverse problems is that the posterior, of course, especially in the non-linear setting, contains much more information than the map. And even in the linear setting, Gaussian setting, you already have in the Gaussian setting, you already have in the posterior covariance matrix components of x that are more or less certain, but in the more nonlinear setting, there is even much more in general in this posterior distribution if we are able to characterize it and compute it. So the challenge, on the other hand, is that when we are faced with what most inverse problems are, infinite dimensional, because otherwise they're just still conditioned. So the problem is The problem is one of high dimension. And therefore, it is, for a start, expensive to compute the likelihood, but this could be also because there might be a PD in the background that you need to solve or some other model. But also, sampling algorithms struggle in high dimensions or become more expensive. And if we especially look at the case of concentrating posteriors, when we Of concentrating posteriors, when we converge, when we get more and more data, the issue with the normalizing constant becomes bigger and bigger. It basically becomes more and more unstable to work with this. So there is a so-called curse of dimensionality due to the dimension. And for many situations, the only way to break this curse is to use a sample-based approach. That's why I'll talk about this today. About this today. So, here is the traditional workhorse. It's the MCMC, the Markov Check Monte Carlo method. I don't think I need to explain it in much detail anymore after the first talks, but in principle, we start with a state x naught. We propose via proposal distribution a new state x prime. And that could be as simple as a random walk, but it could be something very sophisticated, as I'll show you in my talk. Show you in my talk. And then we accept and reject this proposal according to an acceptance probability that's stated here, which guarantees that as we under certain conditions of erodicity, properties on this distribution Q and the distribution pi, it guarantees that this converges to the correct distribution and that we can. Distribution, and that we can use the samples for inference. Even if they're not IID, they will still give us a correct estimate of the expected values under the posterior. Okay, so now this is the traditional workhorse, but the problem is, and this is not a completely foolish version of it, this is already one where one computes first the map point and then uses some algorithm to. Some algorithm to estimate the covariance. So, this is DRAM, just so that if you know that, delayed rejection adjusted metropolis. But you see already that this is very inefficient. You need a lot of samples. And even after I've shown those samples here, we haven't really explored some of the yellow parts, which are high probability regions of this distribution. So, what would be really nice is if we could have a sampler that does something more like. Have a sampler that does something more like this. So, this is what's in the background are isolines of the distribution or a two-dimensional projection of an 11-dimensional distribution that we want the sample of. And we would like to get better proposals because in any case, we have a slow convergence of order n to the minus a half. And what I'm talking about here is the constant in this O. So we have the slow convergence plus this slow mixing that we see. This slow mixing that we see in these pictures. Okay, what would be the alternative if we did not go along those lines? It would be, for example, to use variational Bayesian approaches, and that's what Youssef talked about yesterday at length. So we could try to analytically get our hand on this posterior distribution or at least approximate it. The challenge is, of course, identical. We have high dimensions. Have high dimensions in a non-linear setting. The posterior is non-Gaussian, and the likelihood is still expensive to evaluate if it relies on a PDE. But on the other hand, we have a lot of tools that we can use. We can use optimization, principal component analysis, mode load reduction, and then a few more alien terms that are sort of part of my talk: transport maps, low-rank up extension. But from my point of view, it's a playground for us. My point of view, it's a playground for us numerical analysts. This is sort of our tools that we have sharpened and that we want to use. Okay, so here is the approach that Yusuf. Please stop me if I'm going too fast, but this is essentially following Yusuf's talk, so I'm going a little bit faster to spend more time on my part. But if there's questions, please do stop me. So, this is this approach that Yusuf described yesterday already, namely, we can try to Namely, we can try to do to compute transport maps, couplings, deterministic couplings of probability measures or of the densities in a finite dimensional setting. So if we have a reference distribution eta, given like a uniform Gaussian on the left here, we can try to find a map that maps this distribution to our target distribution. So a map from Rd to Rd, that is the diffeomorphic. That is a diffeomorphism that's smooth, so we have a lot of freedom. I mean, this is one of the challenges. There's a lot of freedom how you can move one of those distributions to another. But if we can compute something like that, and then we can push forward the measure, or in more algorithmic terms, we can just map the points from one distribution to the other. And in principle, this enables you to do exactly independently and weighted sample from the distribution path. And even if you cannot compute it exactly, but only approximate it in a suitable hypothesis class, H, it's still useful. And I just put one term. There's many terms for saying why this is useful, but I would call it preconditioning. So we can essentially get ourselves a handle on some very efficient. On some very efficient, very effective precondition for our other numerical tools like the MCMC algorithm, the Metropolis Hastings, I showed you before. So here are typical hypothesis classes and typical methodologies that have been studied a lot. So I should say, especially the second item is an absolutely exploding field, normalizing an optoregressive flow. So basically, the machine learning literature is getting full and full. Is getting fuller and fuller off because, of course, neural networks and other machine learning tools are ideally placed to do a high-dimensional function approximation or high-dimensional map approximation. And so there is a huge literature out there and we have made our little contribution there as well. But what I'm more focused on is this Note-Rosenblatt rearrangement that Yussef already mentioned. Then there is, we've already heard as well on the first step. We've already heard as well on the first day, there are kernel-based methods, and in one specifically, the Stein variational gradient methods or Stein variational methods, which we have also done some contribution to. But the one I chose to focus on because it is a little bit more alien to community, and I wanted to highlight it because I see a lot of potential, is to use tools from computational chemistry, computational physics, low-rank tensor approximation. Low-rank tensor approximations to approximate such maps between measures or between densities. And in particular, I will talk about one. So the red papers are the ones I'm going to focus on and present you. Okay, so what's the classical approach to variational inference? And I'm being a bit specific because in my last talk to statisticians, people asked me afterwards. So I mean. Me afterwards. So, I mean, I'm just bringing this up because this is how normally variation inference is posed. It's often as a minimization problem of finding, and very often the Kulberg-Leiber divergence, although it could be another distance or divergence. But one tries to find a suitable T, a suitable map in this hypothesis class, in this type of approximation. Class in this type of approximations that we aim to use, that is, in some sense, an approximation or the exact minimizer of this distance, or this Kubernetes-Labber divergence between the push-forward T-sharp of eta, of the reference, and the target. And vice versa, sometimes it's phrased the other way. One can also minimize. One can also minimize the pullback of the target and the divergence of that with the reference. And this is... What was eta again? Eta is in this case the reference. So think of it as a standard Gaussian, for example. But we actually, in our setting later on, we use uniform on a unit cube. And the And the task then for most of these classical variational inference algorithms is either to use a hypothesis class where this minimization becomes very easy, or to just brute force try and solve. But a particularly useful class that was already mentioned by Yusuf yesterday is that of a triangular transport. So there's, I mentioned optimal transport at one point before. Optimal transport at one point before, but it's not really so important, although it can be optimal transport, but it's not so important that we find the optimal transport of our reference to target measure. We just need to find a good transport that's easy to compute and to apply. So, one such is the Rosenblatt triangular transport. So, I'm phrasing it here as a transport problem, as a map to fit in with the rest of my. map to fit in with the rest of my talk so far but on the next slide you will see that it's something the statisticians will see that it's something very familiar to you and so i uh i will and and and yusuf gave my my uh gave already the spoiler yesterday so i i guess i'm not hiding much but uh but but from from a purely uh mapping point of view you see already one nice feature and the one nice feature is that the log determinant of the of the jacobian of the Of the Jacobian of this map is essentially the sum of logs of univariate of these maps with respect to the respective argument. And so this is a lot easier to compute than many other log determinants that you might have with other maps. This is used a lot in the machine learning literature in auto. Machine learning literature in autoregressive flow because that makes it easy to design good neural networks for it. So, can I just ask a stupid question quickly? So, you would be optimizing then over all these D functions of increasing number of variables, but you don't because it's triangular, you can sort of do it in a go through it one by one in some ways. But you don't constrain the t's any further, or no, it's it's let's. No, it's. Let me show you the next slide. Then you know why this is a reasonable suggestion. Note the Rosenblatt transport is nothing else than what in statistics is called conditional distribution sum. So any density under some minor conditions about the smoothness of the density can always be written as the product of conditionals. And that's basically the market. So I'm going to show it in a bit more detail now, but that's how you construct. A bit more detail now, but that's how you construct the map. So, what I'm saying here is that there exists, in fact, a unique triangular map that satisfies this. And it is the conditional distribution sampling method. So here I'm just calling the first one the pi 1 of x1. Pi2 is then the conditional of x2 given x1 and so on. And these individual pi. And these individual pi k's can actually be written down. So once we have this, I think I have even a formula later on. Once we have this, we can then sample from these individual conditionals. And so the first step, yeah, I have more detail. So the first step would be simply you take the marginal. You take the marginal with respect to all of the letter variables, so the marginal of the first variable, integrate out all the letter variables. This would be pi one. And then I think everybody can see quite easily under certain smoothness or monotonicity conditions, the 1D CDF inversion, the cumulative density inversion of a 1D distribution function is numerically very easy. Is numerically very easy. Okay, so it's only when you go to high dimensions it gets difficult. So, so you have to always take my word for it that from now on, one DCDF inversion is for me a solved problem. I'm just going to take some method off the shelf. Then in the kth step, I've already sampled my k, my one to k minus one samples, the first k minus one components of my sample, and I now have my formula, namely. Have my formula, namely, I integrate out the remaining variables from k plus 1 to d. I fix the first k minus 1 variables, and I have again a one-dimensional CDF, and I can again do a one-dimensional CDF inversion. The only problem, sort of the usual snakes and ladders back to square one, we still have to solve integrals. I mean, especially in the first one, we have to solve a d minus one-dimensional integral. Dimensional integral. So we haven't really, we've just spitted ourselves in our own tail. Unless I do now what will be the rest of my talk, I approximate the pi in such a way that these integrals become cheap or become doable in a complexity that's only linear in B. So we will find, we will not use appropriate. We will not use approximations of our. I'm not going to use the variational inference idea in terms of then just analytically scribbling my method. I'm going to use it to design an efficient conditional sampling, distributional sampling algorithm. And therefore, I can then actually sample, I can then still do the biasing in the same way as you would do with other with other variants vector, I mean, other pre-existing. Variant spectrum, I mean, other preconditioning techniques, for example. Okay, so here is the methodology that I am promoting. It is a low-rank tensor approximation method. Okay, so this is in itself a big field and has a huge literature, and it is not trivial. So I will not be able to describe in all detail. I'm going to try to get the main idea across. So the first So, the first thing you should think of is lowering tensile decomposition is in some sense separation of variables. So, we are basically trying to write our multivariate function pi of x1 to xd as a sum, a large sum, arguably, a large sum of products of univariate functions, where these little Blocks in the middle are then these core tensors. And I will do in particular a very specific one later on, which is very easy to do linear algebra on and to compute with. Okay, so here this is still very abstract, and you can do many different ways of how you would choose these multi-indices alpha here in order to group them together. But in principle, what we are trying to do is we're trying to write a multi-indices. We're trying to write a multivariate function as the sum of products of univariate functions. Okay, this is still, of course, not something I can stick on a computer. I need to discretize. X is still a continuous random variable, a continuous real value. And so I'm going to, first of all, need to discretize my function in each coordinate direction. So basically, as it says already. So basically, as it says already in the name, the idea is that we use a tensor product approach. So we use a tensor product basis. In each direction, we can, for example, discretize with piecewise linears or with a polynomial basis. There is different polynomials you would use for bounded intervals or for infinite intervals. But in principle, you're essentially discretizing the univariate dependency of the function along with for simplicity. Along with for simplicity, the same number of points in each direction, n. Then, once I've done that, you can think about your problem as now a problem of multilinear algebra. It's simply a tensor now, a tensor, like an extension of a matrix to higher rank. So, this is the block I've drawn here. So, it's an n to the d object, and we want to approximate it with something which is on the right. It with something which is on the right, which is essentially a product of long thin needle-like things, a low-rank approximation of it. And there's many formats and there's books about this. So why, so coming back to what I said before, if I can find a good approximation pi tilde where this conditional distribution sampling works, then I'm in business. So I've just now So I've just now said I will try to write my multivariate function as a sum of products of univariate functions. And then I think statisticians know this from other tricks they have used before. I'm easily now seeing that my integrals now become easy because now I have sums of repeated 1D integrals and this can be very efficiently implemented by running forward three dimensions and backward three dimensions. Dimensions and backward three dimensions. Once you're integrating out one set of variables, and once you're sampling, as you go forward. So this process of integrating out the remaining variables and sampling, doing a 1D CDF inversion are both linear in the operations now. And all this, of course, under the premise that my rank here, R, is small. Rank here R is small, otherwise, obviously, that's the bit that becomes costly. So, let me explain it in two variables because the multivariate case would take me too long. And I think we get the idea with the two-variable case. So, the two-variable case is obviously very familiar to many of you because it is very related to principal component analysis SVD. So, you can, for example, just do an SVD of your, now it's a matrix. SVD of your, now it's a matrix once I've discretized it. I can do an S V D of my matrix and truncate it after R terms. Then I have a lot of analytical tools at my disposal to analyze the error. It's just the remaining singular values. And I have reduced my storage from n squared to 2nr. I can basically choose how accurate I want to make my approximation. How accurate I want to make my approximation by fixing a tolerance epsilon and then choosing the rank so that this is guaranteed. But the problem is, and bear in mind always that we want to go to d variables and not two, that it requires all n squared entries of p. So in high dimensions, it requires all n to the d entries in d dimensions, which for high d is simply astronomical. It's not possible. So we need to have another way of dealing with it. To have another way of dealing with it, we can't do SVD, it's not going to work with SVD. So, here is the tool. It's called the cross-algorithm. It's been reinvented a few times. It's originally been in the chemical literature, theoretical chemistry literature already, but also in the mathematical literature several times. So, what you do is you essentially, in one word, you do a truncated LU factorization, a truncated pivoted LU factorization. Truncated pivoted LU factorization. So if I find the optimal rows and columns in my matrix, such that this little block, the cross, that's why it's called the cross algorithm, is optimal. I'll say that in the next point. Let me put it down. Namely, has the maximum determinant over all possible blocks with index sets. With index sets i and j of the same size, rank r, then this gives you the best possible block LU factorization of this matrix, or truncated block LU factorization, because we simply set the rest to zero. And this has been analyzed and proposed by the Russians in this paper. But obviously, to find this optimal block is an NP-hard problem, so we don't want to. Problem. So we don't want to really do that. The nice thing, though, is that if I just say if my determinant is up to a factor eta within the maximum of the best one, then my bounds on the norm just reduce by a factor eta. So you can get a quasi-optimal result as well, and that's sufficient for our purposes. Sufficient for our purposes. But how can we compute? I said it's an NP-hard problem. How can we do it at least for the optimal sets in practice? And how can we generalize it to higher dimensions? So how do we do it for time? Okay. So can I ask a stupid question? Yeah. So in this case, the approximation quality is that ensured? I mean, before you had. It, I mean, before you had whatever approximation quality you wanted, and now is that clear too? There is a clear proof that the uh for this matrix setting that the um the under all rank R matrices, you you will find uh a quasi-optimal one. So I didn't write the I had a the formula there, so it's in in in the in the spectral norm, you can uh bound the best one against the uh One against the against, or the one you're computing against, the true matrix P is less than or equal to R plus one times the best approximate one in another norm. Thank you. So I'm only doing it pictorially, the algorithm. What you do is you randomly select three columns for a rank three approximation, and you evaluate along these three columns. These three columns in all positions, so you really take so statisticians who are doing sort of a Gaussian process or something called it the design. So the design is I'm evaluating along the whole line in one direction. And then I can, in linearly in D, linearly in N, I can compute the optimal determinant along those lines. That's not obvious, but it's doable. You can find the optimal determinant, the max wall. The optimal determinant, the max wall, at least for this submatrix, for these three columns, for this thin matrix. Now that fixes three rows, I1 to I3. And now I forget my j's and I evaluate along rows. And I again find the max wall, the best determinant along the rows. And that gives me three new columns. And I alternate this algorithm forward, backward, and it can be proved that. can be proved that that truncates in a fast and converges towards a quasi-optimal, two quasi-optimal subsets for row and column indices. And that's basically the algorithm. You implement it in practice doing first the QR factorization to make it more stable and to be able to adapt the ranks. And there is, as I said, the adaptive cross-appropriation. I said, the adaptive cross-approximation by Babendorf or the empirical interpolation method by Made, Barot, and so on is two other similar algorithms now. How does it extend to multiple variables? So first of all, I need to describe what my actual format that I'm using is going to be. It's this tensor train format, which is a very specific low-rank tensor format where I choose particular multi-indices, namely I'm Multi-indices, namely, I'm literally just decoupling the first variable from all the remaining ones, then I'm decoupling the first and second from the remaining ones, and so on. So I do the simplest decoupling. And so you see immediately that the ordering of the unknowns will play a role, and the quality of the approximation or the size of the ranks will depend on which way you order your unknowns. And here are the original physics papers where this was proposed originally. And then now, these individual blocks in the middle are three-dimensional tensors. These are these needle-like things, which are RK minus 1 and Rk tensors. And if all of the ranks of these tensors are significantly less than n, then I have reduced my storage to something that's linear in D. And I have I have, I can do the algorithm that I described before the cross algorithm on this. So I'm realizing that my time is running out. So I'm not going to show you this algorithm for the tensor setting, but what just to explain in one word, we are unrolling the tensor in all the other variables. So it becomes a very long thin matrix and applying the algorithm on the very long thin matrix. So we're evaluating for three random choices. Three random choices of tuples J1, K1, J2, K2, or if it's more variables, more of them, along all entries of the I variable. And then we're calculating again this Maxwell, the determinant, optimizing determinant. And this can be then alternated through all the dimensions, backwards and forwards, and gives you, after a few iterations, the optimal or the quasi-optimal index sets for all. Optimal index sets for all the decouplings between the different dimensions. And the number of samples you need to evaluate along those lines is order d and r squared. Obviously, the constant in O depends on the number of iterations you do, how many times you go forwards, backwards. And the number of flops will also be only linear in D. Now, I've hidden, of course, and that's the important bit to discuss is what the rank does. What the rank does, what the maximum rank does. It could obviously go very high. It can't go higher than N. Sorry, it can go higher than N because we have, of course, these extended matrices here. But we would like to know what the ranks does in order to guarantee that this method is actually practically of interest. But in principle, everything can be done linearly in D. We can sample linearly in D, we can. Sample linearly in D, we can construct this approximation linearly in D, we can integrate with respect to the variables linearly in D. And in fact, what I'll show on the next slide, we can actually prove for certain very preliminary for certain distributions or densities that the growth of the rank is only polylogarithmic in the dimension. And then I'll, if I have time at the end, I will show some. I have time at the end, I will show some ways of using this in inference. So, here is the theoretical rank bound purely for Gaussians. But as Richard already told us, at least for the concentrating posteriors, we expect or we hope that these will become more and more Gaussian-like. And therefore, it's already an interesting thing to see whether Gaussians allow us to do the TT approximation efficiently. And the answer is yes. And the answer is yes, provided the covariance matrix has not an arbitrarily high coupling between all the variables. So this is quantified in terms of the singular values of these off-diagonal blocks for all possible separations of matrices of the covariance matrix into two blocks. Now, under those conditions and with these, the rank bound specific, I mean, explicit. bound specific i mean explicit in in this singular value and in the in in the other constants in this uh in this uh in this covariance matrix we can prove that the growth is log polylogarithmically where the poly the the exponent here rho depends on the rank of this off diagonal block there's a second version in this paper where we can also have uh decaying covariance like smoothly decay Covariance like smoothly decaying, then you need to guarantee that it decays fast enough away from the diagonal. This is the version with the rank bound with the ranks. So this is promising. We haven't got any proofs for more sophisticated or for more difficult functions, but at least this tells us that Gaussian or Gaussian-like distributions will be well approximable by this methodology. So, how can we use it? So, how can we use it? So, the first way would be to do the classical variational approach and to say, I'm happy with it. I'm just going to use the bias density, my approximation that I computed like Yusuf did yesterday. And I just compute the expected value with respect to this bias density. And then, as I told you, I can integrate explicitly because we have this product density. So, this would be easy for smooth functionals. But if we have a non-smooth functional, We have a non-smooth functional, for example, a CDF of some parameters or of some functional of a parameter, then you can't really integrate and we need to do sampling. But now we have a map that maps from uniform to our posterior and approximation our posterior. So we can, for example, map Gauss quasi-Monte Carlo points through our map and get the convergence rates of quasi-Monte Carlo. Although we haven't been able to Although we haven't been able to prove that yet, that's been a long-standing question to prove that rigorous. So, this is what we do in the practice, practical version. We use pi tilde as an importance weight. Sorry, now I'm de-biasing. So, I'm saying what a statistician would prefer is if I can give him actually an unbiased estimate of this expected value. So, I'm using my pi tilde simply as a debiasing, as an importance weight. I sample from pi tilde, but then I. sample from pi tilde but then I re-weight my quantities by pi over pi tilde and that gives me and I compute the normalizing constant but this is not difficult now because I've got an approximation here and I can use quasi-Monte Carlo again to estimate both of those integrals or I can use samples from my proposal from my proposal distribution from my approximate distribution as proposals in an MCMC in the Proposals in an MCMC, independence proposals in an MCMC. And then you can essentially get an integrated autocorrelation time that's one. So that's the other one. And you can use it also in multi-level MCMC, but I won't talk about that. So here is the model problem again, our diffusion problem with some Kaun and Leuf type expansion of the coefficient here with uniform coefficients, but we've also got results with. Coefficients, but we've also got results with log normal, with proper log normal. And we take, for example, this is a synthetic experiment. So we take the truth is some coefficient field that we've just taken from our distribution. And then we take nine averages of the pressure and add some artificial measurement noise. And then we essentially. And then we essentially have the posterior being this Aldiff Gaussian model, with obviously the nonlinearity coming through that the dependency of our PDE solution on the parameters is obviously non-linear. And the quantity of interest is the probability that the flux through this region exceeds some value. So it's a non-smooth quantity of interest. If you discretize in space, the PDE with Discretize in space the PDE with finite elements, you discretize in the parameter by some polynomial approximation and stochastic collocation. And we then compute this tt. So this would be what I said earlier, how you get from the functional density to the actual algebraic tensor. We're using a polynomial chaos approximation here. And then we can compute the TTF. Compute the TT approximation of our density, and we need to do something in order to make sure that it stays positive. So, the more interesting approach that we're now perpetuating is to approximate square root of pi, because actually it's possible to also work with these TT approximations when you have to do the square of your approximation efficiently and linearly in D. I'm not going to mention that. Here is I'm not going to mention that. Here is just a numerical experiment comparing for this inverse diffusion problem with 11 dimensions against the DRAM sampler, both the Metropolis Hastings with proposals coming from our distribution and the importance we're weighting with the QMC quadrature rule. And you see clear gains. And you see that if I increase, if I concentrate the posterior more. If I concentrate the posterior more by reducing the noise level, the cost, the learn, so I call this the training because we're basically training our TT first becomes more expensive. So this is the cost it takes us to build the TT approximation. But once it starts sampling, it beats the other ones. Okay. And this is not shifted, the DRAM, because the DRAM needs some burn-in. And so it also can't start off doing. Can't start off doing immediately converging fast. And this is, in fact, now the picture I showed before. So the one on the right is our seeds coming from this TT conditional distribution sample. And the costs are all polylogarithmic in the dimension, as I said. So they are all just converging algebraically in the dimension. In the dimension. Okay, this is just to advertise this paper. In fact, it's now published, a 60-page paper in Fundamentals of Computational Mathematics, that you can make a deep version of it by using tempering, by using bridging densities between your before you go, before you build the final distribution. Here it would be this double banana. Here it would be this double banana. You start with a tempered one, much easier to approximate with low rank. And then, because we have all these sampling algorithms at our disposal, we can push the sample through several layers of our approximations. And each of the layers has much lower rank because we're essentially only approximating these four maps here. And we're using this currently in a recent collaboration where we're working. Collaboration where we're working on a new paper to do ray events basically. So now we're approximating not only in the ratio ester, we're not only approximating the density, we're actually also approximating the optimal biasing density. So we are making a dirt approximation of the optimal biasing density, of the posterior density, and then you can essentially get at ray events extremely efficiently with very few evaluations. Extremely efficiently with very few evaluations of your density. So, this is what I wanted to tell you about: how variational inference can be used to do efficient sampling. And the central idea is to find good deterministic couplings. But the real crucial thing was that these TT methods lead to these products of univariate function approximations, which are really useful for building cheap sampling algorithms. There's other methods in the same class, and that Methods in the same class, and I've already said that we are extending this towards rare events, and we would like to extend the rank bounds also to this deep to this layered version of this transport method. Thanks.