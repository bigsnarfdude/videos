Before we start, I would like to start with some definitions according to me at least. So what do I mean by instrumentalist and instrumental? What do I mean by machine learning? Let's start with machine learning. Honestly, I have a very open-minded vision on machine learning. Everything that includes a machine learning to do something useful for me, that's machine learning. So I would not draw the line on your network, network, expert system, I don't care. Anything that, so linear regression, yes, that's that's. That. So, linear regression, yes, that's machine learning, why not? Instrument science and engineering, that's my group, what I do most of the time. That deals with all the aspects that have to do with the instrument. That means designing the instrument, means controlling the instrument, making sure that it operates at the right operating points. There's a lot involved with characterizing the instrument and doing noise hunting, understanding the noise. And there is a lot about detector characterization. A lot about detector characterization and glitches. So, the reason two things here are shaded out is because I am not going to spend any time today talking about how machine learning can help us designing the instrument, mostly because I don't have time and also because there isn't really much out there really. And also, I'm not going to spend any time talking about glitches because Derek, after me, and the rest of the talks, the session this morning, will be about that topic. So, let's just dive in. So, my goals here is to read. So, my goal here is to review what are the hard problems, according to me, for the instrument. So, for noise hunting, for noise characterization, for noise reduction, and for control. And then discuss what we've done so far, of course, with a focus on machine learning, what machine learning can do for us. And look for new ideas. You'll see that we have I'll have a lot more questions than answers for this talk, which I think is a good segue into the discussion we'll have this afternoon. Discussion we have this afternoon, which we have two full hours scheduled for that. We'll see how that goes. But if there's a good discussion, we keep going, otherwise, we all go for a hike outside after a while. Okay, let's start for the first topic, noise. So why do we care about noise? Well, maybe some of you have already seen this kind of very busy plot. Let me explain what that is. So that is what we call a noise budget. So this shows you the measured spectral density of the noise. Spectral density of the noise in strain, meters in this case, or displacement, but dividing your mind by 4000, you get strain for one of the lag detectors. This is the Hanford detector. So what you see here is the red trace shows the measured sensitivity of the detector for a given time. This is in October, so it's very fairly representative of what we typically get from the market. Then all the other traces are all model or measurement or estimate or guesses of Or estimate or guesses of noise sources. And when I say noise sources in this case, I mean mostly stationary noises that couple into the detector main output channel linearly and in a linear stationary way. That's not true all the time, and that's one of the sources of problems. So you see, there are some fundamental things: quantum noise, coding thermal noise. There are some other less fundamental things, like all the control noise down here. Control noise down here, laser input linear right there. So, all our knowledge, always evolving knowledge of the noise that influenced the detector goes into this kind of project. So, there's another interesting trace, which is the black trace here, which is the sum of all we know. So, you notice a very important thing, there is a fairly significant gap between what we know and what we measure. Some of you were chatting with me in the past days and mentioned some mystery notes. Well, it's not. As a mystery noise. Well, in some sense, we can call all this gap from 10 hertz all the way to 100 hertz, the difference between what we know and what we measure as the mystery noise. So the goal is to understand what that is and get rid of it. Simple. So how can machine learning help as with that? And also maybe you might ask, why is this a hard problem? Well, it's a hard problem because there's a lot of unexpected noise sources. Part of a lot of the work of the commissioning is With the commissioning is to understand what limits the detector, figure out where the noise that you haven't designed for comes, and then try to fix it. Then a lot of this means also that we don't understand a lot of those noise couplings. For example, that curve that we showed in the previous plot shows a lot of coupling from input beam jitter. Ideally that should be zero, it's not zero, we don't quite understand why. So there's an effort in modelling and uh characterization of the detector to understand uh what's going on there. To understand what's going on there. Then we have this problem, if you want, that we don't have really enough monitoring channels. We can't measure everything everywhere, of course. We can't measure where the laser beam is and in all possible places in the detector. We can't put seismometers or semerometers in every single surface that could vibrate. So that's a problem. At the same time, we have too many monitor channels because our system, one little detector, produces about 4,000 fast channels. Produces about 4,000 fast channels, faster meaning everything sampled faster than about a kilohertz. 200,000 slow channels that monitor pretty much a lot of things everywhere in the detector at a slower rate, 16 hertz. So this means that we produce normally about 28 megabytes of data per second. So that's where we have to dig and mine and see if there is information. So that's a hard problem from a machine learning or data mining point of view. That's a lot of data. View, that's a lot of data, that's a lot of fast data. So, how can we go into that and try to figure out where is there any information that can help us understand the source of all the noise? And also, there's another big thing that we are fairly good with linear stationary noise couplings because we know how to deal with that. I know we're physicists, everything is linear, everything is a sphere, right? But when we start going to non-linear couplings or stuff that deviates a little bit from that, then the problem becomes a lot. That, then the problem becomes a lot harder. And I'll focus more on that. So, let's see what we've done so far in terms of linear coupling. So, linear is kind of easy because the coherence is the two thread. So, we can take our main detector output, H of T, compute the coherence with any of the fast channels that we have, and then say is the coherence high or is the coherence low? If the coherence is close to one, then that's a hint that that channel might witness some of the noise or detector. Might width some of the noise of a technical. Doesn't really necessarily mean that that's the source of the noise, it's a correlation, not quite a causation. But hey, if you can reduce it to a smaller subset of channel, then people can start looking at that and put their own expertise. Levit is a key point. At the end of the day, all the tools we have will give us hints. At the end, one of the commissioners, one of the instrument people needs to look at that and say, yeah, this makes sense, and potentially invent an experiment to prove it does actually the noise company. To prove it does actually the noise coupling, either by increasing or decreasing the noise, and then try to design a way to fix it. So, we have been pretty good at doing this linear noise characterization, and we have tools that are being used quite routinely, produce gigantic tables that for every frequency, what are the top channels that are coherent with our detectorsensitivity. And this has allowed over the years to identify a lot of noise sources and also track. Sources and also track if the performance of the detector changes over time, track why that happens. So, I think we have the both the aspect of the discovery of noise couplings and the characterization of those noise couplings pretty much well covered in the case of linear noise. And if you want, is this machine learning? Well, I would say this is data mining and it is accessible machine learning because you can have a tool that gives you information from a big amount of From a big amount of data gives you some summary of that. So, yes, we call this machine learning if you want. So, once you find a potential noise source, ideally some of the commissioners, some of the instrument physicists like me will go there and engineer a way to get it rid of it. Sometimes you cannot do that, but if we have a linear stationary coupling, we can subtract that noise. And we've been also pretty good and successful at subtracting noise. There are two examples here. The first one There are two examples here. The first one is from O2. You barely see this gold, yellow, UUP, what's the well-known trace that show the sensitivity of the Hanford detector during two. And you see that there's a lot of structures and peaks that are due to input bin jitter. And so we used the winner filters at the time to subtract this noise offline. You can see you can reproduce the blue colour, which is much better. So linear. Much better. So, linear couplings, fairly stationary coupling, easy to subtract. We have now a new approach that allows us to do pretty much the same thing directly online in real time, 16 kilohertz sample by sample, by using infinite impulse-response filters. And this is another example taken from one of our summary pages that showed you that online we are still subtracting input to each other. So, you see that it's much lower than before. So, we were able to improve that noise. To improve that noise coupling by improving the detector, but there's still a residual. And if you screen, you see that some of those peaks are removed. That now happens online. But again, it's a linear noise subtraction. So that's something that we are pretty good at doing. So let's go now to what's really hard, non-linear. I think non-linear is hard because the way you define non-linear is it's everything but linear. So we don't have really a unifying model that we can put parameters. Model that we can put parameters on it and try to figure out fit those parameters of what to do. So, how can we approach this? So, I think a lot of people ask me, why don't you use a neural network? Mathematically, a neural network is a universal function approximator. Some people prove that if a neural network is big enough or sophisticated enough, you can approximate any function. So, our noise coupling is. Our noise coupling is a function, so a neural network should be useful for that. I'll talk about what are the problems and the limitations of this. Another way is, okay, say, yes, of course, we have non-linear cuttings, but let's not try to solve the most general problem. Let's try to focus on the things that we know. Can we put some of our knowledge of the physics of the detector into the m the the cutting model and use some sort of reduced order, reduced model approach? Use model approach. So, this has been fairly successful. And then, of course, we have some other cases where we have some unknown, complicated non-linear processes that we don't quite characterize. So, how can we have an approach, a system that will look at our data and find any possible complex nonlinear count? I don't know if that's even possible. Should we give up on that? I hope no, but this is one of the places where I don't have any answer. So, let's see what we've done so far. So let's see what we've done so far for non-linear. So one of the most successful approaches so far is this non-stationary noise subtraction. So the idea is that the most general non-linear process is way too complicated, so let's make it simpler. So the simplest non-linear thing you can image is just a quadratic colour. So then we notice that the way we built our detector, all our signals, all the motion of our system, the angular motion, the longitudinal motion, System, the angular motion, the longitudinal motion of our mirrors. They are characterized by the fact that they have a lot of motion, a lot of signal, of power, spectral power, a very low frequency, and not much at high frequency. So there's several orders of magnitude from something at the order of 10 to the minus 1 to something at the order of 10 to the minus 8 here, going from 1 Hz to 10, 20 Hertz. So, in some sense, there's a separation between the low frequency part and the high inband part. So, if you assume a quantum. So, if you assume a quadratic coupling and you enforce that kind of separation, then what happens is that you can say that a lot of the noise couplings are linear instantaneously, but the parameters are modulated by the low frequency content of other signals. So, this is pretty powerful as long as you know what are the signals that are modulated. So, you can use this to do some sort of non-stationary. Some sort of non-stationary linear cutting subtraction like that, and also you can use that to do discovery because you can take combinations like one witness signal times your one signal that witness the low frequency component times a signal that witness the high frequency noise, do the multiplication of those two and then still look for coherence. And that's still successful. Of course, you can't do that for everything because you remember we have 4,000 charges. 4000 square is a very big number, so that's not practical. So that's not practical. It's not practical computational, it's not practical to discover because when you have 4,000 square channels, the false alarms there, they're way too much, way too many, right? But you can restrict yourself to all your fast channels and a very small number, like 10 of channels that witness the low frequency motion, and explore that. And that's being also successful. So, this approach helped us, for example, during 03, doing a lot of subtractional time for the low frequency noise, including some. For the low-frequency noise, including some non-stationary subtraction around the 60 Hz line that was showing silence due to this modulation. It also, what I was showing just a couple of slides before, this online subtraction of laser noise is also performed using this method. So this is an example that I think is fairly interesting because it gives you the idea that instead of trying to solve the most general problem possible, we try to add as much knowledge as we can of the physics of the Knowledge as we can of the physics of the systems, of our system, of our instrument, and try to use that. So, of course, as I said, another approach would be to try and draw a deep neural network to it. In theory, this should be working. As far as I know, there were some tests some years ago that shows that, yes, you can train a neural network to get almost to perform as well as the reduced order model. Well, as the reduced order model that was shown in the previous slide, but not quite there. Even if you make the network bigger, if you train longer, if you use sophisticated training, you always have the problem that it's very difficult to find a good solution. My understanding of the best performance is about deep training that managed to match the linear subtraction in a couple of examples. But that's great, but again, the neural network has the promise of being able to tackle new networks. Able to tackle non-linear and non-stationary noises and didn't quite show that potential yet. And I think the problem is that it's extremely hard to train. And also, at the end of the day, a thing that I personally don't like about neural network is this black box problem, right? So, for me, being an instrumentalist, if I can subtract the noise from my signal, that's great, but I want to understand where. But I want to understand where that noise is coming from and how it couples, and so be able to go out there and fix the instrument, so then we don't have to subtract that anymore. That's an important piece of information. It's hard to extract from your network. Then there are other non-linear problems that are extremely difficult because they are highly non-linear. One example is scatter light. Scatter light means that some of the photons for our main beam are going out of the main angle of the beam. The main angle of the beam, if you want, because of roughness of the surfaces, because of the diffusion, they might hit some parts of the detector, like the vacuum tubes or some buffer, some other components, optical mount, whatever, that vibrate more over mirrors, of course, and then those photons bounces back, recouple into the main beam, carrying a random phase information. That random phase information creates noise. Why is this a complicated problem? Because it's highly non-linear. Non-media. So you have something moving at very low frequency, fraction of a hertz. That motion gets to modulate very deeply the motion of a piece of vacuum tube that then is vibrating at some higher frequency. And you see, a way to represent that is that your strain sensitivity gets a sign of your motion. And that motion can be many wavelengths. So that's a lot of stringy practices. That's a lot of fringe wrapping. It's a highly non-linear system. Non-linear problem. The problem is also that we don't have a precise witness of all the variables going to that. We don't have a precise measurement of the motion of exactly the object of scattering, either a low frequency or a high frequency. You can't cover every single surface with seismometers. So that's a complicated problem because of the highly nonlinear nature and because of the fact that we don't have enough information. How can we tackle it? How can we solve this? That's a question for you, I have no idea. That's a question for you. I have no idea. Okay, noise. Enough about noise. So let's now move to the second part: controls. Maybe you're less familiar with this. So what do I mean by control? Well, I mean that our system is an interferometer. It's a complicated system. We need to keep all the mirrors within such that the difference between them is kept at the proper operating point with very high accuracy, something between 10 to the minus 15 to 10 to the minus 13 meters, depending on the distance. 10 to the minus 13 meters, depending on the degree of freedom. And there's similar requirements for angles, so there are hundreds of degrees of freedom that needs to be controlled precisely to their operating point so that the detector produces the good sensitivity that now that we do every day that allows all of you to then analyze the data. It's a hard problem. In general, the way you do it is by using feedback control. So you have your interferometer, which is a plant control. You have some sensors. We have some sensors. Those sensors could be seismometers, could be accelerometers, could be even photodiodes that read out some phase modulation that you imprint on the laser. And then from that, you have a controller, which is some sort of algorithm that decided how much force to apply on your mirrors to maintain them where they're supposed to be, or how much to change the voltage of the laser to maintain the power where it's supposed to be, all of that. So the thing is that our detector is a highly non-linear system. Detector is a highly non-linear system in normal configuration. The reason because the mirrors are swinging around and they're covering multiple wavelengths. And think about it: if the distance between two mirrors jumps, but half of a wavelength, the route trip length changes by a full wavelength, and everything is periodic by one full wavelength. Means that if the mirror is moving a lot, you're going through multiple repetitions of the same pattern, the same response. So, this is clearly highly non-linear. It has to be highly non-linear, right? So, one of the goals of So, one of the goals of the feedback control is to go from this initial random motion covering an enormous phase space, where the response is almost completely non-linear, to the operating point, which is a fraction maybe 10 to the minus 9 or even smaller of the total phase space, where the system is actually linear, almost linear, you can control. So, there are two problems here. The two-part is what we call the log acquisition, going from the initial random. The log acquisition, going from the initial random motion and complete random exploration of the phase space to the final operating point where the systems are actually linear. And then the linear control, the low-noise operation. Once you get to the low-noise operation, then yeah, most of it is linear feedback. That is fairly well known, although our approach to linear feedback has its own problems. So how can machine learning help for this? So there are two different approaches. So the problem of block acquisition is that the same The problem with the log acquisition is that the sensing is non-linear, the responsive photodiode is highly non-linear. Control, the actuation is mostly linear, so that's not too complicated. So how can we tackle this? Of course, in machine learning, the buzzword for controls is reinforcement learning. Everybody is trying to do reinforcement learning. I'll talk a little bit about that later. Another approach is say, well, my sensing is non-linear, the actuation is linear, and the system itself, the mechanic of the system, And the system itself, the mechanic of the system, is linear. So can I build a non-linear state estimator using machine learning that will tell me at every second where my mirrors are? And then it's triggered to bring them where I want them to be. In low-noise operation, the thing is that our approach to low-noise operation and linear operation is very different from what most people do in controls. You know, control engineers make airplane flight, even if they're a stable system, the airplane still flight. But they care not But they care not really too much about low-no as they care more about robustness. Luckily for us, otherwise, we wouldn't be here, probably. Sure, they care about the vibration, you don't want your airplane to vibrate a lot, but there's a difference between what their requirements are in our six, seven, eight, ten orders of magnitude, the difference between the motion we need to control and how quiet we need to be able to fit. So how can machine learning help us that? Learning helps us there. So, I want to talk a little bit about this. So, refreshment learning. So, reversemed learning deals apparently exactly what we do. So, with how to create an agent that can control complex nonlinear systems. It's very successful in robotics, sort of. I mean, I would guess that's successful enough. So but to my knowledge there has been no successful application to gravitational web instruments as of now. Of instruments as of now. Why? Well, we have several problems. First, time wobble detector is very expensive. Nobody can convince me to give you five modes of detector data and run your agent to train on that. First, because that five modes are very expensive in terms of money, science, etc. Also, our detector is kind of fragile. You don't want to have your agent run around and trying random policies that my have my mirrors swing around for hours and hours and hours. Around for hours and hours and hours. So, in theory, we have numerical simulations, numerical simulations that are close enough, the fidelity is not 100%, but it's not too bad. So, ideally, you could say, why don't you train your agent on a model, on a nomadic simulation, and they see how you develop it. This is something that, as you can see, there are people do in robotics. So, this is my challenge for all of you. If some of you know more than me about referral-based learning, can we prove that reflex-based Can we prove that reinforcement learning is potentially useful for gravitational waves in interferometers by showing that we can lock a fabric cavity? That's something that we can do fairly easily. We've been doing that for decades without problems in a lot of different ways for our detector. Can we have an agent learn to do that? Open question. So the other approach that has been successful in model at least is to build user neural networks. Build using a neural network to extract to solve the inverse problem. What I mean by inverse problem, okay, I'm almost there. What I mean by inverse problem is the motion of the mirrors of the normal system creates through a non-linear process the photodiodes that we measure. Can we have a neural network system learn how to invert the photodiodes and give us the mirror position? So, this seems to be working in simulation. Working in simulation. A student working for me years ago managed to produce it fairly well. There's some subtleties there, for example, you have to, we use the Kalman filter to enforce the solutions to be realistic because the neural network had some issues ensuring that the mirror velocity, for example, was physical, was realistic, all of that. So this is seems seems to be promising so far in simulation. Who knows if we're gonna ever be able to deploy it in a in a real uh system. In a real system. So, the other thing that is somewhere in between machine learning and other aspects of what we call optimal control, there's a whole field of control theory which is called the optimal control, which means decide what are your goals and then run optimization algorithms to get the controller you want. That's very successful in a lot of feedback control engineering, but the kind of requirements of low frequency at Low frequency actuation and high frequency don't do any noise are very difficult to implement in all these optimal control strategies. So can we skip all of that and just throw machine learning algorithms and say just go and try to find the optimal control? Difficult part is to specify our requirements that are often influenced in domain, which is reduce the motion below three hertz as much as you can. 3 Hertz as much as you can, but don't do noise above 10 Hertz. And that 6, 7, 8 order minus of difference are always a killer for any machine learning algorithm. There has been other interesting things that have been tried out in the instrument. For example, Geo600, which is an interferometer in Germany as you all know, they use a reinforcement learning agent to train a system to look at images from cameras or looking at the beam and reconstruct. Are looking at the beam and reconstruct angular motion of the mirrors. That was fairly successful, but the way what I'm showing here with this graph is that basically that machine learning algorithm managed to match the performance of the current state of the art sensors we have. So if we didn't have those sensors, this would be interesting. But again, machine learning is somehow the point of showing that we can reproduce the current state of the art of other sensors, but not quite beating them. With that, I know my time is almost over. With that, I know my time is almost over. So, final thoughts. I think my key takeaway here, we have a lot more this afternoon to discuss about it, is that I think, but I'm a little bit skeptical, of course, about machine learning, mainly because there's been a lot of work, not quite as much work in the instrument science side as in the data analysis side, but I have still yet to see a machine learning algorithm, and I'm using the term very generally here. The term very generally here that can beat human-tuned controls, human-tuned algorithms to make our instrument work. Some of those algorithms manage to get to the point of almost match what we have been doing for decades, but not quite period. So, why is that? And maybe it's because our application is very different from mainstream machine learning. Maybe it's because we don't have the resources that deep Resources that DeepMind of Open AI has, they have, sure. Or maybe, you know, who knows why. The problem we have also, and something that I'm really interested to see what you guys think, is this double curves here that we have too much data and we don't have enough data. Meaning that we have a lot of signals that are recorded, it's a lot of stuff to go through to mine information, but maybe we don't even have the user information there. So if you deploy an algorithm that is being So if you deploy an algorithm that has been proved to be very successful to discover noise in whatever field, how can we know? I would like to know the answer. Do we have enough sensor to actually do the subtraction? Or all are the algorithms that are failing because we don't even have information there? That's an important question too. Thank you. Sorry if I was wrong. Okay, questions? Very interesting, there we go. So, I have possibly nine questions. So, if we have a good enough simulation of the detector, then couldn't you read off these couplings and away couplings in the simulation itself? It's not very important now. So, the reason is that, well, that's one way of saying that we don't have that good enough simulation. So, we have a so we are, we know how to simulate an instrument which is ideal, an instrument which is very close to be ideal. Which is very close to be ideal, but there are so many ways that the instrument can be non-ideal that if you start adding all of them, you can pretty much reproduce almost whatever you want, but not to the 2D. Let me rephrase that. It's not that you can reproduce all you see, but there's too many variables there, too many imperfections that you could add. And so we don't have a good enough simulation that reproduces the complexity of our system. Then, how could machine learning help in that? That's very data. Yeah, I have no idea. Yeah, I had no idea. So, the point is that can we have a machine learning agent that is able to generalize? So, when you train reinforcement learning robotics, the idea is that not all robots will be exactly the same, right? They still have differences. Maybe they don't diverge as much as our instrument can from our model. But if your model is good enough and your agent is built to be robust against perturbation, then maybe that's enough. Against perturbation, then maybe that's enough. That's just, I, yeah, by me, any of those users wall, I don't know how to do that. I have no idea. That's the whole thing. Don't have time for more questions. That's all. Thanks. Get your ahead. We'll discuss more. Our next speaker, we have Derek Trigger, which is