Me to this place. And I apologize for changing the topic. So I was originally planning to talk about privacy in federated learning, but I felt like I'm not sure if there are many people here who work on federated learning, but I felt like that it requires more background maybe to appreciate some of the nuances of that discussion. So, whereas this one here, this is still on privacy, but in a slightly different framework. Different framework. I think the framework is really simple. The goal of this talk is just to share, I think, a cute observation that has a nice practical application. And it relates to some very basic things in information theory. So I believe everything in this talk is very, very basic. Okay, so, and we'll talk about training generative models from privatized data by using this notion of entropic optimal transport. This is a joint work with my PhD students. My PhD students, Daria Resha-Doba, and Beijing Chang. And in order to tell you about this, I first need to tell you about optimal transport. And I assume maybe not everyone here is familiar with that notion, so I'll start defining it from scratch. So this optimal transport problem, this actually goes a long way back to the French mathematician Monge, 1781, on his note on land excavation and infield. On land excavation and infield, where he formulates the following problem. So, assume we want to excavate some land here, and the shape is given, and we want to take the dirt we excavate and then transform it to this shape, and we need to come up with a transportation plan that will say, take the mass, whatever mass you have at the point X, and then put it to the point Y, and so on. And the total mass will be preserved, so without loss of. Be preserved, so without loss of generality, we can assume that it's normalized to one. In which case, you can think of these shapes as defining probability distributions. And now you can state your problem in these probabilistic terms here. You can say, I have a random variable x that has this first distribution px, and I'm looking for a mapping, a function that will transform this random variable to a random variable y, which has this target distribution py. This target distribution Py. But among all such mappings, functions that transform this distribution to this one, I would want to find the one that minimizes the expected volume of a given cost function. And the cost function here can capture the cost of taking one unit of mass at the point x and carrying it to the point Y. Okay? And we want to basically minimize that cost in. minimize that costing expectation. You may be wanting to pay attention here is that we start with two marginals here, px and py for x and y, but once you fix that coupling between x and y, this basically defines a joint distribution for x and y with those marginals, right? And you can think of this expectation as taken over that joint distribution. So this is Moon's problem in this probabilistic total. Charge. And this problem gets rediscovered by Kantarobech some 160 years later, the Russian mathematician and economist who later on wins the Nobel Prize for his work on this problem. And he redefines the problem, but he also introduces a small but important generalization. So in Monge's problem, Mons wants this mapping here to be deterministic, a deterministic function. Function. But if you insist on a deterministic function, in some cases, this problem becomes ill-defined. For example, if this first distribution is just a unit mass, a point mass at one point, then with the deterministic mapping, you can't create a shape like this because a deterministic function will take everything at the point x and you'll map it to a single point, right? You can't split maps with a deterministic function, and that's precisely. Function, and that's precisely the relaxation that Kantarovich introduces. So he says this mapping, coupling between x and y, can be potentially randomized. In which case, you can think of it just as a conditional probability distribution, or a channel radiate between x and y, so that when the input distribution is px, the output distribution is py. And this basically defines a joint distribution now with the channel for x and y with these given marks. And y with these given marginals px and py. And again, over all joint distributions with these given marginals, we would like to find the one that minimizes the expected volume of a given cost function. It turns out that in a lot of applications, a natural choice for this cost function is to take it to be some p-norm. P-norm, some p-distance between x and y, in which case the resultant optimal cost has a name of its own. It's called the Washerstein distance, the P-Washerstein distance. And you'll be specifically interested in two sub cases here that I will explicitly refer to later on, where P is equal to one, the cost is just the L one distance between X and Y. Distance between x and y, and then the corresponding optimal cost in this case is called the one wascherstein distance. And if you take p to be 2, then you're measuring the cost with the squared Euclidean distance between x and y, the R2 distance. And then the corresponding optimal cost, when you take the square root here, this is called the two-stein Wascherstein distance or the quadratic Wascherstein distance. And one can actually prove that this thing. One can actually prove that this defines a proper distance between distributions. So, intuitively, right, if you start with two distributions here that are already very similar to each other, then the cost of transporting one to the other will be small. And if the distributions are very different, then the cost of transporting one to the other will be very large. So you can use this notion in order to measure how dissimilar two distributions are to each other. Two distributions are to each other. Introduce a notion of distance between the distributions. So, we will be interested in a variation of this quantity that we will call entropic optimal transport or information constraint optimal transport. So, if you go back to the definition here, let's say the quadratic Russian distance, right, the optim this this quantity inherently in its definition involves a minimization problem. Minimization problem overall joint distributions with these two given marginals. And what we would want to do now is we would want to introduce a constraint in this minimization problem. We will say we will only allow those joint distributions, again with marginals Px and Py, that introduce that lead to a bonded amount of mutual information between X and Y. Between x and y. Okay, so why do we want to do that? The whole point of this talk is to give you one application of this quantity. But for now, you can think of this constraint as follows. So remember before I told you that in Morse formulation, the coupling between x and y had to be deterministic, right? And if you couple two random variables, if you relate them with a deterministic mapping, Relate them with a deterministic mapping, then the mutual information between them can be potentially very large, even unbounded if we are talking about continuous random variables. So by bonding that mutual information, essentially what you're doing is, and I should say that even though Kantorovich's problem formulation is more general, he allows for randomized mappings, it turns out that in a lot of interesting cases, the optimal coupling will still be deterministic. Coupling will still be deterministic. Okay? And what this motion information constraint does is it enforces some amount of randomization noise in that coupling you introduce between X and Y. Because, again, if the coupling is deterministic, this mutual information can be very large, right? By controlling that mutual information, you are controlling how much randomization you would like to enforce in that topic. In the kind of extreme case, when r is equal to zero, Extreme case when r is equal to zero, the mutual information is zero. So the only allowed joint distribution here is the one where x and y are independent from each other. I have just a quick question. So there's also the notion of entropic cultural transport in which you do regularization. Yes. That's exactly what I'm trying to get to. Okay. That's essentially this, but I'll write it in a Lagrangian form so that it looks like a regular answer. Ask. Okay, that's exactly my next slide. So, for me, this notion basically, I wasn't actually aware of this notion of entropic optimal transport, but we were working on a problem related to the relay channel. And this quantity popped up from the proof. And we kind of realized that once you bound this quantity with a simple bound, then in two lines you can prove a problem that. You can prove a problem that was formulated by Kohler that relates to this capacity of the relay channel. But I should say it has actually appeared in the previous IT literature as well. So actually, you may realize that this is very close to a rate distortion function, right? It's like a distortion rate function. The only difference really is that in the definition of the rate distortion function, the only typically the We only typically the source distribution is fixed, and we don't fix the distribution of the reconstruction. Whereas here, it's like both the input and the output distribution is fixed. So that's why people have called it output constraint, rate distortion function. And interestingly, it actually can be traced back all the way to Schrodinger, 1931, and this is even before Kantarowicz formulated. Formulated his optimal transport problem. And as you mentioned, it also appears in the recent machine learning literature, mostly in this Lagrangian form. Basically, this is a constraint optimization problem. You can take the constraint and make it part of your objective here by introducing this Lagrange multiplier. And in this form, people call it entropy regularized optimal transport or entropic optimal transport. Optimal transport. And the reason why they've been interested in this quantity in the ANA literature, at least in the beginning, is mostly for computational reasons. So if you don't have this mutual information here, then this is just your standard Washerstein distance, right? Your standard optimal transport. And it involves, in order to compute that standard Washerstein distance between two distributions, you would need to solve this optimization problem, right? Optimization problem, right? And indeed, this optimization problem, you can see that it's a linear program because the expectation is linear in this joint distribution, and these marginal constraints are also linear constraints. And actually, this was in the context in which linear programming was invented. But it turns out that the complexity of that linear program is too high. So, for a lot of practical applications, people want to compute the Wascherstein distance between two distributions. Distance between two distributions, but it just takes too much time, so they can't compute it. What they instead do is they introduce this mutual information as a regularizer, and then that makes the optimization problem strongly complex. And this quantity you can compute much faster. So you can't compute the standard Wescherstein distance. Instead, you compute this one, and you take it as an approximation for the true Wescherstein distance. That's how it has first appeared in this null literature. In this null literature, there's quite a bit of excitement and work on it already. Okay, and one context in which this notion of Wescherstein distance has proven useful in the machine learning literature is in the context of these generative adversarial networks, GAMS. So, let me tell you what GANs are. So, GANs are one way to learn. Are one way to learn a distribution from their samples by modeling the distribution that you're trying to learn as a function of a known distribution. Okay? So basically, in this framework, you're trying to learn a function, an app, that's called a generator, and it's usually implemented with a neural network that can take samples from a simple distribution, like a low-dimensional Gaussian, and once you press Gaussian, and once you pass these samples through this function, so the transformed samples should look like they come from your target distribution. So for example, if you're trying to train a generative model that will generate images, photos of realistic photos of human faces, then you want to be able to find a mapping here that can take, let's say, just samples from a simple distribution, like a Gaussian distribution, and then A Gaussian distribution, and then you apply this function, the output should look like a realistic photo of a human image. And you can pose this as an optimization problem if you fix a way to measure the dissimilarity, the distance between the distribution you are able to generate at the output of the generator and your target distribution, the distribution you are trying to learn. Trying to learn. In which case, basically, your problem reduces to this optimization problem. Among all functions in a given class, I would like to find the one that will minimize the Wescherstein distance between the distribution at its output, the generated distribution, and the target distribution. Okay? So that's my loss function, my way of measuring how well I'm doing here. How well I'm doing here. But obviously, in practice, we don't know the distribution that we are trying to learn. We typically only have access to samples from that distribution. So instead of solving this problem, we would need to solve the empirical problem where that target distribution is replaced by the empirical distribution of our samples. So we'll solve this problem. Is this setting clear? So this is Is this setting clear? So, this is the GAN setting in mathematical terms. Basically, you're trying to learn this mapping so that you minimize the distance. And a popular distance measure here is the Wascherstein distance between the distribution of the output here that you're able to generate and the distribution of your samples in this case. Small question. So, from the previous slides, we see that randomized. Slides: We see that randomization is important, but in these generators, there is no randomization, right? Yes, yeah, so far, no, yeah, so far, basically, I'm defining this problem formulation by assuming that this as a loss function, as our measure for the way we measure the distance between the generated distribution and the started distribution, is by just using the classical Muscherstein distance. But a lot of times in a lot of these GAN applications, uh so the samples that we would like to use can contain personal and private information. For example, if you want to train a generative model, let's say that's able to generate realistic images of human faces, then you need to train it with a lot of pictures, photos of real. Pictures, photos of real human faces. And maybe I wouldn't want Google to use my selfies on my phone to train their generative model because I would worry that one day that model, when sampled, can output a picture that's very close to my personal picture and it can give it to a stranger, right? Actually, these generative models are known to suffer from this problem. Sometimes they memorize the training. Sometimes they memorize the training set, and the samples they generate are very similar to the ones in the training set. So, one way to get around that problem could be the following. Instead of providing them the raw samples, for example, my photo, what we can do is we can first privatize our samples and provide the algorithm only the privatized samples. And ideally, we would do that with a pro- Ideally, we would do that with a proper locally differentially private mechanism. I won't define what differential privacy means. It has a metaphorical definition. But think of it just taking your sample and passing it through a noisy channel so that you're able to hide some of the features. For example, if I want to train a generative model for the, let's say, NNIS data set that's able to generate images that look Images that look like Henry Clinton digits, the normal way to train that model would be to give it samples like this, right? Instead of giving it these samples, what I would like to do is I would like to first privatize these samples. Think of these on the right, these are obtained from these images by adding a lot of pixel-wise noise. So enough noise so that you cannot recover the original image. And then what we want to do is we want to provide Want to do is we want to provide the algorithm only these noisy images, these privatized images, and we hope that the algorithm can still learn to generate, obviously we wouldn't want to learn to generate like noisy images, but we would want to be able to generate basically images that look like these low samples. And if you want to do that, basically, by just using the previous framework from GANS. Framework for GANs, we will have a problem, right? So, this was our previous framework. I can just use the privatized images in this framework because then my generator, right, the G that will minimize this metric, will be such that this distribution is close to this one, to the distribution of the privatized images. Essentially, I will learn to generate noisy images, which is not what I want. The question is, how do we modify this framework so that This framework so that we can work with noisy images. We have only access to privatized images, but we are actually able to learn the original, the raw data distribution, the unprivatized data distribution. And kind of the key observation here is that just replacing this Washerstein distance here with the entropic Wascherstein distance, with the one where you regularize with neutral information, you'll have that denoising. You'll have that denoising effect for you. So you shouldn't minimize this objective because Gandharth washes transcends, but instead you should minimize this one. And this is actually true when privatization is done with the Gaussian mechanism. So let's say privatization is done with the Gaussian mechanism. This is actually the row on the top by adding a lot of Gaussian noise to the image. Then what you can To the image, then what you can do is you can take those prioritized images and then solve this problem here, where these are the prioritized images, and this is the information regularized Washerstein distance. And actually, this regularization parameter here will need to be chosen to match the noise levels in your mechanism. And the claim is that if that's the case, then this will make this distribution of the output distribution converge to the true. Converge to the true raw data distribution. So PU, U are our raw samples, and PU is the raw data distribution, the unprioritized one. Is this clear what I'm saying? So I'm just saying basically you regularize with mutual information and that has that denoising effect, not only individual samples, but you will be learning the right distribution. Okay, and you can say a similar thing, for example, for other privatization mechanisms. If you use Laplace for privatization, then the claim is that now you should minimize this objective. This is information regularized W1 Washerstein distance. Again, the regularization parameter has to be chosen to match the noise level in your mechanism. And then even if you plug in basically the If you plug in basically the privatized data distribution here, the output of your generator will converge to the raw data distribution. So you'll be learning to generate clean samples and not privatized samples. And you can generalize this. You can say you can do that for any privatization mechanism. Again, a privatization mechanism is just like a noisy channel you apply on your samples. And now for any privatization mechanism, Any privatization mechanism, you can do the following. You can use this information regularized optimal transport as your loss function. But what you need to do is here in the cost for your optimal transport, you have to use the right cost function that matches the privatization mechanism. Actually, this is the more general result. So, when the privatization is done by the addition of Gaussian noise. By the addition of Gaussian noise or the Laplace noise. These are two popular ways people do privatization in practice. Then this will reduce to just the cost will reduce to the L1 distance or L2 distance. So that will be reduced to just information regularized Washerstein one distance or information regularized Washerstein two distance. So this is really the main message I would like to convey. Like introduce mutual information as a regularized Introduce mutual information as a regularizer, and that will have this effect of kind of deprivatizing the distribution of the individual samples. Just sorry, I'm a bit naive, but if you look at the optimization problem, there is no U anywhere, but P G star of Z is actually becoming P U, right? So how is somehow U must come into the picture in the optimization problem, right? Well, this Py, right. Well, this PY, right, this is the privatized distribution. So this is basically the PU, the original source distribution, convolved with the privatization channel, basically, right? So inherently this one is the outputs of PU, you take the PU distribution and you pass it through this prioritization mechanism. This is the distribution we get. And in the objective here, you play. In the objective here, you plan for reversing, if you like, the privatization you applied. We assume that you know the privatization mechanism. This is usually the case because it's the engineer that privatizes these images. So you know what privatization was applied. And then this basically, the claim is that this reverses that effect and this PGZ converges to the part in PY that corresponds to the raw data distribution. I don't know. Distribution. I don't know if this is. Thank you. Yeah, thank you. Yeah. And this actually has a very simple reason why adding mutual information has this effect. And it just comes from the rate distortion function on a squared error for a Gaussian source, which is like one of the basic things I guess we learn in a first information theory course. So this is the rate distortion function under squared. function under squared error, right? This is exactly what it is. And we know that we can characterize this rate distortion function for any source, any distribution of the source. So I denote the source by y here and the reconstruction by y hat. But we know in one case we can exactly characterize this function, and that's when the source is Gaussian, zero mean with some variance, and the basic result is. And the basic result is that in this case, the optimal test channel will be such that the reconstruction distribution will also be Gaussian with zero mean and now less variance. And this is how the source random variable will be related to the reconstruction random variable. And this is just a standard result from Thomas and Hauer, I guess. But one thing I think I didn't realize when I was taking my I didn't realize when I was taking my first information theory courses that you can extend this a little bit to say the following. So it turns out that if the source distribution is of this form, let's say, it's not Gaussian anymore, but it has this part U, and you can have an arbitrary distribution, plus you have this independent Gaussian noise adding to U. Obviously, this now can have some. Obviously, this now can have some arbitrary distribution, so we can't characterize the rate distortion function for y in general, but you can characterize it at one point. If the distortion you want matches precisely the variance of the Gaussian parts here, then it turns out that you can write down the function exactly by the same arguments here, and the optimal test channel will be such that this reconstruction distribution. This reconstruction distribution will have the same distribution as this part here. So essentially, you are starting with the source distribution, which is some distribution plus Gaussian noise, and the reconstruction distribution just corresponds to the distribution of Gaussian. This is what happens in ray distortion theory, and this is exactly what happens in my problem. The significance here is that this distribution. Is that this distribution matches the distribution of this unit? That's why I'm able to start with this, and I have the mutual information as a regularizer, which is like the Lagrangian version of this, and then I recover this distribution. Obviously, in my problem, there is no rate, there is no distortion, but it's essentially the same. Okay, and here are some basically experiments here. These are just samples from the NLIST data set. So, normally, of course. Data set. So, normally, of course, you will train your generative model by using these images. Now, what we do is we first privatize these images by adding a lot of noise. And we only give the algorithm these privatized images. And here we did a little bit of experimenting. Basically, we were trying to see if you can use state-of-the-art denoising algorithms and take these images and individually denoise them. Denoise them. Turns out that there is so much noise here, you can't really individually recover the original images here. But what you can do is you can take these privatized images, you can give them to this entropic W GAN, and that will basically learn to generate images that look like handwritten digits. So, for this noise level, this is not perfect. You see that there is still a little bit of noise, but you see that the algorithm. But you see that the algorithm is actually able to learn, so it does that denoising, deprivatization of the distribution and is able to generate the wrong images. Whereas if you just do standard GAN with the standard Washerstein distance and you just give it these privatized images, you won't get anything, any image. Okay, I guess this was really the observation I wanted to convey. There is more to the story. Convey. There is more to the story. So basically, a lot of the effort in the paper is devoted to proving convergence rates. So this is the second part of the story. So remember, in the beginning, I told you that ideally in the GAM problem, we want to solve the first problem. That PY is. Did I run out of time? It seems like I lift it. Okay? Maybe I'll just say one minute. Okay, so I'll just say the following. This is the population problem, right? And that's what you want to ideally solve, but you don't know the distribution you are trying to learn. So obviously, you solve this problem where the target function is replaced by this empirical distribution. And there is this question of how fast this empirical solution approaches the population solution. And it's known that with standard Washer's time distance, That with standard Washer's time distance, this convergence is very, very slow. You suffer what is called percursive dimensionality. So, when D is like a pan or something, this will be very, very small. And adding mutual information, instead of using the mutual information that is your loss function, the regularized Bosch's time distance, also removes this curse of dimensionality. And now the you can prove that the empirical solution will converge to the population solution. To the population solution much faster if everybody partners. So I'll start with some