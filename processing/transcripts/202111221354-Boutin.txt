Everybody here alright? It's nice to talk without a mask and use the blackboard. This is a talk. This is some work with my physician Kindle King and Uli Valka. First time I worked with Uli, guess you know, pandemic forced us to start working together. Alright, so I'm going to talk about an inheritance theory problem. There's no moving from really the methods that I've used for this problem, but towards the end, I'd like to, you know, like there are some follow-up questions that I think might be solvable. Up questions that I think might be solvable using moving frames, but the problem that I'm looking at is the problem of recognizing the shape of a point configuration from some noisy measurements. So we know how to do things when the measurements are exact, but when noise comes in, sometimes things get a little bit more difficult. So let me start with the exact labeled case. Just to situate yourself a little bit, we'll start with what we know how to do. Little bit, we'll start with what we know how to do. So, the thing, the problem is, I have a set of endpoints in Rd, and I have another set of endpoints in RD. And they're labeled. So, I would like to determine if I can take the first point configuration, rotate it, and translate it, and map it onto the second point configuration. So, I want to determine. If there exists an orthogonal transform, I'm going to write it as R because otherwise it's too many O's. An orthogonal transform and a translation such that Pi is equal to the orthogonal transform applied to Qi plus the translation. Plus the translation for all i from 1 to x. So this is an example from, and we know the solution. The solution is to check the pairwise distances between the points. So we know that this is going to be the case. You can map one point configuration onto the others. If and only if the Euclidean distance between PI and PJ is the same as the Euclidean distance between PJ. The Euclidean distance between Qi and Qj for Oi and J from 1 to N. So this is an equivalence problem that is solved by looking at some invariants, right? Why is that the case? Well, that's because you can actually reconstruct the P's from their pairwise distances. Well, you cannot reconstruct exactly the Ps, but you can reconstruct them up to a Euclidean transform. I'm going to say not E V from, well, you could start from the matrix, the following matrix. So I'm going to describe entry ij of this matrix, and it's going to be the distance between It's going to be the distance between pi and pn squared. And add to this the distance between pj and pn squared and subtract the distance between pi and pj. So this is entry ij of my matrix and I list all ij from 1 to n minus 1. So this matrix is just made of distances. Squares. Did I put a square? Yes. Thank you. See, if I had slides, it'd be perfect. I would have my 2 there, but this is so much more, you know. Two there, but this is so much more, you know. All right, so why is it that I can reconstruct? Because this matrix, which only contains distances, can actually be factored as two times the dot product between the difference Pi and Pn and the difference between Pj and Pn. I j from 1 to the n minus 1. So in other words, this is a gram matrix. And you know, I construct it from my distances, and I can factor it using singular value decomposition. So this will give me Q times the diagonal times the transpose of the first matrix. And lambda here is diagonal and has a zero at the very bottom. And to get your P1 and your Pn, you take the square root of that diagonal. The square root of that diagonal matrix and you multiply it by Q. Obviously, this is not the unique solution, but if you want to recover your P's, P1 minus Pn, P minus Pn, that we've got until Pn minus 1 is Pn, and of course here you have the 0 at the bottom. You're going to have to put some orthogonal transform here to get your piece. So you can reconstruct your P. So you can reconstruct your piece up to a translation and an orthogonal transform. So this is why you can determine if a one-point configuration is equivalent to the other one modular Euclidian transform. So we all know this. I'm just going through the basics. So what happens when I add noise? So I have to add some noise. So when I add noise, I'm going to measure my point, but not exactly. But not exactly. So I'm gonna be the noisy labeled case. So I'm gonna write the noisy variables with a bar. So Pi bar is going to be approximately Pi. This is all my noisy measurement of Pi. I from 1 to n, and similarly, each one of my q's is measured as qi bar. I equal 1 up to n. And I want to determine if you can take your PI bars and map them approximately to your QI bars. In other words, does there exist a recovery? In other words, does there exist a recognition transform a translation such that Pi, whoops not tilde, Pi bar is approximately a QI bar, the noisy QI rotated and translated for all. So the analog of the other question. Well, how do we do this? Well, how do we do this? A solution, of course. Very, we can take that previous solution and sort of extend it to this case. We just have to check if the distance between the BI bars are approximately equal to the pairwise distances between the QI bars and the QJ bars. So the invariants I described earlier are the exact invariants. They remain similar. They remain similar when you have a similar point configuration. So comparing the invariance and asthma to be closed will guarantee that your point configuration was closed to begin. And why is that when we add noise, you know, we can reconstruct exactly the same way as before and by continuing the reconstruction process, it gives you something similar to what you had earlier. So these are the two easy cases. But things get fun when you look at practical applications. Get fun when you look at practical applications because a lot of times, then you don't actually have a label. You don't know which point is the first one, which one is the second one, and which one is the third one. So you have the very interesting case of an unlabeled point configuration. So now I'm going to look at the exact unlabeled case. I'm probably the first one to write on this board in like a year and a half. So I'm going to drop those labels now. I don't know what they are, so this will add a permutation group SM to my problem. To my problem. Alright, so this time I'm going to take endpoints in RD and another set of endpoints in Rd, but I'm going to need to ask that they are generic. So there's going to be exceptions, so I need to specify that they're generic. And I want to determine if There is a permutation of the label, so pi m Sn, the group of permutations of an element, an orthogonal transform, and a translation such that PI is equal to R times Q relabeled by the pot. Labeled by the pot, by the pi map, plus the translation. So I take my point configuration. I don't know if I have to draw a picture. I don't give them a label. I just want this set of points to be rotated and translated and create this other set of points. So no more label. I don't worry about the correspondence. So the solution to that problem, well, there's many solutions. Well, there's many solutions, but one of them is to simply look at the set, the pairwise distances between the P's and then drop the label. Forget the label. So these are going to be equivalent if and only if the multi-set of the distance between Pi and Pj for all i and j. So it's a multi-set because you may have potentially repeated. Because you may have potentially repeated distances. You just list them all. And if that multiset for the P's is the same as the multiset for the Q's, then you are guaranteed that the P's were equivalent up to a relabeling in a region motion. So this is of course called a bag of distances. You just list all the distances that you have and you forget their ordering. It's also called the distribution of the distances. So instead of comparing the labeled distances, you compare just their values. So that's the solution. However, we need to add noise. So let's cover the noisy on the cold case. Mimi, can I ask you a question? Sure, Laura. So if you compare those two balance, right? And you say, oh, they're the same, so I can maybe put some sort of relationship saying this difference is the same as this, and this and that, et cetera, because there is a bijection, right? So at least one. So you can, is there a way to require? Is there a way to recover the order or not? Or under some conditions? You mean to recover the positions? Yes. Yes. It's a little bit brute force, numerics. Okay. Yeah. The best I've seen was a paper by, oh my god, I'm drawing blanking on the name, somebody from Vancouver, UBC, who had written a quadratic programming approach to solving that problem. And the problem is that we're running into. The problem is that we're running into like very complex, like theoretically, it could have been very complex, but their algorithm was actually very fast when there was only a unique solution. And we were able to say, look, there's a unique solution pretty much all the time. And so they guarantee that their algorithm will run fast all the time. But their algorithm is actually able to handle the case where there's more than one reconstruction and give them all. So yeah, brute force numerics, I guess. And I'm not going to change. I don't have to show you a solution for reconstruction. Okay, thank you. My pleasure. So this is just like Gromohov.distance equal to zero in the linear case, right? That's just one instance of this. You just take two sets, pediment, and then just say Chromohov.distance equal to zero. Alright, so let's add some noise. Now in contrast to the other case where we just able to add noise and it was not a big deal, this is actually a big deal. Big deal. Noise is very difficult to handle. So this is a noisy, unlabeled case. Alright, so this time, again, we are given a noisy measurement of Pi, a noisy measurement of Qi, I equal 1 to 2n. We have the same in each set. I mean, you could argue that that's not a good setup because when you numerically find your points, sometimes you drop one or you add an extra one. So one could argue this is not necessarily a good setup, but even that simpler setup is hard to solve. We want to determine if there exists a perpetuation of the labels, an orthogonal transform, and a translation. And the translation such that the noisy PI is approximately the same as some noisy Q. Not sure which one, but they're labeling back and tell me what it is. Alright, so here is not a solution. This is not a solution. It is not okay to choose. It is not okay to check whether the back of the multi-set of those distances are similar. We cannot do that. This would not work. It is true in the exact case. I can just compare those pairwise distances, but it is not true in the noisy case at all. However, the reason why is The reason why is because of my assumption that the points be generic up here. This is the problem. There are exceptions, and those exceptions really come and interfere with the noisy case. So let me explain why this doesn't work, because people try to do this in practice, and it doesn't work. So, why? We're going to start from two non-generic equivalent configuration, and I'll draw some. Configuration, and I'll draw some to illustrate. So start from two non-generic point configurations with the same set of distances. So I'm going to draw, of course, you know, the kite and the trapezoid. So the kite took us a long time. Took us a long time to find an example. And the trip reason. So that's four points in each case. And I'm going to put root 2 and root 2 as those sides. I'm going to put root 10 and root 10 as these other two sides. This one will be 2 and this one will be 4. Now I'm going to take the same exact sticks and stick them. Sticks and stick them into my trapezoid. This will be my root 2, my root 2, this will be the 4, this will be the 2, and in the middle will be the 2 remaining group 10. And we can check this actually fits. So this is a counter example to my reconstruction, to my comparison method over there. These are two sets that definitely do not have the same shape, yet they have the same multi-set of distances, namely every two. Root 2, root 2, 2, root 10, and 4. 1, 2, 3, 4, 5. I'm missing one. Root 10 and 4. So the multi-set of distances is the same in both cases. Now it's not a big deal when you're doing exact comparison because these examples are very rare and we have a pretty good understanding of how to avoid them. but when you start adding noise this create a problem. So what we're going to do is we're going to pick a generic point configuration near them. So I'm going to put my P1 somewhere near the kite, the first point of the kite. I'll put my P2 here somewhere near, but I'm going to make it generic. So wiggle, make it generic. So I have P3 and then P4 near my kite. And we can do the same for the Q's and think them near the Same for the Q's and think them near the trapezoid. So somewhere near, generic. Q1, this is going to be my Q2, this is going to be my Q3, and my Q4. And you notice, of course, I put 1, 2, 3, 4 there for my ordering. Here is, you know, 1, 3, 2, 4 because the labeling is really ad hoc. Okay, so I have, these are not my noisy points. These are some generic point configuration in the vicinity of my non-generic pair. And then I'm going to add noise to that. And then I'm going to add noise to that one. So I'm going to measure, you know, P1 a little bit further. So this will be my P1, this is P2, sorry. P2 a little bit further, P2 bar. My P1 measure it a little bit further, P2 bar, P1 bar here. Just measure, you know, add a little bit of error to that generic point configuration and do the same here. So a little bit of error, a little bit of error. So you get your Q two bar a little bit further, your Q three bar over here. Your Q3 bar right over here. This was Q, I don't know which one. Q1 bar and Q4 bar somewhere there. So I'm measuring my generic point configuration with a little bit of noise. So, obviously, the pairwise distances between the noisy measurement is going to be approximately this for the noisy kite. No, not the noisy kite. The noisy generic configuration that's near the kite. point configuration that's near the kite and it's also going to be close to the same set for the noisy point configuration that's near the trapezoid. So it is not true that similar distances yields a similar shape. Because in this case I have approximately a trapezoid and approximately a kite. Does that make sense? There's a ton of these counterexamples. There's a ton of these counterexamples, well there's not a ton, like there's a set of measuring zero of those counterexamples where they're kind of everywhere in the space. And when you're in the vicinity of them, even if you're at the engineering point, you're going to encounter an ill conditioning. Ill conditioning means small changes in here yields big changes in your solution. So if you took a tubular neighborhood of your exceptional set, which is kind of what you do. What's left over? Oh, that's a good question. Will he start to understand that with his genius a lot? Really is trying to understand that with his students a lot. Some people are trying to understand. I think it's really, really messy. It's everywhere. Yeah. But it's not a problem if you're trying to do exact stuff, because this is very real. But with noise, these counterexamples create problems. There's noise. So you can't take that method and extend it. So that's actually doom, this direction of work for trying to recognize things approximately in real life. This approach? Approach. And there's a lot of all this stuff that you're trying to do with. I mean, it's a nice, I like it. I think it's a really nice way of comparing my configuration, but in terms of application, it's limited. So what can we do? We are stuck. We really need an approach that's going to address that from the get-go. So I want to propose a different sort of different view. Sort of a different view of the problem, if you will, that's specifically made for handling noisy measurements. And the good thing about noisy measurements is that in many cases you can measure several times. You know, if you're able to take one measurement, maybe you can repeat the measure again and again and again. So you're able to get more information. So using that principle, we're going to take a different point. We're going to take a different point of view. So instead, we're going to view each point Pi, the true measurement, not the noisy one, as the mean of a Gaussian. So we're going to have n Gaussians. And the measurement Pi bar will be viewed as a random variable drawn following a normal distribution with mean A normal distribution with mean Pi and some covariance matrix sigma representing my noise. Okay? Now I want to go slow through the setup process because even though it's super simple when you work with distributions all the time, we need to make sure we understand what I'm doing there. So feel free to stop me. So now I'm going to look at this whole Let me make a picture. So, I have my P1 here, and I'm measuring P1 following maybe some normal distribution. And I have my P2 somewhere there, and then when I measure it, I'm probably going to be off by a little bit. Don't mind about P3 here, and so on. So, I have really a mixture of Gaussian. So, I'm going to build that mixture of Gaussian, and I'm going to call it F of X. So, I'm going to allow the Gaussians to have their I'm going to allow the Gaussians to have different weights. The weights are going to be alpha i, i from 1 up to n. I have n points, therefore I've ended up with n Gaussians. And then each component is a Gaussian centered at Pi with noise sigma. I guess the noise can depend on I too, but I don't have a solution for the full case, so I'm going to have to specialize sigma in just a bit. So what's the expression for Gaussian? 2π. 2 pi to the d over 2, the determinant of sigma i to the 1/2, and then e to the minus x minus pi, sigma i inverse transpose x minus p i divided by 2, right? So I'm just going to sum Gaussians, right? So one Gaussian, another Gaussian, another Gaussian. Some might be more likely to be measured than others, representing the fact that maybe one is in a different corner. One is in a different corner of your image, and you don't catch it very well for whatever reason. And so, this is what I call a mixture of inducts. So I put them all together. And I can sample from that Gaussian. I can obtain random sample. So x1, x2, x3. X1, X2, X3, can get a bunch of samples, probably as many as I want. And these are all drawn from my Gaussian mixture. So what does that mean? That means I pick one of those clouds following the probability of the weight, let's say the second one. And then once I've picked, okay, it's going to come from this Gaussian, then I've picked following that Gaussian, so more likely to be here than there. Every time you pick, you end up picking a Gaussian, and then within that Gaussian, you pick a point. Gaussian, you pick a point. And if you pick a lot of samples and you draw them in plates, it's going to look kind of like this. Well, it's here, a bunch of them together, there'll be a few in between, but by and large you'll have a clump, a clump, a clump. So this is the object that I want to play with, this here. This is an object that represents my point configurations, the means of the Gaussian, but it also represents the noisy measurement process as part of it, through that sigma here. Okay, so here's the question. So here's a question that I want to solve. So very, very similar to this. Okay, so I'm going to take f of x, like acid mixture, and just Mixture and G of X, another Gaussian mixture with the same number of components. So it'll be two mixtures of n Gaussians. Okay? And what I want to do is determine if there exists an orthogonal transition. An orthogonal transform in a translation such that I can get the first Gaussian from the second after moving my coordinate space with the rotation and the translation. Can I map one Gaussian mixture onto the other with the rotation and the translation? So very, very similar, right? So obviously I'm mapping the point configuration form by the means together, but also the whole By the means together, but also the whole entire noise model. So here's the solution. Not in the whole general case, there's still some computations to be done, but if the covariance matrix sigma is a diagonal matrix, like just a scalar diagonal matrix, we have a solution. What is a solution? So this is true if and only if the, you have to bear with me here, I'll have to define my error. With me here, I'll have to define my error. So, what I'm going to do is I'm going to take the probability density function of the distance between two samples squared when they're drawn following F. And if that distribution of distances is the same as the other, namely the distribution of the distance squared when the x1 and x2 come from the second Gaussian. They're the same. Gaussian. If they're the same distribution, then the original Gaussian mixtures are the same of the original ocean. So this one here is the probability, can I use this? Probability density function of the distance squared when x1 and x2 are independently drawn following f of x. And this thing here is the PDF. Here is the PDF of the distance squared when x1 and x2 are drawn from each other. So the distribution of the distance uniquely characterized the shape of the gauss. So sort of natural extension. I need my, this is, I cannot prove it in general, but this is when, okay, the first thing I need to assume is that sigma i is sigma squared times. This sigma squared times z identity is a very special noise model, which actually makes sense physically, you know, where every direction is equally noisy. And the other thing I need to assume, uh-huh, well, naturally, the means of the gasoline mixtures form a generic point of configuration. Those are exceptions, but just like they were exception in the case. In fact, if I let, if my Gauss's become direct deltas, then I'm back to square one or the other exact case. So this is one way to reformulate the problem with the solution. And I want to show you why, you know, you look at this and you think, oh, you're trying to use densities. Use densities, and so it's going to be lots of F on deltas, you know, real analysis type of stuff. No, it's an algebraic problem, it's completely algebraic, which makes it very beautiful. So, I'll show you how to prove it. This is an algebraic question. It's about neosymmetric polynomials, power sums. It's all algebraic. Like sigma squared here, is it sigma or it should be the sigma small sigma? It's a little sigma. Yeah, is it does it depend on i or should it be the same for So all I'm saying what I'm saying is that those here spherical and the variance is sigma. But for each point it's the same. For each, yes. Is that answering? Yes, every single one. So this is my P1, and then P2 has the same exact variance. I guess that should be sigma, right? Alright, alright. And then P3 and so on. So if you do this in Rd, there will be a spherical region representing the equiprobable lines. Yeah. I mean, it's a very special case, but the end of means can be anything. Oh, sorry, if the sigma squared is known. Yeah. Big deal. So, I know my noise model. If I know my noise model and it's the same in both cases, then I'll be able to determine if the gas admixtures are the same. Great, so I'm not even with time. You need to see how much detail. Hopefully, that will give some ideas on how to address the future questions. I'll bring up at the end. Okay, so to solve this problem, So, to solve this problem, to prove that this is true, we look at the moment-generating function of the distribution of distances. This is really the key. In other words, we look at the expected value. At the expected value of e to the t, where t is an indeterminate, times the distance between x1 and x2 squared when x1 and x2 are drawn from f of x. So this is a function of t. We can write this as f of t, and we can expand it as a power series in t from 0 to infinity. From 0 to infinity. So this gives us power t to the n over n factorial. And the coefficient in front of it, mn, is naturally the moment of order n of that distribution. Alright, and these are the coordinates I'm going to use. And I'm going to look at the action of a group on the density, but really what I'm doing is I'm acting on these. Acting on these moments. So, what are these moments? Mm depends on the parameters of my Gaussian. So, it depends on P1, P2, all the means of my Gaussians. It also depends on the weight. And it is polynomial. It's a polynomial expression, and I can actually compute it. So, fairly, these are the means of my Gaussians, these are my weights. These are my weights. So, in other words, I'm asking, okay, I have some polynomial functions of my means and my weights. Can I reconstruct my means and my weights? Well, I can't reconstruct them per se, but I can hopefully reconstruct them up to a relabeling, right? Because I don't know which guessing was first and which one was second, but can I reconstruct this set of means with the associate set of weights? So this is like, you know, the This is like you know the act. I'm acting on like if I look at the expect the moment generating function for this f, I'm going to have moments, I act on them, and I can invariantize. Anyway, here we know what the invariant is, and we're going to show we can reconstruct. So what are they? We know if I know my distribution of distance, I know its moments. I can compute it. You know, even if numerically. Even if numerically I can just draw a bunch of samples, I can estimate my moment. So I can assume that I know the value of these polynomial functions. And the question is, can I reconstruct the variables? Modulo between motion in the leeway. So what are these moments? Well, if you write them down, they look like a mess. They look like a mess until you realize a little trick. So, this is where it gets anti-climatic because this was a really difficult problem that we looked at from every angle and we just didn't think we were going to be able to solve it until we saw this trick, of the trick. As we are going to decompose our moment-generating function into a sum of moment-generating functions where the x1 comes from one pair and the x2 comes from the x1. pair and the x2 comes from the the x1 comes from one Gaussian and the x2 comes from the other Gaussian. So if I write my Gaussian mixture as a sum alpha i times Gaussian x fi, i to 1 to n, where fi, of course, is my Gaussian. Now, uh sig squared times 2x minus q. Then I can write my m of t as a summation ij from 1 to n, m i j of t, where m i j is the moment, actually I'm not going to put the alpha i and alpha j here. alpha i and alpha j here. Where alphij then becomes the moment generating function when x1 is from f1 and fi and x2 is from fj. What is the expected value of e to the t distance between x1 minus x2 squared when x1 is picked from fi and x2 is picked from fj. And then we can integrate that. Not a difficult integration. And then we actually get the expression for mij of t. And then we hope we can expand it. It is 1 over the square root of 1 minus 2d sigma squared t times e. So the t times, here comes the pairwise distance between, oops, I'm going to put the means squared over some not too complicated denominator. This is MiGFT, which I can expand as a power series. The terms of that power series are going to be functions of, well, they have to be invariant under read labeling, and they have to be functions of. Functions of these distances, right? And that's sigma. And it turns out that the first moment is just essentially the power summation of pi minus pj squared over all ij. Sort of like the power sum of the green one in my distances. And the next one is the power sum in degree two, essentially. Essentially. So knowing the moments of m ij of t actually gives me the knowledge of this it also gives me. It also gives me the knowledge of summation i equal 1 to n alpha i distance between pi and pj squared. It also tells me what is the value of this and so on. I find oops 2 this should be yeah so this says 2 to the 1 to the 2 to the 3 and so forth. I find out all the power sums. And so forth. I find out all the power sums. So then I just use standard results to recover my multi-set of pi minus pj and the corresponding alpha i that go with it. Just alpha. So that's how you prove it, and so it's true. When the sigma is more general, the equations get more complicated. We haven't been able quite to figure them out yet, but it should be okay. But obviously, this is very It should be okay, but obviously, this is a very discrete problem, boiling down to symmetric polynomials. What if we wanted to look at other objects, like a curve with some noise, right? Is there some kind of, we'll look at other types of distribution. What if f of x is some other distribution? And I look at the statistics of some invariant. That's essentially what I did. I took an invariant, a distance squared, and I look at its density. So again, I look at the density of an invariant and show that the density of that invariant is enough to characterize my original density up to a World transformation. So there's lots of possibilities there that one could look at. Of course, the case of That one could look at. Of course, a piece of curve is kind of interesting. What if I have a curve? So this is like my mean, and then I add some noise. So I don't know, I guess you could pick the points, you know, first pick a point uniformly on the curve, and then once you've found the point, then deviate in this direction, some kind of noise model for the curve, then you would have a density here from which you could sample. What invariance could you compute? And then you get the new density, R of that some invariant here, and can you go back? No. So I think there's lots of problems of this nature one could look at besides this very discrete case that I considered. So I guess that's about time, right? That's all. Anyone has any questions for me? I do. So if you go to the initial problem that you started with, so just like measuring distances, of course, remember configuration if configuration is numbered, right? But if configuration is not numbered, as I show an example by Ben and Snowden, but what if you include not only a D. You include not only distances but scalar products or whatever, right? So then this comes with different portabilities. So, in a sense, you carry more information from ground metrics, right, to this. And yeah, so does it help actually? So we looked at those cases, there's always exceptions as far as we can tell. Like even if you add distance areas or volumes or things like this, there are feelings that there's always going to be exceptions. Always going to be exceptions because of the way the proof is built. The idea is you look at the reason why the theorem is true is you look at the functional relationship between the invariants, which are generated by these layers, right? They're a little zero determinant. And what you want to show is if you permute the distances in a way that isn't permuting the points, then you're taking some of these syzygies and maybe mapping them outside of the ideal generated by them. But if you happen But if you happen to be at a point that that's so essentially this you take this polynomial and you map it to one outside the ideal and you're saying well if this polynomial is not equal to zero then there's no way it could have been zero therefore the syzygy was violated. That's essentially the idea. So anytime, any invariance you add is going to satisfy some syzygies and you're going to say, well, if a point happens to make it zero, make this polynomial equal to zero outside of the set. Equal to zero outside of the set generator, if I did my syzygies, then it will potentially be an exception. So I don't see any reason why adding more information would actually resolve. Probably, but because you know it's actually finite generators, right? And of course if you just take and symmetry some of the polymer storms there, maybe you don't get enough generators, but you know that there's finite number generators, that's enough, right? You maybe just have to extend this, right? Yeah, if you find a way to remove the exceptions, I mean, because there's To remove the exceptions, I mean, you know, because there's lots of good things that could come out of it. I just haven't been able to do that. Yeah, it's just portion by finding rules. So, of course, you should be able to. It's just been more complicated to generate that, but I still theorem applicable to this, of course. As a question, like whether just including the other information from metrics will suffice, probably yes. Yeah. Yeah, let's do it. If it's possible, let's totally do it. Yeah. I'm going to ask the opposite question. What happens if you have missing data? Happens if you have missing data. Right, so in this other, in this second formulation, it's possible to miss a Gaussian. Yeah. Right? It's saying you sample, and then it's possible that one alpha is very, right, is very small, and you miss it. Right. Or you sample a lot, you can just get a few samples, or you have extraneous. So that formulation sort of natural lends itself to having extraneous components or missing some. Now, obviously. Missing some. Now, obviously, at the end of the day, you'll have to phrase that as a hypothesis testing problem. You'll see, my hypothesis, I have this. How far am I so that it can reject my null hypothesis? And so there's always going to be some probability of being wrong when you reject your null hypothesis. But yeah, it definitely allows for that. And that's one of the reasons why we're excited about that formulation. People on Zoom, maybe that doesn't sound like it. They're there, yes, yeah. I made sure you're there. Okay, that's good. Well, then, thank you very much for having me. Another plan is to take a 20-minute break. We're the next stop. I was tempted to take, like, shorten my time, use like an extra 10 minutes and talk on my algo problem, which has moving forward. Talk about my algo chrome, which has moving phrases in it. Do you want to present it? Improvise stuff recording. Oh, I'll get that. Okay.