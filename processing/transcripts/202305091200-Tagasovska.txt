Optimization procedures, example, new molecules or new antibodies that actually comply with multiple properties at the same time. And then the second part will be more related to the selection process because with sampling, with machine learning algorithms, we can of course generate millions of samples, but we cannot test all of them in the lab. So we have to use a smart way that does a trade-off between what are the most informative and the most promising. Most informative and the most promising antibodies to be sent into the lab, so that will be more related to active learning and Bayesian optimization. But we also try to constrain that more and to tailor it to antibody design. So this is the part that I'm not going to go too much into. Because as we saw before, usually for antibodies, we start with some immunization campaign. The animals have been introduced to some antigens, they produce antibodies, but we cannot directly use. Produce antibodies, but we cannot directly use those antibodies as drugs because they have to be humanized and optimized for multiple properties, such as binding, potentially non-specificity, and so on, before being actually sent into inventory or used as drugs. So the first part will be, as I said, related to how do we actually sample, because actually, what I didn't mention is that from the immunization campaigns, usually the Campaigns, usually the antibody engineering team would select a number of seeds or lead molecules that look like good antibodies. But then, before optimization, we would actually like to look into the neighborhood of all of those molecules because we might have something that is actually an antibody with stronger properties and better humanization features and so on. So, we actually want to explore the neighborhood around antibodies that we already know that are promising. That are promising drugs, but potential drugs. So, for the sampling part, actually, before going into the sampling part, I just wanted to make it a bit more formal and say that although antibodies can have multiple representations, so the first image is how we think that they look in real life, the second one is the structure representation that we already saw in multiple talks, and the third one that I'm going to use throughout the talk is based on. Throughout the talk, it is based on sequence representation. So, actually, we use the chain of amino acid acids that constitute an antibody. Usually, they are of length around 250. And in each of those positions, each of those 250 positions, we can have one out of the 20 amino acids. So, this is just how we represent our data. And then, for each of those antibodies, we are interested in optimizing a number of scalar properties. A number of scalar properties. In this case, we denote them as M. And what we want to do is also that not to just generate as many antibodies with those properties, but also make sure that each of those antibodies has all of those M properties because they can be competing with each other. And as we saw, that there is a Pareto frontier. So we want to take that into account. So we are starting from the sampling procedure. We have many generative models at present. Many generative models at present, but one of the most promising ones that was, as Rich said, our leading method up until two months ago was based on energy-based models or the Langevin manifold sampler. And we preferred it because it's actually very good in generating realistic data and it was able to generate realistic antibodies that were expressing quite well. And this is just the basic introduction into. The basic introduction into energy-based models, where we actually have a neural network that lets us parametriz the energy function. So we learn how the data distribution looks like. And by doing that, we don't only have representation of the data distribution, but we can also sample new molecules in our case from that data distribution. And for the sampling procedure, we use the Langevin diffusion, which is just an iterative process where we start from some noise, which can be Gaussian or unit. Some noise which can be Gaussian or uniform, and then we pass it through the energy-based model that was trained on the realistically looked data. We take the gradients of that first pass, and then we iteratively refine it at every step by also adding a noise. And this works quite well for generating antibodies. But then we mentioned that we want those antibodies to have multiple properties at the same time. And another nice thing: Time. And another nice thing about energy-based models is that we can combine multiple energy-based models by the means of product of experts. So then at the sampling step, so what we do is we train three, five, ten energy-based models, depending on how many properties we have. And then at the sampling time, when we want to generate new molecules, we take the gradients with regards to each of those models, then we sum them. And then this gives us an antibody that has all of those. Antibody that has all of those properties. But this is kind of too easy to work. So, because as we said, we cannot actually optimize for all of the properties at the same time. In one-dimensional case, it's quite easy because we can easily see that the two samples at the top are actually dominating all of the other solutions. But then, in two-dimensional case, this doesn't work anymore because we have multiple solutions. Anymore because we have multiple solutions. And to pick a good one, it's kind of a trade-off because if we pick the one on the top right, we are actually maybe optimizing for binding, but then we are maybe breaking expression and the other way around. So actually what we want is to be able to sample antibodies that lie on the Pareto frontier. And since we're talking about energy-based models, in this case, we are not maximizing. So the Pareto front looks like this. Looks like this. So it's not maximization, it's minimization. So we want to minimize the energy scores in this case for affinity and expression. And this is the starting molecule. So if you remember from the first slides, what we get from the immunization campaign, we have some seed or leading molecule. And then we want to generate as many similar molecules to that one that are on the Pareto front. And we see that if we just use the compositional energy-based model. Introduce models here.  I'm afraid to press. Maybe use the keyboard. Yeah. It doesn't work with a okay. Thank you. Um, yeah, so. Um, yeah, so as I said, we want to sample from the Pareto front, and to do that, we're actually using multiple gradient descent. Um, so with the note with the NABLA operator, the gradient with regards to a certain property. So, we have an already trained energy-based model that represents that property, and then we do the standard procedure as in gradient descent. At each step, we add the sum of the multiple gradients, but Of the multiple gradients, but we actually choose the direction that is aligning most of all of the properties, and we actually optimize the one that has. Yeah, sorry. So, no, I just wanted to say that the direction that we choose is actually the one where the property that is doing worst at that iteration. So, there are results that show that by choosing the worst direction for a certain property, you are actually also optimizing all of the rest. Also, optimizing all of the rest so you don't break the other properties. So, this is why we use this procedure from multi-objective optimization. So, you can ask me now. Maybe when you added properties to compose everything, you still want the diffusion model to generate realistic examples. Is there something there? So, one of the properties is antibody-likeness. So, we have one energy-based model that is trained on the Have one energy-based model that is trend on dental web data, one energy-based model trend of good affinity, one energy-based model on non-specificity, so that the antibody doesn't stick to multiple. Yeah, the first baseline that you can do is the linear scalarization, so where you basically have different weights for each of those properties, but that doesn't really guarantee that you optimize one but don't break the other, which usually One, but don't break the other, which usually is what happens. So, this should make all of them good at the same time. Actually, at the final step, not at the same time. And there has been, it was shown, I mean, this has a closed form solution in the two-value case, and then a fast algorithm in more than two dimensions. So it was quite convenient. And it already works with the Langevin diffusion sampling framework. So it was very nice for us to actually put all of this together. Actually, put all of this together. And some of the results. So, the results on the previous slide were from real data that we used. And then, as an evaluation metric, first I have to explain that I don't know if it was mentioned in the previous talks, but it's actually also tricky how to evaluate which solutions are good or not, because we have two or more dimensions. So, for doing that, we're actually using the hypervolume as a metric. So, if As a metric. So, if the yellow points are our baseline designs, we also choose a reference point and then we kind of bound a polytop and we actually compute the hypervolume that is preserved by this polytop. When new designs come in, we have a new hypervolume that we can actually compare to other sets of data points. And this is our metric that we use for evaluation. So, yeah, this is just if we want to see how much we improve by taking a new. See how much we improve by taking a new design or not. We can just take the difference between the two hypervolumes, and this would be a good metric as well. And we apply this to antibody design. The orange line is actually if we just do the compositional energy-based model. And as I said, in this case, we're doing minimization. So we want our Pareto front to be as low as possible. And we see that with the Pareto compositional energy-based model, we have a much better Pareto front. We have a much better Pareto front. And this is just a sub-selection of the data points. But basically, if we just plot all of the designs generated by the two models, all of the ones from the compositional EVM are at the top and from the parat optimal one are at the bottom. So it's doing much better job. And in the second plot, we are trying to show that we can also use this procedure for optimizing for certain property. Because, for example, maybe we have an antibody that has good affinity, but a bad specificity. Bed specificity, so it's sticking to many other things, and we try to improve for that. And we can just pass it through our method. And starting from seeds, we see that some of them have quite low BV score, which is our non-specificity score. And with the compositional energy-based model, but even more with the Pareto-compositional one, we are actually improving that property. And since I already mentioned hypervolume, this Mentioned hypervolume. This kind of leads to the second part of the talk because hypervolume is a nice metric, but it's also not very scalable where you put it into an acquisition function in a Bayesian optimization procedure. Because in antibody design, we have multiple properties that we are trying to take into account. And the existing hypervolume improvement acquisition functions are not so suitable for our setup. Our setup. So, this is just a very small refresher on why we actually want to use the Bayesian optimization procedure because we have, as Rich mentioned yesterday, we have this lab in the loop setup where we have oracles that actually predict those properties, but we're not certain in their prediction. So, we want to take into account their uncertainty. So, basically, we want to not just predict the hypervolume metric. The hyper-volume metric per design, but also for all of the samples in that posterior for that design. So, this is what we do. However, these metrics are quite sensitive to monotonic transformations and to different scales. So, if we, this is the same data set when the scales are same, so it's zero to one for both objectives. In the first case, the two blue points have the identical scores. However, if we change one. However, if we change one of the scales, if we use monotonic transformation or just rescale the variable, the first one will get much higher score, which is not what we want to happen in real life. And also, this is from one of the state-of-the-art methods paper. We see that as we increase the number of objectives, which is M, the time, the computation time is quite high, both on CPUs and GPUs. CPUs and GPUs. And especially in our case, when we have at least five properties that we want to optimize, this method doesn't really make much sense. So we wanted to kind of improve for that. And this is something we are currently working on with members of our team. So we want an acquisition function that will allow for more scalability. So more than two properties. We want to also be able to encode domain knowledge. We want to be robust to different transformations. Transformations, and we also want to be able to accommodate fertile behavior. So, in the results today, I will only talk about a few of these properties, but we are hopefully getting all of them by the end of the month. So, the name comes from Bayesian optimization, but also from the fact that we want to use the cumulative distribution function as a score because it's actually giving you a ranking. So, if we're in the maximization case, when we wanted to. Optimization case, when we want to choose something that is on the Pareto front, we want a point that is as close as possible to the right corner here, for example. And that is also very similar to what we would get if we just take the CDF score for those data points, which is what we see in the slide, because the CDF is just telling us how many of the data points, like what is the probability that a certain value is higher than the rest of the data. So it's kind of like quantiles. And in machine learning, people are more used to. And in machine learning, people are more used to using the PDF or the density function instead of the CDF, where we can actually use both of them to represent the same distribution. It's just that in the first case, we see that the PDF has the highest scores for where the distribution has the highest mass. So it's in the first case, it's for values of x1 and x2 around zero. But if we look at the CDF, which is what we want to use for our acquisition function. What we want to use for our acquisition function, we see that for the highest values of both x1 and x2, we have the highest CDF scores, and this is what we want to leverage for bow type. This was just an empirical validation that our proposed metric, which is based on the CDF score, tracks very well the hypervolume improvement and there is a very high correlation. This I already explained it, but it's just a formal representation of. But it's just a formal representation of what our acquisition function looks like. And we are also computing the CDF score. As I mentioned, we have not just the prediction for each candidate, but also for all the neighboring candidates. So we take the uncertainty into account. So this is why we have this kind of formulation of the problem. But again, CDF in high dimensions is not as easy to compute. So we maybe solve the problem. So, we maybe solve the problem, but not completely. So, we have to use something else as well. And for that, we use copiwas. Copiwas are quite popular in finance, I think, and they're usually favored because they give us a flexible way on how to decompose any grain density, because we decompose it into one part, which is just the dependent structure between the variables, and a second part, which is all the margins. And this is very nice for estimation. Very nice for estimation down the road. I think the most intuitive way to present them is that if we have two independent variables, x1 and x2, we can factorize their joint probability by just doing the product of those. But if those variables are dependent, as in the figure here, we have to account for the dependent structure. And that dependent structure is basically the copyoad that we want to use. And we want to extend to higher dimensions. And we want to extend to higher dimensions, as I mentioned. So, to do that, we are actually using bind copulas. I thought that the best way to present them is with an example. So, here we want to decompose this four-variable density. And the way to do that is that we build a graphical model, which is called vine. And it's represented by trees, so three levels of trees in this case. And each of those trees has edges. In the first tree is edges. In the first three is edges between each of the variables, each of the four variables. In the second three is conditional ambivariate densities and so on. So basically the whole decomposition boils down to just pairs of copulas. So it's giving us a very flexible way to estimate higher dimensional densities and distributions by decomposing them into pairs of variables and, of course, the product of the. And of course, the product of the margins. And then these pairs can be actually estimated in a parametric or non-parametric way. We use kernel density estimation, but since we already have this decomposition, it's much more flexible than just doing kernel density estimations in five or ten dimensions because then it doesn't scale as well. So this was kind of our solution to the higher dimensional CDF estimation problem. And we tried it on the standard. And we tried it on the standard benchmarks in Bayesian optimization. This is a toy data set where we have two-dimensional input and two-dimensional objective space. And these are just some of the standard Bayesian optimization baselines. And we see that the CDF-based score is doing quite well. But then in higher dimensions, so when the input is 10-dimensional, going to four-dimensional, the CDF-based score is doing much better. And then we are working now on. And then we are working now on data sets where we actually have small molecules, and then we have 10 objectives that we're trying to optimize for. And then the other thing that I mentioned at the beginning of this part was that we also want to be more robust in terms of monotonic transformations. So, what we will do next is still just the same example for a two-dataset where we had two dimensions. For a three-dataset where we had two-dimensional objective space, but we will actually transform one of the objectives and leave the other one the same. So, in this case, what happens is that the method, which is based on the hyper-volume improvement that we saw before, with gray, we mark the points that are selected before transformation and we read the ones that are selected after transformation. And we see that this method is quite biased. And we see that this method is quite biased after the transformation, so it's not actually choosing the same points. Whereas the copula one, since it's working in the pseudo-observation space, so to estimate the copula, we have to transform everything to uniform distributions between zero and one. So then it doesn't make, actually, it doesn't matter if we do any transformation or any rescaling of the objective. So the choice of the selected points with the acquisition functions is the same before and after the transformation. Before and after the transformation, which was quite nice. And another benefit of doing this is that we reduce a lot the computational complexity compared to the current state-of-the-art method. So this is my last slide. Just to summarize, what I talked about was that we use energy-based models to sample new molecules. Then we can use compositional energy-based models to actually account for multiple properties. Actually, we account for multiple properties. Then we added the Pareto optimal version with the minimum norm gradient, which actually allows us to make sure that when we optimize for one property, we don't break the other. We also saw our bow date acquisition function. And then what we are interested in and what is quite an important problem at present is the distribution shift. So we actually want to develop methods that are more robust and work well in. More robust and work well in out-of-distribution cases, out-of-the-main cases. So, this is what we're working on now. And thank you for bearing. Very nice about that, Rachel. Thank you so much. Okay, how many ways which is the high dimensional that you have tested with your approach? So for the moment in the toy data set, the DTLZ1, we can use any dimension that we want. We tested it up to 15, I think, which is quite high. For the other methods, it's very difficult. Like it becomes very slow. So we have to drop some of the baselines because it was too much of a wait time. Too much of a wait time, but then in the real data for small molecules, I think it's 10. So nice talk. Thank you. The distribution shifts is on properties or also on like the antibody? So it's more on the antibody features. Because of the rounding the loop stuff, we change methods all the time. We change the predictors, we change the generative model. So it's kind of unstable. So it's kind of unstable, whatever we build on. A more technical one, the sequential tree structure, the selection of which you condition on first, is that trait or is that? So for the vine copula, so in the first tree, it's based on the highest dependence. It can be candlestar or sperm and raw, and then there is a jigstra algorithm to just build the tree. And then the second tree is basically. Then the second tree is basically joining. So if they have a common edge, these two will be conditioned on X2. So that's the next. But it's a non-unique decomposition. So you can decompose it in many different ways. But in any case, you get a valid factorization at the end. Perfect. Thank you. Maribel wants to make an announcement. Thank you, Natasha. Thank you. So I'm sorry to put pressure on everyone, but our taxis are already waiting outside. So if you want to leave your drop your