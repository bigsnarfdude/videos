He'll be speaking on topological analyses of network dynamics. Am I connected to Zoom? Yes, you should be connected to Zoom. It looks like that's displaying out there. If anybody has it open, you can check. But the Zoom is automatic to the system. Well, thanks for the invitation. It's great to make it. I'm sad I missed a couple of days of talks, but I don't know. Maybe my students back home are happy I started off the semester in person, or maybe not. Probably not. They would have enjoyed to just skip the first class. So, yeah, let me just dive in. So, I'm interested in dynamics on networks and with lots of applications, mostly related to complex systems, where I'm thinking about systems that have some self-organization to it, and more recently, thinking about how self-organization can play into and contribute to. Play into and contribute to like structural dynamical mechanisms of computation and eventually intelligence, however you want to define that. But I study lots of different networks because this sort of complex system shows up in many different applications. Here's just a brief survey of some of the applications I've looked at recently. Human mobility, open transportation networks. I'm studying this data set, encoding the flow, the mass. Encoding the flow of math PhDs among U.S. universities and kind of using that flow of PhDs to study prestige and hierarchy and biases and information flow is kind of the direction I'm headed there. I look at some data or I collaborate with some molecular biologists with models looking at the spatiotemporal dynamics of the yeast genome. Yeast genome, also a project with the human genome with a different collaborator. And I'm thinking about network models, of course, of neural networks and machine learning, and also thinking about models of social networks coupled to networks of AI agents. So, pretty broad interests. I'm generally in just about every application, I'm interested in the structural dynamical mechanisms. How does the dynamics interact? Mechanisms? How does the dynamics interplay with structure to give rise to some organization of the dynamics? So, for this talk, I'm really interested in cycles. How do cycles affect dynamics? And cycles can arise for many different reasons. They can arise in directed networks, undirected networks. Here's some examples. The techniques we'll use to study the effect of cycles on dynamics will be drawn from the field of homology, which is the subfield of. Which is the subfield of topology, which is really interested in the connectivity of, say, k-dimensional holes. So cycles are just one-dimensional holes. So if you have a tree, you might have some cycles. This is probably not a family tree. If you have a data set that has some noisy data, then noisy connections might give rise to these long-range edges, the red edges. Edges, the red edges, which kind of imposes these long-range cycles in the graph. I call these networks on the right noisy geometric networks. So in this model I have the blue edges are some kind of geometrically constrained edges and then the red edges are noise. You can think of them as a topological perturbation. This could be face-to-face human interaction. This could be face-to-face human interactions and then connections through the airline network giving raise to long-range. So, cycles can come from different reasons in the graph or I should say in a network. In this talk, I'll talk about using graphs to represent the structure of the network as well as simplicial complexes to represent the structure of the network. So, I'll talk about dynamics over graphs as well. Dynamics over graphs as well as dynamics over synclitial conflict. So we're going to characterize k-dimensional holes. At least we'll count them with the Betty numbers. So we have zero-dimensional holes, which are connected components. This is connecting what I'll talk about to graph theory. We have one-dimensional holes, cycles, two-dimensional holes. We won't really talk about, there'll be voids. We can count those, you know, how many of these cycles. Count those, how many of these cycles exist, how many of these holes exist at different dimensions with the betting numbers. Donut has one connected component. This surface of the sphere, one connected component. This graph on bottom has three connected components. C1, one, three. There's one cycle around the handle here. No cycles that arise inherently for the For the surface of the sphere that arise outside of the void that's contained on the circuit of the sphere. And for the graph, there's no 2D holes. We do have these two 1D cycles, one-dimensional holes. Okay, so we're going to use these tools to think about structure of a graph or a Of a graph or a simplicial complex and understand the effect of cycles on dynamics. And I'll go through three different applications of using homology. So the first is I'll formulate a model for consensus dynamics over a simplicial complex. And in this case, the dynamics is converging to a subspace that's determined by the homology. It's going to converge to the harmonic subspace. In this next project, I'll In this next project, I'll look at essentially a Markov chain that's converged to a stationary state. And so I can look at the stationary flows, and I'm going to study the cycle patterns that's arising in the stationary flows of the Markov chain. And I'll use persistent homology to characterize the cycle patterns. And then finally, I'll use persistent homology to study bifurcations that occur for. Bifurcations that occur for a model for a cascade process. This includes a model for a social network or like information spreading. And then I'll get to a model where we're thinking about neuron excitation being cascaded. And we'll use persistent homology to study bifurcations for those dynamics. Okay, so this is probably review based on the previous talks. So we have an abstract simplicial complex and it generalizes an underactive graph, vertices, edges, I'll call them triangles. Formally, these are 0, 1, and 2 dimensional simplices. Each edge intersects at a vertex. We'll say those are lower adjacent. Are lower adjacent. If they intersect at a too simplex, they are upper adjacent. We can encode these adjacencies in the boundary matrices, coupling the vertices to edges, edges to the two simplicies. And here the plus and minus are keeping track of the orientation of, in principle, each edge edge is undirected, but we'll give it an orientation so that we know the. So that we know the head and the tail, the edges, and then the two simplices. Give them orientation that's consistently going around. We're always increasing. The default is the index is increasing 1, 3, 4, not 14. Okay, so I'll define the Hodge Laplacian, which is has two parts. Has two parts, the part that comes from lower adjacencies and the part that comes from upper adjacencies. The zero Hodge-Laplacian is exactly the Laplacian of Victor graph. The unnormalized Laplacian, also called the combinatorial Laplacian. The one Haji Laplacian, now these are edges that share vertex and then edges that share their They're at the side of two of the triangles that are adjacent to the edge. We have the Hodge decomposition. So we have the image of the lower boundary matrix, the image transposed, the image of the upper boundary matrix, and then we have the kernel of that Hodge Laplacian. These are three orthogonal subspaces. Any function can be split into or projected onto those three subspaces. Subspaces. And so here we can also build an intuition of the boundary. And the intuition we want is the boundary of a boundary is equal to zero. This is probably showed up. So if I want to know the boundary of this edge, we have A minus B. The boundary of the two simplex, we have B minus A plus C minus B plus A minus C. And then the simplifies cancellation. Okay. Okay, so if the boundary of the boundary is zero, the boundary of any cycle is zero. So if we look at a cycle and the boundary is zero, either that cycle is just the boundary of a two-simplex or some kind of fundamentally different cycle through the graph that comes around. So that's what's giving rise to the two. We can have basically these are cycles that arise because of Cycles that arise because of the existence of a two-simplex must-have a boundary. And then these are cycles. Any function living in the kernel is a cycle that's existing, but it's existing not because it's just a boundary of the two sync clicks. Okay, so yeah, I mentioned we can project any function onto these three orthogonal subspaces and Subspaces and they have names when we're interested in the one-hodge Laplacian. So we have the gradient, the curl, and our harmonic subspace. And yeah, essentially the curl subspace, these are functions that live only on the boundary of two simplices. If there's no two simplex around, it's gotta be a zero entry. Um the gradient, I guess this is also called the transitive part um in uh other language. Um Other language, other areas of math. And then the curl is the harmonic subspace. There's this, it's isomorphic to the homology. So here we have two cycles that are kind of fundamentally eque, in that these two cycles, they're just the boundary of those filled in two simplicities. Okay, so there's Okay, so there's, you know, how many cycles? I guess there's one, two, three, four different cycles here. Two are just the boundary, two simplicies and two are the online. We'll jump here. Okay, into the first application. So we're going to use again homology to think about cycles in graphs, dynamics, or cycles in nature. Dynamics or cycles in networks, dynamics over those networks, and the homology is going to play a role, or we're going to use it as a tool to study the dynamics. So, the first is: I'll formulate a model for consensus. So, in this case, we're going to call it a balanced Hodge Laplacian, and I'm going to call it alpha a balancing parameter. So, I can put alpha and 1 minus alpha here, obviously, because the Because these matrices have orthogonal ranges, multiplying by a scalar is not going to affect the subspaces. So it's the same, you know, throw an alpha in there. It's not going to affect the subspace decomposition, the Hodge decomposition. And so we'll play with these alphas to try and optimize dynamics in a second. So we're just defining a linear ODE. So we're just defining a linear ODE, dx equals minus LX. And here, so if alpha is 1, that means consensus is mostly influenced by lower edge-edge interactions. Alpha close to 0, that means the upper interactions are having the more strong influence. Here we're defining the consensus, so we're assigning states to the edges. Usually there's a lot of models on consensus, the states are assigned to the vertices. The vertices. One of the applications I think that makes the most sense for this type of model, well, comes from kind of synchronization and collective behavior over physical systems. Because in those systems, it might make sense if you have, like, I think the mitochondrial network was one example that kind of made sense to me, where there's these long tubules and there's like And there's like an oscillating flux on the tubules. And then the intersections of the tubes is just a very small 3D region. So maybe you don't want to model that. Maybe you want to model the state along the long tube because that's where most of the chemicals are along. And so just physical networks in general, I think assigning states to edges makes a lot of sense. So there are a lot of other There are a lot of other recent models for consensus over simplicial complexes where states are assigned to vertices and then those vertices can interact through common edges or common higher dimensional simplices. But here the real focus is assigning states to the edges. And you can also think, or it's also related, this is mostly what we're working on right now, extending this. Extending this is looking at synchronization models. There's some models from the group of Genester Bianconi and some other groups looking at this where it's like a non-linear oscillator model. And if you linearize, you can get back to the Hodge Laplacian. So our model would basically be setting the natural oscillator frequencies to zero. Then the omega drops out. We get something. But we said it. What we said. Okay, so the Hodge subspaces are invariants for this. They're invariant subspaces for the dynamics. So I can take any initial condition, separate it, or project it onto the three subspaces, and look at, you know, by the one Hodge Laplacian, you get a lot of cancellation. So that lets us write out the solution to the ODE just as. The solution to the ODE just as the separate subspaces. The first thing you'll notice is whatever initial condition is in the harmonic subspace just doesn't change. It'll stick around. So if you want that empty, you need to define your initial condition to be orthogonal to the harmonic subspace. The gradient subspace, the curl subspace, your initial condition is going to converge according to Converge according to kind of the slowest eigenmode of your boundary matrix, either lower or the upper boundary matrix. And again, how I've defined the balanced Hodge Laplacian and scaling those by alpha and y minus alpha. So here's an initial condition, and we get convergence to something. The subgradient in the curl subspace, these eigenvalues are positive, so these things with the minus there, they The minus there, they converge to zero over time, and the harmonic subspace just isn't changed. In this case, gradient subspace converges faster. That's because the slowest eigenmode of P1 is slower than the slowest eigenmode, or sorry, faster than the slowest eigenmode of P2. So we can adjust alpha and try and balance those two the dynamics. Balance those two dynamics on those two subspaces, which is pretty easy to come up with the optimal balancing parameters so that these two subspaces are converging at the same convergence rate. Okay, so let's understand the dynamics and what convergence is happening. So, the easiest way to think about this is let's just look at more of a traditional consensus dynamics. States are assigned to the vertices. States are assigned to the vertices, and the system converges. So, this is usually, you know, if we have our flocking fish and they all go the same direction, they've agreed on the direction. They haven't split apart. Talking about this at lunch. But maybe we have two connected components. And so these flocking fish form two groups and then they just go in different directions. Or three connected components. So one way to think about this equation is your solution is going to Is your solution is going to converge to a point if it's a connected graph, which if you're defining a machine learning algorithm based on consensus, it should be a connected graph. The system will converge to a point, which is the projection onto the null space. So this is just a pretty common type of algorithm if you are trying to do decentralized machine learning and get all the models converted. And get all the models converted to the same exact model. Each parameter converges to the same limit. So things are converging to a single point here. This point, if it's an unbiased consensus algorithm, it's the mean of the initial condition. If you have two connected components, you have, depending on your initial conditions, you can converge to different spots in a 2D plane. In a 2D plane. About a linear combination of the middle space associated with each of those components. Through connected components, we're converging to somewhere in a 3D subspace. Alright, so now let's look at the one-Hodge Laplacian. So now I'm assigning the states to the edges, and they're converging according to the one-Hodge Laplacian. If it's a connect, If there's no harmonic subspace, x of t is going to converge to the origin. Everything converges to zero. We have a one-dimensional, you know, if there's one cycle, so Benny 1 equals 1, the system will converge to the harmonic subspace. But now the harmonic subspace is a one-dimensional system. So the system's going to converge somewhere. So the system's going to converge somewhere on that line, depending, different spots depending on the initial condition. Two cycles we're going to converge to somewhere on that team. So yeah, the homology, or I should say the dimension of the homology, is dictating where the system's going to converge. How complex, you know, once convergence happen happens, how complex that or the how uh diverse the different uh Diverse the different limits can be. Okay, so another thing: you know, if we want consensus in the traditional sense, where the system converges to, you know, the definition of the word consensus, everything converges to the same thing, either the harmonic subspace is empty or the initial conditions orthogonal as well. So, this, if you're going to So this, if you're going to kind of port this into some engineering application or machine learning application, that's kind of what you're going to cook into your outgrowth. Alright, so that's the first application. Maybe I'll pause for questions. I'll go back a slide. And so for the balance hoxlet, I guess one or zero, either one, is Um is biased is it unbiased? Question. The so in this case here are biased in the sense that what is the what I mean projection onto the harmonics of space, so it's not no w how we think health. How we think how it is in the house. It's like whatever initial parts in the harmonic subspace live and whatever the projection onto the other subspace is. This doesn't quite apply here. It's a little bit different. So in the traditional case would be the top left picture here. Top left picture here, and it's converging to the mean. And it's not a weighted average, it's the mean. In this case, the dynamics is converging to the board. So it's not doing like a weighted average, which is what you would want the machine learning algorithm to do. Oh, yeah, I guess I spent more than zero case. The the most coefficients do you still have converges to the average? Oh, um but but in there. There is no there is no second part, I suppose. No, there's no second part there's no. I'm sorry. Um there's there's a there's no low up signal, but drop low plus into the how do you track my question? There's oh it's recorded for all time on signal. I can tell you something we've done, which is look at this case and we look at the just the traditional consensus. The traditional consensus algorithm, and it's converging to a biased limit. And we study as we have the social network more strongly influenced by AI agents, or vice versa, as we play with that, we can study the bias of that limiting what consensus is reached. And in that case, this system is converging to a scalar, and the way A scalar and the weights are essentially related to the eigenvector. So yeah, the eigenvector associated with lambda 2. That's not right. They're related to the a a dominant eigenvector of the transition matrix of the positive, so I think that they're not So I think that they're not with the balance hydroclosing you shared that there's the optimal can you show that again? Sorry. Yeah. So I suspect there might be something similar here but we haven't done this calculation. So here we're asking what is when we come in this case When we con in this case, what is the scalar that we converge to? Well, it's a weighted average. Well, it's a mean in this case. If L0 becomes, or if the graph becomes a directed graph, then it can be a weighted average. It won't be the mean. And so here, I think everything is converging to the zero, so you probably can't just look at the limit and say the weighted average. And say the weighted average, but if you look at long-time behavior and maybe approximate it with like a first-order approximation, you'd be going towards zero and you could ask, as you're going towards zero, what do things look like? And I don't know. I have no idea the effects of alpha. I would lay a bet that there would be it would be a biased averaging just because um actually Um, actually, that's not true either. I would say it's probably unbiased because L1 is a symmetric matrix. That would be my guess. That the only way to introduce the bias so that the limit like a weighted average would be Or if L1 is time variant or then hard to guess what the bias is in general. Yeah, there'd be some history to it. Other questions? That is application one. Application two. Okay, we're going to step away from Hodge Laplace. From Hodge Laplacians and Hodge theory, and just look at persistent homology. And we'll switch up the dynamics. So I'm looking at a Markov chain, essentially, with the stationary distribution. I'll compute the stationary edge flow. So each, you know, here I'm indicating the fraction of the flow across the edge. Some of these edges are bidirectional, so some of these flows are bidirectional. And then I define Direct tool. And then I define flow imbalance. So if the flows are Fij, then this is just Fij minus Fji. So this, then this associated graph fits the directed graph. Maybe someone knows. Well, we didn't do this. I didn't think about it till now. But maybe. I didn't think about it till now, but maybe one can make an argument that when you do this operation, this function completely lives in, or might be orthogonal to the transitive. Actually, I'll talk about this in my talk. I can say something about that. Yeah, I've thought about it, but I haven't done anything with it. But it seems like these are all cycles here. You know, these are all cycles here. But yeah, triply, yes, the resulting steady state that has to be triggered in cyclic balance. Yeah, they have to live in the null. That's cyclic balance, yes. Yeah, if it's undirected graph, then these are all zeros. So here we're strictly focusing on directed graphs and then. And then, because we are looking at the ASI symmetry. So, yeah, delta E would be zero for detailed balance, yeah. Then so then delta would be all zeros. So, we're gonna avoid that situation. And what we're interested in is what is this pattern of? Is what is this pattern? I'm thinking of pattern formation. What is the pattern of these cycles? Cycles of different, you know, large cycles, small cycles, they're connected together in some way. I'm going to use persistent homology to think about that. So in this case, we're just studying a directed graph, but I'll lift it, I'll define the cleat complex and fill in the triangles. So this is my, and now here the arrows, I think I keep the arrows having the same. I think I keep the arrows having the same orientation, but once I lift it to the cleat complex, these edges don't have directions. They're under directed edges. But I give them a direction just an orientation just for. And then we do a filtration. So I'm going to start my filtration by doing kind of a reverse filtration. Kind of a reverse filtration. So I'm going to start here and then I'm going to start removing things. Or if I start here, we can add things left to right. It's an equivalent way to think about it. But what we're going to do is first add in the edge with the largest weight, the largest flow, ends up being this one. Then I add in the edges with the next largest flow, which means keeping track of. A keeping track of the value of epsilon such that an edge appears here if and only if the flow, the flow imbalance is larger than epsilon. Then I'm basically decreasing epsilon, which allows more and more of the graph to exist. And so there's more and more of the simplicial complex being left to right. Call it an edge value clique filtration. And so this is the homology generators for zero dimension and one dimension. So originally we have seven isolated nodes, seven components. Eventually at this value of epsilon, this edge from A to B appears, and so we can go down to six connected components. Five and then four. Five and then four. Eventually, we just were down to one connected component. A cycle appears here for this range of epsilon. You can see it. Then it gets filled in by this edge and these two triangles. So it dies, and then eventually this cycle appears and this cycle appears. So this is our persistence bar plot that summarizes the cycle structure and kind of the And kind of the multi-scale features or multi-scale aspects of the cycle structure. We think of this as just kind of characterizing the cycle patterns. Last example. When do triangles get filled in? Triangles get filled in immediately. If there's a triangle, it's there. If there's a triangle, it's there. That's the I mean when you're when you're say except I think of amplify So as soon as there's an edge from B to D, the higher dimensional simplicities appear as well. And that decision is the keep-click complex decision. Once the load differential is bigger than epsilon. Bigger than epsilon. Yeah, yeah. So I'm decreasing, you look at the scale as 0.05 and then 0.1203. So I'm decreasing epsilon, and once the edge, the flow from B to D is larger than epsilon, that edge appears, and its triangles appear as well. I don't know which triangles appear. As soon as it's there, it's certainly not. Oh, I think, yeah. Oh, I see. Yeah. So 100% of triangles are correct. Oh, okay. And 0% of any other kind of. Yeah. And that's the complex assumption. Yeah, yeah, yeah. If there's a triangle, or if there can be a triangle, there is a triangle. That's my question. Yeah. Why don't you draw directly to errors? Because that would be assuming there's an error. So these directed arrows, I mean, we could have dropped, they're not directed edges. So whichever one is more, you draw it that way, I guess. Yeah, basically I chose the edge direction, just keeping in mind these edge directions. Yeah, the direction is indicating like the the net flow. So there's more flow to E to F and vice versa. I'm a charger. I'm gonna try to add one well that's how all clarifying questions are. Like once you know the answer, go bring it back. So here's one application. I came across or saw a talk by Evelyn Tang looking at kind of emergent cycles. It's called a chiral edge flow. In the biology context. And so the application is: consider a polymer, a polymer can grow. So that's like one dimensional axis, literally. And then the other dimensions, it can kind of fold up. And maybe we represent this as four states in a Markov chain. So we're kind of doing this cross product of the polymer can be in one of these four orientations, and then the length can grow. And then the length can grow longer or smaller. So that the Markov chain can be represented here. This is just showing a single polymer. This is showing a bipolymer. So there's actually two polymers. One can grow and get shorter, the other can grow and get shorter. Together, and then they can fold into four configurations. So if you have like a ring. So if you have like a rate of change between these configurations, and you have a so that's the external so changing size is an external change and then changing orientation or configuration is internal change. So we have these two Internal change. So we have these two rates that we can assign, and if we write out all the transitions, it kind of looks like a lattice with these directed edges. And what happens is when the transitions internally are, I think, much faster than externally, then the system converges to the outside of this lattice. So, the states with very low probability you'll be on one of these internal states. You'll most likely be on one of these external states. So, what happens is, this is mostly like the system is kind of one polymer grows, it's wiggling around, but generally one polymer grows, and then the other polymer grows, and then the first polymer gets small, and then the second polymer gets small. So, that's this large. So that's this large scale cycle. So it's kind of a change. So the polymer at a fast time scale can do these quick cycles, but out of the system emerges this very long time scale cycle of polymer growth, polymer growth, polymer shrinking, polymer shrinking. So this is similar, at least at the statistical model for the quantum Hall effect. Model for the quantum Hall effect, where electrons are kind of moving on the surface of the conductor in one direction and in the other direction, and not so much on the interior. So we look at this Markot chain model and we look at the flows, and basically, as we change the rate, as we vary the rate of change for the external transitions to be higher probability. Higher probability or higher rate, there emerges kind of a large external cycle that's indicated in the other application we looked at is page rank. So if we look at page rank for this particular graph, so alpha is the probability you just do A random walk according to the graph. 1 minus alpha is the probability you do a random walk for an all-to-all graph, where you can jump from anywhere to anywhere. So it's just kind of like a linear combination of the two transition matrices, one for the graph and one for all-to-all graph. And what you can see is as you have more and more, as alpha gets smaller, that's with higher probability you can jump from anywhere to anywhere. And so what that's doing is it's And so, what that's doing is it's kind of, you know, there's inherent cycles in the graph. As you add the teleportation to it, it's basically dampening out those cycles. And eventually, you know, alpha decreases from 0.8 to 0.4. There's no longer three cycles, there's just two. I think it's this one doesn't show up when you do the filtration. So you can keep track of it. So, you can keep track of it. I call it a homological bifurcation diagram. So, we're changing a parameter, and we're setting how the dynamics changes, and we're setting how the homology properties of the dynamics is changing. Death. I have the birth time and death time for these generators, and we can see transition from three down, or let's see, two to three occurs right around. Occurs right around 0.55. I'm just plotting the page rank here, and you can see the page ranks are kind of switching around the same value of alpha. As far as I know, it's a coincidence. Maybe it's more than that. I'm not sure. Application two. Questions? Yes. So I have a question about the construction of the filtration. I guess this is a Construction of the filtration, I guess this is a question and a proposal, so maybe you can make anger track a proposal as well. But so here you're going one edge at a time, right? Or you're choosing to add a reduction. Not necessarily, but presuming the values don't exactly agree. Which I wonder if there are problems with that because you might want to retain the fact that the flow has zero divergence. Right? That you're, if you go through the filtration, that probably no step in the process retains the fact it has zero divergence. Fact has zero delivery mass. Yeah, can you define an entire cycle at once rather than just an entire edge at once? Like, say you have over complete bases of cycles, like sparse reconstruction, so you try to represent your flow, it's a few of them, and then add those in one at a time. Do you think that would give you significantly different, like, your like, would your barcode be very different? The barcode would definitely be really different for better or for worse. For better or for worse. I don't know. That might answer one of the questions I have posed at the very end of the talk. If it's possible to do it, well, it was a different question. But yeah, I think the general feeling is, you know, is it possible to use some sort of dynamics to build your filtration and build, like, in a more kind of dynamics respecting way. Respecting way, which is what you're saying. And so then the question is: you know, can we use dynamics to build persistence diagrams in a way that's going to be faster than building the persistent diagram in the usual way? And so maybe the answer is yes, and maybe that would be really useful. And maybe the answer is no, I don't know. But yeah, I think it's probably possible. So if you're adding in. Possible. So if you're adding in edges so that you always have, you know, each one of these would be decisions about which cycle comes first. And that would kind of change everything, I think. But maybe that and there would be a right way to do it. Yeah. Yeah. Don't need to retract the proposal. Good question. Okay, the last one. So now I'm going to use homology and persistent homology, topological data analysis, to study a spreading process over a network. I call it a noisy geometric network. You can think of a lattice with also long-range edges. With also long-range edges. This could be representing human movements, and then the long-range edges are people getting on flight, coming at a very far distance. Or this could be, if this is information flowing through a social network, it could be face-to-face conversations in blue, and the red would be conversations over Twitter. Other means that's not so geographically constrained. Strand. And when you have a network like this, at least to me, the most interesting thing is: you know, is spreading looking like a wavefront going through the space, traveling outward along the manifold that the blue edge is discretized, or is it jumping long distances? So I call these events or these properties of the spread, wavefront propagation, and then cluster appearance. So if I start a contagion at the middle, So if I start a contagion at the middle and I just kind of color each node by the time at which it becomes infected or at which it adopts the spreading social contagion. You can see the splotchy pattern or you can see the splotchy pattern. And these are competing. So if it's just wavefront propagation, there's no cluster appearance. So what we do We do, at least for this model in our first paper, it's based on a paper by Duncan Watts called the Watts Threshold Model. It's also kind of before that called like just the threshold model. Granoveter studied lots of them. Or so I'm told. And we have an irreversible dynamic. So we have Xi of T is 1. If it adopts the contagion. If it adopts the contagion, or zero if it's not adopting the contagion. FIFT is the fraction of neighbors that are affected or who have adopted the spreading process. And we have a threshold, so if your fraction of friends surpasses T and they all recommend you see movie. So that's how it's spreading. And so here's a simulation of this model that I made. This is That I made. This is spreading through London. I made the video about flying into London. Let me go for my shoulder that they don't catch me or think I'm planning something horrible. But this was, yeah, 20, 15 years. And what you can see is not only, so when you have a large threshold, so in order to adopt, So, in order to adopt an idea or a contagion, you have a high threshold. This slows the propagation, and when it's a spatial network, it can promote the propagation to just be away from. This is how Black Death was propagating through Europe a long time ago. It would be traveling at the speed at which a horse could travel or at which a person could walk. Slow propagation. And this is more like. Slow propagation. And this is more like the airlines now. But it's not just whether or not there are these long-range edges. Having the threshold mechanism can filter out long-range propagations. So what we do is I go here, what we think about is: so for a given initial condition, we spread, we run a simulation, and we figure out the time at which. And we figure out the time at which each node becomes activated the first time. And I do that for all different initial conditions. So, what that does is it embeds my network as a high-dimensional point cloud. So, here. So, the coordinates would be, you know, so node I gets mapped to vector i, and the entries would be, when does i get infected when the container is. When does I get infected when the contagion starts at node one? When does I get infected when the contagion starts at node two? And so on. So it's mapping each node to a vector. If I have n nodes, it's an n-dimensional vector. You can't visualize it, so we can use PCA to project onto 2B and look at it. And the idea here is if the original network of the nodes are lying on a low-dimensional manifold, even if there's long-range edges, Even if there's long-range edges, but there's also short-range edges, if the spreading process is propagating by away from a propagation and not the long-range jumps, then the manifold structure underlying the graph also underlies that point cloud. And so we can use manifold learning and topological data analysis to look at the point cloud and then say something about the spreading process. If we see this Process. If we see this ring manifold, then we know that the dynamics are spreading predominantly by wavefront propagation. If not, then we have more cluster appearance than wavefront propagation. So we can design manifold learning, machine learning algorithms using the spreading process here, but we're kind of pointed in the other direction of using the techniques to study spreading. To study spreading, we have some parameters: you know, how many nodes are there, how many short-range edges, how many long-range or non-geometric edges. These approaches have been extended in a few ways. Recently, I'll focus on our recent extension that was led by my PhD student. That was led by my PhD student. Where now we're defining or extending this threshold model so that it's spreading over a simplicial complex, not a graph. We developed bifurcation theory to describe when there's wavefront propagation, when there's not, what's the speed, when is there a new cluster appearance when there's not, what's the speed. And then we define a simplicial cascade map, which basically takes a simplicial complex and embeds it into a latent metric space. Into a latent metric space based on this spreading process. Let's see, we're at 49 minutes, so I'm going to probably skip pretty quick. Basically, we figure out for each vertex, we assign vertices, edges, two simplices, and higher-dimensional simplices as active or inactive. And then a vertex can become active if it was previously inactive. If it was previously inactive, if it's a boundary protect, meaning it has some neighbors that are active, and then it can spread and become active itself if its exposure surpasses the threshold. The exposure is a linear combination of the exposures of different dimensions. So, k is the dimension of the simplex. How many of your neighboring edges? That would be, you know, F1 of I. This is the fraction of your neighboring edge. This is a fraction of your neighboring edges that are affected. So if delta is zero, this is exactly that Watts threshold model. And we study how delta affects the spreading processes. In order to do that, we have to generalize the noisy geometric network. So I call it a noisy geometric simplex. We have to generalize the idea of shortest path and path. So we introduce the notion of a channel or a geometric channel. A channel or a geometric channel. So this would be, you know, triangles connected by sharing an edge with more triangles. This would be a channel over which, or in higher dimensions, you could have a tetrahedra sharing a triangle with another tetrahedra. That would be a higher dimensional channel. So you can think of these channels, you know, that higher dimensional channel, you could kind of think of it as like the Toblerone chocolate type of a channel. And then the spreading process can go along the center of that or just along the surface of it, depending on the dimension of the spreading deadline. Bifurcation theory, you can ask quantitatively: is the manifold structure of the network reflected or mirrored in the point cloud? You can apply a future speed. You can apply the Taurus RIPS filtration to that point cloud and quantify whether or not it looks like a ring manifold and those transitions that we predict. Here the color of these pixels is showing the scalar that measures using TKA, whether that's a point cloud looks like a ring manifold. And then the lines are bifurcation theory. So we see kind of these. Theory. So we see kind of these topological transitions to the point cloud occurring where our theory says the dynamics is undergoing bifurcations. Alright, so in summary we have these three applications I talked about where homology is being used in different ways. The first was consensus dynamics was converging to a harmonic subspace and the dimension of that is determined by the homology. The second was we have this diffusion We have this diffusion or this flow problem, and we're trying to characterize the cyclic patterns with persistent homology. We're applying the filtrations directly to the graph with the weighted edges. And then the third, we use persistent homology to study bifurcations for the sporadic process over the network. And we're doing classical course for its filtration applied to the point cloud. Thank you guys. Thank you guys. This work was led, kind of the three projects were led by my three PhD students. I will point out Cameron and Olgin are going on the job market next year, looking for positions to start in January 24. The papers. Maybe I'll just end with questions that I have for you guys. Thank you. Any questions for me? Yeah, so it's just backlash question also. It would be useful to look at other types of filtrations where you're updating them. Um probably. I wouldn't I don't have like an application for doing that. We do that a lot, I guess, when we're looking at micronical community structure using random walks to come up with the criteria for aggregation that can produce fast algorithms. Usually, when I'm thinking about compressing vertices together, it's because I'm coarse-graining nodes in the groove. Nodes in the groups, and that assumption has kind of, or that approach has basically aligned me with the idea that I'm studying zero-dimensional homology. So I think of connected components, you know, as zero-dimensional homology. I can relax that maybe with kind of low-weight edges between those connected components. So community structure, hierarchical community structure, it's kind of a zero-dimensional question. Yeah, a zero-dimensional question or pursuit. So, yeah, and I think I've tried using TDA and looking at this diagrams, focusing on the zero-dimensional features, and say, can we use that to find And say, can we use that to find community structure? And usually, I found that's not as useful as the and this doesn't seem to be as good. Maybe I'm just not getting it quite right how to try to do that. But yeah, and then I don't know. If you're looking at cycles, would be the next dimension up. I think if you're trying to construct something like a reef graph, you might be doing this like, you know, I don't need all four nodes to represent that cycle, so I could find some of these. I haven't thought about that at all, and I don't have the kind of stronger topology training, I think, to think through how to do it and what it might mean. Yeah. And what it might mean. Yeah. Don't need to redirect that question. Other questions? We'll ask you guys questions. Here's some things I don't know how to do. Richard, you might. So, yeah, so I'm going to go ahead and do So, yeah, so the first one is related to your question, which is: you know, can we use a Hodge-Laplacian to approximate a persistence diagram? Or can I use my consensus model or a random model on simplicial complexes to try and come up with an approximate persistence diagram? Maybe that's faster than the normal way we create persistence diagrams. We can think of that as like, you know, we have numerous. You know, we have numerical linear algebra, and then we have randomized algorithms. Is there space for randomized persistent diagrams? I think people are working on that a lot, but more from like the sub-sampling perspective, not like a linear algebra perspective, or not like how we might study random walks on a hierarchical communication community structure and look at the time series and say, oh, there's. And say, oh, there's two groups containing two groups. And we can keep those approximations by looking at the dynamics, isn't it? I'll just throw out some thoughts on this because it kind of connects to this idea of what is in all those cycles. I mean, if the concentration rate you go and then it's simple and the cycle appears in some what is it that makes it before almost a cycle that before almost a cycle then a cycle is that encoded somehow in the in the eigenvalues of the harsh plus maybe as well I mean the zeros you you definitely I mean okay you you get the homology put in the long bars that persist right now and there is this persistent whatever you call it I don't know basically you have people where you track the eigenvectors of the You track the eigenvectors of the of the while cranking out sort of armies, right? Like you're doing the filtration? Yeah. Doing the filtration and you build the Laplacian or you keep track of the eigenvectors of the Laplacian. Yeah, so we have a Hodge Laplacian for each one of those. But that's drawing numerically on the front in a randomized fashion because that way you're you're going to paste things fast? Or yeah, you can so okay, so yeah, there's two parts there. The first was bringing the Hodge Laplacian in to approximate. And then the second would be don't compute eigenvectors. We're going to simply approximate that. Run another water. I don't know what it is if you're volume. It's arguing about the mobile phone. Yeah, we should actually stop. Without me, ask one. Maybe I'll just pose my next question and discuss it. You can go on here and think about it and have something to say. Yeah. How many people have thought about it? There have been people who've thought about computing just homology maps by choosing, maybe doing it in a smart way, choosing some initial condition, running the Hajj La Pasha until you find a cycle, and then adding that to a list. So then maybe if you keep finding more classes, you take linear combinations, and you just start with build it up. But I'm not sure how to get the matching or like personal. There are some dietarians between like different, I think let's thank you. Go to the speaker's middle time for half an hour. 