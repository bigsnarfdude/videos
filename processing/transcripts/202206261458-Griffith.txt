Hi. My name is Sky Perfora Griffith, and this is the most nervous I've ever been for a talk because yesterday I entered the Zoom meeting and I saw my reference page, except for it wasn't my reference page, it's just that the attendance of this particular workshop is a lot of people that I really admire. So I'm a PhD student, D student at Queen's University, and I'm very out of my league here. This is a very simple talk. It's a little bit more theoretical than a lot of the talks that we've talked about at this conference or at this workshop rather. And I've made it very accessible. Rather, and I've made it very accessible to people outside of the multi-taper sphere of research. Part of that is because some of these slides were from my SSC and I've kind of like extrapolated on that. And otherwise, yeah, at least I hope there's something that you can get out of it, or maybe that you'll have a new visual for, or you can have fun correcting me on whatever naiveties might be tarnishing the following slides. So let's get started. Okay, so really fast through some background and motivation. This is just going to be some overview of time series regression and why. Of time series regression and why we might want to use some multi-taper techniques. So, this is a classic regression equation. We have a response, coefficient, predictor, and error. And of course, all of the components are time series. So, this assumes that the errors are going to be uncorrelated with respect to time, which is problematic because what if there's a signal or something, right? As we know, that means that we're going to miss a lot of the temporal trends in the data, shared signals or coherency between predictor and response, all of that is going to be lost by this, right? lost by this right so for a visual for this of course um if this is just like a basic scatter plot with just a um a line find about found by the lm function in r and uh we have a predictor time series down here and a response time series up here and despite neither of them well despite both of them being time series uh there's actually nothing to do with time here these are just ordered values on each of the axes right so we can imagine that the residuals could look like this when plotted against time and obviously there's some kind of signal there that's being missed in this regression Signal there that's being missed in this regression. So, what if we do this in the frequency domain? So, in general, I'm going to use lowercase letters for time domain objects, and I'm going to use capitals for their frequencies domain counterparts. Okay, so this model is actually the frequency domain counterpart of what would be a linear filter model in the time domain, okay? So, x and y are now eigen coefficients of the original predictor and response series, respectively, and the regression coefficient. Espectively. And the regression coefficient in this case is known as a transfer function. So, this is reflected in the time domain as what would be a sequence of linear filter coefficients in that model that we were talking about before. But yes, we have in the frequency domain that it is going to be a complex valued transfer function of frequency. So the advantage to this model is that it isn't confined by the previous model's error assumptions. So you'll notice there's actually no additive error incorporated here because that was taken care of by the filter coefficients in the time domain. So, given the multiple tapers that were used in order to find this transfer function, this is actually the multi-taper transfer function, and that's what we're going to be talking about today. So, before we talk about that, let's just talk about some distributed, like some building blocks that will actually get us to the distribution of the MTFE or the multi-paper transfer function by just starting with some eigen coefficients and everything. So, we're going to go through a lot of assumptions. Yesterday, Frank Marshall gave a much juicier discussion on some distribution. Marshall gave a much juicier discussion on some distributional theory behind eigen coefficients, but I'll go over the baby version under a stiff set of assumptions so that we can establish a more inviting framework for some of the signal detection techniques we're going to be talking about shortly. So all I'm worried about right now is signals embedded in noise. Okay, so our predictor is just going to be a signal plus noise. Same with our response. And we're going to assume that the noise is Gaussian distributed with a mean of zero. Like we were saying earlier, it's very easy to demean a series. And because To demean a series and because, or to demean a noise series, and because we're just working with simulations here, that's very easy to do. And we're also going to assume that that Gaussian noise is white. Okay, strongly stationary. So now we can actually talk about the eigen coefficients. So with that said, we can arrive at some very tidy conclusions. Without loss of generality, let's just look at the predictor eigen coefficient. So this can be broken down into its deterministic and is stochastic parts. And a stochastic parts. So inside here, we have our signal and we have our noise, right? And then this is going to take us to the frequency domain. So just by the distributive property of addition, we can actually write this as the eigencofficient of the pure signal and the eigencoefficient of the noise. Okay, so the capital Z term is just a complex linear combination of Gaussians. So that means that overall, X of K, because this is deterministic right here, is going to be complex Gaussian distributed as well. Okay, so its real parts are going to be. Well, okay, so its real parts are going to be Gaussian distributed, its imaginary parts are going to be Gaussian distributed, and assuming that they're uncorrelated, we're going to see this kind of spherical or circular shape on the complex plane. Okay, so let's talk about their mean. So the stochastic part is going to have a mean of zero, right? Because the Eigen coefficient of a noise series is going to be zero on average. Mathematically, all we're doing is just using the linearity of the expectation and assuming the noise is being demeaned. And what we get is that it's equal to zero. So what's left is zero. So what's left is the eigencoefficient of the response, which is deterministic. So this is going to be the average of or the mean of our eigencoefficient. Now the variance is a little bit messier. What we have inside here, after a lot of arithmetic and algebra, we've actually gotten rid of all of the complex components. So this is going to be kind of nice here. So this is a double sum over s and t up to n minus one. We have our slopians here of s and t. We have our ACBF or the ACDF, or the audio covariance function of the original predictor series. And then we have this code. So we got rid of the i sign because it's an odd function. It actually ends up working up to zero when you take the variance. I'll let that be an exercise to the audience. But yeah, so this is really nice because we have that the variance is real valued. Variance can only be real valued as far as I know. So that's nice that we got that. But this also simplifies greatly when Zt is firmly stationary, which we did assume. So although there was So, although there was a signal in the original X series, we can actually, if we take the ACVF and we take the variance of this eigen coefficient, this actually works out to be zero whenever s is not equal to t. So if we set s equal to t, because all of the other terms in the sum are going to go to zero, then what we have is we actually have the squared Slepian at t. We have the variance of the predictor. Variance of the predictor, and then a cos of zero is just one. And this is really simple. We can just take the variance out in front, and as we discussed yesterday, the Slepians are orthonormal. And so this is actually just going to work out to be the variance of the predictor. So what about correlation structures? Well, we know that they're going to be uncorrelated across orders from k equals zero to k minus one, capital K minus one. So this is fairly straightforward. This also comes from the fact that those Lepians are orthonormal. But there is Are orthonormal, but there is some covariance across Fourier frequencies or across frequencies in general if we're not talking about something so discrete. So if we assume that delta is some fund some integer multiple of the fundamental Fourier frequency, then this is actually what we get for the covariance. And this isn't necessarily zero. It could be negligible, but in general, it's not going to be zero. And so that means that we don't have frequency stationarity, essentially. So as we, if we were to write an ACBF of the frequencies, then we would not have stationary. Okay. Okay, so let's talk about the actual like multi-taper transfer function estimates. And they look like this. Okay, so this is just what happens when you solve that regression equation. All you're left with is this H hat. So there's a couple of things that we want to talk about here. Firstly, we're assuming that the predictor time series is fixed, because remember, this comes from a regression in the frequency domain. And we're also going to use that the Gaussian response time series is, where the response time series is Gaussian. The response time series is Gaussian. And as we saw when we were talking about the distribution of the eigencoefficients, that means that the eigencoefficient itself is going to be complex Gaussian, this y here. So that means that H overall is a complex linear combination of complex linear combinations of Gaussian random variables, and that leaves us with a complex Gaussian as well. So what about the mean? Well, the mean is fairly easy to find. All we need to do is coast on the linearity of the expectation. And what we're left with, because everything else is considered fixed, Is considered fixed is that the mean of the transfer function is going to be proportional to the eigencoefficient of the pure response signal, because we know that from before that the average of this eigencoffient or the mean of this eigencoffient is in fact equal to its pure response eigencoefficient. So that's going to be the mean of the MTFE. The variance is really interesting. So we know the variance of y is going to be like this capital Y here is going to be the variance of the Here is going to be the variance of the original response noise. We saw that when we were looking at the variance of eigen coefficients. So, what about this? So, we know that if we're taking the variance of something and there is a constant coefficient out in front, we just square it and take it out. So, if it's complex, you probably know that we would, instead of squaring it, we multiply it by its conjugate. So this will become x multiplied by its unconjugated form, and that gives us a sum of the squared moduli. And if we square this on the bottom as well, And if we square this on the bottom as well, because again, this is considered fixed, then this is actually going to cancel out. We're only going to have one of these terms on the bottom, so that leaves us with this. Okay, that means that the variance of the MTFB is inversely proportional to the spectrum of the predictor, because this is actually proportional to the spectrum, the multi-taper spectrum estimate of the predictor. Okay, so in summary, when we're talking about the MTFE, we know that it is complex Gaussian. We know that its mean is going to be proportional, and the mean is complex. Is going to be proportional, and the mean is complex valued, is going to be proportional to the eigen coefficient of the pure response. We know that the variance is going to be inversely proportional to the spectrum of the predictor. The real and imaginary parts are uncorrelated. This is a little bit more complicated. We've assumed a lot of things here. We haven't gone over why they're uncorrelated, but they do work out to be fairly negligible, although that is an entire different discussion. But for the most part, we are going to see circular symmetry for these distributions of H. We also have the Of H. We also have that the MTFE is not frequency stationary, and that comes from the fact that the eigen coefficients are not frequency stationary. So that's going to actually carry through when we're looking at the MTFE itself. Okay, so we're going to take a look at what this looks like when we actually do have signal frequencies, because right now we're just kind of talking about the ethereal frequency, right? But there are going to be some frequencies of interest that are going to change the behavior a lot. So this is what we see from a bird's eye view. This is what we see from a bird's eye view. This is 1000 simulations of H at a single frequency. So, what's being randomized here are the noise components of the original response time series. Again, we're keeping x fixed. And a dot in the center is the mean, and then the circle has a radius equal to the empirical standard deviation. Okay, so what we see here is that there's a large variance. We don't have anything to compare it to, but trust me, this is fairly large. We also have a zero mean, and there is no signal at this frequency. So, if we have a frequency, So, if we have a frequency where there is a predictor signal, we're also going to have this mean because, again, the mean is proportional to the response eigen coefficient, not the predictor. But it is, the variance is going to be inversely proportional to the spectrum. So that means we're going to see this contraction about zero here, right? Here, we don't see that contraction anymore because there's no longer a signal in the predictor. This is a response signal. And we do see a departure from the origin, however, because that mean is proportional to, and the mean is. Proportional to, and the mean is complex valued, is proportional to the eigencofficient. So, as the eigencoefficient spikes, we're going to see this depart from the origin, just as if the eigencoefficient in the predictor spikes, we're going to see the variance contract. So, if we do have both signals or we have both the predictor and the response both have a signal, then we're going to see this. And you'll notice that it's on the real line and it also has a modulus of about one on average. That's just because I simulated this such that the amplitude ratio between the response and the predictor was one. And also, Predictor was one. And also, they are in phase. So, if they weren't in phase, you would see a jump around somewhere on the unit circle, but they were in phase. So, we see a contraction due to that predictor signal in the denominator, right? And then we also see a departure from the origin due to that mean or due to the eigen coefficient of the response. Okay, so again, like these were at single frequencies, right? These are a thousand simulations over a single frequency. So, here we're looking at the modulus. So, here we're looking at the modulus of the expected transfer function estimate, and this is actually across all Fourier frequencies. So, what we have here is that it's not telling us anything at the signal frequency for f of x, which represents the predictor signal frequency, right? But we do see this massive spike at the frequency in f of y, or sorry, at f of y, which is the frequency in the response. So, that's what happens. If we were to animate these so that it went through every single frequency, then you would see. Went through every single frequency, then you would see it kind of just pull away and go back. So that's what we're seeing there. Over here, we see the variance. So the variance is really all over the place. It really spikes here, it's all over the place, but it bottoms out pretty clearly where there's a predictor signal. And that's due to that contraction. So as we get to that predictor signal, we're going to see the variance fall flat. And the width here is just the bandwidth from the multi-taper. So this is going to be F minus W, F plus W, that bandwidth there. Bandwidth there. So we can use that information to try and develop a test statistic. Okay, so let's see here. We developed a test for detecting signals in the response. And that is just the modulus of the expected value of the MTFE, right? So we'll see in a second how this relates to magnitude squared coherence or the MSC. It actually gets pretty interesting when we talk about these coincidal coherencies. These coincidal coherencies. That seems like an oxymoron, but we're going to define that later. Now, for T2, we also have that the variance of H should be able to detect, or variance of the modulus, I should just be saying the variance of H, not the modulus here, pardon that. That should be detecting a signal in the predictor. Okay. So the nice thing about this is that it's more robust to frequency modulation than the harmonic F test, which is not robust to frequency modulation. So that's actually a really interesting property. And we actually Actually, a really interesting property, and we actually have that T1 has that property as well. So, that's kind of one of the motivating factors for why we would actually want to develop these test statistics in the first place because we do have magnitude squared coherence. We do have their harmonic x-test. So, what is the point of utilizing the multi-technical transfer function estimate? And that's why. Okay, so T1 looks like this, and it is distributed chi-squared with two degrees of freedom, one for each of these, under the null, of course. So, this is assuming that F is a noise frequency where the frequency Noise frequency or the frequency not corresponding to some signal in the response. So the real and imaginary parts of the complex Gaussian are each Gaussian, and under the null, we expect h to be centered at zero, so that's why we get this. And I'll talk about this j in a second. But inside the brackets, what we have here is the squared modulus of h tilde, and h tilde is a little bit different than h. So firstly, this j, if we count the Fourier frequencies in the band defined by the application of that multi-taper method from f minus w to f positive w, f plus w, rather than. w to f positive w f plus w rather there are j of them and we're multiplying by it by that so that we can force it to be chi squared and then this tilde here so this h has been transformed by our standardization by a covariance matrix defined by the real and imaginary parts of the covariance and pseudocovariance matrices of h. So if you don't know what pseudocovariance is, rather than taking the covariance of something and another thing in complex terms, we would agree. In complex terms, we would take the covariance of thing A and then the conjugate of thing B. But the pseudo-covariance is when you don't conjugate the second term. And that actually is pretty descriptive when it comes to something like circular symmetry in this case. So that's why we have to incorporate that. But basically, what we've done here, it requires a lot of like standard but ugly manipulation of a lot of complex valued random objects. But if you want details, you can check out 3.3.1 of my master's thesis. Essentially, what this does is it destroys the variance. This does it destroys the variance structure, which is why we can do this for T1, but we can't do it for T2, because T1 actually doesn't incorporate the variance, but T2 does. Hopefully, that made sense. I kind of like breezed through a lot of that. So let's compare this to the MSC. So the MSC, of course, is the ratio of the squared modulus of the cross-spectra to the product of the individual spectra. Okay. So similarly, we have the squared modulus of the transfer function. So this instead is going to be, again, we had the squared modulus. Again, we had the squared modulus of the cross spectrum. These are multi-paper estimates, all these tense little m. But in the denominator, we no longer have this response factor, right? So we're not going to be normalizing by the response spectrum. And that's going to affect whether or not we're going to pick up coherency, which is what the MSC does. So when we talk about coherent noise, like I said, that's a little bit of an oxymoron. So this is what we've done. Okay, we've forced noise to be coherent in the frequency domain by choosing a parameter lambda. By choosing a parameter lambda between 0 and 1, I think in the plots that I'm using, I used lambda equals 0.45, I believe. But essentially, we're using a convex combination of the predictor eigen coefficients at some f and complex valued noise. So I don't know why I put an x here. This would really just be f star. The point is that there is no signal here. Instead, there's just going to be noise that has a very similar structure between the predictor and the response, but there is no signal. It's just noise still. Okay? Just noise still. Okay, so that's what we've forced here. So, this is what we see on average across 1000 simulations. T1 over here, so T1's on the right, and the magnitude squared coherence is on the left. T1 isn't actually picking up coherence at this green line here. It's just picking up the response signal, which does exist here. So, essentially, we have the blue is going to be FX star, which is just the coherent noise frequency. Then we have a response signal frequency and a coherent frequency. So, obviously, the coherent. So, obviously, the coherent frequency is picked up very well by the magnitude squared coherence, but so is this noise frequency, which certainly isn't picked up by the T1 test statistic. So, over here, I didn't choose the greatest color scheme or line width for this, but this actually goes all the way up to 0.9. Okay, so there's actually a massive spike here telling us that there's some kind of coherence, but that coherence isn't related to a signal. So, if we want to bypass that, T1's a way of doing that. So, yeah, just interesting how these two things behave, even though they. How these two things behave, even though they have a pretty similar mathematical expression defining them. So, what about frequency modulation? And this is actually going to be a little bit more interesting because we were talking about the robustness in comparison to the f test. So, let's say the signal in x has a frequency given by a function of t, this g of t here. And we're going to leave the response alone for now, but we'll modulate it later. So g of t is going to be what's modulating the signal, and a here is just any constant, but we're just going to assume that it's one. We're just going to assume that it's one, and that's what I use for the simulations. Okay, so we didn't modulate the response signal. So the left plot is pretty typical, but notice the variance, which is supposed to detect predictor signals, it is still fairly effective, right? So this is the variance over a thousand simulations. Again, we're doing this over simulations and ideally we would only have one simulation, but in general, the variance across the simulations does bottom out fairly nicely. It has a pretty wide bandwidth here. Quite wide bandwidth here. But compared to the F test, it actually is pretty effective because if we apply the harmonic F test to predict dormant response individually, we don't actually pick anything up. The biggest spike that we see is here, and that's just completely coincidental. I didn't do anything at this frequency. So, and it doesn't reach the 99.99% significance that we would need to for the harmonic F test. Okay, so that's kind of what we're talking about here. This plot is wrong, I think. Well, it's labeled incorrectly, but the plot itself is correct. This was a different set of Itself was correct. This was a different set of simulations, and we're actually looking at a coherent signal, I believe. So this x of t, the y of t uses the same g here in order to modulate. So this is a coherent signal. Otherwise, it wouldn't be picked up by the squared modulus, of course. Yeah, this is what happens. We see that it's being picked up fairly well. Again, we see this large bandwidth is definitely not as good as if we didn't have that modulation, but at least we can tell that something is going on, right? Whereas the F test, Right. Whereas the F-test, given that same setup, is not very effective. It freaks out, definitely, but it doesn't actually like bump up and tell us that there's anything going on. It doesn't rise to the significance level that we would need it to. Okay. So that's just kind of a demonstration of why we might want to actually use these test statistics. So again, T1 is less sensitive to coincidental coherencies, and both T1 and T2 are actually more robust to frequency modulation than the harmonic F test. And there are some Harmonic F test, and there are some results of that in my master's as well. If you want to see a table of that, um, so yeah, the issue here is that again, we're doing this across simulations, and that's no good because we're only going to have one realization in real life. If we're actually given data, it's not like we're going to have, you know, a thousand different sets of the same data or anything. I mean, I guess you could, but overall, we're not going to have that luxury, right? So how are we going to actually look at this when we only have one realization? We can only really look at one frequency at a time. We can only really look at one frequency at a time. So, here's an animation to represent that. So, essentially, what we're doing is we're moving through Fourier frequencies used to estimate a single realization of H. Okay, our view is restricted to a little band of frequencies, and that's what the animation represents. So, there's a signal right here, and that's why it moved away from the origin. The variance is a little bit tighter, but for the most part, we just saw that the modulus was taken away from the origin. Okay, so that's around the 700th Fourier frequency. Yeah, we can infer. Yeah, we can infer just given this like little band that the average over whatever dots are visible at a time is going to be non-zero, right? So that's what that's why we can do this with T1. But with T2, if we were going to look at the variance and how that changes as the frequency goes by, or as we move through the Fourier frequencies, the variance due to that lack of frequency stationarity is actually going to be pretty small throughout this entire cloud. So that's why that's problematic. And if we were to standardize, And if we were to standardize and if we were to try and get rid of that frequency stationarity, then that would just destroy the entire frame structure in the first place. So, yeah, I have a fun little like analogy that I use when I'm talking to people who don't do multi-taper or people outside of math even. And what I do is I would describe what I'm, what essentially I'm trying to do is I'm trying to find the shape of a sombrero. Okay. And the problem is I'm only allowed to see maybe like 10, well, however many frequencies, J, I guess. However, many frequencies, J, I guess, stitches at a time, like 10 consecutive stitches, and these are the stitches that we're looking at right now. So, if this were math, I would just be able to extrapolate and say like, okay, that's the shape of the sombrero. But because this is stats, randomness stitches with a very shaky hand. And so we're going to have to try and find another way of looking at this sombrero. Maybe we can sign a flashlight on it, see what kind of shadow it makes on the wall. Maybe we can submerge it in water and see how much is displaced. You know, we're going to have to try and get creative because only looking at Creative because only looking at 10 stitches isn't going to tell us anything about the variance. Okay, hopefully that makes sense. So, what we haven't looked at though is phase. Okay, and this is something that I haven't written up formally, so we're getting into some newer material here. So, this is the density function of a complex Gaussian random variables phase, which I'm just going to call a CGRE, as I've been calling the multi-taper transfer function estimate, the MTFE. So, at a given frequency, So, at a given frequency, this is what we have. Or actually, no, this isn't in terms of frequency. So, this is the density function of the phase. But this is simplified in that we are assuming that x and y are uncorrelated. So, x and y here aren't necessarily time series. They're just the abstract notions of the x and y axes, right? If we included a fifth parameter that was for correlation between x and y here, I wouldn't be able to fit the equation on one slide, but that's okay, and we'll see why in a second. Second. So at some particular frequency, G sub F, the MTFAs will follow this density, okay, with these parameter entries. So we already discussed that the real and imaginary parts of H are going to be roughly uncorrelated. So that's why we don't have that row factor. And also, because of that circular symmetry, if it was elliptical parallel to either the imaginary axis or the real axis, then we wouldn't have this. But because it is perfectly circular, we do have that the variance is going. Circular, we do have that the variance is going to be equal between the two. So, this is the density that we get. Okay. So, let's see. Ah yes. So, yeah. On the left here, we have a common signal at the 700 Fourier frequency, similar to what we were seeing in the previous animation. And then over here, we just have a response signal. So, we do see that when we approach that Fourier frequency associated with a signal. That Fourier frequency associated with a signal, then we see that the density gets very, very small. And that's because the phase, like the distribution of the phase, as the variance of that overall cloud from a top-down view gets smaller, then so will the distribution of the phase, right? Whereas if it just moves away from the origin, yes, it will depart from a uniform distribution, but it's actually going to have a little bit more variance to it. And we'll see an example of what that looks like. To it, and we'll see an example of what that looks like in a second. But basically, you can see this as the side view of the sombrero that I was talking about. Okay, so we can break this down a little bit more carefully, and I'll put the animation up here. So, the phase distribution will be uniform whenever the MTFE cloud is centered at zero due to that circular symmetry that we talked about. And it will only depart from that at response signal frequencies. So, it doesn't tell us when there's a lone predictor signal, but it will tell us anything about response signals or. Anything about response signals or coherent signals. So its variance will be extra small when the cloud is far from the origin and small in radius due to a predictor signal. We now only have to examine variance within a frequency band defined by that response signal. And so we don't have to worry about that, the 10 stitches of that Semburo analogy anymore. And we do, there's a little bit of ambiguity here, though, because we do want to distinguish between a massively or a relatively massive response. Or a relatively massive response signal, which could also give us that small variance. If it's very far from the origin, we would still see a small variance in the phase. And coherent signals with moderate amplitude. So maybe it's like fairly close to the origin, but because the signals are coherent, then we do see that contraction. So there isn't a lot of ways to distinguish that yet. So that's something that we can work on. So just revisiting these plots, this should be able, this should give you an idea of why we're seeing the distributions that we are. Why we're seeing the distributions that we are. So, the top two flots actually have the same phase distribution. They're both uniform, right? If we were going to take the distribution over these dots, the response-only case is non-uniform with large variance, like I was saying. The coherent signal case is non-uniform with small variance. And again, the mean of the phase is just telling us whether or not the signals are in sync. But yeah. So, what next? We can, well, we want to develop a test stack. Well, we wanted to develop a test app because we sort of developed T1 and it's working pretty effectively and it has some nice properties and everything. T2, we couldn't really get to because of that frequency stationarity issue. So what about T3? We want to consider the amplitude ratio versus, well, we want to consider coherent signals of a similar amplitude versus massively amplified response signals because that might result in similar behavior for this particular distribution. We want to investigate what advantages or disadvantages this method has. Advantages or disadvantages this method has in comparison to magnitude squared coherence and the harmonic F-test as well. We have to generalize and explore a little bit too. So, what happens if there are multiple signals in either series? Moreover, what if those signals have frequencies that are too close to distinguish between a frequency band? So, if they're within two radius, sorry, two W radius of each other, then it might be a little bit more difficult to distinguish. We also want to generalize over other noise types. Over other noise types. I've tried this for ARMA sequences with embedded signals and it's worked so far, and we still have that circular symmetry, in fact. But further generalization is definitely desired here. And finally, how does this theory translate to the linear filter model we would see representing this in the time domain? And how can we kind of go back and forth between both? And it might be interesting to try and interpret those filter coefficients now that we have a really good handle on what is happening with them in the frequency domain. So that's where I'm going to leave it for today. These are my references. I hope you had fun. My references. I hope you had fun, and I certainly had fun communicating these ideas. So, thanks for giving me the opportunity to do so. And thanks, of course, to my incredible supervisors who are so encouraging and supportive. And thank you to the organizers of this workshop, too. I've learned so much at this workshop. So, thank you for allowing me to give my little talk.