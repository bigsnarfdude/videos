Me, this nice opportunity, and of course, I'm very sorry I am not able to be with you in Banff right now. I'm going to talk about a joint work with Yves Mazari, who is now met for the conference in the University Paris-dauphin, and you can find the preprint both on Archive and on PALA. I'm going to talk about spectral optimization of parabolic operators, but first I want to present I want to present a simpler result for matrices. So, no PDs for the moment. I'm considering irreducible non-negative matrices. So, when I say non-negative matrices, I'm thinking of matrices whose entries are all non-negative. And irreducible basically means that you can apply the perimenous theorem. So, these matrices you can decompose. So, these matrices, you can decompose them as is written here. So, you have the first diagonal part that I write a diagonal of R because it will basically stand for growth rates in classical mathematical biology models. And then you have this S minus I times the diagonal, which is the mutation matrix in classical models. So, S is a double-stochastic matrix. Stochastic matrix. Double-stochastic matrices are matrices such that if you sum all lines, you get one, and if you sum all columns, you get one also. So S minus I is a matrix such that if you sum all columns, you get zero. And so if you multiply by a diagonal on the right, it's still the same. So it's the very basic property we want for mutation terms in classical models. Classical models. So it's, as I said, it's a mutation matrix in Darwinian models. It's also a dispersal matrix in discrete space because basically you can think of S minus I as the discretization of a laplation, for instance. And so the theorem I want to present briefly is a theorem due to Newman and Zay in 2007. This theorem states that the This theorem states that the peromphogenous eigenvalue has a minimizer and a maximizer, which are permutation matrices. So if you go through all double-stochastic matrices, you can find minimizers and maximizers among permutation matrices. This might seem like a classical convex optimization problem, especially because we know that the set Because we know that the set of W stochastic matrices is the convex hull of the set of permutation matrices. However, this is not actually a convex optimization problem because it is also known that the permobenous eigenvalue of an irreducible non-negative matrix is not a convex function of the off-diagonal entries. It is maybe a log convex function if you add the log convexity assumption on the A low convexity assumption on the diagonal entries, but in general, it is not a convex function. So, you cannot use a classical convex optimization theory. Before presenting the proof, I want to make a few remarks. First, it is a bang-bound type result. Optimizers, so minimizers or maximizers might be reducible if even if a priori we are working with irreducible matrices. There might be many optimizers. You can also find optimizers that are not permutation matrices. So they need not be in the set of permutation matrices. In fact, the mapping can be constant and the result can be extended to more general decompositions. The proof has four steps. In the first step, you strengthen the irreducibility in order to work only with irreducibility. In order to work only with irreducible matrices and to apply the performance theorem. In the second step, you add a clever rank-1 correction that increases the number of entries equal to one. Then you verify that this correction preserves the minimality or maximality of the parent of a unisecond value, and then you can conclude. In order to simplify this exposition, I'm going to skip step one and assume that I have already succeeded. Sufficient irreducibility. This means that I work with matrices L that are the sum of a matrix B, which is irreducible positive, and of a matrix S, which is double-stochistic. First, you assume that you work with minimizers, for instance. The proof for maximizers is mostly the same. You take a minimizer and you assume that it is not a permutation matrix. So it has a number of entries equal to one, which is not maximal, which is not capital. not maximal which is not capital n and which we call n zero up to permutations of the line and of the columns of the of the matrix you can assume that it has this particular form so the minimizer s s wedge has this block diagonal form with a matrix s top and a matrix s bottom s bottom is a permutation matrix so you put all entries equal to one in this bottom part and on the contrary s top And on the contrary, STOP has no entry equal to one. And in addition, you can assume that it is such that the left and right peromphobinous eigenvectors satisfy some minimality and maximality assumption. Basically, the first entry of the right eigenvector is maximal among the n minus n zero first entries, and vice versa for the Vice versa for the left pair of menus eigenvector. Then you can define the rank one corrector. So you define these two vectors A and B. A is the first column of your minimizer S minus one in the first entry. And B is the first line of your minimizer minus one in the first entry. The character T is a renormalized product of A and B, so A is a column. And B. So A is a column and B is a line. And so if you sum S and T, you get this new matrix, and you immediately see that it has, by construction, one more entry equal to one. You can also verify that it is still double-stochastic. And by construction, you have this sign properties on the scalar product between A and the left pair information eigenvector and between B and the And between B and the right eigenvector. These two assigned properties will turn out to be crucial. And then you can calculate the derivative of the eigenvalue in direction of the corrector. So you define an affine transformation of your matrix L, so L alpha with a parameter, a real parameter alpha, which is between 0 and 1. These new matrices P and Q are just the matrices that reorganize the line. The matrices that reorganize the line and the columns of the minimizer that I did not write before. So you can mostly forget about P and Q. And so you have this B plus S plus alpha times the corrector. So the perim of Benius eigenvalue is denoted lambda of alpha. It has eigenvectors. So peron for benefits eigenvectors u and v, u alpha and v alpha. And then with a very classical calculation, scalar product, you can find that the derivative find that the derivative at zero has exactly this form. Because lambda of zero is minimal among all Doby stochastic matrices, in particular along the affine transformation, the derivative has to be non-negative. But because of the properties in the scalar products A times V and B times U, you also find that it is non-positive. So in fact, it is zero, and because it is zero, one And because it is zero, one of the two real numbers on the numerator here has to be zero. So the corrector has either V or U in its kernel. Because of that, you can identify the perimfor-benefit eigenvalue along the whole affine transformation. It is constant, actually, up to alpha equal one. And so you have a new minimization. You have a new minimizer at alpha equal to one, and you can conclude either by contradiction, assuming that this minimizer had to begin with a maximal number of entries equal to one among all minimizers, or you can iterate over n zero and get a constructive proof. For maximizers, it's exactly the same proof, except that you change one of the permutations P and Q so that the the the product has the opposite sign. The product as the opposite sign, and it's exactly the same proof. So, how do we do to extend it to generalize this result for parabolic systems? The systems we consider have this form. So, it's a diagonal, so it's a Q, a diagonal of scalar parabolic operators, very general, without a zero for the term, minus a coupling matrix L. So, Q acts on vector value. acts on vector valued functions u that are functions of time and space. The space dimension is small n and the vector dimension is capital N Ai, Qi and L are all space-time periodic functions with periods, so time period T and special periods L1, L2, etc., up to Ln. You assume that the operator is uniformly elliptic and Is uniformly elliptic and regular enough, regular enough, and the coupling matrix L is cooperative, so this means that it is pointwise essentially non-negative. Essentially non-negative means that it is non-negative, except maybe in the diagonal, so you can have negative diagonal terms, but all of diagonal entries are non-negative, pointwise in time and space. And L is also fully. And L is also fully coupled, and fully coupled is a property on average in the pio D C T cell, so it is irreducible on average. If you have these assumptions, there are many applications. So applications in population dynamics with structured populations with discrete edge classes, for instance, in continuous space, in evolutionary biology with phenotype structure. With phenotype structures and continuous space, or vice versa, and also in chemistry in nuclear electrocourse. So, a very classical system, it's basically the most general cooperative space-time periodic system. And the result we prove with Evis Mazari is the following: if you can generalize the decomposition, so diagonal of R plus mutation matrix. Mutation matrix with S double-stochastic almost everywhere in time and space. Then the Krein-WÃ¼rttemberg principal eigenvalue has a minimizer and a maximizer, which are almost everywhere in Taymian space permutation matrices. The new difficulties are: first, the decomposition might not exist if L is not irreducible pointwise. Next, the existence of optimization. Next, the existence of optimizers is not obvious because for matrices you just have to there is a finite number of matrices so you can just go through them one by one but you cannot do that obviously for in this case. And the main difficulty is that you have to go from localized defaults to almost everywhere corrections. So how can you do that? So you introduce a function capital Phi that measures the default point Y. So S phi Y so s phi is gives the number of entries equal to one at time t and location x for the minimizer s that you are considering. Then you can write the spacetime locus of this default. So it's this omega periodic of s and n zero. So n zero is the number smaller than n of n tree is equal to one. Entry is equal to one. And you see that S is a permutation matrix almost everywhere, if and only if all sets omega periodic of S and N0 are negligible, have a zero measure if N0 is smaller than N. Then you can construct a localized correction. So this is basically the path where you repeat the proof of Newman and Zay. So you construct a minimizer on a So, you construct a minimizer, and I don't want to get into details about that. You assume that you have a number n0 smaller than n, such that the set of times and space where you have a default is of positive measure. You construct the correction at tnx with this time u and v principal eigenfunctions of q and of its adjoint operator q star. And then you calculate you you And then you calculate the same scalar product as before, but for P D, so with this integral over the whole pie disease cell with the character T, which is defined pointwise via a Noma and Z construction in some measurable subset little omega of the larger set. And the corrector is extended by zero elsewhere. If you do so, you can apply. If you do so, you can apply a Lebesgue differentiation theorem and find that you have the wanted property almost everywhere in the locus. So the same calculation as before. And so one of these two numbers have to be zero almost everywhere. So you can define the set where the first number is zero, the set where the second number is zero. One of the two is Is larger than half the measure of the locus necessarily. And if you assume that your corrector corrects only in the first set, you find this equality. And similarly, if it corrects only in the second set, you have the other equality. In all cases, if your corrector corrects only in a subset omega, which is either omega. Subset omega, which is either omega u or omega v, then via the same calculations and in the matrix proof, S plus the corrector is still a minimizer. Then you need to correct in the whole locus. So how do you do? You choose the locus of correction as the largest between omega u and omega v, and you iterate. So you repeat. And you iterate. So you repeat with the new minimizer, S1, which is S plus the corrector, and you construct the sequence, you pass the sequence to the limit. And since each correction corrects at least half the locus, it converges. And in the end, you obtain a new minimizer S infinity, whose locus is now of measure zero. And more precisely, the locus for number L0 is now gone into the locus. Now gone into the locus for number n0 plus one. So you can iterate over all values of n0, and in the end you have constructed a minimizer whose which is sorry, which is a permutation matrix almost everywhere in time and space. As a conclusion, so the problem is optimizing the periodic principal eigenvalue with respect to the off-diagonal entries of a cooperative matrix. Entries of a cooperative matrix. It is not a convex optimization problem, and this is very, very important. But still, there exists bound-bound minimizers and maximizers, which are almost surely permutation matrices. Perspectives are so what are the properties of these optimizers? So they might not be irreducible point-wise or on average. What can you say about their What can you say about their fixed points, etc. etc. What I presented works for the periodic principal eigenvalue. It also works for more generalized principal eigenvalues, but it does not work for the generalized principal eigenvalue lambda one, which is the limit of Jiricette principal eigenvalues in both of radius r. So it does not work for this for this generalized principal eigenvalue, and it's still an option. Principal eigenvalue, and it's still an open problem. And one more interesting perspective is: how do you go from this discrete population structure where you have continuous time, continuous space, but discrete phenotype, let's say, to a fully continuous population structure U of T, X and Y, where you would consider instead of these matrix operators L, let's say a differential operator. Let's say differential operators or integral differential operators. So, how do you optimize, let's say, a long-range dispersal or long-range mutations in this framework? Finally, I also want to mention more results that are in our preprint. You can find Hanack inequality, which is a generalization of inequality by Peter Poleczik and Paul Bess. You can find relations between Relations between many principal eigenvalues associated to this system. There are monotonicity results, concavity results, and asymptotic dependence results for these principal eigenvalues. Explicit formulas or estimates in special cases. Many, many counter-examples of properties that are true for scalar operators but fail to be true for systems. And also a comparison with the space periodic rearrangement of the media. Biologic rearrangement of the matrix. Thank you for your attention. Are you questions or comments? Well, it's very interesting, even though I'm not familiar with the area. I'm just could you explain uh a bit more why this maximizer or minimizer uh like uh communication matrix? Communication metrics. That's very interesting and a strong result. I'm not sure if we understand your question because of the echo. You want to know about maybe more explanation. One, the maximizer or minimizer of a computation matrix? Any more like an intuitive or explanation? Okay, so for me, the intuition is if you. The intuition is if you think about a discrete space. So it's not a mutation matrix anymore, it's a dispersal matrix. So you have patches, discrete patches, and you want to know the best strategy or the worst strategy to disperse between patches. So you know that there is an intrinsic growth rate associated with each patch, and you want to know where and how to move between these patches. That's it. Thank you. Any other questions? Okay, any other questions? Alex? Yeah, okay. So at the end, you well can you hear me? Yeah. Okay, at the end, you said some results on the comparison with space-paradigm. Does that mean that if you compare the arbitrary function arrangement? The function arrangement there is always inequality. Okay, so the result on the special rearrangement has nothing to do with the minimization or maximization of this matrix S. It's a different result. But basically, what we find is that if the matrix L does not depend on time, Not depend on time, depends only on space, so periodically on space, then it is always better to work with the space-periodic rearrangements. But I'm saying this like way too fast for it to be precise, so I would like to redirect you to the preprint where the result is stated correctly. Thank you. Any question more questions, comments? question or questions comments okay if martin is thank uh speak again thank you we're going to take uh 20 minutes