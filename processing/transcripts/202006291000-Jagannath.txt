So, yeah, so today's course is supposed to fit within the larger theme of this Semini de Mathematics Superieur. And sort of the broad goal, I guess, of the summer school was to sort of introduce people to ideas from statistical physics and discrete probability and how they can. And discrete probability and how they connect to questions related to optimization and statistics. So, with that in mind, I was sort of tasked with giving an introduction to sort of the ideas coming from Meanfield Spin Glasses. And so, the goal of this course is to give you a high-level sort of introduction to a lot of the core ideas that will show up throughout the other lecture. Show up throughout the other lectures. And my hope isn't that you'll sort of really learn in detail what necessarily any one thing is, but hopefully you'll sort of get sort of an intuitive idea about sort of what the various sort of important buzzwords would mean as you come across them in the later courses. So the course is going to be split into two days. So the first Days. So the first day there's sort of part one, which is the first day. And that'll be on what I'll call statics. So this is sort of, you know, questions related to the equilibrium theory of spin glass models. And the second day will be sort of an overview of recent results related to dynamics. Dynamics, and sort of natural algorithmics. Okay, so with that in mind, rather than sort of getting right into it, I want to give you sort of a warm-up just to sort of at least get your feet wet in understanding why there's the connection between spin glasses. Between spin glasses, which is nominally sort of a theory of disordered media and questions related to optimization and statistics. And so, what I want to do is start with the following warm-up exercise. And so, the goal is we want to understand. Optimization in large but finite dimensions. Okay? And so what I'd like to do is let's start with a fairly simple sounding class of optimization problems in high dimension. I'm sure many of you have either taught or TA'd a calculus 101 course, and you've probably given sort of low-dimensional examples of these in your class. Low dimensional examples of these in your classes. But the question is, sort of, what's going to change in the large dimension limit? So, the question I want to consider is: well, okay, so suppose I have just a homogeneous polynomial, you know, of fixed degree, say P in Rn. In Rn. So you have real values and real coefficients. So I'll denote this in a couple of ways throughout the lecture. Sometimes I'll call it HNP. I want to make the dimension dependence sort of explicit, but most of the time, just, you know, for readability, I'll just denote this by HP. And I'll suppress the dimension. And the thing you should just always remember is it's some map from Rn to R. And we'll be interested in the regime where n is very large. So, okay, so more concretely, what does this look like? More concretely, we have h p of x. Well, what is it? Sort of in most general form, I sum over p indices, I have some coefficients. And then I look at sort of monomials of the following flavor. Right, so here I'm going to allow sums over all sort of I1 through IP, all ranging over, you know, ranging from 1 to n. So for example, you know, I1 is equal to I2 is equal to, et cetera, all the way to IP is allowed in the sum. So every term is possibly showing up here. Possibly showing up here. So, this is sort of the general form of a homogeneous polynomial in degree p. And the question I want to ask is, you know, what can we say about the maximum of this function? So, of course, on full space, that's sort of not such a great question. So, let's just add a simple constraint. Let's add the constraint that this function is lying on this sphere. That this function is lying on the sphere, the unit sphere. And so, what I'll be interested in is sort of not sort of specific polynomials, but I'll be interested in trying to understand the average case behavior. So, what do I mean by this? What I'll do is I'll take my coefficients to be random, and for simplicity, I'll just take them. Simplicity, I'll just take them to be, say, IID Gaussians with the same mean and the same variance. Okay? So let's start to think about how this problem behaves. So when P is 1, this problem's not so bad, right? What you're doing is you're optimizing a linear form on Rn, right? Right, so h1 of x, well, that's just the function a, you know, okay, so you know, using the notation from earlier, it's I sum over indices a sub i, x sub i. But if you sort of take this a, all of these a sub i's, and sort of wrap them up into a vector a, you can sort of write this sort of concisely. Write this sort of concisely as a dot x, right? And so my question earlier was about, you know, maximizing h over the unit sphere. But this you know, right, by the variational sort of formulation for the L2 norm, this is just the L2 norm of this coefficient vector. And furthermore, you know, you know, the optimizer is just going to be. Is just going to be sort of the unit vector corresponding to A. So the only place we're really using the fact that this is average case is to say, for example, that A is non-zero almost surely. Sort of more broadly, if you want to sort of start thinking about the landscape of this function, you see that the landscape's fairly straightforward. Straightforward, right? There's a global optimum at the North Pole, a minimum at the South Pole, and then the function is just going to sort of increase as you go up the sphere. Okay? So, not such a complex problem. So, now let's think about the case that P is 2. Think about the case that p is 2. So, what are you doing is you're optimizing a quadratic function. And just for simplicity of discussion, I'm just going to assume for now that it's symmetric. So then what are we looking at? Homogeneous polynomial degree 2, it's just a sum on i and j, two indices, aij, x i, xj. So, again, if we play this trick of wrapping all these coefficients into a matrix, what is this? Well, if you look at it for a couple of seconds, you realize that, okay, well, you can write it as x dot ax. And we're looking at this restricted to the unit sphere. Right, so what is this? This is nothing but the Rayleigh quotient. Of A. Now, you know, because we're looking at average case, you know, A has almost surely non-degenerate spectrum, right, all the eigenvalues have. All the eigenvalues have multiplicity one. And so, again, you know, from a maximization perspective, what you're doing here is you're doing an eigenvalue calculation, right? You're calculating, you're trying to find, you know, the sort of top eigenvalue of the matrix A, and the corresponding optimizer is just plus or minus the corresponding eigenvector. So, the landscape of this problem is a little more interesting, right? Because, sort of, just if you remember, well, what's the space of, you know, the collection of critical values of the Rayleigh quotient is exactly the spectrum. And there are two n critical points, right, which correspond exactly to plus or minus the eigenvectors. Plus or minus the eigenvectors. Here we have sort of the global minimum, the bottom eigenvector, the global max is the top eigenvector, and everything in between is a saddle. The you know eigenvalue of or the eigenvector of order k is a saddle point of Of index k. Okay. So again, in terms of the landscape, this problem is sort of not so scary, hopefully. So now let's sort of upgrade the problem again. I'm starting to get some confidence, hopefully. And so we think, okay, well, let's think about cubics. Think about cubics. So, okay, so what am I looking at? I'm looking at a cubic function. And you're trying to optimize this. Now, at the moment, it's not super clear what to do, right? You can try playing this game again where now you have a coefficient. This game again, where now you have a coefficient tensor, and then you say, Okay, well, this is the inner product between the coefficient tensor and a three-fold tensor product of x with itself. And then if you're feeling really fancy, you can you know you can give this maximum a name, it's related to you know an injective tensor norm. But when it comes to actually trying to understand, for example, the maximizer, you know, it's less obvious, right? There isn't an immediate connection to spectral theory that you can sort of run to. So then you can say, okay, well, you know, it's a fairly simple looking polynomial, constrained optimization. How hard could this be? How hard could this be? Let's just try to use calculus, right? Let's, you know, you know, you can try to use Lagrange multipliers, say. So if you're going to do something like this, you need to sort of cycle through the collection of critical points and try to classify them. So one thing that sort of helps your life is it's going to be not so hard to see that the Hessian Is almost surely non-degenerate. So, in particular, this function is almost surely a Morse function. And so then, but okay, so we need to classify the critical points. So then you ask yourself, how many critical points are there? Well, it turns out there's actually Well, it turns out there's actually exponentially many of these with overwhelming probability. This was a result of Ofinger, Benarus, and Cherny combined with a later result of Subag. I think this was from 13, I think this was from 16. And you say, okay. And you say, okay, you know, that's not so great, but I only care about critical points that correspond to local maxima, say. So maybe I only care about critical points of the fixed index. So you can do is you can try to compute the log number of critical points of a fixed index lying above a certain energy level. Since we're maximizing, we'll look at index n minus k. x n minus k and again sort of this will this will be exponentially large the the number of critical points will be exponentially large and in the end sort of using those results what you find is that you know after a certain sort of global number you have exponentially many points of index n so you have exponentially many local So you have exponentially many local maxima, exponentially many critical points of index n minus one, exponentially many critical points of n minus two, et cetera, et cetera. So yeah, so what's the upshot of this discussion? Discussion. So, one thing you realize is this problem is very complex as soon as p is greater than 2. And if we like to think about things from a sort of mathematical programming perspective, this seems to be in some sense a highly non-convex problem. Of course, whatever that means on the sphere. Of course, whatever that means on the sphere. But things are sort of even funnier, right? You have exponentially many critical points of every index. One thing you can ask yourself is, you know, if I look at a typical sort of ball, will I see a critical point? And that's where you start getting nervous, right? Because if you look at the covering number, Of the sphere, this will also scale exponentially in the dimension. So, sort of, you have this sort of fundamental but scary sort of observation that the entropy of the set of critical points is comparable to sort of the entropy of the space itself. So, this in the statistical physics literature, when people see this, they see, okay, this is actually sort of a hallmark of the behavior of a class of statistical physics models called spin glasses. So, okay, so what are spin glasses sort of historically? Splined glasses were introduced as a model for something called the glass transition. The hope was to try to understand sort of from a thermodynamics perspective the properties of actual sort of real-world glasses. So sort of as a starting point, a class of models, short-range models, were introduced called the Edwards-Anderson models. And so, this is a, you know, you can think about it as kind of a random version of the Ising model on Z D. Right, so what's the picture? You have, you know, say in two dimensions, you have your integer lattice. And then the Edwards-Anderson model, you know, the energy. The energy is you're going to have some you're going to have a two-body interaction, but the two-body interaction between any two vertices is going to be random, right? So to each edge, you're just going to flip a coin and assign a value. So, the Edwards-Anderson model turns out to be extremely difficult to study, both rigorously and non-rigorously. And so, shortly after, so in a sort of very important paper, Sherrington and Kirkpatrick introduced sort of a mean field version of this. We're now in Where now it's instead of looking at sums over, instead of just restricting yourself to vertices that are sort of connected to each other, everyone's going to be interacting with everyone else, right? So you can think about this as sort of a random analog of the Curie-Weiss model. And then later, another sort of important class of models that sort of helped understand, develop the theory of spin glasses that we'll be talking about a lot. That we'll be talking about a lot today is what's called Dairy Daws P Spin Model. So I'll define what these are in just a bit, but I wanted to get sort of to one of the main upshot of today's lecture, which is Which is, you know, the following idea, which was sort of introduced by Giorgio Parisi in the late 70s. And his idea was that, you know, maybe, you know, okay, so in the theory of critical phenomena, you expect that phase transitions correspond to some form of Correspond to some form of symmetry breaking. Right, so for example, if you think about, say, the Curie Weiss or the Ising model, once you sort of cross the Curie temperature and you enter the ferromagnetic phase, the global spin-flip symmetry has been broken. And so, what Parisi suggested is that, well, okay, well, if, you know, what should be the there's, you know, if the glass transition is. There's, you know, if the glass transition is any type of thermodynamic transition, there should be some kind of phase transition happening here, some sort of symmetry breaking here. And what he suggested is a very exotic kind of symmetry, which sort of hadn't been seen before. And we're still only just being able to understand what it means from a mathematical perspective. And it's called replica symmetry breaking. Breaking. So this replica symmetry gets broken is the idea. And so, okay, so I won't really be able to give you a complete explanation of what replica symmetry breaking means as the physicists understand it. But the hope for the rest of today's lecture is for you to sort of get a feel for at least the current sort of working understanding of replica symmetry breaking we have in the mathematics. Of replica symmetry breaking, we have in the mathematics literature. In the sense that, sort of, all the key hallmarks of replica symmetry breaking, we can at least now sort of understand from a rigorous perspective. So with that in mind, I'll sort of take a pause and next up, we'll try to understand a very simple model of replica symmetry break. Model of replica symmetry breaking, which is called the random energy model. All right, yeah. Okay, so we'll have a short break, maybe let's say till 12.30, and you can feel free to ask questions in the chat if anything isn't clear. And also, Okash can go over the questions that were asked. They were asking. I think there was only one. There are more coming now. So, yeah, Agosh, you have access to the chat. Yeah. Yeah, so you can answer. There's a question by Yogashvi on I think you can uh I don't know if you're typing, but you can probably just ask them on the answer them directly. Yeah, so the question was why are the curves of critical points decreasing in the okay I was just the number of critical points so I so I guess maybe you're used to seeing this plot a different way here. I'm plotting things from the perspective of maximization. Maximization. So, you know, this top curve is, say, the number of critical points of index n. And so, all I'm saying is that, you know, so here I'm plotting, you know, if I call this theta k, here I'm plotting theta k as a function of the energy. And, you know, what I'm saying is as the energy gets higher and higher, there'll be exponentially many critical points, but sort of the exponent is going to get smaller. Sort of the exponent is going to get smaller and smaller, right? You know, if e is really large, then you expect to have no more critical points. I hope that answers your question. Yeah, so that's not so. Okay, so Yogesh is asking. So, okay, so Yogesh is asking: why is it decreasing in K? Yeah, so I mean, this is one of the interesting things. And for the P-spin models, you know, sorry, these homogeneous polynomials of degree P's, you know, you have exponentially many local sort of maxima. You also have exponentially many local saddle points of index n minus one, but there are exponentially many more local maxima than saddle points of index n minus one. Settle points of index n minus one. Sort of the intuition for that, I can't really say much, but it sort of comes out of the calculation. I don't think it's sort of a hard and true thing about random functions in general. I think that sort of happens for these P-SPIN models. Any other questions for Akash Mikhail before we continue? It looks like we're good. If you want to continue on, that'd be great. Okay. So, all right. So, now I'd like to sort of talk, I'd like to talk about sort of a very simple model of replica symmetry breaking. You know, it was introduced as a model of replica symmetry breaking in the late 80s by. Symmetry breaking in the late 80s by, I mean, the early 80s by Derrida. But as you'll sort of see in a couple of seconds, it's actually connected to very sort of classical ideas in probability theory related to extreme value theory. So what we'll do is we'll think about sort of the following toy spin system. So the configuration space for my spin system. For my spin system will just be the discrete hypercube in dimension n. So we'll assign spins plus or minus one to n particles. And what I'll do is, and so, okay, so corresponding to each configuration, there's an energy. And what I'll do is, you know, okay. Know okay, so the energy is some function from the hypercube to the real nine. Well, one way you can think about sort of a random such function is think about it as a stochastic process that's indexed by the discrete hypercube. And sort of, and Derrida's idea is: well, let's think about the following very simple case where I just assign independently at random. Independently at random energies to all possible configurations, right? So this is just a collection of IID Gaussian random variables. Each of these will be, say, IID Gaussian random variables with variance for normalization reasons. We're going to want it to have variance at. Want it to have variance at. So, the question I want to ask about this model. So, there's two questions I want to ask. The first question I want to do, I want to ask is I want to understand a certain object called the Gibbs measure for the system. So, here, what do I mean by the Gibbs measure? You know, I mean the measure. That assigns to a point in the space a mass that's proportional to an exponential of the energy. And then I'll normalize it to be a probability function. And here, so this normalization constant plays an important role in statistical physics if you're sort of used to thinking about things from a statistical physics perspective. About things from a statistical physics perspective, this normalization constant has a name, it's called the partition function. And sort of related to this, you know, one thing I'll want to compute is a certain quantity called the free energy. And the free energy is just the log of the partition function. Let's give it an index n. So this is fn of beta. And our hope is to understand as n goes to infinity. The limit of this quantity. It turns out it'll exist almost surely. Now, the motivation for looking at the free energy, you know, there's, you know, by itself, it's an intrinsically interesting quantity for sort of condensed matter reasons. But from the perspective of understanding these questions related to the Gibbs measure, sort of one thing to keep in mind is as you differentiate To keep in mind is as you differentiate the free energy in various parameters, you pick off statistics of the Gibbs measure. So, for example, if you differentiate the free energy in beta, you're differentiating the log of the sum in beta, and what you'll pick out is the, you know, the average energy under the Gibbs metric. So, what we'll find in this setting is the following. So, there's a certain critical temperature. Beta critical just square root 2 log 2 has an explicit formula in this case. The idea is that above this critical temperature, you have sort of the measure, while it's not going to be uniform, it's going to be sort of uniform-like in the sense that you have sort of the bulk of the energies sort of dominate the behavior. Dominate the behavior of the system. But above this critical temperature, you'll have what are called Poisson-Dirclay statistics. And so in And so, in particular, what you'll find is that a very few states, there's a small number of states that will sort of completely dominate the behavior of the system itself. So, okay, so how do I see that coming out of this simple model? The starting point is to observe that, you know, if x is a Gaussian random variable, then e to the beta x is sort of a power law-like random variable. It has power law tails. And in particular, in the cases we'll be interested. And in particular, in the cases we'll be interesting, this power will sort of be inversely proportional to beta. What does that mean for us? Well, let's think about it. We'll define this quantity wx, which is this exponential of e to the minus beta h. H is a standard is a centered Gaussian, so h and minus h are the same thing in law. And then when I'm And then, when I'm thinking about the partition function, I'm looking now at an IID sum. And when I'm looking at the Gibbs measure for the system, I'm looking at the ratio of an individual term to the sum of all the terms. And so here's where you start remembering your sort of parallel random variables 101. If wx is a parallel random variable with power alpha, you have the following trade-off. If alpha is bigger than one, then the maximum Then the maximum is dominated by the sum. So no individual term sort of noticeably contributes to sort of the entire partition function. But when you cross alphas less than one, what you have is now the max. Is now the maximum and the sum are of the same order of magnitude. In particular, this ratio is going to converge. It'll have a non-trivial limit. Um Um okay. So you could go a bit further. You could try to start to understand things like order statistics. So, what I'll do is, I'll look at the ranked weights. And what you know from sort of classical extreme value theory is that suitably normalized, these ranked weights will converge to a Poisson point process. Sunpoint process with intensity alpha, intensity measure given by sorry, a power law, right? Its mean measure is a power law. And furthermore, if you look now at this ratio of the weights to the sum. This is sort of one of the definitions for the Poisson der clay process. And so, here, you know, one thing to keep in mind when I say Poisson d'Erclay, there's sort of two different types of Poisson d'Arclay that are sort of really thought about. And there's sort of in general, there's a two-parameter family. Of Poisson-Dirichlet random variables. And if you want to learn more about them, there's this nice survey by Pittman and Yor. So the thing I want to emphasize here is that You know, if I'm saying that these quantities here are macroscopic in the limit, that tells you that each individual W contains a huge fraction of the mass. If you look at the first, say, finitely many of them, it'll sort of have most of the mass. So, okay. So, at the moment, we've been talking about extreme value theory. You know, let's get to the point. Now, let's get to the point. What does this have to do with this fancy sort of terminology, replica symmetry breaking? Where's the replica symmetry breaking here? Well, what we'll do is we'll do the following. We'll look at two independent draws from the Gibbs measure. And sort of just a bar. And sort of just to borrow the physics terminology, or at least to translate, independent copies for us are called replicas in this language. And what I'm going to do is I'm going to look at the following question. I want to measure the similarity between my two replicas in terms of a quantity which I'll call the overlap. Call the overlap, which I'll denote by R12 when I have my sort of copies one and copy two. And all it is, is it's the normalized inner product. So, you know, when they're the same, this is one. When they're in typical, it's minus one. And when they're very different, this is essentially zero. And what I'll do is, I'm going to look at what I'll call the overlap distribution. My question is the following. Let me look at the sort of random law, which is the probability, the Gibbs probability, that the two replica have overlap, which lies in some interval. In some interval. And what we have is the following For any beta, if you look at a set A in the unit interval. In the unit interval that excludes the points zero and one, then the expected Gibbs mass of having two replica with inner products lying in that interval goes to zero. On the other hand, If I look at the overlap distribution corresponding to the point one, which is to say, if I ask myself what's the probability, the expected probability that my two samples are equal to each other, there's a phase transition. If beta is less than the beta critical, you get zero. So, you know, and you can think about this, you know, when beta is zero, what's the chance that two uniform at random points are the same? Of course, the answer should be, you know, tending, the probability should tend to zero. But when beta is zero, But when beta is above beta critical, it's one minus beta critical on beta. And just sort of to match beta critical to the discussion from above, you know, where does beta critical come from? The idea is that if you look at the power. Corresponding to beta critical, in that whole sort of power law discussion, beta critical is the beta whose power corresponds to one. And again, just as a reminder, beta critical is just the square root of two log two in this case. So here you can already see something sort of So, here you can already see something sort of non-trivial is happening with these independent draws. You know, at very high temperature, independent draws are basically orthogonal to each other. And so the idea is, you know, in some sense, they sort of morally look the same. But at low temperature, when you start taking independent draws, there's sort of a few very important points that will matter. So let's think about this phase transition from a different perspective. From a different perspective. Instead of looking at the Gibbs measure, we can also look at the free energy. So remember, what was the free energy? It was just, I took my IID sum of weights, and I took the log of it and divided by n. So what would happen if you had law of large numbers behavior for this object? Well, if you had law of large numbers behavior, now each term. Now each term is gonna be, you know, you have e to the beta critical n, beta critical squared over 2n many terms. And each term has mean e to the beta squared on 2n. And so the Laura of large numbers would suggest that the partition function would converge to this. I mean, sorry, the free energy would converge to this. And this will happen when beta is less than beta critical. But when beta is bigger than beta critical, what happens is Is that instead of having quadratic growth in beta, you suddenly get linear growth in beta? So the upshot from this discussion is that while at high temperature, which is to say small beta, Small beta, you have law of large numbers like behavior, but at low temperature, you have what's called condensation phenomena in the statistical physics literature. In particular, you know, one thing you could try to do is you could think sort of as a toy, you could try to think about what the Gibbs measure is going to look like at infinite volume. Look like at infinite volume. So, what I can do is I can take the unit, I can take the discrete hypercube and normalize it in such a way that it lives on the unit ball. And I can try to ask myself where most of the Gibbs mass behaves, it lives. And the upshot is, you know, at low temperature, you have these sort of very few points which have most of the mass. Which have most of the mass. And these points, well, okay, I can't draw it that way, but these points are all going to be sort of all going to be orthogonal from each other. All right. So this is sort of, you know, the simplest sort of picture of a replica symmetry breaking system. And so we'll. And so, what we'll do is we'll stop here and up next. My goal is to sort of at least explain sort of what's called the full Mazard Parisi-Virasoro picture of replicas. Of replicosymmetry breaking that's supposed to occur in sort of the canonical models of spin glasses, which I'll also sort of introduce after the break. Okay, so maybe we stop the recording. It's all right for now. And once the recording and the live streaming on YouTube YouTube is down, then you can give the opportunity for people to unmute themselves and ask questions.