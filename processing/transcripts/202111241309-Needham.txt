Get a metric measure space. In practice, where you care about these things is more like a point cloud sitting in a metric space. So I have like a finite metric space, and I can take, say, uniform density over this point cloud, or I can perhaps like the particular application I care about has some natural weight attached to each point. And that would be recorded in the probability density. Okay, so those are the things I care about. Those are the things I care about comparing. This is just a reminder. So, remember that Gromov-Hauserf distance, which usually I see it defined in this kind of embedding formulation. We can also define it as infamizing an L infinity norm of a certain function. So now we have measures in the mix, and the idea is I can relax this L infinity Gromov-Hausdorff distance to an L P type distance between spaces. And that is what is called the Gromov-Wasserstein P-distance. Called the Gromov-Wasserstein pedestals. So, this was invented by my collaborator Frakundo. The big paper is around 2011, although he had been doing stuff earlier than that in this direction. Okay, so what is this thing? So, this is a way to compare two metric measure spaces, X and Y. So, once again, a metric measure space, I'm handed like a triple of data, a set, a metric, and a measure. How do I compare these two things? How do I compare these two things? Well, I just take this kind of Gromov-Hausdorff distance as my template. So I keep the same distortion function, same metric distortion. Now I can replace the L infinity norm with an L P norm, where the measure I'm integrating against in this L P norm is pi cross pi, where pi is a coupling of mu x and mu y. Okay, so pi Pi think of as being a probability measure on x cross y, whose marginals are mu x and mu y respectively. So then that means that pi cross pi is like a probability measure on x cross y cross x cross y, which is the domain of this function. So it makes sense to integrate against that sort of thing. Now, once again, this is like an optimization problem. I want to infimize overall coupling to try to find the smallest value of that. Value of that. Okay, so I don't know. To me, this is like a lot to take in. So let's maybe think about this for a second more and see if we can kind of make intuitive sense about what this does. Okay, so here I have like a little cartoon of a metric measure space X and a metric measure space Y, and these are really abstract spaces that don't really necessarily live in the plane together. And so these have distances between points, and each point has. Distances between points, and each point has like a mass, say, represented by the size. So, once again, I can, just like in classical optimal transport, I can think of pi as like instructions for how to move mass from x to y. Okay, so in a discrete setting, I really have a value pi xy. And, you know, if x is here and y is here, pi xy is telling me how much mass do I move from this point x to this point y. Okay. Okay, now the difference here from optimal transport is in optimal transport, you're able to have kind of some absolute comparison between any two data points because they live in a common metric space. Here, I can only compare things relatively, right? I can't say what the distance is between that point and that point because they live in different abstract metric spaces, but I can compare relative distances, okay? And that's exactly what's going on here, okay. Exactly, what's going on here, okay? So now the intuition is that when should I move a lot of mass from x to y? Well, yet you have to kind of say this in pairs. I move a lot of mass from x to y and a lot of mass from x prime to y prime when these distances are roughly the same. Okay, so if these are the same, then I want to try to move mass in that way and vice versa. So if I have very different distances, then I want to try to avoid assignment mass in this case. In this kind of coupled way. Okay, so that's the idea. Grommel-Bosterstein distance is more or less a way to softly assign points between metric spaces, metric measure spaces. That's the way I think of it. Mostly we'll talk about kind of more theoretical issues, but I'll just mention a couple of applications. So I've been interested in the applied side of Nermal-Bosterstein distance recently. So here are a couple of things that I've done. So, here are a couple of things that I've done. Here's an application to segmentation transfer. So, the idea is: I have like these 3D point clouds, like airplane or something. An expert has gone in and said, all the points here belong to the wing of this airplane. And now I'm handed a new scan of an airplane, and I want to automatically register which points are the wing, which points are the fuselage, et cetera, without having to look at every picture of an airplane. Picture of an airplane or every 3D scan of the airplane. So the idea is I take this known segmentation, I take an unsegmented point cloud, imagine this is just totally colored gray, and I say, okay, think of these as metric measure spaces, compute dormal-Wosterstein distance between them. And in the end, that's going to give you a coupling between those two metric measure spaces, which I think of as giving like a soft registration between the. A soft registration between the point clouds. Then I can transfer my segmentation by saying, okay, if a lot of mass moves from a wing to some point over here, I'm going to feel confident to call that target point a wing point. And that's basically the idea. And so some reasons why you might use a Gromov Vosterstein instead of like a Vosterstein distance is maybe you don't want to try to align point clouds, like rigid registration. Like rigid registration. So, this would be an isometry invariant type of comparison. If I take an airplane and I rotate it, that's distance zero from where I started in Gromo-Bosserstein distance, or it's positive distance in Bosterstein distance. Or if I was trying to register like human shapes or something, I have one person with their arm up and one person with their arm down, then using kind of the ambient distance metric, these are pretty different spaces. But if I use geodesic distance, Spaces, but if I use geodesic distance, then these are pretty similar spaces. And if I use geodesic distance as my metric, then I would be able to register arms correctly, for example. Another application is graph partitioning. So here the idea is pretend these nodes aren't colored here. I'm handed a network or a graph, and I want to, without any labels, I want to try to learn communities in the graph. This is a totally classic graph analysis problem. Graph analysis from community detection. A way to do this, so this is building off work of Zhu, Luo, and Karen from a couple of years ago. So our contribution was to kind of introduce some spectral geometry into the algorithm. But the idea is I take my graph I want to partition. I compute a German-Wosterstein matching with kind of a template graph that has ground truth communities, like a totally disconnected graph. The result is a The result is a coupling, which gives me a soft assignment from the nodes here to the nodes here. And then I say, okay, if a node sends a lot of mass to the left, then I'm going to group all of those nodes into a community and vice versa, sending mass to the right. So here are just other people's work. This has become sort of a popular tool in the last couple of years for doing machine learning type stuff. So natural language processes. So, natural language processing, doing generative adversarial networks, and then some biology applications have all come out in the last few years. So, it's become kind of a tool that people are excited about. One thing to mention here, though, a very important thing to mention, is that all of these applications I just mentioned are all approximations of remote ostracize distance. They're all computing couplings really just by doing gradient descent. Really, just by doing gradient descent in the space of couplings, using this norm of the distortion function is like a loss function. And why is that? It's because you can't really compute German-Osterstein distance, just like you can't really compute Gromov-Hausdorff distance. Gromov-Wosterstein distance, like maybe kind of feels like you have more of a chance because the optimization space is over couplings, right, which is like a continuous space. Right, which is like a continuous space, and the space of couplings is even like a convex polytope, so that feels okay. Um, the issue is that the objective function is non-convex, and this is a quadrature, what's called a quadratic programming problem because the pi appears twice in the integral. And it's known that this is not something that you can actually solve attractively. Okay, so you can use gradient descent to get approximate solutions, and maybe you're happy with that. But the lost landscapes tend to be like very bad. Tend to be like very bad. Just empirically, it's really easy to get stuck in a local min, meaning that your approximations of GW distance are maybe not good if you're using these kind of standard gradient descent algorithms. So for that reason, we would like to have something that's actually computable and related to German-Bosterstein distance. And the thing we can do is derive lower bounds for the distance that are actually computable. So this allows you to kind of verify the So, this allows you to kind of verify the quality of your approximations by gradient descent. And the point for the rest of the talk is that the lower bounds themselves are just kind of interesting mathematical objects. So, that's where we'll go now is to kind of discuss what these lower bounds look like. And these are the distributional invariants as mentioned in the title and kind of try to understand some of the properties of these things. Okay, so the first one is something that we saw in Murielle Bouton's talk on Monday, which is the distance distribution of a metric measure space. So if I have a metric measure space x, I'll use this notation. The distance distribution is this function h sub x, which eats a real number, spits out a number between 0 and 1. And what does it do? It takes, okay, so. Takes, okay, so for any point X, I have my R value here picked out. For any point X, I can take the closed metric ball centered at that point. Okay, now I have a measure lying around, so I can take the measure of that closed metric ball. And then to remove the dependence on the point x, I can integrate that quantity over all x's with respect to mu x. And I can think of this as like the probability that two points are within distance r of each other. Are within distance r of each other or expectation of this the volume of a metric ball of radius r for finite x this is basically just like the cumulative of the bag of distances so like if I take the distance matrix for my metrics for my metric space look at the distribution of the numbers that appear in that distance matrix and take the cumulative that that's more or less what this is doing. Is doing. Okay, here are just a couple examples. So if I take a circle with geodesic distance in the circle, not Euclidean distance, then I get this nice constant increasing distribution of distances, which eventually levels off at pi, I suppose. Whereas if I take an interval with Euclidean distance, I already start to see that the boundary effects kind of giving me a different distribution of distances. So it's able to. Distribution of distances. So it's able to pick up things about the geometry of the metric space. Okay, so that's a distributional invariant and it is related to Gromo-Bosterstein distance as a lower bound, as I hinted at earlier. So what I've written here is a lower bound for the p equals one case, just because this is like the easiest version to write out. But you have general lower bounds in terms of these distributions. Terms of these distributions. Let me get my blinds real quick. It's super sorry. Okay. Yeah, so we have the solar bound, which says that Gernol-Wosterstein one distance is bounded below by the integral of differences in distance distributions over R, over all R. Okay, so there's our lower bound. And so you can use this to compare data sets, use this sort of metric. Use this sort of metric. But the theoretical question we're interested in is: when does this lower bound collapse? Meaning, when do I have non-isomorphic spaces, X and Y, with the same distribution of distances? Okay, so this is a totally classical problem, and there are results or counterexamples and results about this very question. The earliest that I know of is this example from the 70s. Example from the 70s of Bloom, which is points in the real line. So these configurations of points in the real line are not related by a translation or a reflection, but they do have the same distribution of distances that says uniform measure. And then there's this example from Bouten and Kemper in 2004, where I think this picture was drawn on the chalkboard on Monday. So you take the four vertices. So, you take the four vertices of this kite versus the four vertices of this trapezoid. You pick your distances carefully in a way that you can actually realize this in the plane, and then look at the histogram of distances, and you get the same thing. Okay, so hx would be the same as hy in this case. Although in this paper, they do have very nice results, which inspired a lot of the word here that says that distance distributions do distinguish point clouds. Distributions do distinguish point clouds generically and locally, at least. So, this is like a very non-generic sort of situation. Later, Brinkman and Ulver asked whether HX can be used to determine continuous objects like plane curves. Okay, so here I'm taking the boundary of the convex hull of the kite configuration. So, I'm thinking of this blue curve as a continuous curve, thinking of Thinking of the boundary of the convex hull of this trapezoid as a continuous curve, this red curve. Now, if I look at the histogram of distances, I get something which seems to be different. So it seems like this counterexample doesn't pass through to this particular continuous set. So the question is: does the same distance distribution for curves imply rigidly isometric curves? So, with Facundo, we found this counterexample, which says that these curves specifically give a counterexample. So, the trick here is I take some octagons, then I'm just going to kind of bubble out certain edges. And these are almost the same. If I take, I think, yeah, this triangle and flip it up here, then I get this curve. Okay, so they're related by this kind of surgery type. Related by this kind of surgery type move, and that's how we prove that these have the same distribution of distances. We have some theorem that tells we have a recipe for checking distribution of distances piecewise in some sense. So if I can take a space and chop it up and rearrange it and preserve kind of like the pairwise distributions of distances, then overall I win. So I'll mention that I found, I was just looking on the archive and found this example in a random And found this example in a random paper on really a very different topic, but traced it back to this paper of Mallows and Clark in the 70s, where this was used as a counterexample to a different conjecture of Blossch. Okay, so yeah, so there are some counterexamples which show that in general, distance distributions do not distinguish objects, but then kind of the But then, kind of, the point of this work is to try to refine that question and see if you can have any kind of positive examples or if you can say things a little bit more detailed about like topology between things which are say iso-distributional. Okay, so we do this in a variety of different categories. So for the rest of the talk, I'm going to kind of just bounce between different situations where you can prove things about distinguishing power of these distance distributions. Distributions. Okay, so the first one is for Riemannian two manifolds. So the result here says that if I have Riemannian two manifolds, X and Y, so I'm thinking of these as metric measure spaces endowed with geodesic distance and normalized Riemannian volume. So these are closed surfaces. If they have the same distribution of distances, then you can say that they are dithiomorphic. Okay, so you have kind of a topology control coming from distance. Coming from distance distributions. And x is constant curvature kappa if and only if y is constant curvature kappa. So if you have extremely regular type of shapes, then being isodistributional tells you that they have the same sort of rigid regularity. So just to say kind of the ingredients that go into the proofs as we go through this, this is kind of based on doing a Taylor expansion of the distribution of distances. Of the distribution of distances. If you do this, you get terms that are like total curvature and total squared curvature. So the total curvature term gets you Gauss Bonet, which gets you diffeomorphism. The total squared curvature term, you have to be a little tricky, but you can play like a Cauchy-Schwartz type trick and get this constant curvature result. Okay, so then just we can maybe refine this even a little bit more. So if x has More. So if X has constant positive constant curvature, then Y has the same positive constant curvature. So then they're both Rerman spheres. So you get not just diffeomorphic, but isometric. In general, this doesn't hold. So you can have closed Riemannian two manifolds with the same curvature and the same distribution of distances, which are not isometric. And so in the case of flat tori, we can totally characterize that. Can totally characterize that. So, flat tori, meaning curvature equals zero, constantly have the same distance distribution if and only if they have the same lattice. So by that, I mean, you know, take a fundamental region for your flat torus and take generating vector, say, and then take integer linear combinations of those. Okay, and that's going to pick out a bunch of points in the plane. And that's what I mean by lattice. Okay, that kind of all the translates of the fundamental regions. Translates of the fundamental regions. Now I can have non-isometric flat tori with the same lattice, and it turns out that that's equivalent to having the same distribution of distances. Okay, so we also have kind of more general sphere rigidity theorems. So we saw, you know, in the last example that constant curvature buys you something when you're talking about distinguishing power of these distance distributions. Distributions. And the same sort of thing holds in higher dimensions. So here we have a result for Riemannian manifolds. So if I have a Riemannian D-manifold, so I'm thinking of that as a metric measure space, once again, with geodesic distance, normalized Riemannian volume. So these should be probably compact for this to work. And I impose a Ricci curvature bound. So I say that the Ricci curvature is bigger than. say that the reaching curvature is bigger than or equal to d minus one then i can find an epsilon which is dimension dependent okay such that if the one waserstein distance between the derivatives of the uh distance distributions is less than epsilon then x is diffeomorphic to s d so let's unpack that for a second maybe so hx i'm thinking of as a cumulative distribution so d hx i'm thinking So, dhx, I'm thinking of it as the distribution. So, these are distributions on the real line, and I can compute their one Vosserstein distance between these distributions in the sense of plain old optimal transport from the third slide. So if the Wasserstein distance is small enough in a dimension-dependent way, then I get diffeomorphism. So, once again, I can control topology in this higher-dimensional set of biodiesel distributions. And if, oh, and this is a little bit nicer than the previous result, where I don't need on the nose to be diffeomorphic, right? So I have some a little wiggle room for diffeomorphism here. And if they are equal on the nose, then X is isometric to the sphere. So just to say what goes into this, so this one Vosterstein distance is due to a result of copy. Is due to a result of Kochndorf in 2008, where he was studying sphere rigidity for one diameters as an invariant. And then the isometry result comes from Chang's rigidity theorem. So yeah, so that's the Riemannian result. We also have kind of an embedded hypersurface version of this. So now let's suppose that X is an embedded closed hypersurface in Rd plus 1. R d plus one. The difference here, so it's still a manifold. The difference here is I'm thinking about this as a metric measure space in a different way. So I'm not using geodesic distance, I'm using extrinsic distance, right? So in the previous result, I'm computing distance like this. In the current result, I'm computing distance like this. Okay. So, but once again, so now I'm going to use, I guess, a uniform measure. Measure on this. Okay, so then now here the result is that once again, if they have the same distance distribution, then X is isometric to S D. I forgot to say something very important here, though. So once again, we have to impose curvature constraints. So here, the curvature constraint is that at every point, at least one of my principal curvatures is non-negative, and I need all principal curvatures less than or equal to one. Principal curvature is less than or equal to one. So, with that imposed curvature constraint, then I can get that if these, if two of these embedded hypersurfaces have the same distribution of distances, then they're or sorry, if one hypersurface has the same distribution of distances as a sphere in particular, then X is isometric to a sphere. And the proof here is based on a result of Karp and Pinski, where they actually work out the Actually, work out the Taylor expansion of the distribution of distances of an embedded close hypersurface with extrinsic Euclidean distance. And then so then the kind of a little bit artificial looking principal curvature constraints allow you to do a little bit of manipulation with the terms of the Taylor series. You can back this result out. Okay, but I was speaking a little bit derogatory. I'm speaking a little bit derogatorily about the curvature constraints because we don't have a counterexample or anything that says do you actually need these curvature constraints? And in fact, when d equals 2 for either the Riemannian version or the extrinsic embedded version, we can get rid of the curvature constraints by using other properties of the two-sphere. But we don't have counterexamples in higher dimensions that are basically imposed to give us access to certain tricks. us access to certain tricks. So the question is, can I drop these in general for either of these results? Okay, so this was all talking about one distributional invariant of metric measure space. There's another more refined invariant that we like to think about, which is called the local distance distribution or sometimes called the volume growth function of a metric measure space. So, this is a function I'll call little hx. So, this eats a point from my space and a real number, spits out a real number. And what does it do? It computes the volume of the closed metric ball centered at the point in my space that I plugged in with radius, the number that I plugged in. Okay, so you know, in particular, the global distribution is, I can think of it as integral. I can think of as integrating out this local distribution. Okay, so this obviously gives me a more refined invariant. It depends on local data. The issue is like, since the domain of h of x, h sub x, little h sub x depends on x itself, how would I compare? On X itself, how would I compare this across spaces? Okay, unlike the global distance distribution, where it's obvious how to compare, here it's not so obvious. A way you can do that is by this thing I call L sub little h. Okay, so given two metric measure spaces, I can compute a number associated to those two, given by this formula. So maybe just ignore this thing in parentheses for a second. So, what I'm doing here is I'm What I'm doing here is I'm doing an integral over with respect to some coupling of mu x and mu y. Okay, and then I'm infimizing over couplings. So ignoring this black box, this is like an optimal transport problem. Okay, you can generalize optimal transport to say I'm allowed to kind of put in any cost function that I want, and I can still, it doesn't have to be coming from a metric on a ground space. So all we're doing here is running an optimal transport problem with this particular. Transport problem with this particular cost. Okay, so the cost associated to a point x in x and a point y in y is this number where I look at the volume growth centered at x, the volume growth centered at y, I compare those two, and then I integrate out over radii. Okay, so this is comparing the volume growth across all radii between these two points, right? Plug that into the optimal transport problem, and I get a number that I can compute. Get a number that I can compute. And one thing to mention: these are lower bounds for normal-Bosterstein distance. They're supposed to be computable. Optimal transport is a thing that you can compute. Because it only depends on pi one time, it becomes a linear problem. So it's like a linear problem over a convex polytope computationally. Okay, so then this also is a lower bound for Gromobosterstein. Once again, we can do versions for different P, but they're harder to write down. They're harder to write down. But the lower bounding looks like this. So in Fucundo's old paper, he knew this lower bound and he knew this lower bound. In our recent work, we show that, yeah, you can actually squeeze that more refined looking lower bound between those two. Okay, so the setting in which we'll look at the volume growth, lower bound, and how. And you know, how does this distinguish spaces will be in the setting of metric graphs? So, a metric graph is basically take a graph in the sense of graph theory, but then say all of the edges are continuous intervals. Okay, so it's like a one-dimensional simplicial complex, you'd say. And the edges have different lengths. And I'm going to use a distance metric on this metric graph, which is take the shortest path between two points. And there's my metric, and let's say, let's. And there's my metric, and let's say things are compact, and I can put a uniform measure on this. Okay, so this setting is interesting for this volume growth invariant because the volume growth is sort of boring at most points. It's like this linear growth because locally this looks like a line, except at vertices where it depends on degree. Okay. And as R grows, then you start to see interesting stuff happen. So like loops are closed. Interesting stuff happen. So, like loops start closing up as the radius grows. So, we like this setting because it's really kind of the volume growth somehow really sees the topology of the metric graph. There's not really any geometry. So, this lower bound, so remember this is the thing I can use to compare different spaces, does not distinguish even metric trees, so contractible metric graphs. So, here's a specific example. These have, if this is These have, if this is X and this is Y, the lower bound between these is zero. Basically, so I'm thinking of every edge as being unit length, and then putting geodesic distance on this. The volume growth gets confused. It can't distinguish kind of once you get past, I think, like that radius, like step two, it can't distinguish this type of cluster from any other type of cluster with 20 leads in it. So, the result we can prove is like a local type of improve is like a local type of result. So this says that for any compact metric tree X, I can find an epsilon, which depends on X, such that if I take any other metric tree Y, which is epsilon close to X, okay, and the lower ground vanishes, then X and Y must be isomorphic. Okay, so if I take my metric tree I care about, I perturb it a little bit, if that perturbation doesn't change, Doesn't change this lower bound. If the lower bound distance is zero to the perturbation, then the perturbation is still isomorphic to the tree that I started with. And just to mention, so this uses some tools actually from a way different field from popological data analysis. So these results have like a much different character than the more geometric Manfoldy results. Okay, so let me wrap up here. Wrap up here. I'll end with a conjecture. So we saw already that metric trees are not distinguished even by the more refined volume growth type of invariant. So since this lower bound is zero, that implies that the distance distribution lower bound is zero, right? Because I have this inequality. So that tells me that distance distributions cannot distinguish metric trees. And if I only care about distance distributions, I can use a much simpler example. Use a much simpler example. You can do this one by hand and see that these have the same distance distribution but are not isometric graphs. However, any counterexample we can find has some topological rigidity, which is to say, okay, these have the same distribution of distances. They're not isometric. They are homotopy equivalent. They're both contracted. And any example we can find has that property. So we'll make this a conjecture. If I have metric graphs, now these are not necessarily. Graphs, now these are not necessarily trees anymore, any topology metric graphs. If they have the same distribution of distances, then they're homotopy equivalent, which is to say, in this metric graph setting, the isodistributional tells me something about topology. Okay, so it's like a geometry topology type corresponds. Just to say a word about why we think this is true, we have a couple of attempts at proofs that never pan. Of attempts at proofs that never panned out. One thing is to take your metric graph and think about trying to thicken it up to like a surface, take a tubular neighborhood, say. Okay, but we have results about isodistributional surfaces. Okay, we know that in particular they're diffeomorphic, which would imply homotopy equivalence. The issue is that doing this tubular neighborhood move, it's hard to It's hard to see what that does to the distance distribution. It changes it by a very small amount if you take the two very small, but more than that, it's hard to say. There's another regularity type argument, which computationally we've seen that if I compute the distance distribution for a metric graph, take its derivative, then I get something like this. And we've seen that it's continuous. seen that it's continuous, the derivative is continuous except for jump discontinuities, which correspond exactly to the radii of variants of the loops appearing in the graph. Okay, so these jumps correspond to the small loop, this big loop, and this total loop in that order. So it seems that if I have two metric graphs with the same distribution of distances, I should be able to read off the number of loops of the two metric graphs and deduce that they're homotopy revolved. That they're amotopia forms. Okay, let me end there. So, all the distance distribution stuff is in this paper with Kundo. Application stuff that I didn't talk about too much are in these papers with Samir Chafford. All right, thanks a lot. Are there any questions for Tom? Go ahead. I have a question, Tom. This is me. Let's talk. Can you hear me all right? Yes, thank you. Okay, so I'm very. Yes, thank you. Okay, so I'm very so maybe I actually have maybe the stuff you presented again addresses my question. I just want to make sure. So when you compare the distance between two objects that are very, very complex, so you got maybe several points, you're trying to measure points and so forth, the phenomenon that happens is that small errors in each of the points can end up being more important than one big error here and there. So that a variation of a cat. A variation of a cat would be further away than the cat and the rabbit, right? Because you're a high dimension, sort of like that sample limit theorem thing that happens in high dimension. So is there a way to modify your distance, you know, sort of your optimal transport to weigh certain things more than others and sort of take care of that issue? Okay, yeah, that's a fantastic question. I don't know the answer, although, okay, so there has been recent work in optimal transport to Of work and optimal transport to try to overcome these robustness issues. And yeah, it's a great question to see if this would go through to a Gromma Vosserstein setting where I don't have an ambient space. I don't know that I can say anything more intelligent than that, that it seems like a great idea, and I don't know how to do it immediately. Thank you. This is Peter. Yeah, very nice call. You didn't mention any. You didn't mention anything about detecting dimensions with these. So, suppose you have a MM space with components of varying dimensions. Are you able to detect that at all? Okay. Yes. Indeed, I did not mention this. So, yeah, I think this is a question that we never looked into in any detail. We've always been saying, okay, assume that you have two manifolds of the same dimension. What can you say? Manifold of the same dimension, what can you say? I mean, it seems to me that at least it's a manifold, the dimension should be immediate somewhere in some of the distributions. But then if you have like sphere with some spikes on it or something like that, then it gets much more interesting. Yeah, no, that's another great question that I don't really know to answer right away. But I wonder, I'd have to look at all these Taylor expansions. Look at all these Taylor expansions, it seems like volume growth, the Taylor expansion of volume growth should somehow see dimension if you're restricting the manifold or something. Dan Brinkman did some computations comparing two-dimensional, one-dimensional. They look very different. And I forget if we made any progress on that. Yeah, okay. What would be interesting is when you have components of different dimensions, how they might pick that up. Yeah, totally, totally. Yeah, I mean, even I wonder if, yeah, yeah, I'd have to look at the trailer expansions and stuff. But it seems like, yeah, volume growth should be able to see this even like really locally. That's my gut feeling. But yeah, we never look at that in any detail. That's a good question. Or it's probably a couple left, I think. Quick, I'd like to ask you. So, can you go back to the torus? What was your statement about Taurus? About the flat Taurus? Yes. Yeah, so if I flat Tauri with the same distribution of distances, that happens if and only if they have the same lattice structure. That's the same as saying, well, that's it as a mix of picture. No, no. Those two Tori have the same. Two tori have the same lattice, but are not isometric. No, torite is quotient of plane by lattice. So they isometric, because they have the same lattice. What do you see? You see just some different generators of Tori, but Tori fires are metric. I think okay. I think that these are no, the same door. You just go around in a different generator, of course, right? Change latest in homology. That's always included here. Okay, well, so I had myself convinced at some point that these were not isometric because that was what I thought at first, too. Thought at first two, but um, not gonna try to push back on that at the moment because, um, yeah, yeah, okay. So if that's the case, then constant curvature zero and same distance distribution happens if and only if isometric. Yeah, the same statements as for sphere. Same thing as for a sphere. For a sphere. Okay, so, and I don't know about the negative curvature setup. So I wonder if that holds. Because that would be tight if there was just a really good one. Oh, yeah, it wasn't Google. Yeah. Well, they don't have to be flat. Well, hold on, Google, they keep going down as possible. As a result, you can move bounce around. Excellent. So let's thank Tom one last time. Thank you very much, Tom. Yes, our next speaker will be Rob. Just going into if you used to be a part of the side. Sorry. Just a video. You really need this here or there. Let's consider you put it there. I'll come in first.