Let's start. This is joint work with Jim Bing. He's at the University of Toronto. And it's based on two works, one in Biometrica and one in the annals, and just appeared. So this is the outline. I'll give an introduction, give some minimax lower bounds, the methodology, rates of convergence, and say something about interpolation. Say something about interpolation. And so let's see. So, what's our model? So, we have our data consists of pairs x and y. X are feature vectors, high-dimensional, and y are labels taking the value 0 or 1. Typically, our p, the dimension of the feature vector, is much larger than the sample size. So, we have to make some assumptions. So, you can do some. To make some assumptions, so you can do sparsity or maybe some lower-dimensional structure, and that's the approach that we're going to take. So, we're going to assume that we have, in fact, a factor model. So, x can be explained mostly by a low-dimensional factor, the z. And there is a stochastic part, the w. And so the x is observed, the rest is not observed. The Z are the unobserved latent factors. And again, the W is the noise, and A is a loading matrix in P by K. And so the case that we're interested in is that K, the latent dimension, is less than the sample size N, and that is less than P, the dimension of the feature vector X. Now the label. Now the label depends on Z, but not on W. So W is independent of both Z and Y. Y is the label. For simplicity, we assume that Z have mean zero. It's not necessary, but just makes the presentation easier. And for sure, we assume that the noise has mean zero. A has full rank. And we're going to assume, so these are. So, these are three assumptions that are kind of general. And now we are going to make a very specific assumption to get into the LDA framework. We are going to assume that the latent factors are normal. They have different means depending on the label. And they may have, but they have the same covariance matrix, which you denote by sigma z given y. So for both groups, it's the same. It's the same. Now, for technical reasons, we're going to assume that the W's are sub-Gaussian and with sub-Gaussian constant gamma. And for some simplicity, we assume that the priors for the two labels, 0 or 1, are bound away from 0 and 1. So the minimum between the two probabilities, pi0 and pi 1, those two priors, is bound away from pi constancy. By constant C. So those are the main assumptions. And so if we're going to look at the base risks for if I use classifiers on the space of X, the in R K where X is AZ plus plus W, and I compare it with. And I compare it with a classification in the space of disease, which are the unobserved, but in RK, you can see that the base risk in the RK is much larger, or is larger than the base risk in RP is larger than the base risk in RK. So what I call in the X space and the Z space. So that definitely has a benefit to reduce yourself to the Z space here. yourself to the Z space here. Of course, we know exactly how the basis looks like in the Z space because of the normality of this formula. And so it depends on the Mahalog distance between the two conditional means of the two populations. The means are denoted again by alpha naught and alpha one. And so And so, if delta squared goes to infinity, which is typically assumed, then this error goes to zero. And that's in the next slide. So in some sense, the asymptotic base error is trivial. It's zero. You can expect fast rates, exponentially fast, in fact. If delta goes to zero, If delta goes to zero and the two priors are different, then the asymptotic basis row is going to be trivial, independent of the feature, in fact. And so you can again expect fast rates. And if the delta goes to zero, the two populations are going to be the same, and pi naught and pi one are half, then it's just a random flip coin essentially. And so it's a hard case, and you can expect slow rates. So what's missing in this picture is So, what's missing in this picture is easy under the conclusion, is the case when the delta actually stays constant. And so, I think that's the more interesting case. And so, then your base risk in the Z space, A, does not go to zero. So, if I look at the conditional covariance of X given the label, you have a part here. part here that is the covariance of A Z given Y, that's sort of A sigma Z Y A transpose, plus the unconditional, because the W is independent of the label, the unconditional covariance matrix of the W's. And so you want to view this first part as a signal, and then the second part, the sigma w, as the noise. And so the signal-to-noise ratio is the large. Ratio is the large, sorry, the smallest non-zero eigenvector of the signal divided by the largest eigenvalue of the noise. And so that essentially should be large in order to get that the gap between the two base errors is getting smaller. So before we move on, let's first see what we can expect for minimax optimal rates. And so we are going to look at the following excess risk. I construct a classifier based on my data, which consists of X's and Y's, right? Right, and so it goes construct a classifier g hat that goes from RP, takes an x and spits out a label. And if it's misclassified, if g hat of x is not equal to the label y. And I compare it to the best possible thing I can do in the z space. So it's a harder benchmark because effectively, since I only observe the axis, I should in fact only look at what happens. Should in fact only look at what happens in the X space, but I'm really interested in to simplify the problem. And simplification is that the Z really, the label depends on the Z, but not on the W. So I believe that this is a good lower bound. So I'm comparing it with RZ star, the base risk in the Z space. But it's a bit ambitious, of course. But it's a bit ambitious, of course. And so the parameter space we make it a little bit simpler so that our parameter consists of the matrix A, the conditional covariance matrix of Z given Y, the covariance matrix of the W, and alpha, which is the mean. I have an alpha and a minus alpha, the commission means. And I'm going to assume for simplicity, to make the calculations a little bit easier. To make the calculations a little bit easier, that pi naught is pi one is equal to a half. I'm going to assume that my sigma w is a nice matrix, essentially sigma squared times identity matrix, something like that. That's what I have in mind. The largest sigma value, sorry, the largest eigenvalue is about the same as the largest, the smallest eigenvalue of the sigma w, which is kind of sigma squared. And the same thing we do for the eigenvalues of the noise matrix, the A sigma A transpose matrix. A transpose matrix, and we're going to say that they're all comparable to a value lambda. Okay, so the signal to noise would be lambda over sigma squared here. And this is going to be our rate. I'm going to show this again later. So, but you have the dimension k over n, that's kind of the intrinsic dimension. And then you see the signal-to-noise ratio times Mahalo-Lier's distance between the two means. It's distance between the two means plus something similar multiplied by some factor. Okay, and so this is more or less what we get. We get for the three different scenarios, the delta-Maumerly distance between the two means, the conditional means, is constant. Then my excess rate is essentially of order omega squared. And it goes, if delta goes to infinity, then my rate is going to be faster. I multiply the rate is going to be faster. I multiply the whole thing by e to the minus one over a delta squared. And if delta goes to zero, I get a slower rate. I just want to focus actually on case one. So my excess risk is essentially omega squared. That's my minimax rate. The assumption is though that k over p is bounded by a constant and so k over n. And so k over n should go to zero, sigma squared over lambda should go to zero, so the sigma to noise ratio, the one over that, the inverse of that, go to zero. And I want to have that bounded is sigma squared times p over lambda times n. Y, that is actually this term right here in front, in the third term. That's that term. So essentially I can combine the two terms in the end. Terms in the end. So let's look at this particular rate. I have k over n. So I have a k-dimensional problem essentially. So k over n would be, if I were to observe disease, that's what I would do. So that would the rate that I expect indeed. The other one is the signal-to-noise ratio, sigma squared over lambda. And that is essentially, I compare it with the best thing I can do in the C. The best thing I can do in the Z space and not in the X space. So that's the price I have to pay there. And a similar price, or a little bit more, is I have to pay that here as well for the third term. And that's, I have, I don't observe the disease. I have to actually have to expect, I have to estimate, I'm sorry, the column space of A, right? The range of A, the matrix A. And so that is another error, and that's the third error. And what we can show is that this is. We can show is that this indeed is the minimax lower bound. We can get an upper bound up to a log term. We'll see that later on. So, this is the minimax rate. And so, this is our goal essentially. But we see that we are going after a parametric rate, K over N, plus a rate that has to do with the signal-to-noise ratio. So it kind of makes sense, I guess. Okay. Okay, so the methodology. So let's suppose that we observe disease, which is not the case, but let's suppose. And so then we know exactly what we need to do because we have essentially observations zi, yi. And I know that my optimal rule is essentially a hyperplane. I know the direction. I have the formula for that. And I know the intercept. And I know the intercept. I have another formula for that. And this is going to be the base error that has this classifier using this hyperplane as the smallest possible misclassification error. So that's what I'm trying to estimate in some sense. So there's quite a bit of work on LDA, of course, and also in the high-dimensional setting. Also, in the high-dimensional setting, but they try to exploit the sparsity of this direction. And so it's probably easy to argue that the difference between the two means is kind of sparse, or they're kind of sitting on a lower-dimensional space, fine. But I multiply that difference by a vector, right? Sorry, by a matrix. Sorry, by a matrix. And so the sparsity gets preserved as long as I multiply by a diagonal matrix. But I'm in high dimensions, and so there's no guarantee at all. In fact, it typically will have some correlations that that would be a diagonal matrix, the sigma inverse of x given y. And so we have the hope that assuming We have the hope that assuming this low-dimensional structure would actually give us something. And so the way we try to argue is we are going to here in the proposition, you see the optimal hyperplane. And we can equivalently write that hyperplane as Z transpose beta. As Z transpose beta plus a beta naught, where the beta is essentially the regression coefficients of the label y onto the z. And so that is the beta and the beta naught as a different formula. Looks a little bit more complicated, but the main message here is that essentially you have to do a regression. That essentially have to do a regression of z onto y. Sorry, y onto z, I'm sorry. And so notice that we use the unconditional covariance matrix of z, right? So we're really onto z, not on z given y. And so if I were to observe the z's, I would do just estimate the beta by any squares. That's clear. Of course, we don't observe it. Of it and we have in matrix formula, so we have n observations. I have x1 to xn, so I get a n by p matrix. That's my bold phase x. And so I can write it then as a Z matrix, an n by K matrix, and then I'm a K by P, a matrix A transpose, plus a W, an N by P matrix. And so if I were to multiply Multiply, take the product xb for this. If I were to know a, I would take this particular choice, and I would get z plus plus some error. And so then I would essentially have a z and would try to do these squares using the xp. And that's exactly what we're trying to do. We're going to estimate the inner product. We want to estimate Z transpose beta by... estimate Z transpose beta by a Z hat transpose, which is X transpose B, times my beta. I'm going to estimate it by X B, the generalized inverse, times Y. And so I'm doing a sense V PCR here. For the beta naught, I'm going to estimate. I just look at the formula and I just use plug-in estimates. I use plug-in estimates for mu naught and mu1. New one and the usual non-parametric plug-in estimates. And for the priors, I use the usual plug-in estimates. The number of observations that have the label one over n the sample size and the number of observations that have label zero over the sample size, and you get my p at one and the p at zero. That's fairly easy to do. And so our proposed estimator there. So, our proposed estimator, therefore, is going to be x transpose theta hat plus beta hat zero. That's our hyperplane, and if that's positive, I'm saying one, and otherwise I'm saying zero. And so the choice is what are we going to do for B? And for B, we're going to take the first R right singular vectors of X. Now, I can take X. Now, I can take X, but for technical reasons, we're going to need an auxiliary n by P matrix which is independent of the training data. And if otherwise I need to split the data. And the question is, of course, why don't I use X? Most of the time I can use X simply, but sometimes I cannot achieve the minimax rate and I need an independent source. Independent source which is kind of interested in this problem. Okay, so we need to estimate the number of singular values that we take and we use earlier work. This is a very simple recipe that typically works as long as the true value for K is not too large. There's this restriction. There's never a free lunch. There's this restriction, there's never a free launch, but otherwise it works wonderfully. You get with exponentially fast rates, you get the correct K. And so I compute the singular values of this matrix X tilde. And then we can see how we do. Let me go to the next slide. Um, let me go to the next slide. I'm running a little bit behind time. So, there are a couple of known, very well-known data sets that are quite challenging. You can see that the sample size is actually small and definitely very small relative to the dimension P. And so, this is our method here, and it does better than other methods that are good methods, by the way. Good methods, by the way, but they are typically based on the sparsity. And so it looks like that there is not just sparsity in this data, but there is also a low-dimensional component, because we can get the errors down quite a bit, the misclassification errors here. So it seems to be a method that seems to work, at least in practice. So what about the rates of convergence? About the rates of convergence? Well, we have this hyperplane, we have the optimal hyperplane that we denote by a g of z, and we have our estimated hyperplane that is essentially a z hat beta hat plus beta hat zero, which is an x transpose theta hat plus beta hat zero. And based on those hyperplanes, that we can construct our estimate, classifiers g hat and g star, of course. And so, what we would like to do is the misclassification error of our classifier. We compare it with the base risk in RK. And that depends on two things. So this is kind of an interesting result. It depends on how well I estimate my hyperplane, my optimal hyperplane, that's the first term here, plus the intrinsic difficulty of the problem, which is essentially the behavior. Problem, which is essentially the behavior of the optimal hyperplane near its boundary, near near zero. And of course, that's the intrinsic part, and we don't have any control over that. It's just this half space of Z, and it's kind of linear combinations of normals. So that's going to be a normal calculation. And we can actually calculate P T, can use a mean value theorem. You get the next slide. We can compute the second term there. The second term there. And so that we can, we don't have control over that, that's just given by the problems, and so we assume that we had Gaussianity, obviously, given the labels. Okay. The other one is more interesting, but if we work out this difference, the estimated half space minus the optimal half space, and it's evaluated at a new point, right? So a new x, which is a new. So, new X, which is a new AZ plus W. And so, what you see on the right-hand side, you see two colors, right? And they're independent. The blue and the red are independent. And so, the Z's and the W's are Gaussian, sub-Gaussian. And so, what you see in blue is going to be, well, the norm of it at least, is going to be the standard error. And so, what you need to control is essentially those two key quantities. Those two key quantities, and okay, you need to look a little bit better in the beta at zero minus beta naught. I leave that out for now, but so that is going to be the main calculation that you have to do. And the second part is the hardest in some sense. And just for simplification of the presentation, let's assume that we are in the same parameter setting as our previous case where. Where I have the simplification for the parameter setting, and I assume that I have that the Malovich distance is of order one and my matrix is well behaved, then we have the following. And here you can see the advantage of using an independent sample or not. The first one, I simply use. Sample or not. The first one I simply use the whole data set, the old data set, and here I use a new data set in order to compute the B. And you see that you don't have that third term there. So you have here K over M plus sigma squared over lambda. That's one over the signal-to-noise ratio. And here you get something extra. And so what you can see is that P is less than N, the two rates are coincide. If P is larger than N, N you. If p is larger than n and your sigma-to-noise ratio is large enough, the two rates are the same. But the more interesting case, I guess, p larger than n and lambda over sigma squared is relatively, is large enough, but relatively small, then you you need to use an any. Then you need to use an independent data set. We verify that in simulations, and that was indeed the case. I don't have that simulation. Oh, yeah, here's the simulation. And so if I use my old data set, the rate is not good, as you can see. Whereas if I use An independent data set and compute the B based on the singular vectors of that matrix, it's going to be much better and it's going to be close to the base risk. How much time do I have left? I can't see any messages. See any message yet? So let me go to the interpolation if I have still some time. So what happens if B is equal to the identity? So I simply my theta hat is just a generalized least squares. That's what I'm doing. The advantage would be I don't have to estimate k and it seems to work in the regression context and also in Binance. The regression context and also in binary classification, there were good results using overfitting. And so what we will see is that if we use the same classifier, but now based on the generalized least squares for theta hat, we get typically zero training error, not always, in fact, but it's going to be inconsistent. And again, we need an independent whole art sample. Why? Because the beta hat zero is going to be inconsistent. It's going to be always minus a half, and that's a problem. The beta hat, the p-dimensional part, that's actually not a problem. That is going fine. And so we have to modify our estimator for beta hat a little bit and use an independent holdout sample to make it work. To make it work. And if we encode the labels differently, say plus or minus one, interpolation is always there. If we do zero, one, it's sometimes there, sometimes not. I'm trying to get the new, I have four minutes left. Okay. Let's see. So let me go to the corollary. If P is larger than N is larger than. If p is larger than n is larger than k, and the sigma w is sort of is an okay matrix in the sense that the trace is p and the largest eigenvalue, say, bounded, then we have that we get interpolation of the data. And if we interpolate the data, what can we take for, so that's for the part X transpose theta hat, what can we take for? Transpose theta hat, what can we take for beta zero? Any value between minus one and zero? You can show that. What if we use the true beta naught? If we were the true beta naught, it depends, as you can see, I'm running out of time, I can see that too. If the true beta naught should be negative, and it's not going to be negative if beta naught is. It's not going to be negative if beta naught is the smaller one, if beta naught is less than a half, and so then you don't necessarily interpolate. So that sounds strange. What's even stranger is if you encode it differently, then they always interpolate. And finally, the beta at zero is going to be minus a half, which is smack in the middle of that interval minus one, zero. So you do get interpolation, but you get also, you don't estimate anything, you always get minus a half, you get a trivial calculation. And so I don't get a consistent estimator. And indeed, the classifier is completely inconsistent. And so my beef in the current literature is typically people say without loss of generality, we assume that pi naughts pi one is equal to a half. But in that case, beta, the intercept is always. The intercept is always zero. And so you don't have to estimate it. And so it's only the direction, but the direction is always fine. It's actually the problem arises at the one-dimensional intercept. And it's simply because you want to disentangle the dependence between the conditional means and the theta hats. If you don't do that, then you get here, because of the interpolation, a trivial calculation. A trivial calculation and this whole thing reduces to minus a half. This reduces to one. The first term times minus a half, and the other term reduces to zero. But you can easily correct that by estimating the conditional means by an independent sample. And then everything is going to be fine. And then you get essentially this rate here. So, if you use that estimator for the intercept, you get this particular type of rate, which is kind of the rate that you typically have. So, if the signal-to-noise ratio is large enough, you get consistency. If it's even larger, then you get essentially a rate k over n plus n over p. And you see the advantage of having a large dimension p, essentially get repeated observations in some sense. And if p is even larger than n, And if p is even larger than n times n over k, then you get the minimax optimal rate. Then the rate on the right is essentially k over n up to log factors. Okay, I think I need to stop here. Thank you so much.