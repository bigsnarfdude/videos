First I want to thank a lot the organizers for inviting me to attend this beautiful workshop, interesting workshop. I am also honored to be delivering the last traditional talk of the workshop. And I will also tell you that this paper has a long story, you will see it. Long story, you will see. Started like 12 years ago, and I'm very happy to present it because this is my first paper with my wife. Probably the last, because she's not in academia. So maybe I should have put that in a picture instead. So what's the purpose of k-dimensional trees is to store Multidimensional trees is to store multidimensional data. In order to be able to efficiently answer so-called associative queries like, okay, so what's the closest, all points, relevant points centered around this one that I'm giving you, in a radius of, say, 10 kilometers. Which is the closest point to this one. Your data set, or what are the points that lie inside this query rectangle? This is the type of queries that we want to be able to answer efficiently for our collection of multi-dimensional data. Now, there are many different data structures that give an answer to this type of problems, and here in this talk, I'm going to talk about And here in this talk I'm going to talk about two variants, the median K V3s and the hybrid median Kd3s. And I will concentrate in the analysis of two measures, the expected internal path length and the expected cost of partial match queries. Internal path length is a measure of the cost of performing exact searches in the tree and the cost of building. In the tree and the cost of building the tree. And partial match queries is the most basic type of associative query and it's a building block to understand how other associative queries work. From now on, I assume that the trees are built from endpoints where each point has k coordinates. K will be the dimension of my space, and each coordinate is drawn in the Coordinate is drawn independently and uniformly from zero. That's a bit strong assumption, but it can be somewhat softened by assuming that you are drawing from some continuous, where points are independent, but they are drawn from some continuous distribution in 0, 1 to the k. Now, one of the first data structures that was proposed to Data structures that were proposed to cope with this type of problem are the so-called standard KV trees, introduced by John Mende in 75. And essentially a KV tree is a binary search tree, but on each node you will be using a different coordinate to decide who goes to the left and who goes to the right. I'm assuming more or less after this week all of you have an idea. All of you have an idea what a binary search tree is. So, here imagine we insert this point with those coordinates, and we will be using x as the discriminant. So, the next point that comes is 1. Since it has an x coordinate that's smaller than 0.6, it goes to the left. And then the same thing all the time. So, this one has a larger x coordinate, goes to the right. But now, this one, this. Goes to the right. But now this one, this node has to discriminate according to the y-coordinate, not the x-correlate. So when you insert something, if it has an x-correlate less than this and a y-coordinate larger than this, it will go here. And the standard KD3 is characterized by the fact that you are alternating in a cy cyclic fashion the coordinates. So you start x, y, x, y, x, y are across the levels. There are more x, y, z, x, y, z all the time. YZ, X, Y, Z, all the time. So there are many variants of KD trees that just differ by the way you choose the discriminants. For instance, there are something called the relaxed KD trees in which you decide which coordinate to use to discriminate just at random. You throw a number between 1 and k minus 1, you're discriminant at that node. So I'm running. So let me just remind... Let me just remind again that we are focusing on internal path length. It's the cost, it's the sum of all paths from the root to all internal nodes in the tree. And it gives us the measure of the cost of building the tree. If you take the IPL divide by n and then add one, that's the cost of a successful search, following a path from the root to some specific node. And then partial match queries, I'll show you an example. An example definition, but that's the formal definition, but I think it's much better to see an example. Basically, what you have is a few coordinates are specified, you are given specific numbers, and others you don't care. So, the goal is to find points such that they coincide whenever the the the query is specifying something, some value, okay? And I'll use S for the number of specified columns. The number of specified coordinates. So here you have, of course, in two dimensions: a partial match queries and an axis parallel line, either like this or vertical line, and then you want to see what are the points such that they have, I don't care for x and the y is zero dot forty five. In this case, there are no points, but you will be visiting all the nodes that are painted green and all the leaves. Painted green and all the leaves. So you start here, and because you are dividing by x, you don't know. You check that this point is not satisfying the query, and then you might find points that satisfy the query in the left sub-tree and also in the left and right sub-tree. So you go recursively to the left and to the right. Now, when you come to this one, this point is dividing. Is dividing the space vertically so, sorry, again you have the same horizontally. So here you compare the y coordinates and this one is larger, so all points that satisfy the query might be in the right sub tree, but no, no, will be in the left sub tree. So here you go only to the right. So, here you go only to the right. And so also. Now, a few known facts about the expected IPL and the expected cost of partial matches. There's much more known, but just for the sake of this talk, I will use this very summarized table. The coefficient that multiplies n log n in the expected cost of the expected IP. The expected IPL is two for a variety of KD trees, of different types of KD trees, and no matter what the dimension is. For partial match queries, there's an exponent that will depend on the s and the k. It's a function of and typically it's a function of the ratio. And here you see I've taken the same ratio, one half here and one half here, but Half here and one half here, but here the k is going to infinity, and you see those are the numbers. For the exponent, in some cases, like for standard and relaxed KDTs, we also know the constant factor in the main order term and the lower order terms, etc. But I will just concentrate in the exponent. So, in linear K3s, So, in media KD3s, what we have is what we do is the following. When you insert a point, it's essentially falling in one region where some rectangle, k-dimensional rectangle, that was empty. So basically, what you will do is take as discriminant the coordinate that's closest to one-half after renormalization. You renormalize. You renormalize, you take this rectangle, hyperrectangle as the q0, 1 to the k, and you say, okay, my coordinate of the point I'm inserting is the, this is the coordinate that's closest one half, that's the coordinate that I'll use for discriminating. In the Irish median KD3s, what you do is essentially the same, but in the first level, you use the median. You have you use the median rule. The next level, you again use the median rule, but you are only allowed to use coordinates that you haven't used before until you have exhausted all them. So, basically, it's a combination of median KD trees and standard KD trees. If you follow some path in an ibridden median KD3, you will see a permutation of this primary and again a permutation. Criminal, and again a permutation of these criminals, and so on. But the permutation might be different after a sequence of permutations, and each one of those permutations that you see of these criminals will differ, whereas in a standard KD is always, let's say, 0, 1, 2, 3, 0, 1, 2, 3, and so on. Here you will have the four numbers, but in different permutations. So, this is sorry. Do you know if it's study to use this version where you're not allowed to reuse coordinates also for standard relaxed PAD trees? Yeah, random, but not the ones you already have. Yeah, this has also been studied. But interesting. That's a formal definition for the rule that will give you the discriminant to use in median kiddies. K-DTs. And it was my Merce Pons who introduced this type of K-dimensional dis in her master physics 12 years ago. And she did, she also introduced the Ibrid-median KD3s and the Ibrid-relaxed KD3s and many other variants. She did a preliminary analysis and those noise results were never never ever published. Never ever published. So, some years later, we took this again and we managed to produce more interesting results. And we said, oh, this deserves to be a try. We have to give a try and try to publish it somewhere. And we have succeeded. So that's an example of a medium pH. I don't think it's worth going. I don't think it's worth going through the details. The idea is that when I put a new point that corresponds to some leaf, like this one, is this region, I re-normalize and I pick the coordinate that's kind of closest to the center, right? Same thing for medium kHs, except that you have to risk. Except that you have to respect this idea of not reusing the same. You see, here you have, for instance, I start with this discriminant, and again I have it in the next level, and again I have it in the next level, and this is not very good. So with ibrid median KDTs, you try to correct this by providing some alternation. That's an example of the ibrid median KDT. You have X, then next. 3, you have x, then next level must be y. But then here you are free to choose x or y at each note in this level, but in the next one, we have to switch to the other one. That's because we are using two coordinates, two dimensions, sorry. Now that's the first theorem. Given the coefficient that multiplies n log n, complicated expression, but for n. Expression, that's for any k I can you plug in the numbers and you can compute this number. Okay? No, not important. And another result is the expected IPL for random every median KD3s. And it turns out that this coefficient is a harmonic mean of the coefficients for median KD3s for the different Dimensions. That's a nice, easy formula. And then you can derive a few useful facts, like for instance, those two coefficients are less than 2 for any k are greater than equal to 2. They are decreasing with dimension. And in the limit, both have the same limit. Me is 1 over long 2, which is optimal. Because remember, the IPL is n long of n. You divide by this constant, you get n log base 2 of n. So, if I would, I click on the on your app. Click on the presentation in your Mac. This is a plot of the values of the C k's. They approach C k at this limit. In the case of median, faster, much faster. Not with every median, but they do. Then we have also a theorem given us the alpha for median k d 3. For median kb3s, and this alpha is the solution of this complicated equation involving the incomplete beta functions. Again, for any k and any s, you can plug in those two values and solve the numerically this equation, no plus four for alpha. And then you can get the the value of alpha to any desired uh degree of accuracy. Degree of accuracy. And one interesting thing is that you can also see what happens when k goes to infinity, and then you can prove that in the limit, when k goes to infinity, this alpha is tending to this limit, where rho is the ratio between s and k. This is assuming that the ratio s divided k and k is going to infinity, this ratio is fixed, then we tend to this limit. We tend to this limit. Here's a kind of a plot depicting this. This curve is for standard k trees. This one is giving us the alpha. Not really the alpha, it's something called the x's. You subtract something to see the principal variation with the x. X is actually the ratio S divided K. And this is the standard, this is relaxed, this is the limit, and those are the curves for different. And those are the curves for different kinds of median. Because in the case of median K3, the alpha, the exponent, is not only a function of the ratio, but also a function of K. So for K equals 6, we are closer to the living, but for K equals 2. What happens with partial matches in even median k3s is that they, what you have to say. What you have to set up is a you have a system of recurrences. You have to set up such a system and you define a matrix and eventually this alpha becomes the root of a function which is the determinant of this matrix. The size of this The size of the system is related, is basically k times s, blah blah blah. It's a big system. So basically, you can do what you can do is you can create for any s and any k construct the matrix, find the determinant of the function, and then you get the values. Like here, in parentheses, I'm giving the corresponding values for standard 80s, and you see it's all. Standard KDTs. And you see, all the time the evident, median KDTs have a much better alpha. So here, if I take again the same table I had at the beginning, I plug in the values for the CKs. As you see, they change with K and when they have a limit, when K goes to infinity, and these are the values of alpha. And these are the values of alpha for this very particular case. When you have k going to infinity and s being the half of the coordinates, we have the limit in this case and we have the conjecture that this goes to 0.5, which turns out to be the optimal value for the exponent. Okay, I think I've got some minutes, right, to give a few now is. A few now is my time to advertise a theorem like Sarah did this morning. And the crucial tool for our analysis has been the continuous master theorem and some extension that we've made in order to be able to solve systems of recurrences, which in principle are not covered by the original continuous master theorem. Original continuous master theorem. And here's a bit a glimpse of the continuous master's theorem. The idea is that you don't need to understand basically anything to be able to apply it. You have some divide and conquer recurrence that has this shape, is your function, your cost, or whatever, as a non-recursive cost of your algorithm. Of your algorithm, it's making this many steps. And then you will be making recursive calls with instances of size j. And you have this weight. You can think of this as the, let's say, the average number of recursive calls that given an instance of size n, you will be making with instances of size j. Okay? So this is the kind of recurrence that you get when you are analyzing Quicksilver. It needs many variables. Analyzing quicksort. It needs many variants. The kind of recurrence that you get if you are analyzing quick point, no, quick run, right? Because no j. You have only the end, the size of the array. If you are analyzing the expected cost of quick run, you'll get this kind of reference. Analyzing the cost of binary search trees, you get this kind of reference, and so on. Now, you have to check. Now, you have to check that some conditions are fulfilled. Most important one is this one that's basically saying that's a divide and conquer recurrency. Basically, on average, the size of your inputs when you make recursive calls is a fraction of the original input. Okay, so this second condition is telling you that your alum is dividing. So at least on average, it's dividing the original input. Dividing the original inputs. And the thing that you have to do is to find a so-called shape function, which is a continuous approximation of the discrete weights. You have here the discrete weights in your occurrence. You need to find such a shape function that must fulfill some conditions, technical conditions, but this one is saying you are not making a big error. You substitute your dispel weight by This feed weight by your approximation. There's a very simple trick that works most of the time. If you are able to compute this, you substitute the j here by Zn, and you multiply by N, you take the limit, then that gives you a shape function 99% of the times. And it's very useful. Another theory says, okay, compute this definite integral, H. integral h involving the shape function and the exponent in this non-recursive cost. If h starts to be positive, that's the answer, that's the solution to fn. It's the tor function divided by h plus lower order terms. If h turns to be zero, then you compute another number, h prime, again with the shape function. Again, with the shape function, a definite integral, so something you can easily ask any computer algebra systems to do for the system to do for you. And then the answer is Tn long of n divided H prime, if H prime is not zero. If it's zero, there's also a version where you have a log log n term, but forget about that. And if H is less than zero, then it's less than zero. Then it's negative, then you get theta of n to the alpha. No information about the constant in front of this, or constant or fluctating function or whatever. You only know that's theta of n to the alpha. And the alpha is the solution of this equation. That's the case that you get when you analyze this partial match. And that's the case you get when you are analyzing the IPA. Here I'm giving Here I'm giving a bit just that's the reference for IPL. Look at these weights. Now I'm using pi of nj. I don't know why, not omega, but this is the way they look. Very complicated. How do I solve this reference? Continuous master theorem gives you this shape function and you can just do this and you get the answer. That's the nice thing about Nice thing about the continuous master theorem. Same thing for, for instance, every median KB3 here you'll have a system of recurrences where it's like I have to solve the problem. Now I only have i coordinates that I can use to discliminate because I have already used k minus i. And of those I and of those i coordinates there are L that have been that are specified, all the others are not specified, then that I can compute what's the probability that I go to I have one less coordinate I can use and it was a coordinate that was specified. Then I can compute what's the probability that happens. And with this complementary probability, what I have is that the coordinate is not one of the ones, it is not one of the ones. Not one of the ones, is not one that discriminates, sorry, they specified, therefore, I have one list, but I have to go to the two such risks. You have a set of linear recurrences, of divide and concurrent recurrences like that. One, and then you can set up matrices that correspond to the call graph. Okay? So, in the end, the elements in the The elements in this matrix are the shape functions corresponding to the weights in your system of recurrences, and you arrive to the this, you can find the determinant, so on and so forth, you get the alpha. Now, so the idea, those two variants are simple to implement, no lot of memory is needed. No lot of memory is needed. Anyway, one of the conclusions is that both those two variants are more balanced than other variants of KD3s, because you have this constant is less than 2 for any k larger or equal to 2. And you have also this ordering for the alphas related to the partial match. Our conjecture is that for if it medium k d 3 k. As K grows, we are approaching this limit. And this one is actually achieved by Squadish KD3s that were a few years ago by Luke and Cooper. Twenty years ago. 20. 20 years ago. I know. We are so old. So, okay. I I insist. The CMT is a very is a wonderful tool. As I say, it should be in our OWMS textbooks, but it's not yet. I hope someday it happens to be. Well, it's only in a German textbook. But much better than anything, right? That's excellent news. Soon there will be a an English translation. Maybe, but I'm not used to read German, so So uh on the last on the previous page there, when you have the alpha inequalities I'm surprised that it that it does worse than the standard median KD tree. I don't want to say something bad about your wife, but it does worse than the standard. That's worse than the standard KD3N. Why is that? Because you... Because you have no alternation. Then you can have bad situations in which you have many, many different points, all many successive levels, dividing in the same direction. And then you get very tiny bonding boxes, which is terrible. Terrible. The reason squarish candidates work so well for partial match is because they produce very squarish bounding boxes. And median candidates, they don't care about this. It seems like what they care is about having half of the points. Basically, it's a heuristic to have half of the points to the left and half of the points to the right. A more balanced. A more balanced tree, but not a more a balanced partition of a space. So the neuroscience version of standard KD trees often is even simpler. You have the data offline and you find the exact media and build the tree from that. I guess the IPL is optimal then. But what happens with Yeah, but what happens with the cost for partial mesh? Is that clear? No. Because it seems like your two variants, the new ones, they're reversed, right? One does better for IPL, but then partial match is worse. Yeah, exactly. Okay, not clear. But I don't know. I don't know. Not clear. Interesting. Next is a software discussion. Our first presenter will be Sean Banner.