I took this picture this morning. Yeah, I'd like to thank the organizers. I've had a great time here. And I'm going to talk about source distribution estimation. I'm going to motivate it with a simple example for a CryoDM and show a bit of related work about how people are sort of dancing around this problem of source distribution estimation, which I think is really what we're trying to do. What we're trying to do in many scientific applications, where we take many measurements to characterize global properties of a system. Then I'll have two vignettes of toy examples and some ideas of how to get this work into practice. So, the source distribution is like the Boltzmann distribution. It's the prior or a fittable prior. Let's consider a simple ion channel where we want to know where this ion is and it's moving up and down through this ion channel. The source distribution would be where it tends to bind, that probability distribution. And we would have many measurements of this at different angles. And some of those would preserve information, like the side view, but some of them would lose information. But some of them would lose information, like the top view. And so, a lot of times the overall global distribution is characterized by samples from the posterior. So, if we care per measurement posterior. So, if we calculate per measurement posteriors of this, we have some sort of approximation of the distribution convolved with the intrinsic noise of each of those single noise measurements. Of each of those single noisy measurements. But this, any distribution, any value in any distribution matches with this type of measurement. So it's a tricky problem. It's related to empirical Bayes. It's been addressed in variational autoencoders applied to some simple problems and then also particle physics problems. And then also particle physics problems and this work on neural empirical base. It's addressed in some of the papers, later papers on manifold VM and kind of general, more general paper on how this plays out in diffusion maps and also in CrowdDragon. It's related to that recent study and also energy paths, which just came out a few days ago. It's very connected. Days ago. It's very connected. So, with Manifolium, we're stitching together data-driven motions from eigen decompositions that come from the diffusion-map embedding of the data. And a distribution on the data space is not going to necessarily match a distribution of the underlying unobservable latent space. And this paper looked at. And this paper looked at how that plays out when that underlying distribution is in coordinate space or voxel space or data space. And maybe a take-home message is that the distribution of the data, so the underlying rotation of this horse might be uniform, but the density in data space isn't necessarily uniform. Isn't necessarily uniform because, depending on the actual structure of the data, the distance you're computing, comparisons, and that can induce these two modes. Here in CrowdDragon, we have a similar case where there's a fixed diagonal in the latent space, and that doesn't necessarily match the data. So the data is flowed through. So the data is flowed through and it doesn't necessarily end up matching a multivariate normal around the origin. And so they use a diffusion map to match that latent space. So the prior samples are in orange, they're this ball, the latent embeddings, when we flow the data through, they end up landing in some interesting shape. And they used a diffusion model to find that map. To find that mapping, so they, after they're trained, they're training, they can just sample as much as they want from this generative model, and it gives things that tend to look like reasonable samples. This recent study from a few days ago built on top of the latent embedding from CrowdDragon and then found and then interpreted that as a That as an energy landscape, and then proposed pathfinding algorithms to find transition states. So we can see here this grand truth path between these states in black with their synthetic data, and then pink was their proposed method of finding that path. And so, if we could do a good job having the latent space be something interpretable. Be something interpretable, be a good estimate of the source distribution, then we could use approaches such as this to find transition states. Okay, so how would we use variational inference to estimate the source distribution? Let's go back to our simple example and look at the elbow objective, which gets optimized in variational inference. So we have the forward model, that's how the local. That's how the global polls, the CTF, the heterogeneity map into the measurement space, not only deterministically but stochastically, including the likelihood, the noise. And then there's, so there's eyes, there's like per-measurement information. And then there's this global distribution. And that's the prior, and that doesn't necessarily. The prior, and that doesn't necessarily have to be fixed. We can fit not only the parameters of the per measurement variation, among variational distribution, but also these parameters when we optimize the L objective. That X psi, that actually had the X I on it. Okay, so here's a simple, one of the vignettes, a simple toy example. We have We have a 1D shifts, alpha i, and then there's a little atom bump that go through a little Gaussian kernel. And then there's some 1D detector, discretized 1D detector, for white noise. This is what the samples look like. A little atom bump on top plus some noise on bottom. This is what the underlying source distribution looks like. The underlying source distribution looks like of all the alpha i's from all the measurements. It has its own parameters, that's what we want to infer, that's what we want to know from collecting all these measurements. We don't care so much about each posterior and each measure. And we can pick a hyperprior over some parameters in that distribution, for instance, this gamma hyperprior, and then do inference of those states. This is an example of the range of measurements. Of the range of measurements, and this is the setup: 20,000 of those measurements. Some initial estimate of the hyperparameter, the hyper prior parameter values. And then a variational posterior that is amortized, it consumes data, it gives a per-measurement latence. This is This is what the samples from the training posterior look like. There's spread around the diagonal because there's some uncertainty. If there's more per-measurement noise, that spread gets bigger. And this is the dynamics of fitting the source distribution. So the actual value on that bell curve of all the alphas is in green, the mean. We start with some hyperprior that is actually misspecified. It's not centered around the value. It's not centered around the value, and it's very broad, and then it tightens up during training to that blue curve. And the same thing happens with the fit of the hyperpower of the variance on the source distribution. It starts to peak around, its mean starts to be close to the ground intrusive delta function. And these are the training dynamics of the elbow objective. Of the elbow objective. So we can see the elbow loss in the top corner. The mean of the mean. And that's the problem we're fitting. So we're fitting these parameters, this and this, and this and this. And then once we're done that, we have estimates on these and expectation. I'm just reading off this. I'm just reading off this, and I think it's like C over R is the analytic mean of that count. And so that's what our in blue estimate of the Boltzmann looks like. In red is the underlying true source distribution. And then probably in a limited infinite data, I would expect them to match. Here's my last vignette, where there's some information lost because of pose. So there's a more complicated problem with information loss because of projection. A problem with information loss because of projection. So we have some double well potential and then some shifting. We have a few reference points. I'll show what the simulator measurements look like. You have two poses, a side pose, a top pose, and then a projection and a little atom bump, and then our 1D detector noise. So this is a we have We have an unrotated bow, so we have three points that get shifted around those double wells, and then sometimes they get they stay there or they get rotated up and projected down. In that case, we don't know where they came from. They just all overlap on each other instead of staying unrotated. So you can So, you can never characterize that permeasurement posterior well. You never know where it came from, all the information has been lost. It's an exaggeration. In practice, the permeasurement posteriors would retain some information, there would be some ambiguity, but even in this extreme case, where the variational posterior is able to characterize the measurements that didn't have information loss. Lost and just go to some arbitrary value, we can still, for the ones where we had lost information, we can still estimate the PO, the source distribution, because there's no bias gradient signal from the information, from the measurements that lost information. They move the parameters up and down, and overall, we don't really learn use them. We don't really learn use them. They don't bias what we're learning. Trending dynamics, they're more fragile, I think because there's not a hyperprior to smooth things out. So these are the four parameters we were estimating. It might just be that they're less stable because the loss landscape is just really flat. And just to finish off, I think this is why we can use an elbow loss to estimate the source distribution. Because the way that some parameterization we've chosen in the model for the source distribution, the way that it gets pushed forward in the likelihood to a data distribution. To a data distribution, although each measurement you might not be able to uniquely characterize the latents, overall the distribution in this, this is a functor, that mapping can be unique. So the important thing is how these distributions map to each other through this whole object. And this is a conjecture, and I think we have a proof that. Proof that Eric Teed and others were flatteringly looked at this on a file. So, overall, extending this from these toy model vignettes to a fuller setup, we would have some chosen parametrized forward model, and we get to choose how we model heterogeneity. We want them to be disentangled from the other latents, we don't want our heterogeneity to have capacity. We don't want our heterogeneity to have capacity in it to also global rotations, for instance, like some people do. We want to be able to identify the source distribution parameters, and then, practically speaking, we want to be able to sample from it quickly, optimize it, and have it give high-quality samples that are PDB ready. And to finish, I'd like to thank Ken for especially for his support and encouragement. I really like working with Ken. I really like working with CAN. A lot of people really like working with CAN. And so I get to work with a lot of intelligent, hard-jerked people. And Sonia, who had a delightful time working with her at the Flat Air Institute this summer. And I look forward to continuing to work and meeting our ambitious conference deadlines. And I have to actually go at 3:30. So if you want to come talk with me during the break, I'd be happy to chat. I'm going to catch a bus from you at 3:30. Is someone some people have good questions? Alright, let's go and require to look at the other brief video left. So the next speaker is Arian Termioriyahi. He's going to talk about optimal transport applied to cryogen.