Thank you for being here at the end of this talk. In this session, I'm going to talk to you about an algorithm that is the result of combining two of my research interests. One is geometric algebra, and the other one is machine learning. The aim of this algorithm is to provide a solution for the A solution for the environmental object mapping problem for robotics. So the robotic mapping problem arises when a robot is provided with a spatial model of the environment. So we are not interested in obtaining maps with real detail, as in computer graphics, but we wanted object. But we wanted object representation rich information. That is to say, we need to know volumes and shapes of the objects in environment. And we want, at the same time, we want to keep this representation with low memory cost. These are some robotic mapping approaches that are. Approaches that are some kind similar to us. These are simultaneous localization and mapping algorithms. They are very popular. And these kind of algorithms provide us with models of the environment based on K-point maps or future maps. Representations. Representations are very low memory cost and very easy to use. But on the other hand, the disadvantages are that we don't have enough information. I mean, we can't recover information about volume and shapes, and therefore, such representations. Such representations cannot be useful to manipulation or object recognition or aerial navigation. Other algorithms based on radial basis functions create extensive object representations, has 3D reconstructions, and they produce rich information maps. Nevertheless, they have high memory cost. High memory cost. Another one is vertical and multi-planar mapping. They are algorithms that obtain maps rich in information with low memory cost, but the restriction on the complexity of geometric entities used to approximate point clouds lead to get less accurate maps. So we have a proposal that Proposal that we are working on since 2017 when we present the hyper-elexoidal neuron. And then we complete our algorithm and publish it in this paper, Applied Science. So let me present to you. As I mentioned before, our algorithm was designed using the framework of geometric algebra, and we combine it with some machine learning algorithms to treat with these inputs with point clouds that we obtain from a leader sensor, and then we are. Then we are able to produce maps that are very rich in information about volumes and shapes and that have very low memory cost. So we are going to present the framework, the frameworks that we use to produce to design this algorithm. The first one is geometric algebra. Geometric algebra. Geometric algebras are cliffhold algebras that are constructed over a bilinear form of the vector space are PQR. PQR is the algebraic signature and the geometric algebra is denoted by the letter G and the sub index PQR. The elements belong to PQR, to heap G PQR are called multi-vectors. The dimension divectors the dimension of the algebra is p plus q q plus r the algebra is generated by the cliffer product and for two basis vector the cliffer product is this one and in this case if we have two different basis vectors then we have a quantity of Yeah, called bivector. The bivector is the result of sweeping the vector E D along the vector E I and what we obtain is an oriented plane. On the other hand, if the basis vectors EI and E G are R belongs to this subset, to the subset P, the square of the vector is equals one. If the basis vector belongs to the subset Q, then the vector squares to minus one and the last subset. The last subset of basis vectors are the vectors whose square is zero. So every vector in the real vector space R PQR is included as one vector in the geometric algebra GPQR. A ch vector describes a multi-vector that is defined by the Vector that is defined by the geometric product of k basis vectors. So the span of the geometric algebra GPQR is the set of grade zero elements, we call scalars, grade one elements, vectors, grade two elements, bivectors, oriented planes, oriented areas, grade three elements, phi vectors, oriented volumes, and so on to grade n vectors. Grade n vectors. In YA, geometric algebra, multi-vectors themselves could represent geometric entities in their inner or outer product representations. Also, it is very important, it is very interesting that the operators of geometric entities are represented as elements of the same algebra. I mean, both operators and Operators and the multi vectors that represent geometric entities belong to the same algebra. So there are many of geometric algebra defined. One of the most popular maybe is the conformal geometric algebra. It is the algebra of the spheres, but we are But we are searching for definitions of geometric algebras that allow us to define more complex geometric entities. And in this searching, we find the hyper-conformal geometric algebra. It is an algebra of ellipsoids, and we are going to present you. The geometric algebra for 3D. Geometric algebra for 3D reals is a geometric algebra G63. It was introduced by Julio Samora, and it could be seen as a generalization of the conformal geometric algebra for 3D. If we perform a stereographic projection over each axis of reals of dimension n, the geometric. Dimension n, the geometric algebra is then g to n. It is called the hyper-ellipsoidal geometric algebra. Specifically for R3, we obtain G63 and we have to define some neural basis basis which whose score is zero. Applying the graphic projection in every axis in 3D will be represented in the hyper ellipsoidal geometric algebra as this point. We have seven coordinates for the point in G63. Also, an ellipsoid with fixes axis. Fixes axis and with a center with these coordinates and semi-axis are x, r d r y, r c is defined with this equation. Again, we have seven parameters or seven coordinates for the ellipsoid. So, besides the geometric entities of conformal geometric algebra in hyper In hyperellipsoidal geometrical algebra, we can represent ellipsoids, pair planes, pseudo-cylinders, spheres, the form quadratic surface, anisotropic dilatations, among others. In this work, we are going to take advantage of this future. Um, future we have an ellipsoid, and this ellipsoid can be the form as a sphere, pseudo-cylinder, or has a pair of planes. How? Well, if you have an ellipsoid and one of sorry, and all the values of their axis. Axis, semi-axis are equal, then this ellipsoid is going to be a multi-vector that represents an sphere. On the other hand, if you have an ellipsoid and one of their axis values goes to infinity, then the ellipsoid is deformed into a cellular cylinder. Have several cylinders. If you have an ellipsoid and two of their semi-axis values goes to the infinity, then the ellipsoid is deformed into a vector that represents a pair of planes. So, we have an algorithm whose input is a point cloud, a dense point cloud. A dense point cloud. The first step of our algorithm is clustering all the points using a clustering algorithm. We use the very popular CAM-INS algorithm for clustering adjacent points for the dense cloud points. And you know, for a set of points, a cam-mines algorithm aims to find the partition with. partition with k less than equal or equal to n, such as we are clustering the points in the cluster whose center is the closest to the point. Once we have the cluster defined, we have centers and we have a semi-axis. Semi-axis of the ellipsoids train with k-means, then we are going to adapt ellipsoids using an evolutionary algorithm. We are going to adapt ellipsoids on each cluster and then we are going to retrieve or describe, we are going to approximate the volume of each cluster. Of each cluster. So, for the adaptation of each ellipsoid, we are going to use differential evolution algorithm, which is an evolutionary algorithm for multivariate function optimization. This is the differential evolution algorithm. These have all these steps of this process. The first one is the initialization. A population of candidate solutions is randomly initializated. And then we're going to process the candidate solutions, and we are going to produce a donor vector. Are going to produce a donor vector for each candidate solution. For each candidate solution, we compute a donor vector by choosing three different candidate solutions randomly. And we produce, compute a donor vector with this simple equation. F is a mutation factor. We set this. We set this value between zero and two. Once we have the donor vector, this is used to recombine the candidate solution xi with the donor vector to obtain a new UI candidate solution using a crossover rate. This is the way. This is the way we compute the new candidate solution. The candidate solution takes the value of the donor vector if this happens and the new candidate solution or the recombinated candidate solution takes the value of keeps the value of xi if this happens. Once the candidate solutions are recombinated, then we perform a selection step. The new candidate solution is compared to the actual candidate solution XI to choose which value is going to be kept. This is the And this is the way we do that. X i takes the value of the new candidate solution ui if the fitness function of the value of the fitness function of a candidate solution ui is less or equal. Okay, I think in your speakers' notes, maybe. Speakers know, maybe. I don't know if you want to share those too. We are talking about recombination. Recombination are finished. Did you hear all about recombination? Then selection. And those steps are iterated until the best performance of the fitness. Performance of the fitness function is returned. This is the differential evolution algorithm, and we use it to adapt the parameters of the ellipsoids. So this is our algorithm. We are going to use a parametrization function for ellipsoidal surfaces. This parametrization was presented. Was presented on this paper, on the hyperellipsoidal neuron paper. And what is about it is that we are going to define the hyperellipse locus. I mean, we are going to define which points are on the surface of one ellipsoid. And we are going to use the inner product, null space representation. The inner product null space representation of the geometric algebra G63. We are going to define all these points. The points that belong to the surface of the hyper ellipse are those points that fulfill this constraint, this restriction. The inner The inner product of the point and the hyper ellipse equals zero. Further, the sign of the inner product will denote if the point is inside or outside the hyper ellipse. Has the point and the ellipse belongs to the ellipse? Ellipse belongs to geometric algebra G to N, and they are represented as one vectors. Therefore, the inner product is equivalent to the scalar product in the linear algebra R2n plus N. So we find a convenient vectorial representation for a point in our geometric algebra and we are going to take the And we are going to take the coordinates of the point, and we define one mapping, C1. That the only thing that this mapping does is taking the parameters, taking the coordinates of the point, and embedded it in a vector. So, for a point in G63, we have a vector. Have a vector that belongs to R2n plus N. In the same way, for a hyper ellipse, E with parameter, center, and semi-axis values that belongs to R2N, we define the mapping C2. And again, we are going to take all the parameters. All the parameters or the coordinates of the ellipse, and we are going to embed them in this vector. Finally, we can compute the inner product to obtain the locus of the hyperellipsoid E. All the points that belong to the surface of the hyper-ellipse E. Hyper ellipse E are those that fulfill that their inner product equals zero. We are using the inner product representation and we also are able to define some kind of decision function. We are using the sign of this inner product to know if the point is inside the hyper ellipse, outside the hyper ellipse, or on the surface of the hyper ellipse. So we for a first step, we are going to after after we we After we have the input, we have these cloud points of dense cloud points, we are going to cluster the points and we are going to fix centroids and semi-axis. And that we are going to do that with the k-means algorithm. We are going to train in centroids similar to the case of RBF networks. RBF networks: the ellipsoid center is trained with k-means, taking the center of the cluster as the center of the ellipsoid. And for a point cloud with endpoints, K clusters and K centroids for the ellipsoids are found. So after we train centroids and clusters, we now We now need to adapt the ellipsoids on each one of these subsets of points. We are going to train semi-axis values of the ellipsoids and we are going to use the inner product of each point of the cluster and the ellipsoid using The ellipsoid using the mappings that we already defined. And we are going to minimize this inner product. Consequently, we are going to be minimizing the points and the ellipsoid distance. The volume of the ellipsoid defined with this equation must be penalized to avoid trivial solution. That is, we don't want to equal equal equal That is, we don't want an ellipsoid that contains all the points on the cluster, and also we don't want one ellipsoid per point. The fitness function that we design is this. The first term, if you see the first term, penalizes the distance from every point in the cluster to the ellipsoid surface by computing the root mean square error. And the second term penalizes. The second term penalizes the density of each cluster. We are computing the ratio of the volume of the ellipsoid to the number of points contained in this subset. The parameter alpha and k of k means controls the granularity of the ellipsoidal map, the number and the size of the ellipsoids that describe an Ellipsoids that describe an object. We can vary the value of k and alpha to obtain more or less definition in the representation. These are experimental results and applications, some applications of our mapping algorithm. Again, we are obtaining the dense point clouds. Dense point clouds using a SRE 500 laser. This laser is capable of scanning 800,000 points per second at distance up to 150 meters across. These are some examples of point clouds. Point clouds. In our experiment number one, we have the input cloud is composed of 10,916 three-dimensional points here in the tense point cloud. And we have that the multi-ellipsoidal map that we obtained contains only 150. only 150 ellipsoids. You can see that we can use these kind of these maps to perform object manipulation, to perform object recognition. And we have a lot of information of volume, a lot of information of the original shape. This is the experiment number two. This is a pine tree. The input cloud point is composed of 31,049 three-dimensional points and the multi-ellipsoid map obtained contains only 400 ellipsoids. This is an important feature to note. Future to note: if you see the ellipsoids on the floor of the pine tree, you can see that a lot of them are deformed into planes. So, we take advantage of this very desirable feature of the geometric algebra, and we can deform these ellipsoids into. These ellipsoids into planes. I have a question, maybe it's relevant, Jay. So it's very interesting that you're sparsifying the data and you're saving a lot of memory, I guess, in this representation. This is like orders of magnitude. Yes, I'm going to present you our savings in memory. That is. Let me. Okay, let me let me follow. This is experiment number three. This is the plan, the input cloud point, and it is composed over 40,000 three-dimensional points. Look or mapping in this experiment, you can note that the concave areas are. Areas are represented very accurately by several ellipsoids. Let's see the front view. Another stand man. The point load is composed of over 37,000 three-dimensional. 7,000 three-dimensional points and the multi-ellipsoidal map only 350 ellipsoids. In this occasion, on the floor, the floor has a lot of texture, is a carpet and a fuzzy carpet. And you can see that the deformation into planes only takes place maybe in this. Maybe in this ellipsoid. The other ones are ellipsoids with volumes for the texture of the floor. Experiment number five: again, a lot of compression, the representation using ellipsoids. And this is the table that I was talking about. Look, in the experiment number one, we have over 10,000 points. If we consider a four-byte floating-point number, then we have Then we have over 13,000 bytes. Using ellipsoids, on the other hand, we have 150 ellipsoids and these ellipsoids are represented with 3600 bytes. The percentage cost of this map. Cost of this map, the ellipsoid map, is 2.74% with respect to the map using all the point cloud. You can see that the percentage cost represents very a lot of savings with respect to. Savings with respect to memory. We have multi-ellipsoidal maps with very low memory cost with respect to the original point cloud map. We want to see the effect of Bargain number of ellipsoids. It's obvious that it's going to affect the definition of the map. You can see the map you can see using the original using only 10 10 ellipsoids this is the original point cloud this is 10 ellipsoids for the map one ellipsoid 3 50 500 you can control this with the k parameter and penalize this with the alpha Analyze this with the alpha parameter on the fitness function. Also, we wanted to see more variety in the deformations of the ellipsoids, but we understood that we need to change the clustering algorithm to do so. We need maybe a hierarchical clustering algorithm to avoid multiple. Multiple clusters, for example, on the floor. So using K means, K-means, we induce the deformation by sensing a cylindrical flashcamp and we perform the clustering only using two clusters, one for the One for the trash can and one for the floor. And we obtain this multi-ellipsoidal map. You can see that the cluster or the ellipsoid that clusters the points on the trash can is deformed into a cylinder and the ellipsoid that The ellipsoid that envelops the floor, the points of the floor, it was deformed into a pair of planes. So maybe changing the clustering algorithm, we can see more the formations of the ellipsoids. We perform environmental Environment mapping experiments. This is our campus, and this is the map that we obtain. Again, you can see that the volume, the shape, information, you can read that from our map without problem. We have this point cloud. Have this point cloud with over 700,000 points, and in the cloud map, we have only 8,880 episodes. Another experiment, one environment, we have this point cloud over 70,000 points in the cloud map and In the cloud map, and in the ellipsoid map, we only have 5920 ellipsoids. This is the memory cost table. You can see again low memory cost using ellipsoid maps. And we have some application of this, these are. Of this algorithm, we can perform. This is a slightly different algorithm because we are not using the parametrization of the points and the ellipsoids. We are using the covariance ellipsoid to keep the motion tracking. We reconstruct internal organs and also we perform motion tracking of the movement. Motion tracking of the movement. This is the right liver affected by breathing movement. We have has input for the magnetic resonance imaging of 8,219 points each with 19 positions. And then we reconstruct the lever and keep the motion tracking. The motion tracking for a temporal resolution of 362 milliseconds reconstructions were made for 400, 800 and 100 ellipsoids. In this paper, you can see the precision that we, how accurate is this map. And the last application, we use the ellipsoidal maps to design a path planning for UAB vehicles. You can see the ellipsoidal path planning algorithm on this paper here. Here, the path planning algorithm takes advantage of ellipsoid entities to represent obstacles and to compute the distance of the ellipsoids of the object with respect to the ellipsoid that envelops the UAB. So So, taking advantage of this representation, you can see again, you can see the results on this paper. And this is it. Conclusions: A new mapping algorithm based in G63 geometric algebra has been presented. It is capable. It is capable of adapting multiple ellipsoids to obtain object representation. It's more complex in memory than the original point cloud. This multi-lepsoidal mapping offers information-rich maps that are suitable for representation and approximation with, again, low memory. With again low memory cost. The use of the geometric algebra framework allows us to work with algebraic representations of ellipsoids that these ellipsoids are capable of deforming into pair of planes, spheres, pseudo-cylinders, and this is a future that is very valuable for robotic mapping. For robotic mapping for future work, well, we are going to try another method for doing the clusters, for compute the clusters. This method, we know that this method has to be a hierarchical clustering methodology in order to see these deformations into cylinders or pair of planes. Or pair of planes, this improvement will reduce the number of geometric entities of the maps. And consequently, we are not going to meet the k parameter of the k-means. Differential evolution also could be changed by one layer of hyperellipsoidal neurons, and this neural network could be. Neural network could be trained for online capabilities. We can train this neural network using extended karma filter and this could lead to a dynamic multi-ellipsoidal mapping algorithm. Furthermore, the parameters k and alpha were selected heuristically for this work and for a better understanding of how these parameters affect the How these parameters affect the granularity of the map algorithm, a new study has to be done. I end this presentation. Thank you for your attention. Gracias porso attention. These are my emails. You can contact me through this. Well, thank you very much for the wonderful presentation. Very much for the wonderful presentation, please jeremy, and thanking Professor Banzik for very, very interesting, fascinating work. I don't know if anyone has a question that they would want to ask at this point. May I ask a question? First of all, thanks so much for the very interesting talk. So I wonder, is there any estimation? So suppose you have a ground two-dimensional surface in R3. So within the choosing of the accuracy of Within the choosing of the accuracy of some, is there any estimation like how many number of the ellipsoids you need to choose? Compare with like point clouds, and this gave a like estimation. What's the rate? I have to say that this depends a lot of how accurate you want that your map is going to be. We can measure the difference between the point cloud, the distance between each one of the point clouds and the ellipsoids that represents a volume. And we could compute some precision, some metric for the precision of the map. So if you So if you have more ellipsoids, you have higher definition on your map, but also you have a lot of more memory requirements. I need to do a new study of how the number of ellipsoids, the granularity of the map, Of the map affect the precision representation. Okay, thanks. Sorry, I have a question too. So I don't know if you already know this, but I just wonder if you know how the processing time grows depending on the number of adipsoids? We didn't measure the time, but if you note the algorithm is highly high, you can parallelize this very easily. You can parallelize the algorithm. Differential evolution is an algorithm that you can easily parallelize. And the whole algorithm is based on. Is based on differential evolution, so you can parallelize. We didn't measure the time of the training. And for sure, we take more time doing the graphics than running the algorithm. It is a fast enough algorithm. But I don't have. But I don't have the running time to measure. There are a couple of questions in the chat. I'm going to read them out loud for the recording. It says, thanks. Did you measure the difference between the information content of the original and the compressed data? From the original, from the dense cloud point? I guess. Cloud point? I guess that's the point. The question is. Yes, yes, we have these tables. I guess the question is about the information content. I'm not sure exactly how they're interpreting that. If it is about the memory cost, these points, if we have. These points, if we have, if we consider a four-byte floating-point number, these are the measurements of how is the value of the cost in memory for keeping in memory this point cloud and for keeping in memory this multi-ellipsoidal map. I don't know. I don't know. Yeah, maybe they can clarify what they mean by. Clarify what they mean by information content. Maybe we can go on to the next question. If you want, I can read it as well. It says: Do you weigh all the points equally? Or do you have a procedure for throwing away or de-weighting outliers? Yes, all the points weight the same. We own weight different some points or another points from another. Other things from the new way. Great. So I guess those are also avenues to consider in different clustering algorithms that could reproduce more as a density the point cloud. And I guess one question related perhaps to that last question is: could these sparsification algorithms help in optimizing optimal transport techniques? Techniques because you kind of would be replacing a whole point cloud by a collection of ellipsoids, and then you could just go the way, go a bit back into just transferring a measure using the ellipsoids. So that would be interesting, perhaps. I don't know if anyone else has a a question or a comment they would want to share. They would want to share.