Thank you, Sergei, and thanks organizers for giving me this opportunity. I'm going to present some technologies that we are doing present today. And which I called, I changed a little bit the title. I called random optical-based green-function method. We are aiming to probing the density matrix of. Probing the density matrix of a larger scale system, this probing can be made by Monte Carlo method and also can also be made very accurate. I will show you how we do this. So, this is my afterline. So, in this talk, I will majorly introduce the idea for changing from this four. This four state space representation to very few state representation. This state I call it superposition state. With this superposition state, we can extract the density matrix very accurately. So in this first part, I will show you the colour method, which is already talked about by Professor Yang Shen Jui. We call it random state-based technology. Random state-based technology. But I can vital it with this Green's function that we define in energy domain. And then I will switch to a random orbital-based accurate method, which is based on this green swanching technology. And I call it the first technique is a coloring technique, which is basically. Technique, which is basically tell us we can construct this random state very carefully so that we can gate the density matrix very accurately. And then we come up with a new idea we call compressing and retrieving technique, which we can also obtain accurate density metrics. At the last, I will present a new application for random state for. For random state for the calculation nano-electronic device, in which we found after an important, what we call important sampling for this density matrix, for this random state, so that we can calculate very large scale electronic device with a single random state very accurately. So, this is about myself. Actually, I worked on this object. I worked on this orbital free DFT more than 10 years ago, but I worked too hard so that I'm afraid about this orbital free DFT at the beginning of my independent career. So I switched to work on something different. So my present group is basically I'm making a software and the initial software constraint DFT. DFT. So, our basis we used for building the constant solver is based on a wave function, a basic function called screened spherical wave based on the Merfenti orbital. This orbital is very special because it's highly localized and also minimal. So, this will be what's directly attached by any Hamilton. And then we use this in combination with In combination with this, we call line freedom mean field theory to build up a theory for simulating disorder nano electronics. And in past years, we have finished the development of one particle, two particle NGF, the ensemble average. NGF, the sample average of this NGF, and also including this non-local correlation effects of disorder scattering, and also covers this we call it short-range order effects. And also, we come up with ideas to treat this very challenging author diagnosis disorder within the MIFIA theory. For this DFT part, we already finished this development of scalar relativistic and also the full relativistic. And also, the full relativistic formula. And also, very recently, we finished the full potential method. This is actually the first full potential method for screen spherical wave after about 30 years of the first proposal. Today, I'm going to show you our new progress for this long-included green function, which is actually actually the Actually, we use for very popular for this learning problem in electronic device energy. And I'm going to present the combination of random state with the green function method. We have already finished two works based on this idea. So, firstly, I would like to acknowledge my students and My students and this Government, Dr. Qing Yui, and also my collaborators. So, the basic idea of a random state, this technology, I learned from Professor Yuan Shenjui, and also he shared a lot of knowledge about this technique. And also, I would like to thank Ke Xia and also my colleague Seijia Yu from Shanghai Tech University. So, basically, my So basically, my motivation is very simple. So, when we look at this nanotransistor that we use that running in our CPU, then you will find out if we want to simulate this, we are facing a very important challenge because right now our simulation limit, we face a very important challenge is that right now our capability is several. Our capability is several, we can only simulate several stars atoms, right? But for a real device, we have contains several mini atoms. So you will find there's three orders of magnitude gap in system size. If you change this to computational complexity, there's nine orders of magnitude difference. So we are working hard to close this gap. Hard to close this gap, then we're looking for a very different and new technology. So, when we look at the constant DFT, we know that there are major three modules. I mean, the time-consuming stack, as already introduced in previous three talks, is this. Is to solve the constant equation to obtain the density. In this part, people want to calculate this wave function or the density matrix to construct the density. So there are different ways to calculate this. So the first way is people know we can use exact organization to calculate against states, right? And the second way, you can also calculate this format drag operator, or you can calculate by. Or you can get it by polynomial expansion. And also, you can use another people very less used method we call the green function method. Use this, you can get equivalently well density matrix. But among this method, we this exact diagonalization and the conventional grease function is cubic scanning. And also, if you use this density metric. use uh this density matrix approximation you can make you can make this density matrix calculation in order and so then uh we will go through this green function method so so the green function we people know from the textbook then if you we start from a hamiltonian you can calculate the green function at energy e by this inversion and this uh And this cost cubically. And also, there's another formula you can define this completeness with all these eigenstates, and then you project the Green's function to this eigenstate space. I call these two methods for state space representation. It's very accurate. So, based on grease function, you can calculate that. Can calculate dense still state and also density matrix very accurately. So, then the question is: Is this necessary for calculating density matrix or densital state? I mean, Professor Yongsin Yun has told us it's not necessary to calculate the density state within the full state space. So, we look at a simple superposition state. Superposition state. This superposition state is composed of all eigenstates, right? Right? Superposition of all eigenstates, and this prefactor is plus or minus one. So then I apply Green's function to this state. You will quickly find something like this. With the Green's function, I can obtain all these eigenstates. eigenstates and their eigenvalues right so so so this uh random state so so this uh this single superfin state contains all the information you need to construct density state and uh density matrix so that so that with this single state you you just project the green function into this single state space you calculate you you calculate trace i mean analytically you you are sure we can show you can give us the exact density state so so this idea uh should also apply to density metrics but i will show you later but the problem is that the question is that uh we don't know the eigenstate so we don't know how to construct this uh this superposition This super to this state. That's a question. And also, another question is that this applying green swan to this state, how much cost is this operation? Right. So later, I will show you how to solve this two question. For this one, we can realize an order n method. And also for solving this, we introduce random states. Random states. So the basic So, the basic idea for constant random state is that we replace this superposition state with a superposition of our basis functions. So the prefactor of these basis functions are plus one, minus one. Okay. And this from this superposition state, it can be rewriting to the superposition of all eigenstates. So, but with the prefecture. So, but with the prefactor not equal to one plus one, it's approximate. So, then we found a very important property is that with this new random state, which contains element of one minus one, you can construct a projector. This projector, you look at the diagonal party, always one, and the off-diagonal part is one and minus one. And mass one. It's a random. So if I sample different states of this random state and I combine them together to form an approximate identity matrix, and you will find this identity matrix, the average of the identity matrix is exactly the real identity matrix. Real identity metrics. So, based on this idea, we can introduce approximation. Before I apply this approximate identity metric to Green's function, I will form a random Green's function. And the sample average of this Green's function gives you the exact Green's function. This guarantees the correctness of this method. So, when we look at further Further in mathematics. So, by doing this, I actually project the Green's function into this random state space. And this random state space contains a dimension of NS. So the NS is much smaller than original four-state space. So then I change this four-state state space representation to this field state representation like this. Representation like this. So we only need to calculate these rectangular matrix elements instead of calculating these four metrics. So you can understand with this technique, it can reduce a lot this calculation cost. Then, how to calculate this? So, the basic problem is that we have to calculate this one. This calculate this one. So then we need to calculate projection of green swans into this random state. Then I will introduce this RI. It contains energy dependence. So how about RI? When we look at it, we find I satisfy a linear equation, right? Because Green's function is one over Z mass H, we have moved this Z mass. Actually, I move this much actually to this side, you form this linear equation. So then we can apply a very powerful technical linear equation solver to solve this problem. But maybe if you look at this further, you will find for this density set calculation, you will need the grease function at different energies. You need to calculate a lot of energies. And also for this density matrix, you need to calculate this energy integration. You also need a lot. Energy integration, you also need a lot of energy, right? So, how to avoid this problem? The advantage for this, important advantage for this linear equation is that this linear equation is called a shifted linear system. So, there's a common Krieloff subspace for different energies. So, we construct a Krillov subspace with this edge and This edge and this quid of space is shared by different energies. Okay, so we only need to concentrate this quiddoff space one time and apply it to many different energies and also to integrate these density measures. So this computation cost is order n. And after that, all the calculations, their computation cost is neglectable. So, with this sub-space, we can introduce a sub-space Green's function, and which is very small in size. And also, for the density matrix, with this approximated density matrix, we can calculate this, I call it a quilo of projective density of state within the queue of subspace. And this computation cost is recognizable. So, then with this technique, we can realize all the simulation method. Simulation method, as you can see. So, this is a test for the dots with the random state. You can see with NIC equal to one, this is very small system. So, a single random state give us a very good density state. We almost there's no spectral needs, okay. But the magnitude is different, different from the cloud. So, we can really clear it to the So we can use the answer to get very agreement with the exact result. Okay. But if you like, like Professor Yuan has talked about, when we make the system larger and larger, you don't need probably a single random state is enough to give a very accurate destined state. So this technique actually can be dated back to 19. It dated back to 1975. If you have interest, you can take a look at this paper. It's a very old paper. And this technology is people, people later people find it very powerful for larger scale simulation, including this many body problem. And also, we also compared to this kind of polynomial method and also Chip Chef Fourier polynomial method. So Krilov projected function is actually a kind of Kriloff polynomial. It's actually a kind of Kuylov polynomial. This Kuylov polynomial, as I said, is more stable and efficient because the Kwilov polynomial, we can use much less polynomials to represent this formidable operator. It contains no temperature dependence. And it's temperature independent. So you can calculate all the temperatures. So then we have some tests for this Kryloff superspace. We will look at this hydrogen silicon with different band gaps, right? So we found, we found for this water molecule system, Kryloff space, we only need 20 polynomials. And for this silicon, I think the zero tenths should be enough. Okay, this is much less. Okay, this is much less than much less than is a chip shift for expansion. And then we will look at the computational cost scales linearly. And how about accuracy? We look at the accuracy in these black dots for this molecule system cluster for this water molecule cluster. This is a charge deviation. The charger deviation versus atomic indexes. So, when you look at it, it's actually not small, right, for different atoms. But there is a very important technique to reduce the error. We call it deflation technique. Basically, we introduce a prepared row zero, and then we calculate this difference between. This difference between real density metrics and rule zero, I mean, I mean, stochastically, and then we'll find we can reduce this statistical error by more than one order. And then we expect to get a very good total energy result. So, for total energy, we look at the calculation with the deflation, you are fair to find compared to the exact result with the exact diagonalization we'll find the total energy for for for total energy per water molecule the the error is is about one milliv per water molecule so so so so you will find this uh technique is very accurate for total energy calculation because there's a lot of error concentration so Constellation. So then the question is: are we satisfied with this accuracy? So I find it's actually not good enough for force calculation. Of course, it's local. And you probably already find this, even with these red dots, this charge division fluctuating around, right? This fluctuation is not enough for the people. It's not enough to give you very bad force. So then, the problem is that with this random state, we sample them very randomly or without any physical insight. So then without physical insight, we can make any error, right? Okay, even there's a lot of basic rules that can get. They can guarantee very good accuracy in total energy, but we cannot guarantee very good accuracy for local quantities. So then we come up with ideas to fix this problem by introducing some physics insight. The physical insight here I will introduce is a near-sign of the density matrix. It decays exponentially, I mean, for this one, where these two variables are far enough. Variables are far enough. Okay, it's kind of decayed to zero. When you look at this decimetrics in this atomic orbital representation, it gives you, for 1D system, this gives you a single band around a diagonal. So then the idea that I will introduce is to, we are trying to make this density matrix accurate. So without this fourth space representation, This four-space representation. Okay, let's look at this just for example. We can represent this four-state representation of density matrix, right? Like this form. This matrix can be very large. But the question is, how many superpositive state required to accurately represent this red part of this density matrix. Density matrix. So the question: so the answer is: we only need three superficial states instead of maybe 10,000 for space. So the technique that we developed is called current technique. So we use current technique in, we learned from mathematics. We choose, we finally carefully choose the three. The three random states like this. So then you can calculate the projector. It's very close to an identity matrix, right? We apply this metrics to this projector, we will find it can give us exact result for this red part. But how about other parts, this zero parts? We make a zero part. We make a zero part wrong. But we know, by the way, from the physics, physics inside, we know what we want is red part. All other parts, even it's wrong, but we don't care, right? We don't use it because we know they are zero. So then I just want to confirm you that for this problem, you don't need full specific representation. It's very time consuming to catch. Very time consuming to calculate this one, but you can very quickly calculate this one, but with the same accuracy. So then we tested this current technique with the water molecule system. We choose a different set of random states. I mean, you will find I should this force deviation for. Deviation for this water cluster. As you see, when we use this Ns equal to about 300, this force deviation for different atoms, right, is gone, becomes zero. It was almost zero. This is accurate enough for molecule dynamic simulation. So, how about total energy? So, how about total energy? If you look at total energy with about 300 random states, you will see for this about 2,000 water molecule, we can only find about 5 mV into the energy, this 5 milli V error into the energy. So, it's already, I think it's super accurate. So, you would. So, you would look okay for this for silicon cluster. We need we need more because silicon, this is the decay behavior of this decimator is slower than water. So, we need more random states. But we can also get very high accuracy. But this is a total energy. And then we look at this efficiency compared to our previous random state, fully random state, we get we obtain. We get we obtain about three times less in computation time. So then we have a question. So can we make this random state even smaller while keeping the accuracy? So we start to think about this problem in this summer. I got a student, a first-year undergraduate student. First year undergraduate students. And he told me there's a technique called compressive sensing, right? Compressive sensing technique. Then we start to think in that way. So we develop a technique we call the compressing and retrieving decimators method by using this random state technology. I will just show you the simple idea. It's very simple. It's a first idea, and the other graduate students can work it out. can work it out. So as I just show you, we have presented a green function technique to calculate this operation, the Green's function operation on these random states. We can calculate by Green's function method, this Krieloff projection method. And then in all the networking costs. And we can apply, I mean, many. Can we can apply many different this random state? We get this result called R. We form a matrix equation. This equation is actually very simple. It's a linear equation, but it's in matrix form, right? And this decimator is very sparse. It contains a lot of zero elements. So we know we don't want to calculate this zero. zero elements so then we only calculate we only calculate this r and with this x right and then we want to our our data we can we use this r and x to find the rope so the answer is yes because there's a very simple mathematical relation For example, if you have a very sparse metric like this, it's a symmetric. For this, I know that I marked it red. And you have also this red part, right? You do this multiplication, you will use this red part in X, you will obtain the red part in R. So, this basically builds a relation, very simple relation. This relation for each column, you will Each column, you will get each column of the R, right? You build some relation with X. This relation is linear, it's only a linear equation. So then I can, we come up with an idea with this linear relation. We only want to minimize this quantity. We can obtain this non-zero part of this row. And this will allow us one column. Allow us one column, column by column solution. So we don't need to solve this, all these zero elements together. Okay, that will cost much more time. And then we think for this very simple equation, we can also calculate by very simple idea for what for this water cluster, for this surface molecule, right? It's coupling much less. much less uh uh for for these uh molecules it can can it's probably much less uh um okay for for the column corresponding to these molecules the surface molecules right this non-zero element is much uh small compared to the inner uh water molecule so we will start from uh for this retrieving process we start from uh uh this the column with uh Uh, this column with the list unknown non-zero elements, and then we can keep the n as the list. Okay, we carry this one column by one column, and then finally we can find very accurate results. This is the major idea that we that we have already implemented. So, because of time, I only show you uh the major idea. So, the next, I will show you uh I will show some new implementation or random state for solving the challenges that I talked at the very beginning for larger scale quantum transport simulation. Basically, how to realize the importance of sampling. For this quantum transport simulation, we use a long-income Greece function based for first principle method. So in this So in this method, you will see our central quantity that we want to calculate is the lesser Green's function. To calculate this one, we need to calculate the retired Green's function by inverse this matrix. So this H is the harmonic of the device. For mini item device, this scale can be. Otherwise, this scale can be very large, right? And we'd never be able to do this in worse, I mean, with a small amount of computational resource. And then even you get this one, you need to do matrix multiplication. It's also a cubic scanning. So then I will introduce this random line group. Introduce this random non-group group function. So, we start from a very simple idea as we did for this retarded group function, for this equilibrium system. We multiply this random identity matrix, but we find it's not good. It's not good at all because it gives us a very large statistical error. Then we will need a lot of random state to reduce this error to This error to acceptable level. Then we think about this. Why don't we introduce a randomization to this lesser self-energy? Yes, we did it. Then you will get much better results. So then we start to write a paper. So then we will get a very good result, like I show you. get a very good result like like i show you uh we calculated this uh this uh this junction uh one side is copper the other side cobalt and also this uh turn junction with the vacuum as a barrier right but you you will notice this interface is is fully random it's orderly okay we use a very large shoop cell with nx and ny as i should uh nx equals 40 by 40 by 20 24 atoms By 24 atoms is already, I mean, 40,000 items. For each item, we use nine orbitals to represent. So when we calculate the results for the transmission for different states, you will find this result is fluctuating, right? It gives us a distribution very like the Gaussian distribution. And then with this. And then with this, we can calculate the average result and with different lambda for random states. I mean, this is for a majority spin, this is for a minority spin. And also for this turning junction, we find for this speed channel, this fluctuation is very large, as you can notice. And for another channel, the fluctuation is small. When we calculate this relative deviation, we can find with We can find with a single state, you will get this random fluctuation around 10%. I mean, for both channels, okay. And for this one, for the terminal junction, you will get with one single state, you will get a fluctuation relative deviation around 40%. It's very large. But you can always reduce this. Can always reduce this relative deviation by increase, you increase this number of random states, right? And you will find very small deviation with the 40 random states. And we are already very satisfied with this because we know for calculate, very accurate to calculate this system, you will need for this scanning state, you will need several sovereigns. So we already reduced this by 100 times. This by 100 times. However, before we submit the paper, we realize there are still very important space to get the improve. So because we didn't find the minimal degree for implementing this important sampling. So we think about this. About this, uh, uh, another method we call the scanning state method. In scattering state method, people calculate this incoming uh eigen mode, this end eigen mode. So, for each eigen mode, you calculate this transmission, and then you sum up the transmission, you will get a total transmission. So, this eigen mode form very uh this the important uh space. So we need to sample, we need to sample the random state within this eigenmode space. But the question is that, okay, I also need to mention this eigen mode space is much less than the space of a sigma, sigma nest or this gamma. Okay. So the question is, how can we find uh how how do we how can we find this eigen model space with grease function math so we we come up uh we come up a uh an eigen uh decomposition eigenstate decomposition to this uh to this gamma and then we can we with this eigenstate composition we can find uh this uh this this eigen mode of this all this incoming state and also this eigenvalue uh is There's eigenvalue is a very important quantity we call velocity. Okay, okay, I wish I'll finish very soon. So then with this eigenselectant decomposition, we introduce randomization within this eigen mode space so that we can realize the importance of sampling. Okay, so then we will have some mathematical details. Mathematical details. But finally, we change this calculation of transmission to calculate this contribution of different random states. And then we do the statistical average to obtain this total transmission. So this is basically an idea change for transport calculation. Transport calculation. So we basically change from this AGA mode solver to this superposition state solver. And then the important thing I want to mention is that this number of random states could be much smaller than this M mode. So this is the final result. So you can see for this red, right, for this majority spin channel, we only need to catch. channel we only need to calculate a single random state a single random state that can give us a error a deviation relative deviation less than one percent it's about 0.5 percent and then if we look at this n mode n mode is very large so we can we get a thousand times uh acceleration uh for another spin channel For another spin channel, you will find for a single random state, you will obtain a relative deviation around 3%. But it's already very good. It's already very good. You can also increase NS to 10, you will get less than 1% relative deviation. For another channel, for this blue channel, blue blue channel we we reduce uh we can reduce uh uh this relative deviation from from 40 percent to about 10 percent i mean with a single uh random state you you can use this uh 40 uh random states i mean to get about uh less than uh two percent about about one percent uh relative error okay so compared to this uh lambda lambda for uh ek mode Lambo for AK mode, so NS is much smaller. We can very surely show that this NS can be hundreds of times smaller than the ECI modes. So we also calculate the transmission at different energies, and also we find a very good agreement with this mean field theory, Long Group, mean field theory that we developed. Line from theory that we developed. Okay, so this would give us a very good verification for random state method. Oh, so this is my very short conclusion. I thank you for all your attention.