Amazing being here. I'm Batsur. I'll talk about going beyond loss minimization and machine learning using this multi-group and distinguishability viewpoint, and I'll tell you what I'll mean better. So just to set the stage, so loss minimization is by far the most popular paradigm in machine learning. What do I mean by that? We have these data points, we find some predictor to minimize some loss function on average across all of this data. Loss minimization has served machine learning really well. Machine learning really well, except that in many modern settings, we are using machine learning in societal applications where these predictions can directly affect individuals. So, I like this picture from this book by Moritz and Ben about how these data points are actually people and how we have to think about how these predictions are being used in applications and how they affect people in those applications. And that often requires thinking beyond minimizing average error, as Jamie talked about and Nika talked about as well. Talked about as well. So, lots of considerations which come up. For example, different individuals for whom we are making these predictions may have different loss functions that they care about. We may care about the model's behavior, not just on average across the entire data, but also on various subgroups, even groups within the data which may be protected by law, say various demographics. And in many cases, we don't care about just making these predictions, we want to use these machine learning predictions as part of some pipeline. Learning predictions as part of some pipeline, making some decisions. For example, who to invite for an interview, who to admit to college. We are starting to use machine learning in many of these applications. And in many of these settings, we are resource constraint. So there are resource constraints, and we cannot make decisions in isolation for different individuals. I think reasoning about these different challenges requires thinking about more things than just minimizing some average error. And we've already seen some great frameworks, so think about those. Seems great frameworks to think about those challenges. And in this talk, I'll talk about this industry mutability viewpoint to give us a framework to reason about these. Let me start by just explaining what I mean by loss minimization, mostly just to set the notation so that we are all on the same page. So I'll talk about binary classification throughout this talk. There's some distribution over labeled instances with binary labels. And there's some Bayes optimal predictor F star. So these are the true probabilities. So, these are the true probabilities of the label being one for any data point. So, this base optimal predictor is in some sense North Star, which is guiding us towards making good predictions. But learning the base optimal predictor by itself might be challenging, both statistically and computationally. So, usually in supervised learning, we think about a hypothesis class that we care about. So, some class of functions within the class of functions that we care about learning with respect to. Care about learning with respect to. And once we define this hypothesis class, we also choose some loss function to measure the quality of any hypothesis that we choose from this hypothesis class. The loss function for a data point which has some true labels y, and given that you're making some prediction t, just tells you how much loss you're incurring on that data point. It's a function of the true label and the prediction. It's pretty much aesthetic notation so far. And then loss minimization, we try to find some function. We try to find some function within this hypothesis class which minimizes loss on average over this distribution. So, one way to think about this is that we're just trying to find the closest function in this function class C to this Bayes optimal predictor f star, where closeness is measured with respect to this distance defined by the loss function and the distribution. So, loss minimization is defined with respect to a loss function. So, which loss function? function. So which loss function should we use? There are lots of different choices. One whole class of loss functions are what are known as proper losses. So these are things like the squared loss or the cross entropy loss and we say that the loss is a proper loss if you know that the true label is drawn from a Bernoulli distribution with parameter p, then the best action is to actually predict p. For example, for the squared loss, if you know that the Example for the squared loss, if you know that the labels are given by a Bernoulli distribution and you know that the parameter is P, then to minimize the loss, your prediction should be P itself. So this probably is not true for all losses. We also have improper losses. Maybe the simplest example is just the L1 loss, the absolute difference between the label y and the prediction. So in this case, it's a simple exercise that if you know that the table is drawn from a Berlin distribution with parameter p, which is more than 0.5, then you just want to predict 1 always. If you know 1 always. If you know that the probability is less than 0.5, then you just want to predict 0. So you're just thresholding the predictions at 0.5. You could also have different false positives and false negative costs, in which case you're much holding it at something other than 0.5. So with these improper losses, you don't want to predict the true probability, even if you know what the probability is. You want to do some post-processing. And you'll denote this post-processing by this post-processing function ks of l. So it depends on the loss function. The loss function. So for the L1 loss, you'll be post-parsing by just truncating the thresholding at 0.5. And for these improper losses, the main thing is that the post-parcessing function is not the identity function. By the way, if you please laugh at me and point a few questions. So we have all these different loss functions, and using different loss functions can lead to very different models. So in going back to this picture, one way to think about it is that One way to think about it is that if you're measuring distance in different ways, then there might be different hypotheses in your head which are the closest to this ground. Recover the function for minimizing some other loss if you minimize, say, a given loss. I think that's easiest to see just by looking at a simple example. So here's a very simple setting. So this is binary classification. The data is two-dimensional, and every point. And every point here is colored by the true probability of the label being 1 for that point. So the bright to green denotes that the probability is very close to 1, dark red denotes that the probability is close to 0. And the count two lines here are just straight lines. So these are the lines along which the probability is a constant. So now if you're learning this for the class of linear classifiers, then depending on different costs of false positives and false negatives, you want to choose a very different. You want to choose a very different linear classifier. So, if you're thresholding at, say, 0.92, maybe you'll choose something like this. If that's like your threshold, depending on your cost of false positives and false negatives, or some other threshold, you might use a different lignin classifier. So, the loss function that you get is different for different linear classifiers. In some sense, this is like the story of blind man and elephant, where all of these different losses capture different aspects of reality without capturing the whole complexity of the underlying criteria. Predictor here. So, all of this would be fine if there was really just one loss that we cared about, but in many settings, they may not be just one loss function, as Nika also said. For example, if you're learning risk codes for someone having a particular heart disease, then depending on these risk scores, you might want to make very different decisions. You might want to use various different medical interventions, and you may not know what those interventions are at the time of training your model. Trading your model. You might want to learn for different interventions. For example, depending on the probability of someone having heart disease, maybe you want to decide whether you want to prescribe some dosage of aspirin or to recommend them for surgery. And there's obviously very different costs associated with these predictions in terms of false negatives and false positives, for example. And you may also want to learn for some future interventions which you don't know about at this point, and you don't know the trade-offs involved with that. And this is increasingly common because we are. And this is increasingly common because we are starting to use MM models for estimating these risk scores in many different applications: healthcare, recidivism, or probability of someone dropping out of school. And we are often using these risk scores for lots of different interventions. So there's not necessarily one use case for learning a machine learning model. We might want to use them in multiple different ways. So if we somehow had true probabilities from F star, then we'd be in good shape because that just gives us the true probability. Good shape because that just gives us the true probability of the label being one, and we can post-positive however we want, depending on the loss function that we care about, like the objective that we care about, really. But aiming to learn f star seems too ambitious, like statistically and computationally. So the main question here in this part of the talk, if we can get similar guarantees, which basically allow us to pretend that we have this phase optimum predictor without actually learning the phase optimizer. Without actually learning this way, after it worked. So, to formalize this, we define this notion of anomal predictor. And this is joint work with Perexikropal, Aram Kodai, Omar Rein Gold, and Gibeta. The anomaly predictor is defined with respect to a family of loss functions L and a hypothesis class C. And we say that some predictor F is an omnipotent predictor for this. F is an omni-predictor for this family of loss functions in this hypothesis class. If for every loss function in this family of loss functions, we have the property that if we take this particular predictor and we post-process the predictions of this predictor based on the post-processing for that particular loss function, so L1 loss, the threshold at 0.5, for example. Then the loss that this predictor incurs on your data distribution is upper bounded by the loss. Is upper bounded by the loss of any hypothesis within this hypothesis class. In particular, you want to compare with the best possible hypothesis with respect to any given loss function in your loss family, and you want to compare with respect to all of these loss functions and all of these different hypotheses using a single model with this post-processing that you do based on the loss function. So, the idea is really that instead of minimizing some given loss on average, you learn once with respect to this family of loss functions, and then you post-process later for any given loss in your family of loss functions using the post-processing function. But Exchequer likes to call it one predictor to do all of this class of predictors. And there is such a predictor. So, we define this notion, the natural question. Sure. The natural question to ask is if this object actually exists or not. The P optimal predictor is an omnipredictor for any loss function and with respect to any hypothesis class because that's really the optimal prediction that you can make. There's nothing better that you can do compared to just actually using the Bayes optimal probability because that's the true probability of the label being one. Post-process that. You cannot hope to get smaller loss than that. The Bayes optimal predictor has this nice guarantee and Nice guarantee and you can use it for any downstream application that we care about. But can we hope to do something as good as this, so learn an optimal predictor for some rich classes of loss functions and hypothesis class without the complexity of actually learning the Bayes optimum predictor? Question. The content of regression, would this mean that you need to learn the conditional density of the outcomes? Right. So sufficient. Yeah. Would that be the answer to sufficiency? Would that be the answer for sufficiency? Yeah, that would be sufficient. I would expect that the answer would be: I learned the mean, there's an F star, I just learned the mean, and then for every loss of process. But then, you know, like if I have the quantile mechanics process, you get there. Right, right. So if you learn these two properties, then that would be sufficient. So that would be an omnipredictor. So what we want to ask is maybe what you're asking as well: can we get why it's something less than that? So are there interesting classes of loss functions such that you don't actually need to learn these true problems? To learn these true probabilities in order to still have this guarantee that you're doing as well as this optimal predictor as measured with respect to your chosen hypothesis class. And we see that we can actually do that. So, like you might not, you in general, you do not need to necessarily learn the space optimal predictive, you still have this guarantee. Because, like, could I just make like basically make K L not to something that would have to be. To something very diverse, and then we just put everything into the process. Yeah, that's a good question. So, one thing, the first thing is that KL is only depend on the loss function. It doesn't depend on your predictors or data points. And moreover, KL is actually just this fixed function. So, this is just the post-processing function that you would do for that particular loss. So, if it's a proper loss, KL f of x would be equal to f of x. Otherwise, it's just the post-processing that you get for that loss function. So, so do I have a loss? So we don't really have choice in the KL here. We are really using the KL which you get for that particular log. There's sort of a fixed processing. Yeah, it's really a fixed postparing. Right, right. So there's a fixed postparing which only depends on this loss function. On this loss function, and we don't really have flexibility in choosing this post processing. So, we basically want to do the same post-processing that we would do for the Bayes optimum predictor. So, the idea is that we can pretend that we actually have the Bayes optimum predictor and do things like that. So, to see if we can have these omnipredors, I'll do what might seem like a somewhat abrupt segue into this concept of multi-calibration. Of multi-calibration. So, multi-calibration is a notion of multi-group fairness. It's a generalization of this classical notion of calibration. Calibration is a pretty simple notion which says that if you look at the level sets of your predictor, so again just find declassification. So, you look at all the data points where, say, the predictor predicts a 0.5 probability of the label being 1, then across all of those data points, 50% of the labels should actually be 1. So, basically, the predictions should mean So basically, the predictions should mean what they say. And this should be true for all of these level sets. That's calibration. In multi-calibration, what we do is basically try to get calibration on some finer sets. So we define multi-calibration with respect to some family of Boolean value functions, C, so some collection of sets. We say that some predictor is multi-calibrated. If you get calibration, even Calibration, even when you look conditioned on a particular set, a particular set in your family of sets, and then you look at this particular sub-level set, like say where the probability of the label being 1 was 0.5 according to your predictor. Then, even within, when conditioning on this particular set and this level set, 50% of the labels are still 1. So, what is the difference between calibration and mean unbiased? And mean unbiased? And mean unbiased. Oh, being unbiased? Well, I guess you can see. You can say that this is some notion of being unbiased with respect to some loss function. So, I guess like bias could mean different things, right? Like, it could be like an unbiased predictor which gets the mean right, but here, like for say binary classification, we're really seeing that even on condition on any particular prediction that you're making, you're still unbiased in the sense that your probability variable is still not the same as what you have predicted. And so, in multi-calibration, you look at these sets and try to get calibration even within these sets. And this can also be thought of as this indistinguishability guarantee, which is one point that I wanted to make. So, basically, an equivalent way of seeing this is that you're asking that even within this level set, let's say where the triggered probability is 0.5, there is no correlation between any function in C within this hypothesis class, this function in class C, and the true labels. And the two labels. So it's easy to go from this to this. You just consider Cx being binary, and then Cx equal to 0 goes away, and then you get this particular expression. So basically, the same thing. But the model here is that with multi-calibration, you're asking for a predictor which is indistinguishable from nature, the true groundwood predictor, with respect to your hypothesis class, this class of function C. And this is like multi-calibration was proposed in this group fairness picture. It's a natural notion of fairness because it basically says that your predictions should mean similar things even across different groups. So nice example is a study by Obermeyer et al. on racial bias in a large-scale predictive analytics system, where they found that if you look at black individuals and white individuals, then you actually have different calibration levels. So with the same risk score, say 50% probability of having. This score, say 50% probability of having severe diabetes, that actually means different things depending on whether you're black or white, which is obviously concerning from the point of view of actually using the system because you're effectively thresholding at different risk levels for different individuals, depending on their race. That's multi-calibration. And what we show here is that you can get omnipreders using this concept of multi-calibration. Here's the result. Here's the result. So we see that if F is multi-calibrated with respect to this hypothesis class C, then it is an omnipredictor with respect to the class of all convex and Lipschitz loss functions and the hypothesis class which is a slight generalization of C. So you allow linear combinations of functions in C as well. The post-parsing is the same as what you do for the Bayes optimum predictor. So basically, So, basically, from the perspective of all form X loss Lipschitz loss functions, you can pretend that you have something like the base optimal predictor as measured with respect to this hypothesis class. And you can get interesting loss functions and discover interesting algorithms in this way. For example, if you have the L2 loss, you're doing linear regression, you have the L1 loss, which is useful because of its noise robustness, and you get something like linear programming, which is used in this class. Programming, which is used in this classical work of KKMS on agnostic free learning half spaces, with cross-entropy laws, you get logistic regression, with exponential laws, you get data rules, and so on. So you can recover guarantees that you would get with each individual loss function as long as you have this multi-carbon guarantee with respect to your chosen hypothesis class. The proof is actually fairly simple, but in the interest of time, I'm happy to talk about it offline. I'm happy to talk about it offline. It's like basically a sequence of pictures lets you see what's happening here. But in the interest of time, I'll skip this for now. And just to summarize what's happening here, with the multi-calibrated predictor, we are getting an ARMI predictor. And if you go back to this visual that we had for supervised learning before, we reasoned that with different loss functions, you get different hypotheses, which are closest to this groundwork predictor x star. Closest to this ground root predictor f star. And in that picture, you can think of this omnipredor f is something which is closer to this ground root predictor f star than any function within your hypothesis class as measured with respect to any convex and Lipschitz-loss function. And I want to go back to this high-level point about indistinguishability here. So you use a usual loss minimization framework, says that we pick a loss function that defines an We pick a loss function that defines an objective and then we train to minimize that objective. With multi-calibration normal prediction, the setup is different. So here, instead of minimizing a particular loss function, we are learning to really fool this class of tests. So it goes back to this multi-calibration guarantee as being saying that you're basically indistinguishable from nature with respect to your class of tests. So this is the hypothesis class C. And once you have this predictor, which But once you have this predictor, which in some sense extracts all the predictive power that you have within this hypothesis class C, then you can post-poss later for any objective that you care about. So the goal with learning here is to really learn something which is indistinguishable from nature as measured with respect to your hypothesis class and with respect to these correlation tests that I defined when I had this multi-calibration expression. And this was formalized in this very nice paper on outcome indistinguishability. Outcome indistinguishability, and this can also be formally related to this notion of forming prediction, which I said. And maybe in the next few minutes or so, I want to talk quickly about how we can use this same indistinguishability notion to get some natural fairness guarantees when we are using machine learning algorithms to come up with rankings. So, setting here remains kind of similar. We have, say, 10 data points, individuals now. Data points, individuals now. We have some model which gives us the probability of any individual being qualified for, say, a particular job. And then we want to have a ranking mechanism which takes as input all of these properties for all of these individuals. And then it produces some randomized ranking of all of these individuals. And maybe then you select the top 100 or so to invite for an interview or something like that. You can ask your question that you. You can ask a question that you can ask here. So, which, like, suppose you're using this machine learning advice as part of your ranking mechanism. Then, how do you define fairness of the ranking mechanism? How do you define fairness in terms of this predictor? And in particular, can we have rankings which inherit some nice fairness properties that a predictor has? So, since this is like this talk is mostly about supervised learning, I won't talk about the mechanism so much. I won't talk about the mechanism so much. You can also define fairness there using some indistinguishability, but I want to briefly mention how we get fair predictors here using this indistinguishability concept. So we have these candidates, we have some mechanism. Again, I won't really go into the mechanism right now. And then I want to define fairness using this cryptographic viewpoint where you have these two different universes. In the first universe, we use this predictor f to come up with a probability of any individual being qualified for the job. Of any individual being qualified for the job. In the other universe, we actually have access to the ground truth. We have access to the base optimal predictor F, which gives a true probability of an individual being qualified. And now, suppose we have some set of group C that we want to be fair with respect to. Then we can measure statistics of, say, the expected number of individuals from any particular group in this set of groups, which are selected in the top K positions for some value. The top K positions for some value of K in the ranking that you get using this machine learning model F. And then you can look at the expected number of individuals which are selected in the top K position and who belong to this particular group when you consider the ranking under the ground truth predictor F star. And if these are indistinguishable, so close to each other, then you say that these rankings are actually fair. So if the rankings are fair with respect to these set of groups, if these two expectations are the same, basically says that as measured with respect to Is that as measured with respect to this set of groups and these first-order statistics, these expectations, our predictor, like the rankings that we get from this predictor, are indistinguishable from what you get from the ground group. And the nice thing about defining fairness this way is that you get nice compositional back properties for fairness. So if your filter is multi-calibrated, same notion as before, or some set of group C, then the rankings are Then the rankings that you get using this particular predictor is also fair with respect to the set of groups. So your ranking inherent fairness properties of the predictor. So if for example your original predictor was multi-calibrated with respect to a very fine set of groups, then your rankings will also be fair with respect to a fine set of groups. So indistinguishability seems to naturally lead to this nice composition because I guess indistinguishability. Because it gets like indistinguishably sort of composes. And this is in contrast to some other fairness notions, say, for example, like statistical parity, where you often have these tensions, and we have impossibility results of how you cannot get composition. And we can also get fairness and some matching problems using a similar framework. Same set of people. And maybe the last minute or so, so Amina is gone, so I have freedom to go on for as long as I like, but I'll still answer this. So I want to briefly mention some work. So, I want to briefly mention some work with Dash and Sir at Students at USC and Peter, who's sitting there, on actually trying to understand how multicollection works in practice. And I won't actually go into details because I don't want to hold you from the break. But we wanted to do a large-scale evaluation to actually see the level of multi-calibration we are seeing in practice. And maybe I'll just, I'm happy to talk about these offline if you're interested. But if I wanted to summarize in one sentence, what we generally find is that deep neural networks are surprisingly well calibrated. Deep neural networks are surprisingly well calibrated with respect to pretty large set of codes. Talk more offline. That's interesting to you. And just to wind up now, so I started with these questions of trying to have notions or frameworks which allow us to reason about some of these challenges. And I think indistinguishability is a natural notion which gets some nice properties. You can learn ones for a large class of class functions. You can get nice fairness properties, which will compose nicely. And there's been other works. Cooles nicely. And there's been other work which also shows that you can get lots of other nice properties, for example, even robustness to some distribution shifts. And I want to thank all my collaborators once more. And thank you for all of your attention. And maybe we can take one question.