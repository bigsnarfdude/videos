With average parts of mildly large clay for resolution, as you might have observed, I changed a little bit about my title, and you will see the reasoning for this. So, two results. The first one is a weak exponential lower bound, and the second one is an ideal lower bound, but only for intermediate. So, let's start with this. So let's start with this equipment, which is uh rather familiar uh to all of us. Um yes, given in a given n-verted simple graph G, is there a k and click? And we are mostly interested in the average case, by which we mean the input graph G is a random graph in the Earlish Renaissance. Because hopefully we think this could give us some clue. This could give us some clue on where the algorithm hardness of the problem comes from. Now, since everything gets down to proof eventually, let's look at its proof complexity version. Now, at least three things to be specified in this question. Namely, hard on average means we take the perdo format, G and P, and what parameters it's P, and what is system. It's P and what is system that read P there and what is the encoding. So let's start with this one. So we can ignore this expression, but simply take the bits, that's a bit of information that the setting is the so-called threshold for the graph to be k-click-free. If the graph is with high probability, has a k-click inside it. Has a k-click inside it, then certainly any refutation for it with high probability would be infinitely long. That doesn't make sense. And the only thing I remote, perhaps if you really want to read it, this C is anything that is greater than one. Think of it as 100 constant. And proof system. So our target will be resolution. Some history. So, okay. And you see. Okay, and you see this is the paper by Osar Paul Impapliaso and Sabova. Sabova. Yes, and that's the reason I changed my title. They showed a lower bound like this for general resolution. And as you can see, to let it make sense, k should be at least n to the five or six or something like that. This is the real large click. And next are two beautiful papers, which Beautiful papers, which achieves other two beautiful papers, which achieve the ideal lower bounds for tree-like preservation. And I should say all of the authors of these papers are presented. Not in this room, but in this in this conference. So, finally, block. Encoding. So finally block encoding. The encoding is the block version. Namely, we partition the input graphs into k parts and we are asked to refute that there is no transversal click, which means the click intersects each part. Each part of your word is set at one word. So in expression, it's like this, but again, don't try to read it. But again, don't try to read it. A bit of information of this seemingly unnatural encoding is that, well, at least from the literature, this is the proper encoding that hopefully tells us some internal hardness really about the G. It peels off all those aspects of some superficial hardness from some other thing, like Peterhoe principle, that's really almost irrelevant of the graph. Almost irrelevant of the graph G. And also, this is the hardest case. I mean, in the unarain coding, meaning that lower bounds of this is strongest. Cool. First result. So, what we can prove is for this range of k, still it's mildly large. And the general feeling is that the smaller your k is, the harder the question. Your K is, the harder the question is. So let's see a bit about the proof. The general strategy I should say is very, very simple as long as you understand what's going on here. So to show that many classes must appear in the proof, the idea is to let us define a class of objects or classes such that each of them is small. Each of them is small, but together they should be complete. So I know this appears rather mysterious to you, and let me provide some extra explanation. So let us think in the following way. So, what do I mean by small? A proof is a rather syntactic thing. A rather syntactic thing. Let's think of it in a semantic way. Namely, for example, let's fix resolution. If you have a class C, I think of it as the set of points that C is falsified. Set at R, defined to be C inverse at 0. So this is inside the Boolean universe. This is inside the Boolean universe. And viewing in this semantical way, actually there is a term called semantic repetition, a proof. A resolution proof is just given a bunch of starting objects which corresponds to the axioms, and you want to somehow collect them together using only objects of this form. Of this form, sub-cube form, and finally reach the whole universe. Under that viewpoint, small object is that I want to define some kind of measure for the objects in the universe. And I want to say that in every resolution proof, there must be many of the small object sizes, which is like you're counting the volumes. You're counting the volumes. Since finally, you reach the whole universe, which has, say, volume one, and you must pass this many small things and do a division, you get a result. And here, for the particular problem, intuitively, the clause that we're trying to define is those that almost reveal a partial, very large partial click, which means it gives you many. To click, which means it gives you many information, much information, which means they are small, because they are specific. Okay, so two definitions. So the first one is that, okay, what is P2 and I is narrow in R. Here R means an object defined by plus. Since we are talking about the plot The block version of the K-click problem. A pigeon eye, I can view the problem as assigning k pigeons to vertices so that their position finally gives you a transversal click. And pigeon eye is click narrowly. R means that, remember, r is an object corresponds to a class. It is this point set that satisfies some condition of this. to satisfy some condition of this form, say p j doesn't go to v, p j doesn't go to and p doesn't go to v prime, or something like that. And sometimes it says, say, p and j goes to w. So you can visualize it as if this is a pigeon i, and this is where you set the i. Then r says, okay, here i come, you can't go places like this. And what's left is the potential places that pigeon to be assigned. So I say this pigeon is narrowing R. If the places that R allows R to go, is this RQ-neighbor, is this not RQ-naber dense? We can ignore this term for a while. This is actually, well, it seems a very natural combinatorial property of the graph, but the earliest reference they can find. The earliest reference that I can find for this is just a regular paper that I mentioned. So think of it as something like saying this part is large and none of that thing, saying that this part is small. And then I say a clause is really useful corresponding to small here if it contains at least say 100% of such features. And you can think of this guy as some kind of with clause small if it has. Clause small if it has large such width. Then to put it to work, we need a framework, as I said, do some clever division, actually. And the one I found very helpful is the paper by Pavel that has been mentioned several times this morning: bottom accounting. Namely, we travel down the proof, starting with Starting with a randomly chosen half-up R click half of, where I should remind you, R is something like 100% of K. And this is how it's chosen. Namely, you choose a set of pigeons first and then decide their places. And in this process, you need to be careful because once you choose, decide a place for a new pigeon, it has to be in the common neighborhood of the previous ones. And branching to some useful RFPs. So, two lemmas or claims kill the problem. The first is completeness, namely whatever starting click is, we can always branch down to the user class. And second is that probability to each clause is small. So label one tells you the probability space is Those are the probability spaces, actually. The probability space that you are allowed really is one, has prob has a measure one. That second one says that one to reach in ceasefire actually is small. And division. Okay. So a summary of this pro. Classic strategy, simple zero random properties. And now, if you want to, if we want to generalize If we want to generalize this or improve this lower bound to the ideal case, we may think that structure might work, but well, we have to capture some finer pseudo-random properties of it. And this is a strong sign pointing to the paper, a regular case, where they actually indeed used finer pseudo-random properties of the graph, which is called mostly this. Called mostly data sense. Again, I will not explain to you what it means here. Remember, this is the key idea inside MRI, to achieve such lower bound. Now, our task is to consider how to generalize or how to apply this to general resolution. There seems uh some obstacles or problems. Problems. A quick reminder of regular resolution. This says I mean of no variables permanent. In that argument, why is regularity really needed, or actually is it really needed? Well, the answer is a bit technical and ad hoc, is that their argument actually, instead of computing the probability of reaching a Computing the probability of reaching a point, they're also computing the probability of traveling through some paths. And in our argument, remember in the first result, we only force ourselves to trouble to useful things. In their argument, we also leave some very simple probability, but trouble to some useless things in altogether. And what they need is a probability product selecting our optional line. But in short, Well, in short, just this. If your available reappears, I think it could gain. Now, should we stop here? Definitely not. But what we can think is that is there really anything that we can say through this example that is different from regular in general? And like all mathematicians do, we want. All mathematicians do, we want to see it in a structural way. So a sensible move seems to be though. Define a new model. And before I tell you some detail about this model, let me give you some impression of it. Okay. Its goal is to refine irregularity. The final piece of the message is that non-regular channels non-regular general separations is actually n-regular separations. And by non-separations actually I mean the two CNF, the two CNF values from the paper are not knowledge at all. And actually we know very little about country samples separately in general in the graph. And the second piece of information is that the lower bound can be extended to that. But in picture it looks like this. But in picture, it looks like this. This means power increasing. That's regular, general. This is what we know as several examples. This is where M pulse. Some other minor feature of M is that it's codracon. And in additional best version, which is what I'm talking about, it can it assumes a variable partition as its input. Partition as its input. I just say accidentally through the talks from previous days, it seems problem from very different perspectives. All more or less consider a partition of variables. For example, the name block with. I think it has a pure albert, macabre, and elaborate response. And perhaps it's sensible to make a model for it and study something like Make a model for it and study something like block with size relationship to something. Although I don't know how. Okay, so this is the model. I call it A irregular resolution, where A is a parameter between 0 and 1. And the input, as I said, is a CMF and a partition of variables. I don't care about the number of partitions at most anymore. The only condition I've asked is this. The condition I've had is this. If you have a block white clause C, meaning that it touches at least a portion of blocks of variables, then under C, you're almost irregular. By almost regular, I mean irregular, except on an eight portion of blocks. And if you compare this model with the one mentioned by Gilaru earlier this morning, this is a stronger model. Pink of the roughly speaking spectrum. And the red part means some monstrous objects. But once you meet them, you have to stay somehow like regular with troubling down. And I should say the previously mentioned separation film has naturally fit in. By naturally, I mean, remember, here is an input of partition. Is an input of partition. So if you look at that as a digital input, there is a question of the same CNF family, which partition do you take? And for those families, actually, they have very clean semantic meaning of the partitions. And also, there is another seemingly useful information about the partition chosen for that families is that those families have large width. Large width, in axioms has large width, but under the partition, variable partition chosen for them, they have small block width. And the upper bounds were n, so the numbers were extremely small. Something like n to the minus one-half, the not constant. Now it's time to state my second result. Now it says, Now it says for GMP the same thing, using the k-block partition, the feature partition of variables. Then this time it's for all small k. The ideal bounds was for this constant irregular resolution. So since time, I guess, let us say a little bit about group. A little bit about this proof. I should say it's rather easy, even easier than the previous one. The idea is to borrow the regular result for self-reduction. And the way I do self-reduction here is simply restriction. Namely, you need a restriction that simplifies the proof, but maintain the hardness of the instance. And here, as Sasha has pointed out in his talk, As Sasha has pointed out in his talk, when we're talking about graph properties, restriction corresponds to taking sub-graphs. And the subtlety is that very useful, actually the key, quizzy randomness, apparently just is not inherited by itself. So there is some technical issues, but fortunately they can be fixed. And in short, it's relative when I said equipment. Not only about the RFG, Not only a value graph G or sub graph G prime, in relative bias to be a property between G and G prime. We'll carefully focus on this. That's it. Some of her problems. There are actually three. The zeroth one is too obvious to be put here. Namely, to opt in the actual into the lambda k value for general resolution. And some perhaps less. And some perhaps less conf some different problems left is that even the weak lower bound for strong worker systems like outverse systems. As far as I know, I didn't see any even degree lower bound for PCL. But it seems doable. The second one is, as I said, regarding this block partition model, let me say something about blockwork size under purpose. Under certainly under some reasonable restrictions. Otherwise it's to I don't think I don't believe there is a general science for any issue. That's it. Regarding the second open problem, what do you think? I mean, we have another What do you think uh I mean we have a natural idea given an example what should be the block. But what uh the abstract notion of the block uh would you think uh uh would be appropriate in order to get some kind of relation like that? You said we already have some idea. I mean uh if you see a formula, then uh one of the formula in literature what is a block somehow is relatively easy to see for Relatively easy to see for that given example. Oh. But what would be an abstract notion of block that would imply or make a reasonable to state a relation like block width versus size? That's the biggest problem. I don't know. Other question? Okay, we'll start with three again. Thank you.