This is going to be an introduction plus a personal point of view on this ILOIDA. So let's start. Okay, so sampling is a fundamental task of machine learning. So what is sampling? So we are given a target distribution, and the goal is to sample points according to this target distribution. And sampling arises mainly in two settings in machine learning. settings in machine learning. So there's first the setting of Bayesian inference. So in Bayesian inference we are given what is known about the target distribution is its density. So we know the density of the target distribution or we know a function that is proportional to the density. And the goal is given this knowledge to sample points according to the target densities. That's the first setting. The other setting is the setting of GANs. So in the setting of GANs what we know about the target distribution is know about the target distribution is a bunch of points that is distributed that is distributed or already already distributed according to the target distribution. So we are given a bunch of points distributed according to the target distribution and the goal is to sample new points distributed according to the target distribution. So for instance, the classical example is the case of BINS. So we are given images of real faces. So we are given images of real faces. Images of real faces. And so this can be seen as points drawn from the distribution of images, from the distribution of faces, and the set of images. And our goal is to generate fake faces, so to generate viewpoints according to this distribution. So this way we generate, this is one point of view that we can take for our forgants. So this way we can generate for the fake faces. Alright, so what I want to show in this talk is a general point of view, general vision, is to show how we can use the extensive toolbox of optimization to design and analyze something algorithms. Because the field of optimization for machine learning is well developed with precise results for a lot of algorithms, for a lot of algorithms. And in comparison, the field of sampling is less well understood. Actually, the the the the convergence properties the the how do we say the non-asymptitic convergence properties of of sampling algorithms are globally well understood uh less understood than the than the than the non-asymptitic convergence properties of optimization algorithms but it turns out that there is a way to cast the problem of sampling as an optimization problem and this way we can import we can use the the toolbox of optimization Use the toolbox of optimization into the field of sampling. It's not direct, but I will show how we can do it, for instance, for SGT. Okay, so the talk will be in two parts. So the first part will be some background on optimization. So it's important to do that because afterwards we will the second part we will stage ten variant descent using this background on optimization. But in the case of sampling, this is optimization in the space of probability measures. In the space of probability measures, so it's not very standard. But we'll provide the material, the necessary material. So the first part will be really an introduction to various things in optimization and optimal transport, and the second part will be really about time virtual emergency. Okay, so let's start. Yes, the flashlight. So when we do math, we like to recognize some patterns. We like to recognize some patterns. Patterns, we like to recognize some patterns of things that we already know. So, when we explore a new field, we like to recognize to come back to things that we already know. This is what I call the flashlight. And there is also a quote by Andrew Weis that says that when you do math, somehow you are exploring a mention. And yes, what you want to have is a flashlight to recognize things that there is one thing that we will use. And there is one thing that we will use in this talk, in this presentation, which is the present-set algorithm as the flashlight. So this is the thing that we want to come back every time because this is something that is well understood and we will try to use that knowledge in the new city. So, what is the business algorithm? So, we want to minimize a function. We want to minimize a function of our RD. We assume that, say, for instance, this function is. Say, for instance, this function is differentiable. And so, the Blue Decent algorithm is an algorithm to minimize this function. So, it will provide a set of points converging to a minimizer of this function. And this set of points is defined recursively. So, it's a sequence. And so to obtain, so given xn, to obtain xn plus 1, one takes what we call a gradient step. So, xn minus some step size gamma n, which can depend on n or can be constant. Which can depend on n or can be constant times the gradient of f at xn. Okay, and this way we obtain xn plus 1. And this sequence is well understood. We know when it converges and so on. We know convergence rates and so on for this algorithm. So it's something very classical in optimization. And one way to derive this algorithm, so why do we do these iterations? So one way to do it, one way to understand why we do it, is to write a Taylor expansion for the function f. The function f. Okay? So there will be a lot of math in the talk. If it's too much, maybe I will skip, but for the moment, it's still okay. So it's just a Terra expansion of the function that we want to minimize. So we are at the point xn, okay, and we want to know who will be the next point, xn plus 1. And we are going to look at are going to look at a nice definition for xn plus one as the form xn plus h. So we have to select what will be h. Okay, and to do that we first we first write a Taylor expansion for f at the point xn. So f of xn plus h is equal to f of xn plus result of f at xn times h plus small o of h. And what we want to find is we want to find the best direction to take the best direction to take the best direction to take to define h okay so h is some direction and we want to find the best h the best direction for for h so what we want to do is actually to find h in the sphere that will minimize f of x n plus h okay but if you want to to solve that problem it can be difficult because it seems to be as difficult as minimizing h okay so so maybe what we will do instead is to look at the first order is to look at the first order term here and instead of minimizing this we will minimize the first order term and this one is easier to minimize and we will take the best H from the first order term. So it's an approximation but it turns out to work well. So when we look at the first order term it's linear, okay it's a first order term so it's linear and essentially this f of x n does not play any role in the minimization. So to minimize it you just need to understand To minimize it, you just need to understand the behavior of h here, because only appears here. And you all know that the best thing that you can do is to take h proportional to minus the term that is here, minus the result of f at xn. So that's why we define xn plus 1 as xn plus h, and h is proportional to minus the result of f at xn. Okay, so if I come back, this is what we do, right? xn minus the result of h. All right. Okay. Alright, let's continue. Okay, so this was just some background on the optimization of our RD. Now, let's go to the space of probability measures, because then, as I said, we will do the same in the space for probability measures. Okay, so I hope this part will not be too boring, but I need to define all the stuff. So, okay, so we're going to work in a So, we are going to work in a matrix space which is called the Wasserstein space. And it's the space of optimal transport. So, P will be the space of probability measures with finite segment moments. W is going to be the two research line distance. So, what is the two restart line distance? So, the square research list, so it's a distance between probability measures. And the square research line distance between two probability measures, mu and mu, is defined as the infimus. Is defined as the infimum of the expectation of the square distance between x and y, where x and y are any random variable such that x has distribution mu and y has distribution mu. Of course, x and y are defined on the same probability space, but there are several ways to couple x and y and you take the inferior couplings. Form of our whole couplings. This gives you the square of a certain distance between mu and na nu. So this infimum here turns out to be a minimum in some cases. So if mu for instance has a density, of course the situation is symmetric here, right? So I can say that for instance if mu has a density, then the infimum here is achieved, so it's a minimum, and the the the the random variables that that will uh that will minimize it Minimize it. So the x and y are actually equal to x and t of x. So y is equal to t of x, where t is called the Bronyer map. So I will pause just one second here. So this means that if mu has a density t, there is a map t such that y is equal to t of x. So this means in particular that y, y equal to t of x has a distribution mu, right? Because y is equal to t of x. y is equal to t of x, so I will write it because it's important. So y is equal to t of x and the distribution of y, so x has a space mu, mu, and y has a split mu. Okay, so this means that the image measure of mu by t, the image measure of mu by t is equal to mu. Okay? So we will say that t is a push forward of mu Is a push forward of mu to nu, to say that the image measure of mu by t is equal to nu. Okay. And also we call L2 of mu the classical L2 space of functions such that the integral of the square number of f with respect to mu is finite. And so the Vasarian space is the space p of R D and with the distance with the matrix W. So it's a mature space. Thank you. Okay, so let me define mu star as the probability distribution which is proportional to exponential minus f. And over the mass of strain space, there is a bunch of functionals that are interesting for sampling. And so the first functional that is important is called the potential. So if you look at the integral of f, so here we have a function f, okay, it's a potential minus. We have a function f, okay, that's a function with f. If you look at the integral of f with respect to mu, okay, and you look at this as a function of mu, it gives you a functional that is defined over the rest of the space, which is called the potential. Now, there's another very important, actually, fundamental for sampling in the Biogen setting, which is called the negative entropy. So, if we assume that mu has a density, we assume that mu has a density, and this density is called mu of x. And this density is called x. So if we compute the integral of x log μ of x dx, this gives us a functional which is called the negative entropy and turns out to have a lot of fundamental properties for optimal transport and for circulation and so on. And finally, the sum of the two other terms, the two previous terms, H plus, so entropy plus potential, so I call it V, so in the talk it will be called V from the beginning to the end. From the beginning to the end. And this V here turns out to be almost the Kilbach-Leiber divergence. So, what is the Q-Back-Librar divergence? So, the Kilbert-Leiber divergence between mu and mu star, so u star is expansion minus f, is the integral of the logarithm of mu over mu star with respect to mu. Okay, so this is the definition of the Quebec-Leibran divergence. So, if okay, if this is. If this is well defined, so if mu is absolutely continuous with respect to mu star, else the cubicle divergence is equal to plus infinity. That was the same here, actually, for the negative entropy. So I said mu has a density mu of x, but if mu doesn't have a density, then the entropy is set to press infinity. Okay, so let me come back to the back-lival diagrams. So this is the definition of the Quebec-Levin diagrams, and it turns out that this one. It turns out that this functional is equal to v of mu minus v of mu star, where v is defined here. Okay, so this is an important property. The Q back Leiber divergence is essentially, essentially, because V of mu star is a constant, is essentially the sum of the entropy and the potential. So this is the message of this slide. Q Black Leiber is potential plus entropy. So if you know the Q back Labor divergence, you know some properties. For instance, it's always non-negative, always non-negative, and it is equal to always non-negative and it is equal to zero if and only if mu is equal to mu star okay so uh this means that if you if we look at this functional as a function of mu it is minimum where when mu is equal to mu star so this is what i said here what i said here if you take mu star proportional to exponential minus f the kl between mu and mu star seen as a function between mu and mu star seen as a function of mu is minimum when mu is equal to mu star. Since the KL is essentially equal to V, then V is minimum when mu is equal to mu star. So V is a functional defined over the Vaster sine space, which has only one minimizer, which is mu star, expansion minus F. So we have reformulated mu star as a minimizer of some functional. And since we want to sample, you will see that we want to sample from expansion minus f. That we want to sample from expansion minus f. The question is, and Neustar is a minimizer, the question is: can we do a kind of a gradient descent for V? Can we do a kind of a gradient descent in the space of probability measures to find Neustar, to target the solution? Okay, any question? No? Okay, so that's the general point of view. Can we take regression descent to minimize V? All right. Alright, if we want to do a gradient descent, we will need research and gradients. We will need a gradient. Okay, we need a gradient. But we are in the space of probability measures. So we need to define a nice notion of a gradient in the space of probability measures. But it turns out that some people did did did it like in the past, like ten ten years ago, twenty years ago. And it's a theory that is well understood. So Theory that is well understood. So there is a differential structure over the Russian space. And basically, so first of all, we need to understand that this differential structure is not similar to the one in Rd, because if we take, for instance, the Versus ingredient of V at mu, it's going to be an element of L two of mu. Okay, so when you compute the Versus ingredient at the point μ, it's an element of L two of mu. So the the space changes. So the space changes. The space changes with the point where when you are computing the Vassarian gradient. Okay, so that's something important. So now, just to give some examples. So for the potential, the Versaching gradient is just the gradient of the function inside the integral. For the entropy, the Versaching gradient is just the gradient of the logarithm of the density. And for the Q backland, for the KL, the Versus the density of the density of For the Kl, the Vashian gradient is just the sum of the two previous Vasarian gradients. Okay, so just a gradient of F, just gradient of F plus gradient log mu. Now, so there's some kind of linearity for the resource regulator. There are some conditions, okay, it's technical, but essentially it's linear. Just to mention, just to familiarize you with the notations, mu star, okay. Mu star, okay, so I will write. So mu star is proportional to exponential minus f, right? Okay, so gradient of f is equal to to uh gradient of uh log of mu star mu star of f okay minus. Great of F? Okay, minus. Great of F is just the greatest of the log of mu stars, because the g the log of mu the minus the log minus the log of mu star is just uh f, so the gradient of f is the greater of the log of mu star. So if so that's why when you compute the gradient of the kl, you obtain the sum of two terms, the gradient of log of mu minus the gradient of log of mu star. Okay, so you can Of mu star. Okay, so you can rewrite it as gradient log mu over mu star. That's something important. Yeah. Okay, gradient log mu of mu star. So it's like a music. So the gradient of K, gradient log mu of a mu star. Okay, so once we have a gradient gradient, we can do a gradient descent. We can do a gradient descent. So it's something that is well defined. And it's as follows. And it's as follows. So given the current distribution, to obtain μn plus 1, one takes the image measure of μn by the map identity minus some step size times the Versoff ingredient of the KL at μn. So this gives you what is called the Versoffing gradient set and you can study it. It has some properties for minimization, for discretizing also PDEs. Also, PDEs. But I will not talk a lot about this album here. Just to see, just to show that you are at μ n and you are taking the steepest descent, which is given by the negative gradient. But we will go back to this vector. Alright. Okay, I hope it was not too boring. So now we can go to Stein Rational and Graduate Descent. Okay, so what do we want to do? We want to sample expansion minus F, okay, and the target solution is good. Okay, and the target solution is u star. So u star is proportional to expansion minus f. So in dimension one, so of course we want to do it in dimension one billion, okay, but I will just illustrate in dimension one what happens. So we have the target distribution here in blue. Okay, so this is the target distribution. And we have a bunch of points that are distributed around minus 10. Okay, and here in green is the empirical distribution of these bunch of points. Distribution of this bunch of points. And what we are willing to do is, with an algorithm, move the points in order to their target distribution, to their employee distribution, to fit the blue curve. So this is what we obtain at the end of the alleyway. So it's really some particles here that are moved so that the operator solution of the particles fit the Alright. So now how do we move the particles? So in this example we use the spy version of USISAT, which is an algorithm that was introduced in the past. And it works as follows. So the particles are called x1 to xn, okay? And they are moved as follows. So the particle, so I will call xin the location of the particle i at time n. And this is how n. And this is how the location of the particle i at time n is updated. From x i n to obtain x i n plus 1, when does x i n minus some step size times the average of something, the average over j of something, and what is the thing? So this thing is the gradient of f at x j n, so the location of the particle j at n, k x at n k x i n x j n minus k dot of k x i n x j n. And here, k of x y is a kernel, so it's a function, essentially, which is associated to a reproducing kernel hilbert space. So I will tell you in a minute what is our reproducing kernel Hilbert space, but curve x y is just a function, actually actually this is this is well defined as soon as k is a function. We don't need k to be the kernel associated to our organization kernel. Kernel SSC or condition kernel in parspace for the definition of this iteration, right? Just take any function of two variables that is symmetric. Okay, we need it to be symmetric. But here for S V G D, S V G D is connected to the theory of reproducing kernel Hilbert spaces. And a reproducing kernel Hilbert spaces is a Hilbert space of functions. Okay, so it's a Hilbert space. Okay, so it's a Hilbert space of functions. So, here in particular, here in this presentation, in the case of SVGD, actually, if you think about the problem, H will be a subset of L2 of mu or any mu. But essentially, it's a Hilbert space of functions. And this Hilbert space, which has some properties, and in particular, there is a function kxy such that kx dots, so kx. That kx dot, so kx as a function of the second variable is an element of h, is an element of the Hilbert space of function. And there is a reproducing property, which is that if you take any function f in the Hilbert space H, then the inner product in H, this is the inner product in H, not in L2, not in, this is the inner product in H, of F times Kx dot is equal to F of X. So we have a way to So we have a way to write f of x as an inner product between f and this function kx dot. So this is the repository property, but it's important to understand that this is the inner product in H. Okay, so k is something that is a distinguished function in H, and we take the inner product in H. Do we have a way to write evaluation as inner products? As inner products. Alright. Okay, so strong-axial agreement is an algorithm that was applied several times in machine learning, and this is just some examples. So we have examples in reinforcement learning, in learning version of auto-encoders, in auto-sequential decision making, and a lot of applications of SVGD. But what we are able to do here is just some theory on Here is just some theory on SVGT. So we do have some results in theory for SVGT. I will mention them at the end. But what I want to do now is to derive... Okay, so this is an Gradient descent from the principles that we use for Gradient Scent. This is what I want to do now. So this is where there's a spine operator. So, this is where there's a spine operator and so on. So, okay, so record. So, mu star is proportional to expansion minus f, and we have we have rewritten mu star as a minimizer of the kl between mu and between mu and mu star, or as a minimizer of v as a minimizer of the functionality. Okay, so mu star is a minimizer of the functionality. Okay, so first, we want to do a kind of a breakdown set. So, what we did first was to Of a business set. So, what we did first was to write a Taylor expansion. Okay, so we need to write a Taylor expansion to decide the kind of a Taylor expansion for the function that we want to minimize it. So, here the function that we want to minimize it is the K. It's the K. So, let's say we are at a point mu n, at the distribution mu n. We are at mu n, and we want to find the next distribution. We are at mu n. We are at mu n and we want to find the next distribution mu n plus 1 in the form identity plus h push forward mu n. So we're at mu n and we want to find some function h so that we can push forward mu n by identity plus h in order to find mu n plus y. That's what we want to do. We want to find the best h. And it turns out that we do And it turns out that we do have a Taylor expansion for this functional. So H is the thing that is going to be small, okay? There's a small row of H. And you can write that the KL of identity plus H before mu n with respect to mu star is equal to the K L of mu n with respect to mu star minus something plus small o of h. And this something here is something linear. Okay, so it's really. Linear. So it's really linear in H. So it's really a first order expansion. Okay, so it's really a first order obtainer expansion. And so what is this? So it's the expectation and value of some function, some function s star of h. And this function s star of h is minus the greater of f times h plus divergence of h. Okay? So it's really something linear in h and this star of this linear operator, estar is called the This denial operator estas of the Stein operator. This is where there's a strain operator. I'm seeing I forgot. Anyway, so this is the Stein operator, and we really wrote here a first order test. We really wrote here a first order Taylor expansion with a mineral term which is given by the Schlein operator. What I wanted to say is about the notation. S is called S star because it it actually depends on mu star. Okay, because mu star is expansion minus f and s star depends on the gradient of f. Okay, that's why I called s star by I should have called it s mu star, but I didn't want to have a okay. S star is easier to pronounce. But okay, s star really depends on mu star. Okay, so now we want to take a gradient step. So we need to, if we want to do the same, we need to minimize the linear part of the Taylor extension. So if we want to find the best direction H, here, the best direction H for the derivation, to design our algorithm, we need to minimize the linear part. How do we minimize the linear part? Well, it's not. I mean, if you write things like that, it's it's not trivial because, okay, this does not depend on h. So this we can get rid of it, but how do we minimize this over the sphere? And I don't know. For the moment, I don't know. And also, h is a function, so the norm here is not defined for the moment. I wrote that I'm minimizing over the sphere, but I did not tell you which sphere actually. Okay, so I need to specify the norm here. Specify the norm here, and once this is specified, I need to minimize, to do the minimization. So, but that was something important in the past, previously when we studied Bradley Sense, is that the linear term was written as an inner product. It was written as an inner product between two vectors. It was like H times something else. And that's why we managed to find what is the best H. H is actually the vector that appeared. That's the piano. So, if we can write this as a linear product as well, we will be able to find H okay and the best H and the best direction. And actually, we can do it. We can do it. We can write this as a near product. Okay, so sorry for the several lines. I will read the lines carefully because actually there's almost nothing. There's only one integration bypass, which I will not justify. So, okay, so what is So okay, so what is expectation expectation under mu of s tar h? Alright, so it's an expectation. So it's an integral over mu, an integral under the distribution. And estar is the sum of two terms, graduate of f times h, and divergence. Okay, so here I just rewrote what is the definition of this term. Okay, so integral value of two terms. integral value of two terms greater than f times h plus divergence of h. The first term here, sorry, the first term here, I will not touch it, so it will remain like that along the computations. But the second term here, so it's the integral, so if I write it with x's, so it's divergence of h at x times mu of x dx, okay. Integrate, okay, so the integral of this over x. And so this is the key part, key part here. We can do an integration by part to change the form of this term. We can put the derivation here, the divergence is essentially derivation, on the other side. So we can say that divergence of h times mu is actually h times greater than mu. Okay, so this is assuming. Okay, so this is assuming proper boundary conditions, but μ is a density, so it goes to zero at infinity, and you know, h is well behaved, and so on. So, essentially, this is true in several cases. So, here I've done the integration by parts to write it as h of x times gradient of mu of xtx. And then, to finish, I just say that gradient of mu Just say that greater than is equal to greater than of mu times mu. It's just a small modification. And in the end, this term here is written as the integral over dv of x of h of x with the total of mu of x. So really there is only one thing that is difficult here, which is the integration background. Okay, and in the end I can put everything on the same In the same integral. So I will have the integral of h times gradient of f plus greater than log mu of x. But recall that gradient of f plus greater than log mu of x is, as I said before, greater than log mu over mu star. Okay, greater log mu over mu star, you remember the music. So greater than log mu over mu star times h. Okay, so in the end, this is the gradient of K L. So in the end, I have minus the inner product. So in the end I have minus the inner product in L2 of mu of h times the greatest of k. So this Stein operator here is actually an expectation and value of the Stein operator here is actually times the gradient of K L in the inner product induced by L total. Okay, so Okay, so once okay, so once once once I I know that, uh so there are several consequences of of this. So first of all, um there's the Stein identity because this is a workshop on Stein's method. So if we take mu equal to mu star, we can take mu is equal to mu star, we will have d mu star over mu star so this will be equal to zero, okay, this will be equal to zero, and we obtain the be equal to zero and we obtain the Stein identity that if mu is equal to mu star expectation of s of expectation of s s star of h is equal to zero okay this is because the the gradient of k l at mu star is equal to zero okay recall that k l is minimum at mu star okay so somehow the gradient has to be equal to zero at minimum at minimum so that's why this is equal to to zero at min mu is equal to mu star. Another justification for for this. Justification for this. Okay, so this is the first consequence. And second consequence is that if we take the norm, in our minimization problem, we want to find the best direction H, if we take the norm induced by L to take the norm induced by L tormu, then we know who is the best H. The best H is given by this research ingredient of KL, okay? Because I have the interconnected. Okay, because I have the interconnecting L to mu, so the best direction it will be proportional to this term. The best direction is proportional to the result of k. Now, what we do in SVGD is to do something else. And I will tell you later why we do that. But we do not take L to of mu as the norm, but we take for the norm, for the space where we are working in, we take an arc Hs. We take the space H. And I will tell you later why it's important to do that. Tell you later why it's important to do that. And if we take H instead of L to of mu, then we are not done because how do we minimize this in H over the RKHS? We don't know. We don't know because the inner product in the RKHS is different from the inner product in L2 of U. So what we need to do is to rewrite this again as an inner product in H. Okay? So this is what we need to do if we want to apply the same principle. To apply the same principle to find the best direction. But this is doable, and I promise this is the last time there is a sequence of equalities like that. So we can do it. We can do it. So if I restart my computations from here, so I know that this is equal to this. So I restart from here. I restart from here. So I know that this is equal to the integral of the certain gradient of KL times H derived. certain gradient of K L times H d mu. Then I can use the, if I assume that H belongs to the archaeologist H, then I can use the reproducing property, the reproducing property that tells me that H of X is equal to an inner product in H, so inner product in the space capital H, H times Kx dots. Okay, I can use the reproducing property. And if I use the reproducing property, And if I use the reposition property here, then I can put okay, so this okay there's many things happening from this to from here to here, but basically what happens is that this is an inner product in H. I can put everything in the inner product in H. Okay? So I will put everything, the integral and so on, everything inside the inner product in H. And this way I obtain that this is equal to an inner product in H of the function. h of the function small h times a big integral here, a big integral here. So this is an element of capital H, it's a function written as an integral. And this function is the integral of this vast searching result of Kl times KX dot. So from here to here, what happens is that I took this inner product. I took this inner product and I put everything on the right hand side of the inner product and I keep single h on the left hand side. So this way I obtain this big term here, but the expression of this term is a bit ugly, but I can rewrite it as a linear operator applied to the vast stretching gradient of K L. So I can rewrite it as a linear operator applied to the vast stretching gradient of K L because this is the vast. Because this is the vast main gradient of K L. And my linear operator will be essentially a convolution. It's a convolution. So I call it P nu. And P nu of f is the interval of f of x kx dot. Okay, so it's a convolution between f and kx dot. Somehow, you can view it as a convolution operator. But essentially, okay, so what is the message? This is a bit complicated, but this term here, this linear term in our Taylor expansion, can be written as an inner product in capital H of Product in capital H of small h and a function. Function that is given by P mu times the versus ring radio k. That's the message. So from this, so now we know we know who is the best H, right? If we take, if we are working in capital H, what is the best H? The best H is proportional to this thing. So the best H is proportional to this thing if we take capital H as our space. Our space. And so this way we found the best edge which is proportional to this. So this way we obtain the iteration, the algorithm. It's not the actual algorithm, but it's almost the algorithm. So from mu n, to obtain mu n plus 1, one takes the image measure of mu n by identity minus. Identity minus some step size times this ugly term. Okay, so that's what I wanted to say. I'm not done yet, but almost. Just some remark. What is this term here? So it turns out that if you look at the A thing here. So, this minimization problem. And you look at the value, the minimum, the value at optimum, the maximum of this thing when you plug in the best H. Then it is equal to something called the Carnell Ash time discrepancy between U and U star. And the K, so the K S D, Carnellage time discrepancy. And it turns out that the K S D was something that was introduced. KSD was something that was introduced prior to SVGD to compare probability measures. So this algorithm is really related to the KSD, and it naturally appears in the derivation of the algorithm. So the KSD is something used to compare probability measures. In particular, we know that, for instance, when the KSD is equal to zero, mu is equal to mu star, typically, under some conditions, of course, but typically this is what happens. And also, we see from We see from this formula here that if I take the best H over the sphere, the value will be just the norm of this ugly term, the H norm of this ugly term. So when you plug in the best H, you obtain the the the H norm of this ugly term. So the K the K S D is actually equal to the H norm of the the ugly term. Lawn of the ugly term. Alright, so this is the update. Now it turns out that the ugly term is not that much ugly. It's not actually ugly. It's actually beautiful, actually. You should say it's beautiful because now I can explain to you why we choose H as the underlying. the the the the underlying underlying space and not L to L. Because when you take H, you have this P mu appearing here, and it turns out that greater of K L is greater than log mu over mu star. So if you want to compute greater log of K L, greater than K L, you need to know the density of mu. So in practice, you are given a bunch of points distributed according to say mu n, and you want to duplicate the mu. new n and you want to do something for instance push forward the points but usually you don't know the density of the you don't know the density of the of the points the density of the samples okay you just have the samples so you don't know the formula for them for them for their density so you cannot use the g dot of kl because the g dot of kl relies on the density of uh of uh of uh of of the of the of mu but it turns out that this when you apply p mu This, when you apply P mu here, when you apply P mu, it turns out that you can make an integration by part as we did here, okay? Same kind of computations as here. So you can do this kind of computation and show that this actually does not rely, does not involve the density of u. This is just an integral of a function under. And with respect to the distribution, this is just an integral of a function with respect to the distribution. So, this is the, in my opinion, the beautiful part, beautiful thing in this algorithm is that you would like to do a gradient gradient set, you can't, because vast gradient set relies on the density of the current distribution. But when you apply this PU here, actually, the Vaster String Graduate no longer relies. No longer relies on the density of mu. It's only an integral with respect to mu. And we know if you are given samples, you know how to compute integrals. You can just apply the law of Lafayament numbers. So that's why when you implement the algorithm here, here, this thing is just an integral with respect to mu n. This thing is just an integral of some function with respect to mu n. With respect to mu n. So say you are at mu n, you are given a bunch of points, x1, xn, distributed according to mu n. You are going to push forward the points by this map. But this map is just an integral respect to mu n. So you can use the current points to estimate the integral with respect to mu n. So apply the law of large numbers. So this is just a particle version of this. So the particles are distributed according to mu n. And I push forward the particles by a map. So this is a function of xin. And this map is an integral, but I can approximate this integral using the points that I have that are distributing according to mu n. Don't know if it's clear. Any question here? Don't know if it's clear. Any question here? So this is okay, because you want to implement this scheme. This is a scheme of our probability measures. You cannot do it directly, but since you have particles, and since every functions here are written as integrals, we can approximate integrals by the particles. Alright, so this is how we derive the algorithm. So there is a discretization step in the end. But this is really the SVGD algorithm. SVGD algorithm. Okay, so to conclude, P mu of the gradient of KL is not exactly the vast searching gradient of KL, but it can be seen as a vast searching gradient under the metric induced by H, under a modified metric. And this change of metric makes the iterations tractable. You would like to use. You would like to use the greater top KL, but you can't, because the greater top KL relies on the density. But when you apply, when you use this thing, when you use this thing, it becomes an integral respect of mu. So the directions become tractable. So really, SUGT is just a gradient set under a new metric. Just a gradient scent under a new metric, and this is explained in a bunch of papers. It's just a gradient scent under. Uh it's just a very simple new metric. So um you wanted to say something, I forgot. Uh yeah, so just one advertisement, shameless advertisement for my work. Since it's a gradient scent, you can just analyze it as a gradient scent. So this is what we did. So if so first we established that if so it's there's a tail condition, which is of course it's a Of course, it's not just plugging in the analysis of gradient scent, but from a high-level point of view, it's establishing gradient-scent results for SVGD. So, there is a condition, but you can prove that mu n converses to mu star, and you can also establish the complexity of SVGD. So, to achieve epsilon accuracy in terms of the case display. Sorry? The condition. Yes, thank you. That's the typo. So the conditional. It's a typo. So the condition is the NW star. So, and you can establish that if you want to find the point, so this is for the population limit. Okay, so this is not the real SVGD. Not the real SVGD. This is just this scheme. The real SVGD for the moment we don't know how to do. And it can be one of our problems that we studied this week. But for this scheme, if you want to find a measure that achieves epsilon accuracy in terms of the KST squared, then you accuracy in terms of the Ks d squared, then you need due to the power of 1.5 over epsilon deterrent derivations. Okay, so I stop here and thank you for listening. Okay, thank you very much for the nice talk. Are there any questions? Yeah, maybe if the virtual participants have questions, maybe. The virtual participants have questions. Maybe you could write those in the chat. So anyone who's going to write to you online, so you know, they should be able to speak up. Okay, very good. Thanks for that. So you could speak up. While we're waiting, so this is very tied to the KL distance, right? So I just wonder, you know, is there any version of this, if you, another distance, is there any hope of getting something like this to work out for some other divergence or metric other than the KO? Yes, so what is what is nice with the KL is that we really understand very w well how So you see there's okay there's something that I didn't talk about in the in the here Which is the continuous time point of view of all these things actually all these things are written in discrete time okay so there's an iteration content Okay, so there is an iteration counter which is n, but they are inspired, they can be seen as a discretization of a continuous time dynamics. Okay, and that continuous time dynamics is called the gradient flow of KL. And it turns out that the gradient flow of KL is well understood, is well understood in the theory of optimal transport. But absolutely, maybe we can imagine taking the gradient flow of something else and discretizing. ask and discretizing but the thing the thing that that would be difficult maybe is would would the magic would the magic happen here right now would the magic happen here right because we need if we change the k here we put something else we need to be able to compute the version result so here there is this very nice trick this is one way to view the thing but there's very One way to view the thing, but there's this very nice trick that allows us to compute it. And also to mention something else: here we view SVGD as a dynamics to minimize the K, but you can also view it as a dynamics to minimize the T square. So this is explained in the paper by this paper here. So you can view it as a dynamics that minimizes the K. The K because essentially this thing here, this P mu greater than K L, it is also equal to P mu star, P mu star greater than K square. This is another point of view that you can take on the problem. Thanks for that. I see that we have both Gacina and Max have questions. Maybe Gazina, do you want to go first? No, I've already put it in the chat. So something that... Can you hear me? Yes. Okay. Something that puzzles me about the low large numbers. So it's a beautiful result. So you have your samples and then you know it converges. But for example, using size method, but also other methods, you get a bound on the rate of convergence with the law of doubt numbers. And we know that the bound gets smaller when gets smaller when the observations are negatively correlated, because they depend on the variation. So often from what I see in SVGD one does samples the exits in somewhat independently from each other. We update, update, and update in this particular way. I wonder whether people have looked at particular ways of creating negative correlations within the samples will make conclusive businesses. Yes, that's a good question. So when we implement as VGD, when we implement as VGD, which is here, I said law of large numbers, but actually that's not correct because SVGD is a deterministic algorithm. It's deterministic. So really you take samples, I say samples, but actually it's just points that you push forward by a deterministic map. From up by a deterministic map. But you hope that the empirical distribution of xin would be close to the real mu n. And this is something that you can show, actually, it's shown in maybe this paper or the next one, that somehow, as the number of particles. As the number of particles increases, then you approximate the mu A. Something like that, shown in the paper. But really, it's not well understood how to go from this to this. So maybe there are some six marks to do, like introducing negative correlation. Maybe I was thinking last time, maybe we. I was thinking last time maybe we should randomize because I said SVGT is deterministic, but if we randomize, maybe I mean, it's not clear to me, but yeah, I think it's important to be able to study that. For the moment, we don't know how to do. But yeah, if you want to think about it, I'm okay. Thanks for that. And Max, do you want to take a turn? Sorry. One thing before I have a question. First up, I wanted to make a remark on, I think it was a question from Larry and the audience person. About the question about using better stuff than the tail distance. So there are people in PD who stuff with spong. Maybe not the the static variation. Maybe not the static variation really the same, but like PD versions of this project with any entropies. And then the PD that comes out is the gross medium equation. And 10 or 10 listings and they have efficient simulation from what I understand. The key point comparing to kind of anti-netwasters is that the integration by parts formula that comes out with this is going to be non-linear. Now the fact that the entropy production and the gradient is density form and the density with byparx, you get the current gradient byparx formulas. If you start using other advantages, there is no reason for it to be production with radicate when the non-linearity ratio type formulas, like the weather possible medium equation. Because that might be connected by table distances. The microbular distance is particular to the unit is actually you have all the infrastructure everything. One thing I was wondering about this okay. This is a bit weird, but it's about the order of magnitude. So okay. If I think about the artwork you propose in the simplest setting, so you're doing gradient distance, so let's look at the situation where the KL function is unbiased as respect space. So you have a lot of concern messages. So you have a lot of okay density and then okay the first thing that would come to mind to me to step would be to write stochastic creative in the sense to run it sorry? To run stochastic created in the sense then okay if you look at the mixing okay simplest case in the world you look at the Gaussian case you want to use this to decyple a Gaussian measure that's a good way of doing it whatever the next thing Whatever. The mixing time of stochastic gradient is going to be in the bug D. And you were showing this estimate and the convergence to be outworked in that way, whether like each part we have. I was wondering, is by example too simple and in real life do we get better page than the algorithm we propose or the compare how does it match compared to stochastic grain it's actually For the purpose of minimization. Okay, so but you are talking about stochastic gradient set to minimize the function f? No, I mean okay, so replacing the gradient here by a stochastic gradient. So I'm taking the PD that comes out from the no, no, I'm not talking about stochastic gradient descent on battery space, I'm thinking about on phase space. So the P D that object the radius. After we bring the center of relative entropy in plaster state space, is it this because of the planet equation? What is time unit the evolution of I wrote precise in time is stochastic gradient percent? Ah, okay, so okay. I run sorry, I I was okay, so that that's the duality. So if you if I understand well, so you take uh uh You take uh you take uh the you take uh the the Foucault-Planck equation, right? So this is a dime dynamics in the space of measures. But it turns out that one way to realize this dynamics in terms of uh in terms of random variables is uh Langevin equation like so I will write for everybody so so there's this okay so there's a continuous time graphic of uh so Uh so uh okay so fine because otherwise people so if you if you forget okay if we forget about the P nu here okay let's say let's say that P nu n is equal to identity it's never equal to identity but let's say that it's equal to identity then we obtain the Vastering set right and turns out the continuous time analog of the Vastering set is the Foucault Planck equation so this is Foucault Planck equation okay this is the continuous time so this is Okay, this is the conscious step. So it's the dynamics in the space of measures. It's a dynamics of mu t where t is a positive number. And it turns out that these dynamics of mu t, there is a stochastic differential equation such that the stochastic differential equation such that the the distribution of the random variables are equal to mu t and this is this one. And this is a vonian motion. Okay, so if you take this SDE, you look at Xt is the only variable, and the distribution of the random variable will be ut. And so the question is about the mixing time, right? So do I think of just the mixing time of this dynamic? Is it like a D or P? I'm not sure if this is too simple an example and your algorithm would also be to parody half of this example. Ah, okay, so you want so basically, okay, so so when you discretize this, you obtain an algorithm that is called Langevin algorithm. That is called Langevin algorithm, and I think you want to compare SVGD to Langevin algorithm. Is that the question? Okay. Okay, so Langevin equation is the properties of Langevin equation are really well understood now, and there's much more work on Langevin equation than SVGD. What I can tell for the moment is that the only non-asymptotic result in terms of mixing time that I'm aware of. Mixing time that I'm aware of is this one. And this holds in terms of the KSD square. In terms of the KSD. Whereas for Langevin, for Langevin equation, for Langevin algorithm, the results, I mean, the KSD does not pop up in the analysis of Langevin, right? So I don't know how to compare. Yeah, but I don't know how to compare this. So I don't know how to compare this. Compare this, so I don't know how to compare this result to the results in Langevin because I don't know how to compare the matrix. But still, one thing you one proxy you can use is that okay, controls W1, I suspect, it's right. At least for nice and nice target distributions. Yes, ah, yes, and for W1, the darker both. Yeah, yeah, that's it. Okay, yeah, okay, yeah, probably, yeah. So, yeah, it's probably possible to compare, yeah. Possible to compare here. But it controls W1, but with an additive term, I guess. I don't know. Okay, I know that we can relate as a case D to W1, so this is. But I don't know if we can, you know, because if there is an additive term, maybe we are stacked. I don't know. I need to think about it. But I don't know for the moment. But what I was about to say is just that these results on Nonjoin, they, I mean, I don't know how to compare the case. I mean, I don't know how to compare the KSD with the metrics that are used in the metric in Bourgeois. But just to mention, there are some works that try to understand the topology of the KSG, right? There's Gohan McKee, but I'm not very familiar with that. I don't know how to compare. Okay. Okay. So I think those are all the questions. And thank you again for the very nice talk. And we're going to take a little break and reconvene at 30 minutes past the hour. Okay. Thank you. Thank you. Just to remind you, I think I'll stop recording by. Yeah. Has anything else been like?