I'll have to make sure that setup is okay here. Okay. If you want, you can make your you can change the panels around so we can see your face a little bit bigger. If you'd like. Okay. Can everybody see my slide here in full screen? Here in full screen, yeah, okay, okay, okay, perfect. Uh, okay, hello, everybody. Um, uh, so today I'm gonna share with you some of our recent work about model building and shape recognition for Crow EM using a new methodology that we developed called FFF. Okay, so. Okay, so Sonia has introduced most of the background, so I'm not going to repeat it too much here, but except one thing is that out of the whole pipeline of the CrowEM workflow, we're focusing on the last part. So it is after you have done all the experiments, collect all the images, and have done the Have done the 3D reconstruction, and now you would like to convert your 3D reconstructed maps into 3D structures, which means atomic models. So what are the challenges? So this step sounds very easy if you really have very high quality Crawl YM map. Unfortunately, most Crow YM maps are. Unfortunately, most query Maps are a bit noisy. And that noisy noise in it not just includes the background noise, but also stuff that you probably are not very interested in. For example, if there are lipids or nanodiscs that Sonia mentioned, and also maybe some other molecules that exist in the solvent that were captured. That it will capture during this 3D reconstruction process. And essentially, the goal is to convert this very noisy sort of density map, query M density map into a very precise atomic model. And traditionally, there are many methods to do it, and I'm going to show a few. So, the most commonly used, obviously, even after. Commonly used, obviously, even up to today, are sort of a very manual labor-based method, mostly using software like Akut or Phoenix. Coot is sort of a very user-intactive sort of model building method where the user just trying to build residues one by one. And Phoenix add a bit of automation into. A bit of automation into it where it can do like structural optimization automatically, but it takes a very long time. This usually take from one week to two weeks for a single person to build one structure. There are also semi-autod methods where if you get a good initial guess of your target structure and you have a map, and there are And you have a map, and there are sort of monocynics-based method. One example is called MDFF. And once you align them, then MDF will take care of the rest. Another sort of related field is protein folding, which doesn't require any experiment data except for the protein sequences. And this has been very sort of a hot topic in recent years. Topic in recent years starting from 2021 when Alpha Fold 2 came out, and then in the same year, Rosetta Fold, and also in this year where people are starting to build even more interesting machine learning-based model called ESM fold. And all these methods pretty much don't require manual labor, but the quality of But the quality of those predictions, the sequence-based prediction, structure predictions, can vary. And also, they have very a lot of limitations. And one important limitation is that they do not take into account the sort of multimeric structure, although people are trying to do that. And also for proteins with multiple stable conformations. Confirmations. This is still sort of an ongoing effort trying to do that based on sequence alone. And so we're trying to sort of bridge those areas by sort of taking advantage of those putting folding predictions and also the CRALEM map, which is experimental information. Information and combine them in a way that they will be able to capture the structure multiple structures confirmations based on the experimental information. So the basic pipeline of this algorithm can be divided into sort of two parts. The first part is if we take the Cryan map and go through deep learning. And go through a deep learning neutral network that can predict the amount of features, which I'm going to explain later in detail. And by combining the sequence, it will be able to predict sort of fragments of protein that are mostly just C alpha choices. And also a sort of a cleaner version of the initial map, we call it the backbone probability map. By the backbone probability map, and by combining those two predictions with initial structure from, say, alpha fold, and by going through a sort of MD-based protocol, then we can get a final optimized structure that will be well fit into the spartan map, but also taking into account all the predicted structural features from the principal. Uh, from the pruning folding. So, first, I'm going to talk about this sort of the structure of map recognition module. So, this essentially we take, so they're pretty much divided into two phases. One is training phase, there is the inference phase. For the training phase, we do collect about hundreds of those experimental crying maps and In maps and predict let the model to predict five different features: the CL probability, CL positions, and something we call the pseudo-pepital vectors, and the amino acid types, and lastly, the polling backbone map. I will spin them later in detail. And for the deep learning network, we pick sort of off-the-shelf sort of Um, off-the-shelf sort of network called 3D RatinaNet, which is typically used for image segmentation tasks. And this where you can input your implementer here, and it will go through layers, layers of abstraction or embedding. Eventually, you will output those four different features plus a backbone probability map for the CR probability. For the CR probability, essentially you can think of the EM input EMAP as sort of a box of smaller containing smaller boxes for the each sort of, or we can call it grid cell. For each grid cell, we predict a probability from zero to one of how likely it is for this grid cell to contain an C alpha atom. And for that, We sort of use two different types of loss functions. One is called a dice loss, which is usually used for image segmentation and also binary cross-entropy loss. By combining those two, we hope to achieve sort of a better balance between the preciseness or the grid-wise. or the grid wise or the sorry yeah so so um pixel wise accuracy and also the sort of take care of uh taking care of the sort of the low representation of the number of cells that are considered with the C of atoms and then there's an the second type of PDF feature is the C of position which is which is Which is uh, which is the uh, we predict sort of the relative position relative coordinates of the CR atom within each grid box, and each box is sort of divided into a two by two by two um uh cube uh where it we use this size uh because it's pretty much like we can only make sure that there is only one self-atom within that box. Uh, and X is sort of the predicted offset. Sort of the predict the offset, which is the coordinate between zero and two Arnstrom. And the third one is what we call the pseudo-pector vectors. Essentially, it's a vector pointing from one C alpha atom to the next C alpha atom. And this is used as weight for later connecting those C alpha atoms into peptide chains. To hepita chains. And V is just the direction between the C alpha, two C alpha atoms. And the fourth one is the amino acid type because we're not only interested in where those C alpha atoms are, but also what type of protein amino acid residue it is. There are about 20 About 20 types of residues exist in nature. And for all those 20 of them, we sort of, as a sort of optimization goal, we want to optimize the conditional probability of given a cell that has a CL of atom, what type of amino acid type it is. And there is, this factor is just. This factor is just sort of trying to make sure that it is the right amino acid residue type. And once we have done the protein feature prediction, so all those four, we can generate small fragments of the protein that are not only has position coordinates. Only has position coordinates, but also what type of amino acid it is. And by combining with the genetic label here, oh, sorry, yes, here. So with protein sequence, we can make corrections to the predicted amino acid types. And also, so that will increase the accuracy of those fragment predictions. And then we take the predicted backbone probability map with the protein. With the protein folding predictions to go through an MD-based protocol. So essentially, it goes through sort of a bias MD where you can think of as conforming those initial structures into those to match those predicted fragments. And also later, a second part where A second part where the structure will be fit into the map. And that is sort of we use, there are many ways of doing it. We took the sort of the MDFM method that I mentioned before, where the map you can think of as sort of a representation of the electrostatic field generated by those atoms. By those atoms, and we want to convert it into some sort of potential energy that can drive MD simulations. So we go through for each map, we first remove the background or the solvent density, and then we compare, so we convert this sort of cleaner vertical cryan map into a yam potential grid. EM potential grid. And this grid is later put into the MD engine using sort of analytical, smooth analytical version, usually using polynomials. And here is sort of an example of how this thing works. So here is the sort of a structure with color by the degree of deviation from the target structure. structure so the uh the the blue color represents a very close target structure and the red one red ones represent the structures that are far away um and those uh so after uh the fitting then this usually uh become a good very good fit um and here is sort of a um a movie of uh Of how this MDF really works. So essentially, it converted the map into the potential energy that I mentioned. And for the specific part that are sort of outside this map, because we built this potential energy based on the Cry Year map, and you'll be able to just follow the gradient. But just follow the gradient and go to lower potential energy, map-derived potential energy, and it will be fit into the CryM map. So to give you an example of how well this method work, so we take a very sort of a representative class of protein called membrane transporters. So those proteins are Necessarily multi-conformational because they have to be able to transport small molecules from one side of the membrane barrier to the other side, and that requires conformational changes. So, usually there are at least two major conformations. One is what they call outward facing, which means one for the protein domains facing the outside of the cell membrane or the cell. Or the cell, and the other being facing the opposite direction. And they alternate between those two major states to convert, to transport this shown as a sphere, small sphere, the cargo from the outside of the cell into the cytoplasm of the cell. And here is a comparison of okay, so here. Okay, so here I'm showing on the top left corner, I'm showing the two major confirmations, one in blue and the other in red. The blue one is the sort of the what they call the outward facing open state and the red one is the inward facing open state. And both of them have so they are corresponding to the same protein. And for protein folding predictions, it actually only can only predict. It actually can only predict one of them, which is corresponding to the inward-facing state. And if you calculate the sort of the distance or the similarity between the predict structure to the type structure, which is the blue one. Sorry, here we're using sort of two types of commonly used metric. One is called TM score, a number between One is called TM score, a number between zero and one, uh, the higher the better, and the other one is the RS score, which is number uh greater than zero, greater or equal to zero, the lower the better. So here we can see that the TM score is sort of 0.5-ish, not very good, and also the MRC is over 10 as shown because this is corresponding to the other state. If you apply the semi-automated MDFM method, MDFM method, it does improve the quality of structure a little bit, but not very much. So the final structure is still sort of staying at the inward-facing open state. And with the MDFI, because it has a good idea of the location of the target atoms by through the By through the map recognition process, it will be able to pick out the signals from those maps and be able to do much better targeting and do a better job than those two methods. And here is sort of our effort of trying to make sure that people can understand and also can. People can understand and also can use it easily without actually having to worry about installation and other things. So, one thing we did is that we put this into sort of a Jupyter notebook, but we sort of polished it into something that we call the Borum notebook. And this is sort of a runnable notebook where you can actually explore. Notebook where you can actually store the algorithm using Python code. And here we also build for people who do not want to code, just want to try to see how well this thing works. And we have this sort of web-based app called just got FFF. And you can people just upload their info map and the And the initial structure and sequence to be able to generate the final optimized structures. So here are some conclusions from our sort of this study. So the current protein folding algorithm are still very limited that at least three major ones. All of them can do pretty good job on single chain. On single chain, single confirmation protein confirmation prediction, but they face quite a bit of difficulty for predicting proteins with inherently multiple confirmations and also for other more complicated behaviors such as protein ligand intactions. And for the existing CRI-EM structure billing method, they usually Computing method, they're usually very sort of inefficient or inaccurate because they do not actually have a good understanding of the map density or the correspondence between the map and the atomic structure. FF, we design FF to be sort of a bridge between those two areas of research that we hope can help people doing quality research. Help people doing query M research to be able to build a very accurate protein structure out of those noisy query M maps and also have very sort of complete like a whole whole structure model rather than something just limited to those protein criminal maps alone. Lastly, I want to thank Lastly, I want to thank two colleagues, Wei Jaeqin and Xinyan Wang, who are sort of co-we collaborated closely in this project, who are also the first two authors of this work. And also my colleague, Jifeng Gao, Gordin Ke, and Dani Feng Zhang, who actually provided quite a bit of Good suggestions during this project, and also the Department of Algorithm Design at my current workplace, which is DP Technology. And also, I want to thank all of you for listening. And I have put my email here. If you have questions, you can ask right now or email me later. Yeah, thanks. Any questions? Yes. Can you hear me? Yes. Okay. Thanks for the talk. So maybe I'd like to get back to the slide where you were showing how you would combine the ASASOX detection with the fragments and Fragment and from the sequence, right? I yet. This one. So I'm not sure how sprouting fragments, how do you output them from the sequence? I guess this is like a partition of your sequence. Sorry, could you say it again? These protein fragments are the are they my guess is that they are just a partition of your whole sequence. A partition of your whole sequence? You're giving a full sequence and you get different protein fragments. My understanding is correct. So this, okay, so for this sequence, this is, so there are two purposes. One thing is for the fully folding obviously. The other one is that we use it to make corrections to our predicted amino acid types. Amino acid types because so the map usually have are usually very noisy, sometimes like can be like difficult or ambiguous in terms of the amino acid type predictions. But if you actually consider their neighbors, or then we can, for example, if you if the sequence is AG, but our we predict it to be AGG, then we can sort of by Then we can sort of by using the input sequence to make prediction that if they match, like to make sure that they have the best match and use that to make the amino acid type prediction more accurate. I'm not sure I have answered your question or not. I think it's just because I'm thinking of some papers that just use the forbidden predictions and the backbone probability maps, right? Backbone probability maps, right? So, I'm just wondering what does it add to have the split in fragments and how they're generated exactly? Sorry, which part generated? The question is, how do you generate the protein fragments? Like maybe how long they are, or if they fully cover the whole sequence, or if they're just certain parts of the sequence. Okay, I see, I see. Okay, so. I see, I see. Okay. So yeah, so this part is a combination of these four different features that we predict. Yes, there are other works that predict usually just stuff at level CF atoms and it predict those CF atoms and then they take some other algorithm to connect them. But we did a bit further than that by predicting, also predicting what we call the Predicting also predicting what we call the pseudo-uh peptide vector, which is uh right here. So, this is sort of a connecting a way of trying to this predict which CF atom most likely this will be the next one. So, this will be a key feature that we use to connect the C alpha atoms and also for the The amino acid types, we try to make a good prediction that and also using the protein sequence as a way for correcting those predictions so that for those connected fragments, they will have more accurate protein type of amino acid types. Okay, at least we have time for one long question or two quick questions. Actually, a very big question. So, how do you think the alpha fold prediction will affect your approach? For example, like for some kind of proteins like alphafo didn't give you a very good result, right? So, how then you put it in your app? And how confident you think your model is. So, this method is pretty robust in the sense that you can, this is mostly focusing on, this will mostly focus on different confirmations, putting with the different confirmations. If the alpha predictions have very large error, for example, the example which I showed right here, where there is about, in terms of RMC, there are over 10 nanstroms of difference. This doesn't This doesn't really matter because we have the map prediction module. We'll be able to pick out the fragments and be able to do a good transformation from one, from the F4 predicted confirmation into the target confirmation. So, this, I would say, will not is not an issue. But if you, if actually, there are How is it? There are errors in terms of the secondary structure. For example, if this alpha headix is not predicted as an alpha headix, but a beta sheet or just loop or just random structure, then I would say this will be a larger, will be more difficult to match unless the map is really, really good where. Is really, really good where this will be able to sort of peak, fit into the sort of the fragments really well.