Yeah, the sufficient condition is that it meets the definition. Yeah, I don't know if there's a general characterization. It's a good question, because it's not true that all G spaces of interest are proper G spaces. This is a key limitation of this theory, which I'll be getting to in a sec. Which is, I'll be getting to in a sec. So, the higher layers are defined by what are called group convolutions. These are relatively well known, they have been for many decades. Incidentally, this formula was new. So, Parker Cohen and Maxwell came up with this formula. It's mathematically very natural. Mathematicians actually co-discovered this at around the same time. So, the Cohen-Welling paper came out in 2016. Mathematicians working in non-commutative geometry discovered this formula in a Mutative geometry discovered this formula in around 2016 as well. That's a funny little factoid. These other formulas, so formulas for G-convolutions more generally, these have been around for a long time. A similar calculation shows that the formula is equivariant. How are the nonlinearities done? Well, the nonlinearities are supposed to be acting on feature vectors. Our feature vectors are functions. And so the nonlinearities can just act by The nonlinearities can just act by post-composition with our feature function. And then, equivariance of nonlinearities just follows from associativity of function composition. So, it's all very nice. So, where does the Cohen-Welling framework fit? Well, every Z squared equivariant network is a proper G network because Z squared acts properly on itself. And then every proper G network, well, this is just a Proper G network. Well, this is just a bunch of maps between vector spaces, and so it fits into the framework of theoretical networks more generally. Incidentally, for those of you who are interested in category theory, I think this diagram can be formalized as a set of embeddings of two categories. Each of these theoretical, so a theoretical network is a category, it's a composition of functions. So, what we have. So, what we have is a category of categories here, and we're embedding categories of categories. So, this should be embeddings of two categories, but I haven't worked out any details on there yet. So, there's a limitation here, of course, which is that many groups do not act properly on the spaces of interest. If we're interested in images, there are lots of groups of interest in computer vision that don't act properly on the space R2. One example is the homography group. This is isomorphic. Group. This is isomorphic to the special linear group on R3. This does not act properly. What this group is, is the space, it's the affine group together with perspective transformations. So we've known about this group at least implicitly since the Renaissance. Very, very natural group to consider in computer vision, but the existing framework does not handle this. And so we wanted to try and build a framework that would. Wanted to try and build a framework that would. So, this is how our framework is going to work. It's going to be essentially the same. These slides are slightly out of order. So, our framework, again, fits into exactly the same framework, but now our vector spaces are slightly uglier. Our vector spaces are no longer spaces of compactly supported continuous functions. The vector spaces are continuous functions. And this is necessitated by the fact that the group is not acting properly, as I'll show in a second. Group is not acting properly, as I'll show in a sec. But again, everything just amounts to pointwise nonlinearities and group convolutions. So, why is it that we have to go to the space of non-compactly supported continuous functions? Well, the reason is that we need to replace the original formula that Cohen and Welling came up with. The Cohen and Welling formula is the same as this, except without the determinant factor there. The determinant factor is Factor there. The determinant factor is because since the group is not acting properly, we cannot be sure of the existence of invariant measures. There's no way of constructing these things. So we need to just work with an arbitrary measure dx. And the group may be distorting this measure. And as a consequence, we need to put a factor here which accounts for this distortion by the group in order to obtain equivariance. And I think, in the interest of time, I think. Time. I think I can go through this calculation actually. It's essentially the same as the previous one, it's that now we have this determinant here. And so when we do a change of variables where previously we used invariance of the measure, we no longer have invariance of the measure. The measure is going to be distorted when we do the change of variables. And that distortion is going to be cancelled out precisely by the determinant factor. That's the upshot of this calculation. Okay, and so now how does our framework fit in with what exists already? Well, we've got Z2 equivariants. Z2 equivariant networks are all proper G networks. Every proper G network is an example of a more general G network. And then every G network is, of course, a neural network itself. Okay, so that's the theory part. I'm going to talk about how you actually implement these things now. About how you actually implement these things now, and I think this is really where there are a lot of question marks still. So, what do I mean by a practical network? By a practical network, I mean a set of maps between vector spaces, which we can actually implement in a computer. So, the vector spaces now have to be finite-dimensional, but I'm still allowing them to be random. I'm allowing my features to be random vectors. And the layers now are. And the layers now are potentially random maps between the vector spaces. And they have to be built out of tensor operations so that we can program them, for instance, in PyTorch. And of course, you know, deterministic vectors are examples of random vectors. So this includes the normal deterministic forward pass networks that we're familiar with. And if g and x are both countable, discrete. Countable discrete sets. So G is a discrete group, X is a discrete countable set. And if G acts properly on X, then you actually have a perfect way of implementing proper G networks. And again, this is not spelled out in these words in the original Cohen-Welling paper, but it is implicit in there. Proper G networks have a perfect implementation, purely in terms of tensors. The indices of the tensors denote essentially the elements of them. Tensors denote essentially the elements of them. But in general, if you just give me a general G network formulated as this theoretical set of maps between infinite dimensional spaces, there's no way of implementing this without losing something. Yep. Right, but at that point it's not a perfect implementation of the original theoretical object. It's a discrete implementation of this theoretical object, and maybe it'll be right. Yeah, I mean, this is another problem if you're working with something like the group of homologous. Problem if you're working with something like the group of homographies, kind of hard to find a discrete set that's preserved by this thing. Certainly, no finite set is preserved by this group. Oh, yes, yeah, yeah, yeah. It's essential here that it's proper G networks. You don't get a perfect implementation even when G and X are both discrete if G is not acting. There are a couple of different. There are a couple of different techniques for giving an approximate implementation. If G is compact, then you can use generalized Fourier theory to turn your convolutions into pointwise multiplications. These are much easier to implement. And this is what was done in this paper again by Tarko Cohen and Max Welling, as well as a couple of other authors in 2018. But for general G, you don't have You don't have a nice generalized Fourier theory. So you're really stuck with just trying to approximate the integrals themselves. And if you're trying to approximate an integral, there's really only one way to do this. You have to use something like a Monte Carlo approximation. And so there are a couple of works now doing this, including ours. So this work by Binzi et al. in 2020, they have to make some additional assumptions on the Lie groups they're looking at. Groups they're looking at. You need to assume that the exponential map for this Lie group is surjective for their theory to go through. In our case, we don't have such a restriction, but there are still problems, as I'll be coming to. So I'm just going to describe our implementation. So we start off with the first convolutional layer. This is a layer which is supposed to take a function on x to a function on g, and it's supposed to be given. And it's supposed to be given by this integral. We can't compute the integral exactly in a computer. We have to approximate it. We approximate it by a finite sum. So, x here, I'm now taking a discretization of the space x. But the formula otherwise goes through in exactly the same way. And obviously, if you increase the number of samples you take, you make this approximation better and better. The second layer and all higher layers where you do have a function on g and you're trying to map it to another function. On g, and you're trying to map it to another function on g. Again, you use a Monte Carlo approximation. You need to be a little bit clever, though, at least if you want to get a faithful implementation with how you do the sampling for the Monte Carlo integration. Because the formula is only supposed to work if you sample from something called Ha measure. Ha measure is a generalization of uniform measure to general Lie groups. In our paper, we In our paper, we do give a way of actually sampling in this fashion, but I think it's not terribly important for the purposes of this presentation, so I'm just going to skip that. But you can see that what we're doing is we're sampling really an element of the Lie algebra from the pullback of Haar measure by the exponential map. And what this allows us to do is it allows us to replace our original filter, which was supposed to be a function on the Lie group, with the push forward of that filter by The push forward of that filter by the exponential map. This is at least allowed if your filter is supported in a sufficiently small neighborhood of the identity, because the exponential map is a diffeomorphism from a small neighborhood of the identity onto a small neighborhood, sorry, from a small neighborhood of zero in the Lie algebra onto a small neighborhood of the identity in the Lie group. So we exponentiate our Lie algebra element and then we just sum over the Lie algebra elements sampled in this fashion. Algebra elements sampled in this fashion. Yes, that's exactly what it is. But it's really the psi2 tilde that's the more fundamental object. So in the implementation, this is the thing that we implement in the computer. And implicitly, it's computing this by the formula and using the fact that exponential is a low. Or end using the fact that exponential is a local diffeomorphism. Pooling? No, no, you have an exponential map for any Lie group. We were fortunate that the groups we cared about were matrix groups. So the exponential map is easy, relatively speaking. Well, I mean, even for matrix scripts, it's not computable in closed form. It's given by an infinite series. So that's another approximation. Yep. But at least in principle, you know that every Lie group has an exponential map. So this is a purely practical problem. This is a purely practical problem. It's not a theoretical problem. Convolutional networks also usually have pooling. Because our feature maps are no longer compactly supported, we need to be a little bit careful about what pooling is doing. We actually have a theorem which says that if you just pool over a subset, a neighborhood of the identity in G, then this operation is going to be locally invariant. Operation is going to be locally invariant, so it's going to be invariant for elements of g that are close to the identity. And we're just going to approximate the max pooling by just taking a maximum over n samples. As the number of samples grow, make continuity assumption on f, the closeness of this to the true supremum of f is going to increase. And all our filters are going to be parameterized by NLPs. So the Lie algebra is a vector space. So, the Lie algebra is a vector space. This is something we're perfectly happy with in a computer. And our MLPs are just maps from some vector space to another vector space. So, we built some models and then we implemented them in PyTorch. We used 100 samples per layer for our Monte Carlo approximations. This is not very many, but as I'll explain in a bit, this is not a huge problem. The tests to the To test that these models were actually invariant to warps coming from the homography group, we trained our models on MNIST and then tested on a homography perturbed version of the MNIST test set. And so you can see, I mean, the training accuracy was 100%. The test accuracy on the homography perturbed test set was quite good. We compared it with some other networks which people were using. With some other networks which people were using for invariance in the literature. None of them were claiming to be invariant to homographies. So, in a certain sense, this is not, I mean, kind of unfair to these models, but there were no networks that were totally analogous to this, which we thought we could compare to. What would happen if you just trained the network of the invariant by force, but you are then you would probably do just as well. Yep. Yep. So I can I can just close my eyes and have a function with variants like that's augmenting my just with. But that's a little bit of a sample and then it force it to be invariant. Yeah, but empirically, you might hope it is. Yeah. Would have been perfectly valid. Yeah. But our network is supposed to be that, yeah. These other ones, they're supposed to be invariant to more restricted sets of transformations. Yes. Yes. That's right. I tried for a while getting this to work for CIFAR 10. I had access to a fairly large GPU for CFAR 10, an A6000. This is a 40 gigabyte GPU, but I could not make a network big enough that I could even train to 100. enough that I could even train to 100% accuracy. And I'll be speaking about why this is. This is really, if there's a takeaway of this talk, it's really this shortcoming. So part three, looking forward, where does all of this fall down? It does fall down. Let's take a simple example where I remove all the nonlinearities. What I'm trying to do then is I'm trying to estimate this following threefold iterated integral. Now, if I want to estimate Now, if I want to estimate this threefold iterated integral using Monte Carlo samples, using Monte Carlo approximation, then I now need three sets of indices. I've got indices for the first layer, indices for the second layer, and indices for the third. And if I use n samples for each of these layers, then the total number of function evaluations I need on this very first psi naught is n to the power of 3. Power of three. More generally, if you want an L-layer network, and you really need more than two or three layers, if you want to get good, even good training accuracy on C10, you're going to need an exponential number of samples with the depth of the network. Baseline might be how many samples do I need in a network by just data augmentation or elements of the group, and there it's all sorts of groups. Maybe. So, I guess the ratio is of the other is what I tell you you're doing good or bad, as opposed to just concluding that you're doing that. I agree, but this is still, I mean. I mean, this exponential scaling is just really, really bad. If I want to do something like ImageNet, it's just going to be impossible. I think I could have access to all of the computing power at Amazon, and I probably still couldn't train one of these things to 100% on ImageNet. It's just too much. And you might think, well, one way of doing approximating iterated integrals is using a path integral method. I tried this as well, but the problem is when you stick the nonlinearities in there, it becomes impossible. Linearities in there, it becomes impossible. Path integral methods only work for linear. But maybe this is not such a problem. So we actually ran some tests where we looked at reducing the number of samples we used to approximate each of these layers. And it turns out that the test accuracy of performance on MNIST test set to G-perturbed MNIST test set, it does drop as that ratio does not. As that ratio does not really drop, even though each of the test accuracies drops absolutely. So it seems like if you reduce the number of samples, the accuracy in an absolute sense is reduced, but the actual equivariance is not being sacrificed. And so we were able to prove this. It's actually very, very simple. It's just by the nature of the expression we were using. If you take any set of points in G as your sample, it doesn't even have to be sampled from Haar measure anymore. Sampled from Haar measure anymore. And you just compute your Monte Carlo approximation like so, this is always going to be equivariant, perfectly equivariant on the nerves. Even if you only use one sample, this is true. And so this decrease in accuracy is coming from poor training rather than poor equivariance. The fewer samples you use, the more stochastic the forward pass is. And so the harder training is. So this is my final slide. It seems like if you want to. It seems like if you want to implement G networks for arbitrary G, you either need to sacrifice scalability or you need to sacrifice faithfulness of the approximation to the true infinite dimensional object. But it seems like faithfulness is not really necessary. And so what I'm raising is an open question, and anyone who's interested in this, I'd encourage you to come and talk to me about it. Can we give a faithless yet still Equivarian implementation which is scalable? Equivariant implementation, which is scalable. The implementation we already gave is pay plus, still equivariant, but it's not scalable. So, can we get scalability as well as equivariant? And thank you all for listening. There are many other things that are and there are other things that are maybe important for considering something what are you So we actually tried a couple of others. One of the other ones we used was called quasi-Monte Carlo schemes, which are supposed to give you a better covering than IID samples. We were having trouble implementing this, actually. We would try implementing it and the performance would just be terrible. We don't know exactly why. We don't know exactly why, but it seems the sampling method we used, it could just be the scale of the. Actually, my feeling is it was because we were sampling from a hypercube instead of something closer to a ball. And even in an eight-dimensional space, there's going to be more volume in the corners of the hypercube, which is probably not really reflective of the actual transformations that you're seeing in the data. The transformations you're seeing in the data like Data are likely to be closer to the identity than they are to the corners of the cube in the Lie algebra. So we did try a couple of other methods. We did not try any important sampling methods or anything like this. So that might be worth trying, but I think it still wouldn't get around this fundamental scalability. Thank you. So the Android video is late, but it's about four and Is about what it and Joshua Boggle I will see them later. So, what we're going to do is to now have another Joshua actor, and Maikandero will then take the next product. And then we will depend on who has the right, which will follow the language. It should be at the top. I just made sure to send it. Yeah. There it is. There it is. So, Joshua is the variant of Actor, which is the way he's going to speak about matrix sensing. Thank you. Okay, so this is based on some fairly recent work that's still very much in progress. Some of the results I'll be So, some of the results I'll be stating are