Thank you. It's really worked up and inviting me. I'm really new, I think, relatively new to the area, but I'm really excited about those applications. I'm going to share some work today that I did with Benkat Chandra Sakaran at Caltech on spectrohedral regression. And I also just want to quickly mention, I'll actually be leaving Caltech pretty soon. A couple of months I'll be at Johns Hopkins. Couple months, I'll be at Johns Hopkins. So I'll be in the teacher. Okay, great. So spectral regression, this is a new approach to the problem of convex regression, where we want to fit a convex function to data. And this ends up being, you know, it comes up pretty naturally in a number of different applications. So I want to give a couple examples. The first Give a couple examples. The first is in economics, where there's this idea of sort of diminishing marginal utility that manifests in data sets with this either like convex or concave relationship between variables. So on the right is an example of a data set that gives the mean weekly wage versus years of experience. And so sort of at the start. And so sort of starting from zero, the more years of experience you have, your wages sort of increase, but at a sort of decreasing rate of increase, and eventually actually sort of peak and end up decreasing as your years of experience increases. You get this concave relationship. And another application is an engineering design, where you might have some sort of engineering design parameters that you want to optimize over. That you want to optimize over, but you don't have a closed-form relationship between these parameters. But you have some way of simulating the data. And so, since the ultimate goal is to optimize the function that you're fitting to the simulated data, it's useful to fit a convex function because for computational reasons, the convex functions are. The convex functions are easier to optimize. So, either if the simulated data presents this convex relationship, or some transformation of that data has a convex relationship, you can fit a convex function that's constrained to be convex and more easily optimize it. Another special case of convex regression that I want to mention and we'll mention throughout the talk. Mentioned throughout the talk is that of support function estimation. So, this is where the goal is to reconstruct the convex hull of an object from measurements that are support function evaluations of the object. So just to recall, the support function of a convex set in some direction u is the maximum value of the inner product of x and u where x is contained in your set. That's just a quick picture. Picture, but this specific case has applications in sort of radar, MRI, and confused tomography, where we're reconstructing some objects from these measurements. And because support functions of convex sets are in one-to-one correspondence with convex and positively homogeneous convex functions, it's a special case of convex regression where we're just restricting that class of convex functions to additionally be positive. To additionally be positively continuous. Okay, so more precisely, our goal here is to estimate some convex function, f hat, from n sort of input and output pairs, xi, yi, such that yi is approximately equal to the output of f hat with input. Of X hat with input XI. And a sort of very natural first attempt to compute such an estimator is to compute the least squares estimator. So we want to minimize over all convex functions this least squares objective. And it turns out, you know, you can actually compute a minimizer of this objective. Of this objective using with a convex optimization problem. But the solution is the maximum of n affine functions, where again, n is the amount of input and outputs that we have. So the drawback of this approach is that the compiler increases. It is growing with the amount. It is growing with the amount of data that you have. And so you kind of become in danger of overfitting to your data. And so this is just, these pictures kind of illustrate this idea where you have noisy measurements of the L2 norm function. And you see that, you know, these red areas, you're really over-fitting to the data, especially in that corner. And here's a picture of the actual L2 function. Actual L to function. And so sort of experimentally, we can kind of see that this overfitting is really a problem. And then there's also sort of a theoretical sub-optimality result that says the least squares estimator is actually minimax sub-optimal for Lipschitz convex regression, which is fitting a Lipschitz convex function to data, and support function estimation is worth high enough. Support function estimation is worth high enough to match. So this means so that, you know, in some sense, this estimator provably does, it doesn't do as well as some estimator possibly could in sort of the worst case over these classes of functions. So, the standard way to address this overfitting issue is to Fitting issue is to restrict the complexity of your estimator. So, a standard way to do this in complex regression is to restrict the class of functions that you are minimizing over to be M polyhedral functions. So, these are functions that are the maximum of the fixed number of M affine pieces. So, if M is equal to N, the amount of data that we have. Equal to n, the amount of data that we have. This is just the same as the least squares estimator. But the idea is that we can constrain it to be sort of a fixed user-defined m less than n to sort of constrain the complexity of the resulting estimator. And it turns out that if you kind of choose m appropriately with n, meaning somehow smaller, definitely smaller than n, you obtain those minimax rates that the usual least squares estimate. That the usual least squares estimator provably doesn't attain. The downside is that we've now made this into a non-convex optimization problem, but there are a few sort of tractable methods meaning tend to work well in practice ways of computing the estimator. But a drawback of both the polyhedral version and Version and hold on, let's just go from this university dice. Is there everything from I know, I know, I'm not sure how to. Sorry, it's my my husband. Sorry, it's my husband. I'll stop. We did just silently sign the parmed event. Oh, he's going to find me here. Okay. Anyway, so getting back to the second college regular. The polyhedral regression, you know, there's a remaining drawback here, which is that you're really only going to get these polyhedral approximations, either with the usual least squares estimator or this constrained version. And what you're trying to reconstruct might not be a polyhedral function. You might want it to have smoother features. And so this is the motivation for our approach. Our approach, which is called spectrohedral regression. And the idea is just to change the class of functions that we're minimizing over in this least squares, or we're to minimize the class of functions for which we're minimizing this least squares objective. Instead of constraining over polyhedral functions, we're going to constrain over what are called m spectrohedral functions. So these are functions. Functions. So these are functions of this form. So they're the maximum eigenvalue of this linear matrix expression. And the parametrizing matrices are m by m, real symmetric matrices. And it's actually, this class actually includes all of the m polyhedral functions, because if your matrices are diagonal, you just recover your polyhedral functions. So we're sort of enriching the So we're sort of enriching the class of polyhedral functions, but instead of enriching it by just adding more alpha adhesives, we're enriching it by, you know, giving it this, adding functions with sort of more complex structure. Okay, great. So here is that same L2 function, and we have, these are now spectrahedral reconstructions of the L2 function for 20, 50, and 200 noisy. For 20, 50, and 200 noisy data points. And we see that we're pretty much sort of nailing this function at the end there. And that's to be expected. I have chosen this function for a reason because it actually is a spectrometral function. And so we'd expect this, you know, if you have, we'd expect this, you know, these are noisy data points, but we'd expect this to do well because the function we're bucking is actually a cost. What's the n? N. N. N is the number of data points. N is the number of data points. Oh, sorry? The other parameter is the size of the matrices. Yes, the size of the matrices. Yeah. Oh, the number of linear functions. So what's N? What's M? What are the two parameters that we're looking at? You indicate one of them, but what's the other one? These effects are. Oh, here. Oh, yeah. Oh, I didn't put it up here. Yeah, M is 3 here, I believe. Okay. Yeah. Okay, yeah, yeah. Yeah, I was three here. Yeah, sorry about that. Okay. Okay, so yes, I want to mention that this generalization of polyhedral regression to spectrahedral regression is sort of analogous to the generalization of linear progression. The generalization of linear programming to semi-definite programming because we can optimize an M-polyhedral function using linear programming. The epigraph of a m polyhedral function is a polyhedron. And analogously, we can optimize an M-spectrohedral function using semi-definite programming because of a graph of these spectrohedral functions are in effect. Okay, so first I before I kind of go into more details about sort of how I computed this estimator and some of our results, I just want to show you some pictures of examples where we have fitted spectrophotial functions to real-world data sets corresponding to kind of those applications I mentioned at the beginning. Of this applications I mentioned at the beginning. So here is an example of spectrahedral and polyhedral estimators of fit to this average weekly wage data. So that data I showed at the beginning is part of this larger data set where you have weekly wages versus years of experience and years of education. And through sort of a transformation of the years of education variable, you can get sort of approximately a concave relationship. Concave relationship, and to that we fit a spectrohedral and a polyhedral estimator. And in particular, we have fit here an m equal to 3 spectrohedral estimator and an m equal to 6 polyhedral estimator because we want to match models with the same degrees of freedom. So for matrices that are 3 by 3, your spectrophedral estimator will correspond to, the degree of freedom will correspond to a equal to 6. I'm equal to six policy costs. Another example from the sort of engineering design, these are sort of simulated data giving sort of this drag coefficient as a function of something called the Reynolds number and list coefficient. I don't know much about airplane drag design, but here you can do a transformation of the data and you get this sort of convex relationship. And here, again, just an example of how you have these sort of. How you have these sort of polyhedral reconstructions, but with ours, you get sort of things with both. You can get some smoother features in your estimator with our expected equal approach. So again, I want to go back to this special case of support function estimation, also, which was, this specific case was also studied by So and Sashandra Sakaran. And polyhedral requirements. Polyhedral regression in this setting corresponds to fitting a polytope with M vertices to your support function data. So your reconstructed object is guaranteed to be sort of polytope with n vertices. And on the right here is actually just the usual least squares estimator. So there's some 200 noisy support function measurements. So here you can see, you know, maybe for this is the L1 ball, and these are polyhedral reconstructions of the L1 ball. And on the right is the usual least squares. That's made it received this really horribly here because it's just way too complex for trying to reconstruct. The spectrahedral generalization is to instead constrain the least squares estimator over M spectrotopes. M-spectrotopes. So this is a class of convex bodies that are linear images of an M-dimensional spectroplex, the slice of the ESC cone with phase one matrices. So that's the class of functions that we're fitting to our support function data. And here is an example of a spectrotope, an m equal to three spectrotope reconstruction of the L2 ball versus the least squares estimate, the usual least squares estimate. Squares estimate, the usual V squares estimator. And again, it's sort of nice because the L2 ball is a spectrum. Okay. So here's a... Sorry, I might have spaced out one slide. No worries. I admit this is way too. I think I wasn't here. So support function estimation means what? What's the data that you're so we have these input and output pairs? So we have these input and output pairs where the input is some unit direction and the output is the support function of the set in that direction. Oh. So yeah, it's like, and you want to reconstruct the object. Why is least square doing so bad? Sorry? Why is least squares doing so sort of? Well, the least squares, I mean, it has to, you know, it's constrained to be this polytope with. Be this like polytope with n vertices. So, yeah. So, just you know, the polytopes do a horrible job approximating to all. So, you know, it's makes that fit. So, but it's a nice set that, you know, our like spectrohedral regression, yeah. The sort of example where, yeah, spectrohedral regression kind of happened is that sort of nice, nice shape that. Nice shape, that maybe ideal shape that wouldn't actually appear in practice, but just to illustrate. Okay. And sort of a more real-world application. This is an example from their paper where they did a constructed, reconstructed a lung from support function measurements. And so this is sort of an argument, I think, even just for like the I think, even just for the qualitative advantage of spectrohedral regression, where if we're wanting to reconstruct the lung, we want this thing to look as much like a lung as possible for diagnostic purposes. And so things that are very blocky and polyhedral just aren't going to do very well. And I like this example because I think it looks, the spectrohedral version looks a lot more like a lovely spectrohalial version. Okay. So, I also want to mention that we, in our paper, we didn't just consider like full spectrohedral functions, but we looked at all these classes that sort of interpolate between polyhedral functions and M-spectrohedral functions, because where we constrain the parametrizing matrices to be k block diagonal. And so, you know, if you kind of like want to increase. If you kind of like want to increase the enrich the class of polyhedral functions, but not go to the full power of n spectrahedral functions, you can do these like constrained constraining the blocks, constraining the matrices to be block diagonal. And so here are just examples of reconstructions of the L2 function and the L2 norm function. And we see even with like F4. And we see even with like a 4x4 but 2x2 block diagonal matrices, we're doing pretty well. And here's another quick example of a function that's neither polyhedral or spectrohedral, an exponential function. And again, three constructions, here we fit polyhedral, block spectrohedral, and full spectrohedral functions to like simulated data from this function. Simulated data from this function. And again, we're comparing all the models we're comparing have the same degrees of freedom. So, you know, m equal to six polyhedral functions, block spectrohedral, where we have four by four matrices, but it's two by two block diagonal, and then the full m equal to three spectrohedral functions. Okay, so two questions I want. Two questions I want to go through or want to answer or give our attempt at answering for the rest of the talk. Our first, you know, I've shown you all these now pictures of examples of spectrohedral estimators, but I want to tell you how to compute these. And then I also want to talk about, you know, what is the expressive power of spectrohedral functions? How well, you know, do they approximate more general classes? Do they approximate more general classes of complex functions? So for their computation, so I mentioned earlier very briefly, you know, so we've constraining the set of functions that you're minimizing over turns this into a non-convex optimization problem, even for just the polyhedral regression. And I mentioned, but I mentioned there are a number of ways, another way. Of ways, a number of ways of computing the estimator that have been proposed. And by far the most popular is something called alternating minimization. And so this is the algorithm that we've generalized to compute our expected petal estimators. And so just for some setup, we're going to let script A be the sort of D plus one tuple of our parameterizing matrices, and script A bracket C is going to be A bracket C is going to be the sum of CJAJ. And then we're going to let these CIs be our input, one of our inputs, hex i, with just one added to the coordinate, so that this expression is our linear matrix expression, where it'll be the sum of x, j, a, j, plus that offset a, a, zero term. Plus that offset A0 term. Oh, actually, oh, yeah, sorry. I realized you put the one in the beginning there. But anyway, so what we want to do is we want to find a set of parameters that minimizes this objective. And the way we're going to do that is by alternating between solving. Alternating between solving the inner optimization problem, the lambda max, and the outer optimization. So we're going to start with our collection of n input and output pairs. We're going to give our algorithm some initial set of parameters. And then we're going to repeat the following two steps until convergence. We're going to compute the optimal eigenvectors for each i. And then we're going to plug those in. And this gives us just a linear. Just a linear, this gives us a linear function of our parameterizing matrices. And so we have for the closed form solution for the minimum of that linear least squares effective. And then we go back and forth until some convergence criterion is satisfied and then output the final set of parameters. So in practice, the initial set of parameters we gave it was just a random set of parameters, and we do this like multiple times and take sort of the Multiple times and take sort of the best performing set of parameters as our model. But in general, there's not really a guarantee that a random initialization is going to do well for this non-convex optimization problem. But what we were able to prove is sort of the local convergence guarantee, which is sort of all that, at least I felt like I could hope for. This is sort of all that's really been used. You know, this is sort of all that's really been even shown for the polyhedral case. So this is sort of our one theoretical result in the computational direction. But now I tried, in the previous one, I tried to see that because the update, so where is the other minimization? I mean, there should be, when you say alternative minimization, I the alternative. Minimization, like you alternate between two processes. Where is the other one? Is it hidden? Because you compute the eigenvector? Yeah, simple that also. Yeah. Actually, not minimization, computing eigenvectors or quite. Yeah, we're just, I mean, this is kind of the term for this approach for minimizing the function. But we're just thinking of this as sort of alternating optimum optimizations. Optimizations. We're solving the inner optimization problem there. I just confused about an A, but because I was expecting alternating between two or several, but it's C. Yeah, yeah. I think it's it's find the maximum eigenvalue. I would value them too. Yeah, I mean in some sense we're crazy. We are I mean we're inputting prejudice we're inputting sort of the optimal value of this, right? Yes, okay. Okay, so to set up our result, you know, and what we have to do to prove such a result is search Prove such a result is sort of assume a distribution on our data samples. So we assume that our n input-output pairs are IID samples from this random pair xy, and we're going to assume that the inputs are Gaussian distributed, and then y is sort of this true spectrahedral function. Our true spectrohedral function of x plus some noise, which we seem to be in Gaussian as well. And then we have to make this additional assumption that our true parameter satisfies this eigengap condition. So we need the difference between the maximum and second largest eigenvalue of a star applied to a unit. Applied to a unit vector to be sort of uniformly bounded away from zero. And we call this the infimum of this eigengap over all even vectors u, kappa, as it's sort of like a conditioning number in that it kind of controls how well conditioned our true spectrophedical function is. And under that condition, we can prove that if our initial parameter is close to our Initial parameter is close to our true set of parameters, and n is large enough, then the error at all iterations of this alternating minimization algorithm simultaneously satisfies this bound. Where the first term shows that sort of in the absence of no noise, we have this geometric convergence to our true parameters. And then we have this extra term where Where, in the presence of noise, where that decay is like log n squared over n is integral. And this bound holds with high probability, meaning the probability converging to 1 as n grows. And I've left, we have more, like, when I say n large enough with high probability, like we have expressions for that in the paper. They're just very ugly, and I didn't want to put them on the slide. But, you know. But, you know, and somewhere is it sort of like there are sort of absolute constants, but like in the paper that C1 is actually like we have a we have an actual number there. But just to kind of make it as clean, I wanted to make the result look as clean as possible. So if you're curious about sort of the precise dependence of things like these, that's in our paper. But one sort of thing that about this result was that we didn't like so much was that this eigengap condition is very strong and isn't satisfied by like polypecral functions. And this was like sort of unsatisfying. And so we have a second version of our we have a second version of a local convergence. We have a second version of a local convergence guarantee where we've relaxed this condition, but of course at the expense of strengthening other conditions. So here, you know, we have just a slight variation on the assumptions, which is that just that our input random variable x is just bounded. And then just to sort of recall the structure, so this, you know, basically, know basically I this version two is kind of for MK spectrahedral regression where K is like kind of strictly less than M because that I think is really where we're in danger of not satisfying our that eigengap condition. So recall that in that setting we have this sort of block diagonal structure and our conditions are now we have to have so this is a So, this is our new condition on sort of the eigengaps, which does weaken the previous one. So, instead of having that eigengap condition hold, generally, we only want it to hold sort of within each block. But between blocks, the difference between the maximum eigenvalues of each between two different blocks, that this gap only needs to hold an expectation. So, that weakens the condition. That weakens the condition to allow, so now we have examples of polyhedral regression that do satisfy these assumptions. But one additional assumption, we also need to make this sort of additional assumption on our input random variable x. Remember, xc is x with a one attached. And this is sort of like a small ball property corresponding to something like a small ball property. Something like a small ball property corresponding to the distribution of C. So yeah, so basically the idea is we've just we figured out a way to kind of relax that eigenvalue or the eigengap constraint, but we just have stronger conditions on our input random variable. And we also have a slightly different upper bound on how close we need our true parameter. Our true parameter, our initial parameter to be to our true set of parameters. So it's a bit of a stronger condition there. And then the other way this has changed the result is that we no longer have probability converging to one, but probability converging to one minus three delta, where remote delta is sort of controlling this eigengap between blocks. Between blocks. Okay. Any general questions about this polydidal case? It's a special case, I don't think. Yeah, exactly. But somehow you want me to say it's not so good to polydeal it. I just wonder whether the results that we prove somehow show that this electrical way gives better things, is it? Yeah, so basically we just we wanted a we wanted a local convergence guarantee for this algorithm that held for polyhedral functions and like general when k is less than m, because I feel like that's really where we worried that this eigenstrong eigengap condition was not going to hold. That condition was not going to hold and definitely doesn't hold for polyhedral functions. So, yeah, our goal was just to generate a second version of this guarantee that held for polyhedral functions. Or for under, you know, we give a, you know, if you, in setting up support function estimation, for instance, if you're your inputs are The inputs are unit directions. If those are uniformly distributed on the unit sphere, these conditions are satisfied. You can show these conditions are satisfied with, I mean, it ends up being sort of lower bounds on a distance between the vectors that are that come from the diag. Like, you have these. Like, you've got these D matrices, D diagonal matrices, and the vectors corresponding to the diagonal elements. You just have a condition on sort of those being separated. And the algorithm, like, how many iterations did you see? I think not thousands. Definitely not thousands. I'm sorry, I don't, it was a wild gap. That's a strange question. Yeah, no, no. I mean, I think that's a, it's, there's a lot of mysteries to me about this algorithm. It's a pretty humble algorithm. Yeah. Like, it's kind of. Yeah, like it's kind of a curious, like, it's kind of a heuristic thing. So, it's like, okay, maybe this will work if we just go back and forth after you know exactly things. But it worked, it tends to work pretty well, and even with random initialization, I did see that in my experience that not, it didn't always converge, but it often did with just a random initialization, and I really don't have a sense of why, so but it's very it's cool and like yeah, but mystery, mysterious a little bit. That's a that's a common problem for most people that are trying to get a project today. Or sort of trying to get a function to data. Right of initialization comes in. Yeah. People are like, it's still a good technique. Yeah. There's something maybe I did not understand before. I mean, now this is about the algorithm. But before, when you state minimization problem, I mean, is it a conex problem? What I'm wondering about is you know before I compute something uh so Something so what's the nature of the thing? Maybe I'll rather local minima or yeah. I mean, I have an idea. Let me know if I'm not answering your question, but I think sometimes this talk does get a little confusing because I'm talking about convex regression, where we're fitting a convex function to the data. And one motivation for doing that is that once you fit that convex function. Once you fit that convex function to your data, you can optimize that convex function. You can optimize over that estimator. Finding that convex function, but maybe this may be a non-convex. Exactly. So that's the algorithm. The algorithm I'm talking about right now. Exactly. So this, this, the algorithm I'm. Yeah, the algorithm. Yeah, the algorithm I'm talking about here is the non-convex optimization for computing the parameters of our estimator. And yeah, so this is a non-convex operation problem. So all we were just. Yeah. Here's the first one, for example. So now, if our, this is just a local convergence. This is just a local convergence guarantee. So it's basically saying if we're close enough to the optimal, then we're going to converge. But there's no yet. I think we would replace a star by open anymore. Um I mean yeah I guess I yeah if you're close to a local yeah possibly go to that local this is I mean there's lots of mysteries like sometimes There's lots of mysteries. Like, sometimes, right, like local minimum aren't actually bad, right? No, I'm not. No, no, I know, I didn't mean that, but that's what I'm saying. Like, that's, yeah, there's, there's lots in terms of the open questions here, right? Like, I think one really interesting question is, like, can we understand this objective better? Is this like a nice function? Is this objective, you know, benign in some way where like local minimum or like many local minimum are local? Like many local minimums are global minimum, or can you know is there some analysis we can do to show why this algorithm tends to work well in practice? Yeah. Any other questions? We have like 10 more minutes. So if I'm telling a statistician about this, would I be making an analogy like the first your answer to the The first your answer to this question that had the the part one and part two. So first you fit a fine A function that you fit, and then you optimize it. So it's like I'm picking whether I want to do singular dash, multiple regression squared or something or something. And once I have that, I have a model, and then I'm optimizing the parameters of that model. Yeah, if your ultimate goal is, I mean, yeah, the The ultimate goal is, I mean, yeah, I think I've seen this is like these engineering problems where you have these design parameters you want to optimize over because you want to minimize the drastic over. First you have to like select the model. Right, but first you have to exactly estimate the logo. I was intriguing to see a log showing up. Is it where is there a truncated harmonic series or something? Or is it easy to say how Is it easy to say how to do it? Oh, um maybe I can answer later. I have to look back at the paper, but I not yeah the I didn't use any what was it the oh like a finite sum of one over A finite sum of one over k or something like this? I can say the main tool of this, of the proof, are these sort of matrix concentration inequalities. Oh, okay. Yeah. And well, and for this term, you have you're like projecting this Gaussian noise. So, yeah, it's a lot of like. So yeah, it's a lot of like, a lot of concentration difficulties. And it relies really heavily. That's the other kind of like, I think, direction. The proof relies really heavily on this Gaussian assumption. You could relax it to like sub-Gaussian, a sub-Gaussian random variable. But it really relies on, the machinery is all based on this assumption of a distribution on your data, and that's IID distributed. Who knows if these things slowly work? You know, who knows if these things work in practice? So, I think this is also, that's like that's how real data comes from that kind of model. So, unless you're actually able to design your inputs, which in some settings maybe you can, that's also kind of a strong assumption. And it'd be interesting to, yeah, just get some sort of guarantees for this, these kinds of algorithms that don't make such strong assumptions on the. Make such strong assumptions on the distribution of the data. Okay, so sorry, I'm definitely happy to answer any additional questions or clarifications on that, but in the last five minutes, I want to talk about the second question I mentioned, which is, you know, what is the expressive power of spectrohedral functions? So the question we ask specifically, you know, how well do MK spectrahedral functions approximate Lipschitz? Lipschitz convex functions. So, this classical result of Dudley implies that for polyhedral functions, you have, you know, in the approximation rate where in the soup norm, M polyhedral functions, the error between an M polyhedral function and a Lipschitz convex function in the soup norm, it's In the soup norm, the worst case, decay is like m to the negative 2 over d as n grows. So since m polyhedral functions are actually a subset of mk spectrohedral functions for any k, because they correspond to mk spectrahedral functions where k is equal to 1. This is an upper bound on the approximation rate for general mk spectrohedral functions. spectrum functions. Sorry? What's D? Oh, D is the dimension of, so F either functions on R D. L doesn't enter the bounds. I mean the October function. Oh, I've incorporated that into the big O notation. I'm just focusing. Yeah, L is there, yeah. Sorry, I'm just focusing on the M first. But L is still there, yeah. Yeah, I put L is for just today. It's not, yeah, for sort of that. Yeah, for sort of that's that's the sort of class of functions we're looking at Okay, so what we did is we proved a lower bound on the approximation rate for n case regional functions. More specifically, we assume that again I didn't say this news? Oh, okay. Okay, so if you assume that k sort of grows like n to the t for some t between 0 and 1, so where t is 1, these are, I don't know, these all. Oh my goodness. And so when we make this assumption, we can prove this. When we make this assumption, we can prove this lower bound on the approximation rate. And in particular, for constant k, this rate is going to match the upper bound. And so that's sort of the one thing we were able to prove is basically for fixed block sizes, mk spectrohedral functions. You know, MK spectrohedral functions are going to, you know, in terms of just the rate with respect to m, you can't do better than polyhedral functions, except maybe like with logarithm, like logarithmic things. But yeah, so if you fix k, and even if you let k kind of grow slowly, like with like log m or something, you're still going to get the same rate approximation. But as soon as k But as soon as k is growing with m in the way where t is greater than zero, you get this gap between the upper and lower bounds. And so it's still an open question whether the approximation rate matches that lower bound, which is sort of what we hope for. And an idea of how we prove this. So we actually proved that lower bound by computing an upper bound on the risk of our estimator. So we are going, so let F hat be our MK spectrohedral estimator. And we're fitting these n input-output pairs, which we assume are IID samples of a random pair, and Y is some true function f of X. We can evaluate our estimator by looking at the quadratic risk, which is the expected value of the square of the difference between our estimator and the true function evaluated at our. And the true function evaluated at our input. And to upper bound, define upper bound, we use the very classical sort of bias variance decomposition of the risk of our estimator. And the bias is the approximation rate of our MK steps to our function, how well this class of functions approximates f. And if we assume f is in this class of convex and L-Libris functions, then there's a known lower bound on this risk in the worst case over this function class that decays like n to the negative 4 over d plus 4. And so this lower bound is what gives us a lower bound on our approximation rate. On our approximation rate. So it's sort of a proof by contradiction where we showed this can't be any smaller than the lower bound on the previous thing because if so, it would violate this minimax lower bound on our disk. Okay, so to summarize, you know, spectrahedral regression, it's a new approach to fitting convex functions. It generalizes polyhedral regression. Generalizes polyhedral regression. And in particular, it returns convex estimators that exhibit both smooth and singular features. So there are other approaches that try to address the issue of these sort of blocky piecewise layer estimators. There are things like constrained polyhedral regression, where you constrain the polynomial regression. You constrain the polynomials to be convex polynomials. The polynomials to be convex polynomials and other things. But those, at least the other approaches I've seen, give like exclusively smooth estimators. So our approach includes functions that exist both smooth and singular features. So it's just another tool for this sort of toolbox. And I think a big open question here is really around the expressiveness of these functions. Empirical evidence. Functions. Empirical evidence shows that, you know, all the experiments and examples I showed, you know, the error seems to match in terms of the degrees of freedom of the model. So like M-spectrohedral regression seems to perform comparably to polyhedral regression with the same degrees of freedom, which would seem to be a very important thing. Which would seem to imply that we should get the approximation rate really should match that lower bound, which would give us the fact, which would allow us to show that these MK spectrohedral functions achieve the same minimax rates that polyhedral regression does. Currently, because our upper bound comes from polyhedral regressions, we have a sub-optimal kind of Have a sub-optimal kind of rate for the risk. But if we could show that it matched that lower bound, then we could show that our estimators possibly achieved the minimax rates that polynomial regression does. Okay, and then there's just tons of open questions. I mean, on the computational side, there's parameter selection and tuning. Which m and k should you choose given your data set? Given your data set, there's, we already kind of talked about this, but there's just all these open questions around the computation of these estimators. There are other approaches, I wanted to mention this, there's other approaches to computing the polyhedral estimators. One in particular that I think is really nice, is really interesting, is it uses the fact that these max affine functions generate a partition of your input space. And so it sort of Your input space. And so it sort of goes back and forth between like optimizing the partition and then fitting the pieces on the outside pieces on that partition. And I don't know what the analogy is of that partition for an M-spectrahedral function. So I don't know if that, I feel like, I wonder if there's, again, like alternate ways of computing this estimator inspired by ways that have been approached for polydeclinal regression. Polygamore regression. And yeah, like I said, the approximation power of these functions. I think even if it seems kind of difficult to prove the approximation power for general convex functions, I'm pretty interested in whether there's natural subclasses of convex functions that these provably approximate better than polyhedral functions. And I wonder if that's sort of what we're seeing in practice, like, like the like like the like these functions are there's a natural class of functions that's kind of a good model for real world data you know that like real world data kind of follows and so yeah I think that's another direction and then you know there's other applications where the estimator that's computed is either like piecewise Is either like piecewise linear or piecewise constant. So it's like other shape-constrained regression, like monotone regression. And then there's things like density estimation, where you also get this sort of piecewise constant estimator. And so I'm interested in maybe, you know, doing a similar kind of generalization to spectrohedral estimators and use different applications. Estimators in these different applications. Okay, I said, thank you so much. Questions?