Okay. I guess the reason not to start. So our next talk is by Oleg Igoshin from Rice, and he will be talking about understanding trade-offs of biological information processing. Okay, first of all, I would like to join everyone and thank you to organizers putting together this what promised to be a very What promised to be a very interesting workshop. I'm here to learn more about information theory, and I mean, there wouldn't be much information theory here, but a little bit of the quite a bit of thermodynamics, and I hope to maybe connect those two directions together. I mean, we already, my wife and I came a few days before and already sort of had a very good time riding our tandem around from one club to just. From Banff to Jasper or Dave. If you are a cyclist, I highly recommend it. It's like an adventure lifetime. Anyway, so I'm inspired to hear more science and will tell you a little bit about ours. So the topic of my talk today is trying to understand how biological system processes information. And the key here is that, I mean, all biology, or I would say, all biochemistry is enzymatically about. Is enzymatic about enzymatic control reactions. And whatever enzyme you choose, it has to select the right substrate out of dozens of chemically similar ones that are present inside the cells. And that's definitely true for every single step of the central dogma, DNA replication, transcription, translation. And in order to correctly read the information called DNA and correctly produce the right cell or phenotypes, enzymes have to be selected. Enzymes have to be selective. So, the selectivity and specificity of enzymes is really key for the biological information processing. So, this would be the sort of top of the mind. So, let me give you a few examples. So, if you think about DNA replication, it's amazingly accurate, depending on the organism, depending on the system, depending on the conditions, but it could be as little as one in ten million of base pairs is mismatched during implications. So the error rate is something like 10 to the minus 7. If you, sorry. Yeah, so transcription and translation are not as accurate, but still quite accurate. 10 to the minus 4 to 10 to the minus 5, 10 to the minus 3 to 10 to the minus 4. So basically, that means that if you have a protein, the thousand amino acids long, less than one on average amino acid. Rich amino acids will be misincorporated. Probably makes sense. You don't need, there are not much many more longer proteins, so you don't really need much higher rate than this. So, sorry, keep moving. So, why is this surprising? Because if you think, remember your classic sort of maybe biochemistry course where like enzyme substrate glove fit model, you can think with the right. Fit model, you can think of the right substrate really fits into the enzyme and binds, so it's stronger. And if you think about it from that perspective and think about how different could be the sort of interaction energies between enzyme substrate complex chloride and one substrate, we are talking here about an extra hydrogen bond here and there, because lots of those interactions are base-pairing interactions. And I mean, you get different between two or three hydrogen bonds, and at room temperature. Hydrogen bonds and at room temperature, the hydrogen bond costs something like five to seven kT. So if everything was going into sort of equilibrium to midnamic limit and you had a Boltzmann distribution, the enrichment factor would be something like e to the minus five, e to the minus seven, and that's something like 10 to the minus two, 10 to the minus 3. So that's really not enough to explain all those small numbers I just did. So, and the difference actually between BOL and this limit and the other is actually quite amazing. So, let's go back to the protein biosynthesis and think about 1% error rate versus 100 of percent error rate. If you have 1% error rate, then you have 300 amino acids of protein, only 5% of the proteins made will have no errors, will be made in there. Will be made in there. On the other hand, if you have 10 to minus 4, then basically 97% of the proteins that you make will be s-coded. So that sort of problem has been solved actually quite a long time ago by this theory of kinetic proofreading, independently developed by two physicists, John Hopfeld and Jack Ninha, somewhere in. In some somewhere in 1974, 1975, they independently published it. This one is in French, but still a fun paper to read, as Google translates. And basically, they came up with this mechanism of kinetic proofing, how biological systems can use active energy equilibrium sort of limits to get much more accurate. And basically, the idea here is that, like, unlike in Michel's Mendel state, where substrate binds, then the complex is formed, and then basically. Complex is formed, and then basically the sort of equilibrium in this reaction determines the enrichment of right versus fraud substrate. What do you get? You get some intermediate step, and from that intermediate step, you still can dissociate. So, you have two, instead of one selection step, you have two selection steps. And then you if you have basically a factor of ten difference in dissociation constant here, and here you have a factor of one hundred of enrichment. Factor of 100 of enrichment of this intermediate before you proceed. So you get pretty much a square. If you have one step, you get a cube of equilibrium error if you have three steps instead of. So what's really important for here is that this step should be reversible, because if you can have bypass it and you can go back here, then you go back to like sort of quicker's mental in just one step. And what happens is that in order to get to this state, it has a high energy intermediate, you have to spend energy in terms of H. Spend energy in terms of ATP or GTP hydrolysis, which is an energy currency of the cell. I mean, the other way to say that if you were friendly in equilibrium, there wouldn't be any flux, so that resetant flux will be zero, because in equilibrium you can have non-zero fluxes in the new rules. Anyway, so that's that. So the take-home message that when you have this, what you have, you have multiple steps you have to go, and then you have a Steps you have to go, and then you have a reset. So you sometimes instead of going directly, you have to go into several cycles. And sometimes you correct the errors that you make, but sometimes you correct the errors that you never made, you can sort of correct amino acids, and then you remove it in here. So there would be futile cycle that wastes time and energy. So overall, you expect there would be some trade-off between speed, how fast you proceed, about the accuracy, how often you make an error, and how much energy you spend. And how much energy you spend. And this is what we sort of, what I'll give you some snippets of our research understanding those trade-offs. So, I mean, again, this goes back to the 70s. There's a lot of physicists working on this here and there. But of course, a lot of them were really based on the sort of COI model where only one step dissociation was different between right and wrong substrate. There were also different other simplified assumptions. Other simplifying assumptions, but we really wanted to look at the. So we started with the central dogma enzymes, and those are the ones I will talk about today. Really see their biochemical parameters and see what are their kinetic parameters, what are their steps, and what are they optimizing in this multi-dimensional trade-off between speed, energy, and accuracy. So, what we did, and so today I'll mainly talk about all three systems. We look at TNA polymerase. T7 is the one which is really well characterized in terms of all. Really well characterized in terms of all the kinetic schemes. We look at the coli ribosomes, and we also look at another part of translation. You need to attach amino acids to tRNA. tRNA is the transferase is the enzymes. And we try to sort of compute those quantities and see how they interplay. So the good thing is that they very well characterize on mechanistic level. So we know for each of those steps, like binding the Those steps like finding the situation, the earliest step, we know the rate, and we know the discrimination between right and wrong. So, basically, that's what we did. We started with the parameters from the literature. We used mean fifth passage time framework to compute error speed and energy dissipation, and then we see how they change, and then sort of see whether there are trade-offs locally or globally in there. In there and try to understand what's happening. So, this is the type of questions we ask. For example, you look at the parameter to the achieve maximum accuracy, maximum speed, or is it right there on the optimal trade-off? So, is there always trade-off between speed and accuracy? This is sort of what many people assume, assuming that it's not the case, just to advertise. So, of course, the energy expenditure is really important that a few del cycles. Really important that a futile cycle burns the ATP. Is that important for the cell, for any of those processes? So, more sort of theoretical question that has been discussed in the show, and I'll talk about whether it's kinetic or thermodynamic features of energy landscape that determine the selectivity. So, again, how generalizable are the trends that we see? And if I have time, I'll talk a little bit what I have time, I'll talk a little bit what would be the tolerable error. So, I'm not going to go into the formalism, I'll just give you a few snippets of the results, and I'll be happy to talk you offline if you want to talk what we do. So, the result number one is that for enzymes that we saw, unlike sort of I mean the the the typical thinking is that the proof reunion evolved to make it seems much more accurate, and of course that's partially true. Course, that's partially true, but for all the interviews, we look at most of the kinetic parameters and see how basically the speed. This is minimal species time, so this is inverse of the speed, and error, how they change as we change those kinetic parameters. And you can see this is green dot is where the native system lies, and the orange rectangle is where the optimal, minimal speed is. You can see that most of those parameters. You can see that most of those parameters evolved to be really near the maximal speed, and you can get much, those enzymes, could be much more accurate if you were to slow down or speed up certain states, but that will come at the expense of much slower speeds. So, sorry, yeah, so Yeah, uh so the other thing uh to notice again, I'm using the same graphs here, but we we saw it in other systems that speed accuracy trade-off is not always observed. You can see that this graph, where as you decrease the error, things become slower, but there is like this is a minimum, and there's another branch here, and you can see here as you this the slope was positive, so decrease in error will result in speed up. You resulted speed up. So it seems here that if you were to increase this rate, you can basically decrease both speed and error trade-off. So that's another thing that there's not always a trade-off. And actually, when I sort of gave this theoretical talk on this in a seminar, I kind of got approached by Leniel Link, who was back then. Who was back then across the street from us in Texas Medical Center? And he said, Oh, we are actually developing an experimental system to measure error rate in translation. And what they do, they put two fluorescent proteins, M cheria and YFP, and they put a stop codon in between. But that stop codon is often frequently misread for, I don't remember which amino acid, but some amino acids. So if you didn't stop and read, you will produce both. You will produce both fluorescent proteins. So, by looking at the ratio of two fluorescent proteins, you can see how often you misread the stop column. And then, basically, once you can measure this error, now you can start doing different perturbations, metabolic perturbations and antibiotic perturbations that slow down ribosomes. And you can also do it in the strains where there are some mutations in ribosomes that made it slower. And in many of those cases, we see. And in many of those cases, we solve lack of trade-off, which thinks the ribosome becomes slower, but it also becomes less accurate. So it's not necessary that improvements of speed will lead to improvement in accuracy. So of course the interesting question is that when there is no trade off and you if and when native system lays a non trade off branch, what prevents it from simultaneously improving both speed and accuracy? Improving both speed and accuracy. And you probably can guess from my introduction, this is the third dimension that I didn't talk to so far, is actually energy cost. So, in here is the example of E. coli Rabazon. And what you see here is this is the sort of this trade of plot against speed as you change, say, this one parameter, which is the proofreading rate. So, if this is decrease of that, if it's very fast, you're very fast. If it's very fast, you're very fast, but your error is very small, then your error improves. But eventually, in here, as you go here, you slow things down, larger time, but it also becomes less accurate. And that's where the nature system lays. So the question is, what prevents it from going to the optimum? Less error, faster time, and or maybe going all the way here, the same speed, but much lower error. And the answer is. Much lower error, and the answer is the cost. So, if you look at how many ATP is wasted, so what percentage of cycles will be you tell how many basically GTP in this case for the ribosomes you will waste per molecule of product form. You can see that normally you waste about something like 3%, and this would be double 6%. And if you do the back-of-the-envelope calculation, I think actually translation consumes. I think actually translation consumes quite a large portion of energy budget of the cell, so that's not a trivial amount of energy for the sample. So another type of results, which was somewhat surprising to us first, and then we sort of generalized that, is that the whole That is that the whole if you look at the sort of the system from an energy landscape perspective and say, okay, this is an enzyme, this is the right product, this is a small period, maybe there is ER star, ERW, the high energy intermediate somewhere in there. And then you try to say, let's try to do some perturbations of this. So the question is, why is the right product more likely to form than the wrong product? More likely to form than the road product? Is it because this state is below that, or is it because the energy barriers are different? So, what parameters of these energy landscapes really determine the selectivity, the error rate? And what we found is that barrier heights really don't matter. If you perturb the barrier height from here to here, you wouldn't affect the error rate. It's completely invariant. Sorry, I say it wrongly. Running. The barrier heights matter, stabilities don't. So changing the position of the minimum don't really change your error rate at all. So that was surprising to us when we first, but then we actually look at this from a very general perspective. We can just prove that whatever you have in some sort of non-equilibrium system with some transitions between the states, you got Between the states that obey this sort of Arrhenius law, where the rate is proportional to the difference between the position of the energy of the state and the energy of the barrier. What you get if you look at any ratio of those fluxes in non-equilibrium and look at how it changes with changes in the position of the minima of this landscape, and this ratio is environment. Olaxis ratio never can be affected. Access ratio never can be affected. And basically, the idea is that if you perturb this, your rate outwards of the state in both directions will change. And that really doesn't affect the ratio of the fluxes. It just re-normalizes both of those fluxes the same way. If you look at the ratio of mean-first passage times, that will change. But the ratio of probabilities of exiting, either way, that doesn't matter. And for example, that really means that other quantities. But other quantities like energy dissipation, for example, per molecule of product form, which can also be explained by ratio of the fluxes are and there. So all the controls is in the barriers, and that really tells us actually that my original motivation of saying you only deal with the differences of a few hydrogen bonds is not quite right because that. Bond is not quite right because that really tells us about the energies of the stable minimum rather than the kinetic barriers of the transition path phase. So, okay, sorry, I'm can go ahead and sort of thinking about going beyond the sort of local to global sensitivities, we really try to understand, given that both And given that both error and normalization are controlled by only the sort of barriers of the error, the question is, is there some universal relation between the error and dissipation in here? And for this one, I'll use an example of amino selection of tRNA. So here is a selection between valanine NA solution and basically the enzyme. And basically the the the enzyme is evolved to do this, but they will now attach tier uh uh isolucent tRNA to validine, so you make a translational error here. And if you sort of take the wilder parameters and estimate what you have, you can have like sort of two seconds per step is the speed, error is 10 to the minus 4, kind of compared to, and then you see how much energy dissipates. You see how much energy dissipation you have is 30.6 kTs, and when you sort of normalize and see per step how much of the energy is dissipated, it's actually quite a lot. So 12% of the cycles will 12% of the cycles will be futile. So you will quite often actually correct an errors that you made, but even in the absence of the wrong substrate, if you put the Rate: If you put the volume country 3-0, it still will be something like 10%. So, you quite often correct the errors that you never made. So, the question is: why did the system not evolve away from that? Why did it evolve to waste, like say, 10% of the energy budget for amino isolation, which again, I would say, a significant part of the energy budget of the Sun. So, we decided to see maybe there is some universal error. see maybe there is some universal error dissipation bound and in so one and eight ATP in this case is wasted so why is dissipation so large so what we did first we started with numerical thing and we looked at started looking at sort of global sample parameters under certain constraints or current thermodynamic constraint you know what energy of ATP so you know how much energy you can have in the driver and you also have some constraints that you don't Constraints that you don't want to sort of say the rate of binding valenin is zero to give unfair advantage, so you keep the ratio of the rates, discrimination factors the same. But when you do that and you try to look at the kinetic parameters, what you see is here is this is like overall the allowable region, and you can see that the wild type is very close to the optimal boundary. So for a given error, it's So, for a given error, it's very close to where all the things are. So, it seems that there is some sort of bound that sort of this numerical simulations indicate. So, we decided to go ahead and try to find where this bound is coming from and what's the interpretation from that. We actually found the ellisical expression and we found what this bound is, and we know it's in relation in terms of fluxes, but I'm really interested in sort of understanding. Really interested in sort of understanding it more thermodynamically in there. So, basically, what happens is the native system is just within 20% of minimal possible dissipation, so you can go much lower than what you have. And the other thing is that if you look at the other way. Yeah, so what happens is that there are two types of proofreading that happens in Turner. There is a post-transfer and pre-transfer. So there are two types of futile cycles you have. You have this post-transfer, so you already sort of attach it and then you remove it, and there is a pre-transfer once. So there are two types of futile cycles, and what we found is that the existence of the second one, which is shown to be evolutionary, emerged at later times. At later times, some of the ancient enzymes don't have that. Some artillery have those that lack those domains. But what happens is that without it, the error bound is much lower and you won't be able to get to the wild type accuracy and go into this and pe those intermediate accuracy will have a ne a tremendous synergy cost. Tremendous energy cost. So, anyway, so basically, I'm not going to go over the math here, but we basically sort of first derive the sort of Pareta front boundary between the error and the dissipation. First, starting with a sort of simplified Hope-Fill scheme where all the discrimination is only a dissociation type of constant. So, the forward processes are the same for the right and wrong person, but the dissociation is what makes it different. So, you have like What makes the difference, and we have like analytical expression of that, and then we sort of generalize it for using the same methodology for the tRNA enzyme. And we have the sort of the expression for this bound, that's the red line, and indeed you can see that basically the nasi system is very, very close to it. This is a blow-up, so it's again within a few percent of the optimal. So it looks like. Of the optimal. So it looks like if you want to maintain your error at this level, about 2 to 3 to 10 to minus 4, which is sort of the same error that ribosome makes, so it kind of makes sense to match them because they're both part of this translation. You attach amino acid to your tRNA and then you attach tRNA with amino acids to the drawing polypeptide chain. So you really need to waste that much energy. So, and the last thing, if I maybe just very briefly, you sort of asked what would be the tolerable error, and it was kind of around 2020 when everyone started working on the COVID. And one thing that sort of blew my mind, they are looking at this COVID, because I read about some other viruses, and usually RNA viruses like HIV and others are very error-prone, but COVID is relatively less so. I mean, now you have. Less so. I mean, now you have all those variants, so it evolves, but overall, it really makes for a very precise thing. And it looks like the enzymes that does RNA-dependent RNA polymerization here, basically makes a second RNA out of the template, it has something like a proofreading domain. So we tried to see what's happening there. And basically, the error rate is pretty small. It's three times 10 to the minus 6, only 3 per million base pairs are made of the. Base pairs are made of the copy, but the genome is very long, so it's a 30-kilobasis genome. So, for the native sort of error rate, you get 90% of the viruses that may will have null mutations, because 10% will have mutations, and that's why you have the variants. But if you have just a ten-fold increase in that, that leads to success. Then, basically, only 40% of the viruses remain will be. The viruses may have no functionals. So we decided to look over sort of the coding thing and see which of those will be synonymous with which will change something like a charge amino acid to uncharge and try to figure out what's happening and then basically define the effective speed is what f fraction of viruses you'll have either normal mutations or mutations that we deem not to be uh very important and basically Very important, and basically, I find that the native parameters for the coronaviruses RNA-dependent equilibrium again lay very close to this top effective speed, being maximized. So, within something like 70% in here. And again, the cost argument may be what prevents you from achieving this maximum because the Because if you go to an optimal, you get 9% decrease in those futile cycles in there. Okay, so to conclude what I told you so far, that to think about selectivity in the enzymes, you have to keep track of the interplay of three quantity, different selectivity or error rate, speed of inverse mean first pressure time, and dissipation. And in many non-rate limiting steps, those schemes were actually optimized to. Or actually optimized to keep the dissipation small. For rate limits, most of the rate limiting steps are optimized for the optimal speed in there. And there's some global trade-off between dissipation and cost. And like in coronaviruses, the proofreading is required, but the genome size is very large. And it seems that it's been optimized to sort of keep the effective speed of making functional viruses. Of making functional viruses would be optimal. I think that's it, and I think we have a few minutes for questions. That was a great time. Thank you very much, Honor. So, if you reduce the number of chaplains, or you diminish the number of proteosomes that could digest proteins with errors. Do you find that actually you would slow speed down? I mean, I don't think that those things are necessarily coupled on a sort of biochemical land scale, they might be coupled on evolutionary land scale, but one thing that I'm really interested in, and I could never get the parameters done, is the ribosomes that we were looking for and the enzymes we were looking at are bacterial enzymes. But if you think about, say, our brain and our cells, where there's actually much higher cost. Much higher cost of making dysfunctional proteins, some hydrophobic patch gets exposed, you start getting prions, and type of thing. So, I would say probably some of those ribosomes, like ribosomes that function in us, may be much more optimized towards accuracy than over speed, because speed may be not as important. And bacteria, of course, ribosome is growth-limiting in majority of conditions. So, I think that's maybe goals. So, I think that maybe goes towards it, but I don't know the answer to exactly the question. Okay? Also, thanks again for this great talk. There was one thing that struck me a little bit in your images you often had. The last error was only irreversibility. Only would mean that there are at least minus infinity of it. How does it fit with you? No, no, it it it's almost reversible, but essentially reversible. So e to the minus 20, which is. Also, e to the minus 20, which is 20 kg, GTP over GTP conditions, is pretty small. I think when we do a lot of our stuff, we put that in under constraint. I draw it as a reversal because it's essentially ease, and most of the results... To calculate the dissipation or entropy production, we have to have everything reversible. For the other thing, speed and error, having it reversible, reversible doesn't matter. Doesn't matter. So I was curious about your results when you said that. So I did it in work a few years ago and we did look at those barrier heights and changing the kinetic effect. Still the kinetic effect happened to change the level of the kinetic kinetic effect was similar to it. I don't know what's really working, so I'm just taking steps. So I'm wondering what is it that's happening there that we're not seeing face? I I think both equilib not I mean in in in equilibrium state state there's no fluxes, so the ratio of fluxes don't does not quite make sense, but Not quite makes sense, but overall, yes, non-decollarians. I think maybe I would say let's talk offline, and you can explain me your thing better, I can explain my things better, because it may be a bit too technical to answer right away. And I think maybe I will run out of time.