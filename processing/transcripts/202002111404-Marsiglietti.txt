It's really great to be here. So my talk, the topic of my talk today will be or can be understood in some sense as part of the so-called discretization of convex geometry where one wants to try to find analogs of problems in convex geometry to the discrete setting. And here I will focus on the powerful technique of localization. And so there have been some So, there have been some recent trends over the past few years about this topic, and I'm going to give a bit of motivation and some reference. So, first, I should mention that this is joint work with James Melbourne. And so, the idea or the goal would be to translate classical problems. From convert geometry to discrete geometry. So there's several work around that, and I'm just going to be very selective. I'm just going to be very selective in the reference. And so I'm going to cite a couple of results. So yesterday we discussed the Bremenkovsky inequality that was discussed several times. And there have been many results around discrete version of the Brimikov inequality. I'm just going to cite, so I'll be very selective, and you might see a pattern in the choice of the reference. So first, there is a result of among all those results. Of among all those results. One of Ignidas, Nicolas, and Zalich in 2020. Another one about discrete version of Kodovsky integrity. And there have been a work of Alexander. There have been a work of Alexander Hank and Zavich 2016 and last reference, there is also some results about discrete analog of Alexandrov's CRM. And there's a result of, among many results, one of Ria Bogin, Jaskin and Jen 2017. All right. Okay, so I'm just going to silence. So The focus will be the localization technique, and so I'm going to state the standard localization technique of Lovas and Simonovitz. So, the assumption, if you have two functions g and h, so if the following inequality holds, if you have two functions g and f, g and h that satisfy That satisfies this. Then you can find a linear function such that similar so then there exists A, so two points, A and B and R N. We can find the linear function n from 0, 1 to upper. Such that similar equity will hold. So you have 0, 1 L of T G of 1 minus T A plus T B D T will be 30 positive. And the same with H. So I minus T A plus T B d T will be 3D body. Will be 30 boots. So this is Lova C mono bits. So it should be 1m. And so that was 93. All right. So the way it's stated, we don't necessarily appreciate really. Don't necessarily appreciate really the powerful, the power of the results. It really says that if you want to prove inequalities in RN for log concave function or log concave distribution, you only need to focus on a logo fine distribution supported on a segment. And so that's considerably reduced the problem to like in some sense one dimension and for specific distribution that are not defined on a segment. So this is really So, this is really a useful result. And so, maybe just a comment: this reduce inequalities on Rn for, so here it's a continuous case, so continuous concave distribution or a concave function. Or Linux function. And this is reduced to log affine distribution supported on segments. So to log affine supported on segments. To illustrate how interesting this result is. Like how interesting this result is. So, there are many isopermetic and concentration type inequalities that was derived from this. And in particular, so very recently, the base bound in the KLS conjecture, so the KLS bound of constant n1 over 4. That was proved by Leon Van Para. So, this uses the stochastic localization. So, it's not, it's a function. So it's not, it's a variation, so uses stochastic localization. So some form of localization, and that was used by, that was developed by Eden. So more a stochastic version. And so yeah, so that's it could be very powerful. Powerful. And maybe just a last reference, because there are really many different settings where there's like localized technique has been developed. There's another setting which is due to graph tag. It's in the Riemannian splitting. So there is an extension in this case, the Riemannian setting. Alright. Okay, so so far it's called the classical correct. Okay, so so far it's called the classical uh version and so there are many uh some extensions, stochastic or manian. And so in this talk we are going to focus on the discrete version. So here we are going to work on discrete random variables. So I'm going to define now the object of study that I'm going that we're going to work on. So for the discrete setting, so here when we look at this inequality, it's not necessarily clear what would be a discrete answer. Clear what would be a discrete analog because the one minus T A and T B don't work very well, the discrete setting. But instead, we are going to use, actually, so that's maybe last reference, and the version that we are going to extend will be the geometric localization. So it's more general than the Lovasi model bits. And this is used for ideas in the P E So this one. So this one will be more clear what will be the discrete analog, and that's the one that we're going to work with. So I'm not going to state it because it will be very similar. The discrete case will look alike. So now let's go to the discrete setting. And so the object of study would be this one. So we have Have so we are going to work on log concave sequence. So a function, so if you have f from n to 0 plus infinity is low con k if, so the following holds, we have f of n greater than f of n minus 1, f of n plus 1. So So that will be the discrete version. So we're looking at those log concave functions. Alright. So there is an extra assumption here. In addition, we assume that, so, is La Punca if you have this inequality and if f has contiguous support. Or in some sense, the support is convex, but easy. So if you want to look at, so F is positive on, so if AB and F positive, this implies that F will be positive for the whole. This implies that f will be positive for the whole. So if you take any k in A and B So can we be that same support is convex. So convex in A so So, there is no horse in the C part, right? So, it's a succession. Okay. So, this is the main object of study in the discrete case. And so, why is it important? I mean, there is different answers to that question. So, I'm going to give maybe one more definition. So, we're going to So we're going to look at discrete Laplanque random variable, which is just, so x is Laplanque if it's probability mass function. So PNF. If this is log monkey, so if it's a log monkey function, is that sense? Okay. And so there are many there is a marginal definition, but okay, let's give some examples. So all the main distribution district case that you think of are like concave. So you look at Pernouille, binomial. Geometric, there's many, much more, Poisson. So they are all concave. So, and so there are many motivations. In the combinatorics or in discrete math, they really lack concave sequences. And so, and discrete probability, the main distribution. The main distribution are local. So it could be good to have some technique, some results that could apply to all of them at once, instead of studying them separately and having almost the same results. So there is some sort of more general properties that will hold for all of them. And the nice thing is that, so in probability, we like summing random variable or something independently. And sum of la concave is la concave, even in the discrete. Discrete in discrete setting. And so if you sum all of them, you can get, so they are still low-concave. So if you derive property for all log concave, then you can get for all sums of those like main distribution. So there is a more general definition that I will introduce, because what we're going to do, the location technique can be used to look concave with respect to a reference measure. So if you have gamma, which is just a measure Which is just a measure integer variable measure with mass function let's call it Q. So it's just function from Q, so from n to R plus. Not necessarily of total mass one, just quality larger. And X will be And x would be not concave with respect, so respect to gamma if, if you look at P over Q, so P will be the, so X will always have P and F P. So P over Q is not concave. So in particular, there is different distributions that are studied in literature that can be seen as being That can be seen as being La Concave with respect to a specific reference measure. For example, there is a ultra la concave distribution. And ultra la concave would be if you take the reference measure to be Poisson. The regular la concave is just to take q equal to one. But you have many much more. And so next, we're going to describe the location technique for the those log conquer distribution with respect to an arbitrary reference measure. So now we state the n-CRN. So I just rewrite a couple of notations. So you have you fix a natural number n and we are going to denote n to be the discrete interval 0n. Now we are going to consider all the measure Are all the measure supported on? And so this is just, I'm going to write it this way. So this will be, so you have Pn gamma, so, okay, so this will be just all probability measure supported on zero. So I'm going to consider that. Okay. Next, uh we have gamma, which is just uh measure with contiguous support. So there is no hole in the support. And so finally, we are going to denote the set of considerations as it is done in the continuous case, in the geometric. In the continuous case, in the geometric localization. So we take another one. So you just have function patch function. Alright. And the set we're going to optimize in subset will be this one. So you get so p h gamma. H gamma N. So I write it this way. It's all measured, C proponent N, so I write it this way. So probability measure N, so such that X is not okay with respect to gamma. And there is a linear constraint that this integral is non-negative. So integral This integral is non-negative. So in the graph, so here is discrete, so it will be a sum, or discrete sum. So this is non-negative. So I wrote it this way: so it's all measured, so x is associated with, or x with distribution px. So all measures, locon caves with respect to gamma, such that you have this linear constraint. So that as a set we are going to consider. Okay. So then others. Okay. And so the main vector that we have is we are able to describe the set of extreme points of pH gamma. And so here it is. So if you have any convex function phi, so it's phi from this set P H gamma n convex if you want to optimize phi on this set. So supremum of phi on P Of phi on P. So our whole distribution is Q2. And PH on IN. So you look at all measure Px in this set. If you want to optimize, want to maximize this function, you just need to look at the extreme points. And the extreme points are exactly the ones that are logarphic. So this will be the supremum of phi px. I put a sharp such that such that Px sharp will be, and so I will call it in A, let's call it A for Fi H gamma. So if you want an equality for all log concave distribution, you just need to look at log Fi. So what is A? So it's just what we expect A to be. Seg A to B. So you intersect this set. And in addition, x is logged with respect to gamma. So what does that mean, logarphine? It's just that the inequality I wrote for La Point function is just an equality. In particular, the shape is clear. The shape is clear, so x will have PMF of the form P to the N and the constant on some support KL or KL. And so you have an explicit expression for those distributions. And so here the computation is explicit. If doable. That's always in localization. You can compute things, but sometimes the computations are complicated, so it depends. The computation are complicated, so it depends on the product you are working on. So, here you can really respect to those distributions. So, it's with respect to gamma, so it's P over Q that would be in this form. So, the proof is similar to the continuous case, you just need to adapt the argument and things work well. And things work well. So, here the first argument is to find the extreme point and you apply some sort of Premium-Mann theorem. And why is extreme point out level fine? So we draw a picture to Or maybe I'll use a more geometric argument. So if you have a log, let's say with function, you have a log concave function, so the algorithm is concave, and you want to cut at some place, so let's say, and if you consider an affine, so you have the same support, say here. So in some sense, you can consider, so if I'm doing this right, you have one part which we Right? You have one part which will be uh let's say something like this. So you you can cut your and something be like okay, maybe not a good picture, but if you have a Laura Poincare function, you can write as write it as a sum of two other Laura Poincare functions, and that's if it's extremal, everything has to be equal, so it has to be equal to to a la. To be equal to a line. So proof with hands. But everything can be written down, and the idea is just to start with the La Ponque function, in this case to be discrete, and you write it as a sum of two other parts, where if the part are equal, which is the case when it's extreme, then it implies that it will be log f. Okay. So you have two parts, one here, and Have two parts, one here and one here. So, or maybe the other one, or maybe you should take this part here and here, and this part here and here. So, if those two parts are equal, it has to be alike. So, everything is So, everything is to adapt the continuous case, and things work well. The way we want to put it in the to frame it in the speed case will work. And so, now I'll just focus on the few minutes I have, just on a couple of applications. So, in particular, we can apply this further to compute probabilities connected to discrete concrete distribution. Distribution. So all the versions that we have in the continuous case, we have it in the discrete case as well. When there is like this Lovas and Simon Lovitz techniques can be written in some way in the discrete case as well. Not with a 100 A plus T B, but it will be very similar. And And all the similar inequalities can be written down. Now I just want to discuss different type of application. And in particular, we can find Laura deviation inequality or probability bound. If we take for phi to be just the linear function, which is standard in the continuous scale, that's what is done. So we can do the same. What is done. So we can do the same argument in discrete case. If you just look at project X in A, you can apply the localization technique with type of function. So if you want to compute a supreme overall dis, you just need to compute it when x is log fi. And in particular, what you can prove, you can prove bound for every log concave discrete. You can prove something like probability that x is greater than t, for example. And you can have like some bound that doesn't depend on x of the form c1 expansion minus c2t divided by expectation of x. So it has to be bound to the expression of expectation of x at some point, and so you can get those type of bounds for arbitrary lowercase discrete. Locke discrete Law concern distribution. And so, as far as we checked on the literature, we couldn't find something such a general inequality for all Law Concerns. So, what exists is for a geometric or some of geometric or specific distribution, but all of them are law concerned. And those ones can be derived using the localization techniques. So, that's one type of application that we can get from it. There's another result which is well known. As for the continuous case, the discrete non-concave distribution have moments of all order. And actually, as a consequence of this, it's not going to be optimal, but another application is we can compare all moments up to some constants that depend on R and S. So you can get the same result. When X is discrete, so all of that. When x is discrete, so all of that is for in the discrete set. So everything that we have in the continuous case, we can have it. So it was known that all moments exist, but not clear bounds between them. And here we can compare. So we have some explicit constants. It's not optimal, but we can do not mention, but it's still a work in progress, so we're still working on trying to have more applications and improving some of the bounds that we have. But basically, we can recover many of the continuous results in this case. results in this case. And another one, the fact that log concave are another application, if you want to prove that the log concave, I guess right is like this, the log concave sequence are stable with respect to reference measure. If you want to prove this is true, it's enough to check the affine. So if you have two affine, two of the affine distribution, if you can prove that it's If you can prove that this is La Concave, then you have it for all La Concave. And in particular, you can compute these things. Everything is explicit, it will be of some form like this, more or less. And in particular, we can recover that if you take gamma equal to one, here the computations are doable. So we recover the classical fact that standard or concave are stable. But you can add many other reference measures and try to find for which gamma, what are the properties on gamma that will give that. So you can get some. That will give that and so you can get some more general results here. So I think I will just stop here and thank you for your attention. So, do you know something about discrete random vector? The middle vector. Yeah, so this is for random variables. The problem with low-concave random vector is that there are many definitions and there is not a common definition of what is a low-concave random vector in the discrete case. And so then you need to make some choice. It's not clear what will be relevant here. So it's a bit different. Okay. 