Thank you. Thank you so much. Just a second. There we go. All right. Quick disclosures. I have a pattern application involving some machine learning stuff, which is licensed to Adela. And I want to thank the organizers for inviting me to this. When I first was invited, I thought I would talk about our work on virtual ChIP-seq. And then I looked at the people who were invited and saw that a number of them have probably heard me. Have probably heard me speak about virtual chip-seek before, including right there at a BERS meeting. So I decided to talk about something else so everyone would have something new. But if you follow the virtual ChIP-seq work, which is for modeling of transcription factor binding site prediction, the journey has finally come to an end and it should be in a journal later this week. So that's my plug for virtual ChipSeq. So that's my plug for virtual chip-seek. And now on to something completely different. So there have been a lot of machine learning publications in biology. This paper attempts to find out how many. And with this search, they have found we've gone from very little in the 90s to now almost 9,000 in the past couple of years. And that doesn't even include preprints and it doesn't include the huge acceleration of things in COVID. things in COVID. So, you know, suffice to say, there are a lot of machine learning publications in biology as we all understand. And if you've read papers in machine learning and biology, sometimes there are critical details that are left out. And there's been an increasing need understood recently to have guidelines so that we can Guidelines so that we can, you know, understand, so that, you know, people are sure to include details that are necessary for others to understand the experiments, understand the models, and reproduce them. So here's a new guideline that I like called the DOME model for data optimization, model, and evaluation. It includes a number of things that you should describe in your paper to make sure that all the bases are covered. By the way, if anyone By the way, if anyone has a question or a comment in the middle of this, feel free to either turn on your video or put it in the chat, put it in Zoom, put it on Twitter with the 22 W5085 tag. I'll try to answer it. There are other reporting guidelines, such as this one is for called Consort AI, which is more focused on more medical trial. Medical trials involving AI or machine learning. All right, this one is a little bit longer. And then we have this other one, which is called Spirit AI. Not only has there been an explosion of machine learning papers in biomedical research, but more recently there's been an explosion in reporting guidelines here. How well do these things actually work for ensuring reproducibility of the work? Let's take an example. Let's take an example that's a little bit older, which is this nature research reporting summary. If anyone here has submitted a paper to something in the nature portfolio journals in the past few years, you probably had to do something like this. It has a number of nice checklist items to make sure that people cover important things like listing their sample size and various other things. And some of it is focused on software and Software and code. Again, you know, at the top, it says the point of this is to improve reproducibility. All right, so here's an example from a paper where a group at Google developed a machine learning system to interpret radiology data and make predictions of cancer diagnosis that was reported in Nature in the beginning of 2020. All right, so this is. 2020. All right, so this is a screenshot from their reporting summary, which is publicly available. Let's look at how well this can help us with reproducibility of things. All right, so the reporting summary is quite detailed. It gives us things like software versions, right? You can see DMCTK 3.6.1 with a particular date, you know, NumPy 1.16.4, very, very specific. 16.4, very, very specific versions, right? So, this is something that, you know, there can be little changes in the way these things work, and scikit-learn works from version to version. This sort of thing can really help us with reproducibility, right? What else do we get in this sort of checklist? We have code that says release, oh no, right here. It says release of the code is not feasible because reasons, right? You know, data, the data. You know, data, the data is not publicly available. So, people went to this great amount of effort to improve reproducibility. They're doing things like listing software versions with 13 digits in them, and then saying, oh, but you can't see the code or the data, right? This is fine, right? This is just like, is this the new normal people can? Is this the new normal? People can, you know, create these elaborate checklists and say you have to supply this detail and this detail and this detail for reproducibility, and then people can just put in A in some of the fields. I don't think this is a good solution, right? In this paper, you know, the authors claimed that all experiments and implementation details are described in sufficient detail in the methods section to allow independent replication with non-proprietary libraries. I was involved in an effort. Was involved in an effort to try to reproduce some of this, led by Benjamin Hape Keynes. And we started looking: is this actually true, right? This claim that it can be replicated based on what's in the method section. I mean, anyone who's ever tried to reproduce some software based on what's in the method section of the paper will probably tell you you know. In this particular case, you know, we can see that a number of hyperparameters for reproducing the study are The study are just missing entirely or unclear. And the summary of our paper is that without this key information, independent reproduction of the training pipeline is not possible. So hopefully the reporting standards, all these new reporting standards, of which I've only focused on three, there are even more coming out every year. Hopefully, you know, the Hopefully, you know, that they can address this problem and not just be extra paperwork. Let's look at the first one, the DOME one I talked to you about. So one nice thing about DOM, they don't actually require for a DOM compliant paper that data availability, model availability, and code availability occur, but you have to disclose if you don't make them available. And they do actually recommend that you make it available. You make it available, right? How are these other things? You have a consort AI with dozens of items that you have to fill out in great detail about your study, right? They don't even recommend that people make their code or model available. They don't even require that people, if they aren't going to make the data available, disclose that, right? I think this just comes from an entirely different culture where people aren't even thinking about other people. Thinking about other people looking at data that they have collected. And to me, since one of the main purposes of these reporting standards is to increase robustness, rigor, reproducibility, they really fall down on the last part, on the reproducibility, if they aren't going to have people include the bare minimum code, data, and models for reproducibility to take place. Spirit AI is much the similar. Spirit AI is much the similar thing. You have to disclose non-availability of data, but they don't require that you make any of these things available. So can you reproduce work that is produced under one of these studies? For DOME, if the authors follow the recommendations and you're lucky, then maybe for Consort AI and Spirit AI, it's theoretically possible, but there aren't any guarantees. And in practice, Guarantees, and in practice, you know, you probably won't be able to. All right. How did we get here? How are we getting these reporting standards that aren't actually fit for the purpose of enabling reproducibility of analysis? Well, one reason is that some of these are produced through something called a Delphi process. Process. And, you know, you may not have heard of this. I had not heard of the Delphi process several years ago, even though it's been around for decades. And now I seem to be invited to all of these Delphi processes on how to set standards for various things all the time. If you haven't participated in them, what they are is essentially a very fancy survey, right? So you do a survey about what practices should be followed in a particular Practices should be followed in a particular area. You know, you get usually dozens, scores of experts, and you might ask whether a particular practice is required or should not be there on some sort of Likert scale. And there are multiple rounds of this. So maybe something in the first round, everyone agrees that it should be included in the statement of what is required for the standard. And in a later round, A later round, you know, things where there's more disagreement will be hashed out a bit more. People will explain their reasons, you know, and usually there's kind of a final in-person or these days on Zoom meeting for people to really hammer these things up. But remember, these are supposed to be consensus statements, right? So the advantages is that they can be informed by very large pools of experts. You can get You can get, you know, tens, dozens, you know, a few hundred people involved in some sort of Delphi. You know, sometimes the people involved in a final meeting might be smaller than the people in the initial survey. And you can derive some sort of authority from common belief, right? So you can say, you know, we polled, you know, 500 experts in this field and, you know, almost all of them agreed that reporting sample size was Sample size was very important. What are the disadvantages? Well, the problem is that not everyone in such a group, maybe everyone will agree that reporting sample size is important, but maybe not everyone will agree that releasing code so that reproducibility is actually possible is important. There's some amount of regulatory capture there, right? So if people think that it's in their interest to not release their code, they might vote. code they might you know vote for not putting releasing code in the in the standard um these are backward looking by nature they're often talking about you know what you know what are the practices that people think are standard now and not really working towards changing the culture and making improvements that will will benefit benefit people so a group of us initially led by Casey Initially led by Casey Green, and including Stephanie Hicks, Ben Howell, and others, decided with all these reporting standards coming around, we wanted to create a different kind of standard, which is instead a reproducibility standard, right? A lot of these reporting standards, people talk about, you know, the reason for the importance is to enable reproducibility or address a reproducibility crisis. If that's the point, then instead of If that's the point, then instead of focusing on reporting and bureaucracy, we need to focus on reproducibility. And that's what we've done here. We came up with a reproducibility standard. And I think the ideal reproducibility standard should be functional, right? So it should be mainly about is something reproducible? How reproducible is it? And not, you know, did you check 47 different boxes? Are all of these. All of these are all of these different statistics included in your paper? You can include all of those statistics and still have something not be reproducible at all. Reproducibility standards should provide some guarantees for readers. If they know that something fits the reproducibility standard or it's claimed to fit some reproducibility standard, they should be able to reproduce it. They should be able to trust the analysis to a higher level. And also they should recognize a spectrum of ease of reproduction, right? Reproduction, right? There's, I think, a wide spectrum, especially if you're talking about reproduction purely of analysis between something where, A, there's zero reproducibility, right? You can't get the code, you know, that's, or near zero if someone is claiming that you can recreate the code yourself based on some prose description and the methods. If you have. And the methods, if you have hundreds of man years available, sorry, human years available, to something where you can run the code, but the code is really hard to run and no one can actually get the code to run except for the initial study team. To something that's on the other extreme, where maybe you can click a single button and rerun the whole analysis, right? It takes a lot of, a very different amount of effort to say, Very different amount of effort to say, dump all of your scripts up on GitHub versus provide something where someone can press that button and reproduce everything. And we wanted to recognize the difference between such things. So instead of having essentially one level of, you know, this fitting the reproducibility standard or not, we came up with different levels, a gold, silver, and bronze level of reproducibility, and we describe these. And we described these standards in this really quite short paper in Nature Methods last year. And I'm going to go over some of these things now. So the minimum standard, the bronze standard here, is that the data be published and downloadable, models be published and downloadable, and code be published and downloadable. And this is something, these are things that a lot of journal policy. Things that a lot of journal policies or funding agency policies essentially say already should be taking place, right? And yet sometimes it just doesn't seem to be happening. But if you are, you know, reviewing a paper, you should maybe consider, you know, does this thing meet the broad standard? If you're producing the paper yourself, you know, if you're trying to explain to people in your lab what needs to be done, this might be helpful. What do these different things mean? Right. Sharing data. You can never reproduce a study without the data. It's just going to be impossible. So you need to share the raw data somewhere. And if someone is using pre-existing data, they need to define it with specificity. So I've seen a lot of papers where people are like, oh yeah, we downloaded data. We downloaded data from ENCODE, right? They aren't creating any new data so they don't have to share it, but they also are just making vague statements about where their data came from. That's not any more reproducible than something where someone has created new data and didn't share it, right? So you have to include information with specificity, so accession numbers, URLs. Ideally, someone would have a script that downloads the data. that downloads the data from some other source and pre-process it. This is what I require people to do in my lab. If they download data from some website, try to set things up so they actually have a script that does that and will be way easier to reproduce. And years down the line when they're wondering exactly where that data came from, they will have a record of it. So this is the point. So no questions. No questions or comments yet. This is the point where usually people start to say, well, what about, you know, what about human subject data? Well, you know, first, there are a lot of ways to do this. And first, there are a lot of things people are working on that don't have human sensitive data involved, right? And for those, you really can just share the data, share the data popular. Just share the data, share the data publicly. And I'm not sure we should be going to the least common denominator of what's going to work for all researchers in every field of research. People who don't have that excuse need to share their data. But even if you have human subject data, there are a variety of ways to share it. There's a nice review here by Bird and colleagues describing ways of sharing human data. So public sharing is one, but another Sharing is one, but another is controlled access sharing, you know, where you use things like European Genome Phenome Archive or dbGaP, where people put data in a centralized repository, and it can only be obtained by bona fide researchers after approval of data access committee and some sort of data access agreement is signed. To me, this would satisfy the requirement of the Braun standard to share data. To share data. Whereas, if someone just says, oh, we can't because it's private, well, you know, that's fine, but they don't meet the standard. So that's another thing is, you know, the standard, maybe not everyone will be able to do this, but then, you know, you just don't get to say that you have that guarantee. There are also a variety of other ways of sharing data. Of sharing data. Often people share summary statistic data. People can add data with noise. People can, if they're creating a model, they can generate new data similar to the model, similar to the data that they trained their model on without actually revealing any data from the original individuals. But it can be, as anyone has tried to use any sort anyone has tried to use any sort of analysis software before, it can be really, really useful to have data that's kind of in the right format, right? Even if it's totally synthetic data, people are not going to be able to reuse the data easily, sorry, reuse the method easily if they don't have any sort of data for it. Celia, maybe I'll get to that particular question later on. Where does the data go? Specialist repositories when possible. Repositories when possible, right? So, gene expression, you know, that should go gene expression omnibus. People are often, you know, come to me and they're like, why have all these microscopy data? And they're huge. And, you know, where do I put them? They're now an increase, you know, there are specialist repositories just like GEO for microscopy data. So BioImage Archive is one of them. Sam Wilson in my lab and I wrote a paper last year about sharing biological data where we go over. Sharing biological data, where we go over lots of different ways of sharing your data, different kinds of data, including genetic and genomic data, microscopy data. You know, and if there's not a specialist repository described in this paper or elsewhere, there's generalist repositories. If things are under 50 gigabytes, you can share it there. If greater than 50 gigabytes, Dryad is an option. Zenodo will often let you put things in there by special request. By special request, but it can never go somewhere where it's 100% under author control, right? So it can't go on the lab website, it can't be on GitHub, it can't be on an S3 bucket, right? That doesn't solve the problem. We need it somewhere where it is under some amount of rigorous third-party control. So the bronze, the next part is sharing models, right? And when I say model, Right. And when I say model, right, that can mean a lot of different things to people, but I mean everything about the model, not just the structure of the model, if it's, say, a neural network, but also things like hyperparameters, also things like trained parameters, right? While it is theoretically possible in many cases, given the data and the right code, to retrain the model and generate, get the same or a similar model, it is kind of a huge waste of effort. Model, it is kind of a huge waste of effort and makes it way harder to actually scrutinize the results or to build on things. Where do the models go? Specialist repositories impossible. Julian, I see Julian is here. Julian is instrumental in the creation of Kipoy. Sorry, I probably just pronounced it's Kipi, right? Kippy, which is a model zoo for genomics models. There are other ones. Genomics models. There are other ones for single-cell genomics, Safara. Otherwise, you can upload data to Zenodo, never under 100% control, not the lab website, not GitHub, not a random S3 bucket. Those things all go away. They're not permanent. Even unintentionally, they can rot pretty easily. Final part of the bronze standard is sharing code. It's unlikely to replicate exactly from a method. Likely to replicate exactly from methods descriptions. Code is a better description than the methods text. You aren't going to miss anything in the code, hopefully. And I treat papers without code if they want to withhold the code, like this Google paper, with similar skepticism to papers without methods text, which is something that people seemed to try a few years ago when, say, particular funders were requiring people to put their stuff on bioRxive and said, okay, well, On bioRxiv, and said, okay, we'll put things, we'll put, you know, we'll put papers up on bioRxiv, but we won't actually describe any of the methods. We're just kind of, you know, data without any of the methods used to present it. And that went over very poorly, I think, for those people, because it's not really science to do that, I think. And I think people should be similarly skeptical if there's papers that don't have any code attached where the code is big. Attached where the code is a big part of the paper. Code, you know, you need to go on Zenodo. You know, just putting the code up on GitHub is not good enough. Here's a broken record here. I don't want to say I don't like GitHub because I love GitHub. GitHub is where we put all our stuff for our lab, and it makes for great resources for further development, for people to file issues, for people to download the software way easier than. Download the software way easier than Zenodo, but it doesn't provide the guarantee of persistence that putting somewhere that meant a DOI like Zenodo does. So in my lab, what we do is, you know, if there's a paper, we bake a snapshot of the code associated with it and put that to Zenodo. And then people are able to reproduce things in Zenodo. And it doesn't stop us from continuing to update the software. But if you don't do this, it's sometimes hard for people to identify. Don't do this, it's sometimes hard for people to identify exactly the right version of the code that you used. All right, so that's the bronze standard. All right, it's already quite a lot. I like to think of the bronze standard as really the minimum that is necessary for someone from the original study team to reproduce the work. It still is often quite a lot of work to reproduce something just based on data. On data and code, especially if the code is a little disorganized. So, the silver standard addresses a few additional pain points for reproducibility that, again, probably some of you, if you've ever tried to reproduce any published study, have come into. One is dependencies have to be well organized. Key analysis details, I'll say what that means in a second, have to be recorded, and analysis components have to be set to Analysis components have to be set to deterministic. So, one is fun with dependencies. There are often lots of dependencies for software. And it can sometimes be a not so fun puzzle to find the right versions of dependencies that can run scientific analysis software because something may have changed in one dependency package that conflicts with another and so on and so forth. There are ways of freezing. Of freezing essentially the versions of the software they used for a particular analysis. And that to meet the silver standard, that sort of thing has to be provided. And people need to be able to install all the dependencies with a single command using some sort of dependency management tool. Either that's the, you know, if something's written in one language like Python, you know, it might be the standard packaging software for Python like PIP. You know, more often these days, people are You know, more often these days, people are using things with lots of different languages and environments. So, things like PACRAT or Conda can help you with that. And the key is, you just don't want people to have to spend days just to get the prerequisite software installed. We really want to end that. Another approach people like to use is container systems. I think Docker has great promise here. Although I will warn you, from a narrow perspective, Warn you from a narrow perspective, having something like Docker can make something really reproducible, but can also make the code brittle if you have code that will only run in a single Docker container or only run in a singular OVM and will not run with later versions of dependency software. It can kind of be a problem for reuse, but it can enable first-level reproducibility. Reproducibility. The analysis details I mentioned, one is just, you know, in what order are the components used? So sometimes people, you know, to their credit, will put up the scripts that are needed to analyze the data that went into their paper. But, you know, someone new looking at it will not have any idea which scripts to run in which order, you know, what goes where, et cetera. There are a variety of ways to solve this. You can put the instructions in a README. Put the instructions in a README. You can order file names, like you can call script, you know, script one, pre-process, script two, train, you know, three, underscore, calculate posteriors, that sort of thing. Even better is to have a run-all script that will do it all yourself. Include in the paper things like, you know, how much time did it take to run this analysis and what kind of resources are required, right? Another challenge with trying to reproduce these sorts of things. With trying to reproduce these sorts of things, is you will try to do it and then find that it requires hundreds of thousands of CPU hours to do it. It's really good to know that sort of information before someone tries to do it. And also, I think it's kind of important information in the consideration of the paper in the first place, right? If someone is saying that something is going to be a method that is going to A method that is going to revolutionize everything and everyone's going to be using it everywhere. If it takes resources that would cost hundreds of thousands of dollars on the cloud, that's something that should probably be considered in the significance of that work. Finally, is the challenge of randomness in deep learning, machine learning models. People are often using random numbers. You have to save the seeds used for these. Save the seeds used for these if you want people to be able to reproduce them exactly. A lot of deep learning libraries are from people who are not in the same training milieu as many of us, and they may not prioritize determinism, right? But increasingly, this is recognized as an issue for some common machine learning packages like PyTorch, where there are additional options that people can use to prioritize determinism to reach. To prioritize determinism to reach the silver standard, you have to use these. Finally, publishing the train models as a backstop. It might be very difficult due to the interaction of random factors to have something be able to reproduce the final train parameters exactly, which is one reason why you probably want to publish the train models and why it's required in the Bronze Data. And why it's required in the bronze standard. All right, the gold standard is the silver standard, plus, the entire analysis needs to be reproducible with a single command, right? This is quite a big deal, right? It's kind of the dream. You know, this is something, you know, again, I don't expect everyone, I don't even expect most people to attain the gold standard. But, you know, for people who do, this is something that. For people who do, this is something that should be recognized, I think. I think this should be recognized in these papers. And I also think this is something that if you set out to do it in the first place, it's going to be easier than if you try to bolt it on at the end. And that's true for all of these different levels of reproducibility. If you have a compute intensive analysis, it can be difficult to imagine something where you hit a single button and it reproduces the entire. And it reproduces the entire workflow and sorry, the entire analysis. And for that, you know, you need to use things like workflow management software. You can store and publish intermediate results, and then there can kind of be a nested approach of reproducibility where, you know, maybe the train model is there and published, and, you know, it might take a certain amount of time to regenerate the figures if you have the trained model in the directory. But if you delete the train model, directory but if you delete the train model then the workflow management software will automatically recreate it just taking more time that sort of thing all right so here's a a table of the reproducibility standards that appeared in our in our paper ron silver silver gold as you can see even the the um gold gold level only has seven items right so this is considerably simpler than um Simpler than all of these other reporting standards and actually provides you guarantees of availability of data, model, and code at different levels of ease of use, and I would say different levels of reproducibility. All right, so that's all about the standard. And then there's the question of how do we make reproducibility a priority? Priority. And I'd say, you know, so I'd like to, this is an interactive portion of this. So I'd like to invite people to use the Zoom reactions feature to share their opinion here, which is the most important aspect of evaluating research output. If you think it's quantity, then use the green check mark. Check mark. If you think it's quality, then use the thumbs up. And if you think it's popularity, then use the ta-da confetti. So if you haven't used this, you know, reactions feature is probably underneath your, where you can see my face. Okay, so we have some answers for quality, it looks like. Like any other answers? Okay, pretty much everyone I'm seeing who's responding. There's there's there's at least uh you know a couple answers for for quality. I'm sorry, this is a little confusing, but thank you for participating. I find that usually for such things, you know, most people say quality is the most important, important thing, and a few people say quantity. Very seldom do people say, say, popularity. Them to people say, say, popularity. How do we measure these things, right? Measuring quantity is pretty easy. You know, you can print out the publications and put them on a balance. You can just look at the number of publications. How long are they? Is someone producing two-page papers? Are they producing 100-page papers? You might laugh at this, but sometimes people are like, well, papers in this journal are super important because they're so much longer than papers in this journal. Often people are You know, often people are looking at, all right, sure, Sala, popularity instead, right? So they'll look at number of citations to the paper. They'll look at number of citations. This is my favorite, right? Number of citations to other papers in the journal was published in, right? This is something where people are using a proxy that they don't even need to use because To use because nowadays you can get the information, the number of citations to a particular paper if you really want to see how popular it is. And then there are, you know, people like to talk about these because of the problems of looking at number of citations to papers in the journal. They like to look at altmetric, downloads, tweets. I have a lot of tweets. I don't think looking at number of tweets is good. I think it's even worse than number of tweets. Sorry, then the number of citations it's easily gamed. I mean, the question was: what do you believe? Right? Then this question of quality: how do you actually measure quality? It's hard. You know, people like to, I think, think that some of these measures of popularity, like how cited a paper is, is a measure of quality, but it's really not. You know, you can see papers that are, you know, like look at this arsenic life paper in science. Arsenic life paper in science from years ago. Lots and lots of citations, lots and lots of talk. You know, most people in the field think it's total, total bunk. You know, you can see this thing sort of thing all the time looking at retraction rate of papers that are in journals that are cited more and so on, so forth. Why aren't we actually looking at quality? So one thing, there's been the Declaration of Research Assessment or DORA, which is, you know, DORA, which has increased in popularity of the last few years, which is so people should stop looking primarily at what journal something appeared in. And it's been endorsed by a lot of people, different organizations, including all the funders in Canada, sorry, the government funders in Canada, major funders elsewhere like Wellcome Trust, European funders, and so on. And, you know, this is something where institutions we mainly look at research. Institutions, we mainly look at researchers at hiring, at annual review, at promotion review, right? And even though institutions, such as my own institution, you know, say they want to sign up for DORA and stop just looking at what journals people publish in, they keep doing the same thing over and over again and expecting different results. In this case, the thing they're doing is submitting a list of article names and journal titles to overworked reviewers, right? That's if you keep doing that, even if you... That's if you keep doing that, even if you tell people, oh, don't look at the journal title, don't look at journal impact factor, you know, it's not, it's not going to get you anything different. So we need to start looking at other ways of, you know, research, what's, sorry, the researchers under review. And, you know, one thing people have been talking a lot more about researchers articulating, you know, what the impact of their paper is rather than just saying, oh, it's in this journal where they got this many citations. And also researchers can do the same thing with quality. Can do the same thing with quality. Researchers can. I know I'm cutting you off a little bit here, but we're cutting into our, yeah, just a couple more minutes, if you can. Okay, sorry. Sorry, sorry. I thought it was 40 minutes. Yeah, so yeah, so quality impact, what formal standards do the work need? And this is something, you know, where you have a standard like this, or even the other standards that I like less, like Dome or Consort, you know, sorry, or Spirit AI or. Sorry, or spirit AI or whatever, you can look at them and say, you know, this is a standard that we meet. So if you want to improve things, you know, I mean, it's like, what are we evaluated by? Well, the people who are doing the evaluation is us, and the people who are setting the norms here are us. So, you know, I would say, you know, if you want to improve things, one thing you can do, if you are meeting these standards, right, is saying, Meeting these standards, right, is saying, you know, here's the standard that I meet in my paper, in my availability section. You know, we met the bronze standard or we met the silver standard, right? And it can also be important for, you know, if you're an overworked PI like me, you know, pushing to your trainees, this is what you need to meet instead of just, you know, repeating everything. And you also need to signal these reproducibility practices are important. If you're reviewing a paper, you know, compare them against this standard, against other standards. If you're, you know, reviewing people's grants, look to see if they have. You know, reviewing people's grants, look to see if they have a commitment to a particular reproducibility standard. When I'm hiring postdocs, I ask for representative code. You know, do anything to focus on quality if that's what people say is important. And I think they believe it is important. You have to start signaling that these things are important and following up on that. Anyway, thank you very much. And that is it.