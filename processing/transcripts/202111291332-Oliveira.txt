Great. Well, let me thank the organizers for this very kind invitation. It's a pity they couldn't go in person, but well, we're all used to that at this point, I guess. And thanks to you all for being here. So I'm going to be talking about joint work with these two gentlemen. Gabriel is a PhD student at Gr√∂ningen, about to finish, and he's working with Danielle. And as I was saying right before the talk, I mean, I sort of promised Governor. I mean, I sort of promised Gobert that I give a talk that's kind of statistics related. It's not quite, but maybe there's something of interest here for a statistical audience that's related to graphs and evolving graphs and things that happen on top of evolving graphs. So I'm going to be talking about this contact process. What is this? Well, it's a continuous time Markov chain. And here we have a graph. And here we have a graph. So the states of the chain are subsets of vertices in the graph. In this case, vertices are infected. So they're always going to be painted red in this talk. All the other vertices are going to call them healthy. And the set of infected vertices evolves, and there are two things that happen. So it's a continuous time process, right? So at rate one, each infected vertex becomes healthy. And you can imagine that each. And you can imagine that each vertex you have a Poisson process. Each time the process rings, if that vertex is sick, it becomes healthy. And there's also contagion or transmission or infection along edges. For each edge, you have a Poisson process with rate lambda. And when that rings, if one of the endpoints of the vertex is infected and the other one isn't, then that other endpoint becomes infected at exactly that time. So it's a continuous time mark of chain, and these are the transitions. Well, I mean, it involves a continuous time. So psi a of t is the state at time t. At time zero, it equals a. So that's why there's an a in the notation. And you add vertices x to that set at a rate that's lambda times the number of edges connecting that x to the set of infected vertices. And you remove vertices at rate one. So there's a huge literature in this process. Huge literature in this process. It was introduced in 1974, actually in the context of infinite graphs with bounded degree that's sufficient to guarantee that the process is modified. And there is a state for this process that's absorbing. So if you reach it, you never leave it, which is a state where everybody's healthy, right? Because contagion needs when an edge rings, you need one of the endpoints to be infected for the other one to end up infected as well. So everybody's healthy, nothing ever happens. So, for finite graphs, you can prove Borough-Contelli is some sort of argument that's really simple: that this all-healthy state is almost surely going to be eventually reached. And the time it takes you to reach that is at most exponential in the size of the graph and expectation, right? Assuming this parameter lambda is fixed. So, the interesting question is: how long, right, on the finite graph, the interesting question is how long it takes you to reach this all-healthy state. This all healthy state. And here's the answer for random regular graphs. So let's say you take n vertex d regular graph with n large. You take that graph uniformly at random. Let's say you start with all vertices infected. Then there is a critical parameter, lambda one, such that if lambda is less than lambda one, the epidemic dies out. That is, you reach this absorbing state where everybody is healthy in time of order log n. In time of order log n. And if lambda is larger than lambda one, the process survives there. There is the infected vertices in the system up to times exponential and then with high probability. And as I said, exponential is as large as it gets. You cannot go further than exponential. So there's a very significant change of behaviors across this parameter, lambda one. When you're below it, then it dies out very quickly. When you're above it, the lambda. Quickly. When you're above it, it lasts a really long time. And our question is very simple: simply, what happens to this process when the graph changes over time? And perhaps if you want to put it kind of in even simpler terms, does somehow the fact that the graph change make it easier for the epidemic to survive or harder? And well, we have a very precise answer to that, but let me first say how. That, but let me first say how the graph changes, right? So, let's look at a graph G again. G is going to be deregular later on, and there is a switching move you can do on the graph. You can replace two edges of the graph in BECG on the left-hand side here with two other edges, which is just well, you take the two parallel edges and you replace them by this cross, right? So, ABCD becomes ACBD or ADBC or something. Or ADBC, or something like that. So there are two switching moves that you can do. And it's easy to show that if you have a deregular graph, your graph can remain deregular at all times. And that the switching moves connect all graphs with the deregular or any graphs with a given degree sequence. We're actually going to cheat a little bit because we're going to do these moves here on the so-called configuration model for the random regular graph, which is sort of made of matchings of half-veg. Matchings of half edges. So you can imagine that switching takes place there, or that it takes place over the graph itself, which is perhaps a bit easier to see. Okay, so these are the moves we allow. And this is our model of dynamical graph. We start from a random deregular graph. So the graph is going to be regular at all times. Maybe it's going to have loops and parallel edges because we're working with the configuration model. But yeah, let's not care too much about that. The main point is that the set of vertices remains fixed, but the set of Vertices remain fixed, but the set of edges evolves via switching. So, given any pair of edges, that pair is going to be switched in one of the two possible ways at a rate that's v divided by d times n. And v is just a parameter of the model. And the way we choose parameters is such that each edge is going to be rewired at a rate that's essentially v. Okay, so now the graph is evolving randomly. It's also a continuous time markup change. Continuous time markup change. And on top of that, we run the contact process that I defined before. And this is our main result. Well, there's a part that we haven't quite finished the proof for yet, but we're pretty sure it's true. Well, the main result is this, that there's still a critical parameter like before, with the static graph, there's this lambda one, such that if lambda is above lambda one, the epidemic lasts for an exponential amount of time with high probability. And that's still true here. So there is again this critical parameter. So, there is again this critical parameter, but this critical parameter is strictly smaller than that of the static graph for any V that's positive. That is, we still have the same, well, I mean, we haven't proven the login part yet, but we're almost done with that. So the preprint VR card doesn't include that, but hopefully replace it with a fruit proof. And, but in any case, we can show this. There is still a kind of an exponential epidemic phase that is for sufficiently. Phase, that is for sufficiently large lambda, the epidemic is going to last an exponential amount of time with high probability. But now the lambdas you can allow for that can be smaller than the case of the static graph. So perhaps in this precise sense, we can now say that it's easier for the epidemic to survive in a dynamical graph. And okay, first time I gave a talk about. Okay, first time I gave a talk about this, and even before that, when I was describing the results to several people, of course, we talk about epidemics these days, and people start asking questions, right? So what are the implications of this for COVID, say? And I really don't want to get into that, but perhaps you can allow, can have a simple interpretation that's nothing specific to do with epidemics. Specific to do with epidemics, but which is just that somehow the fact that the graph changes the specific way we consider makes this contact process, which is a process of things spreading around and make what we can say is that it seems like from the proof and everything that the randomly changing graph makes the epidemic spread faster, at this set of vertices spread faster. Okay. So there's a question. So there's a question. There's a good, good. So it's not a question, but I'm just trying to understand what's happening. So it's not surprising, is it? I mean, it's just the fact that if you have a cluster of infected and you start to randomly change the connection, then you allow it to reach out outside of this cluster. That's, I agree completely that it's not surprising, but it's one of those things. I mean, it's, I think of it as a bit like Thompson's principle for renovation. Uh, Thompson's principle for random walk, which is somehow if you increase the resistances, it becomes more transient rather than more recurrent, right? So, if you somehow transient is a monotone increasing property, so it's, I think the theorem goes in the direction I expect it to go, but it's not entirely obvious if you start thinking hard about it, which way it's going to go really at the end. But I agree that's not surprising. So, as we're going to see, as you said, it's going to As you said, it's going to be very clear from the proof why it is, and you basically gave us the correct reason why it's easier for this epidemic on dynamical graph to survive. But I mean, that requires a little bit of work. But I agree that it's not surprising. So maybe another question is, if, but maybe that's so hard, is could you show that the relation goes the other way around for certain types of deregular graphs? So like fixed a deregular graph, but a bad one, one where random. A bad one? One where randomly changing is actually going to make it, you know, more difficult to survive? Like that would that's a very good question. I don't have an answer for that. So the I mean, you can make it you can make it extremely hard for the epidemic to survive a bad starting graph just by making like a bunch of disconnected and newly disconnected parts. But then, if the parameter v is large, the graph is sort of going to fix itself very quickly. Fix itself very quickly, so it's unclear how these two things are going to balance out. So, yeah, I don't have a good answer for your question, but it is an interesting question. Any more questions? No, thanks. Go on. All right. So, because now I'm going to move on and try to explain why this result is true and also give you an idea of previous results that are known and sort of make it obvious what kinds of objects we should. Obvious what kinds of objects we should be looking for in trying to prove this theorem. Okay, so we're going to move on and discuss the contact process. I mean the original one on a static graph, but over a graph that's infinite. I mean, that started the classical setting, which was introduced at Harris in 74. And he started out with regular lattices. I mean, the advantage of considering infinite lattices is that you have transitions. Transitions they are not between, say, logarithmic and exponential survival times, but they're more like either the process dies out in finite time or it survives indefinitely, right? The kind of true phase transitions in the meaning and the sense of mathematical physics. Okay, so here's a very fundamental result, which is just for the infinite deregular lattice, you start with one infected version. You start with one infected vertex, right? And the result is that there is a critical parameter such that if lambda is either below or at this critical parameter, the process is going to go extinct almost surely. And if lambda is above this critical parameter, then there's a positive chance that the process will survive forever and survive in a strong sense, which means that, say, the origin is going to be reinfected infinitely. Is going to be reinfected infinitely many times, or any other vertex is going to be infected infinitely many times, almost surely. Okay, so well, the lattice is nice, but it's not quite the graph that matters the most to us. So the graph that's closer to the closest, the infinite graph that's the closest to a random deregular graph is the infinite deregular tree. Right? So, and this model has a much more And this model has a much more interesting behavior in some sense because it has an intermediate phase. So, again, we can show that for a small lambda, the process is going to go extinct almost surely. Again, I'm assuming I start with one infected vertex. For a large enough lambda, the process survives strongly with possible probability, meaning that every vertex is going to be reinfected infinitely many times. But there's an intermediate phase between lambda one and lambda two, whereas there's weak survival. One and lambda two, whereas there is weak survival. That is, the epidemic survives forever with positive probability, but it doesn't return to the origin many, many times. So it's like you know the epidemic is still out there, but you know I'm never going to be sick again. I'm not going to be sick more than a finite number of times. Right, so okay, these are results about infinite graphs. How are they important for finite setting? Well, there's this general principle which is that which is this. So if you have a finite graph, This. So, if you have a finite graph, but then vertices and n is very large, and it locally looks like a certain infinite graph, then weak survival in G infinity should mean exponentially long absorption time in Gn. Okay, what I mean by locally, it looks like, I don't want to define the concept, but it's like Benjamini-Schrung convergence or local weak convergence. I don't think there is a theorem saying that if there is local weak convergence, then you necessarily have this behavior, but that's the general principle. But that's sort of the general principle. I mean, it should be that if you have local weak convergence and your finite graphs are not nasty somehow, this sort of result will hold. Right, so you remember I told you about this result by Emoi and Valazin and also by Lally and Jou, where they proved that there's a threshold, right? There's a critical parameter for logarithmic versus exponential. For a logarithmic versus exponential time survival in the random regular graph. And well, that results somehow an instance of this general principle. So the point is that, as probably many of you know, if you look at a typical vertex in a random deregular graph, up to any boundary radius, the ball around this vertex looks like an infinite deregular tree truncated up to a certain level. And since the critical parameter for one For weak survival in the deregulatory is lambda one, then it's natural to conjecture, and indeed that's what those authors prove, that this is a critical parameter for this jump from logarithmic survival time to exponential survival time. And indeed, this is what holds, right? So, and this idea inspired our proof in the sense that, okay, if you want to prove something about a dynamical graph, we need to find a local model. A local model, right? It's not the same as before because now we need to consider a local model that's somehow dynamical. I mean, it's not going to be enough to think about one static graph that's somehow the limit of our random regular graph. So we have to have a dynamical limit somehow. And this is what I'm going to call the Hertz process. Turns out it's not that complicated. And then, okay, once we find this process, it's going to be a process over infinite graphs. Going to be a process over infinite graphs, and it's uh, we're going to prove that it has a critical parameter that separates between extinction and survival. And this critical parameter is going to be strictly smaller than the parameter for the ordinary contact process over the irregular tree. Then, the last part of the argument, which I'm really not going to get into in the time I have, is how you go back from the infinite model to the finite setting. I'm only going to say. I'm only going to say a few words about this. So, okay, what's the local limit? So, I'm going to start with some heuristics, and then hopefully it's going to be obvious by the end what the right model should be. So, let's say I start over a finite graph, but I mean, I really want to study the process with all vertices initially infected, but I'm going to right now consider the process where only a single Now, consider the process where only a single vertex infected in the beginning. So it has a positive chance of dying with nothing serious happening. It could be just died before any transmission takes place. But I'm going to start want to think about what happens if it survives for a long time. So here I have a large stream deregular graph. And really, in these next slides or this animation, I'm not going to insist too much in the difference between finite and infinite. We are going to be in this liminal space where I sort of No space where sort of finite but very large things are the same as infinite objects. Well, it's all heuristics, anyway. But okay, this you could think of as either a very large tree that corresponds to a ball in the graph or as simply the infinite tree. And then the process evolves over this, right? So there are infections and sometimes cures and things happen. And then there's going to be eventually an edge switch. An edge switch, right? So remember that the graph changes. So eventually there will be a Poisson clock rigging somewhere, and two edges are going to be switched with one another in one or two possible ways. Most likely, okay, this edge happens to be close to where it started. The other edge, because I started with only one vertex infected, the other edge is going to be in a faraway region of the graph that's completely. The graph that's completely clean, there's nobody infected there. And not only that, it's probably going to be in a typical ball, a large ball that also looks like a tree. So now if I want to continue the process, one way to do it is just take this beautiful drawing and take the two blue edges and replace them by an X or something like that. That's not going to be very nice. So we're going to do something else instead. We're going to just look at the subtrees that you have on the two sides. Sub-trees that you have on the two sides of the two edges, right? So again, I'm pretending that really the whole graph is a tree, that I have two separate trees, they're infinite, everything. So I have sub-trees, right? So each edge is going to split the trees into two. And what I'm going to do is simply swap the sub-trees. That is, I transfer the sub-tree on the left-hand side to the right-hand side, and vice versa. Well, the other one is empty, right? So it's just saying, okay, to go back. Saying, okay, to go back, I replace the sub-tree on the left-hand side by an empty sub-tree, and I just transfer these two vertices to the other side. Okay, so that's one way of seeing it. And of course, once you do that, the process continues, right? So there are still things going on. Okay, so now let's try to, we're going to skip steps and we're just going to look at the edge switch steps, right? So what do we want to see? So, well, the process evolved, over evolved, and eventually you have an edge switch. And eventually, you have an edge switch. So, the way you're going to think of this is: well, there's going to be an edge far, far away in a place where nobody's infected. And again, I'm going to do this move where I just kind of transfer the infected guys from the original tree to the other side. And I keep evolving, right? So, other things are going to happen. And again, I have an edge switch that, well, maybe happens in the other tree, right? Notice that before that, the two processes are. Before that, the two processes are on kind of distinct and disjoint trees are evolving separately and independently. So they evolve, and now there's an edge switch. And what I'm going to do is I'm simply going to create another one of these trees and copy, well, everything that's on one side of the edge to the other tree. So this is the process that we're going to analyze. So the edge switches, they become this thing where you sort of, when an edge rings, you create a new tree and you transfer. Create a new tree and you transfer everything that's to one side of the edge that ring to the other tree. Okay? And each time you create a new tree, it evolves independently with the same transitions as the original contact process. So is the process clear? I'm still going to have to give you a formal or more formal definition, but is the idea clear? Good. Thank you. So this is a hurdle. So, this is a herds process. So, the herds are precisely these trees that evolve independently, right? So, I mean, if you want to be formal about it, there's a set of indices that index herds. So, each herd, data TI, evolves with infections and cures, just like in the contact process. And there are also these herd split transitions, which are the things I showed you. So, let me, of course, if a herd goes empty, it's removed. So, the absorbing state for this process. The absorbing state for this process where the set of NC is empty. And okay, so what's the Hert split transition? So it's just you have a herd, and let's say it's a subset A of the infinite deregular tree, and there's an edge E that rings that disconnects A into two parts. So there are two disjoint parts of A that can only be connected by paths that go through E. So what happens is that when So, what happens is that when this chart transition happens, right, that this happens at rate V, you simply create a new herd. So, you take a new index, n minus JT, or minus, and you create a new herd. So everything to one side of the edge stays in the old herd, and everything to the other side goes to the other herd. So, that's the process that I try to show to you in the animation. I mean, the it's and again, right, new herds evolving. And again, right, new herds evolve independently from all the herds, so they might also split into new herds, as we saw in the animation. And there's this built-in branching mechanism, which is perhaps a somewhat more precise version of what Seth commented earlier, right? So because you're getting these switches, you're sort of creating new epidemic spots in completely different places of the graph. And two, I mean, from the lineage, right, that looks like in a Looks like in this joint trees being created. And well, okay, so one thing that you can easily show, I'm not going to spend too much time on this because I don't have much time, is that it's easy to produce a set of marked particles that lie inside the Hertz process, such that if you look at these marked particles, right, of, I mean, take the union of all marked particles over all Hertz, that is a contact process in the original sense. Contact process in the original sense. And that's basically saying, well, marked particles, when they contaminate some vertex, they make that vertex a marked particle unless there happens to be in one of the herds a marked particle at that place that's been contaminated at that point. That's the rule. But this concept of marked particle is important, not only because it gives us this weak domination, right? This would tell us that the critical parameter for the The critical parameter for the Hertz process is at most that of the original contact process. How do you prove strict inequality? Which is what we want to prove, right? So, in some sense, this is the main result of the paper, right? That lambda v is less than lambda 1 t. So, how do we prove this? Okay, we create this process with frozen hertz, which is okay, we have marked and unmarked particles. Inevitably, unmarked particles are going to appear. There's not enough room for Are going to appear. There's not enough room for only marked particles. And we can imagine a hertz process where hertz that only contain unmarked particles get frozen. Okay. So now look at the number of frozen hertz in this process called that k. We can show that if the expectation of k is greater than one, there's a positive chance of eternal survival with the original hertz process. And the argument for that is very simple. It's that, I mean, you're freezing these hertz, right? So it's like. Herds, right? So it's like saving up for hard times, right? So you freeze these herds, and maybe the original contact process survives, or maybe it doesn't. When it doesn't, you take all the frozen herds and you thaw them. You're now nothing evolved, all these marked particles, and they're going to evolve independently. And that's the original Hertz process, right? The original Hertz process, what happens when you thaw the frozen particles. But now you think about it, and if you have K-such processes, right, K frozen trees that you Frozen trees that you now thaw, these guys evolve independently like copies of the original process. So you have this branching structure, right? And okay, so the reason why the threshold is expectation of k greater than one is precisely the same reason why a branching process must have, well, you must have more than one child on average for it to survive a positive probability. And to give a very quick And to give a very quick sketch, I mean, how do you prove this? It's basically this: okay, take a slightly subcritical parameter for the contact process. We can show, and it's easy classical, that this process is not going to live forever as the original contact process, but it creates a huge number of particles. And then we can show, well, that means there will be lots of unmarked particles, lots of edges when the marked particles are both sides, and lots of moments when new frozen herds are going to be created, which means a large cake, at least an expectation. Right, so okay, so that's sort of the first two steps of the proof. We found the Hertz process, which is supposed to be the local limit for the process with the dynamical graph. We proved strict inequality for a critical parameter using this, I mean, because the Hertz process is a contact process with additional branching. So that's what gives you a kind of bigger chance to survive. And then we must do something to move back to the setting of finite graphs. And then we need to look, what we need to look is that a kind of careful. Need to look is at a kind of careful truncation of the Hertz process. And that's sort of interesting because it can, in the end, we show that the Hertz process can sort of purely survive by branching. So even a Hertz process where trees are kind of restricted to be finite can still survive. And we can use moted-type branching process to analyze this truncated Hertz process and also to relate that to the finite graph. But I'm really not going to get into this if you want the papers every. This, if you want the papers every so, okay, this is our main result. Uh, again, the idea is to define the local limit, which is this HERT process. There are some there's some technical details that the distruncation and stuff that I'm not going to get into, but here's some open problems. So, we want to know more about the Hertz process. So, in particular, we want to know more about the subcritical phase, and that's what we're doing right now, right? So, because I mean, there's one limit that we need for the subcritical phase. There is one lemma that we need for the subcritical phase so that we can prove that for lambda less than lambda v, there is survival for a logarithmic amount of time in the finite graph. But the other two items are more interesting. One is what happens with more general degree distributions. So we know that very precise results saying that when you're expected to have a critical parameter between zero and infinity or zero critical parameter, you can define dynamical, I mean, for other degree distributions. So you can define. Degree distributions, so you can define dynamical versions of those processes, and it'd be nice to study them. And here's another question: For the static contact process, you can show that if you take a lambda greater than lambda c of z, for reasons that I'm not going to get into, right? But for any graph with n vertices, if you take such lambda, the process is going to survive for a time that's nearly exponential. It's like exponential of n divided by a power of one. Divided by a power of one. Okay. So it should be possible to prove such a result for now over connected graphs, right? But even if they change dynamically. I mean, I don't see why it shouldn't be possible to prove this, given that there's a result for static graphs and it's completely general. But it seems like a nice and challenging problem to try. And that's all I had to say. So thank you very much. Thank you, Roberto. Any questions? Any last one? Luke is asking a question. Can you hear me, Roberto? Yes. Back to the original static situation that you started out with, you had this critical lambda one for basically your first slide, the lambda one for the deregular graphs. Do you, it's enough. It's a nice parameter for the graph. Do you know for which graph this lambda one is small and for which ones it is large? Can you characterize the best and the worst? Right, so I'm not sure I understood the question. Let me go back to the, you mean this thing here, right? No, no. So, oh, no, no, not the previous one. Okay, this one. This one, yeah. Yeah, so that the lambda one, it's the parameter that comes from the infinite deregulatory for weeks. Tree for weak survival in the infinite deregular tree. So, I mean, it's just for deregular trees and therefore for the random deregular graphs. I mean, there's no best and worst, just one parameter. Maybe what you're talking about is that there is a for the infinite tree, there are two parameters, right? There's lambda one and lambda two, this term here. And then, if you want to find a finite graph for which the threshold is lambda two, there's an easy choice. The two, there's an easy choice. You take a deregulatory of height age when age goes to infinity. And somehow, I mean, if you think about it in terms of these local weak limits, if you look at these truncated trees, their local weak limit is not the infinite diregular trees. What's called a canopy tree, right? Just because if you think about it, if you pick a vertex uniformly at random, this tree, there's a high chance that it's a leaf, say, right? There's a positive chance. To leave, say, right, there's a positive chance of leaf, and it turns out that the critical parameter for weak survival in a canopy tree is equal to this lambda two. So, there is a role for lambda two also in the setting of finite graphs if you look at truncated trees. I have a related question. So, what about the dependency on the original set A of infected vertices? So, imagine right now you're assuming A is everything. Yes. Everything? Yes. And if we assume that the, you know, maybe there is only an epsilon fraction that can be infected, maybe you can start to ask Luke's question: you know, where should you put this epsilon fraction? Right. So our proof method tells us that if you take a random epsilon fraction, that you're going to have the same result. As long as there are kind of sufficiently many randomly chosen initial vertices, you're going to have the same result. Chosen initial vertices, you're going to have the same result because it's going to be like, well, from these vertices, you can choose some vertices that are far away, and then it's like an independent attempt at survival. But we haven't looked at the question of kind of choosing the best possible, worst possible set. So that's a very interesting question again, but I don't have a good answer to give right now. You have to think about it. Do you know anything about the values of? Do you know anything about the values of these lambdas? Is it like constant over d or something like that? Or, yeah, it's constant over d. I mean, I yeah, I think that for yeah, for large d, it's uh, it's, I mean, just basically look at a branching random walk that dies, and that's that gives you the critical parameter. It's something like one over constant times t. And we can prove also that. And we can prove also that if you take very large V in our model, right? So if you have the same switching, but if you look at the infinite Hertz process, so basically you're branching forever, right? You have no risk of collision, of trying to infect the vertex that's very infected. And then our lambda V converges to the critical parameter of branching random one as well. So here's another problem that's sort of simple that we haven't looked at, but maybe it's easily solvable. We cannot prove that this lambda v is more. cannot prove that this lambda v is monotone decreasing in v. We can prove that lambda of zero, which is the original critical parameter, is bigger, is the biggest one, right? But we cannot show that it's that you decrease with the monotony. Anybody else? Thanks so much. See you, Roberto. See you. Yeah, we have a little break. Okay. And then the group photo. Yeah, I'm sorry, but I'm not going to be able to be there for that. No. Okay. All right, guys.           