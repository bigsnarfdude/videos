Yes, yes. Yes. Yes. Okay. Sorry about that. Okay. Can you see the screen? Yes. Cool. Okay. All right. Okay. Okay. So hi, I'm Ryan. Technical difficulty. Hi, I'm Ryan. Today I'm going to talk about Oblivious Online Vector Balancing. This is joint work with Yangamatabur in the audience. With Yang and Matabur in the audience. All right, so the central open problem in discrepancy theory is the Komos conjecture. This conjecture states that if I give you a bunch of vectors with an L2 norm at most one, there's a way to sign them and sum them so that the L infitting norm is at most the constant. All right, so the current best known bound in this is root log n. We'll just talk about n dependence here. It's due to Monoshik and it's not algorithmic, but there's a paper of Bonzaldedouche of Argon Levette that makes an algorithmic. Of Argon Levette that makes an algorithmic. And since then, algorithmic discrepancy has been quite an active field. All right, so in this talk, we're also going to talk about a lot of questions that aren't in the Colmlo setting. So, for example, it's in the online setting. So, this is when you have some vectors, but they're fed one by one, and you have to sign them as soon as you see them. And the goal is to keep the LMC norm of all the partial sums small. All right, so this was introduced by Spencer all the way back in 1977, actually, which is surprising. 1977, actually, which is surprisingly a long time ago. But in this game, you're fed the vectors one by one, you have to sign them as soon as you see them, and you want to minimize the LMC norm of the partial sums. All right, so here's an example is that if the first vector is this, and I give this sign, I have this partial sum. Okay, so in this example, the discrepancy is two because it's the largest LM free norm of any of the partial sums. All right, so vector balancing has had a lot of applications all over computer science. Here are some of them. Here are some of them, including to the second one to geometric discrepancy and immunization. Okay, so these are just a lot of these are discussed in this paper of Bonsalja and Singla and Sinha and John Kukardi and Singla and this other paper of John Kulkarney. Okay. So there are several models for vector balancing. So there's the coma shedding where everything is offline and there are some other models. So one is a stochastic where the vectors all come One is a stochastic, where the vectors all come from some distribution that the algorithm knows ahead of time. Okay, one is adversarial, which turns out to not be interesting. This is where the adversary knows everything that's going on and could pick the vector, but this is bad because the adversary can always just pick a vector that's orthogonal to the current partial sum and make the L2 norm go up. And so, this is actually not an interesting model. Okay, the third model, which we're going to talk about a lot today, is the oblivious model. So, the adversary has to commit to the vectors beforehand, but then But then feeds you them one by one, and then you with the algorithm have to make a decision about what to do. All right, so then there are some other variations like the profit model, which is there's a different distribution for each index. Okay, so before our paper, here are all the lower and upper bounds known for the various models with t vectors in Rn and L to norm at most one. All right, so these are discussed in this BGSS paper as well as these other papers of Bonzal and Spencer and John Cocardio. Bonzal and Spencer, and John Cocardio and Singla. So it's a little more general than the combo shedding where everything is offline. So this is the most general at the top and then least general at the bottom. And wait. Okay, so we have this large improvement for the Oblivious case, which also gives improvements for the profit and stochastic case. So we have this poly root, this poly T bound, this root T bound, which gets improved to like log N T. So you can think of N is about T. Like in this talk, N and T will only be off of polynomial. Like in this talk, N and T will only be off of polynomial factors. All right, so first I'm going to talk about the stochastic case because that's where a lot of the previous work has been done. Okay, so as I said earlier, in the stochastic case, you have vectors and the algorithm knows the distribution. So you can just get root t by just greedily picking whatever vector minimizes the L2 norm in each stage. And if the vector is uniform, we can guarantee this bound. And so in some other settings, you can get. And so, in some other settings, you can get all sorts of bouts. Okay, but what's going on here? Well, the big picture is that, like, all of these algorithms are working with some potential function. So, this has that worked with the oblivious setting. I mean, essentially because you don't have any control over the vector you see on a given step. And also, because in the oblivious set, so in the oblivious setting, any deterministic algorithm also has to get root g because basically for the same reason that the adversarial model gets root g. That the adversarial model gets routine, which is that at any stage you might see just a bad vector and then can't. So, any deterministic algorithm is essentially equivalent to an adversarial one. All right, so this is not great. Okay, so what do we do instead? What we do instead is we have a randomized algorithm, which, as I said, it needs to be randomized, and it is self-balancing in this funny way. Okay, so here's what the algorithm is doing. Okay, so C is essentially. Okay, so C is essentially 30 log entry over delta. You could think of delta as like one over 100, doesn't matter. So the algorithm does the following. Okay, if the current partial sum has a large inner product with a current vector or has a large LM3 norm, we just give up. Otherwise, we set a probability, which this probability is linear in the inner product between the current partial sum wi minus one and the next vector we see vi. And the point is that it's further. that it's further away from one half based on the dot product of Vi and Wi minus one. So if Vi is orthogonal to Wi minus one, it's a coin toss. It's 50-50. However, if Vi has a large inner product with Wi minus one, we're biased toward the choice that makes us go toward the origin. And this bias is stronger as Wi minus one itself is further away from the origin. So Vi you can think of as a unit vector. And so then And so then we pick one of the signs with that probability. So it's this very linear thing. And this linearity is going to be very key later when we need to analyze the algorithm. All right. So again, this is just what I said. All right. And yeah, of course, in order to set these probabilities to be linear, you need to make sure that the algorithm fails if you're outside the range. If the norm is too large, you can't have a negative probability. All right. All right, so the theorem is that this gives an L infinity bound of log n g over delta. All right, cool. Okay, so this our main theorem has several applications. So this is just a straightforward application. All this, there's, there's, so all this slide is saying is you compute an inner product and number of non-zero entries time as opposed to a linear time. And this makes progress toward a question posed by the douche and also at his application. The douche, and also its applications to some questions in geometric discrepancy. And it even nearly matches the best-known offline bounds for Truznati's problem. As for the Kommosh, it gets logarithm for the Kommelsh and root log is the best. Okay, so how do we analyze this? So as I alluded to before, this linearity is going to be very, very key in the analysis. All right, so you have at some point some partial sum. All right, so the first thing we're going to keep track of is the covariance. To keep track of is the covariance matrix of the process at any time. So after the i step, you know, the i, the i, uh, well, we're at some distribution over all the possible vectors plus minus v1, plus minus v2, plus minus dot dot dot, plus minus vi. So we're going to have to deal directly with this distribution. So we're going to look at this matrix and we're going to analyze it. Okay. Right. So the first lemma and perhaps the key lemma is to show that the difference between To show that the difference between C, where C is this 30 log into per delta, times the identity matrix and this covariance matrix of Wi is PST. And you can see the computations here. We're just using some linearity. All right, gloss over the computations because it's a talk. All right, so now there's some sort of issue that comes up. So if you think about this problem for a bit, you will realize that, like, because we're in the online setting, these These because we're in the offline setting, rather, these um oblivious. Sorry, oblivious setting. Okay, because we're in the oblivious setting, uh, most mardingal inequalities and other things that you might want to do don't really work because you don't get like good point-wise control over everything. So you really need this global control, okay? So how are we going to get this global control? So, this global control is going to come from something called spreading. All right, so uh, uh, we're permitted to laugh for a second, okay. So, so. Okay, so what is spreading? Okay, essentially, the point is we're going to very what? Yeah, okay. All right. Okay, so what is spreading? Okay, so the point of this spreading is that we're essentially going to very, very aggressively very, very aggressively going to couple with a Gaussian. Sorry, okay, I pulled up the chat window. Okay. Chat window. Okay. Okay. So the point is that we're essentially going to very, very aggressively couple our distribution with a Gaussian. Hold on. I'm having some, sorry. Stupid Zoom issues. Okay, let me, let me, let me, okay, okay, sorry about this. Okay. All right. So my spreading is the following. Okay. Y is a spread of x is if you can sample y by first sampling x and then condition on x, sampling some mean zero normal. X, sampling some mean zero noise, or equivalently, y is the spread of x if there's a way to couple x and y. So the expectation of y given x is just x. All right. So this definition appears in the mathematical economics literature. It's called a mean-preserving spread related to something called second-order stochastic dominance. All right. Okay. So what are some properties of spreading that we're going to use? Okay. So one which is very, very important is this. One which is very very important is this linearity. So, as I was saying, this algorithm is linear. Um, the fact the probability is pi is linear, and so this is going to be very, very important. Okay, another one is transitivity. It's not too hard to say if z is a spread of y and y is the spread of x, then z is a spread of x. And another one is convexity, which is for any convex function is at least as high on y as it is on x in expectation if y is a spread of x. Okay, so another one is that a real variable. The other one is that a real variable that's bounded is spread out to a Gaussian. So, I mean, the point here is that if you have a random variable that's plus or minus C with probably one half each, then you can spread it out to the Gaussian. And anything else you can spread out to that and spread out to the Gaussian. And if you have two normal distributions with the difference of the covariance matrix being PSD, then the bigger one spreads the smaller one. All right. So for that, that one I'm going to show a proof. For that, that one I'm going to show a proof of. And this is just because at every you could sample N0B by sampling N0A, and then to every point, you just add an N0B minus A, a different Gaussian. So here this expected zero noise is the same at every point. It's N0B minus A, but you don't necessarily need that to be the case in general. And for our application, it's not going to be the case. All right. All right. Okay. So, and again, you have this property of convex functions that any convex function is bigger on the more spread out one than on the smaller one. Okay. All right. So the key lemma is going to be that our distribution is always spread by the normal distribution. So after the ith step, if you look at the distribution of where the ith partial sum is, it's spread. The ith partial sum is it's spread by the normal distribution. Now, um, so this is a little bit of a lie because we have to round the extra bad mass to zero, but this is ends up being a small fraction of the mass, so this is this is some technical issue. But once we have this, I mean, because everything we care about is a convex function, like inner products and L infinity norms, you essentially immediately get what you want. So it suffices to show that our distribution is spread by the normal distribution. Okay, so how are we going to do this? Okay, so how are we going to do this? So essentially, we're going to use the fact that the algorithm is linear. We're going to use the fact that the probability is linear. So what's going on? So let me explain this slowly. What's going on is that, you know, given wi minus one and vi, there's an expectation of where we end up, which is this linear function times wi. And this, the reason that it's linear is that we pick the probabilities in such a nice way so that In such a nice way, so that it's linear, and then there's this random term, and this random term is essentially this thing which is one of two quantities with some probability. So, all we did here was just rewrote what the algorithm is doing in a way that has a linear term and then another term. And my point is for this linear term, we're going to deal with it using the fact that spreading is linear. And from this other term, we're going to deal with it using the fact that like this two-point mass can be spread out. Like this two-point mass can be spread out, spread out to a Gaussian. Okay, so this is this again fact. This is again this factor we're going to use about this two-point mass being spread out to a Gaussian. So more generally, a real variable can be spread out to a Gaussian if it's bounded. And we're going to induct. So at some point, we're spread by a Gaussian. Then we take a linear transformation. So we're still spread by a different Gaussian. And then we add some mass. And this mass we add is spread by a Gaussian. And so because everything is transformation. Spread by a Gaussian, and so because everything is transitive, you're spread by you're still you're spread by a Gaussian plus a Gaussian, so you're again spread by a Gaussian. So that's what's going on here. So these are just the computations, but the idea is you're spread by Gaussian, then you take this linear shift, and then you add, and then you add in this RWIVI term, and then that term is also spread by a Gaussian. So everything is just preserved, and everything works by induction. Okay, so obviously, because this is a short talk, I can only communicate so much, but the main takeaway here is that. Much, but the main takeaway here is that linearity is important, and that because everything is transitive and these two points can be spread out, everything works the way we want. Okay, so all right, so all right, so Shahar asked for an open problem session. There's going to be an open problem here. Okay, so what does our algorithm do? It gives us logarithmic bounds for the online COMOS problem against oblivious adversaries. We know that the best known is root log for the offline COMOS problem. It does this linear thing. It does this linear thing. It uses spreading. Okay, so an important question here is: can we get rid of log? So, can we match the best known offline balance to the Kobbles problem using a similar algorithm? So, there's an obstacle to using something like this algorithm, and it's sort of because of this cutoff thing where we have this c. You can't get away with letting c be root log n t because when you're bounding a Gaussian, you have an expression like essentially like e to the minus x squared over c has to be like less than c. Like less than C. The point is that there's a quadratic factor that comes up. And also, just our algorithm doesn't work if you have root log. You can see you can flip some coins, it just doesn't work. So the point is that because of this weird cutoff thing that our algorithm is doing where at C, if you go past C, you can't do anything, it seems that it's very hard to get it down to square root log. So we want an algorithm along the same lines that can get root log nt. We have some candidates. The issue does not seem to be the key. And candidates, the issue does not seem to be the candidates. It seems to be really that, like, it's very hard to analyze them because, well, because nothing is linear. I mean, the fact that linearity makes your life a lot easier. All right. So I guess I'll stop there. Our paper's in the archive. And the title, again, is discrepancy minimization via self-balancing walk. And I guess I'll pause here for questions. I guess I finished four minutes early. All right. So I guess we'll pause here for questions. We'll pause here for questions. Thank you, Ryan. Yeah, let's have some questions. So, sorry, I think I did not. Yeah, we can have virtual claps. We did not, I did not tell you how we're going to ask questions. So let's say if you want to say your question, you can raise your hand or you can also write the question in chat if you don't want to speak. Oh, actually. Shakar has a question, yes. Shakar has a question, yes. You can read it, I guess, and answer it. Online setting. I think there's a root log lower. I think there's a root log lower bound in the online setting. Yeah, I was just wondering, you know, what's the best lower bounds we know in the offline setting? Yeah, so there's something from CSS. I think I even... It was on the sorry. Hi, this is Mitab. There's a root log T over log log T lower bound. Okay, sorry. There's an extra root log. Okay, sorry. There's an extra root log log factor. Yeah, it's in this VGSS paper, I believe. But yeah, you can't prove the Colmos conjecture in the unlock. Unfortunately, there's nothing like that. I see. I think Brian Masky has a question or has, yeah. Just it was curious. So, right, the problem that's going on here has to do with taking coefficients that can either. Do with taking coefficients that can either be one or negative one. Have you seen any interest of versions of this problem where somehow instead of just one or negative one, maybe like roots of unity occur as coefficients? Right, right. So there is this variant of the question where at every point there's like two different vectors you can pick. And if they're like two different randomly assigned things on the unit circle, it's not clear how we can prove that one. A roots of unity. Prove that one. Roots of unity, I think. Roots of unity. Okay, well, certainly if it's an even number, then we can do it because then you have plus and minus one. Odd roots of unity is an interesting question. Yeah, I think maybe that it should be equivalent because you can just treat them all as like three different dimensions or something and like view it as a projection of a higher dimensional thing. I think this should. Projection of a higher dimensional thing. I think this should work. I think that works. Okay. Like, you know, you think about, instead of, if someone gives you three of the roots of unity, you think of them as 1, 0, 0, and 0, 1, like, you can just use more dimensions. I think this reduces. I should check this, but interesting question. Yeah, but it seems that plus minus one is like the most fundamental case. But yeah, you can consider all sorts of variants. Sorts of variants. All right. Yes, I'll ask just one quick question. So you briefly mentioned this in the talk, but you were saying you have some candidate algorithms that could potentially do better. And I'm imagining these are like the way you said the probabilities at each step would become non-linear. Is this what the candidates look like? You don't have to do this cutoff. Okay. One idea is to do something that's linear, but to have this aggressive cutoff somewhere. Aggressive cutoff somewhere. So, like at some point, we just start always picking. Yeah, there are some ideas. I mean, the main issue seems to be that just it's hard, they're hard to analyze. We have several candidates, some which are somewhat linear in some places, some which are just completely non-linear. But the point is that it seems hard to analyze. Thanks. I have a question. Might there possibly be any interest in looking at such problems if there are constraints? Problems if there are constraints, for example, I think the question was: can you consider such questions when they're constraints? And then I didn't hear the rest. Could you repeat? For example, you're not allowed to have two plus ones in a row. Every plus has to be followed by a minus, for example. Interesting. Interesting. Yeah, so if there are a lot more minuses and pluses, I guess then it just essentially becomes a sort of distributional question. Okay, if every plus, oh, sorry, sorry. Oh, that's what you're asking. Okay, okay. Ah, so you're now not allowed to like sign the vectors in the way that you want. Ah, yeah, that certainly makes it trickier. I don't know. I mean, for instance, you have to alternate, then I. You have to like alternate, then obviously you can't do anything, then you're just forced to do something deterministic. Uh, if you can't use too many plus or minus signs in a row, my feeling is that the answer will be polynomial. I don't have an immediate proof, but my guess is that in that case, you can't do better than a polynomial. I'm asking because there could this type of question could have some applications in coding information compression. Uh-huh. And it's a symbolic question. I think that if it's like any compression, then, yeah, I think for any question, like what you're asking, my intuition is that the answer will be polynomial. I don't have a proof, but I don't think you can get good balance in that setting. Oh, yeah, I just did, it doesn't seem like you could get some. Maybe, yeah, I'm not sure exactly where you would. I'm not sure exactly where this would stop working. Like, if you impose a very mild constraint, maybe like you can't pick plus one 10 times in a row, maybe you can still run this. I would have to think about it. That's an interesting question. But I think for any serious compression, the answer will be, will be polynomial. At least that's my guess. Thinking about it for one minute. Thank you. Thanks for the nice discussion. Let's maybe now turn over to Samantha. Sorry, let me just stop the recording. Okay. Okay.