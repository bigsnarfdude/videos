It's actually, I used to spend, I've been enjoying the hospitality of UNSER for two years at UBC in the 90s, and I just couldn't believe that it would take me 28 years to get back to Western Canada. I have to get back to Western Canada. So it's really nice to be here. I'm going to talk about unswitched random access with binary subspace chirp, but because both presentations and life is short, I just concentrate on binary subspace chirp and very little. Subspace chirp and very little about the abstract random access part. There's only one slide out of 20 on that subject. Because to explain, but this binary subspace chirp is takes quite a lot of time and energy. So here's the outline of that part of the talk. So basically, this is what we see here. So we have a complex word in n dimensions, and we have In n dimensions, and we have binary words in two m dimensions where m is a base tool logarithm. And then we have various groups and structures and cosets on both sides. And then we are mapping between them using homomorphism, isomorphism, and such things. And at the end of the day, we are then managing to create a set of code books in n-dimensional space which carry the structure from the binary work. And so we start with some mathematical basics. So we have the Pauli group in n dimensions. It's a group of objects that we can see that this is kind of they are created from tensor products of two-dimensional Pauli matrices. So they are n squared matrices in n dimensions. They are indexed by binary vectors A and B. So there are two M dimensional binary vectors A and B that are indexing those. Of A and B that are in those matrices. And basically, we have the relationship between the binary domain and the complex binary domain is that if we have two Pauli matrices in n dimensions, if they commute, then the corresponding index vectors are symplectic. We have some kind of a symplectic geometry in binary binary over the binary field is embedded into the Embedded into the unitary matrices in n dimensions. And this symplectic orthogonality is with respect to this omega matrix, which is the off-diagonal identity matrix in some sense. And if, as I said, that conductivity in one domain means orthogonality in the other. So then, here's an example. I have some small examples in four dimensions so that we get some kind of feeling of the object that we are talking about. So, here we have the power group, the full power group module center in four dimensions. So, we have 16 elements here, and multiplied by the center, we get 64 elements. And that's what we should have. Oh, yes. Word, indeed. So they are sparse matrices. There is always in each column in each row there is one element and so on and so forth. And they are both also unique. Now then the next group that we are going to talk about is the Clifford group. So that is now a group operating in n dimensions and it's acting on capital. It's acting on capital m qubits, so n is n2 to the power of n. So we have m qubits, each qubit is a two-dimensional object, then the tennis-pro space is n-dimensional, and we have a Clifford group, which is then defined as a group freely generated by Hadamard, phase, and C-naught gates acting on single qubits or pairs of qubits out of this set of M qubits that we have. Then we get a group, and that is. And that is called the Clifford group. And this group is a full automorphism group of the Pauli group, so which means that any mapping, any mapping that maps the Pauli matrix to another Pauli matrix by conjugation, so that it maps all Paul matrix to some Pauli matrix by conjugation. Any element belongs to the K-fort group. Okay, then the next group. Okay, then the next group that we talk about is the binary symplectic group. So now we go back to binary dimensions. We have two m binary dimensions. So essentially the dimension of these vectors that are indexing the power matrices. And there we have the transformations that preserve symplectic orthogonality. That is That is a group of a binary symplectic group. We have binary vectors. We have the concept of orthogonality here. And the transformations that are preserving these orthogonality relationships, they are now form a group, which is the binary symplectic group, in two m dimensions. So these are, we have We have here the automorphism group, which is all possible mappings from Pauli to Pauli by computations. And now we have then the Pauli matrices were indexed by two m-dimensional binary vectors. And here we have all possible transformations from two m-dimensional binary vectors to two m-dimensional binary vectors, which preserves this. Preserves this orthogonality in the two dimensions, the symplectic orthogonality. And now perhaps it then doesn't come as a big surprise, but it's perhaps not completely evident that they are partially the same object. So there's a part of the Clifford group which is isomorphic to the binary symmetric group. So binary symbolic group. Binary black group. And that part of the Clifford group is the outer automorphism. So we have the inner automorphism, which is the Pauli group itself. Then we have the outer automorphism, which are the Clifford group divided by the Paul group. And this beast, out of automorphisms, is isomorphic with the binary symbolic group. Which may not be so surprising when we think about it backwards, that we have here all mappings of binary vectors to Or binary vectors to other binary vectors, which preserve orthogonality. And then they map to, in the unitary domain, to some conjugations, some objects that map Pauli objects, parallels by conjugation, preserving, of course, commutation relationships, which means preserving this orthography. And that's why we have this picture running. This picture running here. So now I come to the motivational part here. So, the motivation of this work comes to the concept of binary charts, which is something that Robert and co-workers have been introducing several years ago. And I will now make a very short run-through of binary churches, what they do. So, they are codebooks of very So they are code books of vectors in n n dimensions, which are essentially exponentiated, second-order realm or codes, which you see from this exponential from here. They are finite alphabet, so they have all-rules of unity entries, scaled in a suitable manner. And they are parametrized by a binary symmetric matrix S and a binary M, M, so M by M binary symmetric matrix S and an M dimensional by Is S and an M-dimensional binary vector B. Here is then the precise formulations of R. Here is this kind of explanation, second-order Reed-Mueller code, and here we have the first-order Reed-Muller part in this binary P. Here are some properties. We have a cardinality, we have the minimum distance, one over square root of two, independently of the dimension, which is an extremely nice algebraic property. And we have this somewhat non-transparent but extremely efficient decoding algorithm that is able to decode this binary chirp in n log n square operations, which is based on transforming the decoding problem into a Herbert transform essentially. And somehow And somehow they also have a direct connection to mutually unbiased bases. So basically, when you think about this B index, as we will see, actually we will see here. Here we see. So here now I have again written the binary chart codebook in four dimensions. It looks like this. So all of this, each column now is a code word. And I have grouped these columns into matrices. So I have four by four matrices. Columns into matrices, so I have 4x4 matrices. Each column now, each matrix corresponds to one of these binary symmetric matrices, and then we have this B index which is now indexing the columns in these cases. So then each of these binary symmetric matrices is corresponding to a unitary matrix. And then when we take the columns from the unitary matrix, we get the whole code book. So somehow what we see here is that perhaps we should be interested in codebooks of matrices. Interested in code books or matrices. And then we look at the characteristics of these codebooks of matrices. Then we just take columns from those matrices and we get nice vector columns. Okay? And now one could ask, of course, the innocent question, which is how I ended up working on this and so on like that. What couldn't one expand the alphabet and add a zero somewhere? And if one does, Way and if one does and add a zero somewhere, how should one do that in a systematic manner? And what do I say then? So here we have this picture. Picture, actually now this picture is coming a little bit too early because I here see below. But essentially we have now the Clifford group. And if we would now draw write out the Clifford group in four dimensions, then we would identify. Four dimensions, then we would identify that these guys are elements of the Clifford group. There are certain types of elements of the Clifford group. It's a subset of the Clifford group that we have here. It's not a sub-group. And so we have a Clifford group and we have a subset of the Clifford group and they happen to be subsets of certain cosets of the Clifford group, which we will see in a coming slide what that means. And from these, we take the From these, we take all the cosets of the Clifford group, we take the part which is described by symmetric M-by and binary matrices, and somehow this or the part of the binary symplectic group which is described by the symmetric M-binary matrices, that is then isomorphic to a set of cosets of the clipboard. Then we take the columns and we get binary terms. And now, of course, looking at this picture, this way, the question asked that. This way, the question asked that: so, why do we just take some subset of the cosets? If we would have the full Clifford group, I could instead subset and then the columns are the subset of those cosets. Why don't I take all possible cosets? And what do I get there? And the answer is yes, I can take all the possible cosets. And if I do that, I get this cobiner subspace terms. So the underlying mathematics is based on the Brou-Hardy composition, which exists for any group. Which exists for any group, you have the largest possible subgroup of a group, which is not the group itself. And then you take this subgroup and you can decompose the group elements so that you have something in the middle, and then you have the largest possible sum group at the right and the left. And this is a Bruha decomposition. So here we have the Bruha decomposition of a symplectic group. There's something in the middle which is described. There's something in the middle which is described by some kind of partial Hadamach matrices. You have the have no partial how the simple partial Hadamach matrix because the Hadamach matrix is this direction. This is the matrices where you interpolate between identity matrix and the omega matrix, shifting like to the from the block diagonal to the block off diagonal element by element and you have have this this. Have this basically, if you take all the products of this, you get the group which is a very small group. And this is the so-called y group, which lies at the heart of the symmetric binary group. And then on both sides, you have the largest subgroup, and the largest sum group can be split into two parts. But the elements in the largest sub-group can be written as terms of products of elements. So this form. Elements of this form and that form. And this form is characterized by a symmetric binary matrix, which we already are a little bit familiar with. And the other part is characterized by invertible binary matrix, P. So now here we have, on the right space, there is something which is characterized by the symmetric binary, something which is invertible binary. In the left space, we have again the same thing. And then when we look at the same decomposition on the Clifford group side, On the Clifford group side, we can get corresponding mapping from the Clifford group to some Clifford elements, which are characterized by a minor symmetric matrix, an invertible symmetric matrix, and a Pauli matrix, because the Pauli matrix is the part which is kind of which the minor symmetric group doesn't see. Okay, now we have the Blue Hardy competition, and we do say that we We say that we start to look at vectors. So, we are interested in vector colour books, and because we are interested in vectors, we just take now columns of these matrices. And because we are interested in columns, it means that any column permutation, meaning permutation operation action from the right, is irrelevant. We are also interested in Grasmanian codebooks, which means that the phase of the vector. Which means that the phase of the vector is irrelevant, which means that any phase, a column phase, is irrelevant, meaning any operation which is chasing the phase from the right is also irrelevant. And this is precisely what these guys do. So this is a column phase, and this is column permutation. This is column phase intermutation. So all of those guys are thrown away. So if we are interested in unique Grasmanian lines that Rasmanian lines that are based on Grasmanian matrices, then we can throw away all of that and we are left with one P matrix, one S matrix, and then we have one object of this file group of this size, or this four. So, this is what I'm saying here. So, what is left is that we take That we take cossets of the Clifford group with respect to this largest subgroup that exists, and what is left we parametrize in this way. And now, this is now, in some sense, in the P and S that we have here, they are not freely selectable anymore because part of this, what we have here, part of this P and S, these objects can be mapped to the Objects can be mapped to the right-hand side if this commutes with this part, this guy. But there is a part of this which commutes with the well-dural element which can be thrown on the right-hand side and which is vanishing when we take the COSEPO. What does not vanish is then listed here. That what does not vanish is a rank that we have here is R, and corresponding to this R, which is that how many, how close it is to the identity version. It is the identity versus the omega matrix. Now, what is surviving here of this inverting matrix is the rank R subspace, the binary subspace of rank R. It's surviving from this P, and from this symmetric matrix, what is surviving is a R binary symmetric matrix. And the rest you can throw away on the other side and finish it when you take the cost. So now we then have constructed a set of, in principle, all So, in principle, all cosets of the Clifford loop with respect to this larger subgroup, and they are parametrized with these parameters, and then we build the code book from there. So, this is now the program that we have here. We have a rank, we have a subspace, we have a symmetric matrix, and in the binary world, in the complex world, the rank corresponds to sparsity, and you get more and more zero's. The larger this rank, the smaller this rank is. This rank is, the more CROs you get into this, actually. Yeah, the more CROs you get into the color. The subspace is then giving you the one-off patterns that where are those CROs in n-dimensional space? So that is the binary subspace. And then we have the symmetric matrix, which is essentially giving you the fully fully fully occupied, in some sense, the binary chirp in lower dimensions. Chirp in lower dimensions in the non-zero positions. And we take the columns of those and we get the code book. Here is some example. Here we have a Clifford group again in four dimensions. We have the rank zero, rank one objects, and rank two. And rank two is a binary chart codebook. We have all the cosets here. Here we have the rank ones. So we have half of the elements are zero, half of the elements are some roots in unity. Some roots of unity, and then we have the rank CRO object, which is just an identity matrix. And how is my time now? Five minutes. I'm two today, so I will make it. I will make it. So basically, we can have skip this if we want to parametrize these subspaces, we have the so-called Schubert symbol. If we have the so-called Schubert cell construction, how we can parametrize those, and there is some kind of a Schubert cell which is related to some index set where you have your entries, and then you have the dual Schubert cell and whatnot. So we jump a little bit over that. And then if we just write these code words in a single line, then we get the form like this, which is, we see that it is a generalization of the form of this binary chart for the Of this binary chirp formulation, which was the second of the pre-muller coding, and we had the invertible matrices creeping in at certain places, and so on and so forth. And when we look at this scorebook, we have basically the cardinality of the scorebook is a little bit more than twice as big as the binary Cherb code book, because the fully occupied code words are more dominant, so it's not that big. Minimum distance remains one over square root of two. Distance remains 1 over square root of 2. So we have a little bit more than twice as many codebooks with the same minimum distance. Then the interesting part, of course, we talk about large dimensional spaces. So what is the decoding? So the decoding is, in some sense, we can use very much the same thinking that is in this Hobart algorithm for Piner-Cherk decoding. And basically, just for the sake of And basically, I just for the sake of argument I wrote the decoder here, that the decoder for the on-off pattern is here. So how how would you do that? We if for a noiseless code word, we have this this we can in some sense take the code word w and we sandwich all polymatrices which are diagonal. So all diagonal polymatrices we sandwich with the code word. And this is non-zero. And this is non-zero if this vector y parametrizing the Pauli matrix is in the column space of this dual of the Sugar Zero representative. So with this one, we can then, in principle, looking at where the CROs are, we can then reproduce the dual of the true cell, which means that we can then reproduce this P part of that code word and we have our decoder done. Our decoder done. And for the non-zero elements, we then do the same as in the binary chip work. Then one can formulate this, reformulate this in terms of observing that these diagonal Pauli matrices, the diagonals are actually diagonals of the rows of the Hademart matrix. So doing that, one can reformate that as basically the row of a Hademart matrix corresponding to the index. Matrix corresponding to the index Y hitting on the outer product of the code word with its own complex conjugate point like the point wise take. So the outer product of the code word with its own complex conjugate. So this is precisely the same thing as that. So which means that one takes the outer product of the code word with itself, one takes the Hadamard transform, looks at where it is non-zero, and from those non-zero locations one is reproducing the subspace that the binary subspace. Subspace, the binary subspace that we have in question. And this can be then extended to a decoder for a generic case. But it is rather precarious because this is an autocorrelation receiver, because we take the received code right now with itself, so noise is enhanced there when we do this. And it's operating in the real-value domain because we are interested in the largest values, so because of that, it is not a very Because of that, it is not a very efficient or accurate decoder. So, one has to extend this by doing all tricks in the decoding literature and doing list decoding and such things if you really want to get it working well. And I have this last slide here. The content is that, of course, now, because this is a virtual large dimensional space, so we should do something which is a bit larger-dimensional. So, here we have 32,000-dimensional thing, and we do. Thousand dimensional thing, and we do one source random matches in thirty-two thousand dimensions. We put the random user sixty-four-bit messages in an all-source math manner, and we compare to what Robert was referring to, that he had this chirrup scheme where you have doing binary chirps and basically repetition coding of binary chirp packets, so the repetition of two, two sec and so on. Of two, two, sig, and so on and so forth. Now we do, instead of that, we inspired by Maxine's work, we do tensor coding where we use a binary chirp and binary subspace chirp and autoproduct them together to make the code work like this. And then we get something which is working pretty well in the order when the intensity is sort of 250-200 users per frame. So that's about all. So I will. All. So I was talking about extended binary charts. It is a code word of a complex, how to say, Rasmann code book in n dimensions, which has an effective n-log n-cube, decoding where, as opposed to the binary chip code, you have one additional log n there. We can describe one of patterns in n dimensions using binary Schubert cells, and this is complete. Schubert cells, and this is completing the picture of a gas manual light that you can find from the cliff or the crew, between. And it can be used in algorithmic structure for random access in ceiling like large spaces.