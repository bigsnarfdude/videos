Introduction and in particular, thanks a lot to the organizers for doing all this work in organizing this very interesting workshop and for giving me the opportunity to present this talk. So, the title of the talk is on techniques for improving finite element solutions of steady-state convection diffusion equations. So, this is joint work with my PhD. So, this is joint work with my PhD student, Dirk Freires from the Weierschrass Institute, and with Ulrich Wilbrand. He is a member of my group, a postdoc in the Weierschrass Institute. So the introduction might be brief because I think you all know the equations. It's just to set some to fix some notations. So we consider the steady-state convection-diffusion-reaction equations where the diffusion coefficient is denoted. The diffusion coefficient is denoted by epsilon and the convection field by B. And so we have some boundary conditions right-hand side. And of course, we are interested in the convection-dominated regime. So where the diffusion is much smaller than a characteristic length scale of the problem or of the triangulation, finally, times the infinity norm of the convection field. Yeah, and as you all know, it's the topic of the workshop. In certain situations, the solution of such a problem satisfies a maximum principle, which is an important physical property. I don't need to emphasize this here. And if a discrete solution does not satisfy a discrete counterpart, then we can also call it physically inconsistent. This might not be that a big problem. Be that a big problem if you have just a single equation. But we have the experience that it might give rise to severe difficulties in coupled problems. Because if we have some non-physical values for the solution of one equations, they are then input for other equations and coupled problems. And then the coefficients of the other equations might not be well defined any longer. And we have the Longer. And we have the experience in simulations for problems from chemical engineering that this situation might lead to a blow-up of simulations. But this was a time-dependent problem. Today I will speak about steady-state problems. Yeah, we all know the remedy one should use, could use discretizations that satisfy discrete maximum principles. So for convection diffusion reaction equations, there are Equations. There are linear discretizations like isotropic artificial diffusion or some upwind discretizations, but it is well known that they are usually very diffusive, so they are inaccurate when it has a strong smearing of layers. And besides the linear discretizations, there are also non-linear discretizations, algebraic flux correction scheme, which will be the topic of the next Which will be the topic of the next talk: non-near up in schemes and certain edge stabilization schemes. So you will see here some references. Here, in this talk, I will speak about other techniques with the goal just to reduce the size of the spurious oscillations. And we will see also in the numerical examples that the oscillations are usually not removed, just reduced. And I'd also like to emphasize this. Emphasize this research. We do not have any application in the background of this research, and actually, I don't plan to use these approaches in applications, it's just driven by scientific curiosity. So, I will present two techniques. And the first technique is for DG methods, post-processing techniques for DG discretizations of convection diffusion reaction equations. So, we had already DG. So, we had already DG in this workshop, for instance, yesterday in the talk by Andreas Rop. So, they are based on the weak formulation of the equation, as usual for finite element methods. And compared with conforming discretizations, they are not really popular for convection diffusion reaction equations. For instance, if you just have a look on the number of papers. But there are some papers, and I also will give a kind of brief. Will give a kind of a brief survey on the analysis. So, for DG methods, one needs a lot of notations. I included them here in the talk just for completeness, but actually, I will not go into much details here because what we are doing here for the discretization is more or less standard. But here, th, this script T, is the triangulation of the domain. The mesh cells are then noted by K. The mesh cells are then noted by k. This symbol is used for the area or volume of k, h, k as usual, the diameter, and h is the maximum diameter. And then we have also a number of notations for the phase sets of the mesh cells, but here, as I said, I will not go into detail. We need to define a normal vector for each unit, normal vector for each phase. It's done in some way. Then one needs Then one needs jumps for the discrete formulation, also in the usual way, and the average one faces. Again, the usual discretization. For definite element spaces, we considered spaces on simplices, the p-spaces, and also spaces on quadrilateral and hexahedral Q spaces. In order to simplify a little bit, then would take. Bit in denotation, we just denote the finite element space by some calligraphic r here, h for finite element, and r is the degree of the polynomials for PR or QR. The functions here are in L2, but usually discontinuous. And then the DG discretization is the short form to find a find element function in the DG space such that a linear equation of this form is Linear equation of this form is satisfied. And this bilinear form on the left-hand side can be split into two parts. One is the part that corresponds to the diffusion and then the part which corresponds to convection and reaction. Yeah, so the diffusive bilinear form again is standard. So we have here the usual. So we have here the usual form, but now as a sum of all mesh cells, and then there are a number of terms on the phase sets. I will not go into detail. I just like to mention there are two parameters here involved, kappa and sigma. And I will say something about these parameters in the next slide. Yeah, then the convection and reaction bilingual form, again, the usual part as a sum over the Usual part as a sum over the mesh size, and then once more sums over the phases. And here is one important parameter, which is eta. So altogether we have three parameters. The right-hand side is the same. The usual form as the sum over the mesh size and some contributions on the facets. So altogether we have three user-choosen parameters. This eta here in the Convection reaction bilinear form and this kappa and the sigma. The kappa is the defines the properties of the diffusion bilinear form. It can be chosen like one, then one gets the so-called symmetric interior penalty method, SIP. We use this in all of our simulations. One can also use zero and or minus. Also, use zero and/or minus one. So, I think, at least in practice, in the convection-dominated regime, the choice of couples is not of that importance because the fusion term is usually quite small. Then we have the sigma. The sigma is a penalty parameter which is needed for proving coercivity of the diffusion bilinear form. It can be shown for. It can be shown in the analysis, so it's well known that sigma has to be just positive for the NIP, the non-symmetric interior penalty, and sigma has to be sufficiently large for SIP and IIP. And the necessary magnitude of sigma depends on the diffusion parameter, a result that can be found, for instance, in the book by Rivia. And finally, eta is the flux plus. Eta is the flux parameter in the Beilinger form for convection and reaction. Zero is central flux and one is upper end flux. We used one in all of our simulations. Okay, with this we have a standard DG discretization of a convection diffusion reaction equation. What is known about the conversions? The first robust error estimate was in a paper by Houston, Schwab and Suri in 2020. Houston, Schwab, and Suri in 2002. Robust means, this is this year, that the constant of the error bound does not explode as the diffusion goes to zero. The analysis in this paper was for the NIP method and the sigma. Here you can see this sigma depends on the diffusion parameter and also on the polynomial degree. And so in the convection-dominated regime, convection-dominated now with respect to the grid length. Nated now with respect to the grid length. So that means on coarse grids, one has an order of error reduction of r plus one half. I don't like to speak of convergence because this always means that h goes to zero, but here this is just a result for coarse grids. And this order is for a norm induced by the DG bilinear form. Another paper that I like to mention is by Gopala Krishnan and Kanchad from 2000. Christian and Keinschart from 2003. They proved also a robust estimate in a stronger norm. And I think this is of interest because this stronger norm contains explicitly a term with streamlined derivative. So the error bound is also robust, and that means this DG method controls the streamlined derivative. One does not need any additional SUPG term as in the SUPG discretization for conforming this screenization. For conforming these criticizations, for conforming finite elements. It's not needed. It can be added, but it's not needed. And here the analysis is for the SIP method. We found three more papers on the analysis. Here, you see just the references. They are for different kappa, so for different diffusive bilinear forms, slightly different norms. And in particular, there is second one here by Ayuso and Marini also. One here by Ayuso and Marini, also for more general DG methods. And there's also one paper which compares numerical solutions of DG methods with conforming finite element discretizations for convection diffusion reaction equations. This is this paper here from 2011. And the statements about the DG approach in this papers are there are two statements, namely that the numerical namely that the numerical solutions have very sharp very sharp layers which is of course a good um a good property but a bad property is that the solutions possessed also very large over and under solutions in the vicinity of layers yeah and so we try to improve the second aspect Improve the second aspect to some extent. We used post-processing in order to do this. So, this is not new. Such techniques for DG methods are known for linear transport equations. And so, I like to introduce them now and also to explain what we extended at these methods. The basic approach of a post-processing method. Post-processing method is as follows. First, one computes the discrete solution with DG. Okay, then one has to identify sub-regions where unphysical oscillations might occur. And then the idea to post-process the solution consists just in reducing the degree of the polynomial approximations in these sub-regions. So I think it's from the point of view of accuracy, it's not of harm. It's not of harm to use low-order approximations in layers or in the vicinity of layers. In order to reduce the degree of the pre-nominal approximation, one utilizes slope limiters. And a good property which makes this very, very easy for DG is that the change of a solution in one mesh cell does not affect any neighboring mesh cells. Yeah, this is in contrast to conforming finite element methods. Conforming finite element methods. As I said, in principle, such techniques have been proposed in these two papers by Cochburn and Schuh and by Doleshi, Feistauer and Schwab. And together with my Ph.D. Soon, we had a look at these techniques and we just proposed some fair generalizations or modifications. In what follows, I will give a description of Will give a description of these techniques for two dimensions. So, the first technique is based on a linear reconstruction across phases of mesh cells. This was proposed in the paper by Cochman and Schuer for triangles. And the assumption in this paper or of this approach is that the spurious oscillations in a discrete solution only arise if they occur in its linear part. Occur in its linear part. The first step consists in marking the mesh cells. This is done in the following way. Here, one computes first the average, that means the integral mean value of the finite element solution on the mesh cell and on all neighbors. And then one checks whether the value of the finite element solution, the solution restricted to the mesh cell in the edge midpoint is between the average. Is between the average values in the mesh cell and in the respective neighbor. And if this is not the case for one of the neighbors, then the mesh cell is marked. If the mesh cell is marked, then one applies the post-processing. One can now compute three affine functions by using the average value on k and in two neighbors. One has three values in two dimensions, which gives an. In two dimensions, which gives an affine function. The slope of the affine function is a constant vector. One can compute the Euclidean norm of this vector. And then, depending on the Euclidean norm, one orders these functions with respect to the size of the slope. And then one checks for all of these functions for decreasing size of the slope. So one starts with the largest size of the slope. Whether the values in each edge midpoint is between Midpoint is between the average value of k and the respective neighbor. So if this is the case for one of these three affine functions, one replaces uh with the linear affine function with the largest slope, satisfying this requirement. If this requirement is not satisfied for any of these three functions, then u h is replaced. Then uh is replaced by a constant by the mean value in k. So there is no user-chosen parameter, which is nice, I think. And some extensions and modifications are as follows. We propose something for triangles with edges on the boundary. There, we use some kind of virtual mesh cell, which is defined by reflecting k, the mesh cell at the boundary. And then we And then we took not only into account the midpoint of the edges, but the values on the whole edge. And we did this by evaluating the mean value on the edge, which is for high-order polynomials usually different than the value at the midpoint of the edge. And we had an observation that the reduction of an overshoot, if you use a linear reconstruction, that the reduction of an overshoot might lead to an increase of the undershoot. An increase of the undershoot, which is not good, of course. And that's why it might be safer to apply to use always a constant reconstruction. And of course, one can apply easily the same ideas on two quadrilaterals. Well, this is the first method. The second method is also proposed in this paper by Cockburn and Shu, and there it was proposed for axis parallel rectangles. We extended. We extended it just for a fine transform to a reference square. This is an affine transform. And then again, one defines on the mesh cell k a linear function or an affine function in this way. Here one uses the integral mean value, then one uses here the linear parts, these functions psi and psi, which are given by the reference transform or more. Transform, or more concrete, by the inverse of the reference transform, and the coefficients a1 and a2, which are a1k, a2k, which are defined by using local functionals with respect to first moments. And this again gives some linear part of you. And then, of course, one has some higher order terms. One neglects the higher order terms, and then one can compute the great. And then one can compute the gradient of the affine function, which is a constant vector. And so these coefficients can be thought of as providing information of a certain weighted mean first derivative of uh in k. The next step consists in marking the mesh cells. There one considers these values here and also the mean value. And also the mean values, the jump of the mean values across edges in the respective direction. So I think one should remove these two things or one considers these values and also jumps of the mean values across edges. And then one applies the min-mod limiter, which involves this value, for instance, and then two jumps. And the min-mod limiter then gives some result, which is denoted by. Result which is denoted by a1k overline or a2k overline. And if these values do not coincide with these values without the absolute value here, then the mesh cell is marked. In the min-mod limiter, there are two user-chosen parameters. And then u is replaced just by this linear reconstruction. Some modifications or remarks to this approach. We used in the This approach we used in the limiter the same parameters as proposed by Cochran and Schu for our numerical studies. Again, for the same reason that I already mentioned for the first method, a constant reconstruction might be safer. And so I presented it here for affine transforms. And in fact, the generalization to delinear reference transforms is still open. The last method is based on evaluating jumps across phase sets. It goes back to this paper by Dulashi, Feistau, and Schwarbe in 2003. The authors write in this paper that this method is based on observation they have on the asymptotic behavior of a DG method for linear finite element functions. And then in mesh sites where the solution is smooth, they observe that this term here. They observe that this term here behaves of order one, and in the vicinity of layers, they observe that this term here behaves of order one. So the only difference is here the power. You have the power five, here the power one. And then they said an indicator whether or not a mesh cell is in the vicinity of the layer is just such a term with some appropriately chosen parameter, alpha. Chosen parameter alpha. Now we have just one parameter. And in fact, in this paper from 2003, the authors used basically the alpha equals to 2.5 and they marked the mesh cells if this indicator then is greater or equal than one. Yeah, and here also the number on the right-hand side is a user-choosing parameter. So we have two user-choosen parameters. And the reconstruction is just And the reconstruction is just to say, okay, if this is the case, then we place the finite element solution on the mesh cell by the integral mean value. Some modifications. Here in the original approach, the authors considered the sum of these jumps on all facets or edges in two dimensions of the mesh size. So we considered each facet individually. And then we are displaying. And then the smoothness indicator we used, it's just that we started with this assumption, and then we solve for the power here, alpha E, which gives this formula, which can be only applied, of course, if the size of the edge, the length of the edge is smaller than one. Now, and then we get for each facet, we get a value. The smallest value defines then the values. defines then the values, the value in the mesh cell k. And k is marked if alpha k is small or equal to some reference alpha. And then we do the same that uh on k is replaced by the integral mean value. Also here we have two parameters, this c naught here, we choose as one and this reference value we choose as four. I like to mention that both of these approaches are not scaling in. Both of these approaches are not scaling invariant. And in fact, at the moment, we do not know how to make them scaling invariant. Yeah, that means if you scale the domain or scale the coefficients or something like this, then it's not invariant. Okay, here are now brief numerical examples. In my talk, I only consider this example, which is a classical test problem for steady state convection diffusion. Uh, steady-state convection diffusion reaction equations. Actually, it's without reaction here. We have here a jump in the Dirichlet boundary condition, and the convection is from here to here. So, we have an interior layer, and here we have two boundary layers. This is the solution. I like to mention that for the dg method, the boundary conditions are imposed weakly, of course, and the dg method does not see the exponential layer or the dg. Exponential layer or the DG solution, at least on cost grids. But of course, it does see the interior layer. How to evaluate the simulations? So we use two criteria. One criterion is just to have a look at the maximal spoogis oscillation. So in this example, we know that the solution should be between one and zero. Yeah, this one, this zero. And here we had a look at the We had a look at the maximal value of the discrete solution, look at the minimal value of the discrete solution, and then we added them up in this form. So here you will see some representative results for triangular grids, first order, second order, third order, fourth order. The blue curve here is the Galokin method, and then we have four other methods. You can see sometimes it worked not. Sometimes it worked not so bad. Sometimes the method failed. Okay, and what we can also see, even though there are sometimes a good reduction of the maximal oscillation, but there is no method which really removes the oscillations. You see zero. This criterion depends just on two points. And we think maybe And we think maybe it might be good also to have a criterion which depends on the global solution. And then we define some measure of the mean oscillations, some formula over here, a sum over the mesh cells. And again, the blue line is the Galiokin method. Here, this is for quadrilateral group with first, second, third, and fourth order. And we see that the blue line is always on top. And some of the other lines are quite below here. But it means the mean oscillations are remote. That means the mean oscillations are removed. There are some post-processing approaches which remove the mean oscillations quite good, but once more, none of these methods removes them completely. So there are much more results in this paper that appeared recently. If you are interested, you should have a look there. We found in all of the results the same principal behavior that I described here. Some of the methods reduce the maximal and the mean of. The methods reduce the maximal and the mean oscillations quite good, considerably, but no method is really perfect. And as we saw in the simulations, all methods reduce the mean oscillations. At least they are doing something positive. For triangles, we have a clear favorite of the methods, which is the modification of this method here. This is this method, which is based on the evaluating jumps across the phase. Evaluating the jumps across the facets. This behaved really best. Whereas for quadrilaterals, there are four methods which work more or less similarly. And from each type of these reconstruction approaches, there's at least one method. Open problems are a parameter studies. We didn't do them so far. Improvement of the marking strategies might be possible. And then, as I already said, there are open problems. And then, as I already said, there are open problems for the method that is based on the weighted mean derivatives, the extension to d linear reference maps, and the scale invariance of methods that variate jumps across facets. The second part of my talk will be on a posteriori optimization of stabilization parameters. And this is an idea that goes back to a paper of Petrarch Knoploch. A paper of Petr Knobloch and myself 10 years ago. And in this paper, what we did there is to consider the SUPG method. And we said that the stabilization parameter should be a piecewise constant function. Then we defined a functional that measures the quality of the numerical solution. And the idea is now to optimize the stabilization parameters with respect to this functional. Okay, this leads to a non-linear constraint optimization problem and the number of unknowns equals. Problem and the number of unknowns equals the number of mesh sets. Here we have a piecewise constant function. So, what did we do in this paper more concretely? Actually, it turned out it's quite hard to define functionals that describe the quality of a solution. If you see a picture, then it's clear often on the first glance how good the solution is, but for the functionality. But for the functional, I think there are still a lot of open questions. What did we do is to use the residual, but not in mesh cells at the boundaries, because if you have exponential layers, the residual will be always large, even if you have, let's say, the perfect finite element solution, which is totally exact. And then we applied a function of a norm of the Cousvarian derivative. So I will come back to this later. And then we applied a gradient-based optimization. Then we applied a gradient-based optimization, which is the solution of the adjoint problem. And there we used a limited memory BFGS method or server, which is some standard approach from optimization. At those times, we implemented it by ourselves. Here are some results from this paper. Here are the SUPG solution, standard SOPG solution. Here is optimized parameters. And you see clearly the improvement nearly perfect. However, a big issue is that the solution of the optimization problem required Of the optimization problem required many iterations. There are, meanwhile, some further contributions to this approach. The extension to two stabilization terms, the second one is the so-called spurious oscillation at layers. The finishing term, a sol term. I will also come back to this later. Was done in the conference proceedings with Peter and in this paper by Petr Lukash and Petr Knobloch. A new objective functional was proposed by again Petr Knoploch, Petr Lukas, and Pavel Schulin. And Petr Lukash also investigated the possibility of using pre-switch linear stabilization parameters, but the recommendation was to use pre-switch constant ones, and we also used pre-swise constant stabilization parameters in our simulations. And the goal of the current work is to explore methods for reducing the dimension of the space for optimization of the control space. Optimization of the control space. As I said, the number of iterations for solving the optimization problem was quite large. And the idea is just that the adjustment of the stabilization parameter is only needed in the vicinity of layers and not in the global domain. Yeah, this is the basic discretization that we consider. So we are given two sets of stabilization parameters, delta 1 and Parameters delta one and delta two. And then we have this problem with the standard bilingual form. Then we have here the SUPG term where we have here the streamline derivative and here the standard parameter and delta one. And here we have this Solt term where we have the crosswind term of this form. And here hk is this diameter of k and this stabilization parameter delta 2. At two. Okay. Well, this is the form, the discretization. And the left-hand side, of course, is the same. Here we have Neumann boundary conditions, and here is the contribution from the residual from the SUPG term. Then the optimization problem looks like this. We like, or the general optimization problem, to minimize these. A functional which depends on the stabilization parameter and the solution and the solution of the discrete problem. We also started with the control space that we used piecewise constant stabilization parameters. Now we have two sets of stabilization parameters. So we have here p naught squared. Objective functional, as I said, this is As I said, this is my opinion still quite open questions. What we did so far is a linear combination of proposals from the literature. So we implemented much more terms. So I have to say, Ulrich Wilbrand implemented them. That's why this is a strange numbering, 3, 5, 9. But I think it's not important. So in the first line, what you see there are residual terms. Here, you have the L2 norm of the residual, and then we apply some function, or one apply some function psi on the. function or one apply some function psi on the L2 norm. One could do it also the other way around, psi on the apply on the absolute value of the L of the residual and then compute the L2 norm. Here we have terms with the crosswind derivative, some function phi, here this L2 norm, here with L1 norm. This term was used already in the original paper from 2011 and I think this term was proposed. I think this term was proposed in the paper by Knobloch, Lukash, and Schulin. Then we have here a term. I will say something to these functions, of course, next slide. Here we have a term that can be only used if the maximal and minimal value of the solution are known. And then one, of course, one can penalize previous oscillations. And then we have here some cost or regularization term. And as far as I have seen, And as far as I have seen this so far, I think nobody can sit such a term in the context of the parameter optimization for stabilized discretizations. We have some parameter alpha and actually we have to find out how to choose a good alpha. So this function psi, which goes here to the Um, to the residual terms is a function which is zero at zero, and then it goes up to one to some value or parameter input t0, and then it's constant. Here is the normalized version, which is just called Ïˆ. You can see it here. Yeah, as I said, even if you have an orderly extract solution, the residual will be very large in layers. In layers, and so one has and one cannot improve then anything, and then one has to do something in order to avoid it. Such large values become of importance, and that's why here this kind of cutoff. The function phi is again a function which is monotonically increasing with a different slope at the beginning, which is somewhat higher than here after the year. After the input one, and the function eta is just, as I said, a penalization of the spoogius oscillations. So, okay, so far what we have is a control problem with a dimension of the control space, which is 2n, n is the number of mesh cells. And the derivative of the function with respect to the control variables can be computed by solving an adjoint problem. Solving in a joint problem, this appropriate right-hand side. So, a joint problem is not that complicated because we have a linear problem, then we just have to use the transposed matrix. And our goal is now to reduce the control space because, as I already said, only not all of the cells are of interest. And also the residual and the cross-final terms are small away from layers. And if these terms are small, And if these terms are small, then the actual choice of these parameters here is not that important. Okay, the first step is the marking of measures. So we have to, again, we have to find the measures where we like to optimize the parameter. And this is not true. It's based on the standard SUPG solution and the functional. So this was the former version. Now we use just the functional. Now we use just the functional of interest. We start with the standard SUPT solution and then we use the functional. And of course, in marking the mesh cells, I'm user-choosing parameters involved. After this, we get the irrelevant mesh cells, NR, which is usually smaller than N, and the smaller control space, YR. Then, as a second step, we have to incorporate a map, ER, from the smaller control space to the original control space. To the original control space. And this can be chosen as an affine linear map in this way. We have here a binary matrix, which where we have in each row either only zeros for the non-relevant cells or just exactly one entry one for the relevant mesh cells. And this is the vector consisting of ones. And in each column, we have exactly one one here in ER. And this map leads to the default parameters in the non-relevant. The default parameters in the non-relevant cells. So, that's what we like to do. And then we have to replace the objective functional. The first step, even before, is that we have to consider a reduced objective functional, because the original objective functional depends on u and d and now the reduced functional depends on u and delta. Now, the reduced functional depends only on delta. This can be easily done by incorporating here some solution operator. And then we have just a And then we have just a functionality depending on delta and the stabilization parameter, which is called reduced objective functional. And the derivative can be easily computed by the chain rule. That means we still have to solve a global adjoint problem with the appropriate right-hand side, even if we use the reduced control space. So I like to present here some preliminary results just from the beginning of this video. I adjusted from the beginning of this week. The same example as before. Here we used grids consisting of squares with conforming finite elements. And we did the optimization for the SUPG term only. This will be denoted by SUPG in the pictures. And also by SUPG and the SOLT term, which will be noted by SOLT. Sorting iterate, as I already said, is the standard SUPG solution. And for marking, we used at the moment that at least At the moment, that at least 25% of the measures are marked, and here it's correct. The values of the target functional for the starting iterate are used in order to find these measures. We have the observation that the relation of the iteration corresponds well to the relation of the computing time. So it's sufficient to present here graphs for the number of iterations in order to get an impression. Again, we used a limited memory BFGS method as server, but this time we Method as server, but this time we took it from some library nlopt. And as I said, I say we present some representative results. The first result is for Q1 finite element with 256 mesh cells, and the function is we call it just J2. Here we have the residual part, here we have the crosswind part, and here we have the part which Part which comes from the regularization with the parameter alpha. As I said, I couldn't find any results that already included this regularization, so we had to get some idea how the regularization acts. And here you see results for different regularization parameters: 10 to the minus 4, 10 to the minus 3, 10 to the minus 2, 10 to the minus 1. We can see that the functionals usually increase, so the bold lines are the reduced. The bold lines are for the reduced control space and the dashed lines for the full control space. And the functional for the reduced control space are usually larger than for the full control space, which can be expected, of course. Here the number of iterations, this is, I think, more important. And here we see that the full lines are below the dashed line, so we get some reduction of the number of iterations. Of iterations here, some information to the oscillations again: the maximal undershoot, maximal overshoot. Here, the dotted line is for the SUPG method. So we see we get with all methods some improvements, and the improvements are better here with the full control space, which might be also expected. If you have a look at the mean oscillations, then we see that we really get a quite good. Really, get a quite good reduction for all methods. And again, one gets a better reduction with the full control space compared with reduced control space. I think this is our result, can be expected. And we see that here there's actually almost no impact on alpha. This is not always the case. Here is another result for Q2 with one. For Q2 with 1024 mesh cells. Here, J3 is the same as before, but we just added here, I just would say, a slight penalty of the spoof oscillations. And here we see that sometimes here we see that at least if alpha is sufficiently large, then again the number of iterations with the reduced space. With the reduced space again, with the full lines, is smaller than the number of iterations with the full space with respect to the functionals. Well, we see this. Here we see some, I would say, some tendency that with larger alpha that there is a slight increase in the mean oscillations over here. Again, usually the results here with respect for the oscillations for the full For the full control space, are better than with the reduced control space, but this is not always here. For instance, some exceptions, some small exception. And also here for the red line. Okay, so as I said, these are preliminary numeric results. We are still just work in progress. The observations that we have so far is that larger values of the regularization parameter alpha lead often to a decrease in the number of iterations. Lead often to a decrease in the number of iterations, not always, so there are exceptions, but often. They increase the final values of the functionals, which is of absolutely no importance in practice. And sometimes there's a slight increase of the size of the spurious oscillations, as we have seen in the second example. Then, if you compare the full and the reduced control space, we have often less iterations with the reduced. Often less iterations with the reduced control space, so we achieve this goal at least to some extent. There are exceptions, as I said, and we are often smaller oscillations with the full control space. So something I think one could expect. For the mean oscillations, the difference of the results obtained with the full and the reduced control space is often not that large. It's often only a small difference. As I said, it's working. As I said, it's a work in progress. And some further topics that we are investigating currently is to use different strategies for the marking of the mesh size, different criteria, and in particular also to use less mesh size than 25%. And we'd also like to have a look at different functionals or different linear combinations of functionals and also of different and more challenging examples than this. Than this, I presented here. Okay, with this, I like to thank you for your attention. Thank you very much for your talk. Are there any questions from the audience? Dimitri. Yes, Folk, I'm wondering if the methods that you consider for DGA preserve the consistency of the diffusive flux? Because I think if we're solving Flux because I think if we're solving convection diffusion equations, the situation is different. Like for hyperbolic problems, you can set all the slopes to zero and you still have a consistent flux approximation. But if you use constant reconstructions for problems with diffusive fluxes, you can destroy consistency. So can you comment on this? Actually, I have to say no. It's an interesting question. We will think about this. Yeah, but yes. If one just replaces some, let's say, arbitrary polynomial by some low-order polynomial, well, one should not exclude that one destroys consistency. Although we have to think about this. I also will have a look at these two papers that I mentioned if the authors looked at this topic, but at the moment I'm not aware that they did. Thanks.