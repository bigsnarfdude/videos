Great. So excited to be telling you about something I've been interested in for a while. So this is really just one vignette and a larger goal. I've been trying to make reinforcement learning algorithmic. So I'll explain to you what I mean by that. I'll also tell you what RL is, and then I'll tell you some cool applications. So the way that reinforcement learning works is that it's always according to this following cartoon picture. So we have an agent who So, we have an agent who learns by interacting with the environment, and the key word here is the word interact. That's what separates it from a lot of other flavors of learning that you might have seen, because the actions that the agent takes affect the underlying environment that they're operating in. So, every time step, the agent chooses some action, and that has an effect on the immediate rewards the agent gets, as well as it changes the underlying state. So, the question is really: how do you balance the immediate rewards with the You balance the immediate rewards with the long-term planning aspects. And let me tell you about the most basic and fundamental of these models. RL has been studied for decades. There's a very powerful model called tabular markup decision processes, where this is where we really understand what's happening. And once you move beyond the tabular setting, that's where things become more complicated. And there are really wide algorithmic gaps. So a Markov decision process has a whole bunch of terms, but they're all very natural. Of terms, but they're all very natural. So you have a state-space script S that describes all the state the environment could be in. There's a special distinguished state, S0, which is where the environment starts off. You have a collection of actions. At every state, you take one of these actions. You have rewards. You can think about these rewards as being a function of your current time step little h. But this notation just means if you're in state S and you take action A at time step little h, what's the reward you get? And now what's You get. And now, what's also very important is that the choice of the action affects the state that you reach next. So we have these transition probabilities, which is the probability that if we start off in state S, take action A, what's the probability that we reach next state S prime? And finally, we have a horizon H, which just describes how many rounds we play this game, and we get the cumulative reward we accrue over that time. So the basic question is how exactly to maximize reward. Maximize reward. Now, the great thing about tabular MDPs is that a strategy is what's called a policy. It's just a mapping from states to actions. It tells you if I'm in some particular state, what's the action I should take? This is really using the Markov property of MDPs, and this will be a critical thing that'll go out the window later. Because I don't have to care about the entire history of what happened before. All that matters is where I currently am. So there's always some policy that. So, there's always some policy that maximizes the expected reward. And now, you know, just to be very pedantic about this, right? Even though RL is a field, really, the way I like to think about it is you can divide it into two types of questions. They don't usually think about this way, but it's important to separate these two things. One is a purely optimization problem, that's called planning. So, this is the model where I tell you everything about the MDP: I tell you its states, actions, transitions, rewards. Transitions, rewards. And then I ask you to solve the optimization problem of finding the best policy that maximizes the expected reward. And what's a different problem is the learning problem. This is a statistical problem. I don't tell you anything about the MVP. You start from scratch at SNOT and you interact with it over a series of episodes. You observe a trajectory, which is a sequence of states and actions and rewards. And you try and learn as much as you need to about the underlying model so that you can find. Underlying model, so that you can find a near-optimal policy. So these two problems are very intertwined, right? Because, in fact, learning is predicated on the optimization problem. Because I really use my optimization oracle as a subroutine in the process of finding what's a good policy and learning about my underlying MDD. Now, in the world of tabular MEPs, there's nothing to reconcile. Everything in this table is easy. If you're given the planning problem, If you're given the planning problem, there are tons of algorithms like value iteration, which is basically dynamic programming. There's policy iteration, you can just write the thing down as a linear program. There are tons of things that we'll spit out the optimal policy for. And there are tons of algorithms for learning, even in episodic settings. There are model-based ones that go back like 20 years. The first one was Duda Kearns and Singh. There are all kinds of other ones that don't explicitly try and construct a model, like Q-learning, actor-critic, policy. Q-learning, actor-critic, policy gradient. And we have a lot of guarantees for these things. So, tabular MDPs are very well solved. In fact, we know the optimal regret guarantees that we can achieve. For planning, like, in the non-tabular type, what's the outstanding thing? Yeah. We'll get to that. Okay. So, the trouble is that the world is not tabular, right? So, in principle, you could write down these things as giant tabular MVPs, but you would just have way too many states to ever possibly explain. States to ever possibly explore. So, this happens, for example, famously in strategic games like Go. That's an astronomical number of states. There's no chance that I can write down or visit all of these states. That's just impossible. So, the real name of the game here is to find what are better models for the fact that we can solve these learning problems in practice. And so, some of the things that people do, they do things like function approximation and block MDPs at a very high level. At a very high level, the idea is that maybe there's a way to take the state and distill whatever is actually relevant about the state into some vector representation so that in that representation, everything becomes nice. Maybe there's a nice way to represent the transition probabilities, the rewards, or even the policies, because it's not even obvious how to write down a policy. There are other settings, like there are all kinds of important applications of MDTs and RL to healthcare. To healthcare. And really, it's a big assumption that you can observe the state you're operating in. Like, I don't know what the state of my environment is. Maybe at best I know a piece of the state. In areas like robotics, maybe I can see part of what's going on. In healthcare, I definitely don't fully know the state of my patient. And there it leads to other models, which are called partially observable Markov decision process. And at a high level, I mean this talk to be an invitation to come join me because really there's a question. Because really, there's a question about what are the right models to begin with. So, people over the last 10, 15 years have been studying what happens beyond tabular RL. We know that that's not enough. And what they've been doing is they've been finding new models that we can study. You know, I can tell you more about that offline, where they accommodate a very large or even infinitely many states. And yet, it's statistically possible to learn from a small number of interactions. Because you don't want to play so many games. Because you don't want to play so many games of Go that correspond to how many possible states of the board there are. That's just not feasible. But we're algorithms people, and there's a very natural third goal on this, which is maybe we shouldn't just be looking for models that accommodate large state spaces and are statistically tractable. But what about algorithmically tractable too? The way I like to describe it is: you know, what if when we're thinking about PAC learning, we just worried about what's statistically tractable, and once we answer that question, And once we answer that question, we'd stop there. We would have missed all of the right models about what makes the problem algorithmically doable, like stability. So what do we actually miss out on when we're ignoring computational considerations? So there's a lot of cool beyond-worst case analysis, and really the way to go back to what's going on in the caricature is that, yes, we have all of these success stories in RL, where we can get sample efficiency, but what's sitting in the agent's head? What's sitting in the agent's head? What is he doing at each step when he decides what's the next policy he plays? He's solving not just an NP hard problem, but usually a P-space hard problem. And usually you just call it Oracle efficiency and call it a get. That's scary. Okay, so I'm going to tell you one vignette from this because we've been doing a lot of work also on the function approximation side. But I'm going to tell you the one vignette that's at least fairly understandable. You know, fairly understandable in whatever time I have left. So, I'm going to tell you about something called POM DPs. So, POM DPs are the slight twist where you don't observe the state, but what you get are observations that depend on your state. So, there's one more twist, which is that we have these conditional probabilities. They can even depend on the current time step little h, not a big deal. But given the state I'm in, I have some conditional distribution on what are the observations that I get. And knowing those observations, they help me discriminate which state. Observations, they help me discriminate which state I think I'm in versus which states I'm uncertain about. Think of the state spaces being large here. The two things emerge, right? Large state space and you get much lower space. Actually intertwine, so you can think of this as just being a small state space for right now. But really, this is an MDP with a giant state space. Because what would happen is I would actually look in the posterior distribution on what's the distribution on states I think I know. But we'll get to that. But we'll get to that. And really, all of these things interact in interesting ways. But I want to start with the easiest problem, which is the optimization problem. So I'll start with this. All right. So there's an old result, which was actually from before I was born. So this is due to Papimetra and Sitziquelis. And they showed that optimal planning in a POMDP is P-space harp. Now, you can look back in this result, and you can try and poke holes in it. It's a very nice result. But it, yeah. But it, yeah. So, planning in this situation means a function from the observations now. I'll get to that in a second. Yeah. So, planning would be maximizing expected reward, but let's get to that in a minute. So, what they show here is that there's an instance of a PONDP where the reward is either 0 or 1 over 2 to the n, and it's p-space hard to decide which of the two they are. So, what you can do is you can teach this old reduction some new tricks, because this was before hardness of approximation. Hardness of approximation. That was before we really knew very much in this space. And it turns out that really, you know, there's a fundamental problem which gets to your point, which is what even is a policy, right? So for an MDP, we had this thing we could easily take for granted, the fact that it satisfied this Markov property. The optimal choice, the action, only depends on our current state. That's not true for POM DPs, right? So no one says I can forget the past. I might need the entire sequence of actions. Might need the entire sequence of actions and observations. And now, writing this down as a table is gigantic. It's exponential in the horizon. There's another way I could think about it. I could think about it in terms of the posterior distribution on what state I think I'm in. The observations teach me about that. The actions teach me about that. And in general, I can think about a mapping from what's my posterior and what state I think I'm in to the actions. But any way you spin it, it's exponential. It's either exponential. Exponential. It's either exponential on the horizon, or I could discretize the belief space, and it would still be exponential in the number of states. And it turns out that this isn't one of these times where we face computationally hard problems all the time in machine learning, and then we just do gradient descent on a deep net and call it a day, right? It works. What? It works. It works, right? See, there's a big problem, which is that the answer isn't even succinct. So, what you can do is So, what you can do is you can teach this old Papa Dimitri Tit SQL lower bound a new trick, and you couple it with some more modern tools. And you can construct an instance of a Pondiky where the rewards are either 0 or 1, and they're at the end. And it has the property that if you had a polynomial-sized description of a policy that's an additive half-approximation, the exponential hierarchy would collapse. So it's not just that in this guy's head, the agent, that you're solving very hard optimization problems. Solving very hard optimization problems. How is he even conveying and what the answer is? I don't know. Do you use some recent work with the budgetary period? Yeah, all kinds of recent stuff. Yeah. Yeah. Just a question regarding the setup. So you said that the rewards and the transitions might depend on the current time. Yeah. And then you said that the policy does depend on the current time? Okay, so the policy. Okay, so you're right. So what I should really do is I should look at the states that are on. Should really do is I should look at the states that are augmented by time steps and the policy, but thank you. Yeah, very good. Yeah. So, isn't it more reasonable to try to get close to the optimum among the best possible? It would be if it weren't empty hard to do that too. But very good. All right. I have to get to at least the thing I want to, but that's a very good question. Unfortunately, that doesn't work. So, really, these things get solved all the time, right? PomPs get solved all the time. So, really, this is. Solved all the time. So, really, this is an excuse to do beyond worst-case analysis because maybe there's something about these instances that's not really realistic. So, it turns out that if you actually look at these reductions, it's not that they're partially observable and you don't know exactly where you are. You actually get no observations whatsoever. So, it's just a blind MDP. You cover your eyes and you play a policy, and then something happens at the end, and that's that. So, that's not really reasonable because, in areas like robotics, Reasonable because, in areas like robotics, I do learn something about my state. Sometimes it takes many steps for me to do that, but it leaks information about my state. That's what partial observability is supposed to mean to begin with. So what happens if the observations leak some positive information about the states? Is the problem actually hard? And could this even better enable tractable learning? That maybe we don't have to rely on oracle complexity, but we can actually implement these oracles and get. These oracles and get end-to-end learning guarantees. So, let's do that. So, what does it mean to leak information? Here's a very natural definition. It wasn't invented by us, and it's very natural for other reasons too. So, remember that I have these conditional distributions. I'm going to organize them as a matrix, so that it's observation by state. So, when I take some belief, which is really a distribution on the hidden states, O times B just tells me, if that really were, the distribution on the hidden states. If that really were the distribution on states, this would be the distribution on observations. And what I want is that well-separated distributions on beliefs lead to somewhat well-separated distributions on observations. They leak some positive gamma information. So this is just the bound on like the L1 notion of the smallest singular value. And it was introduced in this very nice paper by Evan Dar et al., which I'll come back to, because they had a very To because they had a very natural question. These types of considerations that go beyond worst-case analysis, usually they're, you know, they come about for very natural reasons and they have many applications. So even if you have a hidden Markov model, one of the things you can do with it is you can compute the posterior distribution on which state you're in using the Vitterby algorithm. But who told you you got the right parameters to begin with? What if I had some sensitivity or misspecification in my parameters? I would want my beliefs to be well-posed. I would want my beliefs to be well posed. So, many of the assumptions that allow us to get around hardness are really the right way to look for natural problems from many different vantage points. And that's what's going to happen here, too. So, cool. Yeah. So, if I, just to make sure I got the definition right, this means in particular that at least statistically speaking, if I observed some reasonable long sequence of states and actions and things, that I would know where I started. We'll have to get to that. Yeah, let's prove that. Okay. Yeah, let's prove that. Okay, so one of the cool things is that this also makes no assumption on the transition dynamic. So the only works I'm aware of in POMDPs, they assume deterministic transitions, or they assume mixing. Mixing is really, really bad because it means mixing under every poly, which I don't really know what that means. Okay, so let me tell you our results. So we give a quasi-polytime algorithm for planning and gamma-observable MDPs. We also get a learning result. That's a whole other candidate. Get a learning result, that's a whole other can of worms. Defined an epsilon suboptimal policy. The observability parameter goes in the exponent, so this epsilon. And the key is something that's very intuitive, but is actually not so easy to prove. It's a stability property for the beliefs. So if you look at the posterior distribution on what states I think I'm in, and I observe a long trajectory, what we show is that the beliefs contract at an exponential rate. The beliefs contract at an exponential rate. And this is, in effect, like a discrete analog of some very classic things in control theory called the exponential stability of Kalman filter. It's just that it's happening over a discrete state space, so the proof is very different. So you can ask me offline about that, and I'll tell you the analogies to control the interestingly, you can prove a matching hardness, too. So quasi-poly is the right answer. You can't beat it. And that's true even in a very special, trivial way. Even in a very special trivial case, which is where you literally just reveal the state with some probability gamma. So in that case, it's immediate that there is a polynomial size description of the optimal policy, because it just depends on what's the last state you saw and how long you've gone without seeing the state. But even then, it's quasi-polyhard to find an optimum policy. So this is tight. Okay, so I have a couple minutes, so I'm going to prove something. So I'm going to prove something, and then I'm going to tell you some open questions. So the key is belief contraction. So let me be precise about what belief contraction is. So imagine that I start with a prior distribution that's arbitrary. You know, I think I'm in this distribution on the states, and then I fix any policy pi, and I have this gamma observable pond dt, and I play according to that policy, and I get a sequence of actions and observations. What I can do is I can look at the posterior distribution. Look at the posterior distribution on what state I'm in. You know, I think I am. Just compute the literal posterior distribution because I'm in the planning case where I know fully the POM DP. I know everything about it. But what happens if I started with the wrong beliefs? Let me coarsen my initial beliefs. Let me start off with the uniform distribution over all states. Let me take the same sequence of actions and observations that came from playing policy pi, starting from B. So B was the true original state. B. So B was the true original state. And then I can look at what's the posterior I get when I started from the wrong prior distribution. And what I claim is that this contracts at an exponential rate, depending on this gamma parameter. So this is an extremely natural thing. I think the first time, you know, I just assumed this would be immediate. You know, you can think about a special case of this when you have no transitions in the states. This is a churnoff because it's really a large. Because it's really a large deviation statement. Each state you're in, you stay there forever. And the law on what observations you see are statistically far from each other. So you contract very, very quickly to figuring out which state you're in. Sorry, I'm missing something really basic. Why is this a good thing? Why do I want to converge the posterior from a medium from a leap state? Oh, oh. Yeah. It's because I can forget the past and I can coarsen it. See, the key is that when I do things like dynamic programming, this allows me to do that. Do things like dynamic programming, this allows me to work over a subset of the states. I'm not going to tell you what value iteration is, but this allows you to do value iteration over some meta states that only have quasi-poly sites. So basically, whatever is a key here that, you know, at which you have, and then that's the mega state. That's right. So once I had this, planning would be almost immediate. You also get this if something makes sense, right? Right. So that's what's kind of funny, right? The turnoff bound, you know, would tell me if nothing happens, I'd contract. If nothing happens, I contract. If I mix perfectly, I've totally forgotten. So, you know, some, I mean, like, there should be some way to just connect these two, right? It's not so easy as far as I know. Yeah. So the dependency on the number of states, it comes in because you go from L2 to L1. This is what happens in like mixing, right? I mean, it can come about for many reasons. For us, it comes through a reverse Pinsker, but you know, yeah. Yeah. All right. So let me. All right, so let me at least show you a proof. I'll show you at least the Evan Dar proof that's not nearly enough for us, but then I'll give you the intuition about how you can improve it. So let me tell you, you know, the crucial thing is how your beliefs update. I start off with some initial belief on what state I think I'm in, and then I get an observation. So I can look at this Bayes operator that just gives me a revised belief on what state now I think I'm in, right? And it's I think I'm in. And it's literally just Bayes rule. Because I'm more likely to be in a state X that has a larger chance of generating the observation I just saw. So given a belief and an observation, I just update using Bayes' rule, and that's B sub H. And then I can put the two steps together because I can take an initial belief, I can take an action. So once I tell you the action that I tell you the action that transitions in the underlying Markov chain. And then I get an observation from the new state I reach. And this update operator updates the posterior just by combining those two things, right? Because I transition according to the choice of the action from my initial distribution. I get an observation why, and then I update using Bayes' rule. And the question is really, how quickly do these things contract? So one trivial observation is that from the Trivial observation is that from the data processing inequality, transitions don't hurt. So if I start off with two beliefs B and B prime, applying a transition for the same action can only decrease their KL divergence. The transitions don't hurt. The subtlety is that there are some observations where the KL divergence can increase. So of course, what you want to do is you want to understand the progress and expectation. So let me tell you how the Evandar proof worked. It's very simple. Evan Dar proof worked. It's very simple. It fits on one page and it just uses chain rule for KL divergence. So what I claim is that the expected KL under a random observation Y from the model is going to have the old KL divergence minus something that depends on how different those observations would be if B or B prime were the true beliefs. Because that's the mark of how much I'm learning from that observation. So I should get some contribution. Observation, so I should get some contraction based on that. So, just for some notation, let me say px is b, qx is b prime. py given x is just the law on the observations given the state. And there's no qy given x because that's the same conditional distribution. And so I have pxy is just the product of those two things, qxy, same thing. So I can write out the KL divergence between B and B prime is the KL. Between B and B prime is the KL between Px and Qx. That's just notation. Sorry, this should be a minus. I think I forgot to fix that. So if I use the chain rule, it'll tell me that the KL between their joint distributions is the KL between their marginals and the KL between their conditional distributions on a random x drawn from Px. But these two things are the same because they have the same law for the observations given the state. Given the state. So this is zero, right? And now all I have to do is I can appeal to chain rule the other direction instead of starting from x and expanding to y given x. I do it the other way around. I get y. And then I get x given y. And then now we're in good shape because this is literally the posterior distribution on the state given the observation. That's what it means. So that's my Kale after applying the belief operator, and I'm good. And I'm good. All right. So, does this imply fast enough convergence? Not even close, right? So, the issue is that I could use Pinsker's inequality to, instead of having the KL between the two observation distributions, I could use the square of the L1 between them. And by observability, I can relate the distance between in TV the observations and their beliefs. That's what observability buys. Their beliefs. That's what observability buys me, is that statement. But now the trouble is that if you use like a reverse Pinsker inequality, which works when you have a bound on the ratio of B and B prime, well, you would get KL squared here. And this is just not good enough, right? Because in order to get the KL divergence to be epsilon, I actually have to take one over epsilon steps. Because as I get closer and closer to a small value, my progress is tiny. I'm only making... Is tiny. I'm only making epsilon squared progress. And this is really terrible because if you think about it, every time step where I discretize my beliefs, I'm losing an epsilon. So if I lose that epsilon over h time steps, and I need my epsilon to be like 1 over h, this tells me the amazing theorem that I can truncate my beliefs and forget about the past more than polyh back. That's not so good because there's only That's not so good because there's only h times 6. So we got no progress whatsoever. But it turns out, and I'll just sketch this very last part, is that this requires a bit of work, but you can show that there's a kind of win-win argument. So either you make a bunch of progress, you make a constant factor, or in the cases I told you where you don't make much progress, it turns out that you can characterize when you make slow progress, and it's only when the Progress, and it's only when the new KL divergence is anti-concentrated, meaning that there's a decent chance it goes way up, and there's a decent chance it goes way down. So now you're in good shape because instead of the progress measure that's the KL between the beliefs, you look at the square root of the KL between the beliefs. And then you can get some very nice contraction rates. And you can do lots of variants of this argument with different twists on the martingale. But what's kind of shocking is that sharp rates. kind of shocking is that sharp rates are actually still open. So we know sharp rates when there are no transitions between the states because it's a turnoff count. We know sharp rates when there's perfect mixing because it's zero. But this is a very basic information theoretic statement and I don't know. Yeah. So this okay, I I guess it was like uh uh a little bit mysterious, but this argument where you um are able to transform this difference in KL divergences into like a contraction. Divergences into like a contraction. This is like tailored to the POM VP setting, or it's like a general thing about like that's a pretty general thing. I guess like a different idea would be, oh, this didn't work because we didn't define observability correctly and we should just describe it in terms of scale. And I think that would make the analysis much easier. Is that what do you think about that? It's a bit tricky. I mean, I'm not entirely sure that that definition. I'm not entirely sure that that definition is a good one. Yeah, that's right. I mean, this is really motivated by the analogy with control theory things. And I think there are reasons why it's difficult to do it as KL, but that's a good question. All right. Let me tell you, yeah, there's a lot more stuff I could say, which I won't. But, you know, there is a, you can turn this into an algorithm for learning. It's not at all straightforward. This was another, this was like an 80-page paper or something. But, you know, the key challenge is that. But the key challenge is that when you don't care about what's the computation hard oracle you're using, you can do lots of things for exploration. So the usual paradigm within RL is what's called optimism. So the types of things you would do is like, I don't know which POMDP I'm in, but let me look at the space of all POM DPs that agree with my data so far. Let me find among them the one that has the best optimal policy. And then you play the policy from that. Policy from that. So that's what optimism is over uncertainty in what the model class is. You also have to not just solve an optimization problem, but you have to solve an optimization problem over the entire class. So then when you really want end-to-end guarantees, it forces you to develop new frameworks that circumvent optimism. We use something called barycentric spanners that are very beautiful geometric objects. I can tell you about that offline. But you really can get an end-to-end guarantee for this. Guarantee for this. It also has other applications. Oops. Yeah, let me tell you one more, which is: for those of you who know about learning results and hidden Markov models, we know that there are algorithms based on tensor decompositions for learning hidden Markov models that have certain full rankness conditions. And we know that when those full rankness conditions are violated, they can be computationally hard, like sparse parody with noise. But what you can do. With noise. But what you can do is you can define a generalization of observability, where maybe you don't leak information over one step, maybe you leak information cumulatively over k steps. So in aggregate, any two distributions that are well separated, they induce different k-step trajectories on what happens next. It turns out that in that case, you can actually learn HMMs, and it's a generalization of the things we know before. So somehow the only hard instances of learning HMMs. Of learning HMMs are exactly things that meet this information theoretic criterion, that they don't leak some information about their state for very long periods of time. So I think this is a very intuitive way to think about the difference between easy and hard that doesn't just rely on smoothing the underlying parameters. So only hard to learn HMMs are the ones that don't leak information. So this is meant as an invitation, you know, all of the things that we typically You know, all of the things that we typically do in PAC learning, where we want computationally efficient algorithms, I claim we know essentially nothing in the context of RL. It's actually maybe worse than that, because we know that we can't get computationally efficient algorithms in the models that people have suggested. So really, it takes algorithmic people to find what are the right models to begin with. You can do this for things with function approximation. I talked about POMDPs, but come talk to me after, and I'd happy. Come talk to me after, and I'd be happy to tell you about other questions I can't solve. So that's it. Thanks. Let's see, I've got another task. So at being used, he said that the transitions and rewards can depend on the time step. On the time step. But if the dependence is too wild, then it'd be hopeless, or like maybe depending with the possible, it would be hopeless to try and learn it. So I guess what kind of assumptions are you? No, so here it's what's called the episodic framework. So you should think about H as being reasonably big, but you're allowed many episodes where you play the full game. So even if the things change arbitrarily, it doesn't matter because you are allowed many episodes of interaction. Episodes of interaction. Alternatively, you can think about it as like the state comes annotated with what time step you are, and then the rewards and transitions all depend on the tuple of the state and the time step. If not, we can take the rest of the questions offline. Thanks, everyone. We'll be back here at 7:45 for a couple more talks. I almost forgot there are more people. We have some good science done this afternoon. I was pushing to go until 11, but they said we got late. That's right. I think we're going to be able to do that in RL. 