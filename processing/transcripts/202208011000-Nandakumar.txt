Now we can see your thank you. Okay. Yeah. Just to give a brief history of these birth workshops, it started in 2016 when we had a meeting in Banff. Basically, it is driven by a need to understand multi-phase multi-physics models. Models so that we can use them to improve the design of chemical process units that involve these operations. And in 2018, Dr. Anthony Walks led a similar proposal, and we had an excellent conference in Oaxaca. In 2020, we tried again and we were awarded, but it was cancelled due to the pandemic. It was cancelled due to the pandemic, but it was rescheduled to 2022. And unfortunately, I had to be out of the country at this time. I am in India. And so I requested Mayank to take over the organization. I'm really very, very grateful to you, Mayank, and to the other organizing committee members, Way Gay and David Bidel and Samir. I hope somebody will pick up this for the next round in two years because I think we are making very good progress in bringing the fundamental advances in turbulent flow and multi-phase flow and algorithms, etc., to the applied side of designing chemical equipment in an innovative way. So, thanks to Charlie. So thanks to Charles for that deep dive into turbulence. But what I'm going to talk about is on the applied side, can these multi-phase, multi-scale, multi-physics models enable innovations in the chemical industry? So we wrote recently an article on perspectives on manufacturing innovation for ACS Engineering AU. And so some of the ideas that Some of the ideas that are discussed is the basis for my talk today. Essentially, in chemical engineering, what we do is take raw materials and put them through the conversion technologies that you see on the left side. And that's where we want to focus on the innovation, because these are highly heterogeneous multi-phase flows. So I will present this in three parts. In three parts. What is the innovation scenario in chemical engineering? Examples of old style of innovation, trial and error using ammonia as an example, and how mathematics with computers can enable us to explore in silico new design innovations before we actually build very expensive pilot scale experiments. The examples that I'm going to present are fairly simple to begin with, but as the model To begin with, but as the models become more reliable and rigorous, we hope that we will be able to advance the innovation using computer simulation mostly. The other aspect about the lack of rapid innovation in chemical space as opposed to in electronics or automotive industry is the standardization. Is the standardization and modularization. And there are efforts going within the chemical engineering community. RAPID is one example where intensive process intensification and modularization are the key driving factors, even though standardization has kind of been left out. But I think if we can make advances in these fields, then we can accelerate the manufacturing processes and innovate. Processes and innovate the processes. So, I'm going to give you a few examples: one with retrofitting an existing problem in a polyethylene manufacturing plant and a new design of a fractal distributor, what we call nature-inspired engineering, to achieve a specific goal of having homogeneous distribution. And if I have time, I will talk about additional work, but I think that probably would be. Work, but I think that probably would be my limit. So, this is a slide, I think, from one of the former CTOs of Dove Chemicals, where he tried to highlight the difference between invention and innovation. Invention is something that you create that doesn't exist, and innovation is something that you improve incrementally year by year. And you see that in the automotive industry, in aerospace industry, and in computer. Aerospace industry and in the computational industry, where tremendous progress has been made through incremental improvement year after year and using advanced modeling and simulation. But in chemical industry, if you look at over the last 30 years or so, there are significant innovations in the new chemicals that are being synthesized and produced in large quantities, but not in the processes themselves. The processes themselves, the processes of mixing, reaction, and separation, these are the three principal processes, are largely carried out by equipment that are very similar to what they looked like 40 years ago. And the reason is these are very expensive equipment, and it's not easy to throw them out and come up with a new design. And it takes a long time, up to 20 years, to develop a single process. And that's because of the heterogeneous. And that's because of the heterogeneous nature of the multiphase flows. And we saw from the previous talk the complexities involved in understanding turbulence. Throw in on multiphase and particularly multiphases with bubbles and droplets where they can deform and break up. The problem is really very challenging to develop robust models that are applicable at all scales, from bin scale experiments to pilot scale to the field scale. So this illustrates the problem. So this illustrates the problem. This is a video from inside a distillation column from the Fractionation Research Incorporated that shows how complex the dynamics is between a vapor-liquid mixture and the ideal operating condition is the middle one called the froth regime. And we identify these from doing practical experimental pilot scale experimental measurements to see how these regimes change as we change. These regimes change as we change the vapor flow rate or the liquid flow rate. And these are the types of equipments that we are dealing with: bubble columns, fluidized beds, agitators, slurry reactors, etc. Every one of them is multi-phase in nature. And there is a dispersed phase, and there are heat transfer, and mass transfer, and chemical reaction taking place between these continuous and dispersed phases, depending on the operation. So it is a challenging problem. So, it is a challenging problem. And the question is: can we wait until we fully understand either multi-phase flow or turbulence or their interactions? Or should we be pragmatic in seeing what we can do to use advanced models to advance the design concepts? This slide essentially shows where the challenges are in terms of carbon management and where the innovation opportunities exist. Innovation opportunities exist in fixing CO2 back into hydrocarbons through fast photochemical or electrochemical conversion pathways. This is an active area for research. The nature does it in terms of its slow photosynthetic processes, but I think we need to realize that we need a balance between the forward rate process of hydrocarbon to CO2 conversion and the reverse process of CO2 hydrocarbon conversion. Of CO2 hydrocarbon conversion. And these are some of the chemicals that are produced in large quantities with a large footprint on energy consumption. Ammonia, ethylene, propylene are the leading ones. Now, ammonia production, for example, was developed by Haber-Bosch process, and both of them won Nobel Prize for their development of the catalyst. Of the catalyst. But the catalyst itself was discovered by trial and error with over trying 5,000 different catalysts to see which catalyst works and what are the temperatures and pressures at which you can maximize the yield. And that kind of problem can still not be solved by pure CFD simulation. It is truly a multi-scale problem in the sense the reactions take place at the molecular level. We bring the molecules together and We bring the molecules together and allow them to react, and then we have to separate them. So you see mixing, reaction, and separation in this flow sheet. But one problem of finding the right catalyst itself took enormous amount of effort. And it is an innovative catalyst that eventually emerged that won them the Nobel Prize that solved the food crisis problem that Maltas predicted, for example. This is a scale of such a reaction. Is a scale of such a reactor, the first ammonia plant in 1913. And the people, as I said, Heber Bosch, but they both won Nobel Prize. They did by experimental observation, but the full understanding came much later from studies of solid surfaces. And Gerhard Ertl also won a Nobel Prize for that work. My optimism for For claiming that we can begin to use these advanced models is based on this understanding of a hierarchical structure of modeling from molecular dynamics at the bottom to the continuum-based models, the Navier-Stokes equation, to the interpenetrating continuum models for multi-phase flows, all the way up to equipment-level models for Aspen, such as Aspen. Those are routinely used in the plant-wide simulator. Used in the plant-wide simulators today. They have been well developed. But innovation of new design requires us to look at detailed distribution of the velocities, the mean velocities, the time average velocities, especially average velocities, plus the fluctuations in them that contribute to micro-mixing. So it is a competition between macro-mixing and micro-mixing, and which mechanism contributes what fraction of the mixing. What fraction of the mixing process? We believe that we can drive an innovation cycle if we can focus on the so-called Euler-Euler model or the two-fluid model. So I'm going to illustrate an example of the design of a fractal distributor and fixing an existing polyethylene loop reactor. But the data scatter, for example, in bubble columns is quite significant, and there are opportunities for data-driven models to For data-driven models to bring in some hope to collapse those data, perhaps not an understanding, but a practical closure model for the drag and lift forces in those situations. Let me, in the interest of time, just move forward. These are the modeling frameworks, and I think this group is very well familiar with these. So, the models for the two-fluid one, and the models for the discrete element one, etc. The discrete element, one, etc. This is an important slide in our conception. That is, what is the problem with scale-up? If I do an example in a lab-scale equipment with a distributor, and if the dark circles represent particles or droplets, and the large circles represent the circulation patterns that develop on that lab scale, the equipment itself acts like a filter, which doesn't allow larger scale circulation patterns to develop. And if you make any measurement in that scale, Make any measurement in that scale and then try to put them in dimensionless form in terms of Nusselt number or Sherwood number and try to use that to predict the performance of the next scale, for example, the pilot scale. They fail often because the larger scale patterns now that are allowed in large scale equipment are not capturing the phenomena of heat transfer and mass transfer. And so typically you need to do an experiment on the pilot scale and gather the data again. And gather the data again and then move on to the next scale. So the uncertainties are in capturing the dispersed scale transport phenomena, the drag lift forces, the heat and interface heat and mass transfer coefficients, etc. And they depend on the dispersed phase particle scale, the structure of the distribution of the particle phase, and the white fraction, etc. So DNS in the turbulent sense is used In the turbulent sense, it is used in a fully resolved simulation, but the term PR BMS is being used for multi-phase flow, where we want to resolve the interaction between the particle or the dispersed phase and the continuous phase without any approximation or closure models. And that work is continuing and there are a number of presentations at this work, but we are going to present two examples. One is using the two-fluid model for low-pressure. For a loop reactor. So, this was the data that was given to us by a company that approached us saying that in a loop reactor for manufacturing ethylene, polyethylene from ethylene, the pressure fluctuations in the reactor are given by a signal like this, which are very large. And they wanted to minimize that and see whether we could use the two-fluid modeling framework to understand and eliminate the problem or reduce the problem. So we built. The problem. So we built a small Mach loop reactor where there are two particles and two phases: a liquid phase and polymerized pallets, which is denser. And when we put a cluster of those particles as a slug and move them through through the internal pump that you have at the bottom, when the slug goes up, the pump work goes up because it is lifting a heavier suspension. Suspension, but when it comes down, the pressure drop and the pump power goes down. And when it hits the pump, you get additional significant fluctuations. Once we demonstrated this to the company, we said, okay, we can take their actual reactor and set up a full-scale simulation, try to see how we can come up with a design improvement to mitigate that problem. So we set up a simple simulator first with a U-band and using the discrete element modeling, put a large Element modeling: put a large number of particles and examine how they move around as they go around the bend. So, the significant force here is the centrifugal force as the particles move around. They are denser and you do see them going to one side as they come out. And once we understood that, we said, how can we remix them? And so, we did that by putting certain veins at the exit of each bend that creates a centrifugal force. That creates a centrifugal force in the circumferential direction of the pipe. And we had three different designs: six vanes, four veins, and two veins. These are all done in the context of a two-fluid model with standard available drag correlations with corrections for the volume concentration. Concentrations can be as high as 30% or so here. And what you see here is when you have six veins, the centrifugal force created by these veins are so large that the particles are. Vanes are so large that the particles are actually thrown towards the outer wall and the concentration is lowest at the center. So, this is too strong a correction in the design. So, we said we will reduce it to four and then to two. And we found that if we have two small veins, we can achieve a fairly homogeneous composition as the slurry moves from the bend to upwards. So, at every bend, as it comes out, we need to have a vein like that, and by the middle of that leg, we Middle of that leg, we can redistribute it to achieve a homogeneous composition. So, this is an example of a retrofit. There are obviously a lot of inaccuracies in multi-phase turbulent flow modeling, but something like this tells us that we can explore alternate design configurations to see whether we can improve the performance. And this is the diagram of pressure fluctuations that they gave us and the pressure fluctuations that And the pressure fluctuations that we predict after this design correction in their full-fledged reactor. And the full-fledged reactor is fairly large in size, so it's only the two-fluid modeling framework that can handle taking the spatial variations in an approximate manner. You cannot use DNS or LES type of simulations. The last example that I'm going to talk about is Is a new design of a fractal distributor for a distillation column. Fractals are well-known mathematical entities, but they are also found in nature, as you see in the broccoli. These are scale-symmetric, scale-invariant geometrical entities, and you find them in nature in many, many situations. And our goal is to come up with a replacement for the distributor that you see on the left-hand side. See on the left-hand side. This is a standard Coke and glitch distributor for packed column where you pump a liquid through an entrance pipe, split it through the manifold, and then have various drip points. But the problem with that is that the flow rates from each one of the drip points is not the same because of the additional pressure drops that you experience. In order to minimize that, you have to increase the inlet pressure so high that the pressure drop between these two neighboring To neighboring outlets is small compared to the total pressure available. And that results in enormous energy waste. And so the idea is to replace it with a fractal distributor where the distance between the inlet and every one of the outlets is exactly the same. That is, you're approximately making the resistance the same to flow from the inlet to the outlet along every path. And so with that picture in mind, we That picture in mind, we said we are going to design a plate and filter type of fractal distributor. And so we take one plate, a plexiglass plate, and this is all done in the computer before we send it out for fabrication and use the same model in our CFD simulation. So one channel is split into four, and on the next plate, we have four inlets on the other side, and they are matched with these four outlets, and each one is split into four, and we repeat. It into four, and we repeat this process however many times we need. And at the end, it goes into the process unit itself where we have some packing and chromatographic separation or other color removal, etc., can take place. So the focus is on the distribution to be as homogeneous as possible. And this essentially shows the animation of how these things are put together. How these things are put together in a plate and frame type of a configuration. And we did some experiments to show that what is the residence time distribution. Ideally, what we would like is every molecule to have the same residence time, same experience as it goes through a process equipment. But because of the piping and connections, there is always some dispersion. But the question is, can we decrease the distribution as we increase the number of these We increase the number of these intermediate plates to increase the total number of outlet points. So, this is the measured residence time distribution with only 16 outlets, meaning only two plates versus 256 outlets. And you see that when you use 256 outlets, it does narrow quite significantly. The distribution RTD is much better. And this is from the simulation of a single. Is from the simulation of a single phase flow, of course. And what you see here is that the Reynolds number is quite high at the inlet, but as it divides and further goes down, the Reynolds number is reduced. So the flow is relaminarized. In addition, the innovation is to increase the by using the cone so that you get as flat an interface as possible inside the channel. And that's where the chemical separation, etc., are going to happen. And you will have a similar device on the And you will have a similar device on the other side to collect the material as well. When we did the simulation, we found out that still the distribution is not perfectly uniform, even though we made the length the same. The coefficient of variation is around 7%. And we examined the reason for that. And that is because of the design flaw here. When it turns around, there is a vortex that forms. And that doesn't completely get eliminated. And its effect is felt in the first split. There is more going to. First, split. There is more going to one side than the other side. And how can we mitigate this problem? So, the student that did that said, I'm going to have aspect ratio as my design parameter and try to study the influence of having a broad versus a narrow channel. The narrow channel will produce a smaller vortex and it'll be fixed or it'll become fully developed by the time it reaches the next splitting point. Using that idea, he developed. Using that idea, he developed hundreds of simulations and developed this response surface, which shows that this region is quite good for design because the coefficient of variation is less, less than 5%. But if you go operate in this regime, it can be as high as 30%. Here are the simulation results that shows that when you have a narrow channel, you do get to the fully developed state by the time. Get to the fully developed state by the time it reaches the next one. Whereas with the broad one, you do have a larger vortex, and so there's a significant imbalance in the split further downstream. Based on that, he went on to develop other types of fractal distributors for non-rectangular square geometries, circular geometries. And we have used that in one of the bubble column designs. I think I'm probably running out of time, so I will skip these. Skip these other design explorations and basically share with you our current pragmatic view of what are the problems that we can solve. If it's single-phase laminar flow, the risk in design is absolutely minimum. You have absolute confidence that the predictions are very good. Turbulent flow, there are still some challenges, but single-phase turbulent flows can still be designed using Be designed using pragmatic K epsilon, K omega type of models, or even LES models, even though further improvements are still needed, as pointed out by Charles Benou. Now, single-phase turbulent reacting flows is even more complicated because the reaction kinetic terms have to be handled in the presence of turbulent fluctuations. So, I rank the uncertainty as medium. Uncertainty as medium. And when you go to multiphase, multiphysics, whether it is liquid-solid systems or liquid systems, there are additional complications that come in. Breakup and coalescence have to be understood and modeled properly. When you have phase change, crystallization, boiling, condensation, you have nucleation kinetics that have to be understood at the molecular level. So the uncertainties are really very, very high. So this gives us a hierarchy of what needs to be done. What needs to be done before we can begin to use CFD for process innovation and designing the next generation of equipment? Now, today, I guess we can design an airplane with reasonable accuracy without ever going into the wind tunnel because turbulence models are, from a practical engineering point of view, in firm footing. We are not there for multi-phase flows yet. Close yet. So, I want to thank all the students that have contributed to this work over the last 10 years. I've been at LSU and I'm happy to respond to any questions. If not, I guess there is a coffee break coming up. I wish I were there. I enjoyed my previous meeting two years ago. So I wish you all the best. Enjoy OHARKA. It's a beautiful city. I cannot hear something hello. Hello. Kumar, can you hear me? Yeah. Okay. Yeah. Okay. Yeah. This is Charles. A very, very interesting talk. I was particularly enthused about your fractal design there, and certainly shows that the way CFD can be used to get better designs. I was wondering if what's the situation with the pressure drop? Because presumably when you made it narrower to get it more uniform, probably the pressure drop went up. Was that part of the design criteria? Is that of interest? It is. When you compare with the traditional design, you see that the pressure drop there is very, very high to maintain uniformity in the flow. So when you compare with that, this kind of a design results in a lower pressure drop. But as you decrease the channel dimensions, it will go up compared to It will go up compared to having a fewer number of outlets with a larger opening. But you pay the price in terms of not having a uniform distribution in the equipment. But I think it still outperforms the traditional coke litch distributor if you want to maintain a uniform distribution in the coke litch one, because you need to have a very high pressure drop there. Pressure drop there. All the energies are just wasted there. So we are back on time though. So if there are not going to be questions on Zoom, we'll take a coffee break and then everybody gets a 25 minutes break and we'll resume at 11 with 