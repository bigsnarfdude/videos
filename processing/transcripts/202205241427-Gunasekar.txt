Uh it would be a disaster, so I apologize in advance, but I think this is a focus and kind enough audience that I'd be like brave to try this. So I'm trying, so instead of the usual way of starting with a big picture, coming down to a result and then showing the proof technique, I'm gonna do a different format where I'm gonna state a lemma and prove a lemma without any context and then Without any context. And then time permitting, I'm going to go to the motivation and talk about where the lemma fits in into the larger representation cost equation. So, but just a teaser, this lemma is kind of one of the important properties we use to show the representation cost of multi-channel convolutional networks with no non-linearity. And I mean, it's kind of like this is the main technical part, which will let us to show that if you have. Will led us to show that if you have only one input channel but multiple output channels, the representation cost doesn't change whether you have independent of how many output channels you have. So that's kind of the teaser motivation. But like I said, I'm going to start with the proof and then go to the big picture. And so the first half, I hope to do it on my iPad with the proof. And then the second half is the usual presentation I have written about. Okay, so just to the paper is listed there and it's mainly joint work with And it's mainly joint work with Meena Chagadishan, who's a grad student at Berkeley, who's also a primary driver in the project, and Ilya Rajanstan, who was a co-author of mine in this art. Just to set up some notation, so I hope people can still see my writing and I don't have the greatest writing, so please stop me if something is not clear. So we'll be for, let's say we have A and B are vectors in R. We have a and b are vectors in rk, we will like implicitly assume that whenever I am talking about d-dimensional operations, I just pad them with zeros. So, if there is Rk dimension where k is less than t, I am just implicitly assuming everywhere that I would overload a with a bar where a bar is padded with zeros to match dimension d. And I define d-dimensional convolution, where d and k are two different parameters. D-dimensional convolution as follows. So, it's essentially like I Follows. So it's essentially like I extend A and B to this D-dimensional vector with zero padding and do standard convolution. So with the caveat that I use like 1 by square root d scaling, that does mostly to make some of the math go through easily, otherwise it doesn't matter. And I also use circular convolution. So in the standard convolution operator, if you reach the end of it and you can compute the parameters there, you have various forms of padding. Here I'll consider circular. Here, I will consider circular convolution. The main thing is, under this definition of convolution, there is a very nice representation of this operation in the Fourier domain, which is if H is like A convolved with B, we can look at the Fourier domain representation. By Fourier domain representation, I would simply imply that we take this Fourier matrix, which is a D by D complex matrix, which converts real vectors into Fourier trans. Vectors into its Fourier transforms. And Fk is essentially the first K columns of L. Is that notation clear? And now the convolution property can be written as the D-dimensional convolution of H is essentially equivalent to taking the Fourier transform of left-hand side. And on the right-hand side, you essentially have a dot product, element-wise dot product of the Fourier transforms of A. Of the Fourier transforms of A and B. So this makes the analysis very simple, or at least intuitive in many settings. And so this is the reason we define it with the scaling and circular convolution. And otherwise you have to care for some edge properties which are not very nice. Okay. So is the definition and the Fourier representation clear? I think many of you have known it, but I just wanted to put the notation in. So actually, so we think your tropical D is much, is like much bigger than the Much bigger than D in K or they can pick up bigger than D plus K. So the specific case I will prove is like K capital D is 2k minus 1. Everything else is like easy case. K? Minus 1. So if you think of k-dimensional vectors and... Yeah. So both are k-dimensional vectors. If you take two k-dimensional vectors and you convolve them, there is only like 2k minus 1 valid non-zero entries. So I'm going to specifically take 2k minus 1. Take 2k minus 1. The other cases are easy. So I mean, the proof would be for t equal to 2k minus 1. In that case, it's not really circular. We never hit that model, right? I'm just trying to see if I'm not, maybe I'm misunderstanding. No, you still construct a D-dimensional vector by doing circular convolution. A circular convolution with the zero padding. With the zero padding. After zero padding, it's a standard circular convolution. So you can just think of. So you can just think of this as a okay. If I look at my actual non-zeros, like my actual A and B, then it's like what I get is essentially a zero parallel convolution. Zero parallel convolution. But okay, think of it as like I'm just defining convolution as the inverse of this operation. Oh, because your age, your final result is a dual. Is d-dimensional. So I'm taking this as my uh the inverse Fourier transform of this as my uh convolutional definition. My convolutional definition. Maybe that's the easier way to say it. Okay. So I define A hat and B hat, which is just matrix multiplication, and then the invertwise product is my H hat and inverse Fourier transform of that is my convolution operation. Okay, and that corresponds to this definition. Alright. So the property I want to show is that if there is A and B are k-dimensional vectors, I want to show that there exists a C, which is also C which is also k-dimensional such that A convolve with B and B can A canvol with itself and B convolve with itself is C convolved with itself. So there are some cases which is very easy. I mean one of the easy cases is K equal to 1 in which case the LHS is just A square plus B square equal to C square by A B C R scalars. And this is obviously And this is obviously true. C is real scalars, and you can just take c to b square root of a square root plus b square root. And k equal to d, we can use the Fourier representation to show that the LHS is essentially a hat elementwise product with a hat conjugate plus b hat elementwise product with b hat conjugate. Actually, this is true for any k and this is exactly equal to a hat square plus b hat square. hat square plus v hat square where this is element y squares. Now if k equal to d then a c is essentially unconstrained I can take any c right and so I can just set c to be square root of a hat square plus b hat square. There are some symmetry properties you see still needs to satisfy but you can set the signs to match this. So see when c is unconstrained this is again very easy. So you just set it to this way and match some signs phase numbers and this is easy. Phase numbers and this is easy. The problem is when k is not sorry? Sorry, C hat. Okay, when k is not equal to d, I want c hat to be essentially in a lower dimensional space. C hat has to be in the span of the first K Fourier transforms. So you cannot like arbitrarily set it to square root of A hat square plus B hat square. So, and I think this is the tricky part where we don't know how to construct the C. If I give you A and B, C. If I give you A and B, I don't know how to construct this D. I'll prove this theorem and I still don't know how to construct C. The way we will prove this theorem, I mean, so essentially we will prove that this indeed can happen even though there is this complication that C has to be in the span of F. But we will prove it implicitly by saying that such a C exists rather than explicitly constructing C out of A and B. So now, like I said, I will prove an even weaker statement where I'll just show it for. Weaker statement where I'll just show it for d equal to 2k minus 1 because I claim this is the most important case. It's easy to see when d is larger than 2k minus 1, you just have extra zeros in the convolved operation. So you can just like, I mean, all I'm saying is like everything which is not, d is not equal to 2k minus 1, you can reduce it to tk minus 1. I hope you can take that on faith even though it's not very obvious to see immediately that d equal to 2k minus 1 is the important special case. And okay, how do I prove this? I'm gonna uh prove this by using a polynomial representation of convolutions. So, the idea is any vector in Rk is essentially equivalent to a polynomial of degree in in say p k of degree k with real coefficients. So, this is degree k polynomial with real coefficients. And the idea is essentially simple. So P A of x is essentially, I'm going to define it as a0 plus a1x plus a2x square ak minus 1x power k minus 1. I use a 0 base indexing rather than 1 2 k. So the k dimensional vector is a0, a 1 to ak minus 1. So I did put more. So this is essentially an isomorphism. Every vector can be represented this way. So there is no approximation or anything. And now we can essentially say what A convert with A is. You can check this, this is simple algebra. It is x power k minus 1. This is specific k. x power k minus 1. This is a specific case when d equal to 2k minus 1. Actually, this holds for d greater than 2k minus 1, but d equal to 2k minus 1 is what we are considering. So this is p A of x, P A of 1 by x. So you can do a simple calculation and see that these two are exactly equivalent. I'm not saying exactly equal to because there is some circular shift in notation, but coordinates are slightly shifted, but they are exactly equivalent. Are slightly shifted, but they are exactly equivalent. So now the property I want to show becomes show that for all a, b in Rk, I want to show that x power k minus 1 P A of x P A of 1 by X plus P B of 1 by X equals essentially. So, this is the property I want to show. And the way I am going to show this is essentially argue that, argue about the properties of the roots of l 's, the left hand side, and say that using those roots, I can construct a polynomial of the form on the right hand side. So, that's the high-level idea. So, this is essentially like the rewriting of the proof. And the other thing I'm gonna assume without proving is that. Assume without proving is that the LHS has no zero roots. This is this is again can be proved. It's the only reason I'm not proving it is for time. And the reason is like you can work out the only way LHS has 0 roots is if a0, a k minus 1 plus b 0, b k minus 1 equal to 0. And in any setting where this happens, you can essentially embed a and b to a k minus 1 dimensional space and then A k minus one dimensional space and then do the same thing in k equal to k minus one. So we assume that d equals 2k minus 1 prime. Yeah, so and d would still become d equal to 2k prime minus 1. So you can essentially convert this case where there are 0 roots to an exact equivalent case where there are no 0 roots and d equal to 2k minus 1. This again like I'm just asking you to take a pair of Like, I'm just asking you to take a faith because these two are like this and the fact that d equal to 2k minus 1 are important things, but they're not like very tricky to show. I mean, if I gave it as a homework problem, everyone here wouldn't give it a success. Okay, so I'm assuming that the LHS has no zero root. So I guess Okay, so now let's start with the proof with like first listing out the properties of properties I need so properties such that I can construct RHS that is the C of x C of 1 by X. So my claim is that if I can come up with a set S of size 2k 2k minus 1 such that this is including multiplicity, which are the roots of RHS which will essentially be roots of RSH with multiplicity. So, if this is not a set in the strict mathematical sense, that it can have multiple repeating That it can have multiple repeating arguments. So, I'm gonna enlist, it's more like a list of sets, a list of the roots. And then I want to be able to write S as S1 union S minus 1, where S1 and S minus 1 are disjoint. And they're of the equal size, that is, so this is so the convolution is of size 2k minus 1, so the highest degree you can. So, the highest degree you can get is like 2k minus 2. So, the number of roots is going to be 2k minus 2. So, such that each of them have the same size. It's just k minus 1. So, I'm going to write it as this and it has to and it has to satisfy two properties. One is that if alpha is real and And alpha in S1 then S1 with multiplicity, say n alpha n alpha, then I want alpha 1 by alpha to be in S2 with multiplicity, same multiplicity essentially. Uh sorry, we're getting you're correct that you're you're not unable to see the bottom of the screen up there because I'm sure. So what can I do? Is there a way to turn off my picture? Can we run downstairs and grab the guy? Okay, but the guy's on the chat. I know it's Carlos here from Burst. Unfortunately, there's just been no way to control the camera and what you have. The only other The only other solution that I would have would be to actually you just join the Zoom meeting and share your screen, but then this way there will be no turnaround and people will not be able to see you at all. Why not? That's the only fix I have for the online people. Just avoid that part of the screen now. Huh? Why not just avoid that bottom corner of the screen? The bottom corner of the screen? Yeah, I can do that too. Like a little patch on the screen. Like a little patch on the uplet. So what happened? Now I can just see myself. If you start writing, I think it'll change. Yeah, if you write, we can see the screen. So I've noticed it switches to that view when someone in the audience makes a noise. Someone in the audience makes a noise. In the audience views. So she has to be the one thinking the noise. Okay. So, I mean, what is it? Yeah, okay, now it's visible, right? Why can't we leave it like this? What changes? The technology is what can't shift. Okay. If you write on the top side. Write on the top side of the pages here. Okay, so maybe I can write in the side. Is there a way to turn off my video in the camera, in the Zoom meeting? To turn it off. Yeah, just my video. To have the screen. Oh yeah, yeah. Yeah, yeah. Okay. Okay. Thanks. Okay, sorry. Sorry for breaking the line of thought. Okay, I'll also rush because I'm kind of running late. No rush, it's fine. It's over. So the so can I continue relating? I'll try to. I'll try to write in the visible area. So, okay, just to recap, I'm just listing out the properties such that I will be able to construct the polynomial on the RHS. And because the right-hand side is a degree 2k minus 2 polynomial, I'm essentially I need to construct 2k minus 2 roots, right? And I'm saying that if I'm going to claim that I'm going to list a set of properties, and as long as the properties are satisfied, I'll be able to construct such a polynomial. Such a polynomial. The properties I'm listing is that my root should be separable into two disjoint sets s1 and s minus 1 and of equal size and they should have the following properties. So whenever an alpha which is a real number is an s1 with multiplicity n alpha, I want 1 by alpha to be in s2 with multiplicity with the same multiplicity. And the second part is that if alpha is complex complex and alpha is an S1 with multiplicity again and alpha, then I want the following. I want alpha star sorry the tab is not working. So then I want alpha star which is the conjugate of alpha or alpha bar which is the conjugate of alpha to be in again in S1 with same multiplicity and And 1 by alpha and 1 by alpha star to be in S2, sorry, S minus 1 with multiplicity in alpha. So I'm probably missing some coefficients of the Yeah, so the right I'm just constructing saying that if there is a S satisfying these properties, I'll be able to A and like what is A and like, what does B add on top of A? Because if it's a real polynomial, it has a complex root, its conjugate, it's also going to be a root. Yeah, that's what that is essentially the property. But it's. But not in the ensemble S1, necessarily. You need to divide the two ensemble. Oh, okay. Exactly. Because it's a product of two real polynomials. It means that it needs to be in the same size. Yes, apart. Yes. Right. Yeah. So if I'm able to construct S1 and S2 in this way, my claim. S1 and S2 in this way. My claim is that I can construct Pc of x essentially as with the elements in S1 as roots and then P C of 1 by x times x power k minus 1 is essentially like the elements in S2. You can cross check this because like you can see that it satisfies all the properties. S1 has to have real coefficients. So as long as alpha is in a root in S1, alpha star should also be in S1. And since if you think about it, like color. Oh, I can't see anything. The tomatoes weren't taken to catch up sometimes. So when you asked me, the requirement is the oxygen switches. So if you can see that if alpha is a root of P c of x, then 1 by alpha is a root of x power k minus 1 P c of x P C of 1 by x sorry. So there is a relationship between the roots. So as long as I have S1 and S2 satisfying these properties, you can check that I can construct You can check that I can construct uh phi z of x as well. So to uh recap I want S1 such that alpha and S1, S1, S2 such that alpha and S1 implies alpha star is in S1 and then 1 by alpha 1 by alpha star in S2 S minus 1 for this. And if alpha is real, the stars don't matter. So, star conditions don't matter. So, now we go to the roots of RHS, where you have x power k minus 1 P A of X P A of 1 by X. Sorry, this is L H S. Our k minus 1 P B of X P B of 1 by X. And I am going to argue about all the roots of this polynomial and say and divide them up into s1 and s2 such that they will satisfy the property. So first case is that if alpha is a root oh sorry. Okay, thank you. You can see. It's okay, no worries. I can also just listen. It was Jason. Sorry. Sorry, I didn't realize that it was either slides or the black. Is there a way to show two um two of the slides at once and just write on the right-hand side? If not, don't I think we shouldn't worry about it anymore. We shouldn't worry about it anymore. Personally, I would suggest to just go away with the slide and write directly on the board if it's possible. This way it should definitely work. But if you really need to be on the slide, I don't think I have any setup for that to change the camera. Like I said, it's unfortunately solid-automated. It usually works better with slide. And since you're writing as you go, I think our As you go, I think our automated system just doesn't really detect what it needs to detect. So it's going to focus either on you, then go back on the slide, then on you, then on the slide. And yeah, unfortunately, there's nothing about that. I did pass it on to our IT personal, and they will try to fix it, but I'll often use it. Maybe I think I'm gonna just write on the because this is just probably another five minutes and then Yeah, yeah. So I'm going to say that if so essentially so I'm going to list the all possible roots of this polynomial. So I'm going to say that if alpha is root with multiplicity 1 by alpha, right and then you also have alpha star as the root with multiplicity 1 by alpha. With multiplicity 1 by alpha. This happens because this polynomial is also having real roots, so all the roots are conjugates. All the complex roots have to be conjugates. P is... Why do multiplicity 1 over alpha? Huh? Alpha. N alpha naught 1 over alpha, right? It has multiplicity 1 over alpha. Oh, sorry, sorry, it's n alpha. Sorry. Multiplicity n alpha and then alpha star will also have. And then alpha star will also have the same multiplicity. Again, because of the, you can check by just substituting if alpha is a, this is something called a palindropic polynomial, which says that alpha is a root with multiplicity n alpha, then 1 by alpha is also a root with multiplicity n alpha. And this, you can just substitute, say that if alpha is a root, then you can also substitute. Is a root, then you can also substitute 1 by alpha and see that it's a root, and you can cancel both and then keep continuing until the multiplicity runs out. So it's easy to check that. So these two are standard properties of palindromic polynomials with real coefficients. So these two we will be using. But these two immediately give you some properties. One is that if suppose alpha is norm of alpha is not equal to 1, right? Then we have two cases. Alpha is real, in which case alpha and 1 by alpha. Alpha and 1 by alpha are distinct roots with n alpha multiplicity, right? And then I can just put n alpha times alpha into S1, n alpha times 1 by alpha into S minus 1. Okay, and then the other case is alpha is complex. Then in this Then in this case alpha is alpha alpha star are distinct and since their norm is not equal to 1, 1 by alpha and 1 by alpha star are also distinct. So you will have distinct roots with each with multiplicity n alpha. So again I can put alpha alpha star into n alpha into 1 s 1. alpha into 1 is s1 and 1 by alpha 1 by alpha star and alpha into s2. So this takes care of all the roots which are not unit which do not have absolute value 1. So the case which is special for this particular property this particular summation property is what happens if alpha equals 1. So again so your q of x is So your q of x is I'm j just gonna ignore x power k minus 1. So now I need to still handle the cases where the roots are unit norm. So if suppose alpha is equal to 1, right. Now my claim is that alpha will always have even multiplicity. Okay, first of all, what is the problem with alpha equal to 1? Then you have that 1 by alpha equal to alpha. So I cannot split this way. I cannot put alpha in S1 and 1 by alpha in S2 without like as disjoint sets because alpha and 1 by alpha can be same. And this is if alpha is real or you also have alpha star equal to 1 by alpha if alpha star equal to 1 by alpha if alpha is complex. So in both cases I cannot do what I did in the end of last slide. So this case the only way I can handle it is like if they have even multiplicity. If they have even multiplicity I can always split half of them into S1 and half of them into S2 then it doesn't matter whether the alpha and 1 by alpha are the same. So now the claim is that I'm going to show that alpha will always have even multiplicity and by the argument Have even multiplicity, and by the arguments I made, this will essentially prove the lemma. And the even multiplicity now essentially comes from just substituting for q of alpha. So, this is where the structure that I'm summing to convolutions matter. So, q of alpha equal to 0, if alpha is like a root q of alpha equal to 0, this implies, so alpha power k minus 1, P A of alpha, P A of 1 by alpha. T A of 1 by alpha plus P A of alpha 1 by alpha equal to 0. But since alpha norm is 1, this implies alpha star equal to 1 by alpha. So this essentially becomes alpha power k minus 1 P A of alpha P A of alpha star plus P B of alpha P B of alpha alpha al of alpha p b of alpha star equal to 0. So, this is again these are two polynomials with conjugate roots that means I can write it as alpha power k minus 1 P A of alpha square plus P V of this can only happen if P A of alpha and P V of alpha equal to 0 so what I So, what I have shown is that if alpha is a root of the sum polynomial and norm of alpha is 1, absolute value of alpha is 1, then it has to be a root of each of the individual polynomials P A and P B. So now you can see that if the second claim is that if alpha is root of P A of alpha with multiplicity N, Multiplicity n, then if you look at the polynomial P A of x, P A of 1 by x times x power k minus 1, alpha will be a root with multiplicity, with multiplicity 2n. And I mean, this you can work out by essentially saying that let's say alpha is complex for simplicity, then X for simplicity, then alpha is a root of P A would imply that alpha star is a root of P A. This would imply that 1 by alpha star is a root of P A of 1 by x because 1 by x tends x bar k minus 1. And this 1 by alpha star is essentially alpha because the root is 1. alpha because the root is 1. This implies alpha is a root of A of 1 by x. So I'm multiplying two polynomials each having alpha as a root with n multiplicity. So the product will have alpha as a root with 2n multiplicity. And the same argument you can make for Pb of x and P b of 1 by x. So I have argued that whenever alpha equal to 1, the multiplicity will be 2n. And now as long as 2n is there, And now, as long as 2n is there, I can go back and do the same operation as I did in the last slide. I can put half n times alpha and n times alpha star into s1 and the other n into s2. So, I created, I took all the roots of the left-hand side and I created two buckets which satisfy all the properties such that I can construct our right-hand side. So, that essentially is a proof of like the original convolution property I talked about. The original convolution property I talked about. So basically, the use of d equals 2k minus 1 is only the representation as polynomials other than that. Yeah, so but you can reduce everything to 2k minus 1. You can see that if d is greater than 2k minus 1, it's essentially like the 2k minus 1 case with both the right hand side and left hand side padded with zeros. So it's d equals 2k minus 1 or above? You can also have d less than 2k minus 1. Also, have d less than 2k minus 1. In that case, it's a little bit more complicated to see, but you can show that in that case, you can write it as a sum of few different sum of multiple such cases. So, you have a proof for that? Yeah, we have proof. Proof is quote. So, the proof for other values of D, maybe not D equals k over D, but is some extension of this. It's based on this. Yeah, it's some extension of this. The proof holds for any D between. This. The proof holds for any d between any k between 1 and d. So as long as d is greater than or equal to k, it would work. So the thing is, all those cases where it's not 1 or d or 2k minus 1 can be reduced to 2k minus 1. So I think there's a proof for that. And even the second thing I mentioned where the LHS has no zeros, it's not an assumption. You can actually prove what. It's not an assumption. You can actually prove what happens when LHS has zero roots. Yeah, that's easy. You just add zero roots on both sides. So that's again easy to do. So in some sense, like the proof I gave is in some restricted conditions, but the real proof in the paper has like for general be. The real proof holds this for all KA less than or equal to B. So how does this tie in with things like Z transforms of FIR filters? Like it I feel like there is a connection, but I can't. So I think like uh so you can I mean the if you take this Fourier representation and replace this Fourier with like some other transform and define a transform in real space as essentially the inverse of that transform, many of all these properties would go through. The only thing I'm using about Fourier representation is that it converts the linear operation into Converts the linear operation into element-wise product, and it maintains the norm. I think the Z transform does that too, but it explicitly has polynomials and roots. It's very closely related. It's very closely related. So essentially, the proof is not specific to Fourier. It might change what is the original operation you are working with. So convolutions and Fourier are tied in with this connection, but any two linear operators tied in with this connection would, the proof would essentially go through all that. Because you were saying at the beginning that you could have this proof. Saying at the beginning that you could have this proof, but you didn't have anything constructive, right? You couldn't construct a C, but couldn't you use just like FIR filter design techniques to do it? I guess that's why I'm making it. I don't know, I haven't tried it, but it's possible. But like, we played with this proof to do a constructive proof for a while, but it seems hard. Like, because even when we try to actually Tried to actually, like, even simple examples where we tried to construct, we couldn't see any gear pattern on what C would be depending upon A and B, how C would look depending on A. Maybe I guess I don't totally get the broader context. So are you looking for an analytical expression for C or are you looking for a computational way to actually construct C? So I don't need to construct C. I'm going to use this as an argument to show that the representation cost of multi-channel networks does not change with how many output channels. Does not change with how many output channels you have. So, this essentially shows that. I see. So, I think I'm out of time, but I did want to think I can take five minutes to show how this fits into a bigger picture. Nadi, you had a question? That was strength to understand. But I think I understand just what you're answering. Okay. Can I ask one question before you go on, Sir? Yeah. Can I change those? Yeah. Good. Is there a way to understand the statement or the proof probabilistically? I mean, I'm often thinking of this self-convolution as being about what happens when you let a rim, an element of the cyclic group, act on a vector. This is by the second moment. Saying something about the fact that the space of possible second moments is convex or something when you restrict to this subspace. Does this connect with any ways that you followed this? Is there any other perspective that you have here? I don't know how to connect it to probability because like the because I think the entire paper is has no mention of probability but I no so I don't I don't know if there is a probabilistic version of it where I mean I guess the argument would be if A and B come from some distribution can can you say something more specific right I meant something like the problem if you if you I I meant something like the following. If you s if you fix a vector A, then just let the cyclic no, just uh determine something A, and then let the cyclic root act on the coordinates of A just by rotating to the coordinates. That gets you a random vector in Rd. And the second moment of this random vector, like covariance matrix of this random vector, corresponds with the convolution of A with its Of A with itself up to these scale x. So, where is the randomness coming from? Sorry. It comes from the fact that you define a distribution by randomly approximating the coordinates of the vector n. Oh, I see. I see. So you're saying that A convolved with A can be written as second moment of A multiplied with randomly related coordinates of A. Yeah, basically, something like that. Basically, something like that. And then this starts to have the flavor of some of the classical moment theorems that say, like, you know, if you can ask when does a, if I give you some data in the form of moments, when can you construct a probability distribution that actually has those moments? Anyway, it just felt a bit like your result. But if this doesn't ring any bells, please. Yeah. No, I actually thank you for pointing it out. I wasn't even aware of this property, so that's why I was confused initially also about the question. Question. I wasn't even aware of this property, and I'll take a look into it and see if there is an alternate way to show this or show a connection. Connect okay? Okay, um so essentially Okay, so essentially this is like the slide version of it. So basically we started by looking into the L2 representation cost. Like many of the, I think Vika talked about it yesterday and Arthur, no, you're not talking about it. Okay. But many of you have worked on this problem of like what does minimizing L2 norm in the parameter space look like in the function space. And that is the primary motivation for this. And so in some sense, you can define this induced regularizer in the function space where given a function In the function space, where given a function f and a network architecture, we want to see what is the minimum L2 norm of the parameters required to represent that function. And so, this is, I mean, this we can define as the representation cost of a particular architecture in terms of L2 norm of parameters. And it has connections in a sense that minimizing L2 norm in the parameter space, like if you do the actual global minimizer, it is equivalent to minimizing this R of F complexity measure in the function space. Yeah. And we set out to study what the specific RF is for convolution, multi-channel convolutional networks with linear activation. So the teaser for this was like we had a version of this result for when the so the convolutional layer has convolutional network has first layer being convolutional and then the second layer is a linear layer and if the first layer is a full dimensional convolutional and one channel we had a this And one channel, we had an explicit closed form of this problem. And that essentially derived from the Fourier domain representation. So essentially, you can write the output of this convolution as a linear function where the linear matrix, it's equivalent to W times sorry, beta uv times x, where beta uv is essentially convolution of the two problems. And if you write it in the Fourier domain, you essentially have the constraint that beta hat is just a product of u hat and v hat. of u hat and v hat. And like I said, when d is unconstrained, like when d is equal to k, u hat and v hat can be, beta hat can be unconstrained. And you can show that the induced regularized cost is just L1 norm of beta hat, where beta hat is a Fourier domain of the situation. So then we try to see what happens for small different case and even for k equal for k equal to 1 again you can have a explicit form but for any other k it becomes a form but for any other k it becomes a very complicated polynomial but there does not seem to be a interesting interesting closed-form solution. So the other thing we wanted to try but you can see that it's some kind of interpolation in the Fourier space but the other thing we wanted to try is like what happens if we increase the number of channels. But this also ties in for different case can you come up with some kind of a relaxation of what is going on? Can you come up with some upper bound What is going on? Can we come up with some upper bound, lower bound, anything? And this is where, if you take this Rk of beta, which is essentially, you can maybe start with this slide, you want to say it's min because we are working with just finite dimensional spaces, min over L norm of u hat V hat, such that beta hat can be written as dot product between u hat and v hat. But the only constraint here is that u has to be in k dimensional, it cannot be d-dimensional anymore. So, you cannot just set So you cannot just set uh beta hat to be square root of u hat times v hat or something like that. Uh so now you can actually see that like both the norm and the constraints can be written in terms of like both objective and the constraints can be written in terms of a lifted matrix. So think of a lifted matrix W as u start with v multiplied by u hat v hat. So it looks like a 2d sorry k 2d sorry k plus d times k plus d dimensional matrix where block matrix where you have u u transpose u v transpose v u transpose v v transpose and the objective is simply like the trace of this matrix and the constraints can be thought of as like linear so each constraint can be thought of as a linear linear mapping of these off diagonal entries right. So I can essentially write the same problem as Write the same problem as something which looks like an SDP, but the caveat is I have a rank constraint that this W matrix has to be a rank 1 matrix because I have stacked U and V with rank 1 matrices. So I can write the same problem equivalently without any approximation as an SDP with a rank constraint for any k. And of course, like that means if I drop the rank constraint, I get an SDP relaxation, which essentially would be a Like which would be a lower one for the complexity measure I care about. The interesting thing about this SGP relaxation is like now if you convert this architecture to multi-output channel convolution, so your input is still one channel, but you have multiple output channels, then it essentially comes down to the same problem with a rank constraint equivalent to the rank being less than the number of output channels. So, the representation cost for multi-channel convolution essentially is the same SDP but with a different rank constraint. So, obviously, this has the same SDP relaxation. So, the SDP relaxation is a lower bound not just for convolution with one filter, but any number of filters. Of course, this SDP is tight as long as soon as your number of output channel is greater than the dimension of the matrix, which is D plus K, that is trivially true. But what we surprisingly show is that the SDP is tight for any K. That the SDP is tight for any K. Sorry, any C out, not any K. It's tight for any C out. It doesn't matter what the C out is, the SDP is always tight. That means the representation cost, I started by saying that the SDP is a relaxation of representation cost. And in some sense, this is saying that the representation cost is completely independent of CR. And this essentially says a bunch of things that the representation cost is convex for this two problem and it is representable, it's computable with an SDP. It's computable with an SDP and so on. But the proof I showed you was essentially a component of showing this proof. So at a high level, like if you just write down the KKT conditions of the SDP, you can easily show that all the solutions of the SDP are of rank K. So this gives you a slightly weaker relaxation that as long as your number of output channel is greater than As your number of output channels is greater than the kernel size, the representation cost stops mattering. But then showing that it is tidy cost stops mattering after this. Yeah, the representation cost is the same as long as your output channel size is greater than K. Just right. The number of channels doesn't matter. Number of channels doesn't matter. Number of channels doesn't matter from a representation cost perspective. Cost perspective. This is, but with just writing down the KKT condition, you can see this for C out greater than or equal to K. Right? For all K, yeah, I think I'm just saying that this is the comments of the proof. So I was just saying that this was just a comment that if you just simply write down the KKT conditions, you will see that all solutions of this SDP will have rank at most K. rank at most k. So that means any rank constraint which is greater than k is like is not does not matter. But then what we show is like it is true for any C out greater than or equal to 1. And there is a slight subtlety here. There are solutions to this S D P which are of rank greater than 1. It's not we are not claiming that all solutions of S D P have rank greater than 1. In fact, like may if you actually implement it even on random Actually, implement it even on random data, you will have rank greater than 1. But what we showed with this implicit proof is that every time there is an SDP solution for this problem with RAM greater than 1, I can convert it implicitly to a rank 1 solution with the same cost and satisfying the constraints. Essentially, that's why the implicit proof is sufficient. So, what the implicit proof does is takes a rank k solution of this SDP. Solution of this SDP and uses this argument to essentially show that there is exist an act one solution for this SDP with the same cost and same and satisfying the same constraints. And this is where the proof I showed comes into play. Sorry, it was a little bit of a messy presentation. So that's only for one input? Yeah, so actually I wanted to talk about multiple inputs too, but then I ran out of time. So for multiple inputs, it's an interesting thing. You can also write it as an SDT, but we are not able to show tightness. I mean, in fact, it's not tight for any output channel. Because even if you