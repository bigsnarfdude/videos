And he will tell us about systematics and flavor physics in our hello everyone. Thanks for the invitation to come here. So as it says from the title, I am a flavor physicist and therefore I will tell you about the systematics that we have in the flavor physics experiment. So I'm currently a postdoctoral researcher at KIT and I'm working with the Belto experiment, but I will also show you some systematics on how we treat them. Systematics and how we treat them, also from other experiments. So, because most of the room are either statisticians or experimental particle physicists, but from other experiments, let me give a small introduction to flavor physics. And in particular, as I already mentioned, I will be talking about two specific experiments that are concentrating on the measurements of Of B case. So these are particles that have a B part in them. And this, I told you, there are two experiments currently in the world that do this. One is LHCb and one is BEL2. And they are actually very, very different. So not only do they reside on the other side of the planet, so LHCb is at LHC and Bell 2 is in Sukuba. And Bell 2 is in Tsukuba in Japan, but also their detector setup is very different. While LHCB is a forward-looking spectrometer, BEL2 is more like CMS and ATLAS. So this is a general purpose detector that has more full pie coverage. One other difference that there is is also the stage of the experiment that each of them is in. So LHCB. So LHCB has been running and collecting data with other LHC experiments. So this is really an experiment that's been running for 10 years. And Velto, on the other hand, only started to take data in 2019. And the reason why I am saying this is because when it comes to thinking about systematic uncertainties, for those who are in this field for a long time, you may think about them differently at the beginning. Beginning while then, after 10 years of running. Now, one small physics remark with respect to how the B experiments are different to ATLAS and CMS. So, we are looking really for the decay properties of B hadrons, and these are particles that are roughly 5 GeV heavy. So, if you compare that to Higgs boson, If you compare that to HIG boson, which is much heavier, you realize we must be sensitive to much lower deposits. And another difference is that our vertex resolution must be really good because one of our pillar programs or why we build these experiments is to look for CP violation, which is basically why we have more matter than antimatter in the universe. In the universe. Now, as other experiments, we do not have only one physics program, but we are looking for different things. So, in particular, we also look at the B dynamics. So, this is covered by looking at different spectroscopy of the B decays. But what I am going to talk about in these talks are mostly rare decays of these B hadrons or V mesons. And V missions, and we use them in order to look for the new physics indirectly. The reason or the rare, they are called rare because the leading order diagram in the standard model for this decay only comes at the loop level or a box level, and therefore it is heavily suppressed in the standard model. And when I say rare, I'm usually talking. Very rare. I'm usually talking about processes which are of the 10 to the minus 5 and less. So you need a lot of statistics in order to make measurements. So the last piece of physics and nomenclature, just to make it clear, the ATLAS and CMS searches, they are mostly looking for directly for Directly for new physics. And so, what you can imagine is that you have a collision, and then you have a production of a new particle. And this particle must be constrained or is constrained with your E is equal to M C squared equation. So, the mass of the particle is limited by the collision energy. When we are looking in these loops that I told you about, here Told you about. Here, this is a quantum effect. So, here we have only Heisenberg's uncertainty principle that guides us. And in this way, the mass of the particle can be very high. So, we are really sensitive to new particles that are much heavier to what the direct searches can produce. So now what are we actually measuring in rare V D case? So what are basically our parameters of interest? Our parameters of interest. So, first of all, we want to measure the rate. So, this is called a branching fraction. Before we get there, we mostly try to look for it. So, we are setting limits. After that, after we measure the rate, we are going to look a little bit more differentially and we are going to look for and try to measure the angular observables. Measure the angular observables. So these are basically our distributions in the angular space. And then once we have that, we can also measure ratios. And depending on what kind of analysis we are doing with these rare decays, we have different parameters of interest, different complexity also of the models that we have to construct. Now, Now, I want to just mention here one more difference between LHCb and BEL2. So while LHCB, while BEL2 can measure the, let's say when we are looking for the rates, the absolute branching fraction using this equation at the bottom here. For L8CB, For LHCB, this is not possible. So, there we are usually doing relative measurements. And when it comes to systematic uncertainties, what one has to think is, okay, in Bell 2, we have more of them because we cannot cancel part of it by the fact that we have another decay that we measure it with respect to with. On the other hand, by doing the relative approach, you have. Doing the relative approach, you have to consider systematic uncertainties that are also on the other decay, not your, let's say, decay of interest. So let me show you how or what kind of tools we are using in order to do statistical inferences. So, this was already discussed in some other talks here. Here. So we have two ways that we can build our model. So we can either do it per event or we can do a stepwise per event model. So this is how we build our models. And I would say that both of them are used and usually the decision on which to use is based as On which to use is based as a function of your data set, but also if you have really peaky structures or not. And lastly, whether you can actually come up with the parametric function. So in the unbind case, you are going to use the parametric functions for your signal templates, for your background templates, and in the bin case, you are going to use the signal. In the bint case, you are going to use parametrized histograms. Now, after we build our model, like in other cases, we are going to include and think about the nuisance parameters by adding in the part in the equation for the constraint terms, which captures basically your systematic uncertainties. Uncertainties and after this you are going to translate you are going to basically get from your functions your likelihood which you are then going to minimize. Now for the rare decay searches especially if you are at the beginning you may only be able or you may have only statistical power to set limits and more set limits and most of uh the time uh we are uh following the limit setting uh procedure with uh the frequent in the frequentest way where we build our test statistics that is based on the profile likelihood ratios like it's done also in other LHC experiments. In the case that we have enough statistics, of course we go towards significance and then we are Towards the significance, and then we are starting to do precision measurements. Now, this was already also mentioned before in many other talks. There is a long way to go from a theory all the way to your fit and your data. And maybe one thing I would like to highlight here in terms of the inclusion of the systematic uncertainties. Of the systematic uncertainties is that especially LHCB has quite a few systematic uncertainties that need to be related to the, I told you, it's a relative measurement that they do. So sometimes this other decay is better measured by other experiments, for instance by BEST string, and so this needs to be propagated into our likelihoods, into our Our likelihoods, into our model. So, how also, first of all, how do we get our systematic variations and then what methods do we use to evaluate them? So, this was also already mentioned, but I just would like to highlight the fact that we are using basically also all the different tools as in the other LHC experiments. So, first of all, Experiments. So, first of all, we use other simulations, we use calibration samples, we use also this single embedding technique that was mentioned already before, or we use other data samples. And in order to evaluate the systematics themselves, we use, like it was nicely explained by Thomas. Nicely explained by Thomas and also reiterated by Nicolas. There are two ways how we can get them. So we can either use the bootstrap method where we generate a large number of solar experiments or we repeat the determination of something that we are interested in by using an alternative model. So now I would like to just give you. I would like to just give you a quick run through a few practical examples of the systematics that we have to think about. I won't go too much into the detail, but there is the mixture of the good, bad, and ugly here. So unfortunately, we have them all as well. So just this is just a slide. I have a slide for each of the components for the systematic uncertainties, but I would just like to say that. But I would just like to say that for the accelerator, as I mentioned, this is something that doesn't need to be considered in the relative measurements. But for instance, for Bell 2, we need to get systematic on how big our data set of V sample is and what is the measured real delivered luminosity. We also have systematic uncertainties on the scale. Is on the scale, on the energy scale and the momentum scale. So, this is also here. One important one, if you have followed the flavor physics experiment result on the particle identification, one, I mean, as I said, we also had ugly ones coming from the theory. So, here I would say that there are two kinds. So, not for all the BDKs, there is a perfect theoretical model that has been put down. So, in that case, really, we have to think about how do we want this model to look like. So, there is a very important systematic coming from there. But if we are kind of, I would say, lucky in this case, and there is. I would say lucky in this case, and there is a theoretical model that precisely describes what you want to measure. There's, of course, also the uncertainty on that, and one has to think about how to propagate that. Of course, you have a lot of systematic data also related to the simulation and also to further analysis technique. This also includes the use of the machine learning algorithms, which we use. Algorithms which we use mostly to suppress the backgrounds. Now, let me just maybe talk a little bit about a few special methods that we use that are really important in terms of evaluating the systematics and how we capture them into our model and then how we assess our data model compatibility. So, correlations are So correlations are very important and I mean this is something that maybe at the beginning has not been at the beginning of the experiments has not always been present but now there are of course a lot of techniques that one can use in order to capture the correlation and in order to put them into the fit. So I just would like to show one or two examples. Examples. So here we have our statistic summary of our distribution. In this case, we are doing a bin fit. And we are doing this bin fit in variables such as the transverse momentum of the kernel and the output of the se of the B D T. But this is not really important. You have the templates that we have. You have from the Monte Carlo and then we have our data. Then we have our data. And we have a systematic uncertainty that is related to the particle identification, for instance. And this systematic in particle identification has been computed in the bins of this transverse momentum and another variable. So, of course, what one So of course what one needs to do is to try to evaluate the correlations because I mean one has to do this for the sample and because the measurement is going to be as a function of this variable. So what we do here is we use the bootstrap method, we produce alternative simulations where we vary the Vary the basically the weight that we give for this PID correction with the statistical uncertainty that we get from the calibration sample. Then we construct our covariance matrix, which is what you can see on the plot on the bottom here. And once we have that, in order to In order to translate this into our nuisance parameters, we perform a singular value decomposition and then we take the leading eigenvectors to be our nuisances. So this will basically capture our correlated systematics. And another example I have is, and this was also discussed yesterday in the talk. In the talk about neutrino experiments, LT2K and so on. This is the discussion about fitting a distribution first and then propagating the fit results, let's say, into another further fit. So in this particular case, this is This is the decay of B going to three muons and a neutrina. Here we did not have, we couldn't simultaneously do the fit because that would have not worked, because there would have been too many degrees of freedom. But we had a sample that was there, so like a control sample, but yeah, it could not have been. But it could not have been fitted simultaneously. So, what we have done is we have fitted it and then we propagated these fit results into the main fit by using the multivariant Gaussian constraints as our systematic. So, this way we have transported the correlations from the previous fit. Now, Of course when we do the now going back to the bin fits one other important step always is to check whether when we construct our model there is the compatibility to the data and for this in for the bin fits we can use It we can use a toy-based approach. So, in this case, what we have done is that we have, once we have unblinded the data, we have generated the toys according to the plots and statistics in the data. But before we have performed the fits, we have subtracted actually the observed data count. Observe the data counts and replace them by the counts that we expected from the default Monte Carlo model or the ASIMO data sets in order not to double count the fluctuations. So this allows us then to assess our data model compatibility. So on the right you see You see the distribution for the minus two log likelihoods and the minimum value of all the toys that we had put in. In the red, you can see, okay, this was 24 bin fits, so you have a asymptotic, I mean you have the basically the asymptotic rice per distribution for 24 bins, and we were actually able to assess. And we were actually able to assess our p-value. So now I would just like to mention a few challenges that are there in terms of how to build statistical models and what kind of pitfalls we fall into, which is also related to the nuisance parameters. So first thing first. First thing first. So, we are doing rare decay searches. So, in order to do them, the main goal of the analysis is to suppress the background as much as we can. And we use, I may have mentioned it, machine learning, MVA techniques to do this. Sometimes we do fall under the category of low statistic samples and And when you have that, this becomes a problem because you are starting to be in the era where you have to trade the smoothness with the fit stability. And you also have to check that the asymptotic condition is satisfied. And once you have the fit stability in your problem, Problem. Another thing to think about is that you may have too many, the more you put the nuisance parameters in to your feed, the worse, the less stable it will be. So I think this is one of the challenges in terms of statistical models for the rare decays. Now, another challenge is that control samples are not Control samples are not so easy to get by, and sometimes if you have them, they are not as large because you want to have something that is very similar to your signal, right? So, this is quite challenging in the analysis that we do. And this also means that the nuisance parameters are very much, let's say, inflated. Let's say inflated, and basically, one would even say that sometimes they are not constrained at all. Now, one other point that is really hard in terms of doing this kind of analysis is that, okay, these are rare decays that you are looking for, and therefore you are not only trying to suppress all the backgrounds, I mean, that are The backgrounds, I mean, that are much higher in the branching fraction. But a lot of times you have decays that have either never been measured or never been measured and not modelled. And then you are entering the territory of the ugly systematics. And you have to basically decide very carefully on how you treat them in your models. And one last point, which is kind of One last point, which is kind of related also to the second point, comes with calibration samples. So especially for not so very rare decays, we don't have actually enough statistics and the MC statistical error becomes actually the prominent systematic in this Systematic in these measurements. So, this is something, these are basically the four different points where the modeling and how we treat our systematics come into the rare decase. So this brings me on to the conclusion. In this presentation, I have summarized basically how we do the statistical modeling in rare BDK searches, showing you some examples. Showing you some examples of systematic uncertainties and how we treat them in our statistical model. I think it is, I mean, we are using and trying to treat every source of systematics and really try to think whether they are correlated. And if so, we have the ways how to propagate them. And I also have highlighted a few problems that came to my mind when talking about the rarity case. When talking about the rarity case. Thank you very much.