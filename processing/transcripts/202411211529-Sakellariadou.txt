So, the title of the talk is Searching for Gravitational Waves Using Dictionary Learning, which is a machine learning kind of technique. And for me, there are two issues where machine learning can be useful for computational waves. One is to identify a subtracting noise, either instrumental or astrophysical one. And the second one, given that the number of candidates we expect that's going to increase a lot, do we have a fast Do we have a fast and efficient way, at least for the first screening, and then we can go more carefully into parameter estimation for the candidates that we seem to be most promising. So having that in mind, I'm going to present three papers, and you can see the titles of the papers, the journal, and the people with whom I collaborate. The first two they have to do with Lisa, and the third one has to do with FPK. Has to do with FPK. So the first is this approach in order to be able for users to detect massive binary black holes in the presence of the noise, which is going to be from white dwarves in the Milky Way. The second one is to find a fast way for MRIs, which for the case of GLIS are very useful and they will give us quite important information about test of GR, massive black holes and so on. And the third is the And the third person would indicate a fast way to do a fast screening of gravitational wave signals which are somehow in the north. So I would like to give credit to two PhD students of mine, Bajer and Martinovic. Martinovic, Katarina, has finished, Bajir is about to finish now, and also Srin Kasab, who was a PhD student at NIST and now his postdoc at Siri. So let me start with the first one, which is Let me start with the first one, which is how you can use this approach to detect binary black holes in the trainings of galactic noise for lisa, so in the millisecond frequency band. So the problem here is that LISA is going to find tens of millions of white warps in the Milky Way, and this can be a problem to identify massive black holes. So, LISA is going to detect gravitational waves from the spider pipeline. It can identify some of them, but the masses are. Identify some of them, but the majority are going to form an unresolved background, which is what we call the galactic confusion. So, the approach which, this dictionary learning approach, is an approach that has been used in the past, the collaboration, here is the reference, in order to classify and denoise advanced lycosplipped noise transients and improve the performance of prediction. So, we try to do the same approach. So we try to do the same approach here in the case to get rid of the galactic noise and therefore to be able to denoise, in other words to identify the remaining CBCs. So the aim is to investigate how suitable is this machine learning approach to classify and reconstruct massive binary black hole mergers in the frequency of LISA. So there have been So, there have been other approaches in the past with some kind of assumptions. What we have done in our approach, we consider a big range of masses with spins, because we found the spins of computer equal to zero, varying redshift, and we have studied the waveforms at the coalescence time. So, I would like to say a few words about what this kind of approach. So, we expect that the signal is going to have two components. It's going to be a superposition. Component is going to be a superposition of the CDC signal plus the galactic confusion noise. So I'm writing down here the loss function, which has two terms. One, which is the error term, that is going to tell you how well you have fit the data. And then you have this regularization term in which you put any kind of constraints. And the idea here is to be able to find the solution that is going to minimize this error term. So here is a reset the signal. So here is, I said, the signal. So we assume that the true signal can be presented like a linear combination of atoms of a dictionary, which is going to be your matrix. And if we assume that the signal can be presented with a few columns of your dictionary, that implies that there is a sparse vector, which I denote by alpha, which is going to be given like a linear combination of the columns of your dictionary. So then, the sparsity of your vector, it is. Of your vector, it is actually this irregularization here, and therefore, to find the solution that I said before, comes down to be able to solve this constraint variational problem. Now, this is going to be much more efficient if indeed you go to train your dictionary with a set of signals. So, what we have done, we select the templates of the CBC waveforms and then we weld them to data. Then, we align the waveforms at the strain maximum and we weld. At the same maximum, and we're going to divide them into batches with the requirement that the number of batches has to be bigger than the length of each batch. And then we're going to solve this constraint variational problem. Okay? And in order to find what is the dictionary that is going to fit better, then these are, I'm not going to go into detail that is going to be the values that we took for the patch, for the length of each patch, and this hyperpanel. Patch and this hyperpanameter lambda. So then we create training signals that contain CBC waveforms only without noise. And then once we have this dictionary, we're going to test how good it is in new signals that they are going to have as well, a galactic noise. So here is the data that we have used. So the waveforms of the binary black holes that are going to be detectable from this, we took them from this. From this IM alpha nom D, which is provided by the Lisa Davatalis. Now, the dictionary is trained on a set of 100 noiseless CDC signals simulated over one day with a cuttons of two seconds. And here are the parameters that we consider for the CDC signals. Now, to simulate the laser galactic background, so the noise, this is the kind of expression which we have used. And then the white dwarf data, that And the white dwarf data that we have considered comes from that simulation. There are five million white dwarf spinaries from which simulation we have extracted the mass, the location, and the original frequency. So here you can see the overlap between the reconstructed signals and the injected signal and the reconstructed signal and the noise as a function of the SNR for SIF equal to 1 and equal to 10. Equal to 10. Okay, so the blue is redshift equal to 1, the brown is redshift equal to 10, and then you have the overlap between the reconstructed and the injected is the bullets, and the cross is the overlap between the reconstructed and the noise. So you can see that as the SNR increases, then you have, of course, that the overlap between the reconstructed and the injected goes up, whereas between the overlap and the noise goes down. Now, the noise. Now the noise overlap goes constant until around the controls of 10 and then it goes sharply down, whereas the overlap between very constructive and the ejected goes up and for a signal that is about for a redshift that is bigger but then increases it can reach even an overlap equal to one. Now we have seen that the redshift does not play a lot of importance in order to find this overlap. In order to find this overlap, you will see a difference, and this difference has to do just because there are different mass ranges of the objects that we have considered. Now, here what we did, we fixed the ratio to one for both black holes and we have created, the screening part one created day the set of 4,000 CBCC events with a uniform spacing of ratios between 1 and 20 and the mass totally between 100 and 10,000 solar volumes. Between 100 and 10,000 solar masses. So here we have two kinds of cases: the case that you have a strong galactic noise, and the case that you have a weak galactic noise. We saw the SNR, we saw the overlap between the reconstructed and the injected, and we saw as well what is the overlap difference between the reconstructed and the injected and the reconstructed and the noise. So this is this curve here that you can see. So the right of that, Can see. So, the right of that, that means that you had a good reconstruction. For the case of strong, that happens for a mass bigger than about 1,000. For the case of weak, for a mass bigger than about 300 solar masses. So, from this plot, what we find is that this approach is quite successful in the case that the mass total is bigger than about a thousand solar masses, for a redshift either bigger than about three or about seven point five, depending. Or about 7.5, depending if you are strong or the weak galactic noise that is in. And we have a quite good waveform reconstruction if the SNR is at least equal to 5. Now, one might think that by increasing the SNR, you can get better overlap, but seems to be actually that the success depends a lot on what is the total mass of the CBCs and their arrays. And play a race. Okay, that was the first. The second thing I would like to say again for Lisa is about MS. Now, this will be very important, as I said, for the case of LISA. And the problem that you have there, even if you are quite conservative and you take not as many parameters as otherwise you would have taken, you need at least 10 to the 40 templates in order to have a fully coherent search for errors. Sets for actors. So, this, of course, is a problem, and it had been approached with neural networks. Here is a number of people who have worked on that. And what we have done in our approach is to consider this dictionary learning approach that I told you before in order to study this problem for long observation type data sets, smaller than one year, with quite small curtains, about five seconds, and to see whether this dictionary length technique. Whether this dictionary learning technique is good in order to reconstruct the data that you could have from EMES. Now, in order to measure how good is our dictionary, then we calculate the matched SNR between the signal and the reconstructed waveform. And in order to see how good is the accuracy of the reconstructed waveform, we define this kind of mismatch which takes values between 0 and 1. Of course, the best one when it's 0 and 1, of course, the best one when it's equal to z. So here I saw in the complex time domains a strain for emerys. So the mu is the mass of the compact objects, dl is the luminosity distance, you have the theta phi, the smoothness of the polar angles, you have A is the amplitude, S is this function that depends on the spin, and then L M K and N as harmonic modes. So here you can see the MRI waveform, okay, at a luminosity distance of 5 gigaparsec. This is going to be seen in a singular LISA data stream after 0.1 years of observation. So what we have considered for the data, I give that here, the compact object is a spinless black hole of 10 solar masses, the initial semi-light spectrum is 16, the polar viewing angle is pi over 3. A viewing angle is pi over three, a simultane viewing angle pi over four, the initial phases equal to zero. And then we create 200 one-year waveforms with a cuttage of five seconds, and the mass of black holes is between 10 to the 5.4, 10 to the 7 solar masses, and this kind of eccentricity. Then we use the LISER response factor to the projected MS waveform, and then we use again for the And then we use again for the noise the PhD that is given from LISA in order to find the Gaussian noise on this projected emeris. And then we will wait on the data. Now, here I can show you the results. First of all, if you have a massive black hole, meaning that the mass of black hole is bigger than 10 to the 6th solar mass, then we are unable to have a kind of etosolic construction because what's happening is that the noise is going to dominate the injection. Is going to dominate the injection. If it were below this kind of threshold, then we have a full year MS reconstruction within two minutes, some of them with a file which is smaller than 10 to the minus 3 per year, with 1.16 day time windows and a mismatch as much as 0.06. Here I give you the data. This is the brown one. The embrace is the brown one, the reconstruction is the green one for two cases which are different eccentricity, 0 and 0.7, for a mass of 10 to the 5.4 solar masses and the mass of the compact companion object to be 10 solar masses at a distance of 0.013 giga years. So if you combine this result with the fact that actually we need quite a few Quite a few short-time training waveforms. That means the dictionary learning approach is quite promising. In addition, one might be able to use this kind of algorithm in order to press screen large time windows and then on the second level you can go and do a more refining study to a smaller time window. And might be that this approach may as well be used for the case of long-lasting sources. Of long-lasting sources as continuous waves. So it seems to be quite kind of promising. Now, the last part, which is about LPK, is again to find the kind of fast pre-screening of the data in order to see whether there is hidden some kind of gravitational wave signal. The most pipelines which are. The most pipelines which are used are the ones which are based on the match filtering technique, and even though it is very promising, it takes quite a lot of time. So the idea is whether we can find a faster kind of way. As the volume of the data we expect that is going to increase a lot. By that I do not mean that you have, at least with the approach that we have done so far, that you have to just replace what are the techniques that are What are the the techniques that I use now? But what I will say, because we are not able to do, for instance, parameter estimations of paramoder, but what I mean you do a first kind of pre-screening and then you pass this test, which is very quick, as you will see, then one can go to the more traditional kind of estimates. So the approach which have used is to have simulated data injected into their plus LIGO Hanford. So so far the study we have is one detector, so we have to extend it more than one. Have to extend it more than one, so so far is one detector, and then we just consider real data from the third LBK observatory. So, what are the black hole population that we have used? So, we construct an astrophysical population of binary black holes with mass between five and forty-five solar masses, the metallicity between one and one hundred the solar metallicity, and formation ratio between zero and eight for one year of observation. For one year of observation. And then, in order to investigate how good our approach is, we study the performance of this approach with a flat prior Y, because in that case we can extend this mass rate that we had before, that was between 5 and 45 solar masses. Here it goes between 5 and 80 solar masses, and the rate goes between 0 and 8, and the mass ratio is between 1 and 5. So, we have checked my. We have checked manually one hundred and seventy one dictionaries in order to use to see which is the best one, in other words the best of the life. We are not trying to find what is the best dictionary. That's another kind of project which is beyond the scope of that. We try to find what is a dictionary which is good enough in order to capture all three kinds of stages: inspiral, emergent, and ringed up. Down because it might, not it might be. In fact, there is different dictionaries that could be the best one for each of the three kinds of stages. So, in order to capture all three stages, this is the best part of dictionary from the 171 dictionaries that we have managed to test. And here I can see you, I can show you what is the reconstruction, which is the green one, and the true is the yellowish one. You can see it is a very You can see that it is a very good reconstruction in the case that we have a kind of signal-to-noise ratio which is pretty high, it's about 75. And the overlap here that I defined before, it is about 0.57. We take that if the overlap is at least a half, then it is pretty good. So, here is the kind of the detection pipeline which is a schematic which we saw. So, we have two cases: the flat. So we have two cases: the flat prior that I told you before, and the astrophysical prior. The one comes from the simulation, the other one is just an extender to test the data. Now, we generate the data, and then we have to put some noise, and we put a Gaussian noise of the detector. Of course, that's something that has to be extended later on. And then we widen the data, we reconstruct with a dictionary the data, and then we go to study what is the overlap in order to appreciate how good To appreciate how good this kind of algorithm works. So, what I'm showing here is first of all the results for the BBH detection with the simulated data and the A-plus sensitivity. So, you can see the flat prior is the blue, the astrophysical line is this orange, and then you have the noise distribution which is down here. So, you can see that the injections, as I said, is only for the light of Hanford. If the SNR is the If the SNR is greater than about 15, then what we find is that it is quite good confidence in the detection of this algorithm. And we have an almost perfect reconstruction with an overlap that goes to about 0.95 if we have the SNR to be bigger than 100. Now, what about the results regarding the simulated Simulated A plus LIGO Hanford sensitivity. So, what I saw here, I saw the overlap distributions in the case of the flat prior population and the case of the astrophysical population. The black one corresponds to the noise, and all the other ones correspond to a different value of the signal-to-noise analysis. And what we see here is the definition of the overlap, here is the position of the signal. Here is the radio of the signal-to-noise ratio. And what you can see is that, as a matter of fact, there is not a big difference between the two. We see that almost all ejections with an SNR greater than 30, so if you take the red kind of graph, they have overlaps with outside the noise, which is the black one. There is a difference in this red curve. In this red curve, as you go to the right. Why? Because, in the case of the flat cryo-population, the binary black holes have a uniform distribution and here the same SNI, whereas if you go to the astrophysical one, as you increase it, that means the reds goes higher, then the number of banded black holes goes down. So this is the results for the simulated A-plus sensitivity. 88 plus sensitivity. And then, what about the detection of the O3B binary black holes signals? So, we apply this algorithm to the 35 confidence detection from the O3B LVK, and we reconstruct the waveform, the reconstructed waveform from the data, and then we calculate the overlap. So what we have found that's a pretty good reconstruction performance. Good reconstruction performance, and it is quite better if we just exclude the glitches. As a matter of fact, we found that the overlap between the signal and the reconstructed waveform is bigger than 0.5, which I said is a signal that's pretty good, and a file which is smaller than 1 per map. This approach seems also to be quite a fast one. So, to tell you what I mean by fast one, mean by fast one, the Dixon Learning approach with 150 training signals from the black hole varied black hole population, it took at most 454 seconds to be created and the reconstructions of all 35 events took less than about 25 seconds. So it's pretty easy. So what are the next steps that one should do here? First of all, the immediate one. First of all, the immediate one is to extend this approach in order to consider more detectors. And that is not trivial because the mathematical explanation that I gave you at first has to be generalized. So it's not a straightforward thing. So that's the first thing to be done. Then, to separate over CBC signals in order to be able to To be able to extract, to subtract weaker signals that could be, let's say, of a terminological object. Now, to put additional noise sources, because so far the only one we took was a Gaussian noise, in this case. And when all these steps have been done, then we have to produce the corresponding pipeline and to compare with what is happening now with more traditional kinds of methods to see how successful it is or not. Successful with this area. Thank you very much. Questions? Yeah, so I'm not, thank you very much for the talk. It's very interesting. I'm not too familiar with dictionary learning. I only like very basic stuff. I just saw there to train one of these dictionaries at a folks it's a Dictionaries and performance. It's 150 training signals that were used. Is that right? No, it's the last one. We tried 100 second-line dictionaries. That means that you change the values of the parameters, by these hyperparameters, and we choose the parameters for which we have the best performance. Now, it seems to be that if you wanted, as I said, to have what is the best performance for the inspiral, might be not the same dictionary and what's the best format for the And what's the best format for the merger, for instance? And the same thing, it seems to be that there are dictionaries which are even better for a given mass range. But you cannot go to say to somebody, look, take this dictionary if the mass is that is the available. That's why we take the one which available is optimal. Now, what is the best dictionary? I don't know. This is a study which we have. And like, I guess I was just one more short question. Just since it's like basically these sparse atoms, or you know, the sparse format, does that make it insensitive to the background noise that you put it on? No, it does, no, no, no. So it's not sensitive to the cycle. Right, okay. What's up? Any other questions? No, I have a question. The Emory work that you're doing, I remember the stuff I I remember the stuff that I was involved in and you you mentioned it, it was SMR fifty we ended up being sensitive to. It was a very hard problem, such a big parameter space. You mentioned a false alarm rate. Do you know how that equates to an SNR for comparison? Can I think about that? I can find out if I want the comparison. No more questions? Then thanks, Mary again. I leave it here for probably the next one to do. I stop, I still don't have enough. I can't put him talker. Bad for me.