Special for the other. You guys should talk about stochastic optimal control and transfer the platform to manifold in drugs. Thank you. Thank you very much. And so first of all, I would like to thank all the organizers to organize a very fruitful, informative workshop, and I learned a lot. I'm also very happy that I climbed to the mountain and I'm so lucky when I was here for the first time in the mountain and I've never been able to climb up. And okay, so today I want to discuss some understanding about the transition path problem and particularly focus on those transition paths on discrete statement, that's a graph. The graph may come from some disquietization of your diffusion process on manifold. If you process on manifold, or the graph, you can just give a graph and how you will be done continuing paths on that. And so this is based on several papers, and this is for the continuous process, and this is how we use the how we describe drifted defiance process on a manifold which is unknown. And this one would be the focus of this talk. The focus of this talk about how we design an optimally controlled random work on graph so that we can use it to do the colour simulations. So first of all, for transition paths, what is transition path? The transition path is some real event. That means an event happened with very small probability. And it is considering a path starting from A path starting from one stable point to another stable point in a dynamic model. The dynamic model could be a deterministic one or so-of-that dynamic model. And the transition, so those local attractors really represent some typical stable states of your dynamic system. For instance, in the protein-folding problem, it represents one. Important, it represents one shape of the protein and B represents another shape of the protein. And so, in the simplest deterministic case, that means suppose you have an OD system that is guided by your potential, or we say you in your energy landscape. Then, the transition path problem is simply a geometric problem. So, how you find the So how you find the V-section from A to B, but in an infinity time collaborative. Because the V-section curve, the parameter of the dissection term is not important, but the path itself is important. So that means you should follow the dissection principle. In frame of all the curves that connecting from A to B and also in frame of the terminal touch. So following that, the insection path for this deterministic system would be the uphill path to steepest ascent and then the downhill path. Passing through a saddle point. Okay, as we assume at saddle point, u has Mo Z index 1. So it will pass through saddle point and go down u with the steepest descent. So that's the transition path problem. But then in our setup, we are going to consider at a finite noise level, for instance, for the continuous state. For the continuous state, configuration states, and you can consider a noise driven by a bound motion. Or in a discrete level, it may be a mark of ch. So first of all, I'm going to briefly review how we use the stochastic optimal control to describe which concentration path that solves the That solves the stochastic optimal control problem. And then I'm going to say: if you have a reversible process, which means your twist is given by a potential of some gradient of some potential function, and then we can design an optimal constraint random work. And the last part I will say: given a general Markov chain, how we do. Mark of chain, how we design an optimal control. And the simple question, so the simple answer for this is the discrete commit function, which just solves a discrete elliptic problem with two boundary values, that will give you the optimal control. Alright, for the first part, let's say for the theorem process, let's assume. Different process. Let's assume your variable space is given by continuous path space and the filtration and the given probability measure of this variable space. And then we consider a stochastic dynamic with this drift given by region u and the noise given by run. And so our goal is to how we effectively compute. How we effectively compute the transition path. That's that I mentioned, connecting from A to B at the fixed noise level. And some other related question is, if your drift diffusion is on a manifold that only suggested by data point, how you approximate it. And also if it's in for the discrete configuration state, how we do the control. The control. Okay, so for the first part, if you have a drift deferred process, then you see you have a clear separation about what is the drift term and what is the randomness term. So when we define a control term, we can just define an additional drift. So this is the velocity field. At each point, at each state, I give you an additional drift for your. Additional drift for your stochastic dynamics. And then, what kind of drift is the optimum once so that it realizes the transition from A could be almost correct? So, here we reinterpret the transition path theory using this stochastic optimal control problem. So, under the original measure P, consider a Consider a SDE that is the original one with an additional twist here. And the starting point could be a deterministic point in a Euclidean space or it could be has some randomness. And then how we realize the transition from A to B? We realize it through a terminal time, a stopping time. Stopping time. Stopping time, that is the first time for your process, your crypto-differing process, hitting on either A or B. So A, so here we'll assume A and B is some local, some closed set. It may contain only one point or it may contain a region at this point. So the first time heating on A or B is the terminal time. Is the terminal time. And we realize the transition by using this terminal cost function. That means we give you a penalty when the process, when the particle hitting on the site A, but we give it a reward zero when it hits on E. So with this penalty for the as a terminal cost, then when I was this value sound On our this value function is finite, then it always finishes one transition. And then to finish this one transition, you need to measure what's the additional cost because you add an additional drill field. So here, for the continuous one, we need to use this kinetic energy in square. So this is the stochastic optimal control formulation, and if And if we regard it, if we just take a fixed terminal time t, then the problem is easy. So if you take a fixed terminal time t here, then you can write the Fokker-Planck equation for your control process, and then using the dynamic problem, you can write the Heinrich-Jacobi equation for your value function, and then you can solve the optimal control. Control. But now we have this stalking time, which means you don't have a time-marginal representation for the expectation of the host. So to prove how to obtain an optimal control, we go to another viewpoint. So that's the optimal change or value viewpoint. So this Report. So, this three step is the Gaussanov transformation machine. You can consider it as a machine. So, starting from the original process, on the P, so which is described as under P, this process satisfies the original drift. Okay, so here we don't assume it's reversible. It's given by gradient. So, starting from So starting from the original process with an additional control. So in other words, on the P, this term is the P mingle. And then we define a new process through this X T L process. This is Z given by this formula. You don't need to remember this formula. But Gaussandov's theorem proved that this Z is Prove this Z is a positive P martingale and under P it has mean 1. Because of this property, you can use this Z to define the Lagoonigram derivative of a new measure, Tz, and through this form. So that means for any measurable set, for any Bolayer set in this filtration, Layer side in this filtration, you can define the value of p at a. Or symbolically, you can through this z to define the conditional expectation of this dynamic derivative. And the most important consequence is that the new measure of the maximum Under the new measure as defined here, the SDE becomes the original one without drift. So that means you give an additional control drift. That means you change the probability measure, and so it's so that in this new measure, you still have the original estimate. So in this machine, to find the optimal control is to find the optimal change of matter so that the running cost of the Lyman derivatives of this two channel measure is minimized. And so the Garsenov theorem for bound emotions is that, okay, the running cost, this term, the kinetic energy, This term, the kinetic energy after expectation, is exactly this relative entry. Alright, through this, we can just recast the value function as the minimum of the expectation of the terminal cost plus this relative entropy. So remember, this is two measures on the continuous path space. Space. Alright, and so this optimal change or measure viewpoint is used to prove the optimal control and what's the optimal value function. Okay, so those are some not important conditions. The Normikov condition is to ensure your PK is absolute continuous with respect to P, and to ensure the Normikov condition. To ensure the normal condition, we use a regular addition for your terminal cost, and then we'll use EDTST to take this regularization goes to the Alright, so this is our conclusion. So after those three formulations, we can prove that the commit function, so the commit function means Committer function means the function h that solves this elliptical problem based boundary cognition Q. This Q is the generator of your drift diffusion process. Ipsum La plus plus that drift term. And this commuter function has this probability representation, so that's the true bio thinking formula. And the optimal control is solvable. It is just given by It is just given by gradient gamma. Gamma is the value function. And the value function is 2 epsilon logarithm h. So that means you only need to solve this linear elliptic problem. And then you obtain an optimal control. And of course, that's the corresponding value function. And so gamma also satisfies the Haming-Jacqueli equation, the quadratic Haming-Jacqueli equation, which is not quite useful because it's now decoupled and the optimal contra is directly given by the committee function. But in the reversible case, you have an effective potential. That means in the reversible case, the optimal control is just the Control is just given by gradient of log H, which means you have an additional drift gradient of log H. So the effective potential is just this term. And the commit function is an important quantity that is used in the transition path theorem proposed by those. Proposed by those guys and then formulated by Kerwinang and Van De Adams. So this is an illustration about the original potential. You do the transition path A to B, which is a rare event. And it's very hard to simulate. But with optimal control, that means you just solve the elliptic. elliptic equation and then you obtain an effect effective potential and then you can use your optimally controlled data diffusion process to simulate the real event which becomes almost real event. Okay so that's the general introduction for the continuous state. And then let's consider the drift deferrer process on a manifold. Inferno process on a manifold, but the manifold is only suggested by point clocks. And how we design a good numerical scheme and how we design an optimally controlled random work on the resulting graph. So, first of all, let me introduce the way we The way we are going to do the disquietization is through the final volume method. But before that, we need a good tessellation and we use the boundary tessellation. And so to do the boundary tessellation, you only need to know the metric of your manifold. So suppose now you have a manifold app, which is and you have collected a well-distributed data point on that manufacturer. Data point on that metric for you. And let's first assume you know the metric and then we approximate that. So suppose you know the metric of your manifold, then you can do the ball intersection. And the important property of that boundary is that suppose you have this bonus, and the bonus cell is denoted as But the cell is denoted as CI, and the cell center is those data point. And the important property of that is the interface, the interface gamma IG between cell CI and CG, so that's gamma Ig, is perpendicular by sector of XI, XG. X pi scheme. And so then we can use this property to design the new Marco scheme, or we can call it a mean scheme or Banana volume scheme. So here in this part, we only consider the reversible case, so that we can recast the Foucault-Planck equation as this relative micro before. Which means in terms of Which means in terms of rho over pi, in terms of this quantity, in terms of this function variable rho over pi, you have maximum principle. Then by the finite volume method, this partial t row is given by divergence, which is approximated by the summation of all the blocks from I. From I and times pi times the normal gradient of rho or pi. And then you obtain this finite volume scheme and we denote the right-hand side at this q, as this q star stroke. And the q would be the generator of the resulting function. And the generators And the generator satisfies the maximum principle. And recast it in the graph calculus, we can define the graph gradient as the difference of two values at G minus I divided by the distance between that. And okay, so here the distance, the volume, and the bonus face area will be. This area will be approximating the later. Okay, we just write it here. When we define the graph gradient and graph divergence. And the graph divergence means at I, you compute all the flux that the flux into I, for instance, into I through this phase gamma IG and you sum it. And that's our steam can be required. And that's our scheme, it can be recast as this one. And so, this with this notation, you can say it is a point-wise approximation for the photon equation in the personal case. Alright, so as I mentioned, you have some quantities, so Ci, gamma, and the metric, and the And the metric and the distance to be approximated. And so, here we are to approximate the born in volume, the cell volume, and the face area. The idea is just to draw a large ball and you use this large ball to give an approximated tangent plan at xi, and then you And then you project all the data points in a smaller ball to that tangent length and do the bottom calculation of that tangent length, which is a nucleus. So if you believe me, then we can approximate the distance, the the cell volume and the phase area. And here R is the typical diameter of your body cell. Of your body cell. And of course, we need to make some assumptions that the data point is well distributed and diameter of one cell. So each one cell, they are comparable. So we can have an upper bound or bound. And then we can get our estimate of our new boundary scheme, which says that the approximated The approximated density at each point xi is close to the original density that computed from Foucault-Planck equation on the benefit. Alright, so here is just the notation to recast. So remember all the right hand side is Q star, which is the matrix. Q star, which is the matrix acting on zhou. So here we just recast it as the right-hand side as the probability flux, which means when you want to calculate the little change of probability at i, you just need to compute how many flags in from the nearest label at J. And those is computed from the Computed from the product of transition probability from k to i and the times the gunfreed lambda j and the minus all the things that flows out. So that's density rho i and flows out by the concrete lambda i and the times p but the p is lambda one. So that's P01, so that's a request of our new markup scheme. And you can easily track it, still satisfies the data balance property in the environment valid part. Alright, then how we control this Markov chain? So regard this as the law for the equation of your original Markov chain. Of your original mark of chin. This mark of chin is one approximation for your crypto determining process. Well, how you control this mark of chain so that the transition path problem becomes an almost really light. Because we are working on the reversible case, so here we can directly borrow the optimal control that we obtained from That we obtained from the continuous case. So remember here, h is still the commit function solved from the elliptic equation of these two boundary conditions. And because the boundary is 0 and 1, so it's always positive. And recall that the effective potential and the effective equilibrium is given by pi times h squared. Each script. So then the controlled mark of chain is simply recast as this one by replacing pi as this h square pi and this pi g and pi i and also this is replaced by h i and h j. And so the resulting control generator Q H Generator QH is just the dube transformation of the original generator Q. So the dube transformation means, duber H transformation means Qij H equals Qij times Hj avoid charm. And to ensure it's a probability matrix, the diagonal one is defined as. One as a defined chain. Alright, so this gives you a controlled Markov chain that can realize the transition path, but I have not proved whether this H is the optimal one in some stochastic optimal contents. And there are indeed, so for discrete configuration, For discrete configuration state, for markup chain, there are indeed lots of algorithms to simulate transition paths, but they never answer the question whether that algorithm is the optimal one. So we want to prove that indeed this Duber transformation and with H solved by this discrete commuter function, so that's just a discrete elliptic problem, is the optimal control. Computer. So that's our part three. Forget about those forget about those equilibrium, those effective equilibrium. In the most general case, you are given a mark of trend. Let's assume it's a finite mark of trend and your graph is connected. So there always exists an environment barrier, but this may not have the data balance component. The data balance condition. Then, how we control the generator, so how we introduce a control, that's a question. As I said, in the Drupal diffuring case, we just add additional drift to push all the things to the mean. But now for the mark of chin, you don't have a separation for the drift term and your noise term. So how you add this term to the property. You add this control process. And another thing is in the infinity time horizon, how you expand the optimal control. Okay? So here, we first introduce a control velocity. So this is the most general case. At the site I, you give the control velocity at each edge from i to j and then you turn your And then you turn your original generator to IG by this control variable V IG. And that means you are controlling the transition rate of your RG. And then we introduce, so let's first say the financial parameter. Then we introduce this running cost given by Running cost given by the entropy of your control variable times QIG. And later I'm going to verify it using the Gaussian of transformation. And you will see that this term is exactly to measure the cost of changing measure in the path space. And the stochastic differential equation. The stochastic differential equation is the common forward equation for your control mark of QN is given by Q P dot equals Q theoda times P. Q theoda star times P. And so here V times Q is the control generator. So under this control you minimize the running You minimize the running cost and atomic cost. And at a finite time horizon, that's the optimal formulation at finite time horizon. And remember that at finite time horizon, you can always convert this to a PD constraint optimization problem. And this is the Fokker Planck equation with the initial data. And the you have a backward hammer Jacobi equation for your value function. Copy equation for your value function. And we can prove that the optimal control in this case is given by this Dubert type transformation. So the optimal control V at the beginning is a very general drift term from I to J, but the optimal one is just given by the quotient of h at j. quotient of h at j over h at i. And h is solved by this linear backward equation with the constant. And so for finite time horizon, we just need to solve the PDE, the forward Fokker-Planck equation, and this linear backward equation with the terminal cost. Okay, so forward backwards coupled or not coupled together. Together. You can further solve H and then solve. And so this one is at Van Anton Horizon. This is quite close to Professor Joe's talk. And when you have additional coupling of your individual population in your running cost, then this Hammer-Dracovi equation is coupled with the Voka-Park equation. Alright, then I want to explain why I choose this running cost through the Gaussianov transformation for pure jump process. And go back to our transition path problem because we need the stopping time here. So there is no time marginal representation for this term. Okay, so we proved that in the discrete setting, the Gaussianov transform for the pure jump is given by this martingale. And this martingale is defined by the control downgrade minus the original downprint minus the logarithm of H is your commuter function. And we proved it is a P-mading gal. It is a P-madinger. So P is the original measure, which under P, your process, your control process XH has the generator QH. So remember QH is Q QRG. So under P, the controlled process has a 0. Controlled process has the generator QH. Well, by the Gaussian transformation, we can again define the probability measure on the path space. Now the path space is catalog space, which means discrete paths. And the doning group derivative for this two measure is restricted. So the conditional probability, so this is the short-handed notation for the conditional probability. Conditional expectation is given by the state P. So under the new measure we denote it as pH, the original process has the generator Q. So that's the whole point, the key point of Gaussian of transformation. So you have sample pass and under the original measure T, you look at 8. You look at it, it has the generator QH. But the control is just equivalent to you look at those paths with a different measure. Then with this different measure, it goes back to the original gamma. And then the cost of this change of measure is evaluated by this relative. By this relative entropy. And for finite time horizon, the time marginal representation for this expectation is just the entropy of your control times QIG. So this is consistent with last page of my slides. But of course, with the starting term, you do not have this representation, but you can direct Representation, but you can directly use this representation as a Latin group derivative. Alright, so with all those calculations, the stochastic optimal control formulation in terms of its H, so now we restrict ourselves for, so originally we add and control. We add and control V and V, but now we restrict ourselves to a smaller class given by the quotient of HJ over HR. And then we just need to minimize the running cost given by this Lagrangian green derivative and the terminal cost. So the terminal cost is still a panelized one which Uh analyze the one which analyzes the path uh 18 on A. So now I can give our scheme our theorem. Assume your XH is a mark of chain which under P has the generator QH. So under the original measure it has the generator Q. The original measure, it has the generator QH. And suppose it's starting from one point because our expectation is linear in P, so it's not hard if you starting from a measure, if you're starting from, if you X theorem has a lot of P theorem. So, here for simplicity, we just take the starting. For simplicity, we just take the starting point as I, which is in the open set, so starting from the open set except A and B. And then define the sorting term is still the first heating term on the set A and B and F is the terminal cost uh which is uh uh infinity but regularized as as a maximum rhythm. As a maximum rhythm data. So that's a penalty at A and minus the running cost given by the Ladonic derivative we just computed. Then we proved that the optimal control is exactly given by the discrete commit function, which is very simple. It solves this discrete indicative problem. Discrete evictive problem with the boundary condition at A is delta, as V is y. And remember, this regularization is to make sure h is always positive. Alright, and the corresponding value function is given by minus h star. And as a consequence, we can say the optimally controlled jump rate does not change. So the control jump rate The control jump rate lambda h star equals lambda i. So the thing that we are controlling is indeed the transition probability Pij. So what's the probability, transition probability from i to j? But the waiting time, the average waiting time is not changed. Alright, so in the end with this QA, Q star, you have the optimally confirmed. You have the optimally confirmed mark of chin, and with that optimally confirmed mark of chin, you can do whatever you want. You do the Takao simulation and go find your transition paths. So here is a quick remark that this stochastic optimal control formulation is also equivalent to this optimal change of measure, which says that given the Which say that given the original probability measure P, you want to find another probability measure on the past base such that your process under your new measure still has the generative Q. But this change of measure can realize this, penalize the technical cost, and you need to minimize the cost. So here is a simulation using the optimally control the random work and so here is the simulation that supposed we already know the equate the original equilibrium so that's the reversible case and here we say the transition pass through one stable state of To one stable state of this type and to another stable state. And numerically, we also say that as epsilon goes to zero, so your noise level goes to zero, this path converges to the zero noise path. So that's the front of Windows most probable path computed from the light. Computed from the large derivation principle. Okay, so as a final conclusion, we first recast the transition path theory, recast the transition path problem as a stochastic optimal control problem in infinity time horizon. So I want to emphasize again, so as you lots of you will have the optimal transport problem. Or as the optimal transport or the infield gain problem. So for the optimal transport, you have the initial and the target value. And at finite time, fixed time, you want to do the transition from transform from 0 to row 1. And so that is equivalent to in the mean field game, you give a hard terminal cost. But in other And but this in our case it is different. It is in a an infinite type horizon, which means we don't care about the target value, we just want it to consume that mean. And another point is finding the optimal control is equivalent to finding the optimal change of value. And this equivalence is particularly important for infinitarian. For infinitum providers with the solving time most problem. And another one is for the discrete setting, the quadratic running cost is not so good in terms of you cannot have a solvable optimal control problem. So, well, by the Bersernov transform for the discrete setting, the entropy of your control. Entropy of your control should be the runoff cost. And the last one is the optimal solution is always simply given by either continuous or discrete commit function. So which is the linear elliptic problem. Alright, that's all. Thank you. Is there any question? So what is the dimension typically that you're imagining for this manifold? For you solving it? Oh, well, I approximate this manifold. I mean, just to get an idea of, you know, like, so your discrete is a manifold dimension. Yeah, the theoretical result is for any dimension. But I mean, if if you had to I mean, do you have small cohesion in mind? I mean, do you have some application in mind where you say, okay, roughly the dimension in that problem would be what order is? Right, right. So in application, we first apply the firm map to get a reasonable lower dimension, a manifold embedding in reasonable lower dimension. And then, I mean, so the fact that you can reduce to the problem to these commuter functions, so then, I mean, essentially, any graph. I mean essentially any graph for which you can, essentially the solution to that equation, the commiter equation there, any graph that would consistently approximate the solution to the associated commuter equation in the continuum work, right? In the sense that it would give you the same solution. It would be a consistent discretization of your problem. Yes, yes. So yeah, so then I suppose you could use other graphs for which you don't. Other graphs for which you don't have to compute, you know, like these boundaries and things like this. No, and a graph. Right, okay. So, even epsilon graphs, kinears, linear graphs, those things would work. Like that you just connect points if they're within distance epsilon. Then you don't have to compute, say, the size of the interaction between points, as in key. Like for example the future, in principle you would have to compute the size of the boundary. Kind of like the size of the boundary and things like this. But I mean, I presume that for epsilon graphers, you just care about the distance, you know, things are connected within distance epsilon, you connect, and then you rescale. Or K-Cars neighbor graph, you just connect. That should work, right? Yeah. One short question? A short question. Can you go back to the diapet type? Yeah, please. Yeah, please. My question relates to temperature, what you call epsilon. Okay. Your epsilon is very small here. So here for this one, it's quite high the drifted deteriorum process. So there is a temperature here, epsilon is 1 moment. But in the front of your delta U, Delta U, there is a friction term that depends on temperature. Yeah, you can make it comparable. You make it comparable. Yeah. You can use the fluctuation dissipation relation to make it comparable. But still, you don't have many transition paths here. I mean, I would expect to be. Oh, oh, so here is a mean path that computed from Mental Call's equation. How do you compute the mean? How do you compute the mean? That's a good point. So, we also have an algorithm to compute the mean path, which should take into account the different waiting times at each point. And do you, with your transition path, do you verify the Primus condition? When do you get the transition? Meaning, this is a rare event. Meaning that in the data. In the dynamics, the system is going to spend a lot of time in A. Exactly. And then at some point, it's going to cross the barrier. And you can compute the time, a mean time, for the transition. And that mean time should be related to the difference in energy between A and C with an exponential term. So do you verify that on your transition path? Uh so so so theoretically you uh we compute the optimal control and under that optimal control you have a optimal value function which gives you the variance from A to C for instance. And while the average transition time, right? The average transition time could also be estimated by the computer function, but that's not our contribution time. But I would be really curious. But I would be really curious to see in this animine dipeptide case if your average transition time is known experimentally. So I would be interested to compare it. Yeah, right, right. That's very important. The question was: how do you identify the SEC A and B in practice? Like, what information do you need to know? Yeah, that's a good question. Yeah, that's a good question. So for the continuous case, we suppose you either know B, then you use the critical point of B, or you either know U. So for the discrete case, given any for financial state, there always exists a pi. And for discrete state, you just compare if at this state i, other state i. i, other state are larger than. So this pi i means smaller than pi j for at the nearest point, then we regard it as a local mean. Okay, let's turn news again. Okay, um our next speaker is Carlos of New York Lean. He will talk about decorated display component and complex or digital cups. And conference called Individual Who Cups. Thank you, Wayne, and thank you for having me here. It's been a nice conference of ours and in a beautiful environment. So I will talk today about work together with Alexander Lubenko and it's about a generalization of discrete conformal equivalence. Hank gave us a nice overview of this theory on Monday. And this generalization And this generalization relies on an additional structure of the polyhedral surfaces we call decoration. But before I go into the details of the generalization, let me review a little bit the theory we are trying to generalize. And I will begin with a little bit of history. So to my knowledge, this whole discrete conformal equivalence theory began with the work of Thursday. With the work of Thus, and he used circle patterns at infinity of hyperbolic space to describe polyhedra and understand polyhedra in this hyperbolic space. And in doing so, he connected the work of Kobe for circle patterns to the work of Andreev on compact hyperbolic polyhedra, and this accumulated in the This accumulated in the now known Kubernetes-Thursten theorem for circle packings. Subsequently, these two fields flourished and there were a lot of theory on circle patterns and hackings. I'm just stating two of these. The Roland-Sullivan theorem on the approximation capabilities of these packings. Yan Wen told us about these on Thursday, and also. On Thursday, and also there were generalizations to allow a little bit more freedom. I named these inversive distance circle patterns by Bauer and Stevenson. And also Jan Wen talked about these and how to generalize this rodent-solvent theorem at this setup. Then there is also this theory about hyperbolic polyhedra. The main question here is to describe hyperbolic polyhedra by intrinsic properties. One of them are dihedral angles. One of them are dihedral angles, and the other is the metrics induced on the boundary of these polyhedra. For us, the second case is particularly interesting. And there was a whole program trying to characterize these polyhedra, and it ended in 2008 with the last cases being solved by the Laske. Now we can actually characterize these polyhedra. In parallel, in the early 2000s, a theory. A theory called discrete conformal equivalence emerged, and in the beginning it wasn't quite clear how it related to this previous theory. So these were from the beginning quite interrelated, but this wasn't. And one goal of this talk is to convince you that this actually has a rightful place on this list. It actually belongs to you. So what is the setup? We are given a topological surface and a finite number of Surface and a finite number of points on the surface, and a triangulation. And then a discrete metric is an assignment of positive numbers such that they satisfy the triangle inequalities. Having these positive numbers, we can construct Euclidean triangles, and using these triangulations, we can glue them isometrically edge to edge, actually giving us a piecewise Euclidean metric on this surface. And for these discrete metrics, And for these discrete metrics, there's a very innocent-looking definition of discrete conformal.