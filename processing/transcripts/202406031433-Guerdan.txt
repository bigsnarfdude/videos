Today, I'm excited to tell you about a framework for human algorithm performance comparisons under incomplete information. And if you're here today, you might know that there are a range of different factors that can threaten the efficacy of data-driven algorithms and algorithmic decision support tools. Some of the issues that we've talked about today are things like target construct mismatch, when the labels that model targets imperfectly reflect the goals more broadly. Also, issues like treatment effects, when prior decisions made under some Decisions made under some status quo policy impact the outcomes that you observe, as well as unmeasured confounding, in which you have some unobserved side information that can also be impacting prior policy decisions. However, unlike really obvious structural failures, such as the collapse of the Fernhall bridge in Pittsburgh, a lot of these challenges are very difficult and very challenging to diagnose and appropriately characterize. Yet, assessing the impacts of these challenges, specifically when we're thinking about Specifically, when we're thinking about which decision-making policy we should adopt, it is increasingly critical. And if you look at recently proposed legislation, such as Section 4 of the Algorithmic Accountability Act that was proposed last year, you'll see language that calls for explicit comparisons against status quo decision-making policies, and in particular, looking at specific types of harms, shortcomings, and failure cases, and assessing what impact on various policies that we might be considered deploying. However, it's really difficult to appropriately characterize the impacts of these challenges. Appropriately characterize the impacts of these challenges. And therefore, we develop a framework for human algorithm policy comparisons under different sources of incomplete information. That I'll describe the specific sources here in a second. But under our framework, you start with a status quo decision-making policy. This could constitute human-only decisions, human-algorithm decisions, or generally any existing decision-making policy that can be summarized by observational data. And then we have some proposed alternative policies, such as algorithmic-only decisions or some other alternative that we might be considering. Other alternative that we might be considering. And then we return bounds on policy performance differences under a couple of different uncertainty sources. One of which is unobserved confounding, like we've talked about earlier today. A second of which, through a separate work, is through outcome measurement error. And then also when we have some uncertainty about the relative cost of false positives and false negatives. And for those in the audience who aren't familiar, generally how these frameworks work through partial identification is we have the regret or the difference in performance across policies that we can plot. And if we have an uninformative regret enter, And if we have an uninformative regret interval that is subject to some uncertainty like confounding, then it's very difficult to make a conclusive assessment of whether policy A or B is better. But if we can get a more informative regret interval, something that more precisely characterizes uncertainty, then if we can exclude zero, then we can have more confidence that there might be some difference across policies. And our framework builds on some existing partial identification frameworks designed for measured confounding outcome measurement frameworks. And in the time remaining, I'll describe a few concrete advantages. Describe a few concrete advances that we make and some applications. But first, we develop an approach for characterizing and reducing confounding-related uncertainty in policy comparisons. And what we can see is that if we just care about the difference in performance across two decision-making policies, then we can actually get rid of some of the uncertainty in the cases in which the two policies agree. So if we just care about disagreeing cases, we can actually eliminate some uncertainty and get a more informative comparison on policies. So what this figure illustrates here is the regret, in this case, the true positive rate. Case than the true positive rate. On the y-axis and on the x-axis is a parameterization of confounding that I can talk more about during the poster session. But the key takeaway is that our delta regret interval certifies policy performance differences up to a larger magnitude of confounding than in the existing baseline approach based on this uncertainty cancellation methodology. Moving on to thinking about outcome measurement error, we can also empirically characterize the impact of outcome measurement error on the qualitative conclusion of our policy comparison. So we develop a pretty simple error model that looks at the difference in policies as a function of outcome measurement error. As a function of outcome measurement error. And while I don't have time to go into details, our framework can basically certify the largest magnitude of outcome measurement error that's permissible before the policy comparison becomes inconclusive. And what's kind of important about this, this is still work that's under reviews, yet it's still in process, but basically the amount of measurement error required to basically draw a policy comparison in question is actually quite small in comparison to some prior estimates of the prevalence of outcome measurement error in empirical studies from criminologists. And empirical studies from phenomenologists in different areas, which indicates that we should be thinking about measurement error quite carefully when making comparative statements about decision policies. So, yes, that's the overview of the work. I'd be happy to chat during the poster session. And thanks again to all my collaborators and wonderful advisors. And yeah, I'll hand it off to the next presenter. One quick question, and let's see just a condition of that. What do you define as regret? What is regret cause? Yeah, so I was quite vague in this. Specifically, it's a. Yeah, I'm sorry, it was difficult in the five minutes to go into detail about it. So regret in this case is a difference in policy performance as measured by either predicted performance measure or some measure of utility, like the expected utility. There's just a difference across policies.