Okay, so today I'm going to talk about how we combine deep learning with a hierarchical statistical model to perform trajectory analysis for single-cell RNA sequencing data. So the trajectory inference is a novel subfield which comes along with the development of single-cell RNA sequencing and has nowadays become a common computational tool when analyzing single-cell. Computational tool when analyzing single-cell RNA sequencing data. And as based on an earlier paper, it has been counted that about half or more than half of nowadays published single-cell RNA-seq studies investigate some pseudo-time of the cells or trajectories. And there has also been a lot of methods developed for trajectory inference, maybe more than the number of methods developed for clustering or cell type annotation for. Cell type annotation for single cell RN sequencing data. And in general, trajectory inference aims to understand the dynamics and cell fate decision in biological process. Well, the cells are experiencing continuous changes, such as the process of differentiation, immune response, or cancer expansion. And the And the typical procedures in a trajectory inference method is that it first tries to infer or assume a type of underlying trajectory structure, such as a linear, a circle, or a tree that can best reflect the dynamics in the underlying biological process. And then it will computationally project and order the cells along the trajectory. And this all Trajectory. And these orders of the cells on the trajectory are also called pseudo-times, which represent the cells' developmental progress along this continuous biological process. So you may wonder, since there are so many trajectory inference methods already, why do we want to develop a new method for trajectory inference? And the first reason is that actually many of these trajectory inference methods are not flexible to Are not flexible to allow any type of trajectory structures. Some methods may only work with a linear trajectory or some other methods. They do not allow any disconnected states in the trajectory. And also, most of the most popular and flexible methods do not have explicit statistical models, while the benefits of a statistical model is that it better Is that it can better incorporate the randomness in our data and may also allow some uncertainty quantification of our estimates so that we have some idea of how trustable our estimates say our estimates of the pseudo-types of each cell. And another main purpose here is that we want to efficiently align trajectories from multiple single-cell RNA sequencing data sets. When you have Data sets. When you have multiple data sets, a common practice is that you may first do data integration and then you can perform trajectory inference on the integrated data. But it's possible that when you do that, you may not have a clean trajectory structure. And we can have a clean, as I will see later, we can have a much cleaner trajectory if we simultaneously. If we simultaneously learn the shared trajectory and do data integration at the same time. And so we develop a method based on the kerning called VIT, which stands for variational inference for trajectory by autoencoders. So before I go into the details, let me first show you an example. Here we have two data sets, both collect cells from the mouse neural core. From the mouse neural cortex. And the difference is that data set A, they only collect cells from the even embryonic days and data set B only collects cells from the odds embryonic days. And so the question here is that can we combine these two data sets so that we can have cells from every embryonic day and then can have a more complete picture of. For a more complete picture of the trajectory of the whole process. And here is a cartoon showing the differentiation process inside the mouse neural cortex and the generation of the neurocortical projection neurons. You can see that there are indeed different things going on at different embrowning days. So if we only have cells from the even or odd states, we may not. Or all states, we may not have a continuous trajectory from our data. And of course, with the single-cell RNA sequencing data, we aim to get a more refined and detailed understanding of this whole procedure than what is shown in the catalog. So, as you can imagine, because these two data sets, they are from two different labs. Two different lefts. So, if you just concatenate the cells and the cells won't merge with each other automatically. But if you use a vit, then you can see that the cells are much where much better integrated. And as I said earlier, we are not just an integration method. At the same time, we can learn a shared trajectory among these two data sets. Among these two data sets. And at the same time, we do data integration, or in other words, we can correct for confounding effects such as data source or cell cycle. And here, the trajectory is represented by these arrows. They reflect the direction of the developmental process. And also, we have different line widths for each of the edge. And the line strength represents the score of an edge. The score of an edge showing how confident we are that there is really a transition between the two connecting states. So now let me talk about more details of our approach. So we start with a definition of the trajectory backbone by a graph where, and we first assume a complete graph where A complete graph where each vertex denotes a distinct cell state or a cell type, and each edge denotes a possible transition between two states or two cell types. And then we describe the position of each cell on the graph. So this position, so each cell can be either on the vertex if it belongs to a particular cell states or cell type, or it Or cell type, or it can be on edge if it is experiencing transitions between two cell states. And then we define the trajectory backbone as a subgraph of this complete graph G. And we only include the edges in the trajectory backbone if there are any cells in our observed data that are on the edge. That are on the edge. And because we determine which edge are included or not based on our data, so we have the flexibility to allow any types of the trajectory structure, either it's linear or a cycle or a disconnected graph. All of these structures are allowed based on this description of the trajectory. Description of the trajectory backbone as a graph. And then the next step is we want to link this graph with the observed single-cell RNA sequencing data. So the idea is that we assume that there is a relatively low-dimensional latent space that can reflect the cell population structure so that we can learn the trajectory from this latent space. And then there is a non-linear mapping from this. Non-linear mapping from this low-dimensional latent space to our high-dimensional observed count data. And specifically, we assume that these latent variables, they follow a Gaussian distribution for each cell and the mean positions of each cell depend on its relative position on the trajectory backbone or the graph that we defined in the previous page. And the U here represents. And u here represents the unknown positions of each vertex on the latent space. And then we assume that there is then the observed data is assumed to, because it counts, it's assumed to follow an active binomial distribution. And we also have a version, and this is for the UMI counts. And we also have a version with zero inflating active binomial for the number. Inflating active binomial for the non-GMI counts. And here we assume there is a non-linear mapping from the latent space C that has a trajectory structure and follows a Gaussian distribution to our high-dimensional observed data. And this nonlinear mapping can depend on both its position, the cell's position on the data space, and also the cell-specific confounding covariance, like the batch indicator or some other. batch indicator or some other continuous confounding variables like the cell cycle scores that we may want to remove and uh and finally we in order to make this uh uh this model complete uh because uh we we also assume a mixture prior on uh each cell's relative position on the graph because we don't want to have too many unknown parameters and specifically And specifically, we assume that each cell, it can first randomly choose following a multinomial distribution whether it wants to stay on an edge or a vertex and which edge or vertex it wants to stay. And then if it chooses to stay on an edge, it can uniformly choose a relative position on the edge as a prime. Okay, so that's the Okay, so that's the structure of our framework. And we also model, and we model in order to have maximum flexibility, we use a neural network to model the nonlinear mapping from our latent space to the observed data. So this is how, based on this framework, this is how our variational autoencoder looks like. So we have Look like. So we have a decoder which reflects the nonlinear mapping from the latent space to the observed data. And we have an encoder to approximate the posterior distributions of our latent space based on the observed data. And as in the standard variation auto-encoder, we use Gaussian distributions to approximate the posterior distributions of Posterior distributions of these latent variables of each cell. And then we have our loss function, which is composed of three different parts. So the first part is the recon, so it's based on the lower bound of negative of a likelihood. So the first part is the reconstruction loss. The reconstruction loss, which reflects how well we can use these latent variables to represent our observed high-dimensional data. And the second part is the KL divergence between our prior distributions of the latent variables that has a trajectory structure with the posterior of each cell in the latent space. Space and in the standard variation of encoder, we have beta equal to one. But here, because we want to encourage the cells to have a trajectory structure on the latent space, we set beta to be in our default, we set to be two. But in general, we can choose a two-new parameter larger than one to encourage the To encourage the penalty if our posteriors are far away from this trajectory structure. And the third part of the laws is to encourage batch correction or data integration. So it's borrowed from an idea in SaverCat, which adds this penalty to encourage we learn as much information as possible from Information as possible from the confounding covariates itself, and that can help us to softly decorrelate our latent space from this confounding covariance, such as batch indicators or cell cycle scores. And we also have a few other details in our implementation. For our model, because we have a graph, so we need a good initial. Have a graph, so we need a good initialization of the graph, and we also need to determine how many vertices do we want to include in the graph. So, what we do is that we first pre-train our model with that do not use any prior information of the latent space, and then we can do a perform a clustering on the latent space after pre-training and use the number of clusters as the number of states. And our model is. And our model is relatively robust to over if we slightly overestimate the number of states because we can merge excessive cell states during training if two cell states become too close to each other. And if we want a fast computation, we can also we can also replace the high-dimensional observed data by just maybe let's say the first 100 principal component scores. And then I will finally I will briefly outline how we perform, infer the trajectories and cell positions by approximate Bayes inference. So after training, we have approximated posterior. Have approximated posterior distributions of the cells' relative positions on the graph. And our first step is we need to infer the trajectory backbone. So we define an edge score, which is based on each cell's relative probability of being assigned to the edge, given that the cell is not too far away from the two state. Away from the two states that the edge connects. And once we can determine the trajectory backbone, we will project the cells onto our inferred trajectory to make sure that the cell's position is within our inferred trajectory backbone. And because we have the approximating posterior distributions of the cell positions, we can also Positions, we can also approximate the posterior variance of these projections of the cells. And we use that as a representation of the uncertainty of the estimates of the cell positions on the trajectory. And then we also, in order to change our trajectory backbone, which is currently an undirected graph, into Into a directed graph, as other trajectory inference methods, we need to determine a route where the developmental process starts, and it can either be determined manually, or we also modify an idea in tempora if we have cells collected from different collection days. So we calculate. Collection dates. So we calculate a score for each vertex, which is the average collection date of cells near each vertex. And we choose the vertex as a root if its score has the lowest number. So it's representing that the cells near this vertex has the earliest collection date. And then given this directed trajectory backbone, Reacted trajectory backbone graph and also our projections of the cells on the trajectory backbone. We can also calculate the pseudo-times of each cell. Okay, so that's the introduction of our method, and I'll now show a few benchmarking results. So we borrow, so we use the benchmarking framework by this very popular benchmarking paper. This very popular benchmarking paper for trajectory inference methods. And we look at 10 different real data sets and about 15 different synthetic data sets. Some are from this benchmarking data and some are using our own deep learning model to generate synthetic data. And here, in this results figure, it shows a data set. So the top 10 are the So the top 10 are the real data sets and the later 15 are the synthetic data sets. And we compare with, so the, and for the methods we compare with, we first include our own VIT methods, the two versions, the original version and accelerated version with the principal components as input. And we also compare with three other very popular trajectory inputs. Very popular trajectory inference methods like monocon 3, PAGA, and slingshots. And for the metrics, we look at different aspects of the inferred trajectory structure. So the first two scores represents the recovery of the trajectory topology. And the next two scores represents the accuracy in cell position estimation. And the last one represents the pseudotype. Represents the pseudo-time accuracy. And you can see that we have comparable or even slightly better results compared to these existing popular methods. And now I'll show, I'll go back to the mouse neural cortex data, which really shows our strengths when we try to align multiple data sets and learn a shared trajectory. So here, I compare to the figure that I've shown earlier, I also include the results if you do. The results, if you do Surat integration. So you can see that Surat also has done a reasonably well job to integrate both two data sets. And if you look at the difference between Surat integration and VIT, there are some small differences. For instance, for this layer one cells, we have a connected structure for all the layer. True for all the layer one cells, but throughout integration makes them to be separate because some of the cells are from data set A and some are from data set B and they're also collected at different time. So that's why thread makes them separate, but they're actually belonging to the same category. And you may not see much difference in terms of data integration, but then if you look at the estimated trajectory, Viet can indeed. Trajectory, Viety can indeed provide a much cleaner trajectory structure compared with a conventional approach, which we first use throughout integration, then we perform trajectory inference using methods like slingshots on the integrated data. You can see though the data integration looks fine, your trajectory inference on the integrated data indeed is much more noisy than what we do, which simultaneously estimates the trajectory structure and the data. The trajectory structure and do data integration. And we also have comparable computational costs with this conventional approach. And here is another representation of the cells where I color the collection days. You can see that for this sub-trajectory, which shows the transition from neural epithelial cells to radia glial cells to the Glia cells to the oligodendrocyte progenitors, you do see very good alignments of the cells in terms of their collection dates. So you can see that we have an increasing alignment of these collection dates in each order, though we have even dates collected in one data set and author dates collected in another data set. So it shows that we can. Data set. So it shows that we can keep some biological meaningful differences between these two data sets, two data sets, while integrating to have a shared trajectory structure. And in addition, we can also identify marker genes along the trajectory. I'll omit the detail of how we do it, but here I show you the top four marker genes in two. The top four marker genes in two of the sub-trajectories, and you can see that we have a consistent trend of the gene expression between the two data sets, also showing that we have a reliable performance when we integrate both data sets. Okay, so the take-home message is that we perform a model-based trajectory inference to understand the cell dynamics, and we Dynamics, and we combine vibrational autoencoder with a mixture prior on the latent space. And the biggest benefit of our method is that we can integrate multiple single-cell RSIC data sets to learn shared trajectories. And there are some other ongoing extensions we are trying now. And that's all. Thank you.