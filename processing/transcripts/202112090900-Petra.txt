Noemi Petra from UC Merced and Noemi, who I want to thank for all the help with the organization, although she's not, you know, doesn't have a name in the, as an organizer, she is a co-organizer of this workshop, as you have figured out by now. Figured out by now. So, thanks so much, Naomi, for all your help. Well, thank you for suggesting to organize this. It was very, very exciting, the whole road from the beginning to the end. And hopefully, this will continue in the future. More on that tomorrow. Okay, thank you very much for the invite, Frishula. So, my presentation is a little bit different. My presentation is a little bit different in the sense that I will not focus on technical research components of my work. This was supposed to be at the end of the workshop tomorrow, like the last presentation. So I thought maybe it's a nice way to wrap up with a software framework that will allow you to implement various algorithms we have discussed during this workshop. And the schedule needed a little bit of update. Bit of an update. So I ended up presenting today. So, but still wanted to keep this because I'm hoping that what I show you guys today, it will be useful for various reasons, for research, teaching, mentoring, whatever. So please bear with me. There are a bunch of pictures, not so many equations that I usually like to present. But again, hopefully, this will be useful. And I will start my timer. I like Carola's. I liked Carola's strategy yesterday. All right, so I will talk about this software framework. My collaborators and I have been working in the last couple of years. It's called Hebrew MUC. This is scalable Markov chain, Monte Carlo sampling methods for large-scale vision universe problems governed by PDEs. This is in collaboration with my postdoc, Kite Kim, my collaborators Umberto Villa, Mal Parno, Justif Marzo. Vila, Matt Parno, Yusuf Mazuk, and Omar Katas. This project started actually about five years ago. Well, Hippilef started way earlier, but the integration of the two HIPELIP and MALC started about five years ago with an NSF proposal. We were very fortunate to get funded. And in fact, I just am due to submit my final report at the end of the month about this project. But this proposal and this. This proposal and this grant helped us to put all this software framework together. And as you will see, it was very useful for at least for our groups. So I already mentioned NSF, but I also want to for the ACI programs, but also would like to mention that the algorithmic parts of this have been funded under my career from the Diffusional. From the Division of Mathematical Sciences Computational Math career NSF grant. Okay, and I will start with some pictures from my last visit at Banff. I just want to show you how gorgeous this place is. And I really, really, I'm really sorry that we have not had a chance to go there in person, but I'm hoping that maybe next time it's gorgeous, but it's also going to be very cold on the right, I'm showing actually the day. Right, I'm showing actually the day when we arrived. Well, the second day, this was a Monday, it was minus 28 degrees Celsius. Okay, that's about 16, minus 16 Fahrenheit. Freezing, freezing. This is the place where we were having breakfast and lunch. Maybe Chrisula mentioned the food is amazing. Yeah, just the scenery. If it's not super cold, you can actually go out and go around and visit. Out and go around and visit and go skiing and whatnot. I did not do that. I was not that courageous. So, again, I'm hoping we'll go back. And if you have not had a chance to visit this place, please don't hesitate. Okay, and this was actually a mathematical modeling in geology group I visited in or I attended in 2020. My maybe main application that I'm working on is related to glaciology. It's ice sheet flow and inversion. Flow and inversion for eye sheet problems. This was the group, fantastic group of researchers who are doing work in this area. And I think the birth organizers really matched the cold and the glaciology and ice of everything. It was just a very nice thing. Yeah, so anyway, I was very, very happy to go to this workshop. As I mentioned, my primary application. Mentioned my primary application is related to greciology. And, you know, I can't say that this is the only thing that inspired me to work in this research field, but I really like the motivation behind it. So I think I'm reaching to the choir that we really need predictive models with quantified uncertainties to accurately anticipate various quantities of interest. Quantitative of interest in this case, I'm interested in the anticipation of future sea level rise because this is important to understand for obvious reasons. And I'm showing this picture a figure that I took from the NASA Earth Data NASA website. This is the sea level projection. And I'm not sure if you guys see the numbers, but I was just looking at New York and the total sea level rise predicted. The total sea level rise predicted by 2100, it's about 0.93 meters. This is a disaster from the point of view of coastal flooding and all that. So it would be amazing. And I think yesterday we had a little bit of a discussion of bringing in our research and apply it to real problems and applications and really contribute to decision making. And this is one of my hopes that. And this is one of my hopes that our research will make it one day to maybe better anticipate these projections and future sea level rise. Okay. All right. So there are a bunch of challenges in this research, in particular in inversion for large-scale ice sheet problems. And probably are recognized that these are the challenges we are all facing and we are wrestling in this field. Wrestling in this field. So, some of the challenges I would like to mention for inverse problems governed by non-linear stokes that model ice sheet flow are complex and very high aspect ratio geometry. For instance, I'm showing on the right the Antarctic ice sheet geometry, highly non-linear and anisotropic rheology. This goes into the non-linearity of the Stokes, extremely air-conditioned and large-scale linear and non-linear algebraic system. Scale linear and non-linear algebraic systems that arise from discretization, lots of uncertainties stemming from basal boundary conditions because we cannot go to the bottom of the ice to actually observe things and measure things. And these are, you know, laws or boundary conditions that we are prescribing in a Prescribing in a more or less phenomenological way. So these are all approximations. And then, so we parameterize these boundary conditions, and then we have to invert for them from available data, such as surface ice flow velocity measurements, which are available out there. So lots of uncertainties, and I mentioned the basal siding parameter, but there is also basal topography, geothermal heat flux, rheology, just lots of things. Just lots of things. There's also a lot of modeling error. These models are more complex, and most of the times we are using more simplified, steady-state type of models to try to say something about the or improve maybe the predictability of these models. There is a lot of modeling error as well infused into these problems. So challenges that probably we are all facing. Probably we are all facing in one way or the other. So, my research is all about exploiting the problem structure and account for model errors and quantify the uncertainties in the inference. So, how can we address some of these challenges? And some of these things have been already mentioned, and we are all talking about this research directions. So, we learn models from data. Models from data, it depends what I mean by this. It's this via inversion, infer unknown certain parameters from available data, and they see this problem as a statistical inverse problem governed by the underlying PD. To make this the solution of such problems somewhat tractable, this is somewhat because we are still not able to solve this fully in the right. Statistical space, if you wish. We need to apply adapt design, fast, mesh-independent, structure-exploiting, inner product-aware, and additional uncertainty-aware methods. So basically, we need scalable, robust, and efficient algorithms. And this is something I will not touch in this talk at all, but I would like to mention that my research is also concerning concerns optimal experimental design. Basically, I'm interested. Experimental design. Basically, I'm interested to collect in an optimal way data in order to minimize the uncertainty in the reconstruction. All right. So this is kind of summarized in this slide I borrowed from my collaborators, Penny and Alan, at UTOS and NC State. So we have seen patient infrastructure problems in this workshop quite a lot. What I want to say. What I want to say here is that these are the components that my research is touching on. So, basically, I assume that there is a complex physical system that we want to maybe learn or improve and use. And in the situation, in the ice problem situation, this would be the Stokes. I'm also working on other applications such as PowerWid and subsurface flow. And subsurface flow, and other things, mostly from Jewish physics. So, these physical systems, as we know, are equipped with unknown parameters or uncertain parameters. And we use vision inference to infirm these uncertain parameters from available data. We start with a prior, something that we know a priori about the parameter without taking into account the data. Without taking into account the data, then run this Bayesian inference framework, and then this will give us a posterior distribution, which is an update of the prior using the data. Okay, so hopefully that has a smaller variance, meaning that we reduce the uncertainty in our reconstruction. We usually want to do more with that. We usually have a quantity of interest, for instance, the A quantity of interest, for instance, the ice flux going out of or the ice flux in the ice problem. This is a so-called quantity of interest. So basically, we do some sort of forward uncertainty quantification to incorporate the uncertainties in the parameters in the model to say something or to predict this quantity of interest or to describe the distribution of this quantity of interest. This content of interest. Okay, and I mentioned also that it's important, if possible, to optimize the way we collect data, and this gives rise to this optimal experimental design additional kind of research task or direction. And one can do also some optimal control with what you learn from this. What you learn from this framework. This Hippily Muck software framework is actually focusing only on the green and maybe the green part, maybe this brown thing and the forward UQ. We have not incorporated yet, at least not in the release public version, the OED and optimal control components. This is something we are working on. This is something we are working on. Okay, so these are the so-called outer loop problems. By auto-loop, I mean the need to execute a forward model, an inner forward model for various reasons for differing parameters and variables. And so in what follows, I will start talking about the software components that go into making this solution of this problem possible. Uh, possible. Before I even start, I would like to say that I'm sure, I mean, we have seen so many results. I'm sure you guys have all kinds of tools and really good software that allows you to solve numerically your inverse problem. What I'm hoping at the end to have a discussion about what else we can add here and how we can incorporate what you have. And so, if you have ideas, please do let me know. Or if you have problems that Or, if you have problems that you would like to try to solve with this software framework, also please do let me know. And any feedback would be very, very welcome. So what are these algorithmic computational challenges? Well, we need to characterize the posterior solution of the statistical inverse problem. And this requires repeated evaluations of large-scale PDE models. And the scale comes from the discretization. And the scale comes from the discretization, and most of the times, these bidding models that we are after in the context of the eye sheet, this is a non-linear stock, so it's quite nasty to solve. The posterior distribution often has a complex structure, stemming from non-linear parameters to observe a math and heterogeneous sources of data. And the parameters are fields, hence, after discretization, these become large-scale or very large vectors. Very large vectors. Yeah, these difficulties renders complex large-scale PD-constrained region inverse problems intractable via standard NCMC methods. So, what are the implementation challenges? Methods that facilitate the solution of these inverse problems require a diverse and advanced background. And I'm not sure how it is in your institutions, but one of the difficulties I'm facing with graduate students is that we need there is a lot of There is a lot of how to say it, there is a big learning curve. It takes quite a while to train a graduate student to be able to get to the point where they can do research. So all this requires all this machinery or algorithmic framework requires knowledge from PDEs, numerical methods for PDEs optimization, numerical optimization, function analysis, and probability theory. Probability theory and statistics, numerical linear algebra, and I hope I did not miss anything, predictional calculus, and so on and so forth. So it takes quite some time to get someone ready to do actually research in this field. And also, if you need to implement every time all these components, it's really maybe not the best use of one's time. And I don't want to say And I don't want to say that students should not implement from scratch things and learn, but when you need to speed research a little bit up, maybe it's better to or useful to build on existing software components. Yeah, so to be efficient, these methods to be efficient, usually you need to use first and second derivative information. These are optimization problems at the end of the day, constrained by PDEs. Constrained by PDEs, and so we need efficient methods to solve these problems. Typically, we rely on Newton's methods, so we use adjun-based gradient and Hessian applies. So, the big vision we had a couple of years ago is to develop an extensible and share an extensible software framework aimed at overcoming these challenges and providing capabilities for additional algorithmic developments for this inversion. Okay, so I will. Okay, so I will start focusing on the first library, which is Hibli. And this is actually what I contributed to. In fact, MUC is not something I worked on, but we interface with MUC. And I will talk about that in a few minutes. So we started with this inverse problems Python library. And what this is, is it's a Python-based library that implements various That implements various algorithms developed by my collaborators and I, or what we found kind of state of the art in the literature. So this contains implementation of scalable adding-based algorithms, PD-based deterministic and vision inverse problems. It builds on Phoenix. We are looking at Firedrake also for the parallel finite element part needed by the PD solves. Needed by the PD SOLPS. And on PETSI for scalable and efficient linear algebra operations, the nice thing with Phoenix is that PETSI is, and with File Deck as well, is that PETSI is really easy to use. Okay, Hippolyve is implemented in a mixture of C ⁇  and Python and has been released. It's open source. It's open source, and if you are interested, here is the Git link. You can download it and let us know if you have questions. One of the main features of this library is that it exposes specific aspects from the model setup to the inverse solution. It's not really a black box. You have access to all these components and it really can be used to implement or apply it to various applications. Apply it to various applications, PD-based applications, or government applications. And it can also be useful for teaching and learning, as I will show you in a second. In case you are not familiar with Phoenix, I just want to say that this is an open source computing platform for solving Please. It's based on Finite Element method. What else? The name comes from Finite Element. CS represents computational software and Represents competition software, and according to the Wikipedia, NI sits nicely in the middle to make up this nice name, Phoenix. What are the algorithmic components that we have implemented? And I want to give you a list in case you want to try to understand if this is something it's useful for your students. It's a, as I said, it implements, we have a, it's set up so that you can just, without changing things too much, you can apply to various possibly non-linear and time-dependent PDEs or time-independent PDs. It implements adjoints and hash and actions needed to solve the deterministic use problem. And I will show you all the setup in a You all the setup in a few slides and will allow you to compute the maximum, so-called maximum aposterior point for the Beijing inverse problem. So, basically, it will solve the deterministic inverse problem. We use inexact Newton conjugate gradient in conjunction with line search algorithms to guarantee global convergence of the optimizer. We implement randomized algorithms to compute low-rank approximations for the hash shift. Complete low-rank approximations for the Hessian. We are also approximating the posterior with a Gaussian. Basically, we construct the Laplace approximation of the posterior. We are also offering sampling. We implemented or we added support for sampling from these Gaussian distributions. And yeah, you can also estimate the point-wise variance to. Variance to look at the results and see how much the uncertainty or uncertainty increased or decreased, if you wish. So, all these are the algorithmic components. And I just want to mention a couple of research kind of directions and results using HIPLIB. I would mention opto-acoustic tomography, acoustic scattering, CO2 sequestration, seismic electromagnetic inversion. Inversion, eye-social interaction, medical imaging, if you want, turbulent combustion, some elasticity type mechanics, vision approximation error for inverse problems, eye-shit inversion, and other applications. We are using Hibileep a lot as a teaching tool, and I would like to mention a couple of words. And I would like to mention a couple of workshops we have been running. For instance, the Jean Goulum Summer School. And I'm not sure if Yvonne is here, but I would like to thank her for support. She was at NSF at that point. And in addition to some funds, we got funding also from NSF to be able to support even more graduate students. At Samsi, at ICERM, we have created several pedagogical tutorials. Several pedagogical tutorials which are all online. We are also using this for our classes, and in fact, this semester, well, I just finished last week, I am teaching an inverse problems class, and this was used a lot for projects and homework assignments. And so it was really useful. I would not be able to teach a competition inverse problems class without this. Yeah, and I just Yeah, I just want to show this picture from Alster, and I think Andrea, that's where I met actually Andrea. And yeah, it was fun. And then this is from the Eugene Goluk Summer School where I actually did not make it because David just was born one week before that. But I really like the Gaussian shape that the students and my collaborators came up with. Looks good. And we have a chat. Looks good, and we have a chat. Uh, Chrisula, can you please let me know if there are questions just in case I'm missing something. Um, all right, the we have some good community already. Um, and just this year we had about 14,000 hits of our websites, pretty much from lots of places. And this is the overall from the beginning, and um, it's um It's quite nice to see that people find this software framework useful. And if you hear anything in the background, that's David going to the daycare. Okay, so yeah, I'm really happy to see that this tool is being used and it's useful. As I said, the second component of this framework, so CPLE gets you to a Gaussian approximation of the procedure, but we But we need more than that, we need to be able to explore efficiently full posteriors, which may not be Gaussian. And so, for that, we are interfacing with MUC, which is a library to do exactly that. MUC provides an easy-to-use framework for defining and solving UK problems. And it gives you a library of proposals that you can use in an MCMC framework. And if you have not used it, I highly recommend. Use it. I highly recommend this tool. It's really nice. It's really nicely done. It's developed by Matt Cornel. He's now, I believe, at Darmut. Also, and his collaborators, Andrew Davis and Linus Sillinger. In the interest of time, I will not go into that too much. So to give you an idea of what are the components of this software and all these are actually Actually, components that one can, it's really not that black boxy. You can actually look, it's open source, you can modify it, you can adapt it, extend it based on your needs. And I think I already mentioned that we use Phoenix and PETSI and what are the algorithms that go into Hiblib. And then the MOC, which has all kind of posterior sampling capabilities, surrogates, to build surrogates, prediction tools. Prediction tools. The interface is really just making the two work together. MOC kind of needs all these derivative-based everything that which comes from the model. So Hippolyte brings all that in. And in addition, the interface is doing a lot of convergence diagnostics and all that. All that is nicely implemented. So, let me give you an example: a coefficient field inversion in a PDE. It's a very simple prototype inverse problem governed by this POSM problem. So basically, we are after inferring the parameter M using data on U. So, here, this is something we propose as a parameter field. So, the PD and a perturbed. PD and perturb it with some noise and get the data which are shown in these white dots. And so we want to estimate the log permeability in a subsurface flow. For instance, you can interpret it as a log permeability field M. Okay, so this is something I don't have to explain to this girl for sure. What I just want to kind of send the notation. Kind of set the notation and all the components that go into this software. Basically, we compute the procedure using a Bayesian or Bayes theorem, which combines the so-called likelihood and the prior to get the posterior. In Hippoly, we work with a Gaussian prior, which is characterized by the mean and the covariance. The covariance, we typically build it using Build it using some differential operator-based procedure. Okay, so the first thing that you can do, you can compute the maximumly posterior point, which is defined as the parameter that maximizes the posterior distribution, which can be also written as minimizing the argument of that exponent. This is basically a deterministic numbers problem with some special weights. Now, if Now, if this so-called parameter to observable, which gives you the solution of the PDE and maybe combined with some observation operator on top, if it's linear, then the posterior is known to be Gaussian and the mean and the covariance are given by the solution of this deterministic inverse problem and the inverse of the Hessian of this objective, respectively. Allow me not to go into Allow me not to go into the detail. I'm sure that most of everyone kind of knows already this. I just want to emphasize that in general, this mapping is not linear, and so the posterior is not Gaussian. And so one needs to use sampling to characterize the posterior. And some results actually, I will go into this website. Krishna, do you see? Krishna, do you see the other window or I need to share? No, we only see the PDF. Okay, good. All right, so I just want to show you all the setup that I'm actually using in my class. So a bunch of things that we are doing, but we built on Hippolyte. And this is the example I'm presenting here to you guys. By the way, this all runs on Exceed, and I want to acknowledge also funding for computing. And all my students have a user and just punch it in in a web browser. And everything is installed for them. And usually, this is what we do in the workshop as well. So I can run these things on Excel right now. And it's basically the same problem. Yet I showed you the possum inversion with the possum problem. And in the interest of time, I will run everything. And I hope Georgia fingers cursed that this works. Live usually never works. Let me see, does it? Yeah, it does. Okay, so just want to show you how an example is set. An example is set. So we load all the modules, the CSA modules in Hippolyte. We generate the true parameter field. Basically, we set up a prior, sample it, set it as a truth, solve the forward problem, add noise, use it as grab some point-wise observations, use it as the data, and so on and so forth. So just to show you how. So, just to show you how simple it is to set things up, this is for a unit square, I think. But if you have something more complicated, one can use GMesh and import the mesh here. We set up the forward problem with all the goodies. So the nice thing is that if your application changes, basically you just change the boundary conditions, the variational form or the width form of the problem, and so on and so forth. Of the problem and so on and so forth. Set up the prior, as I said, we are using a differential operator-based prior and kind of following this year's framework to stay consistent with the infinite dimensional or have a well-defined prior in infinite dimensions. So, here we are using a so-called bi-laplacian prior, I believe. Okay, so this is a true parameter, this is the prior mean that we. Parameter: This is the prior mean that we start with. I don't know if I use the variance probably later. So we set some constant mean, some larger variance, and then use the data. And hopefully, that variance decreases usual gain. We use 50 observations, about 1% noise. These are the observations we produce, synthetic observations. Okay, of course, we have tested. Okay, of course we are testing the gradient and the hash rate with finite differences. Of course, you can may not be able to do that in a large-scale setting, but at the prototype level, which we always start with, we check this adjoining-based gradient that Hessian applies against some finite difference. And then use inexact Newton Cg to compute the solution of this determinant. Solution of this deterministic inverse problem. Basically, that's how we get the map point. As you can see, it's converging quite nicely. We use inexactness, so you see small number of CG iterations at the beginning, more iterations towards the end. And the tolerance changes because it's relative to norm of the gradient. And we decrease the we start when the norm of the gradient is here, it's quite large. It's quite large, about 10 to negative 5 or 6. Okay, this is what we get, the recovered state solution that is obtained by if you plug in the solution, the parameter that you get into the underlying PD, this is the map point basically. Point basically, we can also compute low-rank approximations of the Hessian at the map point. And since you can see, there is a lot of kind of unimportant directions. I think Elizabeth did a really good job explaining this low-dimensional subspace that usually is happening for these inverse problems. So we measure the lower end means. Low-ranking is decide what is small based on some Sherman Woodbury, a Morrison-Woodbury formula, which I unfortunately I cannot explain, but probably you might know about these things. You can also look at eigenvectors just to confirm that high oscillations are occurring for small eigenvalues. We can compute the point-wise variance here. We can compute the point-wise variance fields. And so, as I said, we start with some large variance, and using the data, we reduce it. I'm not sure maybe the scale is not ideal or optimal, but I hope you can see that where you have observations, just what you expect, that the variance is reduced or the alternative is reduced where you have observations. Then you can use also, as I mentioned, Hebrew offers the capability to look at samples of both prior and posterior. Higher and posterior. This is pretty much it. And let me just go to the tutorial with the MUC. So if you take now this problem and combine it with MUC and do a full-blown MCMC exploration of the posterior, again, this is all public and open source. I just want to bring up I just want to bring up again, we set up the problem the same way, slightly different setup, but I want to go down and say that. So for instance, if you are interested at the end of the day, the quantity of interest, for instance, you want to characterize or find the distribution of a log of the flux through the bottom boundary, for instance, you could define a quantity of. You could define a quantity of interest like this, and you can do some forward UQ to get samples for this distribution. You know, there is a lot to say. I mean, I could probably one can teach a whole class about this. And those of you who are familiar with MCMC methods, you know that there is, if you use some sort of metabolising, except reject criterion, there is a proposal that you build, the proposal should. Proposal that you build, the proposal should have some properties, but also should be cheap, but a good approximation of the posterior. And just want to emphasize the fact that applying or using or trying various proposals available in MAC, it's as easy as a couple of lines. So basically, one, maybe I should make it a bit bigger. Make it a bit bigger. For instance, this is the preconditioned crank Nicholson-based MCMC method. So basically, this is how you get the proposal. You choose the metabolis hazing, so-called kernel, accept, reject criterion, get the samples of chains, and so on. So forth, if you want to use a mala, so-called mala, and I always forget the name. Mala, and I always forget the name of it. Metopolis justice linger. This is, you know, a couple of changes in the code. And then, of course, here I try to advocate, probably everyone advocates for curvature-inflamed MCMC proposals. And this is the Hessian-informed precondition Grant Nicholson. And the Hessian. And the Hessian comes from Hebelie, and then the Hessian could be updated at every sample or approximated with some Gaussian around the map or around the sample and so on. And then yeah, various methods that can be called from MUC. And then what we also built, we try to have a nice mark kind of gives you. A map kind of gives you lots of convergent diagnostics, but we have to somehow compile it and combine it with what we have from Dublin. And at the end, all this gives you pretty much a table, which allow me now to go back to my slides and give you just a table. Here, big table I'm showing you here, but basically, these are just a very special. Basically, these are just the various pathets that we are trying from MAG. And we have not, I think MAC currently implements also a multiple level in Monte Carlo. We do not have a chance to add it yet. But if you are developing some improved MCMC type method or improved proposal and you would like to try it out, we would like to add it to our comparison. So basically, this table like this, hopefully, is useful for the Hopefully, it's useful for the community to see: okay, if I have, if I can afford these many samples on this type of grants, and if I have gradient or Hessian or what exactly can I use, and does it pay off? And yeah, how to choose these methods. So, I just want to bring up the HPCN. This is the Hessian-informed precondition crane Nicholson. I'm looking at the acceptance rate. The MPSRF kind of measures the convergence of the chains. Of the chains. It's called multivariate potential scale reduction factor. I will also look at the, yes, as the effective sample size, basically how many independent samples you get. And then this, the last column is one that we monitor closely because this is the number of forward and or adjoint PD salts required to draw a single independent sample. And we try to keep this low because we cannot, usually, we cannot afford to. Usually, we cannot afford too many piny soils, and so we are after methods that kind of are able to approximate the posterior with affordable number of PD source. And also, yeah, I will focus on the HPCN because this is quite good. There are not too many parameters to tune. Everything is in the parentheses. There are some knobs that one has to tune. It's really difficult to do that, especially when it's. To do that, especially when it's a large-scale problem, you cannot afford trying too many things. So, from this table, really, I really like the HPCN because it gives you maybe not the best acceptance rate, but the MPSRF looks good. The number of PV solves needed to get some, and I have the timer. Yeah, so that's what I will, but of course, the better. What I will, but of course, the better the best is the so-called dimension, some likelihood dimension independent likelihood informed math. Pretty much TC and Code de law and Yusuf's work on this. It came up the most efficient one, but HPCN is something that I like to use when at the prototyping level, at least. And so one can look at the autocorrelation function, which will Autocorrelation function, which will tell you something about how efficiently you estimate the effective sample size. And again, we are comparing all these methods available in MAP. We can look at the mixing of the chains or the trace plots. As you can see, anything that is not curvature informed, they are not as efficient. So these are probably most likely out of question when it comes to large-scale. Out of question when it comes to large-scale problems governed by PDs. And we are interested in methods that show some sort of mesh independence. And the HPCN seems to look quite well. So we are testing, for instance, up to about 66,000 parameter dimensions and the acceptance rate stays quite stable, almost constant. The MPSRF looks good, the average effective. The average effective sample size is also somewhat constant. So, this is very encouraging if one wants to solve larger scale problems. And on the right, I'm showing also the spectrum of the Hessian, which as expected, it's mesh independent. And this actually gives it kind of the basis of why we are getting what is scalable. With these scalable algorithms and machine dependence. Okay, so to summarize, we like to think that we have a robust and scalable software framework for the solution of large-scale vision inverse problems covered by PDs. The software that I showed you integrates two components, HIPLIB and MUC. The objectives are to help the community to speed up research, but also to To educate maybe the next generation inverse problems community. I want to emphasize that everything I presented here is written up nightly in this draft at this point. This has been submitted to TOMS basically last or two weeks ago. You can download it from archive. If you have comments, feedback, anything, it's very welcome. This is right now under review. We can still improve on things. We can still improve on things. So please don't hesitate to let me know if you have ideas. Everything also is available on websites, open source, and on GitHub. Before I end, I just want to say that this is not the end of the story. I mean, we have this nice software framework, but the algorithms are still, they are still working on algorithms to solve a large-scale Bayesian inverse problem. A large-scale vision inverse problems governed by expensive to solve PVs. We are still wrestling in that war. So, of course, the software can do that much and kind of bring the software along the algorithmic development. But we hope that PBLMOP can be used as a prototyping environment to study new methods that further exploit problem structure or bring in other innovations for pushing the boundaries for this in this. The boundaries for this in this field. And in particular, I'm mentioning the reduced models and building surrogates and advanced Hessian approximations that go beyond low-rank. I'm talking about off-diagonal low-rank approximation and so on and so forth. These are the directions my graduate students and I are working on and hopefully we'll add some more to this framework. And with that, I will take questions if we have any time left. Thank you so much for listening and I hope this was not too boring. And I hope this was not too boring, but I'm happy to talk about research anytime, my research anytime. So please reach out if you would like to discuss more. That's all. Thank you very much. Thank you, Noemi. I think Balvara has a question. Yes. No, I have no question. Sorry. I just came in two minutes ago because of that finding committee. You know about this. Committee, you know about this. I regret very much that I couldn't hear your talk. I just want to make a comment. It's a Hippie Lib is really a great tool. So, my PhD students, Anna Schlintel, whom you know, I think from the summer school three or four years ago, used it a lot and it was really, really valuable for us. Thanks a lot for that. Oh, fantastic. Thank you so much, Barbara. I'm really happy to hear these things. And this is one of the, you know, I love doing research. I love doing research in universal problems, but I also like to share and work on community codes. And I mean, I'm working on them anyway. So I really like kind of collaborating on putting together a nice maybe support for our community. So happy to hear that it's useful. Shoo also wants to say something. Wants to say something. I know, Emmy. You showed these really impressive, huge problems on the table at the end, where you got an MPSRF below what we needed to do to indicate convergence. Yeah, that takes. Yeah. You comment on maybe it was in here, but how many samples did you need for students? Yeah, good. Yes, I did not have a chance to, these are the numbers. Have a chance to these are the numbers. Um, so we had 20 independent chains, each with 25,000 samples, and so total was half a million samples. Okay, all right, good. And how long did it take to do that? 500,000 samples? Um, so so we in the report or the draft or the archive paper, we show, I think, two problems. One is decent scale and one is a little bit larger scale in 3D. In 3D. The one with the larger scale took a little bit more. I know my postdoc was kind of working on it a lot and running on our Merced cluster for a couple of days. The other one was not too bad, but as you can imagine, I mean, one of the reasons it took us so much time to put this draft together was that creating this table was not easy. And tuning all these knobs and running it. All these knobs and running it, and then you realize, oh, I changed something, so it was not all consistent. Go back, redo it. I'm really grateful to Kite, who did a fantastic job to run all these things. I don't know exactly the time, but I can ask him and tell you probably a order of a couple of days for the table. Wow, that's great. Okay, thank you. Thank you. I don't see any. Thank you. I don't see any other questions. I think I have the chat that you saw when we were talking, Ms. Andrea, who said that she loved the workshop. I don't know any other remarks.